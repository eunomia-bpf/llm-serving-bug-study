[
  {
    "number": 3568,
    "title": "how to use fp8 for inference on h20?",
    "body": "I have a problem, that is, the model I am currently deploying is: neuralmagic/DeepSeek-R1-Distill-Llama-70B-FP8-dynamic.\nThe graphics card is h20.\n\nI would like to ask how to infer the fp8 capability of this graphics card?\nCurrently, sglang is used for deployment. The deployment instructions are as follows:\n\npython -m sglang.launch_server --model-path neuralmagic/DeepSeek-R1-Distill-Qwen-7B-FP8-dynamic --port 30000 --host 0.0.0.0 --tp 2 \n\nWhat I want to know is that my command has enabled fp8 for inference operations? If not, can you tell me how to do it? Thanks",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-02-14T05:59:47+00:00",
    "closed_at": "2025-04-16T00:18:29+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3568/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3568"
  },
  {
    "number": 3923,
    "title": "[Feature] Add a hash for each new release",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n## Summary\nImplement SHA-256 hash verification for all package releases to enhance security for users installing from mirror sites.\n\n## Description\nUsers who download packages from mirror sites instead of the official PyPI repository need a reliable way to verify package integrity. Adding SHA-256 hashes for each release would provide a standard method to confirm packages haven't been tampered with or corrupted.\n\n## Implementation\n- Generate SHA-256 hashes automatically as part of the CI/CD pipeline\n- Include hashes in package metadata files\n- Make hashes accessible through the official website\n- Update documentation to explain the verification process\n\n## Benefits\n- Enhanced security for users with limited access to official repositories\n- Protection against supply chain attacks through compromised mirrors\n\n## Technical Considerations\n- Minimal changes required to existing build processes\n- Can be applied retroactively to previous releases\n- Leverages pip's existing hash verification mechanisms\n- Low maintenance overhead once implemented\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-02-27T08:39:19+00:00",
    "closed_at": "2025-04-30T00:18:49+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3923/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3923"
  },
  {
    "number": 5498,
    "title": "[Bug] Qwen-gme embedding model: cannot get fused embedding from text+image, and image input format may be incorrect",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThanks for the great work!\n\nWhile using the /v1/embeddings endpoint with the gme-qwen2-vl model, I encountered two issues:\n\n**1. Incorrect handling of image input**\nAccording to the docs, the image input is passed like this:\npayload = {\n    \"model\": \"gme-qwen2-vl\",\n    \"input\": [\n        {\"type\": \"text\", \"text\": text_input},\n        {\"type\": \"image\", \"url\": \"image_path\"},\n    ],\n}\nHowever, this does not seem to be properly recognized. The server throws the same results when input different image_path unless \"url\": \"image_path\" is replaced with \"image\": \"image_path\"\n\n**2. No fused embedding returned for multimodal input**\nWhen sending both text and image in the input, the server currently returns separate embeddings for each (i.e., a text embedding and an image embedding), but not the fused multimodal embedding as described in the Qwen-gme official documentation.\n\n\n\n### Reproduction\n\nhttps://github.com/sgl-project/sglang/blob/main/examples/runtime/multimodal_embedding.py\nqwen-gme embedding model\n\n### Environment\n\nPython: 3.11.2 (main, May  2 2024, 11:59:08) [GCC 12.2.0]\nCUDA available: True\nGPU 0: NVIDIA A800-SXM4-40GB\nGPU 0 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.161.08\nPyTorch: 2.5.1\nsglang: 0.4.5\nsgl_kernel: 0.0.8\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.51.0\ntorchao: 0.10.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.5\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 24.1\npsutil: 6.1.1\npydantic: 2.10.2\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.29.0\nuvloop: 0.21.0\nvllm: 0.7.0\nxgrammar: 0.1.17\nopenai: 1.75.0\ntiktoken: 0.7.0\nanthropic: 0.49.0\nlitellm: 1.66.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    NIC0    NIC1    NIC2    NIC3    NIC4    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     SYS     SYS     NODE    PIX     60-119  1               N/A\nNIC0    SYS      X      SYS     SYS     SYS     SYS\nNIC1    SYS     SYS      X      NODE    SYS     SYS\nNIC2    SYS     SYS     NODE     X      SYS     SYS\nNIC3    NODE    SYS     SYS     SYS      X      NODE\nNIC4    PIX     SYS     SYS     SYS     NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n\n\nHypervisor vendor: KVM\nulimit soft: 1024768",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-04-17T13:09:28+00:00",
    "closed_at": "2025-07-11T00:20:24+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5498/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5498"
  },
  {
    "number": 2671,
    "title": "[Bug] HuggingFace and SGLang inference don't match",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [ ] 2. The bug has not been fixed in the latest version.\r\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [ ] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nThe accuracy of the model is degraded due to inconsistent outputs from SGLang. While HF and vLLM produce consistent results such as \"A\" or \"B,\" SGLang occasionally outputs responses like \"I can't process that request.\" or \"A.\" / \"B.\" This inconsistency impacts overall accuracy.\r\n\r\n### Reproduction\r\n\r\nWhat command or script did you run?\r\nA script for generating outputs using a LLaMA 3.1 8B Istruct model with LoRA.\r\n\r\nWhich model are you using?\r\nLLaMA 3.1 with LoRA applied.\r\n\r\nSteps to reproduce:\r\n\r\n1. Run the script with HF, vLLM, and SGLang configurations.\r\n2. Compare the single-token outputs between the frameworks.\r\n3. Observe the inconsistent behavior in SGLang.\r\n\r\n------------------------------------------------------------------------------------------------------------------------\r\n**Hugging Face Code Snippet :** \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\nmodel = LlamaForCausalLM.from_pretrained(\r\n    model_name,\r\n    load_in_8bit=False,\r\n    torch_dtype=torch.float16,\r\n    device_map='auto',\r\n)\r\nadaptor_path = './model_spec/checkpoints/checkpoint-200-vllm'\r\nmodel = PeftModel.from_pretrained(\r\n    model,\r\n    adaptor_path,\r\n    torch_dtype=torch.float16,\r\n)\r\n\r\nmodel.config.pad_token_id = tokenizer.pad_token_id = 0\r\nmodel.config.bos_token_id = 1\r\nmodel.config.eos_token_id = 2\r\nmodel.generation_config.pad_token_id = tokenizer.pad_token_id\r\nmodel.eval()\r\n\r\n\r\ndef evaluate(\r\n    instruction,\r\n    input=None,\r\n    temperature=0,\r\n    top_p=1,\r\n    top_k=-1,\r\n    num_beams=4,\r\n    max_new_tokens=128,\r\n    stream_output=False,\r\n    **kwargs,\r\n):\r\n    prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\r\n\r\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\r\n    input_ids = inputs[\"input_ids\"].to('cuda')\r\n    generation_config = GenerationConfig(\r\n        temperature=temperature,\r\n        top_p=top_p,\r\n        top_k=top_k,\r\n        num_beams=num_beams,\r\n        **kwargs,\r\n    )\r\n\r\n    with torch.no_grad():\r\n        generation_output = model.generate(\r\n            input_ids=input_ids,\r\n            generation_config=generation_config,\r\n            return_dict_in_generate=True,\r\n            output_scores=True,\r\n            max_new_tokens=max_new_tokens\r\n        )\r\n    s = generation_output.sequences[0]\r\n    output = tokenizer.decode(s, skip_special_tokens=True)\r\n    result = output.split('assistant')[1].strip()\r\n    return result\r\n\r\n------------------------------------------------------------------------------------------------------------------------\r\n**SGLang Code Snippet :** \r\n\r\nimport sglang as sgl\r\nfrom sglang import *\r\nimport logging\r\nimport json\r\nimport torch\r\n\r\nfrom typing import Union, List\r\nfrom vllm import LLM, SamplingParams\r\nfrom vllm.inputs import TokensPrompt\r\nfrom vllm.lora.request import LoRARequest\r\nfrom collections import defaultdict\r\n\r\nfrom utils.usecase_prompts import UseCasePrompter\r\nfrom utils.lora_adapters import UseCaseLoraAdapters\r\nfrom utils.prompter import Prompter\r\n\r\nlogging.basicConfig(format='%(asctime)s %(message)s')\r\nlogger = logging.getLogger()\r\nlogger.setLevel(logging.INFO)\r\n\r\nclass SimpleSGLangLlama2:\r\n    def __init__(self, base_model_path, number_of_gpu=1, gpu_memory_utilization=0.4):\r\n        self.base_model_path = base_model_path\r\n        self._model = sgl.Engine(model_path=self.base_model_path)\r\n\r\n    def generate(\r\n        self,\r\n        prompt: Union[str, List[int]],\r\n        temperature: float = 0.0,\r\n        top_p: float = 1.0,\r\n        top_k: int = -1,\r\n        use_beam_search: bool = True,\r\n        max_new_tokens: int = 128,\r\n        best_of: int = 4\r\n    ) -> List[str]:\r\n        sampling_params = {\"temperature\": temperature, \"top_p\": top_p, \"top_k\":top_k, \"max_new_tokens\": max_new_tokens}\r\n        # Build final_prompt_text, then:\r\n        outputs = self._model.generate(\r\n            [final_prompt_text],\r\n            sampling_params,\r\n            lora_path=adaptor_path\r\n        )\r\n        results = [output['text'] for output in outputs]\r\n        return results\r\n\r\nif __name__ == \"__main__\":\r\n    model_path = \"./models/meta-llama/Meta-Llama-3.1-8B-Instruct\"\r\n    llm = SimpleSGLangLlama2(model_path)\r\n\r\n\r\n------------------------------------------------------------------------------------------------------------------------\r\n\r\n\r\n### Environment\r\n\r\nRun the class in notebooks environment.\r\n\r\nSGLang 0.4.0 (with flashinfer 0.1.6+cu121torch2.4)",
    "labels": [
      "bug",
      "inactive",
      "lora"
    ],
    "state": "closed",
    "created_at": "2024-12-30T22:54:09+00:00",
    "closed_at": "2025-05-03T00:18:08+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2671/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2671"
  },
  {
    "number": 5409,
    "title": "[Bug] Auto-truncation still uses full context length instead of (context_length - max_tokens)",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI'm experiencing an issue where prompt auto-truncation doesn't properly account for max_tokens when using the HTTP server, even with allow_auto_truncate=True enabled. This persists after the changes in https://github.com/sgl-project/sglang/pull/4919.\n\n### Reproduction\n\n1.  python -m sglang.launch_server --model-path NousResearch/Hermes-3-Llama-3.2-3B --host 0.0.0.0  --max-total-tokens 7192 --disable-overlap --allow-auto-truncate\n2. Send a request with a prompt exceeding 7192 tokens, and specify max_tokens=100\n3. Observe that truncation occurs at 7192 tokens total (prompt + response) rather than reserving space for max_tokens\n\nExpected Behavior:\u200b\u200b\nTruncation should preserve space for response tokens by truncating the prompt to (context_length - max_tokens) tokens, as implemented in other frameworks like vLLM.\n\n\u200bSuggested Fix:\u200b\u200b\nAdd a truncate_prompt_tokens parameter to the HTTP API request schema to explicitly control this behavior, mirroring [vLLM's implementation](https://github.com/vllm-project/vllm/blob/b590adfdc15fc716f6d120aeefeb587f491f8fce/vllm/entrypoints/openai/protocol.py#L262C5-L262C27\u3002)\n\n\n### Environment\n\nPython: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]\nCUDA available: True\nGPU 0: NVIDIA RTX 6000 Ada Generation\nGPU 0 Compute Capability: 8.9\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\nCUDA Driver Version: 550.54.14\nPyTorch: 2.5.1+cu124\nsglang: 0.4.5\nsgl_kernel: 0.0.8\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.51.0\ntorchao: 0.9.0+cu124\nnumpy: 2.1.2\naiohttp: 3.11.16\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.24.1\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 24.1\npsutil: 7.0.0\npydantic: 2.11.3\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.17\nopenai: 1.72.0\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.65.4.post1\ndecord: 0.6.0\nNVIDIA Topology:\nGPU0 NIC0 CPU Affinity NUMA Affinity GPU NUMA ID\nGPU0 X SYS 44-65 1 N/A\nNIC0 SYS X\n\nLegend:\n\nX = Self\nSYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\nNODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\nPHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\nPXB = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\nPIX = Connection traversing at most a single PCIe bridge\nNV# = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\nNIC0: mlx5_bond_0\n\nulimit soft: 1048576\n\n",
    "labels": [
      "good first issue",
      "inactive"
    ],
    "state": "open",
    "created_at": "2025-04-15T07:33:29+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5409/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5409"
  },
  {
    "number": 2659,
    "title": "[Feature] Clear PAT_TOKEN in CI",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n![image](https://github.com/user-attachments/assets/d62f4957-2802-4068-9c16-fbcaee2584f4)\r\n\r\n@shuaills Would you like to take this? Pretty easy.\n\n### Related resources\n\n_No response_",
    "labels": [
      "documentation",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-12-30T07:44:56+00:00",
    "closed_at": "2025-03-01T00:18:50+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2659/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2659"
  },
  {
    "number": 4221,
    "title": "[Feature] SGLang Support for TileLang",
    "body": "We recently came across an interesting project: [TileLang](https://github.com/tile-ai/tilelang). It appears to offer significant advantages over Triton in many cases while maintaining a clean dataflow and simple syntax.\n\nDo we have any plans to support a TileLang backend in SGLang?\n\nFor instance, TileLang has demonstrated up to **5x speedup** over Triton\u2019s Flash MLA implementations on H100, with a kernel implementation of just **80 lines of code (see document:** https://github.com/tile-ai/tilelang/tree/main/examples/deepseek_mla). Given these promising results, it would be valuable to explore its potential integration.\n\nWould love to hear thoughts on this!\n",
    "labels": [
      "help wanted",
      "high priority",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-09T05:34:49+00:00",
    "closed_at": "2025-05-27T00:18:53+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4221/reactions",
      "total_count": 9,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 5,
      "eyes": 4
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4221"
  },
  {
    "number": 1036,
    "title": "[Feature] Allow arbitrary logit processors",
    "body": "### Motivation\r\n\r\nThere's some great projects out there that modify logits, mostly for guided decoding or novel sampling techniques. Supporting every single one of them will cause too much bloat and distraction, but if SGLang were to allow arbitrary logit processors then the community can plug and play their own processors.\r\n\r\nFor example, I would have interest in using [https://github.com/noamgat/lm-format-enforcer](lm format enforcer) because it allows for optional JSON fields and recursive classes (unlike outlines). The API of lm format enforcer is also clean and simple and it is simple to make custom parsers for other formats than JSON (e.g. SQL).\r\n\r\nOne way I would imagine the API to work is:\r\n\r\n```python\r\ndef my_logits_processor(inputs: list[int], logits: torch.Tensor) -> torch.Tensor:\r\n   ...\r\n\r\n\r\n@sgl.function\r\ndef character_gen(s, name):\r\n    s += name + \" is a character in Harry Potter. Please fill in the following information about this character.\\n\"\r\n    s += sgl.gen(\"output\", logits_processor: my_logits_processor)\r\n```\r\n\r\nI'm not familiar with the internals of SGLang at all, so I am just throwing out the idea of supporting an async logits processor. Often we only care about logits masks that can already be calculated without knowing the scores yet. This would be more efficient as the CPU can calculate the masks while the GPU runs the model. Right now, the lack of such implementation makes logit processors a performance bottleneck in vLLM. \r\n\r\nAn async logit processor could simply work like this:\r\n```python\r\nasync def my_logits_processor(inputs: list[int]) -> AsyncGenerator[torch.Tensor, torch.Tensor]:\r\n   # All the preprocessing steps here to calculate the mask in parallel\r\n\r\n   logits: torch.tensor = yield\r\n\r\n   # Apply the mask to the logits here to calculate the new logits\r\n   yield new_logits  \r\n```\r\nOf course, the async approach would only work if the model's calculations and the logits processor do not run from the same python process. I'm not sure if this would be the case in SGLang's server implementation.\r\n\r\nThe added benefit of integrating it in SGLang over other inference systems is the ability to easily enable logits processors for only certain sections of the generated output. \r\n\r\n### Related resources\r\n\r\n_No response_",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-08-11T19:34:38+00:00",
    "closed_at": "2024-10-21T01:13:28+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1036/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1036"
  },
  {
    "number": 299,
    "title": "aglang",
    "body": "I test yi-vl-6B with `srt_example_yi_vl.py`\r\nget error:\r\n```\r\nAttributeError: 'TokenizerManager' object has no attribute 'executor\r\n```",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-03-14T09:10:13+00:00",
    "closed_at": "2024-07-25T06:32:43+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/299/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/299"
  },
  {
    "number": 1792,
    "title": "[Bug] Got error with awq_marlin quantization args.",
    "body": "### Checklist\r\n\r\n- [x]  I have searched related issues but cannot get the expected help.\r\n\r\n- [x]  The bug has not been fixed in the latest version.\r\n\r\n- [x]  Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n\r\n- [x]  If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n\r\n \r\n\r\n- [x] Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nI used the AutoAWQ tool to quantize [Deepseek-V2](https://huggingface.co/deepseek-ai/DeepSeek-V2) model . The quantization script is as follows, resulting in a quantized network. I expect to obtain a model in awq_marlin quantization format.\r\n```\r\nfrom awq import AutoAWQForCausalLM\r\nfrom transformers import AutoTokenizer\r\n\r\n\r\nmodel_path = 'path/to/Deepseek-V2'\r\nquant_path = 'path/to/Deepseek-V2_marlin'\r\nquant_config = { \"zero_point\": False, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"Marlin\" }\r\n\r\n# Load model\r\nmodel = AutoAWQForCausalLM.from_pretrained(\r\n    model_path, **{\"low_cpu_mem_usage\": True, \"use_cache\": False}\r\n)\r\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\n\r\n# Quantize\r\nmodel.quantize(tokenizer, quant_config=quant_config)\r\n\r\n# Save quantized model\r\nmodel.save_quantized(quant_path)\r\ntokenizer.save_pretrained(quant_path)\r\n\r\nprint(f'Model is quantized and saved at \"{quant_path}\"')\r\n\r\n```\r\nThe config.json corresponding to the quantized model is as follows.\r\n```\r\n{\r\n  \"_name_or_path\": \"/path/to/Deepseek-V2\",\r\n  \"architectures\": [\r\n    \"DeepseekV2ForCausalLM\"\r\n  ],\r\n  \"attention_bias\": false,\r\n  \"attention_dropout\": 0.0,\r\n  \"auto_map\": {\r\n    \"AutoConfig\": \"configuration_deepseek.DeepseekV2Config\",\r\n    \"AutoModel\": \"modeling_deepseek.DeepseekV2Model\",\r\n    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV2ForCausalLM\"\r\n  },\r\n  \"aux_loss_alpha\": 0.001,\r\n  \"bos_token_id\": 100000,\r\n  \"eos_token_id\": 100001,\r\n  \"ep_size\": 1,\r\n  \"first_k_dense_replace\": 1,\r\n  \"hidden_act\": \"silu\",\r\n  \"hidden_size\": 5120,\r\n  \"initializer_range\": 0.02,\r\n  \"intermediate_size\": 12288,\r\n  \"kv_lora_rank\": 512,\r\n  \"max_position_embeddings\": 163840,\r\n  \"model_type\": \"deepseek_v2\",\r\n  \"moe_intermediate_size\": 1536,\r\n  \"moe_layer_freq\": 1,\r\n  \"n_group\": 8,\r\n  \"n_routed_experts\": 80,\r\n  \"n_shared_experts\": 2,\r\n  \"norm_topk_prob\": false,\r\n  \"num_attention_heads\": 128,\r\n  \"num_experts_per_tok\": 6,\r\n  \"num_hidden_layers\": 60,\r\n  \"num_key_value_heads\": 128,\r\n  \"pretraining_tp\": 1,\r\n  \"q_lora_rank\": 1536,\r\n  \"qk_nope_head_dim\": 128,\r\n  \"qk_rope_head_dim\": 64,\r\n  \"quantization_config\": {\r\n    \"bits\": 4,\r\n    \"group_size\": 128,\r\n    \"modules_to_not_convert\": null,\r\n    \"quant_method\": \"awq\",\r\n    \"version\": \"marlin\",\r\n    \"zero_point\": false\r\n  },\r\n  \"rms_norm_eps\": 1e-06,\r\n  \"rope_scaling\": {\r\n    \"beta_fast\": 32,\r\n    \"beta_slow\": 1,\r\n    \"factor\": 40,\r\n    \"mscale\": 0.707,\r\n    \"mscale_all_dim\": 0.707,\r\n    \"original_max_position_embeddings\": 4096,\r\n    \"type\": \"yarn\"\r\n  },\r\n  \"rope_theta\": 10000,\r\n  \"routed_scaling_factor\": 16.0,\r\n  \"scoring_func\": \"softmax\",\r\n  \"seq_aux\": true,\r\n  \"tie_word_embeddings\": false,\r\n  \"topk_group\": 3,\r\n  \"topk_method\": \"group_limited_greedy\",\r\n  \"torch_dtype\": \"float16\",\r\n  \"transformers_version\": \"4.45.2\",\r\n  \"use_cache\": false,\r\n  \"v_head_dim\": 128,\r\n  \"vocab_size\": 102400\r\n}\r\n```\r\nThen, I used SGLang to run quantized model with the following command. \r\n```\r\npython -m sglang.launch_server --trust-remote-code --model-path $MODEL_PATH --port $SERVER_PORT --quantization awq_marlin --tp 4 --mem-fraction-static 0.9\r\n```\r\nAnd got the error\r\n```\r\n[2024-10-25 18:12:30 TP0] Traceback (most recent call last):\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 1115, in run_scheduler_process\r\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank)\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 146, in __init__\r\n    self.tp_worker = TpModelWorker(\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 58, in __init__\r\n    self.model_runner = ModelRunner(\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 147, in __init__\r\n    self.load_model()\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 234, in load_model\r\n    self.vllm_model_config = VllmModelConfig(\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/vllm/config.py\", line 227, in __init__\r\n    self._verify_quantization()\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/vllm/config.py\", line 296, in _verify_quantization\r\n    raise ValueError(\r\nValueError: Quantization method specified in the model config (awq) does not match the quantization method specified in the `quantization` argument (awq_marlin).\r\n\r\n```\r\nIf I change quantization_config to `--quantization awq` ,  also got error.\r\n```\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 1115, in run_scheduler_process\r\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank)\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 146, in __init__\r\n    self.tp_worker = TpModelWorker(\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 58, in __init__\r\n    self.model_runner = ModelRunner(\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 147, in __init__\r\n    self.load_model()\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 251, in load_model\r\n    self.model = get_model(\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 19, in get_model\r\n    return loader.load_model(model_config=model_config,\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 341, in load_model\r\n    model = _initialize_model(model_config, self.load_config,\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 170, in _initialize_model\r\n    return build_model(\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 155, in build_model\r\n    return model_class(config=hf_config,\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/models/deepseek_v2.py\", line 648, in __init__\r\n    self.model = DeepseekV2Model(config, cache_config, quant_config)\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/models/deepseek_v2.py\", line 608, in __init__\r\n    [\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/models/deepseek_v2.py\", line 609, in <listcomp>\r\n    DeepseekV2DecoderLayer(\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/models/deepseek_v2.py\", line 551, in __init__\r\n    self.mlp = DeepseekV2MoE(config=config, quant_config=quant_config)\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/models/deepseek_v2.py\", line 113, in __init__\r\n    self.experts = FusedMoE(\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 192, in __init__\r\n    assert self.quant_method is not None\r\nAssertionError\r\n```\r\n So how to run an awq_marlin or marlin quantized model with SGLang?\r\n\r\n### Reproduction\r\n\r\n1.  Quant the Deepseek-V2 model; In fact you can use small model to reproduce;\r\n\r\n2. Run quantization model with SGLang.\r\n\r\n\r\n### Environment\r\n\r\n```\r\npython -m sglang.check_env\r\nPython: 3.10.15 | packaged by conda-forge | (main, Oct 16 2024, 01:24:24) [GCC 13.3.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA H800\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\r\nCUDA Driver Version: 550.90.07\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.3.4\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.45.2\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.26.4\r\naiohttp: 3.10.10\r\nfastapi: 0.115.2\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.26.0\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 11.0.0\r\npsutil: 6.1.0\r\npydantic: 2.9.2\r\nuvicorn: 0.32.0\r\nuvloop: 0.21.0\r\nzmq: 26.2.0\r\nvllm: 0.5.5\r\nmultipart: 0.0.12\r\nopenai: 1.52.0\r\ntiktoken: 0.8.0\r\nanthropic: 0.36.2\r\n\r\nHypervisor vendor: KVM\r\nulimit soft: 1048576\r\n\r\n```",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-10-25T10:21:16+00:00",
    "closed_at": "2024-12-26T00:16:32+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1792/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1792"
  },
  {
    "number": 7332,
    "title": "[RFC] Bi-weekly release",
    "body": "After thorough internal discussions, the SGLang team has decided to standardize the release cycle as follows:\n\n- A new version will be released every two weeks under normal circumstances (e.g., v0.4.8, v0.4.9).\n\n- If urgent issues or high-priority features arise between regular releases, we may publish a patch release or an additional stable version as needed.\n\n- Bi-weekly releases will typically occur around the middle and end of each month.\n\n- Each release will aim to include a set of planned features, usually discussed and finalized by the SGLang team in advance.\n\n",
    "labels": [
      "high priority",
      "collaboration"
    ],
    "state": "open",
    "created_at": "2025-06-18T23:17:05+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7332/reactions",
      "total_count": 16,
      "+1": 11,
      "-1": 0,
      "laugh": 0,
      "hooray": 5,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/7332"
  },
  {
    "number": 5962,
    "title": "[Bug] sglang 0.4.4.post2 Latency greatly increases when tp=1 and dp > 1",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nNoticed that after bumping sglang to 0.4.4.post2+, when setting dp > 1, the latency would increase 10+ times, the more dp we set, the more latency would increase. The issue is not found in sglang version <= 0.4.4.post1.\n\nData size: 4k per prompt\nEndpoint: v1/completions\n\nWith max_token=1, latency for tp1 dp1 is ~40ms, but latency for tp1 dp8 is 1000ms+\n\n### Reproduction\n\npython -m sglang.launch_server --model-path /Llama-3.2-3B-Instruct --port 30000 --host 0.0.0.0 --tp-size 1 --dp-size 8\n\n### Environment\n\nPython: 3.10.14 (main, Jul 14 2024, 22:24:12) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.6, V12.6.77\nCUDA Driver Version: 550.54.15\nPyTorch: 2.5.1+cu124\nsglang: 0.4.4.post2\nsgl_kernel: 0.0.5.post3\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.50.0\ntorchao: 0.10.0\nnumpy: 1.26.4\naiohttp: 3.11.14\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.11.1\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nxgrammar: 0.1.16\nopenai: 1.76.2\ntiktoken: 0.9.0\nanthropic: Module Not Found\nlitellm: Module Not Found\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     SYS     SYS     SYS     SYS     SYS     0-63,128-191    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     PHB     PHB     SYS     SYS     SYS     0-63,128-191    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     PIX     SYS     SYS     0-63,128-191    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     0-63,128-191    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     SYS     64-127,192-255  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     64-127,192-255  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     PIX     64-127,192-255  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     64-127,192-255  1               N/A\nNIC0    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS\nNIC1    SYS     PHB     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     SYS     SYS     SYS\nNIC2    SYS     PHB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      SYS     SYS     SYS\nNIC3    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS\nNIC5    SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n\n\nulimit soft: 10000000",
    "labels": [
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-05-02T00:30:20+00:00",
    "closed_at": "2025-05-11T01:58:01+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5962/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5962"
  },
  {
    "number": 3142,
    "title": "[Feature] Accuracy test of VLM",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nIn sglang, LLMs have accuracy tests with Hugging Face models:\n\nhttps://github.com/sgl-project/sglang/blob/main/test/srt/models/test_generation_models.py\n\nhttps://github.com/sgl-project/sglang/blob/main/test/srt/test_nightly_math_eval.py\n\nWe need similar one for VLM also.\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "help wanted",
      "high priority",
      "MLLM"
    ],
    "state": "open",
    "created_at": "2025-01-26T06:25:40+00:00",
    "closed_at": null,
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3142/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3142"
  },
  {
    "number": 6753,
    "title": "[Bug] PD Failed to register memory on H200",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n```\nroot@nccl-test-host-1:/diagnostic# python3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-Instruct --disaggregation-mode prefill --disaggregation-ib-device mlx5_0\nCuda graph is disabled for prefill server\n[2025-05-29 23:22:47] server_args=ServerArgs(model_path='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer_path='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='meta-llama/Meta-Llama-3-8B-Instruct', chat_template=None, completion_template=None, is_embedding=False, enable_multimodal=None, revision=None, host='127.0.0.1', port=30000, mem_fraction_static=0.8717961202189594, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=783536351, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, bucket_time_to_first_token=None, bucket_e2e_request_latency=None, bucket_inter_token_latency=None, collect_tokens_histogram=False, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_nccl_nvls=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=None, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, mm_attention_backend=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='prefill', disaggregation_bootstrap_port=8998, disaggregation_transfer_backend='mooncake', disaggregation_ib_device='mlx5_0', pdlb_url=None)\n[2025-05-29 23:22:53] Attention backend not set. Use fa3 backend by default.\n[2025-05-29 23:22:53] Init torch distributed begin.\n[2025-05-29 23:22:53] Init torch distributed ends. mem usage=0.00 GB\n[2025-05-29 23:22:53] init_expert_location from trivial\n[2025-05-29 23:22:54] Load weight begin. avail mem=139.20 GB\n[2025-05-29 23:22:55] Using model weights format ['*.safetensors']\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.29it/s]\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.22it/s]\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.77it/s]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.53it/s]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.49it/s]\n\n[2025-05-29 23:22:58] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=124.23 GB, mem usage=14.98 GB.\n[2025-05-29 23:22:58] KV Cache is allocated. #tokens: 871371, K size: 53.18 GB, V size: 53.18 GB\n[2025-05-29 23:22:58] Memory pool end. avail mem=17.66 GB\n[2025-05-29 23:22:58] max_total_num_tokens=871371, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=4097, context_len=8192\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nI0529 23:22:59.041800 67580 transfer_engine.cpp:350] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\nI0529 23:22:59.041816 67580 transfer_engine.cpp:44] Transfer Engine starting. Server: 10.72.0.9, Metadata: P2PHANDSHAKE, ip_or_host_name: , rpc_port: 0\nI0529 23:22:59.041846 67580 transfer_engine.cpp:100] Transfer Engine RPC using P2P handshake, listening on 10.72.0.9:15360\nI0529 23:22:59.041899 67580 transfer_engine.cpp:112] Auto-discovering topology...\nI0529 23:22:59.042371 67580 transfer_engine.cpp:127] Topology discovery complete. Found 1 HCAs.\nI0529 23:22:59.047586 67580 rdma_context.cpp:125] RDMA device: mlx5_0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:c0:a8:01:03\nE0529 23:22:59.553362 67580 rdma_context.cpp:203] Failed to register memory 0x7c1cda000000: Bad address [14]\nE0529 23:22:59.553401 67580 rdma_context.cpp:203] Failed to register memory 0x7c1c6e000000: Bad address [14]\nE0529 23:22:59.553411 67580 rdma_context.cpp:203] Failed to register memory 0x7c1c02000000: Bad address [14]\nE0529 23:22:59.553417 67580 rdma_context.cpp:203] Failed to register memory 0x7c1b96000000: Bad address [14]\nE0529 23:22:59.553424 67580 rdma_context.cpp:203] Failed to register memory 0x7c1b2a000000: Bad address [14]\nE0529 23:22:59.553435 67580 rdma_context.cpp:203] Failed to register memory 0x7c1abe000000: Bad address [14]\nE0529 23:22:59.553443 67580 rdma_context.cpp:203] Failed to register memory 0x7c1a52000000: Bad address [14]\nE0529 23:22:59.553449 67580 rdma_context.cpp:203] Failed to register memory 0x7c19e6000000: Bad address [14]\nE0529 23:22:59.553457 67580 rdma_context.cpp:203] Failed to register memory 0x7c197a000000: Bad address [14]\nE0529 23:22:59.553462 67580 rdma_context.cpp:203] Failed to register memory 0x7c190e000000: Bad address [14]\nE0529 23:22:59.553467 67580 rdma_context.cpp:203] Failed to register memory 0x7c18a2000000: Bad address [14]\nE0529 23:22:59.553473 67580 rdma_context.cpp:203] Failed to register memory 0x7c1836000000: Bad address [14]\nE0529 23:22:59.553479 67580 rdma_context.cpp:203] Failed to register memory 0x7c17ca000000: Bad address [14]\nE0529 23:22:59.553486 67580 rdma_context.cpp:203] Failed to register memory 0x7c175e000000: Bad address [14]\nE0529 23:22:59.553491 67580 rdma_context.cpp:203] Failed to register memory 0x7c16f2000000: Bad address [14]\nE0529 23:22:59.553498 67580 rdma_context.cpp:203] Failed to register memory 0x7c1686000000: Bad address [14]\nE0529 23:22:59.553504 67580 rdma_context.cpp:203] Failed to register memory 0x7c161a000000: Bad address [14]\nE0529 23:22:59.553510 67580 rdma_context.cpp:203] Failed to register memory 0x7c15ae000000: Bad address [14]\nE0529 23:22:59.553517 67580 rdma_context.cpp:203] Failed to register memory 0x7c1542000000: Bad address [14]\nE0529 23:22:59.553526 67580 rdma_context.cpp:203] Failed to register memory 0x7c14d6000000: Bad address [14]\nE0529 23:22:59.553532 67580 rdma_context.cpp:203] Failed to register memory 0x7c146a000000: Bad address [14]\nE0529 23:22:59.553539 67580 rdma_context.cpp:203] Failed to register memory 0x7c13fe000000: Bad address [14]\nE0529 23:22:59.553544 67580 rdma_context.cpp:203] Failed to register memory 0x7c1392000000: Bad address [14]\nE0529 23:22:59.553550 67580 rdma_context.cpp:203] Failed to register memory 0x7c1326000000: Bad address [14]\nE0529 23:22:59.553556 67580 rdma_context.cpp:203] Failed to register memory 0x7c12ba000000: Bad address [14]\nE0529 23:22:59.553563 67580 rdma_context.cpp:203] Failed to register memory 0x7c124e000000: Bad address [14]\nE0529 23:22:59.553570 67580 rdma_context.cpp:203] Failed to register memory 0x7c11e2000000: Bad address [14]\nE0529 23:22:59.553576 67580 rdma_context.cpp:203] Failed to register memory 0x7c1176000000: Bad address [14]\nE0529 23:22:59.553586 67580 rdma_context.cpp:203] Failed to register memory 0x7c110a000000: Bad address [14]\nE0529 23:22:59.553599 67580 rdma_context.cpp:203] Failed to register memory 0x7c109e000000: Bad address [14]\nE0529 23:22:59.553608 67580 rdma_context.cpp:203] Failed to register memory 0x7c1032000000: Bad address [14]\nE0529 23:22:59.553618 67580 rdma_context.cpp:203] Failed to register memory 0x7c0fc6000000: Bad address [14]\nE0529 23:22:59.553624 67580 rdma_context.cpp:203] Failed to register memory 0x7c0f5a000000: Bad address [14]\nE0529 23:22:59.553632 67580 rdma_context.cpp:203] Failed to register memory 0x7c0eee000000: Bad address [14]\nE0529 23:22:59.553639 67580 rdma_context.cpp:203] Failed to register memory 0x7c0e82000000: Bad address [14]\nE0529 23:22:59.553644 67580 rdma_context.cpp:203] Failed to register memory 0x7c0e16000000: Bad address [14]\nE0529 23:22:59.553650 67580 rdma_context.cpp:203] Failed to register memory 0x7c0daa000000: Bad address [14]\nE0529 23:22:59.553658 67580 rdma_context.cpp:203] Failed to register memory 0x7c0d3e000000: Bad address [14]\nE0529 23:22:59.553664 67580 rdma_context.cpp:203] Failed to register memory 0x7c0cd2000000: Bad address [14]\nE0529 23:22:59.553671 67580 rdma_context.cpp:203] Failed to register memory 0x7c0c66000000: Bad address [14]\nE0529 23:22:59.553678 67580 rdma_context.cpp:203] Failed to register memory 0x7c0bfa000000: Bad address [14]\nE0529 23:22:59.553683 67580 rdma_context.cpp:203] Failed to register memory 0x7c0b8e000000: Bad address [14]\nE0529 23:22:59.553690 67580 rdma_context.cpp:203] Failed to register memory 0x7c0b22000000: Bad address [14]\nE0529 23:22:59.553695 67580 rdma_context.cpp:203] Failed to register memory 0x7c0ab6000000: Bad address [14]\nE0529 23:22:59.553701 67580 rdma_context.cpp:203] Failed to register memory 0x7c0a4a000000: Bad address [14]\nE0529 23:22:59.553707 67580 rdma_context.cpp:203] Failed to register memory 0x7c09de000000: Bad address [14]\nE0529 23:22:59.553714 67580 rdma_context.cpp:203] Failed to register memory 0x7c0972000000: Bad address [14]\nE0529 23:22:59.553719 67580 rdma_context.cpp:203] Failed to register memory 0x7c0906000000: Bad address [14]\nE0529 23:22:59.553725 67580 rdma_context.cpp:203] Failed to register memory 0x7c089a000000: Bad address [14]\nE0529 23:22:59.553730 67580 rdma_context.cpp:203] Failed to register memory 0x7c082e000000: Bad address [14]\nE0529 23:22:59.553736 67580 rdma_context.cpp:203] Failed to register memory 0x7c07c2000000: Bad address [14]\nE0529 23:22:59.553745 67580 rdma_context.cpp:203] Failed to register memory 0x7c0756000000: Bad address [14]\nE0529 23:22:59.553750 67580 rdma_context.cpp:203] Failed to register memory 0x7c06ea000000: Bad address [14]\nE0529 23:22:59.553756 67580 rdma_context.cpp:203] Failed to register memory 0x7c067e000000: Bad address [14]\nE0529 23:22:59.553763 67580 rdma_context.cpp:203] Failed to register memory 0x7c0612000000: Bad address [14]\nE0529 23:22:59.553771 67580 rdma_context.cpp:203] Failed to register memory 0x7c05a6000000: Bad address [14]\nE0529 23:22:59.553776 67580 rdma_context.cpp:203] Failed to register memory 0x7c053a000000: Bad address [14]\nE0529 23:22:59.553782 67580 rdma_context.cpp:203] Failed to register memory 0x7c04ce000000: Bad address [14]\nE0529 23:22:59.553788 67580 rdma_context.cpp:203] Failed to register memory 0x7c0462000000: Bad address [14]\nE0529 23:22:59.553794 67580 rdma_context.cpp:203] Failed to register memory 0x7c03f6000000: Bad address [14]\nE0529 23:22:59.553800 67580 rdma_context.cpp:203] Failed to register memory 0x7c038a000000: Bad address [14]\nE0529 23:22:59.553807 67580 rdma_context.cpp:203] Failed to register memory 0x7c031e000000: Bad address [14]\nE0529 23:22:59.553810 67580 rdma_context.cpp:203] Failed to register memory 0x7c02b2000000: Bad address [14]\nE0529 23:22:59.553815 67580 rdma_context.cpp:203] Failed to register memory 0x7c0246000000: Bad address [14]\n[2025-05-29 23:22:59] INFO:     Started server process [67238]\n[2025-05-29 23:22:59] INFO:     Waiting for application startup.\n[2025-05-29 23:22:59] INFO:     Application startup complete.\n[2025-05-29 23:22:59] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)\n[2025-05-29 23:23:00] INFO:     127.0.0.1:55706 - \"GET /get_model_info HTTP/1.1\" 200 OK\n[2025-05-29 23:23:00] Start of prefill warmup ...\n[2025-05-29 23:23:00] FakeKVSender init with kv_indices: 4, aux_index: 0\n[2025-05-29 23:23:00] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 0, token usage: 0.00, #running-req: 0, #unbootstrapped-req: 0, #queue-req: 0, #transferring-req: 0\n[2025-05-29 23:23:01] FakeKVSender send with kv_indices: [1 2 3 4]\n[2025-05-29 23:23:01] FakeKVSender poll success\n[2025-05-29 23:23:01] INFO:     127.0.0.1:55712 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-05-29 23:23:01] End of prefill warmup with status 200, resp: [{'text': '%', 'meta_info': {'id': '494830e6c2fe459a89b96d31703b3cc1', 'finish_reason': {'type': 'length', 'length': 0}, 'prompt_tokens': 4, 'completion_tokens': 1, 'cached_tokens': 0, 'e2e_latency': 0.9215409755706787}}]\n[2025-05-29 23:23:01] The server is fired up and ready to roll!\n[2025-05-29 23:23:17] INFO:     127.0.0.1:49146 - \"GET /v1/models HTTP/1.1\" 200 OK\n[2025-05-29 23:23:23] INFO:     127.0.0.1:55356 - \"POST /v1/completions HTTP/1.1\" 200 OK\n[2025-05-29 23:23:23] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 0, token usage: 0.00, #running-req: 0, #unbootstrapped-req: 0, #queue-req: 0, #transferring-req: 0\nE0529 23:23:23.432950 69588 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c1cda002800\nE0529 23:23:23.432989 69589 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c1c6e002800\nE0529 23:23:23.433007 69590 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c1c02002800\nE0529 23:23:23.433671 69590 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c1b96002800\nE0529 23:23:23.433724 69589 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c1b2a002800\nE0529 23:23:23.433776 69588 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c1abe002800\nE0529 23:23:23.433799 69588 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c1a52002800\nE0529 23:23:23.433817 69588 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c19e6002800\nE0529 23:23:23.433835 69588 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c197a002800\nE0529 23:23:23.433849 69588 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c190e002800\nE0529 23:23:23.433863 69588 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c18a2002800\nE0529 23:23:23.433880 69588 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c1836002800\nE0529 23:23:23.433909 69589 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c17ca002800\nE0529 23:23:23.433997 69590 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c175e002800\nE0529 23:23:23.434022 69588 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c16f2002800\nE0529 23:23:23.434060 69589 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c1686002800\n[2025-05-29 23:23:23] Session 10.72.0.9:16756 failed.\n[2025-05-29 23:23:23] Prefill transfer failed for request rank=0 req.rid='a5eda8e756dd4d19a913a469ce943fbc' req.bootstrap_room=3583236771377794168 with exception KVTransferError(bootstrap_room=3583236771377794168): Failed to send kv chunk of 3583236771377794168 to 10.72.0.9:44781\n```\n\n```\nroot@nccl-test-host-1:/diagnostic# python3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-Instruct --disaggregation-mode decode --port 30001 --base-gpu-id 1 --disaggregation-ib-device mlx5_0\nKV cache is forced as chunk cache for decode server\n[2025-05-29 23:22:47] server_args=ServerArgs(model_path='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer_path='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='meta-llama/Meta-Llama-3-8B-Instruct', chat_template=None, completion_template=None, is_embedding=False, enable_multimodal=None, revision=None, host='127.0.0.1', port=30001, mem_fraction_static=0.8717961202189594, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=787569464, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=1, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, bucket_time_to_first_token=None, bucket_e2e_request_latency=None, bucket_inter_token_latency=None, collect_tokens_histogram=False, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=True, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=None, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, mm_attention_backend=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='decode', disaggregation_bootstrap_port=8998, disaggregation_transfer_backend='mooncake', disaggregation_ib_device='mlx5_0', pdlb_url=None)\n[2025-05-29 23:22:53] Attention backend not set. Use fa3 backend by default.\n[2025-05-29 23:22:53] Init torch distributed begin.\n[2025-05-29 23:22:53] Init torch distributed ends. mem usage=0.00 GB\n[2025-05-29 23:22:53] init_expert_location from trivial\n[2025-05-29 23:22:54] Load weight begin. avail mem=139.20 GB\n[2025-05-29 23:22:55] Using model weights format ['*.safetensors']\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.30it/s]\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.24it/s]\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.80it/s]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.55it/s]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.52it/s]\n\n[2025-05-29 23:22:58] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=124.23 GB, mem usage=14.98 GB.\n[2025-05-29 23:22:58] KV Cache is allocated. #tokens: 871371, K size: 53.18 GB, V size: 53.18 GB\n[2025-05-29 23:22:58] Memory pool end. avail mem=17.66 GB\n[2025-05-29 23:22:58] Capture cuda graph begin. This can take up to several minutes. avail mem=17.56 GB\n[2025-05-29 23:22:58] Capture cuda graph bs [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256]\nCapturing batches (avail_mem=15.05 GB): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:03<00:00,  8.79it/s]\n[2025-05-29 23:23:02] Capture cuda graph end. Time elapsed: 4.10 s. mem usage=2.51 GB. avail mem=15.05 GB.\n[2025-05-29 23:23:03] max_total_num_tokens=871371, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=4097, context_len=8192\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nI0529 23:23:03.621326 67574 transfer_engine.cpp:350] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\nI0529 23:23:03.621340 67574 transfer_engine.cpp:44] Transfer Engine starting. Server: 10.72.0.9, Metadata: P2PHANDSHAKE, ip_or_host_name: , rpc_port: 0\nI0529 23:23:03.621372 67574 transfer_engine.cpp:100] Transfer Engine RPC using P2P handshake, listening on 10.72.0.9:16756\nI0529 23:23:03.621439 67574 transfer_engine.cpp:112] Auto-discovering topology...\nI0529 23:23:03.621906 67574 transfer_engine.cpp:127] Topology discovery complete. Found 1 HCAs.\nI0529 23:23:03.633222 67574 rdma_context.cpp:125] RDMA device: mlx5_0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:c0:a8:01:03\nE0529 23:23:04.135735 67574 rdma_context.cpp:203] Failed to register memory 0x7b46f6000000: Bad address [14]\nE0529 23:23:04.135766 67574 rdma_context.cpp:203] Failed to register memory 0x7b468a000000: Bad address [14]\nE0529 23:23:04.135774 67574 rdma_context.cpp:203] Failed to register memory 0x7b461e000000: Bad address [14]\nE0529 23:23:04.135782 67574 rdma_context.cpp:203] Failed to register memory 0x7b45b2000000: Bad address [14]\nE0529 23:23:04.135789 67574 rdma_context.cpp:203] Failed to register memory 0x7b4546000000: Bad address [14]\nE0529 23:23:04.135797 67574 rdma_context.cpp:203] Failed to register memory 0x7b44da000000: Bad address [14]\nE0529 23:23:04.135804 67574 rdma_context.cpp:203] Failed to register memory 0x7b446e000000: Bad address [14]\nE0529 23:23:04.135811 67574 rdma_context.cpp:203] Failed to register memory 0x7b4402000000: Bad address [14]\nE0529 23:23:04.135818 67574 rdma_context.cpp:203] Failed to register memory 0x7b4396000000: Bad address [14]\nE0529 23:23:04.135824 67574 rdma_context.cpp:203] Failed to register memory 0x7b432a000000: Bad address [14]\nE0529 23:23:04.135831 67574 rdma_context.cpp:203] Failed to register memory 0x7b42be000000: Bad address [14]\nE0529 23:23:04.135838 67574 rdma_context.cpp:203] Failed to register memory 0x7b4252000000: Bad address [14]\nE0529 23:23:04.135846 67574 rdma_context.cpp:203] Failed to register memory 0x7b41e6000000: Bad address [14]\nE0529 23:23:04.135854 67574 rdma_context.cpp:203] Failed to register memory 0x7b417a000000: Bad address [14]\nE0529 23:23:04.135862 67574 rdma_context.cpp:203] Failed to register memory 0x7b410e000000: Bad address [14]\nE0529 23:23:04.135870 67574 rdma_context.cpp:203] Failed to register memory 0x7b40a2000000: Bad address [14]\nE0529 23:23:04.135879 67574 rdma_context.cpp:203] Failed to register memory 0x7b4036000000: Bad address [14]\nE0529 23:23:04.135885 67574 rdma_context.cpp:203] Failed to register memory 0x7b3fca000000: Bad address [14]\nE0529 23:23:04.135892 67574 rdma_context.cpp:203] Failed to register memory 0x7b3f5e000000: Bad address [14]\nE0529 23:23:04.135900 67574 rdma_context.cpp:203] Failed to register memory 0x7b3ef2000000: Bad address [14]\nE0529 23:23:04.135905 67574 rdma_context.cpp:203] Failed to register memory 0x7b3e86000000: Bad address [14]\nE0529 23:23:04.135912 67574 rdma_context.cpp:203] Failed to register memory 0x7b3e1a000000: Bad address [14]\nE0529 23:23:04.135918 67574 rdma_context.cpp:203] Failed to register memory 0x7b3dae000000: Bad address [14]\nE0529 23:23:04.135924 67574 rdma_context.cpp:203] Failed to register memory 0x7b3d42000000: Bad address [14]\nE0529 23:23:04.135931 67574 rdma_context.cpp:203] Failed to register memory 0x7b3cd6000000: Bad address [14]\nE0529 23:23:04.135937 67574 rdma_context.cpp:203] Failed to register memory 0x7b3c6a000000: Bad address [14]\nE0529 23:23:04.135943 67574 rdma_context.cpp:203] Failed to register memory 0x7b3bfe000000: Bad address [14]\nE0529 23:23:04.135949 67574 rdma_context.cpp:203] Failed to register memory 0x7b3b92000000: Bad address [14]\nE0529 23:23:04.135957 67574 rdma_context.cpp:203] Failed to register memory 0x7b3b26000000: Bad address [14]\nE0529 23:23:04.135963 67574 rdma_context.cpp:203] Failed to register memory 0x7b3aba000000: Bad address [14]\nE0529 23:23:04.135972 67574 rdma_context.cpp:203] Failed to register memory 0x7b3a4e000000: Bad address [14]\nE0529 23:23:04.135978 67574 rdma_context.cpp:203] Failed to register memory 0x7b39e2000000: Bad address [14]\nE0529 23:23:04.135987 67574 rdma_context.cpp:203] Failed to register memory 0x7b3976000000: Bad address [14]\nE0529 23:23:04.135995 67574 rdma_context.cpp:203] Failed to register memory 0x7b390a000000: Bad address [14]\nE0529 23:23:04.136003 67574 rdma_context.cpp:203] Failed to register memory 0x7b389e000000: Bad address [14]\nE0529 23:23:04.136010 67574 rdma_context.cpp:203] Failed to register memory 0x7b3832000000: Bad address [14]\nE0529 23:23:04.136018 67574 rdma_context.cpp:203] Failed to register memory 0x7b37c6000000: Bad address [14]\nE0529 23:23:04.136024 67574 rdma_context.cpp:203] Failed to register memory 0x7b375a000000: Bad address [14]\nE0529 23:23:04.136044 67574 rdma_context.cpp:203] Failed to register memory 0x7b36ee000000: Bad address [14]\nE0529 23:23:04.136054 67574 rdma_context.cpp:203] Failed to register memory 0x7b3682000000: Bad address [14]\nE0529 23:23:04.136060 67574 rdma_context.cpp:203] Failed to register memory 0x7b3616000000: Bad address [14]\nE0529 23:23:04.136070 67574 rdma_context.cpp:203] Failed to register memory 0x7b35aa000000: Bad address [14]\nE0529 23:23:04.136076 67574 rdma_context.cpp:203] Failed to register memory 0x7b353e000000: Bad address [14]\nE0529 23:23:04.136083 67574 rdma_context.cpp:203] Failed to register memory 0x7b34d2000000: Bad address [14]\nE0529 23:23:04.136091 67574 rdma_context.cpp:203] Failed to register memory 0x7b3466000000: Bad address [14]\nE0529 23:23:04.136098 67574 rdma_context.cpp:203] Failed to register memory 0x7b33fa000000: Bad address [14]\nE0529 23:23:04.136106 67574 rdma_context.cpp:203] Failed to register memory 0x7b338e000000: Bad address [14]\nE0529 23:23:04.136111 67574 rdma_context.cpp:203] Failed to register memory 0x7b3322000000: Bad address [14]\nE0529 23:23:04.136118 67574 rdma_context.cpp:203] Failed to register memory 0x7b32b6000000: Bad address [14]\nE0529 23:23:04.136124 67574 rdma_context.cpp:203] Failed to register memory 0x7b324a000000: Bad address [14]\nE0529 23:23:04.136132 67574 rdma_context.cpp:203] Failed to register memory 0x7b31de000000: Bad address [14]\nE0529 23:23:04.136140 67574 rdma_context.cpp:203] Failed to register memory 0x7b3172000000: Bad address [14]\nE0529 23:23:04.136147 67574 rdma_context.cpp:203] Failed to register memory 0x7b3106000000: Bad address [14]\nE0529 23:23:04.136154 67574 rdma_context.cpp:203] Failed to register memory 0x7b309a000000: Bad address [14]\nE0529 23:23:04.136161 67574 rdma_context.cpp:203] Failed to register memory 0x7b302e000000: Bad address [14]\nE0529 23:23:04.136171 67574 rdma_context.cpp:203] Failed to register memory 0x7b2fc2000000: Bad address [14]\nE0529 23:23:04.136179 67574 rdma_context.cpp:203] Failed to register memory 0x7b2f56000000: Bad address [14]\nE0529 23:23:04.136188 67574 rdma_context.cpp:203] Failed to register memory 0x7b2eea000000: Bad address [14]\nE0529 23:23:04.136195 67574 rdma_context.cpp:203] Failed to register memory 0x7b2e7e000000: Bad address [14]\nE0529 23:23:04.136204 67574 rdma_context.cpp:203] Failed to register memory 0x7b2e12000000: Bad address [14]\nE0529 23:23:04.136209 67574 rdma_context.cpp:203] Failed to register memory 0x7b2da6000000: Bad address [14]\nE0529 23:23:04.136217 67574 rdma_context.cpp:203] Failed to register memory 0x7b2d3a000000: Bad address [14]\nE0529 23:23:04.136225 67574 rdma_context.cpp:203] Failed to register memory 0x7b2cce000000: Bad address [14]\nE0529 23:23:04.136234 67574 rdma_context.cpp:203] Failed to register memory 0x7b2c62000000: Bad address [14]\n[2025-05-29 23:23:04] INFO:     Started server process [67302]\n[2025-05-29 23:23:04] INFO:     Waiting for application startup.\n[2025-05-29 23:23:04] INFO:     Application startup complete.\n[2025-05-29 23:23:04] INFO:     Uvicorn running on http://127.0.0.1:30001 (Press CTRL+C to quit)\n[2025-05-29 23:23:05] INFO:     127.0.0.1:49286 - \"GET /get_model_info HTTP/1.1\" 200 OK\n[2025-05-29 23:23:05] Start of prefill warmup ...\n[2025-05-29 23:23:05] FakeKVReceiver init with kv_indices: [1 2 3 4], aux_index: 0\n[2025-05-29 23:23:05] FakeKVReceiver poll success\n[2025-05-29 23:23:05] INFO:     127.0.0.1:49288 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-05-29 23:23:05] End of prefill warmup with status 200, resp: [{'text': '!.Sep 12, 201', 'meta_info': {'id': '4751d88f75eb40e78683311f4dcf0861', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 4, 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 0.8205685615539551}}]\n[2025-05-29 23:23:05] The server is fired up and ready to roll!\n[2025-05-29 23:23:23] INFO:     127.0.0.1:55580 - \"POST /v1/completions HTTP/1.1\" 200 OK\n[2025-05-29 23:23:23] Decode transfer failed for request rank=0 decode_req.req.rid='06a4dea161644533872e81c5c0ddf9d7' decode_req.req.bootstrap_room=3583236771377794168 with exception KVTransferError(bootstrap_room=3583236771377794168): Failed to get kvcache from prefill instance, it might be dead\n```\n\n```\nroot@nccl-test-host-1:/diagnostic# python3 -m sglang.srt.disaggregation.mini_lb --prefill http://127.0.0.1:30000 --decode http://127.0.0.1:30001 --host 0.0.0.0 --port 8000\nINFO:     Started server process [68774]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\nINFO:     127.0.0.1:57752 - \"GET /v1/models HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:45502 - \"POST /v1/completions HTTP/1.1\" 200 OK\n```\n\n```\nroot@nccl-test-host-1:/diagnostic# python3 -m sglang.bench_serving --backend sglang-oai --port 8000\nbenchmark_args=Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=8000, dataset_name='sharegpt', dataset_path='', model=None, tokenizer=None, num_prompts=1000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=inf, max_concurrency=None, output_file=None, output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\nNamespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=8000, dataset_name='sharegpt', dataset_path='', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, num_prompts=1000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=inf, max_concurrency=None, output_file=None, output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\n\n#Input tokens: 296523\n#Output tokens: 186737\nStarting warmup with 1 sequences...\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/diagnostic/sglang/python/sglang/bench_serving.py\", line 1866, in <module>\n    run_benchmark(args)\n  File \"/diagnostic/sglang/python/sglang/bench_serving.py\", line 1616, in run_benchmark\n    return asyncio.run(\n  File \"/usr/lib/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"/diagnostic/sglang/python/sglang/bench_serving.py\", line 1232, in benchmark\n    raise ValueError(\nValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):\n  File \"/diagnostic/sglang/python/sglang/bench_serving.py\", line 222, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n```\n\n### Reproduction\n\n```\npython3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-Instruct --disaggregation-mode prefill --disaggregation-ib-device mlx5_0\npython3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-Instruct --disaggregation-mode decode --port 30001 --base-gpu-id 1 --disaggregation-ib-device mlx5_0\npython3 -m sglang.srt.disaggregation.mini_lb --prefill http://127.0.0.1:30000 --decode http://127.0.0.1:30001 --host 0.0.0.0 --port 8000\npython3 -m sglang.bench_serving --backend sglang-oai --port 8000\n```\n\n### Environment\n\n```\nroot@nccl-test-host-1:/diagnostic# ifconfig\neth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1460\n        inet 10.72.0.9  netmask 255.255.255.0  broadcast 10.72.0.255\n        ether 76:4c:cb:b9:7e:bc  txqueuelen 0  (Ethernet)\n        RX packets 4573150  bytes 46775778978 (46.7 GB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 2239792  bytes 167302216 (167.3 MB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\neth2: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 8896\n        inet 192.168.1.3  netmask 255.255.255.255  broadcast 0.0.0.0\n        ether c2:e0:73:df:b9:01  txqueuelen 1000  (Ethernet)\n        RX packets 11526  bytes 733798 (733.7 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 11355  bytes 723978 (723.9 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\neth3: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 8896\n        inet 192.168.2.3  netmask 255.255.255.255  broadcast 0.0.0.0\n        ether be:71:a0:05:d0:04  txqueuelen 1000  (Ethernet)\n        RX packets 10953  bytes 659154 (659.1 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 11026  bytes 665102 (665.1 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\neth4: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 8896\n        inet 192.168.3.3  netmask 255.255.255.255  broadcast 0.0.0.0\n        ether fe:ab:97:d2:02:07  txqueuelen 1000  (Ethernet)\n        RX packets 10953  bytes 659154 (659.1 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 11026  bytes 665176 (665.1 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\neth5: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 8896\n        inet 192.168.4.3  netmask 255.255.255.255  broadcast 0.0.0.0\n        ether 42:e3:56:2b:76:0a  txqueuelen 1000  (Ethernet)\n        RX packets 10953  bytes 659154 (659.1 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 11021  bytes 664746 (664.7 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\neth6: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 8896\n        inet 192.168.5.3  netmask 255.255.255.255  broadcast 0.0.0.0\n        ether 5e:d6:47:c8:c4:0d  txqueuelen 1000  (Ethernet)\n        RX packets 10953  bytes 659154 (659.1 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 11015  bytes 664120 (664.1 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\neth7: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 8896\n        inet 192.168.6.3  netmask 255.255.255.255  broadcast 0.0.0.0\n        ether 2a:ee:0b:71:1c:10  txqueuelen 1000  (Ethernet)\n        RX packets 10953  bytes 659154 (659.1 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 11021  bytes 664766 (664.7 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\neth8: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 8896\n        inet 192.168.7.3  netmask 255.255.255.255  broadcast 0.0.0.0\n        ether 82:fa:1e:d1:9e:13  txqueuelen 1000  (Ethernet)\n        RX packets 10953  bytes 659154 (659.1 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 11016  bytes 664230 (664.2 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\neth9: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 8896\n        inet 192.168.8.3  netmask 255.255.255.255  broadcast 0.0.0.0\n        ether ca:52:0a:55:15:16  txqueuelen 1000  (Ethernet)\n        RX packets 10952  bytes 659094 (659.0 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 11016  bytes 664240 (664.2 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\nlo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536\n        inet 127.0.0.1  netmask 255.0.0.0\n        loop  txqueuelen 1000  (Local Loopback)\n        RX packets 410600179  bytes 39325030589 (39.3 GB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 410600179  bytes 39325030589 (39.3 GB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n```\n```\nroot@nccl-test-host-1:/diagnostic# rdma link\nlink mlx5_0/1 state ACTIVE physical_state LINK_UP netdev eth2\nlink mlx5_1/1 state ACTIVE physical_state LINK_UP netdev eth3\nlink mlx5_2/1 state ACTIVE physical_state LINK_UP netdev eth4\nlink mlx5_3/1 state ACTIVE physical_state LINK_UP netdev eth5\nlink mlx5_4/1 state ACTIVE physical_state LINK_UP netdev eth6\nlink mlx5_5/1 state ACTIVE physical_state LINK_UP netdev eth7\nlink mlx5_6/1 state ACTIVE physical_state LINK_UP netdev eth8\nlink mlx5_7/1 state ACTIVE physical_state LINK_UP netdev eth9\n```\n```\nroot@nccl-test-host-1:/diagnostic# ibv_devices\n    device          \t   node GUID\n    ------          \t----------------\n    mlx5_0          \tc2e073fffedfb901\n    mlx5_1          \tbe71a0fffe05d004\n    mlx5_2          \tfeab97fffed20207\n    mlx5_3          \t42e356fffe2b760a\n    mlx5_4          \t5ed647fffec8c40d\n    mlx5_5          \t2aee0bfffe711c10\n    mlx5_6          \t82fa1efffed19e13\n    mlx5_7          \tca520afffe551516\n```\n```\nroot@nccl-test-host-1:/diagnostic# nvidia-smi\nThu May 29 23:20:05 2025\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA H200                    Off |   00000000:8F:00.0 Off |                    0 |\n| N/A   35C    P0             78W /  700W |       1MiB / 143771MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA H200                    Off |   00000000:90:00.0 Off |                    0 |\n| N/A   37C    P0             78W /  700W |       1MiB / 143771MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   2  NVIDIA H200                    Off |   00000000:96:00.0 Off |                    0 |\n| N/A   35C    P0             78W /  700W |       1MiB / 143771MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   3  NVIDIA H200                    Off |   00000000:97:00.0 Off |                    0 |\n| N/A   38C    P0             78W /  700W |       1MiB / 143771MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   4  NVIDIA H200                    Off |   00000000:C4:00.0 Off |                    0 |\n| N/A   35C    P0             77W /  700W |       1MiB / 143771MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   5  NVIDIA H200                    Off |   00000000:C5:00.0 Off |                    0 |\n| N/A   37C    P0             77W /  700W |       1MiB / 143771MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   6  NVIDIA H200                    Off |   00000000:CB:00.0 Off |                    0 |\n| N/A   34C    P0             77W /  700W |       1MiB / 143771MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   7  NVIDIA H200                    Off |   00000000:CC:00.0 Off |                    0 |\n| N/A   35C    P0             78W /  700W |       1MiB / 143771MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n```",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "open",
    "created_at": "2025-05-29T23:27:04+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6753/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/6753"
  },
  {
    "number": 5249,
    "title": "[Feature] add more CIs for VLM",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled\n\nhttps://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct\nhttps://huggingface.co/google/gemma-3-27b-it\nhttps://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority",
      "MLLM"
    ],
    "state": "open",
    "created_at": "2025-04-10T18:44:02+00:00",
    "closed_at": null,
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5249/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5249"
  },
  {
    "number": 7427,
    "title": "\ud83d\udea7  RFC: Redesign Batch Processing as an Offline Workflow",
    "body": "### **Summary**\nThis RFC proposes removing the existing `/v1/batches` and `/v1/files` endpoints from the main OpenAI-compatible server and replacing them with a standalone offline batch processing service.\n\n> **Note:** As part of the ongoing OpenAI API refactor, the batch support has already been removed from the main server. This RFC serves to document the rationale and formalize the replacement plan.\n\n\n---\n\n### Problem\n\n#### 7.1 Fundamental Issues with the Current Batch API (#7068 )\n\nThe current design for online batch processing is flawed and not production-safe. Key issues include:\n\n- **Server Stability Risk**: Uploading and processing thousands of requests at once can overwhelm online API servers.\n- **Timing Constraints**: Difficult to enforce `completion_window` in a real-time environment.\n- **Resource Contention**: Batch jobs run alongside latency-sensitive requests without proper isolation.\n- **Architecture Mismatch**: Batch workloads are inherently asynchronous/offline, conflicting with the synchronous nature of standard OpenAI endpoints.\n\n---\n\n### Proposed Solution\n\n#### 1. **Simplify Online Endpoints**\n- Remove logic for handling list-wrapped input in `/v1/chat/completions`, `/v1/embeddings`, etc.\n- Accept only single request per HTTP call (OpenAI spec-compliant).\n- Cleaner code and better performance for common-case usage.\n\n#### 2. **Split Out Batch Service**\nImplement batch processing as a **separate offline job runner**, modeled after how vLLM does it.\n\nThis batch runner will:\n- Accept batch jobs in OpenAI-compatible `.jsonl` format\n- Spawn a new process/container to handle the job\n- Stream output to a results file (local or presigned S3 URLs)\n- Optionally enforce `completion_window` guarantees in the background\n\n#### 3. **Remove from Main Server**\n- Remove `/v1/batches` and `/v1/files` routes from the main OpenAI-compatible HTTP server.\n- These should live in a separate service (`batch-runner`) to enforce separation of concerns.\n\n---\n\n### \ud83d\udccc Action Items\n\n- [ ] Finalize and approve this RFC\n- [ ] Implement batch runner\n- [x] Deprecate online batch endpoints\n- [ ] Update docs and integration tests\n",
    "labels": [
      "high priority"
    ],
    "state": "open",
    "created_at": "2025-06-21T18:23:45+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7427/reactions",
      "total_count": 8,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 1,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 1
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7427"
  },
  {
    "number": 3942,
    "title": "Why are there a group of processes concentrated on a single GPU?",
    "body": "I deployed DeepSeek - R1 on a 8*H20-96G server using the following command.\n\n```\npython3 -m sglang.launch_server --model-path DeepSeek-R1 --tp 8 --trust-remote-code --mem-fraction-static 0.9 --host 0.0.0.0 --port 50050 --max-running-requests 128 --context-length 32768 --enable-flashinfer-mla --attention-backend flashinfer\n```\n\nHowever, when using the following command to initiate a request on the H20 server, eight processes will be concentrated on GPU0, as shown in the following screenshot.\n\n```\ncurl -k -X 'POST' \\\n    'http://localhost:50050/v1/chat/completions' \\\n    -H 'accept: application/json' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n\"model\": \"DeepSeek-R1\",\n\"messages\": [{\"role\": \"user\", \"content\": \"Hello\uff0cWho are you?\"}],\n\"stream\": false\n}'\n```\n\n<img width=\"1280\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e44e0c0e-52d2-4c4d-8a58-6224da273e9d\" />\n\nIs this normal? Is there any way to distribute these processes across all GPUs to prevent GPU0 from running out of memory?",
    "labels": [
      "high priority",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-02-28T03:50:01+00:00",
    "closed_at": "2025-05-01T00:21:11+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3942/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3942"
  },
  {
    "number": 922,
    "title": "TTFT latency for long context (16K) is very high around 15 seconds for llama3.1 70b model. (same or worse than vLLM)",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n\n### Describe the bug\n\nI am experimenting with SGLang and vLLM for long context(16K) RAG application which requires real time responses.\r\nI am using single Nvidia A6000 48GB GPU and llaam3.1 70b awq 4 bit model.\r\n\r\nCurrently I am seeing Time for first token latency is around 15 seconds which is very high.\r\nExperimented with parameters like --chunked-prefill-size , --mem-frac etc\r\n\r\ncan you please suggest what are the parameters I need to mainly focus on to get the optimal TTFT for long context ?\n\n### Reproduction\n\nna\n\n### Environment\n\n```Shell\nna\n```\n",
    "labels": [
      "high priority",
      "inactive",
      "performance"
    ],
    "state": "closed",
    "created_at": "2024-08-04T23:14:23+00:00",
    "closed_at": "2024-10-09T01:10:58+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/922/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/922"
  },
  {
    "number": 4384,
    "title": "[Feature] integrate FlashMLA",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nSince SGLang now supports page sizes greater than 1, we should integrate FlashMLA https://github.com/deepseek-ai/FlashMLA.\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-03-13T10:43:57+00:00",
    "closed_at": "2025-03-25T04:14:02+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4384/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/4384"
  },
  {
    "number": 3106,
    "title": "[Bug] Qwen2-VL-7B with sglang has significant numerical calculation errors compared to HF Transformers",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nIn practice, we found that sglang Qwen2-VL model has numerical calculation errors compared to HF Transformers model in both Qwen2VisionTransformer and Qwen2Model parts.\nOur input image has 720 tokens input to Vit encoding, and the lowest embedded cosine similarity in the output is 0.1775. In addition, we directly feed the Vit output and text embedding of Transformers to the LLM part. The cosine similarity of the lowest among the 830 HS outputs in the Prefill stage is reduced to 0.9499, and in the generation stage, as the output increases, the cosine similarity may further decrease to 0.580.\n\nHere are the code blocks we found that caused the differences\uff1a\nRMSNorm's CUDA Kernel and PyTorch Native\nResidual Sum Precision in Transformer Blocks\nQKVParallelLinear vs nn.Linear in Transformer Blocks\nSilu in Qwen2MLP\nCalculation of sin/cos cache in RotaryEmbedding\n\nAfter eliminating the above differences, we achieved precision alignment. But for performance reasons, is there a repair plan for the above issues\uff1f\n\n\n\n### Reproduction\n\nQwen2-VL\n\n### Environment\n\nPython: 3.10.16 (main, Dec  4 2024, 08:53:37) [GCC 9.4.0]\nCUDA available: True\nGPU 0: NVIDIA GeForce RTX 4090 D\nGPU 0 Compute Capability: 8.9\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\nCUDA Driver Version: 550.90.12\nPyTorch: 2.5.1+cu124\nsglang: 0.4.0.post1\nflashinfer: 0.1.6\ntriton: 3.1.0\ntransformers: 4.45.2\ntorchao: 0.7.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.6\nhf_transfer: 0.1.8\nhuggingface_hub: 0.27.0\ninteregular: 0.3.3\nmodelscope: 1.18.1\norjson: 3.10.12\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.4\nmultipart: 0.0.20\nzmq: 26.2.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.58.1\nanthropic: 0.42.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    NIC0    NIC1    NIC2    NIC3    NIC4    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     SYS     SYS     SYS     SYS                             N/A\nNIC0    SYS      X      PHB     PHB     PHB     PHB\nNIC1    SYS     PHB      X      PHB     PHB     PHB\nNIC2    SYS     PHB     PHB      X      PHB     PHB\nNIC3    SYS     PHB     PHB     PHB      X      PHB\nNIC4    SYS     PHB     PHB     PHB     PHB      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n\n\nHypervisor vendor: KVM\nulimit soft: 1048576",
    "labels": [
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-01-24T11:32:46+00:00",
    "closed_at": "2025-01-28T06:04:43+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3106/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3106"
  },
  {
    "number": 1912,
    "title": "[Feature]Support Qwen2_5...etc tools calling by OpenAI API",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nTools calling are becoming mainstream. If you can adapt some updated OpenAI APIs, I would be very grateful.\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-11-04T02:32:41+00:00",
    "closed_at": "2025-02-12T02:12:07+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1912/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1912"
  },
  {
    "number": 4748,
    "title": "[Feature] beat torch compile",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled\n\nLast year and in the first few months of this year, a significant part of my work focused on removing vLLM dependency. Many reliable teammates joined in this process, and we successfully removed the vLLM dependency on the NVIDIA platform for SGLang. Next, I will co-lead progress on beat torch compile. Past experience shows that torch compile is effective - we just need to write some simple torch ops and let torch compile handle the rest. However, in actual production serving, it is not as smooth as expected - for example, slow startup even with cache enabled, compatibility issues when upgrading torch versions leading to previous features breaking in new versions. We need to profile, benchmark, rewrite the bottleneck ops with CUDA/CUTLASS and ensure that **performance without using torch compile can surpass performance with enable torch compile**. Currently [sgl-kernel](https://github.com/sgl-project/sglang/tree/main/sgl-kernel) has secured a size of **500 MB**, I believe everything is ready and now we just need everyone to collaborate together. Cheers!\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "help wanted",
      "high priority",
      "collaboration",
      "performance"
    ],
    "state": "closed",
    "created_at": "2025-03-25T06:18:28+00:00",
    "closed_at": "2025-05-26T16:55:12+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4748/reactions",
      "total_count": 15,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 15,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/4748"
  },
  {
    "number": 2399,
    "title": "[Feature] support constrained decoding benchmark",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled\r\nused for outlines, xgrammar and etc\r\nref https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_serving_guided.py\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2024-12-08T10:56:36+00:00",
    "closed_at": "2025-05-29T21:48:23+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2399/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/2399"
  },
  {
    "number": 4779,
    "title": "[Bug] RecursionError: maximum recursion depth exceeded while calling a Python object",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n\nI confronted this issue today by using docker-latest and docker-dev when using QWQ-32B  but no issue in QWQ-AWQ model. \n\n\n\nFollowing is my error log\n\n```\n  File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 1277, in send_signal\n    self._send_signal(sig)\n  File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 1259, in _send_signal\n    os.kill(pid, sig)\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/engine.py\", line 403, in sigquit_handler\n    kill_process_tree(os.getpid())\n  File \"/sgl-workspace/sglang/python/sglang/srt/utils.py\", line 524, in kill_process_tree\n    child.kill()\n  File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 1323, in kill\n    self._send_signal(signal.SIGKILL)\n  File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 1247, in _send_signal\n    self._raise_if_pid_reused()\n  File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 455, in _raise_if_pid_reused\n    if self._pid_reused or (not self.is_running() and self._pid_reused):\n  File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 630, in is_running\n    self._pid_reused = self != Process(self.pid)\n  File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 317, in __init__\n    self._init(pid)\n  File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 350, in _init\n    self._ident = self._get_ident()\n  File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 390, in _get_ident\n    return (self.pid, self.create_time())\n  File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 772, in create_time\n    self._create_time = self._proc.create_time()\n  File \"/usr/local/lib/python3.10/dist-packages/psutil/_pslinux.py\", line 1646, in wrapper\n    return fun(self, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/psutil/_pslinux.py\", line 1884, in create_time\n    ctime = float(self._parse_stat_file()['create_time'])\n  File \"/usr/local/lib/python3.10/dist-packages/psutil/_pslinux.py\", line 1646, in wrapper\n    return fun(self, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/psutil/_common.py\", line 462, in wrapper\n    raise err from None\n  File \"/usr/local/lib/python3.10/dist-packages/psutil/_common.py\", line 460, in wrapper\n    return fun(self)\n  File \"/usr/local/lib/python3.10/dist-packages/psutil/_pslinux.py\", line 1712, in _parse_stat_file\n    data = bcat(f\"{self._procfs_path}/{self.pid}/stat\")\n  File \"/usr/local/lib/python3.10/dist-packages/psutil/_common.py\", line 814, in bcat\n    return cat(fname, fallback=fallback, _open=open_binary)\n  File \"/usr/local/lib/python3.10/dist-packages/psutil/_common.py\", line 802, in cat\n    with _open(fname) as f:\nRecursionError: maximum recursion depth exceeded while calling a Python object\n```\n\n### Reproduction\n\nHere is the code that I tried\n\n\ndev docker (found it updated 4 hours ago but still does not work)\n\n```\ndocker run --gpus all \\\n    --shm-size 32g \\\n    -p 10000:10000 \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --ipc=host \\\n    lmsysorg/sglang:dev \\\n    python3 -m sglang.launch_server --model-path Qwen/QwQ-32B --host 0.0.0.0 --port 10000\n```\nlatest docker (found it updated ~10 days ago so it does not work):\n\n```\ndocker run --gpus all \\\n    --shm-size 32g \\\n    -p 10000:10000 \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --ipc=host \\\n    lmsysorg/sglang:latest \\\n    python3 -m sglang.launch_server --model-path Qwen/QwQ-32B --host 0.0.0.0 --port 10000\n```\n\n### Environment\n\nGPU: NVIDIA L40S \nNVIDIA DRIVER: NVIDIA-SMI 560.35.03 \nCUDA Version: 12.6 \n\nOS info:\nAmazon EC2 instance:\nAmazon Linux release 2023.6.20241121 (Amazon Linux)\n\nDocker dev(4 hours before updated https://hub.docker.com/layers/lmsysorg/sglang/dev/images/sha256-8575bbe5aa8efebd22f141f6ca6f166371b6110f485ac0497ab38f2f350feebe)\n\nDocker latest (12 day before updated https://hub.docker.com/layers/lmsysorg/sglang/latest/images/sha256-7245dda2c55e58ed1a26f4758bf3c2bfa5e2c571bf8ad888e6e3a31d972ff7b6)\n\n",
    "labels": [
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2025-03-26T04:14:50+00:00",
    "closed_at": "2025-03-26T15:59:08+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4779/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4779"
  },
  {
    "number": 1616,
    "title": "[Feature] GGUF support",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nHi! Since .gguf format is already supported by vLLM, is it be possible to add support for it in SGLang server?\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-10-09T05:45:17+00:00",
    "closed_at": "2024-12-01T10:51:57+00:00",
    "comments": 26,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1616/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1616"
  },
  {
    "number": 376,
    "title": "Loading Chat Template in a more flexible way?",
    "body": "The Chat models like [codellama-instruct](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf/blob/main/tokenizer_config.json), [qwen](https://modelscope.cn/models/qwen/Qwen1.5-14B-Chat/file/view/master?fileName=tokenizer_config.json&status=1) all have a `chat_template` field in the JSON which defines the chat template of the model. But I notice it seems that sglang currently hard-coded the chat-template in the [.py](https://github.com/sgl-project/sglang/blob/1bf1cf195302fdff14a4321eb8a17831f5c2fc11/python/sglang/lang/chat_template.py#L79) file. Would it be more flexible to load the default chat template from the tokenizer_config file if provided? It seems [vllm](https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/openai/serving_chat.py#L335) did in this way.",
    "labels": [
      "good first issue",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-04-21T12:50:17+00:00",
    "closed_at": "2024-07-25T06:33:13+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/376/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/376"
  },
  {
    "number": 91,
    "title": "Support Yi-VL-6B/34B",
    "body": "The Yi-VL adopts llava but with silightly different in weights and inference. see [disscusion](https://huggingface.co/01-ai/Yi-VL-34B/discussions/3)\r\n\r\nhf repo:\r\nhttps://huggingface.co/01-ai/Yi-VL-6B\r\nhttps://huggingface.co/01-ai/Yi-VL-34B",
    "labels": [
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-01-24T03:49:46+00:00",
    "closed_at": "2024-02-01T21:38:25+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/91/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/91"
  },
  {
    "number": 3908,
    "title": "[Docs]  Improve DPSK docs in dark mode",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n<img width=\"1393\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/39d60ef8-c7fa-42e0-9961-5bd9c082209f\" />\n\nI use html to write this docs and it looks bad. So could someone fix it here?\n\nhttps://github.com/sgl-project/sglang/blob/main/docs/references/deepseek.md\n\n### Related resources\n\n_No response_",
    "labels": [
      "documentation",
      "good first issue",
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2025-02-27T05:00:48+00:00",
    "closed_at": "2025-02-27T08:13:05+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3908/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3908"
  },
  {
    "number": 2662,
    "title": "[Feature] Change contribution guide",
    "body": "### Checklist\r\n\r\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [x] 2. Please use English, otherwise it will be closed.\r\n\r\n### Motivation\r\n\r\nhttps://sgl-project.github.io/references/contributor_guide.html\r\n\r\nThis has been outdated for long. We need to add guide on:\r\n\r\n1. How to run docs CI, build it locally, compile it and clean the output and make PR.\r\n2. How to do unit tests locally and add unit tests to CI.\r\n3. How to write elegant unit test following other tests.\r\n4. How to pre-commit.\r\n\r\n### Related resources\r\n\r\n_No response_",
    "labels": [
      "documentation",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-12-30T07:53:12+00:00",
    "closed_at": "2025-04-29T16:22:21+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2662/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2662"
  },
  {
    "number": 3615,
    "title": "[Feature] Parallelism Experiments on AIMO and LIMO",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCan anyone help test @Simon V\u2019s branch? It\u2019s pretty complete, but we\u2019d like to run some parallel experiments \n\nhttps://github.com/sgl-project/sglang/pull/3532\n\nFeel free to submit a PR reporting the results of the parallel experiments, including std, var, etc. Thanks!\n\n### Related resources\n\n_No response_",
    "labels": [
      "documentation",
      "good first issue",
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2025-02-16T19:11:32+00:00",
    "closed_at": "2025-02-20T19:11:38+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3615/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3615"
  },
  {
    "number": 3188,
    "title": "Any benchmarks comparing with TGI?",
    "body": "As the tittle says, is there any benchmark comparing with TGI (https://github.com/huggingface/text-generation-inference)? I see some results comparing directly with vLLM, but would love to see also a direct comparison against TGI, as in the last release the got a good performance improvement, thanks for the info in advance!",
    "labels": [
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2025-01-27T22:14:34+00:00",
    "closed_at": "2025-01-30T17:42:07+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3188/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3188"
  },
  {
    "number": 3383,
    "title": "[Feature] Use xgrammar as default grammar backend to aviod I/O errors while using Outlines in a multi-node setting",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nrelated issues:\n#3375 \nrelated discussiton:\n[#vllm 4193](https://github.com/vllm-project/vllm/issues/4193)\nrelated pr:\nhttps://github.com/sgl-project/sglang/pull/3379\n\n### Related resources\n\nxGrammar stores its cache in RAM instead of disk, avoiding file system conflicts.\nCache size is small (typically <0.5MB per schema), meaning it doesn't require persistent disk storage.\nxGrammar is thread-safe, ensuring it can run across multiple Slurm nodes without concurrency issues.",
    "labels": [
      "good first issue",
      "help wanted",
      "grammar-backend"
    ],
    "state": "closed",
    "created_at": "2025-02-07T23:11:12+00:00",
    "closed_at": "2025-05-26T21:08:02+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3383/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3383"
  },
  {
    "number": 3263,
    "title": "[Feature] Support ipv6 in SGLang",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n@shuaills \n\nhttps://github.com/sgl-project/sglang/issues/2892#issuecomment-2629436443\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2025-02-02T19:37:24+00:00",
    "closed_at": "2025-05-15T00:49:46+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3263/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3263"
  },
  {
    "number": 3304,
    "title": "[Bug] RuntimeError: RMSNorm failed with error code invalid configuration argument",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHi, I am using the main branch of SGLang, and downloading Mixtral-8x22B from huggingface. \n\nCUDA: 12.4\n2 nodes, each has 4 H100 96GB.\n\nI am deploying the server using:\n```\npython -m sglang.launch_server --model-path Mixtral-8x22B-v0.1 --tp 8 --dist-init-addr xxx:5000 --nnodes 2 --node-rank 0 --trust-remote-code --disable-cuda-graph\npython -m sglang.launch_server --model-path Mixtral-8x22B-v0.1 --tp 8 --dist-init-addr xxx:5000 --nnodes 2 --node-rank 1 --trust-remote-code --disable-cuda-graph\n\n```\nAnd I am running the MMLU benchmark:\n```\ncd sglang/benchmark/mmlu\npython3 bench_sglang.py --nsub 10\n```\n\nIt pops out the error:\n```\n[2025-02-04 21:18:29 DP3 TP3] TpModelWorkerClient hit an exception: Traceback (most recent call last):\n  File \"sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 109, in forward_thread_func\n    self.forward_thread_func_()\n  File \"python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 140, in forward_thread_func_\n    logits_output, next_token_ids = self.worker.forward_batch_generation(\n  File \"sglang/python/sglang/srt/managers/tp_worker.py\", line 164, in forward_batch_generation\n    logits_output = self.model_runner.forward(forward_batch)\n  File \"sglang/python/sglang/srt/model_executor/model_runner.py\", line 787, in forward\n    return self.forward_idle(forward_batch)\n  File \"sglang/python/sglang/srt/model_executor/model_runner.py\", line 770, in forward_idle\n    return self.model.forward(\n  File \"sglang/python/sglang/srt/models/mixtral.py\", line 314, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)\n  File \"python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"sglang/python/sglang/srt/models/mixtral.py\", line 286, in forward\n    hidden_states, residual = layer(\n  File \"python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"sglang/python/sglang/srt/models/mixtral.py\", line 232, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"python3.10/site-packages/vllm/model_executor/custom_op.py\", line 26, in forward\n    return self._forward_method(*args, **kwargs)\n  File \"sglang/python/sglang/srt/layers/layernorm.py\", line 59, in forward_cuda\n    out = rmsnorm(x, self.weight.data, self.variance_epsilon)\n  File \"python3.10/site-packages/sgl_kernel/ops/__init__.py\", line 156, in rmsnorm\n    torch.ops.sgl_kernels.rmsnorm(out, input, weight, eps, _get_cuda_stream(device))\n  File \"python3.10/site-packages/torch/_ops.py\", line 1116, in __call__\n    return self._op(*args, **(kwargs or {}))\n  File \"python3.10/site-packages/torch/utils/_device.py\", line 106, in __torch_function__\n    return func(*args, **kwargs)\n  File \"python3.10/site-packages/torch/_ops.py\", line 1116, in __call__\n    return self._op(*args, **(kwargs or {}))\nRuntimeError: RMSNorm failed with error code invalid configuration argument\n```\n\n\n### Reproduction\n\nModel: Mixtral 8x22B\nScript: MMLU benchmark\n\nPlease see above.\n\n### Environment\n\n```\nPython: 3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:16:10) [GCC 13.3.0]\nCUDA available: True\nGPU 0,1,2,3: NVIDIA H100\nGPU 0,1,2,3 Compute Capability: 9.0\nCUDA_HOME: cuda/gcc/11.3.1/12.4.1-r5e7ajh\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.90.12\nPyTorch: 2.5.1+cu124\nflashinfer: 0.1.6+cu124torch2.4\ntriton: 3.1.0\ntransformers: 4.48.2\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.61.0\nanthropic: 0.45.2\ndecord: 0.6.0\n```",
    "labels": [
      "good first issue",
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2025-02-05T02:25:13+00:00",
    "closed_at": "2025-05-11T15:17:16+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3304/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3304"
  },
  {
    "number": 2877,
    "title": "[Bug] finish_reason is not right when Qwen call a tool",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n{\r\n    \"completion\": {\r\n        \"created\": 1736822678,\r\n        \"usage\": {\r\n            \"completion_tokens\": 75,\r\n            \"prompt_tokens\": 43,\r\n            \"total_tokens\": 118\r\n        },\r\n        \"model\": \"Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4\",\r\n        \"id\": \"a82af6309caf48a0994c77acbedbc846\",\r\n        \"choices\": [\r\n            {\r\n                \"finish_reason\": \"stop\",\r\n                \"matched_stop\": 151645,\r\n                \"index\": 0,\r\n                \"message\": {\r\n                    \"role\": \"assistant\",\r\n                    \"content\": \"I don't have real-time data access, so I can't provide the current temperature in San Francisco right now. Additionally, I don't have the capability to predict future weather conditions like tomorrow's temperature. For the most accurate and up-to-date information, you can check a reliable weather website or app, or visit a site like the National Weather Service or Weather.com.\"\r\n                }\r\n            }\r\n        ],\r\n        \"object\": \"chat.completion\"\r\n    }\r\n}\r\n\r\nAs the response above, the `finish_reason` is `stop` but excepted `tool_calls`.\n\n### Reproduction\n\n\"request\":{\r\n  \"model\": \"Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4\",\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"system\",\r\n      \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant. \"\r\n    },\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"What is the temperature in San Francisco now? How about tomorrow?\"\r\n    }\r\n  ],\r\n  \"tools\": [\r\n  {\r\n    \"type\": \"function\",\r\n    \"function\": {\r\n      \"name\": \"get_current_temperature\",\r\n      \"description\": \"Get current temperature at a location.\",\r\n      \"parameters\": {\r\n        \"type\": \"object\",\r\n        \"properties\": {\r\n          \"location\": {\r\n            \"type\": \"string\",\r\n            \"description\": \"The location to get the temperature for, in the format \\\"City, State, Country\\\".\"\r\n          },\r\n          \"unit\": {\r\n            \"type\": \"string\",\r\n            \"enum\": [\r\n              \"celsius\",\r\n              \"fahrenheit\"\r\n            ],\r\n            \"description\": \"The unit to return the temperature in. Defaults to \\\"celsius\\\".\"\r\n          }\r\n        },\r\n        \"required\": [\r\n          \"location\"\r\n        ]\r\n      }\r\n    }\r\n  },\r\n  {\r\n    \"type\": \"function\",\r\n    \"function\": {\r\n      \"name\": \"get_temperature_date\",\r\n      \"description\": \"Get temperature at a location and date.\",\r\n      \"parameters\": {\r\n        \"type\": \"object\",\r\n        \"properties\": {\r\n          \"location\": {\r\n            \"type\": \"string\",\r\n            \"description\": \"The location to get the temperature for, in the format \\\"City, State, Country\\\".\"\r\n          },\r\n          \"date\": {\r\n            \"type\": \"string\",\r\n            \"description\": \"The date to get the temperature for, in the format \\\"Year-Month-Day\\\".\"\r\n          },\r\n          \"unit\": {\r\n            \"type\": \"string\",\r\n            \"enum\": [\r\n              \"celsius\",\r\n              \"fahrenheit\"\r\n            ],\r\n            \"description\": \"The unit to return the temperature in. Defaults to \\\"celsius\\\".\"\r\n          }\r\n        },\r\n        \"required\": [\r\n          \"location\",\r\n          \"date\"\r\n        ]\r\n      }\r\n    }\r\n  }\r\n],\r\n\"parallel_tool_calls\": false,\r\n\"temperature\":0.7,\r\n\"top_p\":0.8,\r\n\"tool_choice\": \"auto\"\r\n}}'\r\n\n\n### Environment\n\nPython: 3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0,1: NVIDIA H20\r\nGPU 0,1 Compute Capability: 9.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 11.7, V11.7.64\r\nCUDA Driver Version: 550.54.15\r\nPyTorch: 2.5.1+cu124\r\nsglang: 0.4.1.post4\r\nflashinfer: 0.1.6+cu121torch2.3\r\ntriton: 3.1.0\r\ntransformers: 4.47.1\r\ntorchao: 0.7.0\r\nnumpy: 1.26.4\r\naiohttp: 3.11.11\r\nfastapi: 0.115.6\r\nhf_transfer: 0.1.9\r\nhuggingface_hub: 0.27.1\r\ninteregular: 0.3.3\r\nmodelscope: 1.22.0\r\norjson: 3.10.14\r\npackaging: 23.2\r\npsutil: 5.9.6\r\npydantic: 2.10.5\r\nmultipart: 0.0.20\r\nzmq: 26.2.0\r\nuvicorn: 0.34.0\r\nuvloop: 0.21.0\r\nvllm: 0.6.4.post1\r\nopenai: 1.59.6\r\nanthropic: 0.42.0\r\ndecord: 0.6.0",
    "labels": [
      "help wanted",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-01-14T03:06:37+00:00",
    "closed_at": "2025-05-13T00:19:06+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2877/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2877"
  },
  {
    "number": 4324,
    "title": "[Bug] fix gemma-2-2b-it-FP8 accuracy",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThe accuracy of `neuralmagic/gemma-2-2b-it-FP8` drops from 0.62 to 0.52 in the main branch. It was detected by our nightly CI run. We need to fix this.\n\n```\nneuralmagic/gemma-2-2b-it-FP8 | 0.512 | 0.6\n```\nhttps://github.com/sgl-project/sglang/actions/runs/13800885290\n\n### Reproduction\n\nN/A\n\n### Environment\n\nN/A",
    "labels": [
      "bug",
      "good first issue",
      "help wanted",
      "high priority",
      "quant"
    ],
    "state": "closed",
    "created_at": "2025-03-12T01:27:58+00:00",
    "closed_at": "2025-05-21T09:30:43+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4324/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/4324"
  },
  {
    "number": 3195,
    "title": "[Feature] Support DeepSeek Janus Models",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nDocker is a valuable tool for the management of dependencies. Indeed, it can simplify the running of Janus Models to a single command:  \n```bash\ndocker run -it --rm \\\n  -p 8000:8000 \\\n  -d \\\n  -v huggingface:/root/.cache/huggingface \\\n  -w /app \\\n  --gpus all \\\n  --name janus \\\n  -e MODEL_NAME=deepseek-ai/Janus-Pro-7B \\\n  julianfl0w/janus:latest\n```\n\nMake sure it's working by navigating in your browser to  \n[http://localhost:8000/webui](http://localhost:8000/webui)\n\nand by running\n```bash\ndocker logs janus\n```\n\nThis keeps all the Torch dependencies contained within the image, meaning the user doesn't have to adjust their base installations to run models like these. \n\nNote: You will have to install NVIDIA Container Runtime (or equivalent)\n\nThe implementation of this Dockerfile can be found at [DeepSeek Janus PR#38](https://github.com/deepseek-ai/Janus/pull/38)\n\n### Related resources\n\n_No response_",
    "labels": [
      "help wanted",
      "inactive",
      "MLLM"
    ],
    "state": "closed",
    "created_at": "2025-01-28T18:37:47+00:00",
    "closed_at": "2025-04-30T00:18:51+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3195/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3195"
  },
  {
    "number": 2561,
    "title": "[Feature] Running multi-node offline engine inference ( via SLURM)",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nA lot of academic institutions only allow access to larger node clusters via SLURM and it is not immediately clear how would I reuse the code to run Llama 405B BF16 on 2 nodes (by starting a server) to perform offline inference\n\n### Related resources\n\n_No response_",
    "labels": [
      "help wanted",
      "collaboration",
      "feature"
    ],
    "state": "closed",
    "created_at": "2024-12-23T15:24:49+00:00",
    "closed_at": "2025-01-31T23:58:27+00:00",
    "comments": 39,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2561/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2561"
  },
  {
    "number": 3196,
    "title": "[Feature] deepseek v3 60 tokens/sec on deepseek API vs. 13 tokens/sec on sglang",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nThe PR for AMD + sglang and NVIDIA + sglang was that it was \"fully\" supported, but it seems something is off by the speed.  A single sequence runs at only order 13 tokens/sec for long generation with TTFT order 2 seconds.  This is consistent with vLLM as well.  True for either 8*MI300X or 8*H200 or 2*8*H200.\n\nFor only 37B parameters + 14B MOE parameters, this seems way too slow.  Also, deepseek API (before it started to break down) was order 60 tokens/sec early on and they advertise 60 tokens/sec.  This is more aligned with the parameters active.\n\nWhat is missing from truly fully suppporting deepseek V3 and R1?  Can these features be enumerated and added in a roadmap?\n\n### Related resources\n\n_No response_",
    "labels": [
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2025-01-28T18:40:18+00:00",
    "closed_at": "2025-02-15T01:21:30+00:00",
    "comments": 29,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3196/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3196"
  },
  {
    "number": 2376,
    "title": "[Feature] Support EBNF in xgrammar",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nxgrammar supports EBNF. We would like to integrate this feature into SGLang.\r\n\r\nWe can add a new parameter called `ebnf` in sampling_params.py and treat it similar to regex and JSON.\r\n\n\n### Related resources\n\nhttps://xgrammar.mlc.ai/docs/how_to/ebnf_guided_generation.html\r\nhttps://github.com/sgl-project/sglang/blob/f5b2a3aa67efb10918965b9f3555ff24ef971902/python/sglang/srt/sampling/sampling_params.py#L36-L38\r\nhttps://github.com/sgl-project/sglang/blob/main/test/srt/test_json_constrained.py",
    "labels": [
      "good first issue",
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2024-12-06T12:07:00+00:00",
    "closed_at": "2025-05-26T00:02:55+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2376/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2376"
  },
  {
    "number": 3762,
    "title": "[Bug] use Eagle with speculative-num-steps=1",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen I attempted to use the Triton backend for Eagle to launch the Qwen-7B model, the process failed.\n```\nTraceback (most recent call last):\n  File \"/data/csl/project/sglang/python/sglang/srt/managers/scheduler.py\", line 1827, in run_scheduler_process\n    scheduler.event_loop_normal()\n  File \"/data/csl/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/data/csl/project/sglang/python/sglang/srt/managers/scheduler.py\", line 478, in event_loop_normal\n    result = self.run_batch(batch)\n  File \"/data/csl/project/sglang/python/sglang/srt/managers/scheduler.py\", line 1089, in run_batch\n    ) = self.draft_worker.forward_batch_speculative_generation(batch)\n  File \"/data/csl/project/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 111, in forward_batch_speculative_generation\n    spec_info: EagleVerifyInput = self.draft(batch)\n  File \"/data/csl/project/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 194, in draft\n    ret = EagleVerifyInput.create(\n  File \"/data/csl/project/sglang/python/sglang/srt/speculative/eagle_utils.py\", line 194, in create\n    build_tree_kernel(\n  File \"/data/csl/project/sglang/python/sglang/srt/speculative/build_eagle_tree.py\", line 124, in build_tree_kernel\n    build_tree_kernel_efficient_preprocess(\n  File \"/data/csl/project/sglang/python/sglang/srt/speculative/build_eagle_tree.py\", line 29, in build_tree_kernel_efficient_preprocess\n    top_scores = torch.topk(score_list, num_verify_tokens - 1, dim=-1)\nRuntimeError: selected index k out of range\n```\nI followed this error and tried to fix the bug, but I still encountered the following error.\n```\nTraceback (most recent call last):\n  File \"/data/csl/project/sglang/python/sglang/srt/managers/scheduler.py\", line 1827, in run_scheduler_process\n    scheduler.event_loop_normal()\n  File \"/data/csl/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/data/csl/project/sglang/python/sglang/srt/managers/scheduler.py\", line 478, in event_loop_normal\n    result = self.run_batch(batch)\n  File \"/data/csl/project/sglang/python/sglang/srt/managers/scheduler.py\", line 1089, in run_batch\n    ) = self.draft_worker.forward_batch_speculative_generation(batch)\n  File \"/data/csl/project/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 116, in forward_batch_speculative_generation\n    spec_info: EagleVerifyInput = self.draft(batch)\n  File \"/data/csl/project/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 199, in draft\n    ret = EagleVerifyInput.create(\n  File \"/data/csl/project/sglang/python/sglang/srt/speculative/eagle_utils.py\", line 194, in create\n    build_tree_kernel(\n  File \"/data/csl/project/sglang/python/sglang/srt/speculative/build_eagle_tree.py\", line 165, in build_tree_kernel\n    index = retrive_index.sum(dim=-1) != -spec_steps - 2\nRuntimeError: CUDA error: invalid configuration argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n```\n\n\n### Reproduction\n\npython -m sglang.launch_server --model-path /data/csl/hf_model/DeepSeek-R1-Distill-Qwen-7B/  --disable-radix-cache  --speculative-algo EAGLE --speculative-draft /data/csl/hf_model/EAGLE-Qwen2-7B-Instruct/ --speculative-num-steps 1 --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 --mem-fraction 0.6 --disable-cuda-graph --attention-backend triton  --mem-fraction-static 0.7\n\n\n### Environment\n\n```\nPython: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1: NVIDIA GeForce RTX 4090\nGPU 2,3: NVIDIA GeForce RTX 3090\nGPU 0,1 Compute Capability: 8.9\nGPU 2,3 Compute Capability: 8.6\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.0, V12.0.76\nCUDA Driver Version: 535.146.02\nPyTorch: 2.5.1+cu121\nsglang: 0.4.3.post2\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.1.post2\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.61.1\ntiktoken: 0.8.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     SYS     SYS     0-15,32-47      0               N/A\nGPU1    SYS      X      SYS     SYS     0-15,32-47      0               N/A\nGPU2    SYS     SYS      X      SYS     16-31,48-63     1               N/A\nGPU3    SYS     SYS     SYS      X      16-31,48-63     1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576\n```",
    "labels": [
      "bug",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-02-21T13:32:09+00:00",
    "closed_at": "2025-04-24T00:18:22+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3762/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3762"
  },
  {
    "number": 5212,
    "title": "[Bug] Llama4 OOM with 400k input request",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI started a server on 8xH100 with `meta-llama/Llama-4-Scout-17B-16E-Instruct` with the following command:\n\n```\npython sglang.launch_server --model meta-llama/Llama-4-Scout-17B-16E-Instruct \\\n--port 8080 \\\n--tp-size 8 \\\n--chat-template llama-4 \\\n--attention-backend=fa3 \\\n--mem-fraction-static=0.8 \\\n--context-length 1000000 \n```\n\nThen sent a request with around 400k input will cause CUDA OOM:\n```\n[2025-04-09 17:19:56] Received sigquit from a child process. It usually means the child failed.\n[2025-04-09 17:19:56 TP5] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2045, in run_scheduler_process\n    scheduler.event_loop_normal()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 608, in event_loop_normal\n    result = self.run_batch(batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1395, in run_batch\n    logits_output, next_token_ids = self.tp_worker.forward_batch_generation(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 175, in forward_batch_generation\n    logits_output = self.model_runner.forward(forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1001, in forward\n    return self.forward_extend(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 962, in forward_extend\n    return self.model.forward(\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/mllama4.py\", line 83, in forward\n    hs = general_mm_embed_routine(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/mm_utils.py\", line 354, in general_mm_embed_routine\n    inputs_embeds = embed_tokens(input_ids)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/vocab_parallel_embedding.py\", line 482, in forward\n    output_parallel = self.quant_method.embedding(self, masked_input.long())\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/vocab_parallel_embedding.py\", line 62, in embedding\n    return F.embedding(input_, layer.weight)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2551, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.43 GiB. GPU 5 has a total capacity of 79.44 GiB of which 2.64 GiB is free. Process 679812 has 76.79 GiB memory in use. Of the allocated memory 72.76 GiB is allocated by PyTorch, with 26.38 MiB allocated in private pools (e.g., CUDA Graphs), and 293.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n[2025-04-09 17:19:56 TP6] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2045, in run_scheduler_process\n    scheduler.event_loop_normal()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 608, in event_loop_normal\n    result = self.run_batch(batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1395, in run_batch\n    logits_output, next_token_ids = self.tp_worker.forward_batch_generation(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 175, in forward_batch_generation\n    logits_output = self.model_runner.forward(forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1001, in forward\n    return self.forward_extend(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 962, in forward_extend\n    return self.model.forward(\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/mllama4.py\", line 83, in forward\n    hs = general_mm_embed_routine(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/mm_utils.py\", line 354, in general_mm_embed_routine\n    inputs_embeds = embed_tokens(input_ids)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/vocab_parallel_embedding.py\", line 482, in forward\n    output_parallel = self.quant_method.embedding(self, masked_input.long())\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/vocab_parallel_embedding.py\", line 62, in embedding\n    return F.embedding(input_, layer.weight)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2551, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.43 GiB. GPU 6 has a total capacity of 79.44 GiB of which 2.64 GiB is free. Process 679813 has 76.79 GiB memory in use. Of the allocated memory 72.76 GiB is allocated by PyTorch, with 26.38 MiB allocated in private pools (e.g., CUDA Graphs), and 293.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n[2025-04-09 17:19:56] Received sigquit from a child process. It usually means the child failed.\n```\n\n\n\n### Reproduction\n\nStart server on a 8xH100:\n```\npython sglang.launch_server --model meta-llama/Llama-4-Scout-17B-16E-Instruct \\\n--port 8080 \\\n--tp-size 8 \\\n--chat-template llama-4 \\\n--attention-backend=fa3 \\\n--context-length 1000000 \n```\n\nRun `python3 send_llama_request.py` \n\nBelow is the content of `send_llama_request.py`\n```python\nimport requests\nimport json\n\npayload = {\n    \"model\": \"sgl-model\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"1 \" * 200000\n            },\n            ]\n        }\n    ],\n    \"max_tokens\": 200,\n    \"temperature\": 0.0,\n    \"top_p\": 0.75,\n    \"top_k\": -1,\n    \"stream\": True,\n    \"stream_options\": {\n        \"include_usage\": True,\n    },\n    \"ignore_eos\": True,\n}\n\n# Send the POST request\nresponse = requests.post(\n    \"http://localhost:8080/v1/chat/completions\",\n    # \"http://localhost:9922/v1/chat/completions\",\n    headers={\"Content-Type\": \"application/json\", \"opc-request-id\": \"xfrjoiwejfioewngrinel\"},\n    json=payload,\n    stream=True\n)\n\ngenerated_text = \"\"\n\n# Check if the response was successful\nif response.status_code == 200:\n    for chunk in response.iter_lines(chunk_size=None):\n        print(chunk)\n        chunk = chunk.strip()\n        if not chunk:\n            continue\n        stem = \"data: \" \n        chunk = chunk[len(stem) :]\n        if chunk == b\"[DONE]\":\n            continue\n\n        data = json.loads(chunk)\n        if \"error\" in data:\n            error_msg = data[\"error\"][\"message\"]\n            error_response_code = data[\"error\"][\"code\"]\n            raise RuntimeError(data[\"error\"][\"message\"])\n\n        delta = data[\"choices\"][0][\"delta\"]\n        if delta.get(\"content\", None):\n            generated_text += delta[\"content\"]\n        \n    print(\"Generated text:\", generated_text)\n    print(\"Status:\", response.status_code)\nelse:\n    print(\"Error:\", response.status_code, response.text)\n    print(response.json())\n```\n\n### Environment\n\n```\nPython: 3.10.16 (main, Dec  4 2024, 08:53:37) [GCC 9.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 560.35.03\nPyTorch: 2.5.1+cu124\nsglang: 0.4.5\nsgl_kernel: 0.0.8\nflashinfer: 0.1.6+cu124torch2.4\ntriton: 3.1.0\ntransformers: 4.51.0\ntorchao: 0.10.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.6\nhf_transfer: 0.1.8\nhuggingface_hub: 0.30.1\ninteregular: 0.3.3\nmodelscope: 1.21.1\norjson: 3.10.13\noutlines: 0.0.46\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.4\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nxgrammar: 0.1.17\nopenai: 1.59.3\ntiktoken: 0.7.0\nanthropic: 0.42.0\nlitellm: 1.56.10\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10   NIC11   NIC12   NIC13   NIC14NIC15   NIC16   NIC17   CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PXB     PXB     NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS     0-55,112-167    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    PXB     PXB     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS     0-55,112-167    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    NODE    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS     0-55,112-167    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS     0-55,112-167    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    NODE    NODENODE     NODE    NODE    56-111,168-223  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    PXB     PXB     NODENODE     NODE    NODE    56-111,168-223  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    PXB PXB      NODE    NODE    56-111,168-223  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    NODENODE     PXB     PXB     56-111,168-223  1               N/A\nNIC0    PXB     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS\nNIC1    PXB     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS\nNIC2    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS\nNIC3    NODE    PXB     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      PIX     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS\nNIC4    NODE    PXB     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX      X      NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS\nNIC5    NODE    NODE    PXB     NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE     X      PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS\nNIC6    NODE    NODE    PXB     NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    PIX      X      NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS\nNIC7    NODE    NODE    NODE    PXB     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    NODE    NODE     X      PIX     SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS\nNIC8    NODE    NODE    NODE    PXB     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX      X      SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS\nNIC9    SYS     SYS     SYS     SYS     PXB     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    NODE    NODENODE     NODE    NODE\nNIC10   SYS     SYS     SYS     SYS     PXB     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    NODE    NODENODE     NODE    NODE\nNIC11   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    NODE    NODENODE     NODE    NODE\nNIC12   SYS     SYS     SYS     SYS     NODE    PXB     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      PIX     NODENODE     NODE    NODE\nNIC13   SYS     SYS     SYS     SYS     NODE    PXB     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX      X      NODENODE     NODE    NODE\nNIC14   SYS     SYS     SYS     SYS     NODE    NODE    PXB     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE     X  PIX      NODE    NODE\nNIC15   SYS     SYS     SYS     SYS     NODE    NODE    PXB     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    PIX  X       NODE    NODE\nNIC16   SYS     SYS     SYS     SYS     NODE    NODE    NODE    PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    NODENODE      X      PIX\nNIC17   SYS     SYS     SYS     SYS     NODE    NODE    NODE    PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    NODENODE     PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n  NIC10: mlx5_10\n  NIC11: mlx5_11\n  NIC12: mlx5_12\n  NIC13: mlx5_13\n  NIC14: mlx5_14\n  NIC15: mlx5_15\n  NIC16: mlx5_16\n  NIC17: mlx5_17\n\n\nulimit soft: 65535\n```",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-04-10T00:22:53+00:00",
    "closed_at": "2025-04-11T08:24:15+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5212/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5212"
  },
  {
    "number": 2554,
    "title": "upgrade setuptools and wheel if you found \"torch module not found\" when installing",
    "body": "I encountered an issue while installing `sglang`. After upgrading pip (`pip install --upgrade pip`), I ran:\r\n\r\n```bash\r\npip install \"sglang[all]\" --find-links https://flashinfer.ai/whl/cu121/torch2.4/flashinfer/\r\n```\r\n\r\nBut it failed with the error:  \r\n`ModuleNotFoundError: No module named 'torch'`.\r\n\r\nI found on the Flash Attention GitHub that running this solved the issue:  \r\n```bash\r\npython -m pip install --upgrade pip wheel setuptools\r\n```\r\n\r\nIt worked for me, so sharing in case someone faces the same problem! I don't know what the exact reason is though as the error itself was pretty strange. ",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-12-23T04:02:50+00:00",
    "closed_at": "2025-01-30T17:37:49+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2554/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2554"
  },
  {
    "number": 1030,
    "title": "[Bug] OOM for concurrent long requests",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n\n### Describe the bug\n\nI am trying to benchmark inference of llama3-8b with long requests, I send **20** concurrent requests each with length of **1k tokens** and I set the **stream to True** and **max_tokens to 1024.** \r\n\r\n\r\nThis is how I start the server:\r\n`python -m sglang.launch_server --model-path NousResearch/Meta-Llama-3-8B-Instruct  --host 0.0.0.0  --port 8000 --context-length 4096 --dtype bfloat16  --chat-template llama-3`\r\n\r\nI add llama-3 template in conversaitons.py as they are present in conversions file of FASTCHAT.\r\n\r\n>> **Note: when I send this to VLLM entrypoint, it works without OOM error!**\r\n\r\n\r\n**Error:**\r\n\r\n```\r\nINFO:   - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\r\n[gpu=0] Prefill batch. #new-seq: 17, #new-token: 13938, #cached-token: 3260, cache hit rate: 17.86%, #running-req: 1, #queue-req: 2\r\nException in ModelTpServer:\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/sglang/python/sglang/srt/managers/tp_worker.py\", line 219, in exposed_step\r\n    self.forward_step()\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/ubuntu/sglang/python/sglang/srt/managers/tp_worker.py\", line 235, in forward_step\r\n    self.forward_prefill_batch(new_batch)\r\n  File \"/home/ubuntu/sglang/python/sglang/srt/managers/tp_worker.py\", line 545, in forward_prefill_batch\r\n    output = self.model_runner.forward(batch, ForwardMode.EXTEND)\r\n  File \"/home/ubuntu/sglang/python/sglang/srt/model_executor/model_runner.py\", line 388, in forward\r\n    return self.forward_extend(batch)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/ubuntu/sglang/python/sglang/srt/model_executor/model_runner.py\", line 356, in forward_extend\r\n    return self.model.forward(\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/ubuntu/sglang/python/sglang/srt/models/llama2.py\", line 314, in forward\r\n    hidden_states = self.model(input_ids, positions, input_metadata, input_embeds)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/ubuntu/sglang/python/sglang/srt/models/llama2.py\", line 281, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/ubuntu/sglang/python/sglang/srt/models/llama2.py\", line 239, in forward\r\n    hidden_states = self.mlp(hidden_states)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/ubuntu/sglang/python/sglang/srt/models/llama2.py\", line 79, in forward\r\n    gate_up, _ = self.gate_up_proj(x)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/vllm/model_executor/layers/linear.py\", line 330, in forward\r\n    output_parallel = self.quant_method.apply(self, input_, bias)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/vllm/model_executor/layers/linear.py\", line 122, in apply\r\n    return F.linear(x, layer.weight, bias)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 764.00 MiB. GPU \r\n```\n\n### Reproduction\n\nSame as describe the bug section \n\n### Environment\n\n```\r\n$ python -m sglang.check_env\r\n\r\nPython: 3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA A10G\r\nCUDA_HOME: None\r\nPyTorch: 2.3.1+cu121\r\nsglang: 0.2.9.post1\r\nflashinfer: 0.1.3+cu121torch2.3\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.26.4\r\naiohttp: 3.10.0\r\nfastapi: 0.112.0\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.5\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.5\r\nuvloop: 0.19.0\r\nzmq: 26.1.0\r\nvllm: 0.5.3.post1\r\nmultipart: 0.0.9\r\nopenai: 1.38.0\r\nanthropic: 0.32.0\r\nNVIDIA Topology: \r\n\tGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \t0-15\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nulimit soft: 1024\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-08-11T10:51:49+00:00",
    "closed_at": "2024-09-22T13:00:44+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1030/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1030"
  },
  {
    "number": 7951,
    "title": "[Bug] Tensor shape is wrong when cudagraph+enable_dp_attention",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI tried to run DSR1 fp4 model on 8xB200, but found that some issue when I opened cudagraph and attndp, the input tensor dimension for each MoE layer is padded to global bs. For example, I take global bs 4096 and attention dp 8, which each rank should have 512 reqs for decode and the input tensor M dimension should be 512 for local rank. \nBut I tried to do some profiling, I found that when cudagraph is on, each rank has input M dim 4096, not 512. When cudagraph is off, each rank has input M dim 512 which looks good.\nIs this known or a bug?\nWithout cudagraph\n\n<img width=\"612\" height=\"260\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/008683e1-9892-4ae9-abb8-6546b0dd54cb\" />\n\nWith cudagraph\n\n<img width=\"592\" height=\"316\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/095ab03b-dd1a-4d1e-b4af-1f14d93b6be2\" />\n\n### Reproduction\n\n**Server:**\npython3 -m sglang.launch_server \\\n--model-path nvidia/DeepSeek-R1-0528-FP4 \\\n--trust-remote-code \\\n--quantization modelopt_fp4 \\\n--dp-size 8 --enable-dp-attention --enable-dp-lm-head\\\n--tp-size 8 \\\n--attention-backend cutlass_mla \\\n--enable-ep-moe \\\n--enable-flashinfer-moe \\\n--cuda-graph-bs 1 2 4 8 16 32 64 128 256 512 1024 2048 4096 \\\n--chunked-prefill-size 16384 \\\n--mem-fraction-static 0.85 \\\n--max-running-requests 4096 \\\n--stream-interval 5 \n**Client:**\nbenchmark_serving.py with isl/osl 1024/1024, concurrency 4096.\n\n### Environment\n\nlatest main.",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "open",
    "created_at": "2025-07-11T10:58:55+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7951/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7951"
  },
  {
    "number": 4935,
    "title": "[Bug] 0.0.0.0 host not supported",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nif we set the host as 0.0.0.0, the warmup function will fail.\n\n<b>Connection to 0.0.0.0 failed.</b></p>\\n</blockquote>\\n\\n<p id=\"sysmsg\">The system returned: <i>(111) Connection refused</I>\n\n_wait_and_warmup -> res = requests.get(url + \"/get_model_info\", timeout=5, headers=headers)\n\ncan we add a fix here to replace 0.0.0.0 with 127.0.0.1 for warmup check? while we still let the server to be deployed in 0.0.0.0\n\n### Reproduction\n\n`--host 0.0.0.0` \n\n### Environment\n\n```\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda-12\nNVCC: Cuda compilation tools, release 12.6, V12.6.68\nCUDA Driver Version: 535.129.03\nPyTorch: 2.5.1+cu124\nsglang: 0.4.4.post3\nsgl_kernel: 0.0.5.post4\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.50.0\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.14\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.3\ninteregular: 0.3.3\nmodelscope: 1.24.1\norjson: 3.9.10\noutlines: 0.1.11\npackaging: 24.2\npsutil: 5.9.8\npydantic: 2.10.6\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.8.1\nxgrammar: 0.1.17\nopenai: 1.69.0\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.65.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    0-47,96-143     0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    0-47,96-143     0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    0-47,96-143     0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    0-47,96-143     0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    48-95,144-191   1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    48-95,144-191   1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    48-95,144-191   1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      48-95,144-191   1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nHypervisor vendor: KVM\nulimit soft: 1048576\n```",
    "labels": [
      "bug",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-30T22:04:01+00:00",
    "closed_at": "2025-05-30T08:43:39+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4935/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4935"
  },
  {
    "number": 1279,
    "title": "[Bug] device-side assert triggered when using run_batch",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThe following error is raised when ever i run run_batch:\r\n\r\n```\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1193,0,0], thread: [124,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1193,0,0], thread: [125,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1193,0,0], thread: [126,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1193,0,0], thread: [127,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n[01:37:48 TP0] Exception in ModelTpServer:\r\nTraceback (most recent call last):\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 234, in exposed_step\r\n    self.forward_step()\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 250, in forward_step\r\n    self.forward_prefill_batch(new_batch)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 489, in forward_prefill_batch\r\n    sample_output, logits_output = self.model_runner.forward(\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 579, in forward\r\n    return self.forward_extend(batch)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 543, in forward_extend\r\n    return self.model.forward(\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/models/gemma.py\", line 302, in forward\r\n    logits_output = self.logits_processor(\r\n                    ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/layers/logits_processor.py\", line 268, in forward\r\n    torch.cat([pruned_input_ids[1:], torch.tensor([0], device=\"cuda\")]),\r\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n\r\n[01:37:48 TP0] Exception in ControllerSingle:\r\nTraceback (most recent call last):\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/managers/controller_single.py\", line 165, in start_controller_process\r\n    controller.loop_for_forward()\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/managers/controller_single.py\", line 102, in loop_for_forward\r\n    out_pyobjs = self.tp_server.exposed_step(recv_reqs)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 234, in exposed_step\r\n    self.forward_step()\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 250, in forward_step\r\n    self.forward_prefill_batch(new_batch)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 489, in forward_prefill_batch\r\n    sample_output, logits_output = self.model_runner.forward(\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 579, in forward\r\n    return self.forward_extend(batch)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 543, in forward_extend\r\n    return self.model.forward(\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/models/gemma.py\", line 302, in forward\r\n    logits_output = self.logits_processor(\r\n                    ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/layers/logits_processor.py\", line 268, in forward\r\n    torch.cat([pruned_input_ids[1:], torch.tensor([0], device=\"cuda\")]),\r\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n```\n\n### Reproduction\n\n```bash\r\npython -m sglang.launch_server --model-path google/gemma-2b-it --port 30000 --tp 1 --context-length 3000 --max-running-requests 30\r\n```\r\n\r\n```python\r\n@function\r\ndef tool_use(s, question):\r\n    s += \"To answer this question: \" + question + \". \"\r\n    s += \"I need to use a \" + sgl.gen(\"tool\", choices=[\"calculator\", \"search engine\"]) + \". \"\r\n\r\n    if s[\"tool\"] == \"calculator\":\r\n        s += \"The math expression is\" + sgl.gen(\"expression\")\r\n\r\n\r\nstate = tool_use.run_batch(\r\n  [{\"question\":\"What is 50 times 80\"}]*100,\r\n  progress_bar=True,\r\n  num_threads=3,\r\n)\r\n```\n\n### Environment\n\nPython: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA A100 80GB PCIe\r\nGPU 0 Compute Capability: 8.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\r\nCUDA Driver Version: 535.54.03\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.2.14\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.44.2\r\nrequests: 2.31.0\r\ntqdm: 4.65.0\r\nnumpy: 1.23.5\r\naiohttp: 3.8.5\r\nfastapi: 0.112.2\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.23.4\r\ninteregular: 0.3.3\r\npackaging: 23.2\r\nPIL: 9.4.0\r\npsutil: 5.9.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.6\r\nuvloop: 0.20.0\r\nzmq: 23.2.0\r\nvllm: 0.5.5\r\nmultipart: 0.0.9\r\nopenai: 1.43.0\r\nanthropic: 0.34.1\r\nNVIDIA Topology: \r\n\tGPU0\tNIC0\tNIC1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tSYS\tSYS\t0-23\t\tN/A\t\tN/A\r\nNIC0\tSYS\t X \tSYS\t\t\t\t\r\nNIC1\tSYS\tSYS\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n\r\n\r\nHypervisor vendor: Microsoft\r\nulimit soft: 1000000",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-09-01T01:51:32+00:00",
    "closed_at": "2024-09-03T13:02:03+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1279/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1279"
  },
  {
    "number": 3538,
    "title": "[Bug]NCCL error if enable the cuda graph",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n<img width=\"1663\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e3b396cc-4771-474d-8843-d43d8d5dbf90\" />\n\nIf I don't disable cuda graph, I will get the error shown in the picture when the cuda graph is being inited. If i use the official docker image, i will not get the error. The only difference of the environment with the docker is the sglang by observing the output of `python3 -m sglang.check_env`. I install sglang via pip and i have observed the sglang of docker image is installed from local.\n\n### Reproduction\n\n```bash\npython3 -m sglang.launch_server --model-path deepseekr1 --tp 16 --dist-init-addr 29.111.44.27:20000 --nnodes 2 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 8000\n```\nwill reproduct the error above.\nadd the option `--disable-cuda-graph` will run well\n\n### Environment\n\n```\nINFO 02-13 14:28:55 __init__.py:190] Automatically detected platform cuda.\nPython: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.161.08\nPyTorch: 2.5.1+cu124\nsglang: 0.4.2.post4\nsgl_kernel: 0.0.3.post3\nflashinfer: 0.2.0.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.61.1\ntiktoken: 0.8.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10   NIC11   NIC12   NIC13   NIC14   NIC15   NIC16NIC17    NIC18   NIC19   NIC20   NIC21   NIC22   NIC23   NIC24   NIC25   NIC26   NIC27   NIC28   NIC29   NIC30   NIC31   NIC32   NIC33   NIC34   NIC35   NIC36   NIC37   NIC38   NIC39   NIC40   NIC41   CPU Affinity  NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-95,192-287  0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    PHB     PIX     SYS     SYS     SYS     SYS     0-95,192-287  0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    PIX     PHB     SYS     SYS     SYS     SYS     0-95,192-287  0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-95,192-287  0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    96-191,288-383        1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    96-191,288-383        1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     PHB     NODE    NODE    PIX     96-191,288-383        1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     NODE    NODE    PHB     96-191,288-383        1               N/A\nNIC0    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE     X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC1    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC2    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC3    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC4    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC9    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC10   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC11   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC12   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC13   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC14   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC15   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC16   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X   PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC17   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX   X       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC18   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX       X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC19   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC20   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC21   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC22   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC23   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC24   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC25   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC26   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC27   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC28   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC29   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC30   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC31   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC32   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC33   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC34   PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC35   NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     SYS     SYS     SYS\nNIC36   NODE    PHB     PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      PHB     SYS     SYS     SYS     SYS\nNIC37   NODE    PIX     PHB     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    PHB      X      SYS     SYS     SYS     SYS\nNIC38   SYS     SYS     SYS     SYS     NODE    NODE    PHB     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    PHB\nNIC39   SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE\nNIC40   SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE\nNIC41   SYS     SYS     SYS     SYS     NODE    NODE    PIX     PHB     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     PHB     NODE    NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0:         NIC41   CPU Affinity    NUMA Affinity   GPU NUMA ID\n  NIC1: MA ID\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n  NIC10: mlx5_10\n  NIC11: mlx5_11\n  NIC12: mlx5_12\n  NIC13: mlx5_13\n  NIC14: mlx5_14\n  NIC15: mlx5_15\n  NIC16: mlx5_16\n  NIC17: mlx5_17\n  NIC18: mlx5_18\n  NIC19: mlx5_19\n  NIC20: mlx5_20\n  NIC21: mlx5_21\n  NIC22: mlx5_22\n  NIC23: mlx5_23\n  NIC24: mlx5_24\n  NIC25: mlx5_25\n  NIC26: mlx5_26\n  NIC27: mlx5_27\n  NIC28: mlx5_28\n  NIC29: mlx5_29\n  NIC30: mlx5_30\n  NIC31: mlx5_31\n  NIC32: mlx5_32\n  NIC33: mlx5_33\n  NIC34: mlx5_bond_1\n  NIC35: mlx5_bond_2\n  NIC36: mlx5_bond_3\n  NIC37: mlx5_bond_4\n  NIC38: mlx5_bond_5\n  NIC39: mlx5_bond_6\n  NIC40: mlx5_bond_7\n  NIC41: mlx5_bond_8\n\n\nulimit soft: 1000000\n```",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-02-13T06:38:16+00:00",
    "closed_at": "2025-02-19T14:35:47+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3538/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3538"
  }
]
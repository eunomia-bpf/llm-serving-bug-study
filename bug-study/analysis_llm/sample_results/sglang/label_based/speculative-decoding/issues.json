[
  {
    "number": 7263,
    "title": "[Bug] Error when running Qwen2 EAGLE spec decoding with the official OFFLINE inference example",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHi, I am running eagle offline speculative decoding example (examples/runtime/engine/offline_batch_inference_eagle.py), and I encountered an error as shown below. Specifically, I modified the target model to Qwen/Qwen2-7B-Instruct and draft model to yuhuili/EAGLE-Qwen2-7B-Instruct.\nI did notice there was a previous [issue](https://github.com/sgl-project/sglang/issues/3315) report similar problem, but in the sgl server mode. Could you please look into this offline inference bug? Thanks!\n\n## The error encountered is as follow: (similar to the previous issue)\nCapturing batches (avail_mem=11.24 GB): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:03<00:00,  2.59it/s]\nLoading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n[2025-06-17 05:21:52] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/home/ubuntu/sglang/python/sglang/srt/managers/scheduler.py\", line 2490, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, pp_rank, dp_rank)\n  File \"/home/ubuntu/sglang/python/sglang/srt/managers/scheduler.py\", line 295, in __init__\n    self.draft_worker = EAGLEWorker(\n  File \"/home/ubuntu/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 113, in __init__\n    super().__init__(\n  File \"/home/ubuntu/sglang/python/sglang/srt/managers/tp_worker.py\", line 78, in __init__\n    self.model_runner = ModelRunner(\n  File \"/home/ubuntu/sglang/python/sglang/srt/model_executor/model_runner.py\", line 212, in __init__\n    self.initialize(min_per_gpu_memory)\n  File \"/home/ubuntu/sglang/python/sglang/srt/model_executor/model_runner.py\", line 253, in initialize\n    self.load_model()\n  File \"/home/ubuntu/sglang/python/sglang/srt/model_executor/model_runner.py\", line 547, in load_model\n    self.model = get_model(\n  File \"/home/ubuntu/sglang/python/sglang/srt/model_loader/__init__.py\", line 22, in get_model\n    return loader.load_model(\n  File \"/home/ubuntu/sglang/python/sglang/srt/model_loader/loader.py\", line 381, in load_model\n    self.load_weights_and_postprocess(\n  File \"/home/ubuntu/sglang/python/sglang/srt/model_loader/loader.py\", line 389, in load_weights_and_postprocess\n    model.load_weights(weights)\n  File \"/home/ubuntu/sglang/python/sglang/srt/models/qwen2.py\", line 513, in load_weights\n    param = params_dict[name]\nKeyError: 'layers.0.self_attn.qkv_proj.weight'\n\nLoading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n\n\n\n\n### Reproduction\n\n## Scripts and reproduction:\nscripts: spec.py (modified from examples/runtime/engine/offline_batch_inference_eagle.py with qwen model)\n```\nimport sglang as sgl\ndef main():\n    # Sample prompts.\n    prompts = [\n        \"Hello, my name is\",\n        \"The president of the United States is\",\n        \"The capital of France is\",\n        \"The future of AI is\",\n    ]\n\n    # Create a sampling params object.\n    sampling_params = {\"temperature\": 0, \"max_new_tokens\": 30}\n\n    # Create an LLM.\n    llm = sgl.Engine(\n        # model_path=\"meta-llama/Llama-2-7b-chat-hf\",\n        model_path=\"Qwen/Qwen2-7B-Instruct\",\n        speculative_algorithm=\"EAGLE\",\n        # speculative_draft_model_path=\"lmsys/sglang-EAGLE-llama2-chat-7B\",\n        speculative_draft_model_path=\"yuhuili/EAGLE-Qwen2-7B-Instruct\",\n        speculative_num_steps=3,\n        speculative_eagle_topk=4,\n        speculative_num_draft_tokens=16,\n        cuda_graph_max_bs=8,\n    )\n\n    outputs = llm.generate(prompts, sampling_params)\n\n    # Print the outputs.\n    for prompt, output in zip(prompts, outputs):\n        print(\"===============================\")\n        print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")\n\n\n# The __main__ condition is necessary here because we use \"spawn\" to create subprocesses\n# Spawn starts a fresh program every time, if there is no __main__, it will run into infinite loop to keep spawning processes from sgl.Engine\nif __name__ == \"__main__\":\n    main()\n```\nto run the script: ``python spec.py``\n\n### Environment\n\n\n## environment:\naccelerate                1.7.0\naiohappyeyeballs          2.6.1\naiohttp                   3.12.12\naiosignal                 1.3.2\nairportsdata              20250523\nannotated-types           0.7.0\nanthropic                 0.53.0\nantlr4-python3-runtime    4.9.3\nanyio                     4.9.0\nasttokens                 3.0.0\nasync-timeout             5.0.1\nattrs                     25.3.0\nblobfile                  3.0.0\ncachetools                6.0.0\ncertifi                   2025.4.26\ncffi                      1.17.1\ncharset-normalizer        3.4.2\nclick                     8.2.1\ncloudpickle               3.1.1\ncodetiming                1.4.0\ncompressed-tensors        0.10.1\ncuda-bindings             12.9.0\ncuda-python               12.9.0\ndatasets                  3.6.0\ndecorator                 5.2.1\ndecord                    0.6.0\ndill                      0.3.8\ndiskcache                 5.6.3\ndistro                    1.9.0\neinops                    0.8.1\nexceptiongroup            1.3.0\nexecuting                 2.2.0\nfastapi                   0.115.12\nfilelock                  3.18.0\nflash_attn                2.7.4.post1\nflashinfer-python         0.2.6.post1\nfrozenlist                1.7.0\nfsspec                    2025.3.0\ngitdb                     4.0.12\nGitPython                 3.1.44\nh11                       0.16.0\nhf_transfer               0.1.9\nhf-xet                    1.1.3\nhttpcore                  1.0.9\nhttpx                     0.28.1\nhuggingface-hub           0.32.5\nhydra-core                1.3.2\nidna                      3.10\nimportlib_metadata        8.7.0\ninteregular               0.3.3\nipython                   8.37.0\njedi                      0.19.2\nJinja2                    3.1.6\njiter                     0.10.0\njsonschema                4.24.0\njsonschema-specifications 2025.4.1\nlark                      1.2.2\nlitellm                   1.72.3\nllguidance                0.7.29\nlxml                      5.4.0\nMarkupSafe                3.0.2\nmatplotlib-inline         0.1.7\nmodelscope                1.26.0\nmpmath                    1.3.0\nmsgpack                   1.1.0\nmsgspec                   0.19.0\nmultidict                 6.4.4\nmultiprocess              0.70.16\nnest-asyncio              1.6.0\nnetworkx                  3.4.2\nninja                     1.11.1.4\nnumpy                     2.2.6\nnvidia-cublas-cu12        12.6.4.1\nnvidia-cuda-cupti-cu12    12.6.80\nnvidia-cuda-nvrtc-cu12    12.6.77\nnvidia-cuda-runtime-cu12  12.6.77\nnvidia-cudnn-cu12         9.5.1.17\nnvidia-cufft-cu12         11.3.0.4\nnvidia-cufile-cu12        1.11.1.6\nnvidia-curand-cu12        10.3.7.77\nnvidia-cusolver-cu12      11.7.1.2\nnvidia-cusparse-cu12      12.5.4.2\nnvidia-cusparselt-cu12    0.6.3\nnvidia-ml-py              12.575.51\nnvidia-nccl-cu12          2.26.2\nnvidia-nvjitlink-cu12     12.6.85\nnvidia-nvtx-cu12          12.6.77\nomegaconf                 2.3.0\nopenai                    1.86.0\norjson                    3.10.18\noutlines                  0.1.11\noutlines_core             0.1.26\npackaging                 25.0\npandas                    2.3.0\nparso                     0.8.4\npartial-json-parser       0.2.1.1.post5\npeft                      0.15.2\npexpect                   4.9.0\npillow                    11.2.1\npip                       25.1\nplatformdirs              4.3.8\nprometheus_client         0.22.1\nprompt_toolkit            3.0.51\npropcache                 0.3.2\nprotobuf                  6.31.1\npsutil                    7.0.0\nptyprocess                0.7.0\npure_eval                 0.2.3\npyarrow                   20.0.0\npycountry                 24.6.1\npycparser                 2.22\npycryptodomex             3.23.0\npydantic                  2.11.5\npydantic_core             2.33.2\nPygments                  2.19.1\npynvml                    12.0.0\npython-dateutil           2.9.0.post0\npython-dotenv             1.1.0\npython-multipart          0.0.20\npytz                      2025.2\nPyYAML                    6.0.2\npyzmq                     26.4.0\nray                       2.46.0\nreferencing               0.36.2\nregex                     2024.11.6\nrequests                  2.32.4\nrpds-py                   0.25.1\nsafetensors               0.5.3\nscipy                     1.15.3\nsentencepiece             0.2.0\nsentry-sdk                2.29.1\nsetproctitle              1.3.6\nsetuptools                78.1.1\nsgl-kernel                0.1.7\nsglang                    0.4.7         /home/ubuntu/sglang/python\nsix                       1.17.0\nsmmap                     5.0.2\nsniffio                   1.3.1\nsoundfile                 0.13.1\nstack-data                0.6.3\nstarlette                 0.46.2\nsympy                     1.14.0\ntensordict                0.8.3\ntiktoken                  0.9.0\ntokenizers                0.21.1\ntorch                     2.7.1\ntorch_memory_saver        0.0.6\ntorchao                   0.9.0\ntorchaudio                2.7.1\ntorchdata                 0.11.0\ntorchvision               0.22.1\ntqdm                      4.67.1\ntraitlets                 5.14.3\ntransformers              4.52.3\ntriton                    3.3.1\ntyping_extensions         4.14.0\ntyping-inspection         0.4.1\ntzdata                    2025.2\nurllib3                   2.4.0\nuvicorn                   0.34.3\nuvloop                    0.21.0\nwandb                     0.20.1\nwcwidth                   0.2.13\nwheel                     0.45.1\nxgrammar                  0.1.19\nxxhash                    3.5.0\nyarl                      1.20.1\nzipp                      3.23.0",
    "labels": [
      "speculative-decoding"
    ],
    "state": "open",
    "created_at": "2025-06-17T05:37:21+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7263/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7263"
  },
  {
    "number": 7185,
    "title": "[Feature] Support EAGLE3 for Llama4",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nSupport EAGLE3 model for Llama4, e.g. https://huggingface.co/nvidia/Llama-4-Maverick-17B-128E-Eagle3.\n\n### Related resources\n\n_No response_",
    "labels": [
      "speculative-decoding"
    ],
    "state": "open",
    "created_at": "2025-06-14T15:50:19+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7185/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7185"
  },
  {
    "number": 7139,
    "title": "[Bug] Run eagle3 failed",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1500: indexSelectSmallIndex: block: [32,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n[2025-06-13 09:59:00] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/hy-tmp/sglang-0.4.7/python/sglang/srt/managers/scheduler.py\", line 2506, in run_scheduler_process\n    scheduler.event_loop_normal()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/hy-tmp/sglang-0.4.7/python/sglang/srt/managers/scheduler.py\", line 664, in event_loop_normal\n    result = self.run_batch(batch)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/hy-tmp/sglang-0.4.7/python/sglang/srt/managers/scheduler.py\", line 1577, in run_batch\n    ) = self.draft_worker.forward_batch_speculative_generation(batch)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/hy-tmp/sglang-0.4.7/python/sglang/srt/speculative/eagle_worker.py\", line 295, in forward_batch_speculative_generation\n    spec_info = self.draft(batch)\n                ^^^^^^^^^^^^^^^^^\n  File \"/hy-tmp/sglang-0.4.7/python/sglang/srt/speculative/eagle_worker.py\", line 455, in draft\n    score_list, token_list, parents_list = self.draft_forward(forward_batch)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/hy-tmp/sglang-0.4.7/python/sglang/srt/speculative/eagle_worker.py\", line 536, in draft_forward\n    logits_output = self.draft_model_runner.model.forward(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/hy-tmp/sglang-0.4.7/python/sglang/srt/models/llama.py\", line 457, in forward\n    hidden_states = self.model(\n                    ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/hy-tmp/sglang-0.4.7/python/sglang/srt/models/llama_eagle3.py\", line 125, in forward\n    embeds = self.embed_tokens(input_ids)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/hy-tmp/sglang-0.4.7/python/sglang/srt/layers/vocab_parallel_embedding.py\", line 490, in forward\n    output_parallel = self.quant_method.embedding(self, masked_input.long())\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/hy-tmp/sglang-0.4.7/python/sglang/srt/layers/vocab_parallel_embedding.py\", line 63, in embedding\n    return F.embedding(input_, layer.weight)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2551, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n### Reproduction\n\nif __name__ == '__main__':\n    llm = sgl.Engine(model_path=\"/hy-tmp/EAGLE/Vicuna-13B-v1.3\", speculative_draft_model_path=\"/hy-tmp/EAGLE/weight/EAGLE3-Vicuna13B-SGLang\",\n                     speculative_algorithm=\"EAGLE3\", speculative_num_steps=5, speculative_eagle_topk=8, speculative_num_draft_tokens=32,\n                     mem_fraction_static=0.7, disable_cuda_graph=True, dtype=\"float16\")\n\n    prompts = [\n        \"Hello, my name is\",\n        # \"The president of the United States is\",\n        # \"The capital of France is\",\n        # \"The future of AI is\",\n        ]\n\n    sampling_params = {\"temperature\": 0.8, \"top_p\": 1.00}\n\n    outputs = llm.generate(prompts, sampling_params)\n    for prompt, output in zip(prompts, outputs):\n        print(\"===============================\")\n        print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")\n\n### Environment\n\nA800,Ubuntu",
    "labels": [
      "speculative-decoding"
    ],
    "state": "open",
    "created_at": "2025-06-13T02:05:16+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7139/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7139"
  },
  {
    "number": 7130,
    "title": "[Bug] token_to_kv_pool_allocator memory leak detected! when page_size>1 and use MTP",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nIn version v0.4.6.post5, if I deploy the model using MTP and set page_size greater than 1, an error will occur during inference: token_to_kv_pool_allocator memory leak detected!\n\nIt seems that this is a common problem, which should be related to the operation process of eagle_worker. There is a similar bug report, not sure where the problem is: \n[https://github.com/sgl-project/sglang/issues/6888](url)\n\n### Reproduction\n\nSGL_ENABLE_JIT_DEEPGEMM=1 \\\npython -m sglang.launch_server --model-path DeepSeek-V3-0324 \\\n    --tp-size 8 \\\n    --host 0.0.0.0 \\\n    --port 8100 \\\n    --mem-fraction-static 0.8 \\\n    --max-running-requests 96 \\\n    --trust-remote-code \\\n    --enable-cache-report \\\n    --log-level info \\\n    --chunked-prefill-size 16384 \\\n    --context-length 65536 \\\n    --attention-backend fa3 \\\n    --page-size 16 \\\n    --speculative-algorithm EAGLE \\\n    --speculative-draft-model-path NEXTN-MODEL \\\n    --speculative-num-steps 5 \\\n    --speculative-eagle-topk 1 \\\n    --speculative-num-draft-tokens 6\n\n### Environment\n\nPython: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA L20X\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.8, V12.8.93\nCUDA Driver Version: 550.127.08\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post5\nsgl_kernel: 0.1.4\nflashinfer_python: 0.2.5\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.10.10\nfastapi: 0.115.4\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.11\noutlines: 0.1.11\npackaging: 24.1\npsutil: 6.1.0\npydantic: 2.10.6\npython-multipart: 0.0.17\npyzmq: 26.2.0\nuvicorn: 0.32.0\nuvloop: 0.21.0\nvllm: 0.7.2\nxgrammar: 0.1.19\nopenai: 1.75.0\ntiktoken: 0.7.0\nanthropic: 0.49.0\nlitellm: 1.67.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    48-95,144-191   1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    48-95,144-191   1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    48-95,144-191   1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     48-95,144-191   1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC1    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     SYS     SYS     SYS\nNIC2    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS     SYS     SYS     SYS\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_bond_0\n  NIC1: mlx5_bond_1\n  NIC2: mlx5_bond_2\n  NIC3: mlx5_bond_3\n  NIC4: mlx5_bond_4\n  NIC5: mlx5_bond_5\n  NIC6: mlx5_bond_6\n  NIC7: mlx5_bond_7\n\n\nulimit soft: 1048576",
    "labels": [
      "speculative-decoding"
    ],
    "state": "open",
    "created_at": "2025-06-12T11:13:56+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7130/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7130"
  },
  {
    "number": 7111,
    "title": "[Bug] Eagle memory leak",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n![Image](https://github.com/user-attachments/assets/6708fe70-a919-49ce-a338-e8318a2d34a0)\n\n### Reproduction\n\nCUDA_VISIBLE_DEVICES=1 python -m sglang.launch_server --model-path /local/path/qwen2.5-7b-instruct --host 0.0.0.0 --port 6800 --speculative-algorithm EAGLE --speculative-draft-model-path /localpath/qwen2.5-7b-instruct-draft/checkpoint-41775 --speculative-num-steps 5 --speculative-eagle-topk 4 --speculative-num-draft-tokens 32 --mem-fraction-static 0.75\n\nThe target model and eagle model are both fintuned on [dataset](https://huggingface.co/datasets/HuggingFaceH4/Bespoke-Stratos-17k)\nAnd then, I use evalscope judge the model's performance by using follow parameters:\n- concurrent: 32\n- max_len : 16384\n- top_p: 0.8\n- temperature: 0.7 \n\nAfter a while, the error occurred\n\n\n### Environment\n\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA A800 80GB PCIe\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.54.15\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post5\nsgl_kernel: 0.1.4\nflashinfer_python: 0.2.5\ntriton: 3.2.0\ntransformers: 4.51.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.9.5\nfastapi: 0.115.12\nhf_transfer: 0.1.8\nhuggingface_hub: 0.32.2\ninteregular: 0.3.3\nmodelscope: 1.26.0\norjson: 3.10.13\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.11.5\npython-multipart: 0.0.20\npyzmq: 26.0.3\nuvicorn: 0.30.6\nuvloop: 0.20.0\nvllm: 0.9.0.1\nxgrammar: 0.1.19\nopenai: 1.55.3\ntiktoken: 0.7.0\nanthropic: 0.42.0\nlitellm: 1.56.5\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PIX     PIX     PIX     SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\nGPU1    PIX      X      PIX     PIX     SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\nGPU2    PIX     PIX      X      PIX     SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\nGPU3    PIX     PIX     PIX      X      SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\nGPU4    SYS     SYS     SYS     SYS      X      PIX     PIX     PIX     28-55,84-111    1               N/A\nGPU5    SYS     SYS     SYS     SYS     PIX      X      PIX     PIX     28-55,84-111    1               N/A\nGPU6    SYS     SYS     SYS     SYS     PIX     PIX      X      PIX     28-55,84-111    1               N/A\nGPU7    SYS     SYS     SYS     SYS     PIX     PIX     PIX      X      28-55,84-111    1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576",
    "labels": [
      "speculative-decoding"
    ],
    "state": "open",
    "created_at": "2025-06-12T03:01:47+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7111/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7111"
  },
  {
    "number": 7077,
    "title": "[Feature] integrate MTP with some new features",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n- [x] compatibility with dp attention #6081 \n- [x] compatibility with eplb\n- [x] compatibility with `enable-dp-lm-head`\n- [x] compatibility with pd disaggregation @Atream #7242 \n- [x] compatibility with two-batch-overlap @Qiaolin-Yu #7225 \n- [x] compatibility with deepep #7206 \n...\n\n### Related resources\n\nhttps://github.com/sgl-project/sglang/issues/6017\nhttps://lmsys.org/blog/2025-05-05-large-scale-ep/#large-scale-expert-parallelism",
    "labels": [
      "high priority",
      "collaboration",
      "deepseek",
      "speculative-decoding"
    ],
    "state": "open",
    "created_at": "2025-06-11T03:33:03+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7077/reactions",
      "total_count": 7,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7077"
  },
  {
    "number": 7050,
    "title": "[Feature] Support Eagle Utils using Triton.",
    "body": "\n\n### Motivation\n\nThese kernels are only implemented in CUDA, which leads to some eagle cases currently unable to run on AMD. We need to support the kernels of PyTorch or Triton.\n```\n    from sgl_kernel import (\n        top_k_renorm_prob,\n        top_p_renorm_prob,\n        tree_speculative_sampling_target_only,\n        verify_tree_greedy,\n    )\n```",
    "labels": [
      "speculative-decoding"
    ],
    "state": "open",
    "created_at": "2025-06-10T10:33:27+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7050/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7050"
  },
  {
    "number": 6949,
    "title": "[Bug] Performance Regression: Eagle Speculative Decoding Causes Significant Inference Slowdown",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\ntarget model : qwq-32b\n\nwithout eagle:\n```\n[2025-06-07 21:25:14 TP0] Decode batch. #running-req: 1, #token: 9322, token usage: 0.00, gen throughput (token/s): 97.52, #queue-req: 0\n[2025-06-07 21:25:14 TP0] Decode batch. #running-req: 1, #token: 9362, token usage: 0.00, gen throughput (token/s): 97.58, #queue-req: 0\n[2025-06-07 21:25:15 TP0] Decode batch. #running-req: 1, #token: 9402, token usage: 0.00, gen throughput (token/s): 97.63, #queue-req: 0\n[2025-06-07 21:25:15 TP0] Decode batch. #running-req: 1, #token: 9442, token usage: 0.00, gen throughput (token/s): 97.50, #queue-req: 0\n[2025-06-07 21:25:15 TP0] Decode batch. #running-req: 1, #token: 9482, token usage: 0.00, gen throughput (token/s): 97.55, #queue-req: 0\n[2025-06-07 21:25:16 TP0] Decode batch. #running-req: 1, #token: 9522, token usage: 0.00, gen throughput (token/s): 97.54, #queue-req: 0\n[2025-06-07 21:25:16 TP0] Decode batch. #running-req: 1, #token: 9562, token usage: 0.00, gen throughput (token/s): 97.61, #queue-req: 0\n[2025-06-07 21:25:17 TP0] Decode batch. #running-req: 1, #token: 9602, token usage: 0.00, gen throughput (token/s): 97.57, #queue-req: 0\n[2025-06-07 21:25:17 TP0] Decode batch. #running-req: 1, #token: 9642, token usage: 0.00, gen throughput (token/s): 97.54, #queue-req: 0\n[2025-06-07 21:25:17 TP0] Decode batch. #running-req: 1, #token: 9682, token usage: 0.00, gen throughput (token/s): 97.53, #queue-req: 0\n[2025-06-07 21:25:18 TP0] Decode batch. #running-req: 1, #token: 9722, token usage: 0.00, gen throughput (token/s): 97.55, #queue-req: 0\n[2025-06-07 21:25:18 TP0] Decode batch. #running-req: 1, #token: 9762, token usage: 0.00, gen throughput (token/s): 97.50, #queue-req: 0\n```\nwith eagle\n```\n[2025-06-07 21:21:09 TP0] Decode batch. #running-req: 1, #token: 9448, token usage: 0.00, accept len: 2.38, gen throughput (token/s): 79.63, #queue-req: 0\n[2025-06-07 21:21:11 TP0] Decode batch. #running-req: 1, #token: 9537, token usage: 0.00, accept len: 2.23, gen throughput (token/s): 77.67, #queue-req: 0\n[2025-06-07 21:21:12 TP0] Decode batch. #running-req: 1, #token: 9635, token usage: 0.00, accept len: 2.45, gen throughput (token/s): 84.50, #queue-req: 0\n[2025-06-07 21:21:13 TP0] Decode batch. #running-req: 1, #token: 9718, token usage: 0.00, accept len: 2.08, gen throughput (token/s): 72.10, #queue-req: 0\n[2025-06-07 21:21:14 TP0] Decode batch. #running-req: 1, #token: 9815, token usage: 0.00, accept len: 2.42, gen throughput (token/s): 82.95, #queue-req: 0\n[2025-06-07 21:21:15 TP0] Decode batch. #running-req: 1, #token: 9890, token usage: 0.00, accept len: 1.88, gen throughput (token/s): 65.86, #queue-req: 0\n[2025-06-07 21:21:16 TP0] Decode batch. #running-req: 1, #token: 9979, token usage: 0.00, accept len: 2.23, gen throughput (token/s): 76.11, #queue-req: 0\n\n### Reproduction\n\npython -m sglang.launch_server --model-path {model_path} \\\n--trust-remote-code \\\n--tp 8 \\\n--enable-cache-report \\\n--log-level info \\\n--mem-fraction-static 0.8 \\\n--host 0.0.0.0 \\\n--port 9122 \\\n--speculative-algo EAGLE \\\n--speculative-draft {eagle_path} \\\n--speculative-num-steps 3 \\\n--speculative-eagle-topk 4 \\\n--speculative-num-draft-tokens 5\n\n### Environment\n\nPython: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.183.06\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post1\nsgl_kernel: 0.1.0\nflashinfer: Module Not Found\ntriton: 3.2.0\ntransformers: 4.51.3\ntorchao: 0.10.0\nnumpy: 1.26.4\naiohttp: 3.11.13\nfastapi: 0.115.11\nhf_transfer: Module Not Found\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: Module Not Found\norjson: 3.10.15\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.11.3\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.3.post1.dev5+g4754911ac.cu124.ant\nxgrammar: 0.1.11\nopenai: 1.58.1\ntiktoken: 0.9.0\nanthropic: Module Not Found\nlitellm: Module Not Found\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    SYS     SYS     0-47,96-143     0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    SYS     SYS     0-47,96-143     0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    SYS     SYS     0-47,96-143     0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    PIX     SYS     SYS     0-47,96-143     0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     PIX     NODE    48-95,144-191   1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     NODE    NODE    48-95,144-191   1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     NODE    PIX     48-95,144-191   1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     NODE    NODE    48-95,144-191   1               N/A\nNIC0    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    SYS     SYS\nNIC1    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE     X      SYS     SYS\nNIC2    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS      X      NODE\nNIC3    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     NODE     X ",
    "labels": [
      "speculative-decoding"
    ],
    "state": "open",
    "created_at": "2025-06-07T14:04:12+00:00",
    "closed_at": null,
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6949/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6949"
  },
  {
    "number": 6863,
    "title": "[Bug] FA3 + EAGLE2: speculative_token_map not supported",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWe conducted a large-scale load test with speculative decoding using EAGLE2. According to our results, enabling speculative_token_map effectively reduces overhead for larger batch sizes. However, this feature only works with the flashinfer backend, while the FA3 backend does not support it.\n\nHere is our graph demonstrating this:\n\n+FR is running with speculative_token_map. Qwen2.5\n\n![Image](https://github.com/user-attachments/assets/81420e3b-0c21-418e-bf18-264f91115d90)\n\n![Image](https://github.com/user-attachments/assets/d2a42f0c-2fad-4f7f-8442-ec787a79eb01)\n\n![Image](https://github.com/user-attachments/assets/994dd5cc-2151-4eac-bdd2-61f296022598)\n\n### Reproduction\n\n-\n\n### Environment\n\nsglang docker 0.4.6.post5",
    "labels": [
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2025-06-04T08:14:37+00:00",
    "closed_at": "2025-06-27T21:00:23+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6863/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6863"
  },
  {
    "number": 6783,
    "title": "[Bug] EAGLE3 perform worse with sequence length larger than the draft model context window",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI am testing EAGLE3 for Llama 3.3 70B model, using [lmsys/sglang-EAGLE3-LLaMA3.3-Instruct-70B](https://huggingface.co/lmsys/sglang-EAGLE3-LLaMA3.3-Instruct-70B), the EAGLE3 heads has `max_position_embeddings`=2048. I observed that when input sequence is > 2k, the performance is worse than the baseline when speculative decoding is not enabled.\n\nI have two questions:\n  *)  I am not sure if this is because I didn't config things correctly? or is just working as expected? If the draft model is trained with 2048 context window, does that mean it wont' work for input sequence > 2k?\n  *) Do people typically train a EAGLE3 with same context window as the base model? What is the guideline here? \n\nHere is the server config I used:\nbase model: [nvidia/Llama-3.3-70B-Instruct-FP8](https://huggingface.co/nvidia/Llama-3.3-70B-Instruct-FP8)\ndraft model: [lmsys/sglang-EAGLE3-LLaMA3.3-Instruct-70B](https://huggingface.co/lmsys/sglang-EAGLE3-LLaMA3.3-Instruct-70B)\n```\npython3 -m sglang.launch_server --port=7080 \\\n--model-path=nvidia/Llama-3.3-70B-Instruct-FP8 \\\n--speculative-algorithm=EAGLE3 \\\n--speculative-draft-model-path=lmsys/sglang-EAGLE3-LLaMA3.3-Instruct-70B \\\n--speculative-num-steps=5 \\\n--speculative-eagle-topk=4 \\\n--speculative-num-draft-tokens=8 \\\n--trust-remote-code \\\n--tp=8 \\\n--dtype=bfloat16 \\\n--attention-backend=fa3 \\\n--chunked-prefill-size=16384 \\\n--mem-fraction-static=0.7 \\\n--host=0.0.0.0\n```\n\nHere are the testing summary (concurrency=1):\n<meta charset=\"utf-8\"><b style=\"font-weight:normal;\" id=\"docs-internal-guid-c5d9bebe-7fff-9abc-7b67-dc401fb71dba\"><div dir=\"ltr\" style=\"margin-left:0pt;\" align=\"center\">\ninput length\u00a0(output length = 1k) | baseline\u00a0output throughput (tok/s) | with eagle3\u00a0output throughput (tok/s) | avg accept length (just by eyeballing)\n-- | -- | -- | --\n1k | 111.76 | 168.56 | 3.2\n2k | 111.04 | 154.53 | 3.2-2.5\n3k | 109.55 | 95.96 | 1.6\n4k | 108.71 | 90.21 | 1.6\n10k | 102.70 | 63.91 | 1.2\n\n</div></b>\n\nI am using an internal benchmark tool, so I cannot share the input dataset.\n\n### Reproduction\n\nHere is the server config I used:\nbase model: nvidia/Llama-3.3-70B-Instruct-FP8\ndraft model: lmsys/sglang-EAGLE3-LLaMA3.3-Instruct-70B\n```\npython3 -m sglang.launch_server --port=7080 \\\n--model-path=nvidia/Llama-3.3-70B-Instruct-FP8 \\\n--speculative-algorithm=EAGLE3 \\\n--speculative-draft-model-path=lmsys/sglang-EAGLE3-LLaMA3.3-Instruct-70B \\\n--speculative-num-steps=5 \\\n--speculative-eagle-topk=4 \\\n--speculative-num-draft-tokens=8 \\\n--trust-remote-code \\\n--tp=8 \\\n--dtype=bfloat16 \\\n--attention-backend=fa3 \\\n--chunked-prefill-size=16384 \\\n--mem-fraction-static=0.7 \\\n--host=0.0.0.0\n```\n\n### Environment\n\n# python3 -m sglang.check_env\n```\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.90.07\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post5\nsgl_kernel: 0.1.4\nflashinfer_python: 0.2.5\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.9.0\nnumpy: 2.2.6\naiohttp: 3.12.0\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.32.0\ninteregular: 0.3.3\nmodelscope: 1.26.0\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.5\npython-multipart: 0.0.20\npyzmq: 26.4.0\nuvicorn: 0.34.2\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.19\nopenai: 1.82.0\ntiktoken: 0.9.0\nanthropic: 0.52.0\nlitellm: 1.71.1\ndecord: 0.6.0\nNVIDIA Topology: \n\tGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t0-51,104-155\t0\t\tN/A\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t0-51,104-155\t0\t\tN/A\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\t0-51,104-155\t0\t\tN/A\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\t0-51,104-155\t0\t\tN/A\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\t52-103,156-207\t1\t\tN/A\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\t52-103,156-207\t1\t\tN/A\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\t52-103,156-207\t1\t\tN/A\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \t52-103,156-207\t1\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nHypervisor vendor: KVM\nulimit soft: 1048576\n```",
    "labels": [
      "speculative-decoding"
    ],
    "state": "open",
    "created_at": "2025-05-30T21:03:19+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6783/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6783"
  },
  {
    "number": 6618,
    "title": "[Bug] qwen2 eagle startup failed",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n@Lzhang-hub @libratiger  Failed startup when using qwen2 eagle\n```\nERROR 16:58:08 scheduler.py:2626] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2611, in run_scheduler_process\n    scheduler.event_loop_normal()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 733, in event_loop_normal\n    result = self.run_batch(batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1829, in run_batch\n    ) = self.draft_worker.forward_batch_speculative_generation(batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 292, in forward_batch_speculative_generation\n    self.forward_draft_extend(\n  File \"/sgl-workspace/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 609, in forward_draft_extend\n    logits_output, _ = self.draft_model_runner.forward(forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1138, in forward\n    return self._forward_raw(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1162, in _forward_raw\n    ret = self.forward_extend(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1105, in forward_extend\n    return self.model.forward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/qwen2.py\", line 422, in forward\n    hidden_states = self.model(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\nTypeError: Qwen2Model.forward() got an unexpected keyword argument 'pp_proxy_tensors'\n```\n\nI found that because Qwen2ForCausalLMEagle did not initialize the parent class Qwen2ForCausalLM when initializing, but used the forward function of the parent class, resulting in undefined variables and mismatched parameters,\n\nthis using `nn.Module.__init__`  init function, as a result, some member variables are not defined, such as `self.pp_group`.\n\nhttps://github.com/sgl-project/sglang/blob/006ead9dcbec360c0a656ee84d34446dc80c88c7/python/sglang/srt/models/qwen2_eagle.py#L119\n\nthis using `sglang.srt.models.qwen2_eagle.Qwen2Model` instead of `sglang.srt.models.qwen2.Qwen2Model`, but `qwen2_eagle.Qwen2Model` does not support `pp_proxy_tensors`.\nhttps://github.com/sgl-project/sglang/blob/006ead9dcbec360c0a656ee84d34446dc80c88c7/python/sglang/srt/models/qwen2_eagle.py#L122\n\nhttps://github.com/sgl-project/sglang/blob/006ead9dcbec360c0a656ee84d34446dc80c88c7/python/sglang/srt/models/qwen2.py#L429-L454\n\n### Reproduction\n\n```bash\nset -ex\nexport CUDA_VISIBLE_DEVICES=2\nh20_4=10.3.8.12\nModel_Path=/goosefsx/x-c60-2k4c4x-proxy/data/model/Qwen2-7B-Instruct\nEAGLE_Model_Path=/goosefsx/x-c60-2k4c4x-proxy/data/model/EAGLE-Qwen2-7B-Instruct\n\npython3 -m sglang.launch_server \\\n    --host ${h20_4} \\\n    --port 33434 \\\n    --model-path ${Model_Path} \\\n    --tp 1 \\\n    --mem-fraction-static 0.4 \\\n    --chunked-prefill-size 8192 \\\n    --max-prefill-tokens 32768 \\\n    --disable-cuda-graph \\\n    --log-level debug \\\n    --speculative-algo EAGLE --speculative-draft ${EAGLE_Model_Path} \\\n    --speculative-num-steps 4 --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 \\\n    --trust-remote-code 2>&1 | tee ${0}.log\n```\n\n### Environment\n\nh20",
    "labels": [
      "speculative-decoding"
    ],
    "state": "open",
    "created_at": "2025-05-26T09:12:43+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6618/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6618"
  },
  {
    "number": 6309,
    "title": "[Bug] eagle2\u3010CUDA error: an illegal memory access was encountered\u3011",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI was running eagle2 online\uff08based on sglang=0.4.5\uff0csgl-kernel=0.0.5 \uff0cflashinfer-python=0.2.5\uff09, and after a while, I encountered an cuda error saying \"an illegal memory access was encountered\". Here are the details of the error. Could you help me take a look?\n\n---------------------\nFile \"/root/python/betelgeuse/srt/managers/scheduler.py\", line 2582, in run_scheduler_process\n    scheduler.event_loop_normal()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/root/python/betelgeuse/srt/managers/scheduler.py\", line 702, in event_loop_normal\n    result = self.run_batch(batch)\n  File \"/root/python/betelgeuse/srt/managers/scheduler.py\", line 1657, in run_batch\n    ) = self.draft_worker.forward_batch_speculative_generation(batch)\n  File \"/root/python/betelgeuse/srt/speculative/eagle_worker.py\", line 254, in forward_batch_speculative_generation\n    logits_output, verify_output, model_worker_batch = self.verify(\n  File \"/root/python/betelgeuse/srt/speculative/eagle_worker.py\", line 490, in verify\n    logits_output,_,_,_ = self.target_worker.forward_batch_generation(\n  File \"/root/python/betelgeuse/srt/managers/tp_worker.py\", line 292, in forward_batch_generation\n    logits_output = self.model_runner.forward(forward_batch)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/root/python/betelgeuse/srt/model_executor/model_runner.py\", line 1257, in forward\n    return self.cuda_graph_runner.replay(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/root/python/betelgeuse/srt/model_executor/cuda_graph_runner.py\", line 569, in replay\n    self.replay_prepare(forward_batch)\n  File \"/root/python/betelgeuse/srt/model_executor/cuda_graph_runner.py\", line 548, in replay_prepare\n    self.model_runner.attn_backend.init_forward_metadata_replay_cuda_graph(\n  File \"/root/python/betelgeuse/srt/layers/attention/flashinfer_backend.py\", line 369, in init_forward_metadata_replay_cuda_graph\n    self.indices_updater_prefill.update(\n  File \"/root/python/betelgeuse/srt/layers/attention/flashinfer_backend.py\", line 738, in update_single_wrapper\n    self.call_begin_forward(\n  File \"/root/python/betelgeuse/srt/layers/attention/flashinfer_backend.py\", line 895, in call_begin_forward\n    wrapper_paged.begin_forward(\n  File \"/usr/local/lib/python3.10/dist-packages/flashinfer/prefill.py\", line 1302, in plan\n    qo_indptr_host = qo_indptr.to(\"cpu\")\nRuntimeError: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n### Reproduction\n\ntarget model is Mixtral\uff0ceagle model trained by myself\nconfig\uff1a\n\"mem_fraction_static\": 0.7,\n\"max_running_requests\": 800,\n\"speculative_algorithm\": \"EAGLE\",\n\"speculative_num_steps\": 3,\n\"speculative_eagle_topk\": 1,\n\"speculative_num_draft_tokens\": 4,\n\n\n### Environment\n\nPython: 3.10.4 (main, Apr  2 2022, 09:04:19) [GCC 11.2.0]\nCUDA available: True\nGPU 0: NVIDIA A800-SXM4-80GB\nGPU 0 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda-12.4\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 470.161.03\nPyTorch: 2.5.1+cu124\nsglang: 0.4.5\nsgl_kernel: 0.0.5\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.51.0\ntorchao: 0.10.0\nnumpy: 1.26.4\naiohttp: 3.11.16\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.11.3\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.1\nuvloop: 0.21.0\nvllm: 0.7.2\nxgrammar: 0.1.17\nopenai: 1.73.0\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.66.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    mlx5_0  mlx5_1  mlx5_2  mlx5_3  mlx5_4  mlx5_5  mlx5_6  mlx5_7  mlx5_8  CPU Affinity    NUMA Affinity\nGPU0     X      SYS     SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    32-63,96-127    1\nmlx5_0  SYS      X      NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS\nmlx5_1  SYS     NODE     X      PIX     NODE    NODE    SYS     SYS     SYS     SYS\nmlx5_2  SYS     NODE    PIX      X      NODE    NODE    SYS     SYS     SYS     SYS\nmlx5_3  SYS     NODE    NODE    NODE     X      PIX     SYS     SYS     SYS     SYS\nmlx5_4  SYS     NODE    NODE    NODE    PIX      X      SYS     SYS     SYS     SYS\nmlx5_5  PXB     SYS     SYS     SYS     SYS     SYS      X      PIX     NODE    NODE\nmlx5_6  PXB     SYS     SYS     SYS     SYS     SYS     PIX      X      NODE    NODE\nmlx5_7  NODE    SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      PIX\nmlx5_8  NODE    SYS     SYS     SYS     SYS     SYS     NODE    NODE    PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576",
    "labels": [
      "speculative-decoding"
    ],
    "state": "open",
    "created_at": "2025-05-15T03:41:17+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6309/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6309"
  },
  {
    "number": 6268,
    "title": "[Feature] Support EAGLE-3 for speculative decoding on DeepSeek model",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nEAGLE-3 appears to provide a higher speculative decoding acceptance rate to improve output throughput. \nFor specific code generation scenarios, we found that focusing on multi-step losses when training the draft model can effectively improve the acceptance rate.\n\n### Related resources\n\n_No response_",
    "labels": [
      "speculative-decoding"
    ],
    "state": "open",
    "created_at": "2025-05-13T12:41:36+00:00",
    "closed_at": null,
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6268/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6268"
  },
  {
    "number": 5924,
    "title": "[Feature] optimize eagle speculative decoding",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n- [ ] optimize top k 1 @merrymercy @zhyncs @Fridge003 @ispobock @Alcanderian \n- [ ] support draft extend cuda graph (flashinfer @merrymercy fa3 @hebiao064 @qingquansong )\n- [ ] support schedule overlap with speculative decoding @merrymercy \n- [ ] minor improvement with profiling @Fridge003 @zhyncs @ispobock @Alcanderian \n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority",
      "inactive",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2025-04-30T17:07:56+00:00",
    "closed_at": "2025-07-01T00:22:50+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5924/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 1,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5924"
  },
  {
    "number": 5886,
    "title": "[Bug] CUDA Error Eagle2 + mixed_chunk",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHello! I faced with some problem when tried to use EAGLE2 with `--enable-mixed-chunk` option. The base model I use is `Qwen2.5-Coder-32B-Instruct-FP8`. The problem occurs at least at versions v0.4.6 and earlier.\n\n```\n[2025-04-29 09:06:521 INFO:\n172.23.0.1:55488 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-04-29 09:06:52] INFO:\n172.23.0.1:55554 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-04-29 09:06:52 TPO] Scheduler hit an exception: Traceback (most recent call last):\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2026, in run_scheduler_process scheduler.event_loop_normal()\nFile \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context return func(*args, **kwargs)\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 626, in event_100p_normal\nresult = self.run_batch(batch)\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1381, in run_batch\n) = self.draft_worker.forward_batch_speculative_generation (batch)\nFile \"/sgl-workspace/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 275, in forward_batch_speculative_generation\nlogits_output, next_token_ids, bid = self. forward_target_extend (batch)\nFile \"/sgl-workspace/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 299, in forward_target_extend\nlogits_ output, next_token_ids = self. target_worker.forward_batch_generation(\nFile \"/sg1-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 177, in forward _batch_generation\nlogits_output = self.model_runner.forward (forward_batch)\nFile \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1034, in forward return self. forward_extend(\nFile \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 995,\nin forward_extend\nreturn self.model. forward (\nFile \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context return func(*args, **kwargs)\nFile \"/sgl-workspace/sglang/python/sglang/srt/models/qwen2.py\", line 383, in forward\nhidden_states = self.model(input_ids, positions, forward_batch, input_embeds)\nFile \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in wrapped_call_impl return self. call_impl(*args,\n**kwargs)\nFile \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in_call_impl return forward_call*args, **kwargs)\nFile \"/sgl-workspace/sglang/python/sglang/srt/models/qwen2.py\", line 291, in forward\nhidden_states, residual = layer(\nFile \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl return self._call_impl(*args, **kwargs)\nFile \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl return forward_call(*args, **kwargs)\nFile \"/sgl-workspace/sglang/python/sglang/srt/models/qwen2.py\", line 232, in forward\nhidden_states = self.mlp(hidden_states)\nFile \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in_ wrapped_call_impl return self._call_impl(*args, **kwargs)\nFile \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in_call_impl return forward_call(*args, **kwargs)\nFile \"/sgl-workspace/sglang/python/sglang/srt/models/qwen2.py\", line 87, in forward\n= self.down_proj (x)\nFile \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in wrapped_call_impl return self._call_impl(*args, **kwargs)\nFile \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in_ call_impl return forward_call(*args, **kwargs)\nFile \"/sgl-workspace/sglang/python/sglang/srt/layers/linear.py\", line 1291, in forward\noutput_parallel = self.quant_method.apply(self, input_parallel, bias=bias_)\nFile \"/sgl-workspace/sglang/python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py\", line 664, in apply return scheme.apply_weights(layer, X, bias=bias)\nFile \"/sg1-workspace/sglang/python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py\"\n, line 151, in\napply_weights\nreturn apply_fp8_linear(\nFile \"/sgl-workspace/sglang/python/sglang/srt/layers/quantization/fp8_utils.py\", line 360, in apply_fp8_linear\noutput = torch._scaled_mm(\nRuntimeError: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling cublastMatmul with transpose_mati t transpose_matz n m 5120 n 751 k 13824\nmat1_ 1d 13824 mat2_1d 13824 result_la 5120 computeType 68 scaleType 0\n[2025-04-29 09:06:52] Received sigquit from a child process. It usually means the child failed.\n[2025-04-29 09:06:52] INFO:\n172.23.0.1:55538 - \"POST /generate HTTP/1.1\" 200 OK\n```\n\n\n### Reproduction\n\nHere is my `docker-compose` file below\n\n``` yaml\nservices:\n  qwen2-5-32b-coder-eagle:\n    image: lmsysorg/sglang:v0.4.6-cu125\n    container_name: qwen2-5-32b-coder-eagle\n    volumes:\n      - data/home/models:/data/models\n    restart: no\n    ports:\n      - \"8035:8035\"\n    entrypoint: python3 -m sglang.launch_server\n    command:\n      --model-path /data/models/Qwen2.5-Coder-32B-Instruct-FP8\n      --served-model-name Qwen2.5-Coder-32B-Instruct-EAGLE\n      --speculative-algorithm EAGLE\n      --speculative-draft-model-path /data/models/EAGLE/EAGLE-Qwen2.5-32B-Coder-Instruct\n      --speculative-num-steps 3\n      --speculative-eagle-topk 2\n      --speculative-num-draft-tokens 4\n      --port 8035\n      --host 0.0.0.0\n      --context-length 32768\n      --schedule-conservativeness 0.3\n      --mem-fraction-static 0.7\n      --api-key dummyApiKey\n      --max-running-requests 512\n      --enable-metrics\n      --dtype bfloat16\n      --grammar-backend xgrammar\n      --attention-backend fa3\n      --tensor-parallel-size 2\n      --enable-mixed-chunk\n    unlimits:\n      memlock: -1\n      stack: 67108864\n    ipc: host\n    healthcheck:\n      test: [\"CMD-SHELL\", \"curl -f http://0.0.0.0:8035/health || exit 1\"]\n      interval: 60s\n      timeout: 10s\n      retries: 3\n      start_period: 120s\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              device_id: [\"0\", \"1\"]\n              capabilities: [gpu]\n    devices:\n      - /dev/nvidia-caps\n      - /dev/nvidia0\n      - /dev/nvidia1\n      - /dev/nvidiactl\n      - /dev/nvidia-modeset\n      - /dev/nvidia-uvm\n      - /dev/nvidia-uvm-tools\n\n```\n\n### Environment\n\nDriver Version: 560.35.03\nCUDA Version: 12.6\nGPU NVIDIA H100 80GB HBM3 (2x)",
    "labels": [
      "speculative-decoding"
    ],
    "state": "open",
    "created_at": "2025-04-29T12:39:41+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5886/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5886"
  },
  {
    "number": 5437,
    "title": "[Feature] optimize SegmentPackBits",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCurrently `SegmentPackBits` is not performant, implement a performant one in sgl-kernel\n\nhttps://github.com/sgl-project/sglang/blob/8ec0bb7d558d1722be4efb8b3abf5e09c0e9c20e/sgl-kernel/csrc/speculative/packbit.cu#L39\n\nhttps://github.com/flashinfer-ai/flashinfer/blob/main/include/flashinfer/quantization.cuh#L98\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority",
      "inactive",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2025-04-15T23:03:41+00:00",
    "closed_at": "2025-06-16T00:20:43+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5437/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5437"
  },
  {
    "number": 5361,
    "title": "[Feature] support merge_state in sgl-kernel",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nI have talked to @deftruth, and he will support it in the sgl-kernel today\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority",
      "collaboration",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2025-04-14T00:56:57+00:00",
    "closed_at": "2025-04-15T04:32:18+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5361/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5361"
  },
  {
    "number": 4351,
    "title": "Speculative Decoding Fails with AWQ Quantized Model",
    "body": "Description:\n\nI am facing an issue when using speculative decoding with an AWQ quantized model in sglang. The same configuration works fine with an unquantized model (Llama-3.1-8b-Instruct), but fails when I switch to an AWQ quantized model(Nvidia-Llama-3.1-Nemotron-70B-Instruct-HF-AWQ-INT4).\n\nSetup:\n\nGPUs: NVIDIA L40s (48GB VRAM) x 2\nCUDA Version: 12.8\nPyTorch Version: 2.5.1\nsglang Version: 0.4.3.post2\n\nWorking Configuration (Unquantized Model):\n\n```\nimport requests\nimport os\n\nfrom sglang import assistant_begin, assistant_end\nfrom sglang import assistant, function, gen, system, user\nfrom sglang import image\nfrom sglang import RuntimeEndpoint, set_default_backend\nfrom sglang.srt.utils import load_image\nfrom sglang.test.test_utils import is_in_ci\nfrom sglang.utils import print_highlight, terminate_process, wait_for_server\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n\nif is_in_ci():\n    from sglang.docs.frontend.patch import launch_server_cmd\nelse:\n    from sglang.utils import launch_server_cmd\n\nserver_process, port = launch_server_cmd(\n    \"\"\"python -m sglang.launch_server --model-path /LLM/model/Llama-3.1-8B-Instruct \\\n    --tp 2 --tokenizer-mode auto --mem-fraction-static 0.5 --enable-torch-compile --device cuda \\\n    --speculative-algorithm EAGLE --speculative-draft-model-path /LLM/model/sglang-EAGLE-Llama-3.1-Instruct-8B --speculative-num-steps 2 \\\n    --speculative-eagle-topk 4 --speculative-num-draft-tokens 16 --cuda-graph-max-bs 2 --dtype bfloat16\n\"\"\")\n\nwait_for_server(f\"http://localhost:{port}\")\nprint(f\"Server started on http://localhost:{port}\")\n```\nFailing Configuration (AWQ Quantized Model):\nI replaced the base model with an AWQ quantized model (Nvidia-Llama-3.1-Nemotron-70B-Instruct-HF-AWQ-INT4) using the same configuration. When I run the server, I get the following error:\n\nINFO 03-12 15:30:44 __init__.py:190] Automatically detected platform cuda.\n2025-03-12 15:30:45,824 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n[2025-03-12 15:30:46] server_args=ServerArgs(model_path='/raid/LLM/model/Nvidia-Llama-3.1-Nemotron-70B-Instruct-HF-AWQ-INT4', tokenizer_path='/raid/LLM/model/Nvidia-Llama-3.1-Nemotron-70B-Instruct-HF-AWQ-INT4', tokenizer_mode='auto', load_format='auto', trust_remote_code=False, dtype='bfloat16', kv_cache_dtype='auto', quantization_param_path=None, quantization=None, context_length=None, device='cuda', served_model_name='/raid/LLM/model/Nvidia-Llama-3.1-Nemotron-70B-Instruct-HF-AWQ-INT4', chat_template=None, is_embedding=False, revision=None, skip_tokenizer_init=False, host='127.0.0.1', port=30218, mem_fraction_static=0.5, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=-1, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=True, tp_size=2, stream_interval=1, stream_output=False, random_seed=632040603, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='sglang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_draft_model_path='/raid/LLM/model/sglang-EAGLE-Llama-3.1-Instruct-8B', speculative_algorithm='EAGLE', speculative_num_steps=2, speculative_num_draft_tokens=16, speculative_eagle_topk=4, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=True, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=True, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=True, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=True, torch_compile_max_bs=32, cuda_graph_max_bs=2, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, return_hidden_states=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=False)\n[2025-03-12 15:30:46] Casting torch.float16 to torch.bfloat16.\nINFO 03-12 15:30:46 awq_marlin.py:111] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\nINFO 03-12 15:30:48 __init__.py:190] Automatically detected platform cuda.\n/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\n/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\nINFO 03-12 15:30:48 __init__.py:190] Automatically detected platform cuda.\nINFO 03-12 15:30:48 __init__.py:190] Automatically detected platform cuda.\n2025-03-12 15:30:49,916 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n2025-03-12 15:30:49,981 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n2025-03-12 15:30:50,001 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n[2025-03-12 15:30:50 TP0] Casting torch.float16 to torch.bfloat16.\nINFO 03-12 15:30:50 awq_marlin.py:111] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n[2025-03-12 15:30:50 TP1] Casting torch.float16 to torch.bfloat16.\nINFO 03-12 15:30:50 awq_marlin.py:111] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n[2025-03-12 15:30:50 TP0] Casting torch.float16 to torch.bfloat16.\nINFO 03-12 15:30:50 awq_marlin.py:111] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n[2025-03-12 15:30:50 TP0] Init torch distributed begin.\n[2025-03-12 15:30:50 TP1] Casting torch.float16 to torch.bfloat16.\nINFO 03-12 15:30:50 awq_marlin.py:111] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n[2025-03-12 15:30:50 TP1] Init torch distributed begin.\n[2025-03-12 15:30:50 TP0] sglang is using nccl==2.21.5\n[2025-03-12 15:30:50 TP1] sglang is using nccl==2.21.5\n[2025-03-12 15:30:51 TP1] Load weight begin. avail mem=43.74 GB\n[2025-03-12 15:30:51 TP0] Load weight begin. avail mem=43.74 GB\nLoading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  11% Completed | 1/9 [00:00<00:04,  1.77it/s]\nLoading safetensors checkpoint shards:  22% Completed | 2/9 [00:01<00:05,  1.39it/s]\nLoading safetensors checkpoint shards:  33% Completed | 3/9 [00:02<00:04,  1.36it/s]\nLoading safetensors checkpoint shards:  44% Completed | 4/9 [00:02<00:03,  1.37it/s]\nLoading safetensors checkpoint shards:  56% Completed | 5/9 [00:03<00:02,  1.37it/s]\nLoading safetensors checkpoint shards:  67% Completed | 6/9 [00:04<00:02,  1.38it/s]\nLoading safetensors checkpoint shards:  78% Completed | 7/9 [00:05<00:01,  1.39it/s]\nLoading safetensors checkpoint shards:  89% Completed | 8/9 [00:06<00:00,  1.13it/s]\nLoading safetensors checkpoint shards: 100% Completed | 9/9 [00:07<00:00,  1.16it/s]\nLoading safetensors checkpoint shards: 100% Completed | 9/9 [00:07<00:00,  1.27it/s]\n\n[2025-03-12 15:31:00 TP1] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=25.09 GB\n[2025-03-12 15:31:01 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=25.09 GB\n[2025-03-12 15:31:01 TP0] KV Cache is allocated. K size: 1.61 GB, V size: 1.61 GB.\n[2025-03-12 15:31:01 TP1] KV Cache is allocated. K size: 1.61 GB, V size: 1.61 GB.\n[2025-03-12 15:31:01 TP0] Memory pool end. avail mem=20.65 GB\n[2025-03-12 15:31:01 TP1] Memory pool end. avail mem=20.65 GB\n[2025-03-12 15:31:01 TP0] Capture cuda graph begin. This can take up to several minutes.\n  0%|          | 0/2 [00:00<?, ?it/s][2025-03-12 15:31:01 TP1] Capture cuda graph begin. This can take up to several minutes.\n2025-03-12 15:31:02,528 - INFO - flashinfer.jit: Loading JIT ops: quantization\n2025-03-12 15:31:02,537 - INFO - flashinfer.jit: Finished loading JIT ops: quantization\n2025-03-12 15:31:02,542 - INFO - flashinfer.jit: Loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\n2025-03-12 15:31:02,559 - INFO - flashinfer.jit: Finished loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\n2025-03-12 15:31:02,569 - INFO - flashinfer.jit: Loading JIT ops: quantization\n2025-03-12 15:31:02,578 - INFO - flashinfer.jit: Finished loading JIT ops: quantization\n2025-03-12 15:31:02,583 - INFO - flashinfer.jit: Loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\n2025-03-12 15:31:02,598 - INFO - flashinfer.jit: Finished loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:25<00:00, 12.92s/it]\n[2025-03-12 15:31:27 TP1] Registering 322 cuda graph addresses\n[2025-03-12 15:31:27 TP0] Registering 322 cuda graph addresses\n[2025-03-12 15:31:27 TP1] Capture cuda graph end. Time elapsed: 25.85 s\n[2025-03-12 15:31:27 TP0] Capture cuda graph end. Time elapsed: 25.85 s\n[2025-03-12 15:31:28 TP0] Casting torch.float16 to torch.bfloat16.\n[2025-03-12 15:31:28 TP0] Init torch distributed begin.\n[2025-03-12 15:31:28 TP1] Casting torch.float16 to torch.bfloat16.\n[2025-03-12 15:31:28 TP1] Init torch distributed begin.\n[2025-03-12 15:31:28 TP0] Load weight begin. avail mem=19.89 GB\n[2025-03-12 15:31:28 TP1] Load weight begin. avail mem=19.89 GB\nLoading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/model_loader/weight_utils.py:447: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = torch.load(bin_file, map_location=\"cpu\")\n/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/model_loader/weight_utils.py:447: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = torch.load(bin_file, map_location=\"cpu\")\n[2025-03-12 15:31:29 TP1] Load weight end. type=LlamaForCausalLMEagle, dtype=torch.bfloat16, avail mem=18.64 GB\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.03it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.03it/s]\n\n[2025-03-12 15:31:29 TP0] Load weight end. type=LlamaForCausalLMEagle, dtype=torch.bfloat16, avail mem=18.64 GB\n[2025-03-12 15:31:29 TP0] KV Cache is allocated. K size: 0.02 GB, V size: 0.02 GB.\n[2025-03-12 15:31:29 TP1] KV Cache is allocated. K size: 0.02 GB, V size: 0.02 GB.\n[2025-03-12 15:31:29 TP0] Memory pool end. avail mem=18.56 GB\n[2025-03-12 15:31:29 TP1] Memory pool end. avail mem=18.56 GB\n[2025-03-12 15:31:29 TP1] Capture cuda graph begin. This can take up to several minutes.\n[2025-03-12 15:31:29 TP0] Capture cuda graph begin. This can take up to several minutes.\n  0%|          | 0/2 [00:00<?, ?it/s]2025-03-12 15:31:30,183 - INFO - flashinfer.jit: Loading JIT ops: batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False\n2025-03-12 15:31:30,186 - INFO - flashinfer.jit: Loading JIT ops: batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False\n2025-03-12 15:31:30,202 - INFO - flashinfer.jit: Finished loading JIT ops: batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False\n2025-03-12 15:31:30,251 - INFO - flashinfer.jit: Finished loading JIT ops: batch_decode_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False\n[2025-03-12 15:31:30 TP1] Registering 0 cuda graph addresses\n  0%|          | 0/2 [00:00<?, ?it/s]\n[2025-03-12 15:31:30 TP0] Registering 0 cuda graph addresses\n[2025-03-12 15:31:30 TP1] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py\", line 80, in __init__\n    self.capture()\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py\", line 101, in capture\n    CudaGraphRunner.capture(self)\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/model_executor/cuda_graph_runner.py\", line 304, in capture\n    ) = self.capture_one_batch_size(bs, forward)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py\", line 164, in capture_one_batch_size\n    run_once()\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py\", line 154, in run_once\n    ret = self.eagle_worker.draft_forward(forward_batch)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/speculative/eagle_worker.py\", line 260, in draft_forward\n    logits_output = self.model_runner.model.forward(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib64/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/models/llama.py\", line 393, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib64/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib64/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/models/llama_eagle.py\", line 88, in forward\n    hidden_states = self.fc(\n                    ^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib64/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib64/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib64/python3.11/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n**RuntimeError: mat1 and mat2 shapes cannot be multiplied (4x12288 and 8192x4096)**\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py\", line 1816, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py\", line 252, in __init__\n    self.draft_worker = EAGLEWorker(\n                        ^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/speculative/eagle_worker.py\", line 99, in __init__\n    self.init_cuda_graphs()\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/speculative/eagle_worker.py\", line 110, in init_cuda_graphs\n    self.cuda_graph_runner = EAGLEDraftCudaGraphRunner(self)\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py\", line 82, in __init__\n    raise Exception(\nException: Capture cuda graph failed: mat1 and mat2 shapes cannot be multiplied (4x12288 and 8192x4096)\nPossible solutions:\n1. disable cuda graph by --disable-cuda-graph\n2. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)\n3. disable torch compile by not using --enable-torch-compile\n4. specify --dtype to the same dtype (e.g. bfloat16)\nOpen an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose \n\n\n[2025-03-12 15:31:30 TP0] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py\", line 80, in __init__\n    self.capture()\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py\", line 101, in capture\n    CudaGraphRunner.capture(self)\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/model_executor/cuda_graph_runner.py\", line 304, in capture\n    ) = self.capture_one_batch_size(bs, forward)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py\", line 164, in capture_one_batch_size\n    run_once()\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py\", line 154, in run_once\n    ret = self.eagle_worker.draft_forward(forward_batch)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/speculative/eagle_worker.py\", line 260, in draft_forward\n    logits_output = self.model_runner.model.forward(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib64/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/models/llama.py\", line 393, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib64/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib64/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/models/llama_eagle.py\", line 88, in forward\n    hidden_states = self.fc(\n                    ^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib64/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib64/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib64/python3.11/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (4x12288 and 8192x4096)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py\", line 1816, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py\", line 252, in __init__\n    self.draft_worker = EAGLEWorker(\n                        ^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/speculative/eagle_worker.py\", line 99, in __init__\n    self.init_cuda_graphs()\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/speculative/eagle_worker.py\", line 110, in init_cuda_graphs\n    self.cuda_graph_runner = EAGLEDraftCudaGraphRunner(self)\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py\", line 82, in __init__\n    raise Exception(\nException: Capture cuda graph failed: mat1 and mat2 shapes cannot be multiplied (4x12288 and 8192x4096)\nPossible solutions:\n1. disable cuda graph by --disable-cuda-graph\n2. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)\n3. disable torch compile by not using --enable-torch-compile\n4. specify --dtype to the same dtype (e.g. bfloat16)\nOpen an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose \n\n\n[rank0]:[W312 15:31:31.952292038 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n[rank1]:[W312 15:31:31.523375542 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n[2025-03-12 15:31:32] Rank 0 scheduler is dead. Please check if there are relevant logs.\n[2025-03-12 15:31:33] Exit code: 0\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/launch_server.py\", line 14, in <module>\n    launch_server(server_args)\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/entrypoints/http_server.py\", line 491, in launch_server\n    tokenizer_manager, scheduler_info = _launch_subprocesses(server_args=server_args)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/PycharmProjects/sglang-hosting/venv/lib/python3.11/site-packages/sglang/srt/entrypoints/engine.py\", line 449, in _launch_subprocesses\n    data = scheduler_pipe_readers[i].recv()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib64/python3.11/multiprocessing/connection.py\", line 254, in recv\n    buf = self._recv_bytes()\n          ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib64/python3.11/multiprocessing/connection.py\", line 434, in _recv_bytes\n    buf = self._recv(4)\n          ^^^^^^^^^^^^^\n  File \"/usr/lib64/python3.11/multiprocessing/connection.py\", line 403, in _recv\n    raise EOFError\nEOFError\n\n\n",
    "labels": [
      "inactive",
      "quant",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2025-03-12T21:30:32+00:00",
    "closed_at": "2025-05-13T00:19:02+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4351/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4351"
  },
  {
    "number": 4328,
    "title": "[Bug] --speculative-token-map error when TP>1",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nbased on https://github.com/thunlp/FR-Spec/issues/1, we found that when TP > 1,\nthe code in https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/speculative/eagle_worker.py#L91-L96\n```\n        embed, head = self.target_worker.model_runner.model.get_embed_and_head()\n        if self.hot_token_id is not None:\n            head = head.clone()\n            self.hot_token_id = self.hot_token_id.to(head.device)\n            head.data = head.data[self.hot_token_id]\n        self.draft_model_runner.model.set_embed_and_head(embed, head)\n```\ncause \n```\n../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [158367,0,0], thread: [0,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.`\n```\n\nThis is because the lm_head from the target model and the draft model is partitioned, so some hot_token_ids do not belong to the current GPU.\n\nPossible way to fix this:\n1. use allgather to get the full lm_head from the target model\n2. or load the lm_head of the target model from the checkpoint file \n\n### Reproduction\n\n```\npython3 -m sglang.launch_server --model ../models/Meta-Llama-3-8B-Instruct --speculative-algorithm EAGLE \\\n    --speculative-draft-model-path ../models/sglang-EAGLE-LLaMA3-Instruct-8B --speculative-num-steps 5 \\\n    --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 --speculative-token-map thunlp/LLaMA3-Instruct-8B-FR-Spec/freq_32768.pt \\\n    --mem-fraction 0.7 --cuda-graph-max-bs 2 --dtype float16 --tensor-parallel 2\n```\n\n### Environment\n\nNot environment related.",
    "labels": [
      "inactive",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2025-03-12T05:19:15+00:00",
    "closed_at": "2025-05-18T00:20:48+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4328/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4328"
  },
  {
    "number": 3956,
    "title": "DeepSeek-R1 Optimization Option Ablations",
    "body": "Updated on **2025-03-20**: #4616\n\nUpdated on **2025-03-04**:\n# DeepSeek Optimization Ablations\n\n## Overview\n\nWe sincerely thanks for the help from [M0gician](http://m0gician.github.io/) for the massive experiments.\n\n**As of 2025-03-04**, SGLang provides the following optimizations for DeepSeek V3/R1 models:\n\n| Name                                        | Description                                                                                                                                                                                                                                     | Enabled by Default | Enable/Disable Argument                                                                                                                                   |\n|---------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| MLA Optimization                            | SGLang custom tailored optimizations, including<br>  - *Weight Absorption*,<br>- *Flashinfer MLA Wrapper*,<br> - *FP8 Quantization*,<br> - *CUDA Graph & Torch.compile suuport*                                                                 | \u2705               | `--disable-mla`                                                                                                                                           |\n| CUDA Graph                                  | Capturing and replaying entire sequences of GPU operations as a single graph, thereby reducing kernel launch overhead and synchronization delays                                                                                                | \u2705               | `--disable-cuda-graph`                                                                                                                                    |\n| Torch Compile                               | Dynamically converting PyTorch models into optimized execution graphs, reducing runtime overhead and enhancing GPU performance                                                                                                                  | \u274c              | `--enable-torch-compile`                                                                                                                                 |\n| Radix Cache                                 | Organizes the KV cache in a radix tree, enabling automatic detection and reuse of shared prompt prefixes across multiple generation calls, thereby reducing redundant computations                                                              | \u2705               | `--disable-radix-cache`                                                                                                                                   |\n| Flashinfer MLA                              | Multi-latent Attention implemented by Flashinfer that replaces the default Triton backend                                                                                                                                                       | \u274c              | `--enable-flashinfer-mla`                                                                                                                                 |\n| Speculative Decoding (`Next-N`)             | Dynamically generating a context-aware draft token tree with a smaller, well-calibrated model and then verifying these tokens in parallel with the original LLM, thereby reducing expensive forward passes while preserving output quality.     | \u274c              | `--speculative-algorithm`,<br> `--speculative-draft`,<br> `--speculative-num-steps`,<br> `--speculative-eagle-topk`,<br> `--speculative-num-draft-tokens` |\n| Tensor Parallelism (`tp`)                   | Splitting the heavy tensor operations\u2014such as the matrix multiplications in self-attention and feedforward layers\u2014across multiple GPUs, thereby lowering the per-device memory burden and enabling simultaneous computation for reduced latency | \u2705 (=1)         | `--tp-size`                                                                                                                                               |\n| Expert Parallelism (`EP-MoE`)               | Distributing the computation of different expert subnetworks across multiple devices, thereby reducing memory constraints and communication overhead while enabling simultaneous, efficient processing of input tokens.                         | \u274c              | `--enable-ep-moe`,<br> `--ep-size`                                                                                                                        |\n| Data Parallelism Attention (`DP-Attention`) | Partitioning the MLA attention across DP workers\u2014each handling independent prefill, decode, and idle batches\u2014to significantly reduce per-worker KV cache size and enable larger, more efficient batch processing                                | \u274c              | `--enable-dp-attention`                                                                                                                                   |\n\n## General Advice\n\n* Speculative Decoding is great for small concurrency (less than 32), but its performance degrades quickly as the concurrency increases.\n* `CUDA Graph` boosts inference performance significantly, at the cost of increased memory usage. Sometimes it's a good trade-off to disable `CUDA Graph` to further increase concurrency to get better throughput.\n* `DP-Attention` is a must for large concurrency (greater than 256), but it hurts per-request decoding speed.\n\n## Known Issues\n\n* Speculative Decoding is not compatible with:\n  - `Flashinfer-mla`\n  - `Radix Cache`\n  - `DP-Attention`\n  - Both `CUDA Graph` and `Torch Compile` enabled simultaneously\n* `EP-MoE` is not supported with both `CUDA Graph` and `Torch Compile` enabled\n* To run `DP-Attention` with large concurrency, you must first run a warmup phase with small concurrency (e.g. `bs=16`, `total req=32`) to avoid CUDA out of memory error.\n\n## Optimization Ablations\n\n### Test Environment\n\n* SGLang version: 0.4.3.post2@[110e006](https://github.com/sgl-project/sglang/commit/110e0066735a3bd431c2640ae168fc040d7c0806)\n* Flashinfer version: 0.2.2.post1\n* Hardware: 2 nodes of H20 (AMD EPYC 9K84 * 2, 2.20 TiB memory, 8 * H20 96GiB each)\n* Model: DeepSeek-R1\n* Model Max Length: 3200 (modified in both model and NextN's `tokenizer_config.json`)\n* CUDA Version: 12.2\n* Operating System: Rocky Linux release 9.2 (Blue Onyx)\n\n### Single Query Performance\n\n* Test query: `\u4e00\u4e2a\u6c49\u5b57\u5177\u6709\u5de6\u53f3\u7ed3\u6784\uff0c\u5de6\u8fb9\u662f\u6728\uff0c\u53f3\u8fb9\u662f\u4e5e\u3002\u8fd9\u4e2a\u5b57\u662f\u4ec0\u4e48\uff1f\u53ea\u9700\u56de\u7b54\u8fd9\u4e2a\u5b57\u5373\u53ef\u3002`\n* Expected output: `\u675a`[^1]\n\n| Runnable           | TPS@1[^2] | Torch Compile | Cuda Graph | Radix Cache | Flashinfer-mla | Next-N | EP-MoE | DP-Attention |\n|--------------------|-----------|---------------|------------|-------------|----------------|--------|--------|--------------|\n| \u2705                 | *37.0<sub>tuned kernel</sub>[^11] |       \u2705       |      \u2705     |      \u2705      |        \u2796       |    \u2796   |    \u2796   |       \u2796      |\n| \u2705                 | 33.6     |       \u2705       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2796   |       \u2796      |\n| \u2705                 | 19.1     |       \u2705       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2796   |       \u2705      |\n| \u274c[^3]            | N/A      |       \u2705       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2705   |       \u2705      |\n| \u274c[^3]            | N/A      |       \u2705       |      \u2705     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2796      |\n| \u2705                 | 6.5      |       \u2705       |      \u2796     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2796      |\n| \u2705                 | 24.4     |       \u2796       |      \u2705     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2796      |\n| \u2705                 | 23.6     |       \u2796       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2705   |       \u2796      |\n| \u2705                 | 13.0     |       \u2796       |      \u2796     |      \u2796      |        \u2796       |    \u2705   |    \u2705   |       \u2796      |\n| \u274c[^4] <br> \u2705[^5] | 41.0     |       \u2796       |      \u2705     |      \u2796      |        \u2796       |    \u2705   |    \u2705   |       \u2796      |\n| \u274c[^3]            | N/A      |       \u2705       |      \u2705     |      \u2796      |        \u2796       |    \u2705   |    \u2705   |       \u2796      |\n| \u2705[^5]            | 16.0     |       \u2796       |      \u2705     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2705      |\n| \u274c[^3]            | N/A      |       \u2705       |      \u2705     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2705      |\n| \u2705[^5]            | 15.8     |       \u2796       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2705   |       \u2705      |\n| \u274c[^3]            | N/A      |       \u2796       |      \u2705     |      \u2705      |        \u2796       |    \u2705   |    \u2705   |       \u2705      |\n| \u274c[^6]            | N/A      |       \u2796       |      \u2796     |      \u2796      |        \u2796       |    \u2705   |    \u2796   |       \u2705      |\n\n### Batched Performance\n\n* Test bench: ThreadPool with AsyncOpenAI client\n* Avg input length = 760 tokens\n* Avg output length = 460 tokens\n\n| Runnable | Torch Compile | Cuda Graph  | Radix Cache  | Flashinfer-mla | Next-N |  EP-MoE  | DP-Attn | Client Concurrency [^7]        | Avg Throughput<br><sub><sup>(p+d, token/s)</sup></sub> [^8]                 | Per-req Throughput<br><sub><sup>(d, token/s)</sup></sub> [^9] |   Total Req    | Max-running-req [^10] |\n|----------|---------------|-------------|--------------|----------------|---------|---------|--------------|--------------------------------|----------------------------------------------------|-----------------------------------------------|---------------------|--------------------------|\n| \u2705       |       \u2705       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2796   |       \u2796      | 768                            | 3909.04                                            | 3.28                                          | 1024                | 768                      |\n| \u2705       |       \u2705       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2796   |       \u2705      | 16<br>512<br>768               | 306.18<br>4329.32<br>5457.14                       | 12.96<br>5.69<br>5.38                         | 32<br>1024<br>1024  | 768                      |\n| \u274c[^3]  |       \u2705       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2705   |       \u2705      | N/A                            | N/A                                                | N/A                                           | N/A                 | 768                      |\n| \u274c[^3]  |       \u2705       |      \u2705     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2796      | N/A                            | N/A                                                | N/A                                           | N/A                 | 768                      |\n| \u2705       |       \u2705       |      \u2796     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2796      | 768                            | 2100.85                                            | 2.79                                          | 1024                | 768                      |\n| \u2705       |       \u2796       |      \u2705     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2796      | 256<br>512<br>768              | 2134.99<br>3842.52<br>3453.49                      | 5.16<br>4.05<br>3.15                          | 512<br>1024<br>1024 | 768                      |\n| \u2705       |       \u2796       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2705   |       \u2796      | 256<br>512<br>768              | 2220.56<br>3583.08<br>3556.76                      | 5.12<br>4.07<br>3.52                          | 512<br>1024<br>1024 | 768                      |\n| \u2705       |       \u2796       |      \u2796     |      \u2796      |        \u2796       |    \u2705   |    \u2705   |       \u2796      | N/A                            | N/A                                                | N/A                                           | N/A                 | 768                      |\n| \u2705[^5]  |       \u2796       |      \u2705     |      \u2796      |        \u2796       |    \u2705   |    \u2705   |       \u2796      | 16<br>32                       | 732.22<br>1227.72                                  | 19.93<br>15.14                                | 256                 | 768                      |\n| \u274c[^3]  |       \u2705       |      \u2705     |      \u2796      |        \u2796       |    \u2705   |    \u2705   |       \u2796      | N/A                            | N/A                                                | N/A                                           | N/A                 | 768                      |\n| \u2705[^5]  |       \u2796       |      \u2705     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2705      | 16<br>128<br>256<br>512<br>768 | 862.10<br>1598.17<br>2664.40<br>4098.18<br>\u274c[^4] | 9.20<br>8.22<br>6.70<br>5.48<br>\u274c[^4]        | 128<br>256<br>512<br>1024<br>1024 | 768        |\n| \u274c[^3]  |       \u2705       |      \u2705     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2705      | N/A                            | N/A                                                | N/A                                           | N/A                 | 768                      |\n| \u2705[^5]  |       \u2796       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2705   |       \u2705      | 16<br>512<br>768               | 406.29<br>3633.20<br>\u274c[^4]                       | 12.29<br>5.74<br>\u274c[^4]                       | 32<br>1024<br>1024  | 768                     |\n| \u274c[^3]  |       \u2796       |      \u2705     |      \u2796      |        \u2796       |    \u2705   |    \u2705   |       \u2705      | N/A                            | N/A                                                | N/A                                           | N/A                 | 768                      |\n| \u274c[^6]  |       \u2796       |      \u2796     |      \u2796      |        \u2796       |    \u2705   |    \u2796   |       \u2705      | N/A                            | N/A                                                | N/A                                           | N/A                 | 768                      |\n\n\n[^1]: DeepSeek-R1 cannot give the correct output if quantization is used or has precision issues (fixed in [b110084](https://github.com/sgl-project/sglang/commit/b110084654a1986f0148901190e2f280c605476f))\n[^2]: TPS@1 (Tokens Per Second for single request) is read directly from SGLang's logging.\n[^3]: CUDA error at graph capture.\n[^4]: CUDA out of memory.\n[^5]: Requires setting `mem-fraction-static=0.7` to avoid OOM errors.\n[^6]: TypeError: object of type 'NoneType' has no len().\n[^7]: All statistics are collected from the test bench. Token count is calculated using the same tokenizer used in inference.\n[^8]: Average Throughput(prefill+decode, token/s) = (total tokens)/(total time).\n[^9]: Average Decoding Throughput = (sum of (output tokens/duration) for each successful request)/(number of successful requests).\n[^10]: The maximum number of requests to run concurrently at a SGLang backend, controlled by `--max-running-requests`.\n[^11]: Tested by [Lzhang-Hub](https://github.com/sgl-project/sglang/issues/3956#issuecomment-2700514223), after tuning **block wise fp8** and **fused-moe** triton kernel on H20\n\n<details>\n<summary>Updated on 2025-02-28:</summary>\nSGLang version: 0.4.3@the latest master branch (2025-02-28).\nMachine: 2 nodes of H20 (8 * H20 96G each)\n`model_max_length` in tokenizer_config.json from DeepSeek-R1 and DeepSeek-R1-NextN are modified to `3200`\n\nTest query:  `\u4e00\u4e2a\u6c49\u5b57\u5177\u6709\u5de6\u53f3\u7ed3\u6784\uff0c\u5de6\u8fb9\u662f\u6728\uff0c\u53f3\u8fb9\u662f\u4e5e\u3002\u8fd9\u4e2a\u5b57\u662f\u4ec0\u4e48\uff1f\u53ea\u9700\u56de\u7b54\u8fd9\u4e2a\u5b57\u5373\u53ef\u3002`\nExpected output: `\u675a` \n* the model cannot give the correct output if quantization is used or has precision issue (found in the previous implementation of Flashinfer-mla)\n\n| Runnable                             | TPS@1 | Torch Compile | Cuda Graph | Radix Cache | Flashinfer-mla | Next-N | EP-MoE | DP-Attention |\n|--------------------------------------|-------|---------------|------------|-------------|----------------|--------|--------|--------------|\n| \u274c (gibberish)                             |   28.5   |       \u2705       |      \u2705     |      \u2705      |        \u2705       |    \u274c   |    \u274c   |       \u274c      |\n| \u274c (gibberish)                             |   -   |       \u2705       |      \u2705     |      \u274c      |        \u2705       |    \u274c   |    \u274c   |       \u274c      |\n| \u2705                                    |  22.4 |       \u2705       |      \u2705     |      \u2705      |        \u274c       |    \u274c   |    \u274c   |       \u274c      |\n| \u274c (EAGLE not support Flashinfer)     |   -   |       \u274c       |      \u2705     |      \u274c      |        \u2705       |    \u2705   |    \u274c   |       \u274c      |\n| \u274c (CUDA error at graph capture)      |   -   |       \u2705       |      \u2705     |      \u2705      |        \u274c       |    \u274c   |    \u2705   |       \u274c      |\n| \u2705                                    |  6.5  |       \u2705       |      \u274c     |      \u2705      |        \u274c       |    \u274c   |    \u2705   |       \u274c      |\n| \u2705                                    |  24.4 |       \u274c       |      \u2705     |      \u2705      |        \u274c       |    \u274c   |    \u2705   |       \u274c      |\n| \u2705                                    |  21.3 |       \u274c       |      \u2705     |      \u2705      |        \u2705       |    \u274c   |    \u2705   |       \u274c      |\n| \u274c (OOM)<br>  \u2705 (mem-fraction-static=0.7) |  40.0 |       \u274c       |      \u2705     |      \u274c      |        \u274c       |    \u2705   |    \u2705   |       \u274c      |\n| \u274c (Capture cuda graph failed)        |   -   |       \u274c       |      \u2705     |      \u274c      |        \u274c       |    \u2705   |    \u2705   |       \u2705      |\n| \u274c (Error)                            |   -   |       \u274c       |      \u274c     |      \u274c      |        \u274c       |    \u2705   |    \u2705   |       \u2705      |\n| \u2705                                    |  15.5 |       \u274c       |      \u2705     |      \u2705      |        \u274c       |    \u274c   |    \u2705   |       \u2705      |\n| \u274c (CUDA error at graph capture)      |   -   |       \u2705       |      \u2705     |      \u2705      |        \u274c       |    \u274c   |    \u2705   |       \u2705      |\n</details>",
    "labels": [
      "high priority",
      "inactive",
      "deepseek",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2025-02-28T09:33:38+00:00",
    "closed_at": "2025-07-06T00:22:09+00:00",
    "comments": 36,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3956/reactions",
      "total_count": 40,
      "+1": 39,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3956"
  },
  {
    "number": 3736,
    "title": "[Feature] [Eagle] Are there any plans to support the feature for batching prefill?",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nNow, I find the qps of sglang is very lower than baseline when use Eagle, I think the reason for this problem maybe is that sglang doesn't support batch prefill yet. So Do you have any plans to support this feature? \n\n\n\n### Related resources\n![Image](https://github.com/user-attachments/assets/20272e3b-14b1-4064-a832-563e4d90b280)\n_No response_",
    "labels": [
      "inactive",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2025-02-21T02:20:15+00:00",
    "closed_at": "2025-04-23T00:18:34+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3736/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3736"
  },
  {
    "number": 3662,
    "title": "[Bug] Eagle mtbench benchmark error",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n**I am evaluating the performance for Eagle and I found that the server will report an error when using the mtbench benchmark.\nIt is worth noting that I performed well in version v0.4.3 (e0b9a423c8413c486f8e6a2c168cd3e6e7a74589), but encountered issues during subsequent commits.**\n\n[2025-02-18 09:03:54 TP0] Prefill batch. #new-seq: 1, #new-token: 159, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.01, #running-req: 58, #queue-req: 5\n[2025-02-18 09:03:54 TP0] Prefill batch. #new-seq: 1, #new-token: 229, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.01, #running-req: 59, #queue-req: 4\n[2025-02-18 09:03:54 TP0] Prefill batch. #new-seq: 1, #new-token: 41, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.01, #running-req: 60, #queue-req: 3\n[2025-02-18 09:03:54 TP0] Prefill batch. #new-seq: 1, #new-token: 58, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.01, #running-req: 61, #queue-req: 2\n[2025-02-18 09:03:54 TP0] Prefill batch. #new-seq: 1, #new-token: 57, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.01, #running-req: 62, #queue-req: 1\n[2025-02-18 09:03:54 TP0] Prefill batch. #new-seq: 1, #new-token: 34, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.01, #running-req: 63, #queue-req: 0\n[2025-02-18 09:03:55 TP0] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1827, in run_scheduler_process\n    scheduler.event_loop_normal()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 478, in event_loop_normal\n    result = self.run_batch(batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1089, in run_batch\n    ) = self.draft_worker.forward_batch_speculative_generation(batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 125, in forward_batch_speculative_generation\n    self.forward_draft_extend_after_decode(batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 300, in forward_draft_extend_after_decode\n    forward_batch = ForwardBatch.init_new(model_worker_batch, self.model_runner)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/forward_batch_info.py\", line 268, in init_new\n    positions, ret.extend_start_loc = compute_position_triton(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/forward_batch_info.py\", line 361, in compute_position_triton\n    compute_position_kernel[(batch_size,)](\n  File \"/usr/local/lib/python3.10/dist-packages/triton/runtime/jit.py\", line 345, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/triton/runtime/jit.py\", line 691, in run\n    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,\n  File \"/usr/local/lib/python3.10/dist-packages/triton/backends/nvidia/driver.py\", line 365, in __call__\n    self.launch(*args, **kwargs)\nRuntimeError: Triton Error [CUDA]: an illegal memory access was encountered\n\n[2025-02-18 09:03:55] Received sigquit from a child proces. It usually means the child failed.\nKilled\n\n### Reproduction\nlaunch_server\n`\npython3 -m sglang.launch_server \\\n--model-path \"meta-llama/Llama-3.1-8B-Instruct\" \\\n--speculative-algorithm \"EAGLE\" \\\n--speculative-draft-model-path \"lmzheng/sglang-EAGLE-LLaMA3-Instruct-8B\" \\\n--speculative-num-steps 3 \\\n--speculative-eagle-topk 4 \\\n--speculative-num-draft-tokens 16 \\\n--cuda-graph-max-bs 8 \\\n--dtype \"bfloat16\"\n`\nrun benchmark\n`\npython3 {sglang}/benchmark/mtbench/bench_sglang.py`\n\n\n### Environment\n\nINFO 02-18 08:52:05 __init__.py:190] Automatically detected platform cuda.\nPython: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.161.08\nPyTorch: 2.5.1+cu124\nsglang: 0.4.3\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.1.post1+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.63.0\ntiktoken: 0.8.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    SYS     PIX     NODE    SYS     SYS     SYS     SYS     0-89    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     PIX     NODE    SYS     SYS     SYS     SYS     0-89    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     NODE    PIX     SYS     SYS     SYS     SYS     0-89    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     NODE    PIX     SYS     SYS     SYS     SYS     0-89    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     PIX     NODE    SYS     SYS     90-179  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     PIX     NODE    SYS     SYS     90-179  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     NODE    PIX     SYS     SYS     90-179  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     NODE    PIX     SYS     SYS     90-179  1               N/A\nNIC0    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     PHB     PHB\nNIC1    PIX     PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS      X      NODE    SYS     SYS     SYS     SYS\nNIC2    NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     SYS     NODE     X      SYS     SYS     SYS     SYS\nNIC3    SYS     SYS     SYS     SYS     PIX     PIX     NODE    NODE    SYS     SYS     SYS      X      NODE    SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     NODE    NODE    PIX     PIX     SYS     SYS     SYS     NODE     X      SYS     SYS\nNIC5    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PHB     SYS     SYS     SYS     SYS      X      PHB\nNIC6    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PHB     SYS     SYS     SYS     SYS     PHB      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n\n\nHypervisor vendor: KVM\nulimit soft: 1048576",
    "labels": [
      "inactive",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2025-02-18T09:05:34+00:00",
    "closed_at": "2025-04-21T00:19:30+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3662/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3662"
  },
  {
    "number": 3574,
    "title": "[Bug] Eagle fail on Llama3-8b",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHi team. I'm testing eagle. The base model is Meta-Llama-3-8B-Instruct and eagle draft model is sglang-EAGLE-LLaMA3-Instruct-8B. The issue relates to max_position_embeddings, which is 2048 in eagle draft config. But in my case context len will be larger. \nI can start sglang server but crash when processing requests.\n\ngpu: A100 80G\ndocker image: lmsysorg/sglang:v0.4.2.post4-cu124-srt\n\nstart script:\nSGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 python3 -m sglang.launch_server \\\n  --host 0.0.0.0 \\\n  --port 80 \\\n  --served-model-name llama \\\n  --model ./Meta-Llama-3-8B-Instruct/ \\\n  --trust-remote-code \\\n  --dtype float16 \\\n  --mem-fraction 0.5 \\\n  --max-running-requests 16 \\\n  --speculative-algo EAGLE \\\n  --speculative-draft ./sglang-EAGLE-LLaMA3-Instruct-8B/ \\\n  --disable-cuda-graph \\\n  --context-length 6000 \\\n\ncrash log:\n[2025-02-14 06:41:27 TP0] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1798, in run_scheduler_process\n    scheduler.event_loop_normal()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 479, in event_loop_normal\n    self.process_batch_result(batch, result)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1119, in process_batch_result\n    self.process_batch_result_prefill(batch, result)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1150, in process_batch_result_prefill\n    next_token_ids = next_token_ids.tolist()\nRuntimeError: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nIt's there any workaround? Or have to train an eagle model myself with proper context length?\nThanks~\n\n### Reproduction\n\nstart server:\nSGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 python3 -m sglang.launch_server \\\n  --host 0.0.0.0 \\\n  --port 80 \\\n  --served-model-name llama \\\n  --model ./Meta-Llama-3-8B-Instruct/ \\\n  --trust-remote-code \\\n  --dtype float16 \\\n  --mem-fraction 0.5 \\\n  --max-running-requests 16 \\\n  --speculative-algo EAGLE \\\n  --speculative-draft ./sglang-EAGLE-LLaMA3-Instruct-8B/ \\\n  --disable-cuda-graph \\\n  --context-length 6000 \\\n\ncall with prompt longer then 2k\n\n### Environment\n\ngpu: A100 80G\ndocker image: lmsysorg/sglang:v0.4.2.post4-cu124-srt",
    "labels": [
      "speculative-decoding"
    ],
    "state": "open",
    "created_at": "2025-02-14T08:34:13+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3574/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3574"
  },
  {
    "number": 3362,
    "title": "run eagle speculative decodeing error!",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n![Image](https://github.com/user-attachments/assets/3f69af85-9c68-44e9-85db-cbf712ffb2fa)\n\ncommanf line:\nCUDA_VISIBLE_DEVICES=0,1,3,7 python -m sglang.launch_server --model /mnt/nvme0n1/ckpt/llama/Meta-Llama-3.1-70B-Instruct --port 9001 --host 0.0.0.0 --tensor-parallel-size 4 --speculative-algo EAGLE --speculative-draft /mnt/nvme0n1/ckpt/llama/sglang-EAGLE-LLaMA3-Instruct-70B --speculative-num-steps 5 --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 --mem-fraction 0.7 --disable-cuda-graph\n\nI got some error like this,please help\nI want to know is this an error, or if any additional details are required.\n![Image](https://github.com/user-attachments/assets/bfe3c3a9-b1d8-4ace-af3b-2c3a35cdf201)\n\n### Reproduction\n\nCUDA_VISIBLE_DEVICES=0,1,3,7 python -m sglang.launch_server --model /mnt/nvme0n1/ckpt/llama/Meta-Llama-3.1-70B-Instruct --port 9001 --host 0.0.0.0 --tensor-parallel-size 4 --speculative-algo EAGLE --speculative-draft /mnt/nvme0n1/ckpt/llama/sglang-EAGLE-LLaMA3-Instruct-70B --speculative-num-steps 5 --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 --mem-fraction 0.7 --disable-cuda-graph\n\n### Environment\n\n![Image](https://github.com/user-attachments/assets/3f69af85-9c68-44e9-85db-cbf712ffb2fa)",
    "labels": [
      "inactive",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2025-02-07T04:56:49+00:00",
    "closed_at": "2025-05-02T00:18:42+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3362/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3362"
  }
]
[
  {
    "number": 7429,
    "title": "[Tracking] Model support",
    "body": "### **[Tracking] Model support**\n\nThe goal is to support other model architectures available. Expand the model zoo \ud83c\udf8a \n\nThe goal is to implement support for all architectures listed below. Anyone is welcome to take any issue or implement the model below.\n\nIf you need help implementing a new model, see https://docs.sglang.ai/supported_models/support_new_models.html\n\n#### Text-only Language Models (Generative)\n- [ ] `OPTForCasualLM` (facebook/opt-125m) #7440 \n- [ ] `AquilaForCausalLM` (Aquila, Aquila2)\n- [ ] `ArcticForCausalLM` (Arctic) #5768\n- [ ] `BambaForCausalLM` (Bamba)\n- [ ] `BartForConditionalGeneration` (BART)\n- [ ] `BloomForCausalLM` (BLOOM, BLOOMZ)\n- [ ] `Cohere2ForCasualLM` #4570\n- [ ] `DeciLMForCausalLM` (DeciLM)\n- [ ] `FalconForCausalLM` (Falcon)\n- [ ] `FalconH1ForCausalLM` (Falcon-H1) #6517\n- [ ] `FalconMambaForCausalLM` (FalconMamba)\n- [ ] `Dots1ForCasualLM` (dots.llm1) #6471\n- [ ] `GPT2LMHeadModel` (GPT-2)\n- [ ] `GPTBigCodeForCausalLM` (StarCoder, SantaCoder)\n- [ ] `GPTJForCausalLM` (GPT-J)\n- [ ] `GPTNeoXForCausalLM` (GPT-NeoX, Pythia)\n- [ ] `GraniteForCausalLM` (Granite 3.0, 3.1)\n- [ ] `GraniteMoeForCausalLM` (Granite 3.0 MoE)\n- [ ] `GraniteMoeHybridForCausalLM` (Granite 4.0 MoE Hybrid)\n- [ ] `GraniteMoeSharedForCausalLM` (Granite MoE Shared)\n- [ ] `GritLM` (GritLM)\n- [ ] `InternLMForCausalLM` (InternLM v1)\n- [ ] `JAISLMHeadModel` (Jais)\n- [ ] `JambaForCausalLM` (Jamba) #1190\n- [ ] `MambaForCausalLM` (Mamba)\n- [ ] `Mamba2ForCausalLM` (Mamba2)\n- [ ] `MiniCPMForCausalLM` (MiniCPM v1) #6900\n- [ ] `MiniMaxM1ForCausalLM` (MiniMax-Text) #2898\n- [ ] `MiniMaxText01ForCausalLM` (MiniMax-Text-01)\n- [ ] `MPTForCausalLM` (MPT)\n- [ ] `NemotronForCausalLM` (Nemotron-3) #5063\n- [ ] `NemotronHForCausalLM` (Nemotron-H)\n- [ ] `OLMoForCausalLM` (OLMo v1)\n- [ ] `OLMo2ForCausalLM` (OLMo2)\n- [ ] `OPTForCausalLM` (OPT)\n- [ ] `OrionForCausalLM` (Orion)\n- [ ] `PersimmonForCausalLM` (Persimmon)\n- [x] `PhiForCausalLM` (Phi-1.5, Phi-2) #7862 @ppraneth \n- [x] `Phi3SmallForCausalLM` (Phi-3-Small)\n- [x] `PhiMoEForCausalLM` (Phi-3.5-MoE) #7907 @byjiang1996 \n- [ ] `Plamo2ForCausalLM` (PLaMo2)\n- [ ] `SolarForCausalLM` (Solar Pro)\n- [ ] `Starcoder2ForCausalLM` (Starcoder2)\n- [ ] `TeleChat2ForCausalLM` (TeleChat2) \n- [ ] `TeleFLMForCausalLM` (TeleFLM)\n- [ ] `Zamba2ForCausalLM` (Zamba2)\n\n#### Embedding Models\n- [ ] `GteModel`\n- [ ] `GteNewModel`\n- [ ] `ModernBertModel`\n- [ ] `NomicBertModel`\n- [ ] `RobertaModel`\n- [ ] `JambaForSequenceClassification`\n- [ ] `BertForSequenceClassification`\n- [ ] `Qwen3ForSequenceClassification` #7314\n- [ ] `RobertaForSequenceClassification`\n- [ ] `XLMRobertaForSequenceClassification`\n\n#### Multimodal Models\n- [ ] `Glm4vForConditionalGeneration` (THUDM/GLM-4.1V-9B-Thinking)\n- [ ] `AriaForConditionalGeneration` (Aria)\n- [ ] `AyaVisionForConditionalGeneration` (Aya Vision) #6304\n- [ ] `Blip2ForConditionalGeneration` (BLIP-2) #4414\n- [ ] `ChameleonForConditionalGeneration` (Chameleon)\n- [ ] `Florence2ForConditionalGeneration` (Florence-2)\n- [ ] `FuyuForCausalLM` (Fuyu)\n- [ ] `GLM4VForCausalLM` PP support #7257\n- [ ] `GraniteSpeechForConditionalGeneration` (Granite Speech)\n- [ ] `H2OVLChatModel` (H2OVL)\n- [ ] `Idefics3ForConditionalGeneration` (Idefics3)\n- [ ] `LlavaNextVideoForConditionalGeneration` (LLaVA-NeXT-Video) #4062\n- [ ] `MiniMaxVL01ForConditionalGeneration` (MiniMax-VL)\n- [ ] `MolmoForCausalLM` (Molmo)\n- [ ] `NVLM_D_Model` (NVLM-D 1.0)\n- [ ] `Ovis` (Ovis1.6, Ovis2) #5018\n- [ ] `PaliGemmaForConditionalGeneration` (PaliGemma)\n- [ ] `Phi3VForCausalLM` (Phi-3-Vision) #1108\n- [ ] `PixtralForConditionalGeneration` (Pixtral)\n- [ ] `Qwen2AudioForConditionalGeneration` (Qwen2-Audio)\n- [ ] `Qwen2_5OmniThinkerForConditionalGeneration` (Qwen2.5-Omni) #4969\n- [ ] `SkyworkR1VChatModel` (Skywork-R1V) #4692\n- [ ] `SmolVLMForConditionalGeneration` (SmolVLM2)\n- [ ] `TarsierForConditionalGeneration` (Tarsier)\n- [ ] `Tarsier2ForConditionalGeneration` (Tarsier2)\n\n---\n**Related Issues & PRs**\n\n- Support TRI-ML/prismatic-vlms: #1129\n- facebook/contriever support: #3720\n- Support Gemma 3 QAT models: #5591\n- Bytedancer: #6724",
    "labels": [
      "good first issue",
      "new-model"
    ],
    "state": "open",
    "created_at": "2025-06-21T22:18:10+00:00",
    "closed_at": null,
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7429/reactions",
      "total_count": 15,
      "+1": 10,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 5,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7429"
  },
  {
    "number": 6724,
    "title": "[Feature]  It is hoped that the deployment of the ByteDance-Seed/BAGEL-7B-MoT model can be supported",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nIt is hoped that support can be provided for the ByteDance-Seed/BAGEL-7B-MoT model.\n\n### Related resources\n\n_No response_",
    "labels": [
      "new-model"
    ],
    "state": "open",
    "created_at": "2025-05-29T03:32:37+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6724/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6724"
  },
  {
    "number": 6517,
    "title": "[Feature] Model FalconH1",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nPlease support the Falcon H1 model series. Support is already available in vLLM and transformers!\n\n### Related resources\n\nhttps://falcon-lm.github.io/blog/falcon-h1/\nhttps://github.com/vllm-project/vllm/pull/18406\nhttps://github.com/huggingface/transformers/pull/38249",
    "labels": [
      "new-model"
    ],
    "state": "open",
    "created_at": "2025-05-22T05:00:53+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6517/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6517"
  },
  {
    "number": 6498,
    "title": "[Feature] Gemma 3n support",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nHello, kindly add support for Gemma 3n model. This model only runs on edge devices. it will be good, if we use this model through sglang framework.\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority",
      "new-model"
    ],
    "state": "closed",
    "created_at": "2025-05-21T09:35:34+00:00",
    "closed_at": "2025-07-01T07:40:13+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6498/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6498"
  },
  {
    "number": 6374,
    "title": "[Feature] When will the GLM4-32B series be supported\uff1f",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nWhen the GLM4-32B family will be supported in sglang? And which parser will be used in tool calls\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "new-model"
    ],
    "state": "closed",
    "created_at": "2025-05-17T14:01:12+00:00",
    "closed_at": "2025-06-26T01:33:13+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6374/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6374"
  },
  {
    "number": 5832,
    "title": "[Feature] GLM-4-0414 support?",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nGLM-4-0414 support is on [VLLM](https://github.com/vllm-project/vllm/pull/16338), does it hard to include support of GLM-4-0414 on sglang.\n\nIt's currently the best no-reason 32b model for some tasks such as one-shoot code generations.\n\n### Related resources\n\n_No response_",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2025-04-28T11:26:34+00:00",
    "closed_at": "2025-06-21T22:10:15+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5832/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5832"
  },
  {
    "number": 5591,
    "title": "[Feature]  Support Gemma 3 QAT models",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nHello SGLang team,\n\nCould you please add support for the quantization-aware training models of Google's Gemma 3? Thanks!\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority",
      "new-model"
    ],
    "state": "open",
    "created_at": "2025-04-21T05:53:29+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5591/reactions",
      "total_count": 13,
      "+1": 13,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5591"
  },
  {
    "number": 5441,
    "title": "[Bug] ValueError: Model architectures ['Glm4ForCausalLM'] are not supported for now.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nValueError: Model architectures ['Glm4ForCausalLM'] are not supported for now. \n\n```\nValueError: Model architectures ['Glm4ForCausalLM'] are not supported for now. Supported architectures: dict_keys(['BaichuanForCausalLM', 'ChatGLMModel', 'CLIPModel', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'DbrxForCausalLM', 'DeepseekForCausalLM', 'MultiModalityCausalLM', 'DeepseekV3ForCausalLMNextN', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekVL2ForCausalLM', 'ExaoneForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma2ForSequenceClassification', 'Gemma3ForCausalLM', 'Gemma3ForConditionalGeneration', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GraniteForCausalLM', 'Grok1ForCausalLM', 'Grok1ModelForCausalLM', 'InternLM2ForCausalLM', 'InternLM2ForRewardModel', 'LlamaForCausalLM', 'Phi3ForCausalLM', 'InternLM3ForCausalLM', 'Llama4ForCausalLM', 'LlamaForClassification', 'LlamaForCausalLMEagle', 'LlamaForCausalLMEagle3', 'LlamaEmbeddingModel', 'MistralModel', 'LlamaForSequenceClassification', 'LlamaForSequenceClassificationWithNormal_Weights', 'LlavaLlamaForCausalLM', 'LlavaQwenForCausalLM', 'LlavaMistralForCausalLM', 'LlavaVidForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniCPMO', 'MiniCPMV', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MllamaForConditionalGeneration', 'Llama4ForConditionalGeneration', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'Phi3SmallForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2ForSequenceClassification', 'Qwen2ForCausalLMEagle', 'Qwen2MoeForCausalLM', 'Qwen2ForRewardModel', 'Qwen2VLForConditionalGeneration', 'StableLmForCausalLM', 'TorchNativeLlamaForCausalLM', 'TorchNativePhi3ForCausalLM', 'XverseForCausalLM', 'XverseMoeForCausalLM', 'YiVLForCausalLM'])\n```\n\n\n\n### Reproduction\n\nsglang version\n```\nsglang                            0.4.5\ntransformers                      4.51.3\n```\n\n\n### Environment\n\n\u65e0",
    "labels": [
      "new-model"
    ],
    "state": "open",
    "created_at": "2025-04-16T01:49:12+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5441/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5441"
  },
  {
    "number": 5018,
    "title": "[Feature] Ovis2 surport",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nThe Ovis2 series multimodal model performance is very good, and the gptq int4 version has been released recently, with a wide range of applications.\n[Ovis2-34B](https://huggingface.co/AIDC-AI/Ovis2-34B)\uff0cOvis2-34B\u6027\u80fd\u8d85\u8fc7qwen2.5-vl-72b.\nThe vllm project is expected to support Ovis2 soon. [vllm Ovis2 surport](https://github.com/vllm-project/vllm/pull/15826)\n\n### Related resources\n\n_No response_",
    "labels": [
      "new-model"
    ],
    "state": "open",
    "created_at": "2025-04-03T02:40:00+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5018/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5018"
  },
  {
    "number": 4692,
    "title": "[Feature] Skywork-R1V support",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n Skywork R1V, the first industry open-sourced multimodal reasoning model with advanced visual chain-of-thought capabilities, pushing the boundaries of AI-driven vision and logical inference!\n\n### Related resources\n\nhttps://github.com/SkyworkAI/Skywork-R1V",
    "labels": [
      "new-model"
    ],
    "state": "open",
    "created_at": "2025-03-23T06:57:56+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4692/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4692"
  },
  {
    "number": 4689,
    "title": "[Bug] Testing new Llama-3_3-Nemotron-Super-49B-v1 by Nvidia: \"Model architectures ['DeciLMForCausalLM'] are not supported for now.\"",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI tried to run on SGLang Llama-3_3-Nemotron-Super-49B-v1 recently announced by Nvidia.\n\nIt seems not to be yet supported by SGLang since `DeciLMForCausalLM`is not yet accepted by SGLang. See below.\n\nCan you add corresponding support?\n\n```\nScheduler hit an exception: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 1748, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 218, in __init__\n    self.tp_worker = TpWorkerClass(\n                     ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py\", line 74, in __init__\n    self.model_runner = ModelRunner(\n                        ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 166, in __init__\n    self.initialize(min_per_gpu_memory)\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 176, in initialize\n    self.load_model()\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 361, in load_model\n    self.model = get_model(\n                 ^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py\", line 22, in get_model\n    return loader.load_model(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py\", line 358, in load_model\n    model = _initialize_model(\n            ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py\", line 137, in _initialize_model\n    model_class, _ = get_model_architecture(model_config)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/model_loader/utils.py\", line 37, in get_model_architecture\n    return ModelRegistry.resolve_model_cls(architectures)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/models/registry.py\", line 65, in resolve_model_cls\n    return self._raise_for_unsupported(architectures)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/models/registry.py\", line 32, in _raise_for_unsupported\n    raise ValueError(\nValueError: Model architectures ['DeciLMForCausalLM'] are not supported for now. Supported architectures: dict_keys(['BaichuanForCausalLM', 'ChatGLMModel', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'DbrxForCausalLM', 'DeepseekForCausalLM', 'MultiModalityCausalLM', 'DeepseekV3ForCausalLMNextN', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'ExaoneForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma2ForSequenceClassification', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GraniteForCausalLM', 'Grok1ForCausalLM', 'Grok1ModelForCausalLM', 'InternLM2ForCausalLM', 'InternLM2ForRewardModel', 'LlamaForCausalLM', 'Phi3ForCausalLM', 'InternLM3ForCausalLM', 'LlamaForClassification', 'LlamaForCausalLMEagle', 'LlamaEmbeddingModel', 'MistralModel', 'LlamaForSequenceClassification', 'LlamaForSequenceClassificationWithNormal_Weights', 'LlavaLlamaForCausalLM', 'LlavaQwenForCausalLM', 'LlavaMistralForCausalLM', 'LlavaVidForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniCPMV', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MllamaForConditionalGeneration', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'Phi3SmallForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2ForCausalLMEagle', 'Qwen2MoeForCausalLM', 'Qwen2ForRewardModel', 'Qwen2VLForConditionalGeneration', 'StableLmForCausalLM', 'TorchNativeLlamaForCausalLM', 'TorchNativePhi3ForCausalLM', 'XverseForCausalLM', 'XverseMoeForCausalLM', 'YiVLForCausalLM'])\n```\n\n### Reproduction\n\nStart SGLang and with `nvidia/Llama-3_3-Nemotron-Super-49B-v1` coming from HuggingFace\nThe message above will appear right after this command\n\n### Environment\n\nAmazon Linux 2023\nSGLang 0.0.4.post1 = last officially published version as of this writing",
    "labels": [
      "enhancement",
      "good first issue",
      "help wanted",
      "new-model"
    ],
    "state": "open",
    "created_at": "2025-03-23T05:40:20+00:00",
    "closed_at": null,
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4689/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4689"
  },
  {
    "number": 4570,
    "title": "[Feature] Support Cohere Command-A (Cohere2ForCausalLM arch)",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nIt would be great to support this new model: https://huggingface.co/CohereForAI/c4ai-command-a-03-2025\n\nWhat's special about this model is that they use an unusual architecture where some layers require sliding windows and some don't:\n\n> The model features three layers with sliding window attention (window size 4096) and RoPE for efficient local context modeling and relative positional encoding. A fourth layer uses global attention without positional embeddings, enabling unrestricted token interactions across the entire sequence.\n\nI've found a `Cohere2ForCausalLM` in this project already but it appears to be a stub that is not implemented yet: https://github.com/sgl-project/sglang/blob/90532b762777302cd46a9a38b667570360661e23/python/sglang/srt/models/commandr.py#L413\n\nI previously attempted to implement this model in TensorRT-LLM ( https://github.com/NVIDIA/TensorRT-LLM/issues/2912 ) but ultimately failed as they do not support layers using sliding windows without forcing a cyclic kv cache which breaks prefix caching, and the code that would need changing to fix it is missing. Extremely frustrating. Will there be better luck in this library?\n\n### Related resources\n\nTransformers impl here: https://github.com/huggingface/transformers/blob/main/src/transformers/models/cohere2/modular_cohere2.py\n\nvLLM impl here (note for some reason they merged the models and added the sliding window support for CohereForCausalLM): https://github.com/vllm-project/vllm/blob/61f412187d972a006aef1653bfe348aeaefb6a0b/vllm/model_executor/models/commandr.py#L336",
    "labels": [
      "new-model"
    ],
    "state": "open",
    "created_at": "2025-03-19T07:08:58+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4570/reactions",
      "total_count": 5,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4570"
  },
  {
    "number": 4414,
    "title": "[Feature] Can you support the VLA series models? For example, openVLA.",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nThe documentation does not support the VLA series large models. Can you support the VLA series models?\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive",
      "new-model"
    ],
    "state": "open",
    "created_at": "2025-03-14T07:00:28+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4414/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4414"
  },
  {
    "number": 4062,
    "title": "[Bug] granite-vision-3.2-2b failing on sglang with \"LlavaNextForConditionalGeneration not supported\"",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHi,\n\nI have successfully run the 3.1 versions of granite models on SGLang project (https://github.com/sgl-project/sglang)\n\nI am now trying to run granite-vision-3.2-2b  \n\nBut it fails, with the messages below: in particular `Model architectures ['LlavaNextForConditionalGeneration'] are not supported for now? `\n\nwill IBM work with SGLang project allow this model to run as well on SGLang to be able to leverage its inference acceleration ? It seems that the collaboration has been working for v3.1. see https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/models/granite.py\n\nNote: it seems to be specific to granite-vision-3.2-2b because granite-3.2-2b-instruct works fine with SGLang\n\nThanks, \nDidier\n\n```\nbash-5.2# python3.12 -m sglang.launch_server --model ibm-granite/granite-vision-3.2-2b --model-path ibm-granite/granite-vision-3.2-2b --port 30000 --host 0.0.0.0 --log-level debug --trust-remote-code --tensor-parallel-size 4 --enable-p2p-check --disable-cuda-graph\nINFO 03-04 09:02:43 __init__.py:190] Automatically detected platform cuda.\n[2025-03-04 09:02:46] Setting Triton cache manager to: sglang.srt.utils:CustomCacheManager\n[2025-03-04 09:02:46] server_args=ServerArgs(model_path='ibm-granite/granite-vision-3.2-2b', tokenizer_path='ibm-granite/granite-vision-3.2-2b', tokenizer_mode='auto', load_format='auto', trust_remote_code=True, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, quantization=None, context_length=None, device='cuda', served_model_name='ibm-granite/granite-vision-3.2-2b', chat_template=None, is_embedding=False, revision=None, skip_tokenizer_init=False, host='0.0.0.0', port=30000, mem_fraction_static=0.85, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=4, stream_interval=1, stream_output=False, random_seed=108653913, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='debug', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='sglang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_draft_model_path=None, speculative_algorithm=None, speculative_num_steps=5, speculative_num_draft_tokens=64, speculative_eagle_topk=8, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=80, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=True, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, return_hidden_states=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=False)\nINFO 03-04 09:02:50 __init__.py:190] Automatically detected platform cuda.\nINFO 03-04 09:02:50 __init__.py:190] Automatically detected platform cuda.\nINFO 03-04 09:02:50 __init__.py:190] Automatically detected platform cuda.\nINFO 03-04 09:02:50 __init__.py:190] Automatically detected platform cuda.\nINFO 03-04 09:02:50 __init__.py:190] Automatically detected platform cuda.\n[2025-03-04 09:02:53 TP0] Init torch distributed begin.\n[2025-03-04 09:02:53 TP0] world_size=4 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:30779 backend=nccl\n[2025-03-04 09:02:53 TP1] Init torch distributed begin.\n[2025-03-04 09:02:53 TP3] Init torch distributed begin.\n[2025-03-04 09:02:53 TP2] Init torch distributed begin.\n[2025-03-04 09:02:53 TP1] world_size=4 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:30779 backend=nccl\n[2025-03-04 09:02:53 TP2] world_size=4 rank=2 local_rank=2 distributed_init_method=tcp://127.0.0.1:30779 backend=nccl\n[2025-03-04 09:02:53 TP3] world_size=4 rank=3 local_rank=3 distributed_init_method=tcp://127.0.0.1:30779 backend=nccl\n[2025-03-04 09:02:53 TP0] Found nccl from library libnccl.so.2\n[2025-03-04 09:02:53 TP2] Found nccl from library libnccl.so.2\n[2025-03-04 09:02:53 TP0] sglang is using nccl==2.21.5\n[2025-03-04 09:02:53 TP2] sglang is using nccl==2.21.5\n[2025-03-04 09:02:53 TP1] Found nccl from library libnccl.so.2\n[2025-03-04 09:02:53 TP3] Found nccl from library libnccl.so.2\n[2025-03-04 09:02:53 TP3] sglang is using nccl==2.21.5\n[2025-03-04 09:02:53 TP1] sglang is using nccl==2.21.5\n[2025-03-04 09:02:53 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n[2025-03-04 09:02:53 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n[2025-03-04 09:02:53 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n[2025-03-04 09:02:53 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n[2025-03-04 09:02:53 TP0] Binding to tcp://127.0.0.1:51275\n[2025-03-04 09:02:53 TP0] Message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<sglang.srt.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f72a6c752e0>, local_subscribe_port=51275, remote_subscribe_port=None)\n[2025-03-04 09:02:53 TP3] Connecting to tcp://127.0.0.1:51275\n[2025-03-04 09:02:53 TP2] Connecting to tcp://127.0.0.1:51275\n[2025-03-04 09:02:53 TP1] Connecting to tcp://127.0.0.1:51275\n[2025-03-04 09:02:54 TP2] Load weight begin. avail mem=21.63 GB\n[2025-03-04 09:02:54 TP0] Load weight begin. avail mem=21.63 GB\n[2025-03-04 09:02:54 TP1] Load weight begin. avail mem=21.63 GB\n[2025-03-04 09:02:54 TP3] Load weight begin. avail mem=21.63 GB\n[2025-03-04 09:02:54 TP0] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 1816, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 240, in __init__\n    self.tp_worker = TpWorkerClass(\n                     ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py\", line 68, in __init__\n    self.model_runner = ModelRunner(\n                        ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 194, in __init__\n    self.load_model()\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 317, in load_model\n    self.model = get_model(\n                 ^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py\", line 22, in get_model\n    return loader.load_model(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py\", line 357, in load_model\n    model = _initialize_model(\n            ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py\", line 136, in _initialize_model\n    model_class, _ = get_model_architecture(model_config)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/model_loader/utils.py\", line 37, in get_model_architecture\n    return ModelRegistry.resolve_model_cls(architectures)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/models/registry.py\", line 65, in resolve_model_cls\n    return self._raise_for_unsupported(architectures)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/models/registry.py\", line 32, in _raise_for_unsupported\n    raise ValueError(\nValueError: Model architectures ['LlavaNextForConditionalGeneration'] are not supported for now. Supported architectures: dict_keys(['BaichuanForCausalLM', 'ChatGLMModel', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'DbrxForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'ExaoneForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma2ForSequenceClassification', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GraniteForCausalLM', 'Grok1ForCausalLM', 'Grok1ModelForCausalLM', 'InternLM2ForCausalLM', 'InternLM2ForRewardModel', 'LlamaForCausalLM', 'Phi3ForCausalLM', 'InternLM3ForCausalLM', 'LlamaForClassification', 'LlamaForCausalLMEagle', 'LlamaEmbeddingModel', 'MistralModel', 'LlamaForSequenceClassification', 'LlamaForSequenceClassificationWithNormal_Weights', 'LlavaLlamaForCausalLM', 'LlavaQwenForCausalLM', 'LlavaMistralForCausalLM', 'LlavaVidForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniCPMV', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MllamaForConditionalGeneration', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'Phi3SmallForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2ForCausalLMEagle', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'StableLmForCausalLM', 'TorchNativeLlamaForCausalLM', 'TorchNativePhi3ForCausalLM', 'XverseForCausalLM', 'XverseMoeForCausalLM', 'YiVLForCausalLM'])\n\n```\n\n### Reproduction\n\nStart SGLang with model Granite with following command line\n\npython3.12 -m sglang.launch_server --model ibm-granite/granite-vision-3.2-2b --model-path ibm-granite/granite-vision-3.2-2b --port 30000 --host 0.0.0.0 --log-level debug --trust-remote-code --tensor-parallel-size 4 --enable-p2p-check --disable-cuda-graph\n\nSimilar command for instruct works fine: \n\npython3.12 -m sglang.launch_server --model ibm-granite/granite-3.2-2b-instruct --model-path ibm-granite/granite-3.2-2b-instruct --port 30000 --host 0.0.0.0 --log-level debug --trust-remote-code --tensor-parallel-size 4 --enable-p2p-check --disable-cuda-graph\n\n\n### Environment\n\nSGLang 0.4.3 containerized in Amazon Linux 2023 and running in an AWS ECS cluster",
    "labels": [
      "inactive",
      "MLLM",
      "new-model"
    ],
    "state": "open",
    "created_at": "2025-03-04T10:05:44+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4062/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4062"
  },
  {
    "number": 3720,
    "title": "[Feature] facebook/contriever support requring",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nI guess facebook/contriever somehow is a popular embedding model so if SGLang supports it, will be very cool.\n\nBut I guess it may be time-consuming to support a model that is not llama-structure, but it is indeed popular in RAG setting.\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "help wanted",
      "new-model"
    ],
    "state": "open",
    "created_at": "2025-02-20T03:55:48+00:00",
    "closed_at": null,
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3720/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3720"
  },
  {
    "number": 2898,
    "title": "[Feature] support MiniMax",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nref https://github.com/MiniMax-AI/MiniMax-01\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "help wanted",
      "high priority",
      "new-model"
    ],
    "state": "open",
    "created_at": "2025-01-15T06:36:10+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2898/reactions",
      "total_count": 18,
      "+1": 18,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/2898"
  },
  {
    "number": 1190,
    "title": "[Feature] Jamba 1.5 Support PLS",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nFirst SOTA ssm based model, vllm currently supports it but there is some parallel work in vllm to optimise it aswell\r\n- https://github.com/vllm-project/vllm/pull/7428\r\n- https://github.com/vllm-project/vllm/pull/7651\r\n\r\nhttps://huggingface.co/collections/ai21labs/jamba-15-66c44befa474a917fcf55251\n\n### Related resources\n\nvllm implementation\r\nhttps://github.com/vllm-project/vllm/pull/4115",
    "labels": [
      "good first issue",
      "new-model"
    ],
    "state": "open",
    "created_at": "2024-08-23T09:49:47+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1190/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1190"
  },
  {
    "number": 1129,
    "title": "[Feature] Support TRI-ML/prismatic-vlms",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nI'm trying to speed up inference for new VLM models on huggingface: https://huggingface.co/TRI-ML/prismatic-vlms/tree/main. I'm wondering if there are additional documentation on how to adapt new models? \n\n### Related resources\n\nThe model I'm trying to adapt is detailed here: https://arxiv.org/pdf/2402.07865. ",
    "labels": [
      "good first issue",
      "feature",
      "new-model"
    ],
    "state": "open",
    "created_at": "2024-08-16T18:15:10+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1129/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1129"
  },
  {
    "number": 1108,
    "title": "[Feature] Do we have any plan for supporting Phi3V?",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nDo we have any plan for supporting Phi3V?\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "new-model"
    ],
    "state": "open",
    "created_at": "2024-08-15T05:03:47+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1108/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1108"
  }
]
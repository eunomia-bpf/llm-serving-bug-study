[
  {
    "number": 3568,
    "title": "how to use fp8 for inference on h20?",
    "body": "I have a problem, that is, the model I am currently deploying is: neuralmagic/DeepSeek-R1-Distill-Llama-70B-FP8-dynamic.\nThe graphics card is h20.\n\nI would like to ask how to infer the fp8 capability of this graphics card?\nCurrently, sglang is used for deployment. The deployment instructions are as follows:\n\npython -m sglang.launch_server --model-path neuralmagic/DeepSeek-R1-Distill-Qwen-7B-FP8-dynamic --port 30000 --host 0.0.0.0 --tp 2 \n\nWhat I want to know is that my command has enabled fp8 for inference operations? If not, can you tell me how to do it? Thanks",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-02-14T05:59:47+00:00",
    "closed_at": "2025-04-16T00:18:29+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3568/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3568"
  },
  {
    "number": 3923,
    "title": "[Feature] Add a hash for each new release",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n## Summary\nImplement SHA-256 hash verification for all package releases to enhance security for users installing from mirror sites.\n\n## Description\nUsers who download packages from mirror sites instead of the official PyPI repository need a reliable way to verify package integrity. Adding SHA-256 hashes for each release would provide a standard method to confirm packages haven't been tampered with or corrupted.\n\n## Implementation\n- Generate SHA-256 hashes automatically as part of the CI/CD pipeline\n- Include hashes in package metadata files\n- Make hashes accessible through the official website\n- Update documentation to explain the verification process\n\n## Benefits\n- Enhanced security for users with limited access to official repositories\n- Protection against supply chain attacks through compromised mirrors\n\n## Technical Considerations\n- Minimal changes required to existing build processes\n- Can be applied retroactively to previous releases\n- Leverages pip's existing hash verification mechanisms\n- Low maintenance overhead once implemented\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-02-27T08:39:19+00:00",
    "closed_at": "2025-04-30T00:18:49+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3923/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3923"
  },
  {
    "number": 5498,
    "title": "[Bug] Qwen-gme embedding model: cannot get fused embedding from text+image, and image input format may be incorrect",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThanks for the great work!\n\nWhile using the /v1/embeddings endpoint with the gme-qwen2-vl model, I encountered two issues:\n\n**1. Incorrect handling of image input**\nAccording to the docs, the image input is passed like this:\npayload = {\n    \"model\": \"gme-qwen2-vl\",\n    \"input\": [\n        {\"type\": \"text\", \"text\": text_input},\n        {\"type\": \"image\", \"url\": \"image_path\"},\n    ],\n}\nHowever, this does not seem to be properly recognized. The server throws the same results when input different image_path unless \"url\": \"image_path\" is replaced with \"image\": \"image_path\"\n\n**2. No fused embedding returned for multimodal input**\nWhen sending both text and image in the input, the server currently returns separate embeddings for each (i.e., a text embedding and an image embedding), but not the fused multimodal embedding as described in the Qwen-gme official documentation.\n\n\n\n### Reproduction\n\nhttps://github.com/sgl-project/sglang/blob/main/examples/runtime/multimodal_embedding.py\nqwen-gme embedding model\n\n### Environment\n\nPython: 3.11.2 (main, May  2 2024, 11:59:08) [GCC 12.2.0]\nCUDA available: True\nGPU 0: NVIDIA A800-SXM4-40GB\nGPU 0 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.161.08\nPyTorch: 2.5.1\nsglang: 0.4.5\nsgl_kernel: 0.0.8\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.51.0\ntorchao: 0.10.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.5\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 24.1\npsutil: 6.1.1\npydantic: 2.10.2\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.29.0\nuvloop: 0.21.0\nvllm: 0.7.0\nxgrammar: 0.1.17\nopenai: 1.75.0\ntiktoken: 0.7.0\nanthropic: 0.49.0\nlitellm: 1.66.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    NIC0    NIC1    NIC2    NIC3    NIC4    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     SYS     SYS     NODE    PIX     60-119  1               N/A\nNIC0    SYS      X      SYS     SYS     SYS     SYS\nNIC1    SYS     SYS      X      NODE    SYS     SYS\nNIC2    SYS     SYS     NODE     X      SYS     SYS\nNIC3    NODE    SYS     SYS     SYS      X      NODE\nNIC4    PIX     SYS     SYS     SYS     NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n\n\nHypervisor vendor: KVM\nulimit soft: 1024768",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-04-17T13:09:28+00:00",
    "closed_at": "2025-07-11T00:20:24+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5498/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5498"
  },
  {
    "number": 2671,
    "title": "[Bug] HuggingFace and SGLang inference don't match",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [ ] 2. The bug has not been fixed in the latest version.\r\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [ ] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nThe accuracy of the model is degraded due to inconsistent outputs from SGLang. While HF and vLLM produce consistent results such as \"A\" or \"B,\" SGLang occasionally outputs responses like \"I can't process that request.\" or \"A.\" / \"B.\" This inconsistency impacts overall accuracy.\r\n\r\n### Reproduction\r\n\r\nWhat command or script did you run?\r\nA script for generating outputs using a LLaMA 3.1 8B Istruct model with LoRA.\r\n\r\nWhich model are you using?\r\nLLaMA 3.1 with LoRA applied.\r\n\r\nSteps to reproduce:\r\n\r\n1. Run the script with HF, vLLM, and SGLang configurations.\r\n2. Compare the single-token outputs between the frameworks.\r\n3. Observe the inconsistent behavior in SGLang.\r\n\r\n------------------------------------------------------------------------------------------------------------------------\r\n**Hugging Face Code Snippet :** \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\nmodel = LlamaForCausalLM.from_pretrained(\r\n    model_name,\r\n    load_in_8bit=False,\r\n    torch_dtype=torch.float16,\r\n    device_map='auto',\r\n)\r\nadaptor_path = './model_spec/checkpoints/checkpoint-200-vllm'\r\nmodel = PeftModel.from_pretrained(\r\n    model,\r\n    adaptor_path,\r\n    torch_dtype=torch.float16,\r\n)\r\n\r\nmodel.config.pad_token_id = tokenizer.pad_token_id = 0\r\nmodel.config.bos_token_id = 1\r\nmodel.config.eos_token_id = 2\r\nmodel.generation_config.pad_token_id = tokenizer.pad_token_id\r\nmodel.eval()\r\n\r\n\r\ndef evaluate(\r\n    instruction,\r\n    input=None,\r\n    temperature=0,\r\n    top_p=1,\r\n    top_k=-1,\r\n    num_beams=4,\r\n    max_new_tokens=128,\r\n    stream_output=False,\r\n    **kwargs,\r\n):\r\n    prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\r\n\r\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\r\n    input_ids = inputs[\"input_ids\"].to('cuda')\r\n    generation_config = GenerationConfig(\r\n        temperature=temperature,\r\n        top_p=top_p,\r\n        top_k=top_k,\r\n        num_beams=num_beams,\r\n        **kwargs,\r\n    )\r\n\r\n    with torch.no_grad():\r\n        generation_output = model.generate(\r\n            input_ids=input_ids,\r\n            generation_config=generation_config,\r\n            return_dict_in_generate=True,\r\n            output_scores=True,\r\n            max_new_tokens=max_new_tokens\r\n        )\r\n    s = generation_output.sequences[0]\r\n    output = tokenizer.decode(s, skip_special_tokens=True)\r\n    result = output.split('assistant')[1].strip()\r\n    return result\r\n\r\n------------------------------------------------------------------------------------------------------------------------\r\n**SGLang Code Snippet :** \r\n\r\nimport sglang as sgl\r\nfrom sglang import *\r\nimport logging\r\nimport json\r\nimport torch\r\n\r\nfrom typing import Union, List\r\nfrom vllm import LLM, SamplingParams\r\nfrom vllm.inputs import TokensPrompt\r\nfrom vllm.lora.request import LoRARequest\r\nfrom collections import defaultdict\r\n\r\nfrom utils.usecase_prompts import UseCasePrompter\r\nfrom utils.lora_adapters import UseCaseLoraAdapters\r\nfrom utils.prompter import Prompter\r\n\r\nlogging.basicConfig(format='%(asctime)s %(message)s')\r\nlogger = logging.getLogger()\r\nlogger.setLevel(logging.INFO)\r\n\r\nclass SimpleSGLangLlama2:\r\n    def __init__(self, base_model_path, number_of_gpu=1, gpu_memory_utilization=0.4):\r\n        self.base_model_path = base_model_path\r\n        self._model = sgl.Engine(model_path=self.base_model_path)\r\n\r\n    def generate(\r\n        self,\r\n        prompt: Union[str, List[int]],\r\n        temperature: float = 0.0,\r\n        top_p: float = 1.0,\r\n        top_k: int = -1,\r\n        use_beam_search: bool = True,\r\n        max_new_tokens: int = 128,\r\n        best_of: int = 4\r\n    ) -> List[str]:\r\n        sampling_params = {\"temperature\": temperature, \"top_p\": top_p, \"top_k\":top_k, \"max_new_tokens\": max_new_tokens}\r\n        # Build final_prompt_text, then:\r\n        outputs = self._model.generate(\r\n            [final_prompt_text],\r\n            sampling_params,\r\n            lora_path=adaptor_path\r\n        )\r\n        results = [output['text'] for output in outputs]\r\n        return results\r\n\r\nif __name__ == \"__main__\":\r\n    model_path = \"./models/meta-llama/Meta-Llama-3.1-8B-Instruct\"\r\n    llm = SimpleSGLangLlama2(model_path)\r\n\r\n\r\n------------------------------------------------------------------------------------------------------------------------\r\n\r\n\r\n### Environment\r\n\r\nRun the class in notebooks environment.\r\n\r\nSGLang 0.4.0 (with flashinfer 0.1.6+cu121torch2.4)",
    "labels": [
      "bug",
      "inactive",
      "lora"
    ],
    "state": "closed",
    "created_at": "2024-12-30T22:54:09+00:00",
    "closed_at": "2025-05-03T00:18:08+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2671/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2671"
  },
  {
    "number": 5409,
    "title": "[Bug] Auto-truncation still uses full context length instead of (context_length - max_tokens)",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI'm experiencing an issue where prompt auto-truncation doesn't properly account for max_tokens when using the HTTP server, even with allow_auto_truncate=True enabled. This persists after the changes in https://github.com/sgl-project/sglang/pull/4919.\n\n### Reproduction\n\n1.  python -m sglang.launch_server --model-path NousResearch/Hermes-3-Llama-3.2-3B --host 0.0.0.0  --max-total-tokens 7192 --disable-overlap --allow-auto-truncate\n2. Send a request with a prompt exceeding 7192 tokens, and specify max_tokens=100\n3. Observe that truncation occurs at 7192 tokens total (prompt + response) rather than reserving space for max_tokens\n\nExpected Behavior:\u200b\u200b\nTruncation should preserve space for response tokens by truncating the prompt to (context_length - max_tokens) tokens, as implemented in other frameworks like vLLM.\n\n\u200bSuggested Fix:\u200b\u200b\nAdd a truncate_prompt_tokens parameter to the HTTP API request schema to explicitly control this behavior, mirroring [vLLM's implementation](https://github.com/vllm-project/vllm/blob/b590adfdc15fc716f6d120aeefeb587f491f8fce/vllm/entrypoints/openai/protocol.py#L262C5-L262C27\u3002)\n\n\n### Environment\n\nPython: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]\nCUDA available: True\nGPU 0: NVIDIA RTX 6000 Ada Generation\nGPU 0 Compute Capability: 8.9\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\nCUDA Driver Version: 550.54.14\nPyTorch: 2.5.1+cu124\nsglang: 0.4.5\nsgl_kernel: 0.0.8\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.51.0\ntorchao: 0.9.0+cu124\nnumpy: 2.1.2\naiohttp: 3.11.16\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.24.1\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 24.1\npsutil: 7.0.0\npydantic: 2.11.3\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.17\nopenai: 1.72.0\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.65.4.post1\ndecord: 0.6.0\nNVIDIA Topology:\nGPU0 NIC0 CPU Affinity NUMA Affinity GPU NUMA ID\nGPU0 X SYS 44-65 1 N/A\nNIC0 SYS X\n\nLegend:\n\nX = Self\nSYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\nNODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\nPHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\nPXB = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\nPIX = Connection traversing at most a single PCIe bridge\nNV# = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\nNIC0: mlx5_bond_0\n\nulimit soft: 1048576\n\n",
    "labels": [
      "good first issue",
      "inactive"
    ],
    "state": "open",
    "created_at": "2025-04-15T07:33:29+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5409/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5409"
  },
  {
    "number": 2659,
    "title": "[Feature] Clear PAT_TOKEN in CI",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n![image](https://github.com/user-attachments/assets/d62f4957-2802-4068-9c16-fbcaee2584f4)\r\n\r\n@shuaills Would you like to take this? Pretty easy.\n\n### Related resources\n\n_No response_",
    "labels": [
      "documentation",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-12-30T07:44:56+00:00",
    "closed_at": "2025-03-01T00:18:50+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2659/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2659"
  },
  {
    "number": 4221,
    "title": "[Feature] SGLang Support for TileLang",
    "body": "We recently came across an interesting project: [TileLang](https://github.com/tile-ai/tilelang). It appears to offer significant advantages over Triton in many cases while maintaining a clean dataflow and simple syntax.\n\nDo we have any plans to support a TileLang backend in SGLang?\n\nFor instance, TileLang has demonstrated up to **5x speedup** over Triton\u2019s Flash MLA implementations on H100, with a kernel implementation of just **80 lines of code (see document:** https://github.com/tile-ai/tilelang/tree/main/examples/deepseek_mla). Given these promising results, it would be valuable to explore its potential integration.\n\nWould love to hear thoughts on this!\n",
    "labels": [
      "help wanted",
      "high priority",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-09T05:34:49+00:00",
    "closed_at": "2025-05-27T00:18:53+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4221/reactions",
      "total_count": 9,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 5,
      "eyes": 4
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4221"
  },
  {
    "number": 1036,
    "title": "[Feature] Allow arbitrary logit processors",
    "body": "### Motivation\r\n\r\nThere's some great projects out there that modify logits, mostly for guided decoding or novel sampling techniques. Supporting every single one of them will cause too much bloat and distraction, but if SGLang were to allow arbitrary logit processors then the community can plug and play their own processors.\r\n\r\nFor example, I would have interest in using [https://github.com/noamgat/lm-format-enforcer](lm format enforcer) because it allows for optional JSON fields and recursive classes (unlike outlines). The API of lm format enforcer is also clean and simple and it is simple to make custom parsers for other formats than JSON (e.g. SQL).\r\n\r\nOne way I would imagine the API to work is:\r\n\r\n```python\r\ndef my_logits_processor(inputs: list[int], logits: torch.Tensor) -> torch.Tensor:\r\n   ...\r\n\r\n\r\n@sgl.function\r\ndef character_gen(s, name):\r\n    s += name + \" is a character in Harry Potter. Please fill in the following information about this character.\\n\"\r\n    s += sgl.gen(\"output\", logits_processor: my_logits_processor)\r\n```\r\n\r\nI'm not familiar with the internals of SGLang at all, so I am just throwing out the idea of supporting an async logits processor. Often we only care about logits masks that can already be calculated without knowing the scores yet. This would be more efficient as the CPU can calculate the masks while the GPU runs the model. Right now, the lack of such implementation makes logit processors a performance bottleneck in vLLM. \r\n\r\nAn async logit processor could simply work like this:\r\n```python\r\nasync def my_logits_processor(inputs: list[int]) -> AsyncGenerator[torch.Tensor, torch.Tensor]:\r\n   # All the preprocessing steps here to calculate the mask in parallel\r\n\r\n   logits: torch.tensor = yield\r\n\r\n   # Apply the mask to the logits here to calculate the new logits\r\n   yield new_logits  \r\n```\r\nOf course, the async approach would only work if the model's calculations and the logits processor do not run from the same python process. I'm not sure if this would be the case in SGLang's server implementation.\r\n\r\nThe added benefit of integrating it in SGLang over other inference systems is the ability to easily enable logits processors for only certain sections of the generated output. \r\n\r\n### Related resources\r\n\r\n_No response_",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-08-11T19:34:38+00:00",
    "closed_at": "2024-10-21T01:13:28+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1036/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1036"
  },
  {
    "number": 299,
    "title": "aglang",
    "body": "I test yi-vl-6B with `srt_example_yi_vl.py`\r\nget error:\r\n```\r\nAttributeError: 'TokenizerManager' object has no attribute 'executor\r\n```",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-03-14T09:10:13+00:00",
    "closed_at": "2024-07-25T06:32:43+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/299/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/299"
  },
  {
    "number": 1792,
    "title": "[Bug] Got error with awq_marlin quantization args.",
    "body": "### Checklist\r\n\r\n- [x]  I have searched related issues but cannot get the expected help.\r\n\r\n- [x]  The bug has not been fixed in the latest version.\r\n\r\n- [x]  Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n\r\n- [x]  If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n\r\n \r\n\r\n- [x] Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nI used the AutoAWQ tool to quantize [Deepseek-V2](https://huggingface.co/deepseek-ai/DeepSeek-V2) model . The quantization script is as follows, resulting in a quantized network. I expect to obtain a model in awq_marlin quantization format.\r\n```\r\nfrom awq import AutoAWQForCausalLM\r\nfrom transformers import AutoTokenizer\r\n\r\n\r\nmodel_path = 'path/to/Deepseek-V2'\r\nquant_path = 'path/to/Deepseek-V2_marlin'\r\nquant_config = { \"zero_point\": False, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"Marlin\" }\r\n\r\n# Load model\r\nmodel = AutoAWQForCausalLM.from_pretrained(\r\n    model_path, **{\"low_cpu_mem_usage\": True, \"use_cache\": False}\r\n)\r\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\r\n\r\n# Quantize\r\nmodel.quantize(tokenizer, quant_config=quant_config)\r\n\r\n# Save quantized model\r\nmodel.save_quantized(quant_path)\r\ntokenizer.save_pretrained(quant_path)\r\n\r\nprint(f'Model is quantized and saved at \"{quant_path}\"')\r\n\r\n```\r\nThe config.json corresponding to the quantized model is as follows.\r\n```\r\n{\r\n  \"_name_or_path\": \"/path/to/Deepseek-V2\",\r\n  \"architectures\": [\r\n    \"DeepseekV2ForCausalLM\"\r\n  ],\r\n  \"attention_bias\": false,\r\n  \"attention_dropout\": 0.0,\r\n  \"auto_map\": {\r\n    \"AutoConfig\": \"configuration_deepseek.DeepseekV2Config\",\r\n    \"AutoModel\": \"modeling_deepseek.DeepseekV2Model\",\r\n    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV2ForCausalLM\"\r\n  },\r\n  \"aux_loss_alpha\": 0.001,\r\n  \"bos_token_id\": 100000,\r\n  \"eos_token_id\": 100001,\r\n  \"ep_size\": 1,\r\n  \"first_k_dense_replace\": 1,\r\n  \"hidden_act\": \"silu\",\r\n  \"hidden_size\": 5120,\r\n  \"initializer_range\": 0.02,\r\n  \"intermediate_size\": 12288,\r\n  \"kv_lora_rank\": 512,\r\n  \"max_position_embeddings\": 163840,\r\n  \"model_type\": \"deepseek_v2\",\r\n  \"moe_intermediate_size\": 1536,\r\n  \"moe_layer_freq\": 1,\r\n  \"n_group\": 8,\r\n  \"n_routed_experts\": 80,\r\n  \"n_shared_experts\": 2,\r\n  \"norm_topk_prob\": false,\r\n  \"num_attention_heads\": 128,\r\n  \"num_experts_per_tok\": 6,\r\n  \"num_hidden_layers\": 60,\r\n  \"num_key_value_heads\": 128,\r\n  \"pretraining_tp\": 1,\r\n  \"q_lora_rank\": 1536,\r\n  \"qk_nope_head_dim\": 128,\r\n  \"qk_rope_head_dim\": 64,\r\n  \"quantization_config\": {\r\n    \"bits\": 4,\r\n    \"group_size\": 128,\r\n    \"modules_to_not_convert\": null,\r\n    \"quant_method\": \"awq\",\r\n    \"version\": \"marlin\",\r\n    \"zero_point\": false\r\n  },\r\n  \"rms_norm_eps\": 1e-06,\r\n  \"rope_scaling\": {\r\n    \"beta_fast\": 32,\r\n    \"beta_slow\": 1,\r\n    \"factor\": 40,\r\n    \"mscale\": 0.707,\r\n    \"mscale_all_dim\": 0.707,\r\n    \"original_max_position_embeddings\": 4096,\r\n    \"type\": \"yarn\"\r\n  },\r\n  \"rope_theta\": 10000,\r\n  \"routed_scaling_factor\": 16.0,\r\n  \"scoring_func\": \"softmax\",\r\n  \"seq_aux\": true,\r\n  \"tie_word_embeddings\": false,\r\n  \"topk_group\": 3,\r\n  \"topk_method\": \"group_limited_greedy\",\r\n  \"torch_dtype\": \"float16\",\r\n  \"transformers_version\": \"4.45.2\",\r\n  \"use_cache\": false,\r\n  \"v_head_dim\": 128,\r\n  \"vocab_size\": 102400\r\n}\r\n```\r\nThen, I used SGLang to run quantized model with the following command. \r\n```\r\npython -m sglang.launch_server --trust-remote-code --model-path $MODEL_PATH --port $SERVER_PORT --quantization awq_marlin --tp 4 --mem-fraction-static 0.9\r\n```\r\nAnd got the error\r\n```\r\n[2024-10-25 18:12:30 TP0] Traceback (most recent call last):\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 1115, in run_scheduler_process\r\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank)\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 146, in __init__\r\n    self.tp_worker = TpModelWorker(\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 58, in __init__\r\n    self.model_runner = ModelRunner(\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 147, in __init__\r\n    self.load_model()\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 234, in load_model\r\n    self.vllm_model_config = VllmModelConfig(\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/vllm/config.py\", line 227, in __init__\r\n    self._verify_quantization()\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/vllm/config.py\", line 296, in _verify_quantization\r\n    raise ValueError(\r\nValueError: Quantization method specified in the model config (awq) does not match the quantization method specified in the `quantization` argument (awq_marlin).\r\n\r\n```\r\nIf I change quantization_config to `--quantization awq` ,  also got error.\r\n```\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 1115, in run_scheduler_process\r\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank)\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 146, in __init__\r\n    self.tp_worker = TpModelWorker(\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 58, in __init__\r\n    self.model_runner = ModelRunner(\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 147, in __init__\r\n    self.load_model()\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 251, in load_model\r\n    self.model = get_model(\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 19, in get_model\r\n    return loader.load_model(model_config=model_config,\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 341, in load_model\r\n    model = _initialize_model(model_config, self.load_config,\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 170, in _initialize_model\r\n    return build_model(\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 155, in build_model\r\n    return model_class(config=hf_config,\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/models/deepseek_v2.py\", line 648, in __init__\r\n    self.model = DeepseekV2Model(config, cache_config, quant_config)\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/models/deepseek_v2.py\", line 608, in __init__\r\n    [\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/models/deepseek_v2.py\", line 609, in <listcomp>\r\n    DeepseekV2DecoderLayer(\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/models/deepseek_v2.py\", line 551, in __init__\r\n    self.mlp = DeepseekV2MoE(config=config, quant_config=quant_config)\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/sglang/srt/models/deepseek_v2.py\", line 113, in __init__\r\n    self.experts = FusedMoE(\r\n  File \"/opt/conda/envs/sglang_py310/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 192, in __init__\r\n    assert self.quant_method is not None\r\nAssertionError\r\n```\r\n So how to run an awq_marlin or marlin quantized model with SGLang?\r\n\r\n### Reproduction\r\n\r\n1.  Quant the Deepseek-V2 model; In fact you can use small model to reproduce;\r\n\r\n2. Run quantization model with SGLang.\r\n\r\n\r\n### Environment\r\n\r\n```\r\npython -m sglang.check_env\r\nPython: 3.10.15 | packaged by conda-forge | (main, Oct 16 2024, 01:24:24) [GCC 13.3.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA H800\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\r\nCUDA Driver Version: 550.90.07\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.3.4\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.45.2\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.26.4\r\naiohttp: 3.10.10\r\nfastapi: 0.115.2\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.26.0\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 11.0.0\r\npsutil: 6.1.0\r\npydantic: 2.9.2\r\nuvicorn: 0.32.0\r\nuvloop: 0.21.0\r\nzmq: 26.2.0\r\nvllm: 0.5.5\r\nmultipart: 0.0.12\r\nopenai: 1.52.0\r\ntiktoken: 0.8.0\r\nanthropic: 0.36.2\r\n\r\nHypervisor vendor: KVM\r\nulimit soft: 1048576\r\n\r\n```",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-10-25T10:21:16+00:00",
    "closed_at": "2024-12-26T00:16:32+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1792/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1792"
  },
  {
    "number": 5642,
    "title": "[Bug] No matching distribution found for sgl-kernel==0.0.9.post2; extra == \"srt\"",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nfollowed same steps from: https://docs.sglang.ai/start/install.html#method-2-from-source\n\n### Reproduction\n\nsame steps and I got:\n\nObtaining file:///D:/ia/sglang/python\n  Installing build dependencies ... done\n  Checking if build backend supports build_editable ... done\n  Getting requirements to build editable ... done\n  Preparing editable metadata (pyproject.toml) ... done\nCollecting aiohttp (from sglang==0.4.5.post3)\n  Using cached aiohttp-3.11.18-cp313-cp313-win_amd64.whl.metadata (8.0 kB)\nRequirement already satisfied: requests in c:\\users\\celso\\appdata\\roaming\\python\\python313\\site-packages (from sglang==0.4.5.post3) (2.32.3)\nRequirement already satisfied: tqdm in c:\\users\\celso\\appdata\\roaming\\python\\python313\\site-packages (from sglang==0.4.5.post3) (4.67.1)\nRequirement already satisfied: numpy in c:\\program files\\python313\\lib\\site-packages (from sglang==0.4.5.post3) (2.2.4)\nCollecting IPython (from sglang==0.4.5.post3)\n  Using cached ipython-9.1.0-py3-none-any.whl.metadata (4.4 kB)\nCollecting setproctitle (from sglang==0.4.5.post3)\n  Using cached setproctitle-1.3.5-cp313-cp313-win_amd64.whl.metadata (10 kB)\nCollecting aiohappyeyeballs>=2.3.0 (from aiohttp->sglang==0.4.5.post3)\n  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.1.2 (from aiohttp->sglang==0.4.5.post3)\n  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\nCollecting attrs>=17.3.0 (from aiohttp->sglang==0.4.5.post3)\n  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\nCollecting frozenlist>=1.1.1 (from aiohttp->sglang==0.4.5.post3)\n  Using cached frozenlist-1.6.0-cp313-cp313-win_amd64.whl.metadata (16 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp->sglang==0.4.5.post3)\n  Using cached multidict-6.4.3-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\nCollecting propcache>=0.2.0 (from aiohttp->sglang==0.4.5.post3)\n  Using cached propcache-0.3.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp->sglang==0.4.5.post3)\n  Using cached yarl-1.20.0-cp313-cp313-win_amd64.whl.metadata (74 kB)\nRequirement already satisfied: colorama in c:\\users\\celso\\appdata\\roaming\\python\\python313\\site-packages (from IPython->sglang==0.4.5.post3) (0.4.6)\nCollecting decorator (from IPython->sglang==0.4.5.post3)\n  Using cached decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)\nCollecting ipython-pygments-lexers (from IPython->sglang==0.4.5.post3)\n  Using cached ipython_pygments_lexers-1.1.1-py3-none-any.whl.metadata (1.1 kB)\nRequirement already satisfied: jedi>=0.16 in c:\\program files\\python313\\lib\\site-packages (from IPython->sglang==0.4.5.post3) (0.19.2)\nCollecting matplotlib-inline (from IPython->sglang==0.4.5.post3)\n  Using cached matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\nCollecting prompt_toolkit<3.1.0,>=3.0.41 (from IPython->sglang==0.4.5.post3)\n  Using cached prompt_toolkit-3.0.51-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: pygments>=2.4.0 in c:\\program files\\python313\\lib\\site-packages (from IPython->sglang==0.4.5.post3) (2.19.1)\nCollecting stack_data (from IPython->sglang==0.4.5.post3)\n  Using cached stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\nCollecting traitlets>=5.13.0 (from IPython->sglang==0.4.5.post3)\n  Using cached traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\celso\\appdata\\roaming\\python\\python313\\site-packages (from requests->sglang==0.4.5.post3) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in c:\\users\\celso\\appdata\\roaming\\python\\python313\\site-packages (from requests->sglang==0.4.5.post3) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\celso\\appdata\\roaming\\python\\python313\\site-packages (from requests->sglang==0.4.5.post3) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\celso\\appdata\\roaming\\python\\python313\\site-packages (from requests->sglang==0.4.5.post3) (2025.1.31)\nRequirement already satisfied: anthropic>=0.20.0 in c:\\users\\celso\\appdata\\roaming\\python\\python313\\site-packages (from sglang==0.4.5.post3) (0.49.0)\nCollecting litellm>=1.0.0 (from sglang==0.4.5.post3)\n  Using cached litellm-1.67.0.post1-py3-none-any.whl.metadata (36 kB)\nRequirement already satisfied: openai>=1.0 in c:\\users\\celso\\appdata\\roaming\\python\\python313\\site-packages (from sglang==0.4.5.post3) (1.71.0)\nRequirement already satisfied: tiktoken in c:\\users\\celso\\appdata\\roaming\\python\\python313\\site-packages (from sglang==0.4.5.post3) (0.9.0)\nINFO: pip is looking at multiple versions of sglang[srt] to determine which version is compatible with other requirements. This could take a while.\nERROR: Could not find a version that satisfies the requirement sgl-kernel==0.0.9.post2; extra == \"srt\" (from sglang[srt]) (from versions: 0.0.1)\nERROR: No matching distribution found for sgl-kernel==0.0.9.post2; extra == \"srt\"\n\n### Environment\n\nwindows 11\npython 3.11",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-04-22T16:43:03+00:00",
    "closed_at": "2025-06-23T00:21:20+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5642/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5642"
  },
  {
    "number": 507,
    "title": "Unable to load 72b llava qwen on 8*A100 40GB",
    "body": "Using the command: `CUDA_VISIBLE_DEVICES=0,1,2,3 python -m sglang.launch_server --model-path lmms-lab/llava-next-72b --tokenizer-path lmms-lab/llavanext-qwen-tokenizer --port=8000 --host=\"0.0.0.0\" --tp-size=4`\r\n\r\nResults in error:\r\n\r\n```\r\ntorch.distributed.DistStoreError: Timed out after 601 seconds waiting for clients. 1/4 clients joined.\r\nInitialization failed. detoken_init_state: init ok\r\n```",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-06-05T23:39:20+00:00",
    "closed_at": "2024-09-28T01:10:44+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/507/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/507"
  },
  {
    "number": 2777,
    "title": "[Bug] Benchmarks with EAGLE-2",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [X] 2. The bug has not been fixed in the latest version.\r\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [x] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nI have tried to use different benchmarks for sglang with EAGLE-2. However, it seems that it cannot work.\r\n\r\n\r\n### Reproduction\r\n\r\npython3 -m sglang.launch_server --model meta-llama/Llama-2-7b-chat-hf  --speculative-algo EAGLE --speculative-draft lmzheng/sglang-EAGLE-llama2-chat-7B --speculative-num-steps 5 --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 --mem-fraction 0.7\r\n\r\n## `multi_turn_chat` benchmark\r\n~/sglang/benchmark/multi_turn_chat$ python3 bench_sglang.py --tokenizer meta-llama/Llama-2-7b-chat-hf --long\r\n\r\n### Result\r\n[2025-01-07 20:24:05] INFO:     127.0.0.1:34396 - \"GET /get_model_info HTTP/1.1\" 200 OK\r\n[2025-01-07 20:24:05 TP0] Prefill batch. #new-seq: 1, #new-token: 313, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\r\n[2025-01-07 20:24:05 TP0] Prefill batch. #new-seq: 1, #new-token: 387, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.06, #running-req: 1, #queue-req: 18\r\n[2025-01-07 20:24:05 TP0] Prefill batch. #new-seq: 1, #new-token: 433, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.13, #running-req: 2, #queue-req: 17\r\n[2025-01-07 20:24:05 TP0] Prefill batch. #new-seq: 1, #new-token: 503, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.21, #running-req: 3, #queue-req: 16\r\n[2025-01-07 20:24:05 TP0] Prefill batch. #new-seq: 1, #new-token: 487, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.30, #running-req: 4, #queue-req: 15\r\n[2025-01-07 20:24:05 TP0] Prefill batch. #new-seq: 1, #new-token: 278, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.39, #running-req: 5, #queue-req: 14\r\n[2025-01-07 20:24:05 TP0] Prefill batch. #new-seq: 1, #new-token: 305, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.44, #running-req: 6, #queue-req: 13\r\n[2025-01-07 20:24:06 TP0] Prefill batch. #new-seq: 1, #new-token: 407, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.50, #running-req: 7, #queue-req: 12\r\n[2025-01-07 20:24:06 TP0] Prefill batch. #new-seq: 1, #new-token: 434, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.57, #running-req: 8, #queue-req: 11\r\n[2025-01-07 20:24:06 TP0] Prefill batch. #new-seq: 1, #new-token: 392, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.65, #running-req: 9, #queue-req: 10\r\n[2025-01-07 20:24:06 TP0] Prefill batch. #new-seq: 1, #new-token: 443, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.73, #running-req: 10, #queue-req: 9\r\n[2025-01-07 20:24:06 TP0] Prefill batch. #new-seq: 1, #new-token: 256, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.81, #running-req: 11, #queue-req: 8\r\n[2025-01-07 20:24:06 TP0] Prefill batch. #new-seq: 1, #new-token: 316, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.85, #running-req: 12, #queue-req: 7\r\n[2025-01-07 20:24:06 TP0] Prefill batch. #new-seq: 1, #new-token: 379, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.91, #running-req: 13, #queue-req: 6\r\n[2025-01-07 20:24:07 TP0] Scheduler hit an exception: Traceback (most recent call last):\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/managers/scheduler.py\", line 1616, in run_scheduler_process\r\n    scheduler.event_loop_normal()\r\n  File \"/opt/conda/envs/rl/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/managers/scheduler.py\", line 411, in event_loop_normal\r\n    batch = self.get_next_batch_to_run()\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/managers/scheduler.py\", line 783, in get_next_batch_to_run\r\n    self.running_batch = self.update_running_batch(self.running_batch)\r\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/managers/scheduler.py\", line 924, in update_running_batch\r\n    self.draft_worker.finish_request(retracted_reqs)\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 163, in finish_request\r\n    - self.finish_extend_len[req.rid]\r\n      ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^\r\nKeyError: '58588572becf4f0997b94a5f2c548495'\r\n\r\nKilled\r\n\r\n## `mtbench` benchmark\r\n~/sglang/benchmark/mtbench$ python3 bench_sglang.py --num-questions 80\r\n\r\n### Result\r\n[2025-01-07 20:16:02] INFO:     127.0.0.1:41452 - \"GET /get_model_info HTTP/1.1\" 200 OK\r\n[2025-01-07 20:16:02 TP0] Prefill batch. #new-seq: 1, #new-token: 48, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\r\n[2025-01-07 20:16:02 TP0] Prefill batch. #new-seq: 1, #new-token: 75, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.01, #running-req: 1, #queue-req: 62\r\n[2025-01-07 20:16:02 TP0] Prefill batch. #new-seq: 1, #new-token: 80, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.02, #running-req: 2, #queue-req: 61\r\n[2025-01-07 20:16:02 TP0] Prefill batch. #new-seq: 1, #new-token: 70, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.04, #running-req: 3, #queue-req: 60\r\n[2025-01-07 20:16:02 TP0] Prefill batch. #new-seq: 1, #new-token: 60, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.05, #running-req: 4, #queue-req: 59\r\n[2025-01-07 20:16:02 TP0] Prefill batch. #new-seq: 1, #new-token: 48, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.06, #running-req: 5, #queue-req: 58\r\n[2025-01-07 20:16:02 TP0] Prefill batch. #new-seq: 1, #new-token: 55, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.07, #running-req: 6, #queue-req: 57\r\n[2025-01-07 20:16:02 TP0] Prefill batch. #new-seq: 1, #new-token: 56, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.08, #running-req: 7, #queue-req: 56\r\n[2025-01-07 20:16:03 TP0] Prefill batch. #new-seq: 1, #new-token: 71, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.09, #running-req: 8, #queue-req: 55\r\n[2025-01-07 20:16:03 TP0] Prefill batch. #new-seq: 1, #new-token: 128, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.10, #running-req: 9, #queue-req: 54\r\n[2025-01-07 20:16:03 TP0] Prefill batch. #new-seq: 1, #new-token: 79, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.13, #running-req: 10, #queue-req: 53\r\n[2025-01-07 20:16:03 TP0] Prefill batch. #new-seq: 1, #new-token: 59, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.14, #running-req: 11, #queue-req: 52\r\n[2025-01-07 20:16:03 TP0] Prefill batch. #new-seq: 1, #new-token: 119, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.15, #running-req: 12, #queue-req: 51\r\n[2025-01-07 20:16:03 TP0] Prefill batch. #new-seq: 1, #new-token: 123, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.17, #running-req: 13, #queue-req: 50\r\n[2025-01-07 20:16:03 TP0] Prefill batch. #new-seq: 1, #new-token: 148, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.20, #running-req: 14, #queue-req: 49\r\n[2025-01-07 20:16:03 TP0] Prefill batch. #new-seq: 1, #new-token: 100, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.22, #running-req: 15, #queue-req: 48\r\n[2025-01-07 20:16:03 TP0] Prefill batch. #new-seq: 1, #new-token: 83, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.24, #running-req: 16, #queue-req: 47\r\n[2025-01-07 20:16:03 TP0] Prefill batch. #new-seq: 1, #new-token: 66, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.26, #running-req: 17, #queue-req: 46\r\n[2025-01-07 20:16:03 TP0] Prefill batch. #new-seq: 1, #new-token: 66, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.27, #running-req: 18, #queue-req: 45\r\n[2025-01-07 20:16:03 TP0] Prefill batch. #new-seq: 1, #new-token: 78, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.28, #running-req: 19, #queue-req: 44\r\n[2025-01-07 20:16:14] INFO:     127.0.0.1:41560 - \"POST /generate HTTP/1.1\" 200 OK\r\n[2025-01-07 20:16:16 TP0] Decode batch. #running-req: 19, #token: 3526, token usage: 0.65, gen throughput (token/s): 2.08, #queue-req: 45\r\n[2025-01-07 20:16:18] INFO:     127.0.0.1:41590 - \"POST /generate HTTP/1.1\" 200 OK\r\n[2025-01-07 20:16:18] INFO:     127.0.0.1:41592 - \"POST /generate HTTP/1.1\" 200 OK\r\n[2025-01-07 20:16:19 TP0] Prefill batch. #new-seq: 1, #new-token: 64, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.63, #running-req: 17, #queue-req: 46\r\n[2025-01-07 20:16:19 TP0] Prefill batch. #new-seq: 1, #new-token: 62, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.64, #running-req: 18, #queue-req: 45\r\n[2025-01-07 20:16:19] INFO:     127.0.0.1:41604 - \"POST /generate HTTP/1.1\" 200 OK\r\n[2025-01-07 20:16:20 TP0] Prefill batch. #new-seq: 1, #new-token: 45, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.62, #running-req: 18, #queue-req: 45\r\n[2025-01-07 20:16:25] INFO:     127.0.0.1:41672 - \"POST /generate HTTP/1.1\" 200 OK\r\n[2025-01-07 20:16:26 TP0] Decode out of memory happened. #retracted_reqs: 1, #new_token_ratio: 0.6348 -> 0.7838\r\n[2025-01-07 20:16:26 TP0] Scheduler hit an exception: Traceback (most recent call last):\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/managers/scheduler.py\", line 1616, in run_scheduler_process\r\n    scheduler.event_loop_normal()\r\n  File \"/opt/conda/envs/rl/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/managers/scheduler.py\", line 419, in event_loop_normal\r\n    result = self.run_batch(batch)\r\n             ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/managers/scheduler.py\", line 966, in run_batch\r\n    self.draft_worker.forward_batch_speculative_generation(batch)\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 73, in forward_batch_speculative_generation\r\n    self.forward_draft_decode(batch)\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 55, in forward_draft_decode\r\n    logits_output = self.model_runner.forward(forward_batch)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/model_executor/model_runner.py\", line 715, in forward\r\n    return self.forward_decode(forward_batch)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/model_executor/model_runner.py\", line 674, in forward_decode\r\n    return self.model.forward(\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/rl/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/models/llama.py\", line 356, in forward\r\n    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/models/llama_eagle.py\", line 95, in forward\r\n    hidden_states, residual = layer(\r\n                              ^^^^^^\r\n  File \"/opt/conda/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/models/llama.py\", line 235, in forward\r\n    hidden_states = self.self_attn(\r\n                    ^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/models/llama.py\", line 172, in forward\r\n    attn_output = self.attn(q, k, v, forward_batch)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/layers/radix_attention.py\", line 65, in forward\r\n    return forward_batch.attn_backend.forward(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/layers/attention/__init__.py\", line 67, in forward\r\n    return self.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/sglang_0105/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py\", line 415, in forward_decode\r\n    o = decode_wrapper.forward(\r\n        ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/rl/lib/python3.11/site-packages/flashinfer/decode.py\", line 589, in forward\r\n    return self.run(\r\n           ^^^^^^^^^\r\n  File \"/opt/conda/envs/rl/lib/python3.11/site-packages/flashinfer/decode.py\", line 673, in run\r\n    out = self._wrapper.run(\r\n          ^^^^^^^^^^^^^^^^^^\r\nRuntimeError: CHECK_GE(paged_kv_indptr.size(0), batch_size + 1) failed. 137 vs 145\r\n\r\nKilled\r\n\r\n### Environment\r\n\r\n2025-01-07 18:47:36.128848: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1736275656.370115    2302 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1736275656.436779    2302 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2025-01-07 18:47:37.039609: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n[2025-01-07 18:47:43] INFO _client.py:1038: HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"\r\n/opt/conda/envs/rl/lib/python3.11/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\r\n* 'fields' has been removed\r\n  warnings.warn(message, UserWarning)\r\nPython: 3.11.10 | packaged by conda-forge | (main, Oct 16 2024, 01:27:36) [GCC 13.3.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA L4\r\nGPU 0 Compute Capability: 8.9\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\r\nCUDA Driver Version: 550.127.05\r\nPyTorch: 2.5.1+cu124\r\nflashinfer: 0.1.6+cu124torch2.4\r\ntriton: 3.1.0\r\ntransformers: 4.47.1\r\ntorchao: 0.7.0\r\nnumpy: 1.26.4\r\naiohttp: 3.11.11\r\nfastapi: 0.115.6\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.27.0\r\ninteregular: 0.3.3\r\nmodelscope: 1.21.1\r\norjson: 3.10.13\r\npackaging: 24.2\r\npsutil: 6.1.0\r\npydantic: 2.10.4\r\nmultipart: 0.0.20\r\nzmq: 26.2.0\r\nuvicorn: 0.34.0\r\nuvloop: 0.21.0\r\nvllm: 0.6.4.post1\r\nopenai: 1.59.3\r\nanthropic: 0.42.0\r\ndecord: 0.6.0\r\nNVIDIA Topology: \r\n        GPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-3     0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nHypervisor vendor: KVM\r\nulimit soft: 1048576",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-01-07T20:25:44+00:00",
    "closed_at": "2025-03-23T00:19:15+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2777/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2777"
  },
  {
    "number": 123,
    "title": "How to use 4bit on LLava?",
    "body": "I am using this code: \"python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.5-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --chat-template vicuna_v1.1 --port 30000\" , but I am not able to use any instruction for quantity 4-bit. \r\n\r\ncan you tell me how to use 4-bit llava on sglang?",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-01-31T03:06:50+00:00",
    "closed_at": "2024-07-25T06:32:04+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/123/reactions",
      "total_count": 5,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/123"
  },
  {
    "number": 309,
    "title": "Can SGlang run on cuda118?",
    "body": "I can't successfully run SGlang using either \"python -m sglang.launch_server --model-path LOCAL_MODEL_PATH --port 30000\" or \"sgl.Runtime(model_path=LOCAL_MODEL_PATH)\", just like the issue mentioned in https://github.com/sgl-project/sglang/issues/199 .\r\n\r\nCould this be related to my CUDA 11.8 installation, or is there another possible reason?\r\n\r\nThank you.",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-03-19T03:20:19+00:00",
    "closed_at": "2024-07-25T06:33:10+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/309/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/309"
  },
  {
    "number": 2181,
    "title": "[Bug] Qwen2-VL-7B IndexError",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nOccasionally, we will see a random \"IndexError\" which crashes sglang when serving Qwen2-VL-7B models. The crash is usually such that sglang will livelock, so the process will not exit, but no new requests will be servable. \r\n\r\nI have tried to rerun the requests again in a local interactive environment, but I cannot get an exact repro case unfortunately.\r\n\r\n```\r\n2024-11-25T12:44:20.292025261Z 2024-11-25 12:44:20,291 - sglang - INFO - Traceback (most recent call last):\r\n2024-11-25T12:44:20.292027162Z 2024-11-25 12:44:20,291 - sglang - INFO -   File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\r\n2024-11-25T12:44:20.292029103Z 2024-11-25 12:44:20,291 - sglang - INFO -     self.run()\r\n2024-11-25T12:44:20.292030905Z 2024-11-25 12:44:20,291 - sglang - INFO -   File \"/usr/lib/python3.11/threading.py\", line 982, in run\r\n2024-11-25T12:44:20.292036434Z 2024-11-25 12:44:20,291 - sglang - INFO -     self._target(*self._args, **self._kwargs)\r\n2024-11-25T12:44:20.292076086Z 2024-11-25 12:44:20,292 - sglang - INFO -   File \"/usr/local/lib/python3.11/dist-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 93, in forward_thread_func\r\n2024-11-25T12:44:20.292096682Z 2024-11-25 12:44:20,292 - sglang - INFO -     self.forward_thread_func_()\r\n2024-11-25T12:44:20.292161264Z 2024-11-25 12:44:20,292 - sglang - INFO -   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n2024-11-25T12:44:20.292182642Z 2024-11-25 12:44:20,292 - sglang - INFO -     return func(*args, **kwargs)\r\n2024-11-25T12:44:20.292204657Z 2024-11-25 12:44:20,292 - sglang - INFO -            ^^^^^^^^^^^^^^^^^^^^^\r\n2024-11-25T12:44:20.292255277Z 2024-11-25 12:44:20,292 - sglang - INFO -   File \"/usr/local/lib/python3.11/dist-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 120, in forward_thread_func_\r\n2024-11-25T12:44:20.292297851Z 2024-11-25 12:44:20,292 - sglang - INFO -     logits_output, next_token_ids = self.worker.forward_batch_generation(\r\n2024-11-25T12:44:20.292338584Z 2024-11-25 12:44:20,292 - sglang - INFO -                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-11-25T12:44:20.292369450Z 2024-11-25 12:44:20,292 - sglang - INFO -   File \"/usr/local/lib/python3.11/dist-packages/sglang/srt/managers/tp_worker.py\", line 147, in forward_batch_generation\r\n2024-11-25T12:44:20.292390642Z 2024-11-25 12:44:20,292 - sglang - INFO -     forward_batch = ForwardBatch.init_new(model_worker_batch, self.model_runner)\r\n2024-11-25T12:44:20.292434844Z 2024-11-25 12:44:20,292 - sglang - INFO -                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-11-25T12:44:20.292452438Z 2024-11-25 12:44:20,292 - sglang - INFO -   File \"/usr/local/lib/python3.11/dist-packages/sglang/srt/model_executor/forward_batch_info.py\", line 266, in init_new\r\n2024-11-25T12:44:20.292496276Z 2024-11-25 12:44:20,292 - sglang - INFO -     ret.compute_mrope_positions(model_runner, batch)\r\n2024-11-25T12:44:20.292534921Z 2024-11-25 12:44:20,292 - sglang - INFO -   File \"/usr/local/lib/python3.11/dist-packages/sglang/srt/model_executor/forward_batch_info.py\", line 190, in compute_mrope_positions\r\n2024-11-25T12:44:20.292546295Z 2024-11-25 12:44:20,292 - sglang - INFO -     MRotaryEmbedding.get_input_positions(\r\n2024-11-25T12:44:20.292570510Z 2024-11-25 12:44:20,292 - sglang - INFO -   File \"/usr/local/lib/python3.11/dist-packages/sglang/srt/layers/rotary_embedding.py\", line 48, in get_input_positions\r\n2024-11-25T12:44:20.292595830Z 2024-11-25 12:44:20,292 - sglang - INFO -     image_grid_thw[image_index][0],\r\n2024-11-25T12:44:20.292621161Z 2024-11-25 12:44:20,292 - sglang - INFO -     ~~~~~~~~~~~~~~^^^^^^^^^^^^^\r\n2024-11-25T12:44:20.292645107Z 2024-11-25 12:44:20,292 - sglang - INFO - IndexError: list index out of range\r\n```\n\n### Reproduction\n\nThis is using v0.3.6 on an H100.\r\n\n\n### Environment\n\n```\r\n/bin/sh: 1: /usr/local/cuda/bin/nvcc: not found\r\nPython: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 0 Compute Capability: 9.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Not Available\r\nCUDA Driver Version: 525.147.05\r\nPyTorch: 2.5.1+cu124\r\nsglang: 0.3.6\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.1.0\r\ntransformers: 4.46.3\r\ntorchao: 0.6.1\r\nnumpy: 1.26.4\r\naiohttp: 3.10.11\r\nfastapi: 0.115.5\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.26.2\r\ninteregular: 0.3.3\r\npsutil: 6.1.0\r\npydantic: 2.10.1\r\nmultipart: 0.0.17\r\nzmq: 26.2.0\r\nuvicorn: 0.32.1\r\nuvloop: 0.21.0\r\nvllm: 0.6.4.post1\r\nopenai: 1.55.0\r\nanthropic: 0.39.0\r\nNVIDIA Topology: \r\n        GPU0    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    CPU Affinity    NUMA Affinity\r\nGPU0     X      PIX     NODE    NODE    NODE    SYS     SYS     0-47,96-143     0\r\nNIC0    PIX      X      NODE    NODE    NODE    SYS     SYS\r\nNIC1    NODE    NODE     X      PIX     NODE    SYS     SYS\r\nNIC2    NODE    NODE    PIX      X      NODE    SYS     SYS\r\nNIC3    NODE    NODE    NODE    NODE     X      SYS     SYS\r\nNIC4    SYS     SYS     SYS     SYS     SYS      X      NODE\r\nNIC5    SYS     SYS     SYS     SYS     SYS     NODE     X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n\r\n\r\nulimit soft: 1048576\r\n```",
    "labels": [
      "bug",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-11-25T17:02:41+00:00",
    "closed_at": "2025-01-31T00:16:28+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2181/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2181"
  },
  {
    "number": 401,
    "title": "Is it possible to define the prompts for KV caching up-front?",
    "body": "For a lot of use cases, there is already a pre-defined system + base prompt that is used.\r\n\r\nCan we define the KV cache for these prompts up front manually? For example, if we are extracting information out of a provided context, the provided context prompt changes but the system + base prompt stays the same. Caching the context will make no sense as it is guaranteed to change on the next inference. ",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-04-29T08:40:04+00:00",
    "closed_at": "2024-07-25T06:33:23+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/401/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/401"
  },
  {
    "number": 2337,
    "title": "[Feature] use SGLang's FusedMoE with quantization",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nref https://github.com/sgl-project/sglang/pull/2300#issuecomment-2514795180\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-12-03T14:55:12+00:00",
    "closed_at": "2025-02-02T00:17:42+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2337/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/2337"
  },
  {
    "number": 6000,
    "title": "[Bug] Tensor model parallel group is not initialized when deploying Qwen3-30B-A3B-AWQ",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHi, SGLang Team,\n\nI am using it to deploy an AWQ quantized model of Qwen3-30B-A3B: swift/Qwen3-30B-A3B-AWQ from modelscope. but encounter the following issue:\n\n```bash\n File \"/home/a/sglang/python/sglang/srt/managers/scheduler.py\", line 2215, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, pp_rank, dp_rank)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/a/sglang/python/sglang/srt/managers/scheduler.py\", line 268, in __init__\n    self.tp_worker = TpWorkerClass(\n                     ^^^^^^^^^^^^^^\n  File \"/home/a/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 64, in __init__\n    self.worker = TpModelWorker(\n                  ^^^^^^^^^^^^^^\n  File \"/home/a/sglang/python/sglang/srt/managers/tp_worker.py\", line 81, in __init__\n    self.model_runner = ModelRunner(\n                        ^^^^^^^^^^^^\n  File \"/home/a/sglang/python/sglang/srt/model_executor/model_runner.py\", line 190, in __init__\n    self.initialize(min_per_gpu_memory)\n  File \"/home/a/sglang/python/sglang/srt/model_executor/model_runner.py\", line 205, in initialize\n    self.load_model()\n  File \"/home/a/sglang/python/sglang/srt/model_executor/model_runner.py\", line 458, in load_model\n    self.model = get_model(\n                 ^^^^^^^^^^\n  File \"/home/a/sglang/python/sglang/srt/model_loader/__init__.py\", line 22, in get_model\n    return loader.load_model(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/a/sglang/python/sglang/srt/model_loader/loader.py\", line 377, in load_model\n    model.load_weights(self._get_all_weights(model_config, model))\n  File \"/home/a/sglang/python/sglang/srt/models/qwen3_moe.py\", line 406, in load_weights\n    weight_loader(\n  File \"/data2/a/miniconda3/envs/cap/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/moe_wna16.py\", line 396, in moe_wna16_weight_loader\n    device = get_tp_group().device\n             ^^^^^^^^^^^^^^\n  File \"/data2/a/miniconda3/envs/cap/lib/python3.11/site-packages/vllm/distributed/parallel_state.py\", line 749, in get_tp_group\n    assert _TP is not None, (\"tensor model parallel group is not initialized\")\n           ^^^^^^^^^^^^^^^\nAssertionError: tensor model parallel group is not initialized\n```\n\n### Reproduction\n\nRunning Command:\n```bash\nmodelscope download swift/Qwen3-30B-A3B-AWQ\n\npython3 -m sglang.launch_server --model-path /home/a/.cache/modelscope/hub/models/swift/Qwen3-30B-A3B-AWQ --attention-backend flashinfer --mem-fraction-static 0.9\n```\n\n### Environment\n\nSglang 0.4.6.post2, 8xH20\n\nPython: 3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.99\nCUDA Driver Version: 535.161.07\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post2\nsgl_kernel: 0.1.1\nflashinfer_python: 0.2.5\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.10.0\nnumpy: 2.2.5\naiohttp: 3.11.18\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.4\npython-multipart: 0.0.20\npyzmq: 26.4.0\nuvicorn: 0.34.2\nuvloop: 0.21.0\nvllm: 0.8.4\nxgrammar: 0.1.18\nopenai: 1.75.0\ntiktoken: 0.9.0\nanthropic: 0.50.0\nlitellm: 1.67.6\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     PHB     NODE    SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    PHB     PIX     NODE    SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    96-191,288-383  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    96-191,288-383  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE    NODE    PIX     PHB     96-191,288-383  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     NODE    NODE    PHB     PIX     96-191,288-383  1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC1    NODE    PIX     PHB     NODE    SYS     SYS     SYS     SYS     NODE     X      PHB     NODE    SYS     SYS     SYS     SYS\nNIC2    NODE    PHB     PIX     NODE    SYS     SYS     SYS     SYS     NODE    PHB      X      NODE    SYS     SYS     SYS     SYS\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PIX     PHB     SYS     SYS     SYS     SYS     NODE    NODE     X      PHB\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    PHB     PIX     SYS     SYS     SYS     SYS     NODE    NODE    PHB      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_bond_0\n  NIC1: mlx5_bond_1\n  NIC2: mlx5_bond_2\n  NIC3: mlx5_bond_3\n  NIC4: mlx5_bond_4\n  NIC5: mlx5_bond_5\n  NIC6: mlx5_bond_6\n  NIC7: mlx5_bond_7\n\n\nulimit soft: 1000000\n",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-05-04T01:08:45+00:00",
    "closed_at": "2025-07-16T00:20:45+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6000/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6000"
  },
  {
    "number": 4562,
    "title": "[Bug] When a node is inaccessible, it will cause the router to crash.",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n**This results in the router being inaccessible, but the health check of /health still returns the correct result. the error like**\n\n`Current running queue: {\"http://localhost:8002\": 7530, \"http://localhost:8000\": 7497, \"http://localhost:8001\": 7495}\n[Router (Rust)] 2025-03-18 14:17:58 - WARN - Generate request to http://localhost:8001 failed (attempt 1/3)\n[Router (Rust)] 2025-03-18 14:17:58 - INFO - Retrying request after 1 failed attempts\n[Router (Rust)] 2025-03-18 14:17:58 - WARN - Generate request to http://localhost:8001 failed (attempt 3/3)\n[Router (Rust)] 2025-03-18 14:17:58 - WARN - Removing failed worker: http://localhost:8001\n[Router (Rust)] 2025-03-18 14:17:58 - INFO - Removed worker: http://localhost:8001\n[Router (Rust)] 2025-03-18 14:17:58 - INFO - Removed worker from tree and cleaned up queues: http://localhost:8001\n[Router (Rust)] 2025-03-18 14:17:58 - INFO - Load balancing triggered due to workload imbalance:\nMax load: 7530, Min load: 7497\nCurrent running queue: {\"http://localhost:8002\": 7530, \"http://localhost:8000\": 7497}\n[Router (Rust)] 2025-03-18 14:17:58 - INFO - Retrying request after 3 failed attempts\n[Router (Rust)] 2025-03-18 14:17:58 - WARN - Generate request to http://localhost:8001 failed (attempt 1/3)\n[Router (Rust)] 2025-03-18 14:17:58 - INFO - Retrying request after 1 failed attempts\n[Router (Rust)] 2025-03-18 14:17:58 - WARN - Generate request to http://localhost:8001 failed (attempt 1/3)\n[Router (Rust)] 2025-03-18 14:17:58 - INFO - Retrying request after 1 failed attempts\n[Router (Rust)] 2025-03-18 14:17:58 - WARN - Generate request to http://localhost:8001 failed (attempt 1/3)\n[Router (Rust)] 2025-03-18 14:17:58 - INFO - Retrying request after 1 failed attempts\n[Router (Rust)] 2025-03-18 14:17:58 - INFO - Load balancing triggered due to workload imbalance:\nMax load: 7529, Min load: 7496\nCurrent running queue: {\"http://localhost:8002\": 7529, \"http://localhost:8000\": 7496}\nthread 'actix-rt|system:0|arbiter:16' panicked at src/router.rs:589:79:\ncalled `Option::unwrap()` on a `None` value\nthread 'actix-rt|system:0|arbiter:84' panicked at src/router.rs:464:62:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:46' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:80' panicked at src/router.rs:588:77:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:10' panicked at src/router.rs:588:77:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:54' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:26' panicked at src/router.rs:588:77:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:79' panicked at src/router.rs:588:77:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:136' panicked at src/router.rs:588:77:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:13' panicked at src/router.rs:588:77:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:50' panicked at src/router.rs:588:77:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:37' panicked at src/router.rs:588:77:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\n\n\n### Reproduction\n\n3 node command: `python -m sglang.launch_server --model-path sophosympatheia/Midnight-Miqu-70B-v1.0 --tp-size 8 --mem-fraction-static .94 --host 0.0.0.0 --port 8000 --attention-backend flashinfer `\n\nrouter command `python -m sglang_router.launch_router --worker-urls http://worker_url_1 http://worker_url_2`\n\n### Environment\n\nrouter machine\uff1a\n\nINFO 03-19 00:54:46 __init__.py:190] Automatically detected platform cuda.\n/home/ubuntu/sglang/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 560.35.05\nPyTorch: 2.5.1+cu124\nsglang: 0.4.3.post4\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.2.post1+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.13\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.3\ninteregular: 0.3.3\nmodelscope: 1.23.2\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.3.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.66.3\ntiktoken: 0.9.0\nanthropic: 0.49.0\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-23,96-119     0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-23,96-119     0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-23,96-119     0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     NODE    NODE    PIX     SYS     SYS     SYS     SYS     24-47,120-143   1               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     PIX     NODE    NODE    SYS     48-71,144-167   2               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     NODE    PIX     NODE    SYS     48-71,144-167   2               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    PIX     SYS     48-71,144-167   2               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     72-95,168-191   3               N/A\nNIC0    PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS      X      NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC1    NODE    PIX     NODE    SYS     SYS     SYS     SYS     SYS     NODE     X      NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC2    NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC3    SYS     SYS     SYS     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     NODE    SYS     SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      NODE    SYS     SYS     SYS     SYS\nNIC5    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      SYS     SYS     SYS     SYS\nNIC6    SYS     SYS     SYS     SYS     PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      NODE    NODE    SYS\nNIC7    SYS     SYS     SYS     SYS     NODE    PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE     X      NODE    SYS\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      SYS\nNIC9    SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n\n\nulimit soft: 1024",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-19T00:58:02+00:00",
    "closed_at": "2025-06-09T00:20:53+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4562/reactions",
      "total_count": 4,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 4
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4562"
  },
  {
    "number": 3198,
    "title": "[Bug] constant errors + hangs using sglang + deepseek v3 + AMD (httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read))",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nMuch of the time it is fine, but there is a abrupt termination of the streaming with:\n```\nhttpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n```\n\nusing the OpenAI API endpoint.  E.g. I see about 250 of those failures over course of 12 hours (even though many more fail because we have 3 retries in exponential backoff).  Interestingly, these events occur in a cluster, suggesting the entire sglang is hung-up with the 8 simultaneous requests.\n\nPerhaps even worse, sometimes the response just gets totally stuck and hangs for an hour.\n\n### Reproduction\n\nimage: lmsysorg/sglang:v0.4.2-rocm620\n\ncommand:\n```\npython3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --host 0.0.0.0 --port 5000 --trust-remote-code  --context-length 65536 --tp 8 --random-seed 1234 --download-dir /root/.cache/huggingface/hub/\n```\n\nThere's no easy repro.  The pattern of usage is ~14k system prompt + query and good number of chat turns afterwards.  Also in some cases large context is filled to do RAG etc.\n\nBut I shared logs.  These are the entire logs from start to finish over which there are these issues.\n\n[logs.zip](https://github.com/user-attachments/files/18579900/logs.zip)\n\n### Environment\n\n```\nroot@ef5e23d28c0e:/sgl-workspace# python3 -m sglang.check_env\n/opt/conda/envs/py_3.9/lib/python3.9/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4)\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\nWARNING 01-28 22:04:48 rocm.py:17] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n/opt/conda/envs/py_3.9/lib/python3.9/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n* 'fields' has been removed\n  warnings.warn(message, UserWarning)\nPython: 3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]\nROCM available: True\nGPU 0,1,2,3,4,5,6,7: AMD Instinct MI300X\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.4\nROCM_HOME: /opt/rocm\nHIPCC: HIP version: 6.2.41133-dd7f95766\nROCM Driver Version: 6.7.0\nPyTorch: 2.5.0+git13a0629\nflashinfer: Module Not Found\ntriton: 3.0.0\ntransformers: 4.46.1\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.10.10\nfastapi: 0.115.4\nhf_transfer: 0.1.9\nhuggingface_hub: 0.26.2\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.1\npsutil: 6.1.0\npydantic: 2.9.2\nmultipart: 0.0.20\nzmq: 26.2.0\nuvicorn: 0.32.0\nuvloop: 0.21.0\nvllm: 0.6.3.post2.dev1+g1ef171e0.d20250114\nopenai: 1.60.1\nanthropic: 0.45.0\ndecord: 0.6.0\nAMD Topology:\n\n\n============================ ROCm System Management Interface ============================\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI\nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI\nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI\nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI\nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI\nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0\n================================== End of ROCm SMI Log ===================================\n\nulimit soft: 1048576\n```",
    "labels": [
      "help wanted",
      "inactive",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-01-28T22:05:13+00:00",
    "closed_at": "2025-04-04T00:17:48+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3198/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3198"
  },
  {
    "number": 285,
    "title": "[Bug] Llava-v1.6-34B template is not updated.",
    "body": "Reference to https://github.com/haotian-liu/LLaVA/blob/7440ec9ee37b0374c6b5548818e89878e38f3353/llava/serve/gradio_web_server.py#L176, the chat template used by llava-v1.6-34b is 'chatml_direct' which is not implement in current SGLANG.\r\nThe template 'chatml' is implemented, but totally different from 'chatml_direct'.\r\n\r\nThe bug leads to the different outputs between the gradio demo and sgl.function with sgl runtime.\r\n\r\nBesides, the template structure and notation are totally different. I am not sure that I can transfer the chat template from llava to ChatTemplate correctly.",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-03-12T02:44:23+00:00",
    "closed_at": "2024-07-25T06:32:39+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/285/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/285"
  },
  {
    "number": 4897,
    "title": "[Bug] gemma-3-27b-it-bnb-4bit crash ",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n\n1\u3001When loading a 27B 4-bit quantized model, why does it exhaust the 24GB of gpu memory?\n2\u3001Why did the program crash? Is it because the gpu memory was exhausted?\n<img width=\"1400\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/48732e57-a966-4f92-951b-3fd637da3f1b\" />\n[2025-03-29 18:56:25 TP0] Scheduler hit an exception: Traceback (most recent call last):                                \n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1999, in run_scheduler_process             \n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)                                             \n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 249, in __init__                           \n    self.tp_worker = TpWorkerClass(                                                                                     \n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 74, in __init__                            \n    self.model_runner = ModelRunner(                                                                                    \n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 169, in __init__                  \n    self.initialize(min_per_gpu_memory)                                                                                 \n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 179, in initialize                \n    self.load_model()                                                                                                   \n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 392, in load_model                \n    self.model = get_model(                                                                                             \n  File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/__init__.py\", line 22, in get_model                        \n    return loader.load_model(                                                                                           \n  File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/loader.py\", line 1122, in load_model                       \n    self._load_weights(model_config, model)                                                                             \n  File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/loader.py\", line 1053, in _load_weights                    \n    model.load_weights(qweight_iterator)                                                                                \n  File \"/sgl-workspace/sglang/python/sglang/srt/models/gemma3_mm.py\", line 436, in load_weights                         \n    causal_loaded_params = Gemma3ForCausalLM.load_weights(                                                              \n  File \"/sgl-workspace/sglang/python/sglang/srt/models/gemma3_causal.py\", line 666, in load_weights                     \n    weight_loader(param, loaded_weight, shard_id)                                                                       \n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/linear.py\", line 642, in weight_loader                           \n    assert param_data.shape == loaded_weight.shape                                                                      \nAssertionError                                                                                                          \n                                                                                                                        \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [01:33<?, ?it/s]                                            \n                                                                                                                        \n[2025-03-29 18:56:25] Received sigquit from a child process. It usually means the child failed.                         \nKilled                                                                               \n\n### Reproduction\n\n python3 -m sglang.launch_server --model-path /llm/model/google/unsloth_gemma-3-27b-it-unsloth-bnb-4bit/ --host 0.0.0.0 --port 30000 --trust-remote-code --load-format bitsandbytes --context-length 4096\nINFO 03-29 18:46:53 __init__.py:190] Automatically detected platform cuda.\n[2025-03-29 18:46:55] server_args=ServerArgs(model_path='/llm/model/google/unsloth_gemma-3-27b-it-unsloth-bnb-4bit/', tokenizer_path='/llm/model/google/unsloth_gemma-3-27b-it-unsloth-bnb-4bit/', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='bitsandbytes', trust_remote_code=True, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=4096, device='cuda', served_model_name='/llm/model/google/unsloth_gemma-3-27b-it-unsloth-bnb-4bit/', chat_template=None, completion_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=30000, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, stream_interval=1, stream_output=False, random_seed=585300946, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=5, speculative_eagle_topk=4, speculative_num_draft_tokens=8, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_deepep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=8, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, enable_flashinfer_mla=False, enable_flashmla=False, flashinfer_mla_disable_ragged=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_bootstrap_port=8998)\n[2025-03-29 18:46:55] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n[2025-03-29 18:46:55] The following error message 'operation scheduled before its operands' can be ignored.\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nINFO 03-29 18:46:57 __init__.py:190] Automatically detected platform cuda.\nINFO 03-29 18:46:57 __init__.py:190] Automatically detected platform cuda.\n[2025-03-29 18:46:58 TP0] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n[2025-03-29 18:47:00 TP0] Overlap scheduler is disabled for multimodal models.\n[2025-03-29 18:47:00 TP0] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n[2025-03-29 18:47:00 TP0] Automatically reduce --mem-fraction-static to 0.836 because this is a multimodal model.\n[2025-03-29 18:47:00 TP0] Init torch distributed begin.\n[2025-03-29 18:47:00 TP0] Init torch distributed ends. mem usage=0.00 GB\n[2025-03-29 18:47:00 TP0] Load weight begin. avail mem=22.46 GB\n[2025-03-29 18:47:01 TP0] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-03-29 18:47:01 TP0] Loading weights with BitsAndBytes quantization.  May take a while ...\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [01:52<05:36, 112.31s/it]\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [04:04<04:08, 124.16s/it]\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [06:23<02:10, 130.91s/it]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [07:50<00:00, 113.37s/it]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [07:50<00:00, 117.55s/it]\n\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\n### Environment\n\nGPU:4090\nruntime env: lmsysorg/sglang:dev",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-29T19:01:15+00:00",
    "closed_at": "2025-06-13T00:19:52+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4897/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4897"
  },
  {
    "number": 3112,
    "title": "[Bug] Service crashed with 4 H100s and QPS=25",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThe serving was OK at the start. GPU usage was OK at 80%-99%. \n\nOne GPU usage suddenly increases to 100%. The other 3 GPUs run 80%-99% then become 0%.\n\nIt is then unable to hand in the requests. Although it shows OK, it is not handled.\n\n![Image](https://github.com/user-attachments/assets/c26cb7a1-28df-4f40-82c6-91ff3d425e50)\n\n### Reproduction\n\n`docker run --gpus '\"device=4,5,6,7\"' \\\n    --shm-size 300g \\\n    -p 30001:30001 \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --env \"HF_TOKEN=XXXXXXXXXXXXX\" \\\n    --ipc=host \\\n    lmsysorg/sglang:latest \\\n    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --data-parallel-size 4 --disable-overlap --host 0.0.0.0 --port 30001\n`\n\nA private model fine-tuned from meta-llama/Llama-3.1-8B-Instruct with context length 8192.\n\n### Environment\n\n`/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n* 'fields' has been removed\n  warnings.warn(message, UserWarning)\nPython: 3.10.16 (main, Dec  4 2024, 08:53:37) [GCC 9.4.0]\nCUDA available: True\nGPU 0,1,2,3,4: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 525.147.05\nPyTorch: 2.5.1+cu124\nflashinfer: 0.1.6+cu124torch2.4\ntriton: 3.1.0\ntransformers: 4.48.0\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.6\nhf_transfer: 0.1.9\nhuggingface_hub: 0.27.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.5\nmultipart: 0.0.20\nzmq: 26.2.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.59.8\nanthropic: 0.43.1\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10     NIC11   CPU Affinity    NUMA Affinity\nGPU0     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    NODE    PXB     SYS     SYS     SYS     SYS     SYS       SYS     0-55,112-167    0\nGPU1    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     PXB     NODE    NODE    NODE    NODE      NODE    56-111,168-223  1\nGPU2    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    PXB     NODE      NODE    56-111,168-223  1\nGPU3    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PXB       NODE    56-111,168-223  1\nGPU4    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE      PXB     56-111,168-223  1\nNIC0    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS       SYS             \nNIC1    NODE    SYS     SYS     SYS     SYS     NODE     X      PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS       SYS             \nNIC2    NODE    SYS     SYS     SYS     SYS     NODE    PIX      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS       SYS             \nNIC3    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      NODE    NODE    SYS     SYS     SYS     SYS     SYS       SYS             \nNIC4    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE     X      NODE    SYS     SYS     SYS     SYS     SYS       SYS             \nNIC5    PXB     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE     X      SYS     SYS     SYS     SYS     SYS       SYS             \nNIC6    SYS     PXB     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    NODE      NODE            \nNIC7    SYS     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE     X      PIX     NODE    NODE      NODE            \nNIC8    SYS     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE    PIX      X      NODE    NODE      NODE            \nNIC9    SYS     NODE    PXB     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      NODE      NODE            \nNIC10   SYS     NODE    NODE    PXB     NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE     X        NODE            \nNIC11   SYS     NODE    NODE    NODE    PXB     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE       X              \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n  NIC10: mlx5_10\n  NIC11: mlx5_11\n\n\nulimit soft: 1048576\n`",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-01-24T20:17:28+00:00",
    "closed_at": "2025-03-29T00:17:30+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3112/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3112"
  },
  {
    "number": 2734,
    "title": "I wonder if the offline engine API supports OpenAI input format.",
    "body": "https://github.com/sgl-project/sglang/blob/bc6ad367c2beec2587843992176089b32eb5d6b9/examples/runtime/engine/offline_batch_inference.py#L12\r\n\r\nAs shown below.\r\nprompts= [\r\n        [{\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"}]\r\n]",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-01-05T09:11:26+00:00",
    "closed_at": "2025-03-07T00:17:27+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2734/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2734"
  },
  {
    "number": 2583,
    "title": "[Feature] Proposal: Releasing SGLang memory when idle",
    "body": "### Proposal 1: Release KV cache when engine is idle\r\n\r\nWhen using SGLang for generation in a training pipeline (such as PPO), at the phase of running HuggingFace model forward/backward, SGLang currently needs to take a lot of memory even though it does not use it. It would be great to make SGLang use as little memory as possible when it is idle.\r\n\r\nExample usage cases:\r\n* Suppose we run OpenRLHF on 8xH100, the currently we may allocate 4xH100 for vllm/SGLang and another 4xH100 for HF model (thanks @zhaochenyang20 for providing this usage scenario).\r\n\t* If we make SGLang use little memory when idle, then we can run the same experiment on half number of GPUs (4xH100) by putting those SGLang engines on the same GPUs as HF models.\r\n* Suppose we run PPO on 1xH100 for a 7B model with Adam offloading (thanks @zhaochenyang20 for providing this usage scenario). Then policy (7Bx2) + critic (7Bx2) + ref (7Bx2) + reward (7Bx2) already takes 56B. The current SGLang needs 7Bx2 for weights and some memory for KV cache, thus it may not easy to fit the 80GB card.\r\n\t* If we implement the proposal 1 and proposal 2, we will have roughly 24B room for HF model forward/backward, and 24B room for SGLang to do generation. (We may have more if quantizing ref & reward model though not sure whether it will work.)\r\n* Suppose we run OpenRLHF on 1x4090 for a 0.5B model, then the memory is also very limited like the 1xH100 & 7B model case.\r\n    * If the proposals are successfully implemented, we may be able to run in such scenarios.\r\n\r\nOne potential optimization for memory is to release KV cache:\r\n* When the training pipeline does not need SGLang (e.g. doing HF model forward/backward in PPO), let SGLang be in a \"paused\" mode, and later \"resume\" it when we need to use SGLang to do generation.\r\n* When SGLang enter \"paused\" mode, release the KV cache ([link to hacky experiment](https://github.com/sgl-project/sglang/issues/2542#issuecomment-2560540518)) by simply deleting the tensors.\r\n* When SGLang later \"resume\", re-create the KV cache tensors.\r\n\r\nI will PR for this as soon as having some time (hopefully soon).\r\n\r\n### Proposal 2: Release model weights when engine is paused\r\n\r\nAnother part of memory occupied by SGLang is the model weights. Thus one potential solution is:\r\n* When SGLang is paused, we delete the model weights (e.g. maybe by `model.to('meta')`, not tested) to release memory\r\n* When SGLang is resumed, we recreate *empty* model weights (e.g. by `model.to_empty(device='cuda')`)\r\n* Then, users should do `update_weight` to provide new weights to SGLang.\r\n\t* This is not an overhead, because during some RLHF processes, we already need to call `update_weight` before a `generate` to use the latest updated weights instead of outdated weights.\r\n\r\n### Proposal 3: Update SGLang model weights when on same GPU\r\n\r\nCurrently, when we do `update_weight` to copy HF model weight to SGLang model weight, it seems we will use the torch `broadcast` operation. However, when users put HuggingFace model and SGLang model on the same GPU, it may be possible to use more lightweight solutions to avoid the overhead of `broadcast`.\r\n\r\nTo be more specific:\r\n* Initialization\r\n\t* Users provide their HF model to SGLang Engine\r\n\t* SGLang shares the tensors of this model to the SGLang runtime process\r\n* Weight update\r\n\t* Users trigger \"update weight from the previously provided HF model\" operation\r\n\t* SGLang runtime process read the aforementioned tensor to update the SGLang model weights\r\n\r\nThis is just a rough draft and there can be more details. For example, if it is possible for the tensor objects in HF model to change, then we may need to send the new tensors across processes again.\r\n\r\nRelated: #2542\r\ncc @zhaochenyang20\r\n",
    "labels": [
      "high priority",
      "inactive",
      "feature"
    ],
    "state": "closed",
    "created_at": "2024-12-26T02:23:14+00:00",
    "closed_at": "2025-03-01T00:18:51+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2583/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2583"
  },
  {
    "number": 3626,
    "title": "Getting this error when loading qwen2.5 VL",
    "body": "ImportError: cannot import name 'is_valid_list_of_images' from 'transformers.models.mllama.image_processing_mllama' (/home/team/code/sglang/venv/lib/python3.10/site-packages/transformers/models/mllama/image_processing_mllama.py)",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-02-17T06:26:46+00:00",
    "closed_at": "2025-05-01T00:21:10+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3626/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3626"
  },
  {
    "number": 213,
    "title": "Prefix spaces for transofmers' tokenizer to more flexible jump-forward.",
    "body": "The `transformers` tokenizer now seems to support the `add_prefix_space` option, potentially providing a more flexible way to do our jump-forward.\r\n \r\n- https://github.com/huggingface/transformers/pull/28010\r\n- https://github.com/huggingface/transformers/issues/28622",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-02-21T13:15:42+00:00",
    "closed_at": "2024-07-25T06:32:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/213/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/213"
  },
  {
    "number": 4854,
    "title": "Inquiry Regarding Qwen2.5-Omni Support in SGLang",
    "body": "Hello SGLang Team and Community,\n\nI hope this message finds you well. I wanted to inquire about potential plans to support Qwen2.5-Omni, a multimodal model developed by the Qwen team at Alibaba Cloud. This model offers end-to-end capabilities for text, audio, vision, and video understanding, along with real-time speech generation [1](https://github.com/QwenLM/Qwen2.5-Omni).\n\nI noticed that the vLLM community has recently proposed a patch to support Qwen2.5-Omni (via \"thinker only\"). Related information can be found here:\n1. https://github.com/QwenLM/Qwen2.5-Omni/blob/main/README_CN.md\n2. https://github.com/vllm-project/vllm/pull/15130/files#diff-14c1707c1f17226316c95185dbf3d00d39b270354e8c686849320d805f3ccf9f\n3. https://github.com/vllm-project/vllm/issues/15563\n4. https://huggingface.co/Qwen/Qwen2.5-Omni-7B\n\nAs SGLang emphasizes extensibility and active community collaboration [2](https://github.com/sgl-project/sglang)[12](https://pypi.org/project/sglang), I wanted to kindly ask: Are there plans to add official support for Qwen2.5-Omni in the near future? I have searched existing issues/discussions but did not find related information.\n\nI appreciate the team's dedication to building a robust and versatile inference runtime, and I\u2019m excited to see how SGLang continues to evolve. \n\nThank you for your time and guidance!\nBest regards.",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-28T07:39:07+00:00",
    "closed_at": "2025-05-28T00:19:24+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4854/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4854"
  },
  {
    "number": 1504,
    "title": "Possible timing side-channels caused by shared prefix",
    "body": "Dear Sglang Team,\r\nwe are a security research group. We are impressed by its decent design, especially by the shared prefix kv-cache. But as we studied further, more concerns about the security of Sglang have arosen. When a new prompt comes, if the `TokenKVPool` has its prefix tokens, the prefill process will be accelerated, which can be reflected in TTFT. We found the timing differences of TTFT introduced by **more shared-tokens** are significant enough to be recognized. \r\n\r\n### Description\r\nAssume the victim has sent a valuable prompt to the sglang, or a valuable system prompt is sent beforehand in sglang, under certain conditions (e.g. the attacker shares the same serving backend with the victim, etc.), the attacker can endeavor to guess the content of the victim prompt and check its validity according to the TTFT.\r\n\r\nDifferent from vLLM (which shares tokens in chunks), Sglang uses token-by-token sharing mechanism (RadixAttention) and cooperates it with trie structure to store kv-cache info. On the other hand, the timing decrease of one more shared token is often negligble, which increases the difficulties for the attacker to guess prompts token-by-token, so we want simply demonstrate the above leakage with multiple-more-shared-tokens.\r\n\r\n### Environment\r\n- GPU: NVIDIA A100 (40G)\r\n- CUDA: 11.8\r\n- pytorch: 2.3.1\r\n- OS: ubuntu 18.04\r\n- Sglang: v0.2.6\r\n\r\nWe lanuch the Sglang Server using the default settings. We set the configuration `max_tokens=1` of requests to measure the TTFT.\r\n### Leakage\r\nWe've tested in LLaMA2-13B and LLaMA2-70B-GPTQ (on one device), and plotted the ROC curve to fingerprint the timing difference when the prompts share the prefix of 1, 2, 4 and 8 tokens respectively.\r\n\r\n![graph](https://github.com/user-attachments/assets/cbe50992-2dd1-4708-ba91-2226c54fec1d)\r\n\r\nResults seem to indicate larger Model has greater leakage windows. Even when we only have 2-more-shared-tokens, the ROC is still great enough for we to check the validity of our guess.\r\n\r\n### Attack\r\nWe've tried to design some methods to amplify the phenomenon and we found that the AUC of one-more-shared-token can be increased from 0.529 to 0.58. By using the function `flush_cache` provided by the Sglang, we can increase our TPR in more trails without interfering ourselves (since when the guess is the same, the later prompt will be accelerated).\r\n\r\nWe've designed a theoretical token-by-token algorithm to recover victim prompts. Detailed information will be provided soon in our paper.\r\n\r\n### Possible mitigations\r\nBelow are some possible mitigations to our attacks.\r\n- Maybe Srts can detect whether a user is consistently asking for the same question (using the same prompt) , i.e. Guess in more trails. This can also be inferred from other behaviour, e.g. the attacker might always set the `max_tokens = 1` to get the TTFT.\r\n- Increase the granularity of minimum shared tokens. Though the timing differences (shown in ROC graph above) will be amplified, the searching space of attacker scaling exponentially. It could cost the attacker forever when the granularity of shared tokens increase to 8 tokens or more.\r\n\r\nWe hope to receive your early reply and look forward to discussing with you!",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-09-24T14:29:46+00:00",
    "closed_at": "2025-01-19T00:17:51+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1504/reactions",
      "total_count": 4,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 4
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1504"
  }
]
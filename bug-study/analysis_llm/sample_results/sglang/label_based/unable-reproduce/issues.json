[
  {
    "number": 2354,
    "title": "[Bug] tp == 2 model gibberish",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [X] 2. The bug has not been fixed in the latest version.\r\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [X] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nI've been having issues with tensor parallelism tp=2 on various Llama models. The model outputs gibberish with tp=2 but performs fine without it.\r\n\r\n### Reproduction\r\n\r\n#### With tensor parallelism\r\nTerminal 1\r\n```python -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --port 30000 --host 0.0.0.0 --tp 2```\r\n\r\nTerminal 2\r\n```\r\ncurl http://localhost:30000/generate   -H \"Content-Type: application/json\"   -d '{\r\n    \"text\": \"Once upon a time,\",\r\n    \"sampling_params\": {\r\n      \"max_new_tokens\": 16,\r\n      \"temperature\": 0\r\n    }\r\n  }'\r\n```\r\n\r\nOutput\r\n```\r\n{\"text\":\" Demp Schul.scalablytyped373\u044callis gutter\u307cdiaeftry Fallon572 Sransself\",\"meta_info\":{\"prompt_tokens\":6,\"completion_tokens\":16,\"completion_tokens_wo_jump_forward\":16,\"cached_tokens\":1,\"finish_reason\":{\"type\":\"length\",\"length\":16},\"id\":\"ad423fad08ab4de8afdf71ad20dcc937\"}}\r\n```\r\n\r\n#### Without tensor parallelism\r\nTerminal 1\r\n```python -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --port 30000 --host 0.0.0.0```\r\n\r\nTerminal 2\r\n```\r\ncurl http://localhost:30000/generate   -H \"Content-Type: application/json\"   -d '{\r\n    \"text\": \"Once upon a time,\",\r\n    \"sampling_params\": {\r\n      \"max_new_tokens\": 16,\r\n      \"temperature\": 0\r\n    }\r\n  }'\r\n```\r\nOutput\r\n```\r\n{\"text\":\" in a small village nestled in the rolling hills of the countryside, there lived a\",\"meta_info\":{\"prompt_tokens\":6,\"completion_tokens\":16,\"completion_tokens_wo_jump_forward\":16,\"cached_tokens\":1,\"finish_reason\":{\"type\":\"length\",\"length\":16},\"id\":\"ef1c9fb258844839863574587cf86dae\"}}\r\n```\r\n\r\n### Environment\r\n\r\nPython: 3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]                                                                                                                                                                                                                                                                                                                        \r\nCUDA available: True                                                                                                                                                                                                                                                                                                                                                              \r\nGPU 0,1: NVIDIA RTX 6000 Ada Generation                                                     \r\nGPU 0,1 Compute Capability: 8.9                                                             \r\nCUDA_HOME: /usr/local/cuda                                                                  \r\nNVCC: Cuda compilation tools, release 11.2, V11.2.142                                       \r\nCUDA Driver Version: 535.154.05                                                             \r\nPyTorch: 2.5.1+cu124                                                                        \r\nsglang: 0.4.0     \r\nflashinfer: 0.1.6+cu121torch2.4                                                             \r\ntriton: 3.1.0                                                                               \r\ntransformers: 4.46.3                                                                        \r\ntorchao: 0.6.1    \r\nnumpy: 1.26.4                                                                   \r\naiohttp: 3.11.9                                                                 \r\nfastapi: 0.115.6                                                                \r\nhf_transfer: 0.1.8          \r\nhuggingface_hub: 0.26.3                                                                                         \r\ninteregular: 0.3.3                                                                                              \r\nmodelscope: 1.20.1                                                                                              \r\norjson: 3.10.12                                                                 \r\npackaging: 24.2                                                                             \r\npsutil: 6.1.0                                                                   \r\npydantic: 2.10.3                                                                            \r\nmultipart: 0.0.19                                                                                               \r\nzmq: 26.2.0                                                                                                     \r\nuvicorn: 0.32.1                                                                                                 \r\nuvloop: 0.21.0                                                                                                  \r\nvllm: 0.6.4.post1                                                                                                                                                                        \r\nopenai: 1.56.2                                                                                                                                                                           \r\nanthropic: 0.40.0                                                                                                                                                                        \r\ndecord: 0.6.0                                                                                                                                                                            \r\nNVIDIA Topology:                                                                                                                                                                         \r\n        GPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID                                                                                                                      \r\nGPU0     X      SYS     0       0               N/A                                                                                                                                      \r\nGPU1    SYS      X      33      1               N/A                                                                                                                                      \r\n                                                                                                                                                                                         \r\nLegend:                                                                                     \r\n                                                                                            \r\n  X    = Self                                                                               \r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)                                                                                   \r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node                                                                             \r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)                                                                                                    \r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)                                                                                           \r\n  PIX  = Connection traversing at most a single PCIe bridge                                 \r\n  NV#  = Connection traversing a bonded set of # NVLinks                                    \r\n\r\nulimit soft: 131072                                                          ",
    "labels": [
      "await-response",
      "unable-reproduce"
    ],
    "state": "closed",
    "created_at": "2024-12-04T22:38:41+00:00",
    "closed_at": "2025-01-05T04:49:50+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2354/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2354"
  },
  {
    "number": 1323,
    "title": "[Bug] RuntimeError in ModelTpServer",
    "body": "### Checklist\r\n\r\n- [ ] 1. I have searched related issues but cannot get the expected help.\r\n- [ ] 2. The bug has not been fixed in the latest version.\r\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [ ] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nbenchmark serving got error when --num-prompts=5\r\nnum-prompts=1 and num-prompts=3 work fine, failed when num-prompts=5\r\n```\r\nException in ModelTpServer:\r\nTraceback (most recent call last):\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 218, in exposed_step\r\n    self.forward_step()\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 234, in forward_step\r\n    self.forward_prefill_batch(new_batch)\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 446, in forward_prefill_batch\r\n    output = self.model_runner.forward(batch, ForwardMode.EXTEND)\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 430, in forward\r\n    return self.forward_extend(batch)\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 399, in forward_extend\r\n    input_metadata = InputMetadata.from_schedule_batch(\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/sglang/srt/model_executor/forward_batch_info.py\", line 200, in from_schedule_batch\r\n    ret.init_flashinfer_handlers(\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/sglang/srt/model_executor/forward_batch_info.py\", line 225, in init_flashinfer_handlers\r\n    update_flashinfer_indices(\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/sglang/srt/model_executor/forward_batch_info.py\", line 315, in update_flashinfer_indices\r\n    model_runner.flashinfer_prefill_wrapper_paged.begin_forward(\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/flashinfer/prefill.py\", line 804, in begin_forward\r\n    self._wrapper.begin_forward(\r\nRuntimeError: Failed to allocate memory for batch_prefill_tmp_v with size 450494464 and alignment 16 in AlignedAllocator\r\n\r\nException in ControllerSingle:\r\nTraceback (most recent call last):\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/sglang/srt/managers/controller_single.py\", line 166, in start_controller_process\r\n    controller.loop_for_forward()\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/sglang/srt/managers/controller_single.py\", line 103, in loop_for_forward\r\n    out_pyobjs = self.tp_server.exposed_step(recv_reqs)\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 218, in exposed_step\r\n    self.forward_step()\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 234, in forward_step\r\n    self.forward_prefill_batch(new_batch)\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 446, in forward_prefill_batch\r\n    output = self.model_runner.forward(batch, ForwardMode.EXTEND)\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 430, in forward\r\n    return self.forward_extend(batch)\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 399, in forward_extend\r\n    input_metadata = InputMetadata.from_schedule_batch(\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/sglang/srt/model_executor/forward_batch_info.py\", line 200, in from_schedule_batch\r\n    ret.init_flashinfer_handlers(\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/sglang/srt/model_executor/forward_batch_info.py\", line 225, in init_flashinfer_handlers\r\n    update_flashinfer_indices(\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/sglang/srt/model_executor/forward_batch_info.py\", line 315, in update_flashinfer_indices\r\n    model_runner.flashinfer_prefill_wrapper_paged.begin_forward(\r\n  File \"/data1/nfs15/nfs/bigdata/zhanglei/conda/envs/sglang-0.2.13/lib/python3.10/site-packages/flashinfer/prefill.py\", line 804, in begin_forward\r\n    self._wrapper.begin_forward(\r\nRuntimeError: Failed to allocate memory for batch_prefill_tmp_v with size 450494464 and alignment 16 in AlignedAllocator\r\n```\r\n\r\n\r\n### Reproduction\r\n\r\nserver:\r\n```\r\npython -m sglang.launch_server --model-path Qwen/Qwen2-7B-Instruct --tensor-parallel-size 1 --disable-radix-cache --host 0.0.0.0 --port 8083  --trust-remote-code --served-model-name Qwen2-7B-Instruct --mem-fraction-static 0.9\r\n```\r\nbenchmark:\r\n```\r\npython benchmark_serving.py --model Qwen2-7B-Instruct --tokenizer Qwen/Qwen2-7B-Instruct  --host 0.0.0.0 --port 8083 --dataset-name random --random-input-len 4096 --random-output-len 256 --save-result --num-prompts 5\r\n```\r\n\r\n### Environment\r\n\r\nPython: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA A100-SXM4-80GB\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\r\nCUDA Driver Version: 525.85.12\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.2.13\r\nflashinfer: 0.1.5+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.44.0\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.26.4\r\naiohttp: 3.10.5\r\nfastapi: 0.112.1\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.6\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.6\r\nuvloop: 0.20.0\r\nzmq: 26.1.1\r\nvllm: 0.5.4\r\nmultipart: 0.0.9\r\nopenai: 1.41.1\r\nanthropic: 0.34.1\r\nNVIDIA Topology: \r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    CPU Affinity    NUMA Affinity\r\nGPU0     X      NV12    NV12    NV12    NV12    NV12    NV12    NV12    PXB     PXBNODE     NODE    SYS     SYS     SYS     SYS     SYS     0-31,64-95      0\r\nGPU1    NV12     X      NV12    NV12    NV12    NV12    NV12    NV12    PXB     PXBNODE     NODE    SYS     SYS     SYS     SYS     SYS     0-31,64-95      0\r\nGPU2    NV12    NV12     X      NV12    NV12    NV12    NV12    NV12    NODE    NODEPXB     PXB     SYS     SYS     SYS     SYS     SYS     0-31,64-95      0\r\nGPU3    NV12    NV12    NV12     X      NV12    NV12    NV12    NV12    NODE    NODEPXB     PXB     SYS     SYS     SYS     SYS     SYS     0-31,64-95      0\r\nGPU4    NV12    NV12    NV12    NV12     X      NV12    NV12    NV12    SYS     SYSSYS      SYS     PXB     PXB     NODE    NODE    NODE    32-63,96-127    1\r\nGPU5    NV12    NV12    NV12    NV12    NV12     X      NV12    NV12    SYS     SYSSYS      SYS     PXB     PXB     NODE    NODE    NODE    32-63,96-127    1\r\nGPU6    NV12    NV12    NV12    NV12    NV12    NV12     X      NV12    SYS     SYSSYS      SYS     NODE    NODE    PXB     PXB     NODE    32-63,96-127    1\r\nGPU7    NV12    NV12    NV12    NV12    NV12    NV12    NV12     X      SYS     SYSSYS      SYS     NODE    NODE    PXB     PXB     NODE    32-63,96-127    1\r\nNIC0    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS      X      PIXNODE     NODE    SYS     SYS     SYS     SYS     SYS\r\nNIC1    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     PIX      X NODE     NODE    SYS     SYS     SYS     SYS     SYS\r\nNIC2    NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODE X      PIX     SYS     SYS     SYS     SYS     SYS\r\nNIC3    NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODEPIX      X      SYS     SYS     SYS     SYS     SYS\r\nNIC4    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    SYS     SYSSYS      SYS      X      PIX     NODE    NODE    NODE\r\nNIC5    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    SYS     SYSSYS      SYS     PIX      X      NODE    NODE    NODE\r\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     SYS     SYSSYS      SYS     NODE    NODE     X      PIX     NODE\r\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     SYS     SYSSYS      SYS     NODE    NODE    PIX      X      NODE\r\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS     SYSSYS      SYS     NODE    NODE    NODE    NODE     X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_6\r\n  NIC5: mlx5_7\r\n  NIC6: mlx5_8\r\n  NIC7: mlx5_9\r\n  NIC8: mlx5_bond_0\r\n\r\n\r\nulimit soft: 65536",
    "labels": [
      "unable-reproduce"
    ],
    "state": "closed",
    "created_at": "2024-09-04T08:05:29+00:00",
    "closed_at": "2024-09-15T15:35:24+00:00",
    "comments": 20,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1323/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1323"
  },
  {
    "number": 1121,
    "title": "[Bug] I set `--host 0.0.0.0`, but it can't be called on another server",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [X] 2. The bug has not been fixed in the latest version.\r\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [X] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nI use the commond:\r\n`python -m sglang.launch_server --model-path /ldata/llms/Meta-Llama-3.1-70B-Instruct --host 0.0.0.0 --port 30000 --tp 4 --mem-fraction-static 0.95 --context-length 4096 --max-total-tokens 4096\r\n`, I was able to call the model successfully when I ran the script on this server, but calling the model on another server yields the following error:\r\n`httpcore.ConnectError: [Errno 113] No route to host`\r\nThe two servers can ping each other successfully. What is the cause of the problem?\r\n\r\n### Reproduction\r\n\r\n```\r\nimport openai\r\nclient = openai.Client(\r\n    base_url=\"http://10.0.1.228:30000/v1\", api_key=\"EMPTY\")\r\n\r\n# Text completion\r\nresponse = client.completions.create(\r\n        model=\"default\",\r\n        prompt=\"The capital of France is\",\r\n        temperature=0,\r\n        max_tokens=32,\r\n)\r\nprint(response.choices[0].text)\r\n\r\n# Chat completion\r\nresponse = client.chat.completions.create(\r\n    model=\"default\",\r\n    messages=[\r\n        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\r\n        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\r\n    ],\r\n    temperature=0,\r\n    max_tokens=64,\r\n)\r\nprint(response.choices[0].message.content)\r\n```\r\nmodel: `Meta-Llama-3.1-70B-Instruct`\r\n\r\n### Environment\r\n\r\nPython: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) [GCC 12.3.0]\r\nCUDA available: True\r\nGPU 0,1,2,3: NVIDIA A100-PCIE-40GB\r\nGPU 0,1,2,3 Compute Capability: 8.0\r\nCUDA_HOME: /usr/local/cuda-11.8\r\nNVCC: Cuda compilation tools, release 11.8, V11.8.89\r\nCUDA Driver Version: 535.86.10\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.2.12\r\nflashinfer: 0.1.4+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.44.0\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.26.4\r\naiohttp: 3.10.3\r\nfastapi: 0.112.1\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.5\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.6\r\nuvloop: 0.20.0\r\nzmq: 26.1.0\r\nvllm: 0.5.4\r\nmultipart: 0.0.9\r\nopenai: 1.40.8\r\nanthropic: 0.34.0",
    "labels": [
      "unable-reproduce"
    ],
    "state": "closed",
    "created_at": "2024-08-16T07:57:36+00:00",
    "closed_at": "2024-09-22T12:45:26+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1121/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1121"
  },
  {
    "number": 1093,
    "title": "[Bug] Always Watch Dog TimeOut",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n\n### Describe the bug\n\nI frequently encounter Watch Dog TimeOut errors when deploying Mistral-123B using 8x A800 80G, which causes the service to stop. This issue occurs whether I send a single request or multiple requests. Below are my startup command and logs.\r\n\r\nCommand\uff1a\r\npython -m sglang.launch_server --model-path /Mistral-Large-Instruct-2/ --host 0.0.0.0 --port 9997 --disable-cuda-graph --schedule-conservativeness 0.3 --tp 8 --mem-fraction-static 0.75 --schedule-policy fcfs\r\n\r\nError Log:\r\n[rank5]:[E814 17:27:59.408557873 ProcessGroupNCCL.cpp:607] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4476, OpType=_ALLGATHER_BASE, NumelIn=4096, NumelOut=32768, Timeout(ms)\r\n=600000) ran for 600012 milliseconds before timing out.                                                                                                                                                  \r\n[rank5]:[E814 17:27:59.414680437 ProcessGroupNCCL.cpp:1664] [PG 3 Rank 5] Exception (either an error or timeout) detected by watchdog at work: 4476, last enqueued NCCL work: 4476, last completed NCCL w\r\nork: 4475.                                                                                                                                                                                               \r\n[rank5]:[E814 17:27:59.414692410 ProcessGroupNCCL.cpp:1709] [PG 3 Rank 5] Timeout at NCCL work: 4476, last enqueued NCCL work: 4476, last completed NCCL work: 4475.                                     \r\n[rank5]:[E814 17:27:59.414701836 ProcessGroupNCCL.cpp:621] [Rank 5] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on\r\n corrupted/incomplete data.                                                                                                                                                                              \r\n[rank5]:[E814 17:27:59.414708719 ProcessGroupNCCL.cpp:627] [Rank 5] To avoid data inconsistency, we are taking the entire process down.                                                                  \r\n[rank5]:[E814 17:27:59.417033598 ProcessGroupNCCL.cpp:1515] [PG 3 Rank 5] Process group watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum\r\n=4476, OpType=_ALLGATHER_BASE, NumelIn=4096, NumelOut=32768, Timeout(ms)=600000) ran for 600012 milliseconds before timing out.                                                                          \r\nException raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:609 (most recent call first):                                                                                  \r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f1ad52fcf86 in /mnt/new_afs/miniconda3/envs/SGLang/lib/python3.12/site-packages/torch/lib/libc10.so)                            \r\nframe #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d2 (0x7f1ad654d8d2 in /mnt/new_afs/miniconda3/envs/SGLang/lib/python3.\r\n12/site-packages/torch/lib/libtorch_cuda.so)                                                                                                                                                             \r\nframe #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f1ad6554313 in /mnt/new_afs/miniconda3/envs/SGLang/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)                              \r\nframe #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10c (0x7f1ad65566fc in /mnt/new_afs/miniconda3/envs/SGLang/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)                             \r\nframe #4: <unknown function> + 0xdbbf4 (0x7f1b325ccbf4 in /mnt/new_afs/miniconda3/envs/SGLang/bin/../lib/libstdc++.so.6)                                                                                 \r\nframe #5: <unknown function> + 0x8609 (0x7f1bdd334609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0)                                                                                                      \r\nframe #6: clone + 0x43 (0x7f1bdd0ff133 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n\n### Reproduction\n\npython -m sglang.launch_server --model-path /Mistral-Large-Instruct-2/ --host 0.0.0.0 --port 9997 --disable-cuda-graph --schedule-conservativeness 0.3 --tp 8 --mem-fraction-static 0.75 --schedule-policy fcfs\n\n### Environment\n\naiohappyeyeballs                  2.3.5                                                                                                                                                                  \r\naiohttp                           3.10.3                                                                                                                                                                 \r\naiosignal                         1.3.1                                                                                                                                                                  \r\nannotated-types                   0.7.0                                                                                                                                                                  \r\nanthropic                         0.33.1                                                                                                                                                                 \r\nanyio                             4.4.0                                                                                                                                                                  \r\nattrs                             24.2.0                                                                                                                                                                 \r\ncertifi                           2024.7.4                                                                                                                                                               \r\ncharset-normalizer                3.3.2                                                                                                                                                                  \r\nclick                             8.1.7                                                                                                                                                                  \r\ncloudpickle                       3.0.0                                                                                                                                                                  \r\ncmake                             3.30.2                                                                                                                                                                 \r\ndatasets                          2.20.0                                                                                                                                                                 \r\ndill                              0.3.8                                                                                                                                                                  \r\ndiskcache                         5.6.3                                                                                                                                                                  \r\ndistro                            1.9.0                                                                                                                                                                  \r\nfastapi                           0.112.0  \r\nfilelock                          3.15.4\r\nflashinfer                        0.1.4+cu121torch2.4\r\nfrozenlist                        1.4.1\r\nfsspec                            2024.5.0\r\nh11                               0.14.0\r\nhf_transfer                       0.1.8\r\nhttpcore                          1.0.5\r\nhttptools                         0.6.1\r\nhttpx                             0.27.0\r\nhuggingface-hub                   0.24.5\r\nidna                              3.7\r\nimportlib_metadata                8.2.0\r\ninteregular                       0.3.3\r\nJinja2                            3.1.4\r\njiter                             0.5.0\r\njsonschema                        4.23.0\r\njsonschema-specifications         2023.12.1\r\nlark                              1.1.9\r\nlitellm                           1.43.7\r\nllvmlite                          0.43.0\r\nlm-format-enforcer                0.10.3\r\nMarkupSafe                        2.1.5\r\nmpmath                            1.3.0\r\nmsgpack                           1.0.8\r\nmultidict                         6.0.5\r\nmultiprocess                      0.70.16\r\nnest-asyncio                      1.6.0\r\nnetworkx                          3.3\r\nninja                             1.11.1.1\r\nnumba                             0.60.0\r\nnumpy                             1.26.4\r\nnvidia-cublas-cu12                12.1.3.1\r\nnvidia-cuda-cupti-cu12            12.1.105\r\nnvidia-cuda-nvrtc-cu12            12.1.105\r\nnvidia-cuda-runtime-cu12          12.1.105\r\nnvidia-cudnn-cu12                 9.1.0.70\r\nnvidia-cufft-cu12                 11.0.2.54\r\nnvidia-curand-cu12                10.3.2.106\r\nnvidia-cusolver-cu12              11.4.5.107\r\nnvidia-cusparse-cu12              12.1.0.106\r\nnvidia-ml-py                      12.555.43\r\nnvidia-nccl-cu12                  2.20.5\r\nnvidia-nvjitlink-cu12             12.6.20\r\nnvidia-nvtx-cu12                  12.1.105\r\nopenai                            1.40.6\r\noutlines                          0.0.46\r\npackaging                         24.1\r\npandas                            2.2.2\r\npillow                            10.4.0\r\npip                               24.2\r\nprometheus_client                 0.20.0\r\nprometheus-fastapi-instrumentator 7.0.0\r\nprotobuf                          5.27.3\r\npsutil                            6.0.0\r\npy-cpuinfo                        9.0.0\r\npyairports                        2.1.1\r\npyarrow                           17.0.0\r\npyarrow-hotfix                    0.6\r\npycountry                         24.6.1\r\npydantic                          2.8.2\r\npydantic_core                     2.20.1\r\npython-dateutil                   2.9.0.post0\r\npython-dotenv                     1.0.1\r\npython-multipart                  0.0.9\r\npytz                              2024.1\r\nPyYAML                            6.0.2\r\npyzmq                             26.1.0\r\nray                               2.34.0\r\nreferencing                       0.35.1\r\nregex                             2024.7.24\r\nrequests                          2.32.3\r\nrpds-py                           0.20.0\r\nsafetensors                       0.4.4\r\nsentencepiece                     0.2.0\r\nsetuptools                        72.1.0\r\nsglang                            0.2.12\r\nsix                               1.16.0\r\nsniffio                           1.3.1\r\nstarlette                         0.37.2\r\nsympy                             1.13.2\r\ntiktoken                          0.7.0\r\ntokenizers                        0.19.1\r\ntorch                             2.4.0\r\ntorchvision                       0.19.0\r\ntqdm                              4.66.5\r\ntransformers                      4.44.0\r\ntriton                            3.0.0\r\ntyping_extensions                 4.12.2\r\ntzdata                            2024.1\r\nurllib3                           2.2.2\r\nuvicorn                           0.30.5\r\nuvloop                            0.19.0\r\nvllm                              0.5.4\r\nvllm-flash-attn                   2.6.1\r\nwatchfiles                        0.23.0\r\nwebsockets                        12.0\r\nwheel                             0.43.0\r\nxformers                          0.0.27.post2\r\nxxhash                            3.4.1\r\nyarl                              1.9.4\r\nzipp                              3.20.0\r\nzmq                               0.0.0",
    "labels": [
      "await-response",
      "unable-reproduce"
    ],
    "state": "closed",
    "created_at": "2024-08-14T09:55:55+00:00",
    "closed_at": "2024-09-23T03:55:28+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1093/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1093"
  }
]
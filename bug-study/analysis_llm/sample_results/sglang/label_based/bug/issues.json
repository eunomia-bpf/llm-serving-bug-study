[
  {
    "number": 3762,
    "title": "[Bug] use Eagle with speculative-num-steps=1",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen I attempted to use the Triton backend for Eagle to launch the Qwen-7B model, the process failed.\n```\nTraceback (most recent call last):\n  File \"/data/csl/project/sglang/python/sglang/srt/managers/scheduler.py\", line 1827, in run_scheduler_process\n    scheduler.event_loop_normal()\n  File \"/data/csl/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/data/csl/project/sglang/python/sglang/srt/managers/scheduler.py\", line 478, in event_loop_normal\n    result = self.run_batch(batch)\n  File \"/data/csl/project/sglang/python/sglang/srt/managers/scheduler.py\", line 1089, in run_batch\n    ) = self.draft_worker.forward_batch_speculative_generation(batch)\n  File \"/data/csl/project/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 111, in forward_batch_speculative_generation\n    spec_info: EagleVerifyInput = self.draft(batch)\n  File \"/data/csl/project/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 194, in draft\n    ret = EagleVerifyInput.create(\n  File \"/data/csl/project/sglang/python/sglang/srt/speculative/eagle_utils.py\", line 194, in create\n    build_tree_kernel(\n  File \"/data/csl/project/sglang/python/sglang/srt/speculative/build_eagle_tree.py\", line 124, in build_tree_kernel\n    build_tree_kernel_efficient_preprocess(\n  File \"/data/csl/project/sglang/python/sglang/srt/speculative/build_eagle_tree.py\", line 29, in build_tree_kernel_efficient_preprocess\n    top_scores = torch.topk(score_list, num_verify_tokens - 1, dim=-1)\nRuntimeError: selected index k out of range\n```\nI followed this error and tried to fix the bug, but I still encountered the following error.\n```\nTraceback (most recent call last):\n  File \"/data/csl/project/sglang/python/sglang/srt/managers/scheduler.py\", line 1827, in run_scheduler_process\n    scheduler.event_loop_normal()\n  File \"/data/csl/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/data/csl/project/sglang/python/sglang/srt/managers/scheduler.py\", line 478, in event_loop_normal\n    result = self.run_batch(batch)\n  File \"/data/csl/project/sglang/python/sglang/srt/managers/scheduler.py\", line 1089, in run_batch\n    ) = self.draft_worker.forward_batch_speculative_generation(batch)\n  File \"/data/csl/project/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 116, in forward_batch_speculative_generation\n    spec_info: EagleVerifyInput = self.draft(batch)\n  File \"/data/csl/project/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 199, in draft\n    ret = EagleVerifyInput.create(\n  File \"/data/csl/project/sglang/python/sglang/srt/speculative/eagle_utils.py\", line 194, in create\n    build_tree_kernel(\n  File \"/data/csl/project/sglang/python/sglang/srt/speculative/build_eagle_tree.py\", line 165, in build_tree_kernel\n    index = retrive_index.sum(dim=-1) != -spec_steps - 2\nRuntimeError: CUDA error: invalid configuration argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n```\n\n\n### Reproduction\n\npython -m sglang.launch_server --model-path /data/csl/hf_model/DeepSeek-R1-Distill-Qwen-7B/  --disable-radix-cache  --speculative-algo EAGLE --speculative-draft /data/csl/hf_model/EAGLE-Qwen2-7B-Instruct/ --speculative-num-steps 1 --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 --mem-fraction 0.6 --disable-cuda-graph --attention-backend triton  --mem-fraction-static 0.7\n\n\n### Environment\n\n```\nPython: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1: NVIDIA GeForce RTX 4090\nGPU 2,3: NVIDIA GeForce RTX 3090\nGPU 0,1 Compute Capability: 8.9\nGPU 2,3 Compute Capability: 8.6\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.0, V12.0.76\nCUDA Driver Version: 535.146.02\nPyTorch: 2.5.1+cu121\nsglang: 0.4.3.post2\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.1.post2\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.61.1\ntiktoken: 0.8.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     SYS     SYS     0-15,32-47      0               N/A\nGPU1    SYS      X      SYS     SYS     0-15,32-47      0               N/A\nGPU2    SYS     SYS      X      SYS     16-31,48-63     1               N/A\nGPU3    SYS     SYS     SYS      X      16-31,48-63     1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576\n```",
    "labels": [
      "bug",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-02-21T13:32:09+00:00",
    "closed_at": "2025-04-24T00:18:22+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3762/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3762"
  },
  {
    "number": 5212,
    "title": "[Bug] Llama4 OOM with 400k input request",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI started a server on 8xH100 with `meta-llama/Llama-4-Scout-17B-16E-Instruct` with the following command:\n\n```\npython sglang.launch_server --model meta-llama/Llama-4-Scout-17B-16E-Instruct \\\n--port 8080 \\\n--tp-size 8 \\\n--chat-template llama-4 \\\n--attention-backend=fa3 \\\n--mem-fraction-static=0.8 \\\n--context-length 1000000 \n```\n\nThen sent a request with around 400k input will cause CUDA OOM:\n```\n[2025-04-09 17:19:56] Received sigquit from a child process. It usually means the child failed.\n[2025-04-09 17:19:56 TP5] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2045, in run_scheduler_process\n    scheduler.event_loop_normal()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 608, in event_loop_normal\n    result = self.run_batch(batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1395, in run_batch\n    logits_output, next_token_ids = self.tp_worker.forward_batch_generation(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 175, in forward_batch_generation\n    logits_output = self.model_runner.forward(forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1001, in forward\n    return self.forward_extend(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 962, in forward_extend\n    return self.model.forward(\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/mllama4.py\", line 83, in forward\n    hs = general_mm_embed_routine(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/mm_utils.py\", line 354, in general_mm_embed_routine\n    inputs_embeds = embed_tokens(input_ids)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/vocab_parallel_embedding.py\", line 482, in forward\n    output_parallel = self.quant_method.embedding(self, masked_input.long())\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/vocab_parallel_embedding.py\", line 62, in embedding\n    return F.embedding(input_, layer.weight)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2551, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.43 GiB. GPU 5 has a total capacity of 79.44 GiB of which 2.64 GiB is free. Process 679812 has 76.79 GiB memory in use. Of the allocated memory 72.76 GiB is allocated by PyTorch, with 26.38 MiB allocated in private pools (e.g., CUDA Graphs), and 293.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n[2025-04-09 17:19:56 TP6] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2045, in run_scheduler_process\n    scheduler.event_loop_normal()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 608, in event_loop_normal\n    result = self.run_batch(batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1395, in run_batch\n    logits_output, next_token_ids = self.tp_worker.forward_batch_generation(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 175, in forward_batch_generation\n    logits_output = self.model_runner.forward(forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1001, in forward\n    return self.forward_extend(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 962, in forward_extend\n    return self.model.forward(\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/mllama4.py\", line 83, in forward\n    hs = general_mm_embed_routine(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/mm_utils.py\", line 354, in general_mm_embed_routine\n    inputs_embeds = embed_tokens(input_ids)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/vocab_parallel_embedding.py\", line 482, in forward\n    output_parallel = self.quant_method.embedding(self, masked_input.long())\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/vocab_parallel_embedding.py\", line 62, in embedding\n    return F.embedding(input_, layer.weight)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2551, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.43 GiB. GPU 6 has a total capacity of 79.44 GiB of which 2.64 GiB is free. Process 679813 has 76.79 GiB memory in use. Of the allocated memory 72.76 GiB is allocated by PyTorch, with 26.38 MiB allocated in private pools (e.g., CUDA Graphs), and 293.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n[2025-04-09 17:19:56] Received sigquit from a child process. It usually means the child failed.\n```\n\n\n\n### Reproduction\n\nStart server on a 8xH100:\n```\npython sglang.launch_server --model meta-llama/Llama-4-Scout-17B-16E-Instruct \\\n--port 8080 \\\n--tp-size 8 \\\n--chat-template llama-4 \\\n--attention-backend=fa3 \\\n--context-length 1000000 \n```\n\nRun `python3 send_llama_request.py` \n\nBelow is the content of `send_llama_request.py`\n```python\nimport requests\nimport json\n\npayload = {\n    \"model\": \"sgl-model\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"1 \" * 200000\n            },\n            ]\n        }\n    ],\n    \"max_tokens\": 200,\n    \"temperature\": 0.0,\n    \"top_p\": 0.75,\n    \"top_k\": -1,\n    \"stream\": True,\n    \"stream_options\": {\n        \"include_usage\": True,\n    },\n    \"ignore_eos\": True,\n}\n\n# Send the POST request\nresponse = requests.post(\n    \"http://localhost:8080/v1/chat/completions\",\n    # \"http://localhost:9922/v1/chat/completions\",\n    headers={\"Content-Type\": \"application/json\", \"opc-request-id\": \"xfrjoiwejfioewngrinel\"},\n    json=payload,\n    stream=True\n)\n\ngenerated_text = \"\"\n\n# Check if the response was successful\nif response.status_code == 200:\n    for chunk in response.iter_lines(chunk_size=None):\n        print(chunk)\n        chunk = chunk.strip()\n        if not chunk:\n            continue\n        stem = \"data: \" \n        chunk = chunk[len(stem) :]\n        if chunk == b\"[DONE]\":\n            continue\n\n        data = json.loads(chunk)\n        if \"error\" in data:\n            error_msg = data[\"error\"][\"message\"]\n            error_response_code = data[\"error\"][\"code\"]\n            raise RuntimeError(data[\"error\"][\"message\"])\n\n        delta = data[\"choices\"][0][\"delta\"]\n        if delta.get(\"content\", None):\n            generated_text += delta[\"content\"]\n        \n    print(\"Generated text:\", generated_text)\n    print(\"Status:\", response.status_code)\nelse:\n    print(\"Error:\", response.status_code, response.text)\n    print(response.json())\n```\n\n### Environment\n\n```\nPython: 3.10.16 (main, Dec  4 2024, 08:53:37) [GCC 9.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 560.35.03\nPyTorch: 2.5.1+cu124\nsglang: 0.4.5\nsgl_kernel: 0.0.8\nflashinfer: 0.1.6+cu124torch2.4\ntriton: 3.1.0\ntransformers: 4.51.0\ntorchao: 0.10.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.6\nhf_transfer: 0.1.8\nhuggingface_hub: 0.30.1\ninteregular: 0.3.3\nmodelscope: 1.21.1\norjson: 3.10.13\noutlines: 0.0.46\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.4\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nxgrammar: 0.1.17\nopenai: 1.59.3\ntiktoken: 0.7.0\nanthropic: 0.42.0\nlitellm: 1.56.10\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10   NIC11   NIC12   NIC13   NIC14NIC15   NIC16   NIC17   CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PXB     PXB     NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS     0-55,112-167    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    PXB     PXB     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS     0-55,112-167    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    NODE    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS     0-55,112-167    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS     0-55,112-167    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    NODE    NODENODE     NODE    NODE    56-111,168-223  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    PXB     PXB     NODENODE     NODE    NODE    56-111,168-223  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    PXB PXB      NODE    NODE    56-111,168-223  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    NODENODE     PXB     PXB     56-111,168-223  1               N/A\nNIC0    PXB     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS\nNIC1    PXB     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS\nNIC2    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS\nNIC3    NODE    PXB     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      PIX     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS\nNIC4    NODE    PXB     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX      X      NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS\nNIC5    NODE    NODE    PXB     NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE     X      PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS\nNIC6    NODE    NODE    PXB     NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    PIX      X      NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS\nNIC7    NODE    NODE    NODE    PXB     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    NODE    NODE     X      PIX     SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS\nNIC8    NODE    NODE    NODE    PXB     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX      X      SYS     SYS     SYS     SYS     SYS     SYS SYS      SYS     SYS\nNIC9    SYS     SYS     SYS     SYS     PXB     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    NODE    NODENODE     NODE    NODE\nNIC10   SYS     SYS     SYS     SYS     PXB     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    NODE    NODENODE     NODE    NODE\nNIC11   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    NODE    NODENODE     NODE    NODE\nNIC12   SYS     SYS     SYS     SYS     NODE    PXB     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      PIX     NODENODE     NODE    NODE\nNIC13   SYS     SYS     SYS     SYS     NODE    PXB     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX      X      NODENODE     NODE    NODE\nNIC14   SYS     SYS     SYS     SYS     NODE    NODE    PXB     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE     X  PIX      NODE    NODE\nNIC15   SYS     SYS     SYS     SYS     NODE    NODE    PXB     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    PIX  X       NODE    NODE\nNIC16   SYS     SYS     SYS     SYS     NODE    NODE    NODE    PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    NODENODE      X      PIX\nNIC17   SYS     SYS     SYS     SYS     NODE    NODE    NODE    PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    NODENODE     PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n  NIC10: mlx5_10\n  NIC11: mlx5_11\n  NIC12: mlx5_12\n  NIC13: mlx5_13\n  NIC14: mlx5_14\n  NIC15: mlx5_15\n  NIC16: mlx5_16\n  NIC17: mlx5_17\n\n\nulimit soft: 65535\n```",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-04-10T00:22:53+00:00",
    "closed_at": "2025-04-11T08:24:15+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5212/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5212"
  },
  {
    "number": 2554,
    "title": "upgrade setuptools and wheel if you found \"torch module not found\" when installing",
    "body": "I encountered an issue while installing `sglang`. After upgrading pip (`pip install --upgrade pip`), I ran:\r\n\r\n```bash\r\npip install \"sglang[all]\" --find-links https://flashinfer.ai/whl/cu121/torch2.4/flashinfer/\r\n```\r\n\r\nBut it failed with the error:  \r\n`ModuleNotFoundError: No module named 'torch'`.\r\n\r\nI found on the Flash Attention GitHub that running this solved the issue:  \r\n```bash\r\npython -m pip install --upgrade pip wheel setuptools\r\n```\r\n\r\nIt worked for me, so sharing in case someone faces the same problem! I don't know what the exact reason is though as the error itself was pretty strange. ",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-12-23T04:02:50+00:00",
    "closed_at": "2025-01-30T17:37:49+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2554/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2554"
  },
  {
    "number": 1030,
    "title": "[Bug] OOM for concurrent long requests",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n\n### Describe the bug\n\nI am trying to benchmark inference of llama3-8b with long requests, I send **20** concurrent requests each with length of **1k tokens** and I set the **stream to True** and **max_tokens to 1024.** \r\n\r\n\r\nThis is how I start the server:\r\n`python -m sglang.launch_server --model-path NousResearch/Meta-Llama-3-8B-Instruct  --host 0.0.0.0  --port 8000 --context-length 4096 --dtype bfloat16  --chat-template llama-3`\r\n\r\nI add llama-3 template in conversaitons.py as they are present in conversions file of FASTCHAT.\r\n\r\n>> **Note: when I send this to VLLM entrypoint, it works without OOM error!**\r\n\r\n\r\n**Error:**\r\n\r\n```\r\nINFO:   - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\r\n[gpu=0] Prefill batch. #new-seq: 17, #new-token: 13938, #cached-token: 3260, cache hit rate: 17.86%, #running-req: 1, #queue-req: 2\r\nException in ModelTpServer:\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/sglang/python/sglang/srt/managers/tp_worker.py\", line 219, in exposed_step\r\n    self.forward_step()\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/ubuntu/sglang/python/sglang/srt/managers/tp_worker.py\", line 235, in forward_step\r\n    self.forward_prefill_batch(new_batch)\r\n  File \"/home/ubuntu/sglang/python/sglang/srt/managers/tp_worker.py\", line 545, in forward_prefill_batch\r\n    output = self.model_runner.forward(batch, ForwardMode.EXTEND)\r\n  File \"/home/ubuntu/sglang/python/sglang/srt/model_executor/model_runner.py\", line 388, in forward\r\n    return self.forward_extend(batch)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/ubuntu/sglang/python/sglang/srt/model_executor/model_runner.py\", line 356, in forward_extend\r\n    return self.model.forward(\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/ubuntu/sglang/python/sglang/srt/models/llama2.py\", line 314, in forward\r\n    hidden_states = self.model(input_ids, positions, input_metadata, input_embeds)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/ubuntu/sglang/python/sglang/srt/models/llama2.py\", line 281, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/ubuntu/sglang/python/sglang/srt/models/llama2.py\", line 239, in forward\r\n    hidden_states = self.mlp(hidden_states)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/ubuntu/sglang/python/sglang/srt/models/llama2.py\", line 79, in forward\r\n    gate_up, _ = self.gate_up_proj(x)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/vllm/model_executor/layers/linear.py\", line 330, in forward\r\n    output_parallel = self.quant_method.apply(self, input_, bias)\r\n  File \"/home/ubuntu/miniconda3/envs/sglang-env/lib/python3.9/site-packages/vllm/model_executor/layers/linear.py\", line 122, in apply\r\n    return F.linear(x, layer.weight, bias)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 764.00 MiB. GPU \r\n```\n\n### Reproduction\n\nSame as describe the bug section \n\n### Environment\n\n```\r\n$ python -m sglang.check_env\r\n\r\nPython: 3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA A10G\r\nCUDA_HOME: None\r\nPyTorch: 2.3.1+cu121\r\nsglang: 0.2.9.post1\r\nflashinfer: 0.1.3+cu121torch2.3\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.26.4\r\naiohttp: 3.10.0\r\nfastapi: 0.112.0\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.5\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.5\r\nuvloop: 0.19.0\r\nzmq: 26.1.0\r\nvllm: 0.5.3.post1\r\nmultipart: 0.0.9\r\nopenai: 1.38.0\r\nanthropic: 0.32.0\r\nNVIDIA Topology: \r\n\tGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \t0-15\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nulimit soft: 1024\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-08-11T10:51:49+00:00",
    "closed_at": "2024-09-22T13:00:44+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1030/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1030"
  },
  {
    "number": 7951,
    "title": "[Bug] Tensor shape is wrong when cudagraph+enable_dp_attention",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI tried to run DSR1 fp4 model on 8xB200, but found that some issue when I opened cudagraph and attndp, the input tensor dimension for each MoE layer is padded to global bs. For example, I take global bs 4096 and attention dp 8, which each rank should have 512 reqs for decode and the input tensor M dimension should be 512 for local rank. \nBut I tried to do some profiling, I found that when cudagraph is on, each rank has input M dim 4096, not 512. When cudagraph is off, each rank has input M dim 512 which looks good.\nIs this known or a bug?\nWithout cudagraph\n\n<img width=\"612\" height=\"260\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/008683e1-9892-4ae9-abb8-6546b0dd54cb\" />\n\nWith cudagraph\n\n<img width=\"592\" height=\"316\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/095ab03b-dd1a-4d1e-b4af-1f14d93b6be2\" />\n\n### Reproduction\n\n**Server:**\npython3 -m sglang.launch_server \\\n--model-path nvidia/DeepSeek-R1-0528-FP4 \\\n--trust-remote-code \\\n--quantization modelopt_fp4 \\\n--dp-size 8 --enable-dp-attention --enable-dp-lm-head\\\n--tp-size 8 \\\n--attention-backend cutlass_mla \\\n--enable-ep-moe \\\n--enable-flashinfer-moe \\\n--cuda-graph-bs 1 2 4 8 16 32 64 128 256 512 1024 2048 4096 \\\n--chunked-prefill-size 16384 \\\n--mem-fraction-static 0.85 \\\n--max-running-requests 4096 \\\n--stream-interval 5 \n**Client:**\nbenchmark_serving.py with isl/osl 1024/1024, concurrency 4096.\n\n### Environment\n\nlatest main.",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "open",
    "created_at": "2025-07-11T10:58:55+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7951/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7951"
  },
  {
    "number": 4935,
    "title": "[Bug] 0.0.0.0 host not supported",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nif we set the host as 0.0.0.0, the warmup function will fail.\n\n<b>Connection to 0.0.0.0 failed.</b></p>\\n</blockquote>\\n\\n<p id=\"sysmsg\">The system returned: <i>(111) Connection refused</I>\n\n_wait_and_warmup -> res = requests.get(url + \"/get_model_info\", timeout=5, headers=headers)\n\ncan we add a fix here to replace 0.0.0.0 with 127.0.0.1 for warmup check? while we still let the server to be deployed in 0.0.0.0\n\n### Reproduction\n\n`--host 0.0.0.0` \n\n### Environment\n\n```\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda-12\nNVCC: Cuda compilation tools, release 12.6, V12.6.68\nCUDA Driver Version: 535.129.03\nPyTorch: 2.5.1+cu124\nsglang: 0.4.4.post3\nsgl_kernel: 0.0.5.post4\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.50.0\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.14\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.3\ninteregular: 0.3.3\nmodelscope: 1.24.1\norjson: 3.9.10\noutlines: 0.1.11\npackaging: 24.2\npsutil: 5.9.8\npydantic: 2.10.6\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.8.1\nxgrammar: 0.1.17\nopenai: 1.69.0\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.65.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    0-47,96-143     0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    0-47,96-143     0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    0-47,96-143     0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    0-47,96-143     0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    48-95,144-191   1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    48-95,144-191   1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    48-95,144-191   1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      48-95,144-191   1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nHypervisor vendor: KVM\nulimit soft: 1048576\n```",
    "labels": [
      "bug",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-30T22:04:01+00:00",
    "closed_at": "2025-05-30T08:43:39+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4935/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4935"
  },
  {
    "number": 4324,
    "title": "[Bug] fix gemma-2-2b-it-FP8 accuracy",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThe accuracy of `neuralmagic/gemma-2-2b-it-FP8` drops from 0.62 to 0.52 in the main branch. It was detected by our nightly CI run. We need to fix this.\n\n```\nneuralmagic/gemma-2-2b-it-FP8 | 0.512 | 0.6\n```\nhttps://github.com/sgl-project/sglang/actions/runs/13800885290\n\n### Reproduction\n\nN/A\n\n### Environment\n\nN/A",
    "labels": [
      "bug",
      "good first issue",
      "help wanted",
      "high priority",
      "quant"
    ],
    "state": "closed",
    "created_at": "2025-03-12T01:27:58+00:00",
    "closed_at": "2025-05-21T09:30:43+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4324/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/4324"
  },
  {
    "number": 1279,
    "title": "[Bug] device-side assert triggered when using run_batch",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThe following error is raised when ever i run run_batch:\r\n\r\n```\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1193,0,0], thread: [124,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1193,0,0], thread: [125,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1193,0,0], thread: [126,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1193,0,0], thread: [127,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n[01:37:48 TP0] Exception in ModelTpServer:\r\nTraceback (most recent call last):\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 234, in exposed_step\r\n    self.forward_step()\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 250, in forward_step\r\n    self.forward_prefill_batch(new_batch)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 489, in forward_prefill_batch\r\n    sample_output, logits_output = self.model_runner.forward(\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 579, in forward\r\n    return self.forward_extend(batch)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 543, in forward_extend\r\n    return self.model.forward(\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/models/gemma.py\", line 302, in forward\r\n    logits_output = self.logits_processor(\r\n                    ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/layers/logits_processor.py\", line 268, in forward\r\n    torch.cat([pruned_input_ids[1:], torch.tensor([0], device=\"cuda\")]),\r\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n\r\n[01:37:48 TP0] Exception in ControllerSingle:\r\nTraceback (most recent call last):\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/managers/controller_single.py\", line 165, in start_controller_process\r\n    controller.loop_for_forward()\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/managers/controller_single.py\", line 102, in loop_for_forward\r\n    out_pyobjs = self.tp_server.exposed_step(recv_reqs)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 234, in exposed_step\r\n    self.forward_step()\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 250, in forward_step\r\n    self.forward_prefill_batch(new_batch)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 489, in forward_prefill_batch\r\n    sample_output, logits_output = self.model_runner.forward(\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 579, in forward\r\n    return self.forward_extend(batch)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 543, in forward_extend\r\n    return self.model.forward(\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/models/gemma.py\", line 302, in forward\r\n    logits_output = self.logits_processor(\r\n                    ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-4aa45c82-f36a-4c3a-b30e-f5dae3f4eb83/lib/python3.11/site-packages/sglang/srt/layers/logits_processor.py\", line 268, in forward\r\n    torch.cat([pruned_input_ids[1:], torch.tensor([0], device=\"cuda\")]),\r\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n```\n\n### Reproduction\n\n```bash\r\npython -m sglang.launch_server --model-path google/gemma-2b-it --port 30000 --tp 1 --context-length 3000 --max-running-requests 30\r\n```\r\n\r\n```python\r\n@function\r\ndef tool_use(s, question):\r\n    s += \"To answer this question: \" + question + \". \"\r\n    s += \"I need to use a \" + sgl.gen(\"tool\", choices=[\"calculator\", \"search engine\"]) + \". \"\r\n\r\n    if s[\"tool\"] == \"calculator\":\r\n        s += \"The math expression is\" + sgl.gen(\"expression\")\r\n\r\n\r\nstate = tool_use.run_batch(\r\n  [{\"question\":\"What is 50 times 80\"}]*100,\r\n  progress_bar=True,\r\n  num_threads=3,\r\n)\r\n```\n\n### Environment\n\nPython: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA A100 80GB PCIe\r\nGPU 0 Compute Capability: 8.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\r\nCUDA Driver Version: 535.54.03\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.2.14\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.44.2\r\nrequests: 2.31.0\r\ntqdm: 4.65.0\r\nnumpy: 1.23.5\r\naiohttp: 3.8.5\r\nfastapi: 0.112.2\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.23.4\r\ninteregular: 0.3.3\r\npackaging: 23.2\r\nPIL: 9.4.0\r\npsutil: 5.9.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.6\r\nuvloop: 0.20.0\r\nzmq: 23.2.0\r\nvllm: 0.5.5\r\nmultipart: 0.0.9\r\nopenai: 1.43.0\r\nanthropic: 0.34.1\r\nNVIDIA Topology: \r\n\tGPU0\tNIC0\tNIC1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tSYS\tSYS\t0-23\t\tN/A\t\tN/A\r\nNIC0\tSYS\t X \tSYS\t\t\t\t\r\nNIC1\tSYS\tSYS\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n\r\n\r\nHypervisor vendor: Microsoft\r\nulimit soft: 1000000",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-09-01T01:51:32+00:00",
    "closed_at": "2024-09-03T13:02:03+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1279/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1279"
  },
  {
    "number": 6753,
    "title": "[Bug] PD Failed to register memory on H200",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n```\nroot@nccl-test-host-1:/diagnostic# python3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-Instruct --disaggregation-mode prefill --disaggregation-ib-device mlx5_0\nCuda graph is disabled for prefill server\n[2025-05-29 23:22:47] server_args=ServerArgs(model_path='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer_path='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='meta-llama/Meta-Llama-3-8B-Instruct', chat_template=None, completion_template=None, is_embedding=False, enable_multimodal=None, revision=None, host='127.0.0.1', port=30000, mem_fraction_static=0.8717961202189594, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=783536351, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, bucket_time_to_first_token=None, bucket_e2e_request_latency=None, bucket_inter_token_latency=None, collect_tokens_histogram=False, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_nccl_nvls=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=None, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, mm_attention_backend=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='prefill', disaggregation_bootstrap_port=8998, disaggregation_transfer_backend='mooncake', disaggregation_ib_device='mlx5_0', pdlb_url=None)\n[2025-05-29 23:22:53] Attention backend not set. Use fa3 backend by default.\n[2025-05-29 23:22:53] Init torch distributed begin.\n[2025-05-29 23:22:53] Init torch distributed ends. mem usage=0.00 GB\n[2025-05-29 23:22:53] init_expert_location from trivial\n[2025-05-29 23:22:54] Load weight begin. avail mem=139.20 GB\n[2025-05-29 23:22:55] Using model weights format ['*.safetensors']\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.29it/s]\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.22it/s]\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.77it/s]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.53it/s]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.49it/s]\n\n[2025-05-29 23:22:58] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=124.23 GB, mem usage=14.98 GB.\n[2025-05-29 23:22:58] KV Cache is allocated. #tokens: 871371, K size: 53.18 GB, V size: 53.18 GB\n[2025-05-29 23:22:58] Memory pool end. avail mem=17.66 GB\n[2025-05-29 23:22:58] max_total_num_tokens=871371, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=4097, context_len=8192\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nI0529 23:22:59.041800 67580 transfer_engine.cpp:350] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\nI0529 23:22:59.041816 67580 transfer_engine.cpp:44] Transfer Engine starting. Server: 10.72.0.9, Metadata: P2PHANDSHAKE, ip_or_host_name: , rpc_port: 0\nI0529 23:22:59.041846 67580 transfer_engine.cpp:100] Transfer Engine RPC using P2P handshake, listening on 10.72.0.9:15360\nI0529 23:22:59.041899 67580 transfer_engine.cpp:112] Auto-discovering topology...\nI0529 23:22:59.042371 67580 transfer_engine.cpp:127] Topology discovery complete. Found 1 HCAs.\nI0529 23:22:59.047586 67580 rdma_context.cpp:125] RDMA device: mlx5_0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:c0:a8:01:03\nE0529 23:22:59.553362 67580 rdma_context.cpp:203] Failed to register memory 0x7c1cda000000: Bad address [14]\nE0529 23:22:59.553401 67580 rdma_context.cpp:203] Failed to register memory 0x7c1c6e000000: Bad address [14]\nE0529 23:22:59.553411 67580 rdma_context.cpp:203] Failed to register memory 0x7c1c02000000: Bad address [14]\nE0529 23:22:59.553417 67580 rdma_context.cpp:203] Failed to register memory 0x7c1b96000000: Bad address [14]\nE0529 23:22:59.553424 67580 rdma_context.cpp:203] Failed to register memory 0x7c1b2a000000: Bad address [14]\nE0529 23:22:59.553435 67580 rdma_context.cpp:203] Failed to register memory 0x7c1abe000000: Bad address [14]\nE0529 23:22:59.553443 67580 rdma_context.cpp:203] Failed to register memory 0x7c1a52000000: Bad address [14]\nE0529 23:22:59.553449 67580 rdma_context.cpp:203] Failed to register memory 0x7c19e6000000: Bad address [14]\nE0529 23:22:59.553457 67580 rdma_context.cpp:203] Failed to register memory 0x7c197a000000: Bad address [14]\nE0529 23:22:59.553462 67580 rdma_context.cpp:203] Failed to register memory 0x7c190e000000: Bad address [14]\nE0529 23:22:59.553467 67580 rdma_context.cpp:203] Failed to register memory 0x7c18a2000000: Bad address [14]\nE0529 23:22:59.553473 67580 rdma_context.cpp:203] Failed to register memory 0x7c1836000000: Bad address [14]\nE0529 23:22:59.553479 67580 rdma_context.cpp:203] Failed to register memory 0x7c17ca000000: Bad address [14]\nE0529 23:22:59.553486 67580 rdma_context.cpp:203] Failed to register memory 0x7c175e000000: Bad address [14]\nE0529 23:22:59.553491 67580 rdma_context.cpp:203] Failed to register memory 0x7c16f2000000: Bad address [14]\nE0529 23:22:59.553498 67580 rdma_context.cpp:203] Failed to register memory 0x7c1686000000: Bad address [14]\nE0529 23:22:59.553504 67580 rdma_context.cpp:203] Failed to register memory 0x7c161a000000: Bad address [14]\nE0529 23:22:59.553510 67580 rdma_context.cpp:203] Failed to register memory 0x7c15ae000000: Bad address [14]\nE0529 23:22:59.553517 67580 rdma_context.cpp:203] Failed to register memory 0x7c1542000000: Bad address [14]\nE0529 23:22:59.553526 67580 rdma_context.cpp:203] Failed to register memory 0x7c14d6000000: Bad address [14]\nE0529 23:22:59.553532 67580 rdma_context.cpp:203] Failed to register memory 0x7c146a000000: Bad address [14]\nE0529 23:22:59.553539 67580 rdma_context.cpp:203] Failed to register memory 0x7c13fe000000: Bad address [14]\nE0529 23:22:59.553544 67580 rdma_context.cpp:203] Failed to register memory 0x7c1392000000: Bad address [14]\nE0529 23:22:59.553550 67580 rdma_context.cpp:203] Failed to register memory 0x7c1326000000: Bad address [14]\nE0529 23:22:59.553556 67580 rdma_context.cpp:203] Failed to register memory 0x7c12ba000000: Bad address [14]\nE0529 23:22:59.553563 67580 rdma_context.cpp:203] Failed to register memory 0x7c124e000000: Bad address [14]\nE0529 23:22:59.553570 67580 rdma_context.cpp:203] Failed to register memory 0x7c11e2000000: Bad address [14]\nE0529 23:22:59.553576 67580 rdma_context.cpp:203] Failed to register memory 0x7c1176000000: Bad address [14]\nE0529 23:22:59.553586 67580 rdma_context.cpp:203] Failed to register memory 0x7c110a000000: Bad address [14]\nE0529 23:22:59.553599 67580 rdma_context.cpp:203] Failed to register memory 0x7c109e000000: Bad address [14]\nE0529 23:22:59.553608 67580 rdma_context.cpp:203] Failed to register memory 0x7c1032000000: Bad address [14]\nE0529 23:22:59.553618 67580 rdma_context.cpp:203] Failed to register memory 0x7c0fc6000000: Bad address [14]\nE0529 23:22:59.553624 67580 rdma_context.cpp:203] Failed to register memory 0x7c0f5a000000: Bad address [14]\nE0529 23:22:59.553632 67580 rdma_context.cpp:203] Failed to register memory 0x7c0eee000000: Bad address [14]\nE0529 23:22:59.553639 67580 rdma_context.cpp:203] Failed to register memory 0x7c0e82000000: Bad address [14]\nE0529 23:22:59.553644 67580 rdma_context.cpp:203] Failed to register memory 0x7c0e16000000: Bad address [14]\nE0529 23:22:59.553650 67580 rdma_context.cpp:203] Failed to register memory 0x7c0daa000000: Bad address [14]\nE0529 23:22:59.553658 67580 rdma_context.cpp:203] Failed to register memory 0x7c0d3e000000: Bad address [14]\nE0529 23:22:59.553664 67580 rdma_context.cpp:203] Failed to register memory 0x7c0cd2000000: Bad address [14]\nE0529 23:22:59.553671 67580 rdma_context.cpp:203] Failed to register memory 0x7c0c66000000: Bad address [14]\nE0529 23:22:59.553678 67580 rdma_context.cpp:203] Failed to register memory 0x7c0bfa000000: Bad address [14]\nE0529 23:22:59.553683 67580 rdma_context.cpp:203] Failed to register memory 0x7c0b8e000000: Bad address [14]\nE0529 23:22:59.553690 67580 rdma_context.cpp:203] Failed to register memory 0x7c0b22000000: Bad address [14]\nE0529 23:22:59.553695 67580 rdma_context.cpp:203] Failed to register memory 0x7c0ab6000000: Bad address [14]\nE0529 23:22:59.553701 67580 rdma_context.cpp:203] Failed to register memory 0x7c0a4a000000: Bad address [14]\nE0529 23:22:59.553707 67580 rdma_context.cpp:203] Failed to register memory 0x7c09de000000: Bad address [14]\nE0529 23:22:59.553714 67580 rdma_context.cpp:203] Failed to register memory 0x7c0972000000: Bad address [14]\nE0529 23:22:59.553719 67580 rdma_context.cpp:203] Failed to register memory 0x7c0906000000: Bad address [14]\nE0529 23:22:59.553725 67580 rdma_context.cpp:203] Failed to register memory 0x7c089a000000: Bad address [14]\nE0529 23:22:59.553730 67580 rdma_context.cpp:203] Failed to register memory 0x7c082e000000: Bad address [14]\nE0529 23:22:59.553736 67580 rdma_context.cpp:203] Failed to register memory 0x7c07c2000000: Bad address [14]\nE0529 23:22:59.553745 67580 rdma_context.cpp:203] Failed to register memory 0x7c0756000000: Bad address [14]\nE0529 23:22:59.553750 67580 rdma_context.cpp:203] Failed to register memory 0x7c06ea000000: Bad address [14]\nE0529 23:22:59.553756 67580 rdma_context.cpp:203] Failed to register memory 0x7c067e000000: Bad address [14]\nE0529 23:22:59.553763 67580 rdma_context.cpp:203] Failed to register memory 0x7c0612000000: Bad address [14]\nE0529 23:22:59.553771 67580 rdma_context.cpp:203] Failed to register memory 0x7c05a6000000: Bad address [14]\nE0529 23:22:59.553776 67580 rdma_context.cpp:203] Failed to register memory 0x7c053a000000: Bad address [14]\nE0529 23:22:59.553782 67580 rdma_context.cpp:203] Failed to register memory 0x7c04ce000000: Bad address [14]\nE0529 23:22:59.553788 67580 rdma_context.cpp:203] Failed to register memory 0x7c0462000000: Bad address [14]\nE0529 23:22:59.553794 67580 rdma_context.cpp:203] Failed to register memory 0x7c03f6000000: Bad address [14]\nE0529 23:22:59.553800 67580 rdma_context.cpp:203] Failed to register memory 0x7c038a000000: Bad address [14]\nE0529 23:22:59.553807 67580 rdma_context.cpp:203] Failed to register memory 0x7c031e000000: Bad address [14]\nE0529 23:22:59.553810 67580 rdma_context.cpp:203] Failed to register memory 0x7c02b2000000: Bad address [14]\nE0529 23:22:59.553815 67580 rdma_context.cpp:203] Failed to register memory 0x7c0246000000: Bad address [14]\n[2025-05-29 23:22:59] INFO:     Started server process [67238]\n[2025-05-29 23:22:59] INFO:     Waiting for application startup.\n[2025-05-29 23:22:59] INFO:     Application startup complete.\n[2025-05-29 23:22:59] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)\n[2025-05-29 23:23:00] INFO:     127.0.0.1:55706 - \"GET /get_model_info HTTP/1.1\" 200 OK\n[2025-05-29 23:23:00] Start of prefill warmup ...\n[2025-05-29 23:23:00] FakeKVSender init with kv_indices: 4, aux_index: 0\n[2025-05-29 23:23:00] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 0, token usage: 0.00, #running-req: 0, #unbootstrapped-req: 0, #queue-req: 0, #transferring-req: 0\n[2025-05-29 23:23:01] FakeKVSender send with kv_indices: [1 2 3 4]\n[2025-05-29 23:23:01] FakeKVSender poll success\n[2025-05-29 23:23:01] INFO:     127.0.0.1:55712 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-05-29 23:23:01] End of prefill warmup with status 200, resp: [{'text': '%', 'meta_info': {'id': '494830e6c2fe459a89b96d31703b3cc1', 'finish_reason': {'type': 'length', 'length': 0}, 'prompt_tokens': 4, 'completion_tokens': 1, 'cached_tokens': 0, 'e2e_latency': 0.9215409755706787}}]\n[2025-05-29 23:23:01] The server is fired up and ready to roll!\n[2025-05-29 23:23:17] INFO:     127.0.0.1:49146 - \"GET /v1/models HTTP/1.1\" 200 OK\n[2025-05-29 23:23:23] INFO:     127.0.0.1:55356 - \"POST /v1/completions HTTP/1.1\" 200 OK\n[2025-05-29 23:23:23] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 0, token usage: 0.00, #running-req: 0, #unbootstrapped-req: 0, #queue-req: 0, #transferring-req: 0\nE0529 23:23:23.432950 69588 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c1cda002800\nE0529 23:23:23.432989 69589 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c1c6e002800\nE0529 23:23:23.433007 69590 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c1c02002800\nE0529 23:23:23.433671 69590 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c1b96002800\nE0529 23:23:23.433724 69589 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c1b2a002800\nE0529 23:23:23.433776 69588 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c1abe002800\nE0529 23:23:23.433799 69588 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c1a52002800\nE0529 23:23:23.433817 69588 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c19e6002800\nE0529 23:23:23.433835 69588 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c197a002800\nE0529 23:23:23.433849 69588 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c190e002800\nE0529 23:23:23.433863 69588 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c18a2002800\nE0529 23:23:23.433880 69588 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c1836002800\nE0529 23:23:23.433909 69589 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c17ca002800\nE0529 23:23:23.433997 69590 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c175e002800\nE0529 23:23:23.434022 69588 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c16f2002800\nE0529 23:23:23.434060 69589 rdma_transport.cpp:323] Memory region not registered by any active device(s): 0x7c1686002800\n[2025-05-29 23:23:23] Session 10.72.0.9:16756 failed.\n[2025-05-29 23:23:23] Prefill transfer failed for request rank=0 req.rid='a5eda8e756dd4d19a913a469ce943fbc' req.bootstrap_room=3583236771377794168 with exception KVTransferError(bootstrap_room=3583236771377794168): Failed to send kv chunk of 3583236771377794168 to 10.72.0.9:44781\n```\n\n```\nroot@nccl-test-host-1:/diagnostic# python3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-Instruct --disaggregation-mode decode --port 30001 --base-gpu-id 1 --disaggregation-ib-device mlx5_0\nKV cache is forced as chunk cache for decode server\n[2025-05-29 23:22:47] server_args=ServerArgs(model_path='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer_path='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='meta-llama/Meta-Llama-3-8B-Instruct', chat_template=None, completion_template=None, is_embedding=False, enable_multimodal=None, revision=None, host='127.0.0.1', port=30001, mem_fraction_static=0.8717961202189594, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=787569464, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=1, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, bucket_time_to_first_token=None, bucket_e2e_request_latency=None, bucket_inter_token_latency=None, collect_tokens_histogram=False, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=True, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=None, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, mm_attention_backend=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='decode', disaggregation_bootstrap_port=8998, disaggregation_transfer_backend='mooncake', disaggregation_ib_device='mlx5_0', pdlb_url=None)\n[2025-05-29 23:22:53] Attention backend not set. Use fa3 backend by default.\n[2025-05-29 23:22:53] Init torch distributed begin.\n[2025-05-29 23:22:53] Init torch distributed ends. mem usage=0.00 GB\n[2025-05-29 23:22:53] init_expert_location from trivial\n[2025-05-29 23:22:54] Load weight begin. avail mem=139.20 GB\n[2025-05-29 23:22:55] Using model weights format ['*.safetensors']\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.30it/s]\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.24it/s]\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.80it/s]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.55it/s]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.52it/s]\n\n[2025-05-29 23:22:58] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=124.23 GB, mem usage=14.98 GB.\n[2025-05-29 23:22:58] KV Cache is allocated. #tokens: 871371, K size: 53.18 GB, V size: 53.18 GB\n[2025-05-29 23:22:58] Memory pool end. avail mem=17.66 GB\n[2025-05-29 23:22:58] Capture cuda graph begin. This can take up to several minutes. avail mem=17.56 GB\n[2025-05-29 23:22:58] Capture cuda graph bs [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256]\nCapturing batches (avail_mem=15.05 GB): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:03<00:00,  8.79it/s]\n[2025-05-29 23:23:02] Capture cuda graph end. Time elapsed: 4.10 s. mem usage=2.51 GB. avail mem=15.05 GB.\n[2025-05-29 23:23:03] max_total_num_tokens=871371, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=4097, context_len=8192\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nI0529 23:23:03.621326 67574 transfer_engine.cpp:350] Metrics reporting is disabled (set MC_TE_METRIC=1 to enable)\nI0529 23:23:03.621340 67574 transfer_engine.cpp:44] Transfer Engine starting. Server: 10.72.0.9, Metadata: P2PHANDSHAKE, ip_or_host_name: , rpc_port: 0\nI0529 23:23:03.621372 67574 transfer_engine.cpp:100] Transfer Engine RPC using P2P handshake, listening on 10.72.0.9:16756\nI0529 23:23:03.621439 67574 transfer_engine.cpp:112] Auto-discovering topology...\nI0529 23:23:03.621906 67574 transfer_engine.cpp:127] Topology discovery complete. Found 1 HCAs.\nI0529 23:23:03.633222 67574 rdma_context.cpp:125] RDMA device: mlx5_0, LID: 0, GID: (GID_Index 3) 00:00:00:00:00:00:00:00:00:00:ff:ff:c0:a8:01:03\nE0529 23:23:04.135735 67574 rdma_context.cpp:203] Failed to register memory 0x7b46f6000000: Bad address [14]\nE0529 23:23:04.135766 67574 rdma_context.cpp:203] Failed to register memory 0x7b468a000000: Bad address [14]\nE0529 23:23:04.135774 67574 rdma_context.cpp:203] Failed to register memory 0x7b461e000000: Bad address [14]\nE0529 23:23:04.135782 67574 rdma_context.cpp:203] Failed to register memory 0x7b45b2000000: Bad address [14]\nE0529 23:23:04.135789 67574 rdma_context.cpp:203] Failed to register memory 0x7b4546000000: Bad address [14]\nE0529 23:23:04.135797 67574 rdma_context.cpp:203] Failed to register memory 0x7b44da000000: Bad address [14]\nE0529 23:23:04.135804 67574 rdma_context.cpp:203] Failed to register memory 0x7b446e000000: Bad address [14]\nE0529 23:23:04.135811 67574 rdma_context.cpp:203] Failed to register memory 0x7b4402000000: Bad address [14]\nE0529 23:23:04.135818 67574 rdma_context.cpp:203] Failed to register memory 0x7b4396000000: Bad address [14]\nE0529 23:23:04.135824 67574 rdma_context.cpp:203] Failed to register memory 0x7b432a000000: Bad address [14]\nE0529 23:23:04.135831 67574 rdma_context.cpp:203] Failed to register memory 0x7b42be000000: Bad address [14]\nE0529 23:23:04.135838 67574 rdma_context.cpp:203] Failed to register memory 0x7b4252000000: Bad address [14]\nE0529 23:23:04.135846 67574 rdma_context.cpp:203] Failed to register memory 0x7b41e6000000: Bad address [14]\nE0529 23:23:04.135854 67574 rdma_context.cpp:203] Failed to register memory 0x7b417a000000: Bad address [14]\nE0529 23:23:04.135862 67574 rdma_context.cpp:203] Failed to register memory 0x7b410e000000: Bad address [14]\nE0529 23:23:04.135870 67574 rdma_context.cpp:203] Failed to register memory 0x7b40a2000000: Bad address [14]\nE0529 23:23:04.135879 67574 rdma_context.cpp:203] Failed to register memory 0x7b4036000000: Bad address [14]\nE0529 23:23:04.135885 67574 rdma_context.cpp:203] Failed to register memory 0x7b3fca000000: Bad address [14]\nE0529 23:23:04.135892 67574 rdma_context.cpp:203] Failed to register memory 0x7b3f5e000000: Bad address [14]\nE0529 23:23:04.135900 67574 rdma_context.cpp:203] Failed to register memory 0x7b3ef2000000: Bad address [14]\nE0529 23:23:04.135905 67574 rdma_context.cpp:203] Failed to register memory 0x7b3e86000000: Bad address [14]\nE0529 23:23:04.135912 67574 rdma_context.cpp:203] Failed to register memory 0x7b3e1a000000: Bad address [14]\nE0529 23:23:04.135918 67574 rdma_context.cpp:203] Failed to register memory 0x7b3dae000000: Bad address [14]\nE0529 23:23:04.135924 67574 rdma_context.cpp:203] Failed to register memory 0x7b3d42000000: Bad address [14]\nE0529 23:23:04.135931 67574 rdma_context.cpp:203] Failed to register memory 0x7b3cd6000000: Bad address [14]\nE0529 23:23:04.135937 67574 rdma_context.cpp:203] Failed to register memory 0x7b3c6a000000: Bad address [14]\nE0529 23:23:04.135943 67574 rdma_context.cpp:203] Failed to register memory 0x7b3bfe000000: Bad address [14]\nE0529 23:23:04.135949 67574 rdma_context.cpp:203] Failed to register memory 0x7b3b92000000: Bad address [14]\nE0529 23:23:04.135957 67574 rdma_context.cpp:203] Failed to register memory 0x7b3b26000000: Bad address [14]\nE0529 23:23:04.135963 67574 rdma_context.cpp:203] Failed to register memory 0x7b3aba000000: Bad address [14]\nE0529 23:23:04.135972 67574 rdma_context.cpp:203] Failed to register memory 0x7b3a4e000000: Bad address [14]\nE0529 23:23:04.135978 67574 rdma_context.cpp:203] Failed to register memory 0x7b39e2000000: Bad address [14]\nE0529 23:23:04.135987 67574 rdma_context.cpp:203] Failed to register memory 0x7b3976000000: Bad address [14]\nE0529 23:23:04.135995 67574 rdma_context.cpp:203] Failed to register memory 0x7b390a000000: Bad address [14]\nE0529 23:23:04.136003 67574 rdma_context.cpp:203] Failed to register memory 0x7b389e000000: Bad address [14]\nE0529 23:23:04.136010 67574 rdma_context.cpp:203] Failed to register memory 0x7b3832000000: Bad address [14]\nE0529 23:23:04.136018 67574 rdma_context.cpp:203] Failed to register memory 0x7b37c6000000: Bad address [14]\nE0529 23:23:04.136024 67574 rdma_context.cpp:203] Failed to register memory 0x7b375a000000: Bad address [14]\nE0529 23:23:04.136044 67574 rdma_context.cpp:203] Failed to register memory 0x7b36ee000000: Bad address [14]\nE0529 23:23:04.136054 67574 rdma_context.cpp:203] Failed to register memory 0x7b3682000000: Bad address [14]\nE0529 23:23:04.136060 67574 rdma_context.cpp:203] Failed to register memory 0x7b3616000000: Bad address [14]\nE0529 23:23:04.136070 67574 rdma_context.cpp:203] Failed to register memory 0x7b35aa000000: Bad address [14]\nE0529 23:23:04.136076 67574 rdma_context.cpp:203] Failed to register memory 0x7b353e000000: Bad address [14]\nE0529 23:23:04.136083 67574 rdma_context.cpp:203] Failed to register memory 0x7b34d2000000: Bad address [14]\nE0529 23:23:04.136091 67574 rdma_context.cpp:203] Failed to register memory 0x7b3466000000: Bad address [14]\nE0529 23:23:04.136098 67574 rdma_context.cpp:203] Failed to register memory 0x7b33fa000000: Bad address [14]\nE0529 23:23:04.136106 67574 rdma_context.cpp:203] Failed to register memory 0x7b338e000000: Bad address [14]\nE0529 23:23:04.136111 67574 rdma_context.cpp:203] Failed to register memory 0x7b3322000000: Bad address [14]\nE0529 23:23:04.136118 67574 rdma_context.cpp:203] Failed to register memory 0x7b32b6000000: Bad address [14]\nE0529 23:23:04.136124 67574 rdma_context.cpp:203] Failed to register memory 0x7b324a000000: Bad address [14]\nE0529 23:23:04.136132 67574 rdma_context.cpp:203] Failed to register memory 0x7b31de000000: Bad address [14]\nE0529 23:23:04.136140 67574 rdma_context.cpp:203] Failed to register memory 0x7b3172000000: Bad address [14]\nE0529 23:23:04.136147 67574 rdma_context.cpp:203] Failed to register memory 0x7b3106000000: Bad address [14]\nE0529 23:23:04.136154 67574 rdma_context.cpp:203] Failed to register memory 0x7b309a000000: Bad address [14]\nE0529 23:23:04.136161 67574 rdma_context.cpp:203] Failed to register memory 0x7b302e000000: Bad address [14]\nE0529 23:23:04.136171 67574 rdma_context.cpp:203] Failed to register memory 0x7b2fc2000000: Bad address [14]\nE0529 23:23:04.136179 67574 rdma_context.cpp:203] Failed to register memory 0x7b2f56000000: Bad address [14]\nE0529 23:23:04.136188 67574 rdma_context.cpp:203] Failed to register memory 0x7b2eea000000: Bad address [14]\nE0529 23:23:04.136195 67574 rdma_context.cpp:203] Failed to register memory 0x7b2e7e000000: Bad address [14]\nE0529 23:23:04.136204 67574 rdma_context.cpp:203] Failed to register memory 0x7b2e12000000: Bad address [14]\nE0529 23:23:04.136209 67574 rdma_context.cpp:203] Failed to register memory 0x7b2da6000000: Bad address [14]\nE0529 23:23:04.136217 67574 rdma_context.cpp:203] Failed to register memory 0x7b2d3a000000: Bad address [14]\nE0529 23:23:04.136225 67574 rdma_context.cpp:203] Failed to register memory 0x7b2cce000000: Bad address [14]\nE0529 23:23:04.136234 67574 rdma_context.cpp:203] Failed to register memory 0x7b2c62000000: Bad address [14]\n[2025-05-29 23:23:04] INFO:     Started server process [67302]\n[2025-05-29 23:23:04] INFO:     Waiting for application startup.\n[2025-05-29 23:23:04] INFO:     Application startup complete.\n[2025-05-29 23:23:04] INFO:     Uvicorn running on http://127.0.0.1:30001 (Press CTRL+C to quit)\n[2025-05-29 23:23:05] INFO:     127.0.0.1:49286 - \"GET /get_model_info HTTP/1.1\" 200 OK\n[2025-05-29 23:23:05] Start of prefill warmup ...\n[2025-05-29 23:23:05] FakeKVReceiver init with kv_indices: [1 2 3 4], aux_index: 0\n[2025-05-29 23:23:05] FakeKVReceiver poll success\n[2025-05-29 23:23:05] INFO:     127.0.0.1:49288 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-05-29 23:23:05] End of prefill warmup with status 200, resp: [{'text': '!.Sep 12, 201', 'meta_info': {'id': '4751d88f75eb40e78683311f4dcf0861', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 4, 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 0.8205685615539551}}]\n[2025-05-29 23:23:05] The server is fired up and ready to roll!\n[2025-05-29 23:23:23] INFO:     127.0.0.1:55580 - \"POST /v1/completions HTTP/1.1\" 200 OK\n[2025-05-29 23:23:23] Decode transfer failed for request rank=0 decode_req.req.rid='06a4dea161644533872e81c5c0ddf9d7' decode_req.req.bootstrap_room=3583236771377794168 with exception KVTransferError(bootstrap_room=3583236771377794168): Failed to get kvcache from prefill instance, it might be dead\n```\n\n```\nroot@nccl-test-host-1:/diagnostic# python3 -m sglang.srt.disaggregation.mini_lb --prefill http://127.0.0.1:30000 --decode http://127.0.0.1:30001 --host 0.0.0.0 --port 8000\nINFO:     Started server process [68774]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\nINFO:     127.0.0.1:57752 - \"GET /v1/models HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:45502 - \"POST /v1/completions HTTP/1.1\" 200 OK\n```\n\n```\nroot@nccl-test-host-1:/diagnostic# python3 -m sglang.bench_serving --backend sglang-oai --port 8000\nbenchmark_args=Namespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=8000, dataset_name='sharegpt', dataset_path='', model=None, tokenizer=None, num_prompts=1000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=inf, max_concurrency=None, output_file=None, output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\nNamespace(backend='sglang-oai', base_url=None, host='0.0.0.0', port=8000, dataset_name='sharegpt', dataset_path='', model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, num_prompts=1000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=inf, max_concurrency=None, output_file=None, output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)\n\n#Input tokens: 296523\n#Output tokens: 186737\nStarting warmup with 1 sequences...\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/diagnostic/sglang/python/sglang/bench_serving.py\", line 1866, in <module>\n    run_benchmark(args)\n  File \"/diagnostic/sglang/python/sglang/bench_serving.py\", line 1616, in run_benchmark\n    return asyncio.run(\n  File \"/usr/lib/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"/diagnostic/sglang/python/sglang/bench_serving.py\", line 1232, in benchmark\n    raise ValueError(\nValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):\n  File \"/diagnostic/sglang/python/sglang/bench_serving.py\", line 222, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n```\n\n### Reproduction\n\n```\npython3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-Instruct --disaggregation-mode prefill --disaggregation-ib-device mlx5_0\npython3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-Instruct --disaggregation-mode decode --port 30001 --base-gpu-id 1 --disaggregation-ib-device mlx5_0\npython3 -m sglang.srt.disaggregation.mini_lb --prefill http://127.0.0.1:30000 --decode http://127.0.0.1:30001 --host 0.0.0.0 --port 8000\npython3 -m sglang.bench_serving --backend sglang-oai --port 8000\n```\n\n### Environment\n\n```\nroot@nccl-test-host-1:/diagnostic# ifconfig\neth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1460\n        inet 10.72.0.9  netmask 255.255.255.0  broadcast 10.72.0.255\n        ether 76:4c:cb:b9:7e:bc  txqueuelen 0  (Ethernet)\n        RX packets 4573150  bytes 46775778978 (46.7 GB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 2239792  bytes 167302216 (167.3 MB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\neth2: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 8896\n        inet 192.168.1.3  netmask 255.255.255.255  broadcast 0.0.0.0\n        ether c2:e0:73:df:b9:01  txqueuelen 1000  (Ethernet)\n        RX packets 11526  bytes 733798 (733.7 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 11355  bytes 723978 (723.9 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\neth3: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 8896\n        inet 192.168.2.3  netmask 255.255.255.255  broadcast 0.0.0.0\n        ether be:71:a0:05:d0:04  txqueuelen 1000  (Ethernet)\n        RX packets 10953  bytes 659154 (659.1 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 11026  bytes 665102 (665.1 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\neth4: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 8896\n        inet 192.168.3.3  netmask 255.255.255.255  broadcast 0.0.0.0\n        ether fe:ab:97:d2:02:07  txqueuelen 1000  (Ethernet)\n        RX packets 10953  bytes 659154 (659.1 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 11026  bytes 665176 (665.1 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\neth5: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 8896\n        inet 192.168.4.3  netmask 255.255.255.255  broadcast 0.0.0.0\n        ether 42:e3:56:2b:76:0a  txqueuelen 1000  (Ethernet)\n        RX packets 10953  bytes 659154 (659.1 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 11021  bytes 664746 (664.7 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\neth6: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 8896\n        inet 192.168.5.3  netmask 255.255.255.255  broadcast 0.0.0.0\n        ether 5e:d6:47:c8:c4:0d  txqueuelen 1000  (Ethernet)\n        RX packets 10953  bytes 659154 (659.1 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 11015  bytes 664120 (664.1 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\neth7: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 8896\n        inet 192.168.6.3  netmask 255.255.255.255  broadcast 0.0.0.0\n        ether 2a:ee:0b:71:1c:10  txqueuelen 1000  (Ethernet)\n        RX packets 10953  bytes 659154 (659.1 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 11021  bytes 664766 (664.7 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\neth8: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 8896\n        inet 192.168.7.3  netmask 255.255.255.255  broadcast 0.0.0.0\n        ether 82:fa:1e:d1:9e:13  txqueuelen 1000  (Ethernet)\n        RX packets 10953  bytes 659154 (659.1 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 11016  bytes 664230 (664.2 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\neth9: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 8896\n        inet 192.168.8.3  netmask 255.255.255.255  broadcast 0.0.0.0\n        ether ca:52:0a:55:15:16  txqueuelen 1000  (Ethernet)\n        RX packets 10952  bytes 659094 (659.0 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 11016  bytes 664240 (664.2 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\nlo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536\n        inet 127.0.0.1  netmask 255.0.0.0\n        loop  txqueuelen 1000  (Local Loopback)\n        RX packets 410600179  bytes 39325030589 (39.3 GB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 410600179  bytes 39325030589 (39.3 GB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n```\n```\nroot@nccl-test-host-1:/diagnostic# rdma link\nlink mlx5_0/1 state ACTIVE physical_state LINK_UP netdev eth2\nlink mlx5_1/1 state ACTIVE physical_state LINK_UP netdev eth3\nlink mlx5_2/1 state ACTIVE physical_state LINK_UP netdev eth4\nlink mlx5_3/1 state ACTIVE physical_state LINK_UP netdev eth5\nlink mlx5_4/1 state ACTIVE physical_state LINK_UP netdev eth6\nlink mlx5_5/1 state ACTIVE physical_state LINK_UP netdev eth7\nlink mlx5_6/1 state ACTIVE physical_state LINK_UP netdev eth8\nlink mlx5_7/1 state ACTIVE physical_state LINK_UP netdev eth9\n```\n```\nroot@nccl-test-host-1:/diagnostic# ibv_devices\n    device          \t   node GUID\n    ------          \t----------------\n    mlx5_0          \tc2e073fffedfb901\n    mlx5_1          \tbe71a0fffe05d004\n    mlx5_2          \tfeab97fffed20207\n    mlx5_3          \t42e356fffe2b760a\n    mlx5_4          \t5ed647fffec8c40d\n    mlx5_5          \t2aee0bfffe711c10\n    mlx5_6          \t82fa1efffed19e13\n    mlx5_7          \tca520afffe551516\n```\n```\nroot@nccl-test-host-1:/diagnostic# nvidia-smi\nThu May 29 23:20:05 2025\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA H200                    Off |   00000000:8F:00.0 Off |                    0 |\n| N/A   35C    P0             78W /  700W |       1MiB / 143771MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA H200                    Off |   00000000:90:00.0 Off |                    0 |\n| N/A   37C    P0             78W /  700W |       1MiB / 143771MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   2  NVIDIA H200                    Off |   00000000:96:00.0 Off |                    0 |\n| N/A   35C    P0             78W /  700W |       1MiB / 143771MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   3  NVIDIA H200                    Off |   00000000:97:00.0 Off |                    0 |\n| N/A   38C    P0             78W /  700W |       1MiB / 143771MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   4  NVIDIA H200                    Off |   00000000:C4:00.0 Off |                    0 |\n| N/A   35C    P0             77W /  700W |       1MiB / 143771MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   5  NVIDIA H200                    Off |   00000000:C5:00.0 Off |                    0 |\n| N/A   37C    P0             77W /  700W |       1MiB / 143771MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   6  NVIDIA H200                    Off |   00000000:CB:00.0 Off |                    0 |\n| N/A   34C    P0             77W /  700W |       1MiB / 143771MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   7  NVIDIA H200                    Off |   00000000:CC:00.0 Off |                    0 |\n| N/A   35C    P0             78W /  700W |       1MiB / 143771MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n```",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "open",
    "created_at": "2025-05-29T23:27:04+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6753/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/6753"
  },
  {
    "number": 3538,
    "title": "[Bug]NCCL error if enable the cuda graph",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n<img width=\"1663\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e3b396cc-4771-474d-8843-d43d8d5dbf90\" />\n\nIf I don't disable cuda graph, I will get the error shown in the picture when the cuda graph is being inited. If i use the official docker image, i will not get the error. The only difference of the environment with the docker is the sglang by observing the output of `python3 -m sglang.check_env`. I install sglang via pip and i have observed the sglang of docker image is installed from local.\n\n### Reproduction\n\n```bash\npython3 -m sglang.launch_server --model-path deepseekr1 --tp 16 --dist-init-addr 29.111.44.27:20000 --nnodes 2 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 8000\n```\nwill reproduct the error above.\nadd the option `--disable-cuda-graph` will run well\n\n### Environment\n\n```\nINFO 02-13 14:28:55 __init__.py:190] Automatically detected platform cuda.\nPython: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.161.08\nPyTorch: 2.5.1+cu124\nsglang: 0.4.2.post4\nsgl_kernel: 0.0.3.post3\nflashinfer: 0.2.0.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.61.1\ntiktoken: 0.8.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10   NIC11   NIC12   NIC13   NIC14   NIC15   NIC16NIC17    NIC18   NIC19   NIC20   NIC21   NIC22   NIC23   NIC24   NIC25   NIC26   NIC27   NIC28   NIC29   NIC30   NIC31   NIC32   NIC33   NIC34   NIC35   NIC36   NIC37   NIC38   NIC39   NIC40   NIC41   CPU Affinity  NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-95,192-287  0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    PHB     PIX     SYS     SYS     SYS     SYS     0-95,192-287  0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    PIX     PHB     SYS     SYS     SYS     SYS     0-95,192-287  0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-95,192-287  0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    96-191,288-383        1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    96-191,288-383        1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     PHB     NODE    NODE    PIX     96-191,288-383        1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     NODE    NODE    PHB     96-191,288-383        1               N/A\nNIC0    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE     X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC1    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC2    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC3    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC4    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC9    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC10   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC11   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC12   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC13   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC14   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC15   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC16   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X   PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC17   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX   X       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC18   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX       X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC19   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC20   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC21   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC22   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC23   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC24   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC25   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC26   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC27   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC28   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC29   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC30   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC31   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC32   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC33   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC34   PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC35   NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     SYS     SYS     SYS\nNIC36   NODE    PHB     PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      PHB     SYS     SYS     SYS     SYS\nNIC37   NODE    PIX     PHB     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    PHB      X      SYS     SYS     SYS     SYS\nNIC38   SYS     SYS     SYS     SYS     NODE    NODE    PHB     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    PHB\nNIC39   SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE\nNIC40   SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE\nNIC41   SYS     SYS     SYS     SYS     NODE    NODE    PIX     PHB     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     PHB     NODE    NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0:         NIC41   CPU Affinity    NUMA Affinity   GPU NUMA ID\n  NIC1: MA ID\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n  NIC10: mlx5_10\n  NIC11: mlx5_11\n  NIC12: mlx5_12\n  NIC13: mlx5_13\n  NIC14: mlx5_14\n  NIC15: mlx5_15\n  NIC16: mlx5_16\n  NIC17: mlx5_17\n  NIC18: mlx5_18\n  NIC19: mlx5_19\n  NIC20: mlx5_20\n  NIC21: mlx5_21\n  NIC22: mlx5_22\n  NIC23: mlx5_23\n  NIC24: mlx5_24\n  NIC25: mlx5_25\n  NIC26: mlx5_26\n  NIC27: mlx5_27\n  NIC28: mlx5_28\n  NIC29: mlx5_29\n  NIC30: mlx5_30\n  NIC31: mlx5_31\n  NIC32: mlx5_32\n  NIC33: mlx5_33\n  NIC34: mlx5_bond_1\n  NIC35: mlx5_bond_2\n  NIC36: mlx5_bond_3\n  NIC37: mlx5_bond_4\n  NIC38: mlx5_bond_5\n  NIC39: mlx5_bond_6\n  NIC40: mlx5_bond_7\n  NIC41: mlx5_bond_8\n\n\nulimit soft: 1000000\n```",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-02-13T06:38:16+00:00",
    "closed_at": "2025-02-19T14:35:47+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3538/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3538"
  },
  {
    "number": 747,
    "title": "[Bug] Assertion error: Exception in ModelTpServer: This happens when we do return_log_prob=True",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n\n### Describe the bug\n\nWhen I have given the argument return_log_probe=True. Its showing the below message. I am getting this in sagemaker environment the instance type is \n\n### Reproduction\n\nINFO:     127.0.0.1:59618 - \"POST /generate/ HTTP/1.1\" 307 Temporary Redirect\r\n[gpu_id=0] Prefill batch. #new-seq: 1, #new-token: 2, #cached-token: 42, cache hit rate: 45.26%, #running-req: 0, #queue-req: 0\r\nException in ModelTpServer:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/controller/tp_worker.py\", line 186, in exposed_step\r\n    self.forward_step()\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/controller/tp_worker.py\", line 216, in forward_step\r\n    self.forward_decode_batch(self.running_batch)\r\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/controller/tp_worker.py\", line 567, in forward_decode_batch\r\n    output = self.model_runner.forward(batch, ForwardMode.DECODE)\r\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/controller/model_runner.py\", line 334, in forward\r\n    return self.forward_decode(batch)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/controller/model_runner.py\", line 274, in forward_decode\r\n    return self.cuda_graph_runner.replay(batch)\r\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/controller/cuda_graph_runner.py\", line 188, in replay\r\n    assert not batch.return_logprob\r\nAssertionError\r\n \r\nException in ControllerSingle:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/controller/manager_single.py\", line 151, in start_controller_process\r\n    controller.loop_for_forward()\r\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/controller/manager_single.py\", line 88, in loop_for_forward\r\n    out_pyobjs = self.tp_server.exposed_step(recv_reqs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/controller/tp_worker.py\", line 186, in exposed_step\r\n    self.forward_step()\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/controller/tp_worker.py\", line 216, in forward_step\r\n    self.forward_decode_batch(self.running_batch)\r\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/controller/tp_worker.py\", line 567, in forward_decode_batch\r\n    output = self.model_runner.forward(batch, ForwardMode.DECODE)\r\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/controller/model_runner.py\", line 334, in forward\r\n    return self.forward_decode(batch)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/controller/model_runner.py\", line 274, in forward_decode\r\n    return self.cuda_graph_runner.replay(batch)\r\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/controller/cuda_graph_runner.py\", line 188, in replay\r\n    assert not batch.return_logprob\r\nAssertionError\r\n \r\nKilled\n\n### Environment\n\n```Shell\n/bin/sh: 1: /usr/local/cuda/bin/nvcc: not found\r\nPython: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA A10G\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Not Available\r\nCUDA Driver Version: 535.183.01\r\nPyTorch: 2.3.1+cu121\r\nsglang: 0.2.1\r\nflashinfer: 0.1.1+cu121torch2.3\r\nrequests: 2.31.0\r\ntqdm: 4.66.4\r\nnumpy: 1.26.4\r\naiohttp: 3.9.5\r\nfastapi: 0.110.3\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.2\r\ninteregular: 0.3.3\r\npackaging: 23.2\r\npillow: Module Not Found\r\npsutil: 5.9.8\r\npydantic: 2.8.2\r\nuvicorn: 0.29.0\r\nuvloop: 0.19.0\r\nzmq: 26.0.3\r\nvllm: 0.5.3.post1\r\nopenai: 1.37.1\r\nanthropic: 0.31.2\r\nNVIDIA Topology: \r\n        GPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-15    0               N/A\r\n \r\nLegend:\r\n \r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n \r\nulimit soft: 65536\n```\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-07-26T13:12:31+00:00",
    "closed_at": "2024-07-28T02:15:16+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/747/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/747"
  },
  {
    "number": 4497,
    "title": "[Bug] Running DeepSeek V2.5 error when enable torch-compile",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI use the latest main ([d1112d8548eb13c842900b3a8d622345f9737759](https://github.com/sgl-project/sglang/commit/d1112d8548eb13c842900b3a8d622345f9737759)), start the DeepSeek V2.5 bf16 model, and when using the `--enable-torch-compile` parameter, an error is report.\n\n### Reproduction\n\n```\npython3 -m sglang.launch_server --model /path/to/DeepSeek-V2.5-1210 --trust-remote --dtype bfloat16 --host 0.0.0.0 --port 30000 --tp 8 --enable-torch-compile --torch-compile-max-bs 4\n```\n\nThe exception information is as follows:\n```\n[2025-03-17 07:53:55 TP7] Registering 2420 cuda graph addresses\n[rank4]:E0317 07:53:56.045000 2265 torch/_guards.py:283] [14/1] Error while creating guard:\n[rank4]:E0317 07:53:56.045000 2265 torch/_guards.py:283] [14/1] Name: \"G['__import_sglang_dot_srt_dot_layers_dot_moe_dot_topk'].grouped_topk.__defaults__[2]\"\n[rank4]:E0317 07:53:56.045000 2265 torch/_guards.py:283] [14/1]     Source: global\n[rank4]:E0317 07:53:56.045000 2265 torch/_guards.py:283] [14/1]     Create Function: CONSTANT_MATCH\n[rank4]:E0317 07:53:56.045000 2265 torch/_guards.py:283] [14/1]     Guard Types: None\n[rank4]:E0317 07:53:56.045000 2265 torch/_guards.py:283] [14/1]     Code List: None\n[rank4]:E0317 07:53:56.045000 2265 torch/_guards.py:283] [14/1]     Object Weakref: None\n[rank4]:E0317 07:53:56.045000 2265 torch/_guards.py:283] [14/1]     Guarded Class Weakref: None\n[rank4]:E0317 07:53:56.045000 2265 torch/_guards.py:283] [14/1] Traceback (most recent call last):\n[rank4]:E0317 07:53:56.045000 2265 torch/_guards.py:283] [14/1]   File \"/usr/local/lib/python3.10/dist-packages/torch/_guards.py\", line 281, in create\n[rank4]:E0317 07:53:56.045000 2265 torch/_guards.py:283] [14/1]     return self.create_fn(builder, self)\n[rank4]:E0317 07:53:56.045000 2265 torch/_guards.py:283] [14/1]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py\", line 1576, in CONSTANT_MATCH\n[rank4]:E0317 07:53:56.045000 2265 torch/_guards.py:283] [14/1]     val = self.get(guard.name)\n[rank4]:E0317 07:53:56.045000 2265 torch/_guards.py:283] [14/1]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py\", line 1148, in get\n[rank4]:E0317 07:53:56.045000 2265 torch/_guards.py:283] [14/1]     return eval(name, self.scope, CLOSURE_VARS)\n[rank4]:E0317 07:53:56.045000 2265 torch/_guards.py:283] [14/1]   File \"<string>\", line 1, in <module>\n[rank4]:E0317 07:53:56.045000 2265 torch/_guards.py:283] [14/1] TypeError: 'NoneType' object is not subscriptable\n[2025-03-17 07:53:56 TP4] Registering 2420 cuda graph addresses\n[2025-03-17 07:53:56 TP7] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 249, in __init__\n    self.capture()\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 333, in capture\n    ) = self.capture_one_batch_size(bs, forward)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 425, in capture_one_batch_size\n    run_once()\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 418, in run_once\n    logits_output = forward(input_ids, forward_batch.positions, forward_batch)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 465, in _fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py\", line 40, in inner\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1080, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1042, in forward\n    hidden_states, residual = layer(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 958, in forward\n    hidden_states = self.self_attn(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1269, in __call__\n    return self._torchdynamo_orig_callable(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n    result = self._inner_convert(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n    return _compile(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 952, in _compile\n    raise InternalTorchDynamoError(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n    guarded_code = compile_inner(code, one_graph, hooks, transform)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n    return _compile_inner(code, one_graph, hooks, transform)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n    return function(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 796, in _compile_inner\n    check_fn = CheckFunctionManager(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py\", line 2261, in __init__\n    guard.create(builder)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_guards.py\", line 281, in create\n    return self.create_fn(builder, self)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py\", line 1576, in CONSTANT_MATCH\n    val = self.get(guard.name)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/guards.py\", line 1148, in get\n    return eval(name, self.scope, CLOSURE_VARS)\n  File \"<string>\", line 1, in <module>\ntorch._dynamo.exc.InternalTorchDynamoError: TypeError: 'NoneType' object is not subscriptable\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1807, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 226, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 74, in __init__\n    self.model_runner = ModelRunner(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 167, in __init__\n    self.initialize(min_per_gpu_memory)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 208, in initialize\n    self.init_cuda_graphs()\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 889, in init_cuda_graphs\n    self.cuda_graph_runner = CudaGraphRunner(self)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 251, in __init__\n    raise Exception(\nException: Capture cuda graph failed: TypeError: 'NoneType' object is not subscriptable\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n\nPossible solutions:\n1. disable cuda graph by --disable-cuda-graph\n2. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)\n3. disable torch compile by not using --enable-torch-compile\n4. set --cuda-graph-max-bs to a smaller value (e.g., 32)\nOpen an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose\n```\n\n### Environment\n\n```\nPython: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA A800-SXM4-80GB\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.54.14\nPyTorch: 2.5.1+cu124\nsgl_kernel: 0.0.5.post2\nflashinfer: 0.2.3+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.13\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.3\ninteregular: 0.3.3\nmodelscope: 1.23.2\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.3.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.66.3\ntiktoken: 0.9.0\nanthropic: 0.49.0\ndecord: 0.6.0\nNVIDIA Topology:\n\tGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tNIC4\tNIC5\tNIC6\tNIC7\tNIC8\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tNV8\tNV8\tNV8\tNV8\tNV8\tNV8\tNV8\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU1\tNV8\t X \tNV8\tNV8\tNV8\tNV8\tNV8\tNV8\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU2\tNV8\tNV8\t X \tNV8\tNV8\tNV8\tNV8\tNV8\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU3\tNV8\tNV8\tNV8\t X \tNV8\tNV8\tNV8\tNV8\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU4\tNV8\tNV8\tNV8\tNV8\t X \tNV8\tNV8\tNV8\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\t32-63,96-127\t1\t\tN/A\nGPU5\tNV8\tNV8\tNV8\tNV8\tNV8\t X \tNV8\tNV8\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\t32-63,96-127\t1\t\tN/A\nGPU6\tNV8\tNV8\tNV8\tNV8\tNV8\tNV8\t X \tNV8\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\t32-63,96-127\t1\t\tN/A\nGPU7\tNV8\tNV8\tNV8\tNV8\tNV8\tNV8\tNV8\t X \tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\t32-63,96-127\t1\t\tN/A\nNIC0\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPIX\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\nNIC1\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPIX\t X \tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\nNIC2\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPIX\tSYS\tSYS\tSYS\tSYS\tSYS\nNIC3\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPIX\t X \tSYS\tSYS\tSYS\tSYS\tSYS\nNIC4\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPIX\tSYS\tSYS\tSYS\nNIC5\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPIX\t X \tSYS\tSYS\tSYS\nNIC6\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPIX\tSYS\nNIC7\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPIX\t X \tSYS\nNIC8\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_2\n  NIC1: mlx5_3\n  NIC2: mlx5_4\n  NIC3: mlx5_5\n  NIC4: mlx5_6\n  NIC5: mlx5_7\n  NIC6: mlx5_8\n  NIC7: mlx5_9\n  NIC8: mlx5_bond_0\n\n\nulimit soft: 1048576\n```",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-03-17T08:23:31+00:00",
    "closed_at": "2025-03-18T04:42:56+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4497/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4497"
  },
  {
    "number": 77,
    "title": "Not able to run AWQ Mixtral on 4xA10",
    "body": "Hi,\r\n\r\nIm trying to run the AWQ version of Mixtral on 4xA10s. However im getting this error. Ive also tried with `--mem-frac 0.7` and still got the same error\r\n\r\nModel I'm using : https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ\r\n\r\nCommand : `python -m sglang.launch_server --model-path /local_disk0/TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ/ --port 30000 --tp 4`\r\n\r\nCode : \r\n```\r\nfrom sglang import function, system, user, assistant, gen\r\nimport sglang as sgl\r\n\r\n@function\r\ndef multi_turn_question(s, question_1, question_2):\r\n    s += system(\"You are a helpful assistant.\")\r\n    s += user(question_1)\r\n    s += assistant(gen(\"answer_1\", max_tokens=256))\r\n    s += user(question_2)\r\n    s += assistant(gen(\"answer_2\", max_tokens=256))\r\n\r\nstate = multi_turn_question.run(\r\n    question_1=\"What is the capital of the United Kingdom?\",\r\n    question_2=\"List two local attractions.\",\r\n    temperature=0.7,\r\n    stream=True,\r\n)\r\n\r\nfor out in state.text_iter():\r\n    print(out, end=\"\", flush=True)\r\nprint()\r\n\r\n```\r\n\r\nError\r\n```\r\nnew fill batch. #seq: 1. #cached_token: 0. #new_token: 34. #remaining_req: 0. #running_req: 0\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 140, in exposed_step\r\n    self.forward_step()\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 155, in forward_step\r\n    self.forward_fill_batch(new_batch)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 349, in forward_fill_batch\r\n    next_token_ids, next_token_probs = batch.sample(logits)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/infer_batch.py\", line 375, in sample\r\n    sampled_index = torch.multinomial(probs_sort, num_samples=1)\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 140, in exposed_step\r\n    self.forward_step()\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 155, in forward_step\r\n    self.forward_fill_batch(new_batch)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 349, in forward_fill_batch\r\n    next_token_ids, next_token_probs = batch.sample(logits)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/infer_batch.py\", line 375, in sample\r\n    sampled_index = torch.multinomial(probs_sort, num_samples=1)\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 140, in exposed_step\r\n    self.forward_step()\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 155, in forward_step\r\n    self.forward_fill_batch(new_batch)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 349, in forward_fill_batch\r\n    next_token_ids, next_token_probs = batch.sample(logits)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/infer_batch.py\", line 375, in sample\r\n    sampled_index = torch.multinomial(probs_sort, num_samples=1)\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 140, in exposed_step\r\n    self.forward_step()\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 155, in forward_step\r\n    self.forward_fill_batch(new_batch)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 349, in forward_fill_batch\r\n    next_token_ids, next_token_probs = batch.sample(logits)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/infer_batch.py\", line 375, in sample\r\n    sampled_index = torch.multinomial(probs_sort, num_samples=1)\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py:179: UserWarning: Warning: available_size=391285, max_total_num_token=391319\r\nKV cache pool leak detected!\r\n  warnings.warn(\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py:179: UserWarning: Warning: available_size=391285, max_total_num_token=391319\r\nKV cache pool leak detected!\r\n  warnings.warn(\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py:179: UserWarning: Warning: available_size=391285, max_total_num_token=391319\r\nKV cache pool leak detected!\r\n  warnings.warn(\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py:179: UserWarning: Warning: available_size=391285, max_total_num_token=391319\r\nKV cache pool leak detected!\r\n  warnings.warn(\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-01-22T18:23:22+00:00",
    "closed_at": "2024-02-22T14:58:11+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/77/reactions",
      "total_count": 3,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/77"
  },
  {
    "number": 2160,
    "title": "[Bug] FusedMoE compatible with vllm 0.6.3.post1",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nN/A\n\n### Reproduction\n\nN/A\n\n### Environment\n\nN/A",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-11-24T13:38:13+00:00",
    "closed_at": "2024-11-24T14:37:05+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2160/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/2160"
  },
  {
    "number": 6592,
    "title": "[Bug] load microsoft/MAI-DS-R1 error: KeyError: 'model.layers.3.mlp.shared_experts.down_proj.weight_scale'",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI used the following shell to load the microsoft/MAI-DS-R1 model, but the following error occurred\uff1a**KeyError: 'model.layers.3.mlp.shared_experts.down_proj.weight_scale'**\n\n```shell\n#!/bin/bash\nset -x\n\nIMAGE_NAME=lmsysorg/sglang:latest\n\nport=xxxx\nname=mai_ds_r1_sglang_multinode1\nranks=0\n\ndocker run --gpus all -d \\\n    --shm-size 32g \\\n    --network=host \\\n    -v /data/models/MAI-DS-R1:/data/models/MAI-DS-R1 \\\n    --name $name \\\n    -it \\\n    --rm \\\n    --ipc=host \\\n    -e GLOO_SOCKET_IFNAME=eth0 \\\n    -e PORT=$port \\\n    -e NCCL_IB_HCA=mlx5_ \\\n    -e NCCL_IB_DISABLE=0 \\\n    -e NCCL_SOCKET_IFNAME=eth0 \\\n    $IMAGE_NAME \\\n    python3 -m sglang.launch_server --model-path /data/models/MAI-DS-R1 --tp 16 --dist-init-addr x.x.x.x:xxxx --nnodes 2 --node-rank $ranks --trust-remote-code --host 0.0.0.0 --port $port\n```\n\n[xxxxxx TP6] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2014, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 258, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 75, in __init__\n    self.model_runner = ModelRunner(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 184, in __init__\n    self.initialize(min_per_gpu_memory)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 194, in initialize\n    self.load_model()\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 437, in load_model\n    self.model = get_model(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/__init__.py\", line 22, in get_model\n    return loader.load_model(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/loader.py\", line 377, in load_model\n    model.load_weights(self._get_all_weights(model_config, model))\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1663, in load_weights\n    weights_dict[shared_expert_weight_name],\n**KeyError: 'model.layers.3.mlp.shared_experts.down_proj.weight_scale'**\n\nThank you.\n\n### Reproduction\n\nmicrosoft/MAI-DS-R1\n\n```shell\n#!/bin/bash\nset -x\n\nIMAGE_NAME=lmsysorg/sglang:latest\n\nport=xxxx\nname=mai_ds_r1_sglang_multinode1\nranks=0\n\ndocker run --gpus all -d \\\n    --shm-size 32g \\\n    --network=host \\\n    -v /data/models/MAI-DS-R1:/data/models/MAI-DS-R1 \\\n    --name $name \\\n    -it \\\n    --rm \\\n    --ipc=host \\\n    -e GLOO_SOCKET_IFNAME=eth0 \\\n    -e PORT=$port \\\n    -e NCCL_IB_HCA=mlx5_ \\\n    -e NCCL_IB_DISABLE=0 \\\n    -e NCCL_SOCKET_IFNAME=eth0 \\\n    $IMAGE_NAME \\\n    python3 -m sglang.launch_server --model-path /data/models/MAI-DS-R1 --tp 16 --dist-init-addr x.x.x.x:xxxx --nnodes 2 --node-rank $ranks --trust-remote-code --host 0.0.0.0 --port $port\n```\n\n### Environment\n\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.161.08\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post1\nsgl_kernel: 0.1.0\nflashinfer: Module Not Found\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.10.0\nnumpy: 2.2.5\naiohttp: 3.11.18\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.17\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.3\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.2\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.17\nopenai: 1.76.1\ntiktoken: 0.9.0\nanthropic: 0.50.0\nlitellm: 1.67.4.post1\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    SYS     PIX     NODE    SYS     SYS     0-89    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     PIX     NODE    SYS     SYS     0-89    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     NODE    PIX     SYS     SYS     0-89    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     NODE    PIX     SYS     SYS     0-89    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     PIX     NODE    90-179  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     PIX     NODE    90-179  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     NODE    PIX     90-179  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     NODE    PIX     90-179  1               N/A\nNIC0    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS\nNIC1    PIX     PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS      X      NODE    SYS     SYS\nNIC2    NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     SYS     NODE     X      SYS     SYS\nNIC3    SYS     SYS     SYS     SYS     PIX     PIX     NODE    NODE    SYS     SYS     SYS      X      NODE\nNIC4    SYS     SYS     SYS     SYS     NODE    NODE    PIX     PIX     SYS     SYS     SYS     NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n\n\nHypervisor vendor: KVM\nulimit soft: 1048576",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-05-25T13:37:42+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6592/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6592"
  },
  {
    "number": 7062,
    "title": "[Bug] test_lora.py bug",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThere is a prompt \"AI is a field of computer science focused on\" in `test/srt/models/lora/test_lora.py ` that can easily break CI, which might be caused some internal bug of lora.\n\nWe remove this prompt temporarily in #7061. It should be added back after this bug is fixed.\n\n### Reproduction\n\nUncomment line 49 of `test_lora.py`\n\n![Image](https://github.com/user-attachments/assets/e91d22ca-9def-400e-abd7-a21fd8985d54)\n\nthen run\n```bash\npython3 test/srt/models/lora/test_lora.py \n```\n\n### Environment\n\nLatest main branch",
    "labels": [
      "bug",
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-06-10T18:10:20+00:00",
    "closed_at": "2025-06-28T04:28:35+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7062/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7062"
  },
  {
    "number": 5897,
    "title": "[Bug] HF_Runner can't produce correct results after applying lora",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nFirst batch input lora_path as [a, a]. Second batch input lora_path as [None, None]. The second batch will be processed as if you had input lora_path as [a, a].\n\n### Reproduction\n\n```\n# Copyright 2023-2024 SGLang Team\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport multiprocessing as mp\nimport unittest\n\nimport torch\n\nfrom sglang.test.runners import HFRunner, SRTRunner\nfrom sglang.test.test_utils import CustomTestCase\n\nLORA_SETS = [\n    # {\n    #     \"base\": \"meta-llama/Llama-2-7b-hf\",\n    #     \"loras\": [\"RuterNorway/Llama-2-7b-chat-norwegian-LoRa\"],\n    # },\n    {\"base\": \"meta-llama/Llama-2-7b-hf\", \"loras\": [\"winddude/wizardLM-LlaMA-LoRA-7B\"]},\n    # {\"base\": \"Qwen/Qwen2.5-14B-Instruct\", \"loras\": [\"mssongit/Qwen2.5-14B-SFT-LoRA\"]},\n    # {\"base\": \"mistralai/Mistral-7B-Instruct-v0.3\", \"loras\": [\"/home/ying/test_lora\"]},\n    # {\n    # \"base\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n    #     \"loras\": [\n    #         \"/home/ying/test_lora\",\n    #         \"/home/ying/test_lora_1\",\n    #         \"/home/ying/test_lora_2\",\n    #         \"/home/ying/test_lora_3\",\n    #         \"/home/ying/test_lora_4\",\n    #     ],\n    # },\n    # {\"base\": \"meta-llama/Llama-2-7b-hf\", \"loras\": [\"yard1/llama-2-7b-sql-lora-test\"]},\n]\nTORCH_DTYPES = [torch.float16]\n\nPROMPTS = [\n    \"\"\"\n### Instruction:\nWrite a poem about the transformers Python library.\nMention the word \"large language models\" in that poem.\n### Response:\nThe Transformers are large language models,\nThey're used to make predictions on text.\n\"\"\",\n    \"\"\"\n### Instruction:\nWrite a poem about the transformers Python library.\nMention the word \"large language models\" in that poem.\n### Response:\nThe Transformers are large language models,\nThey're used to make predictions on text.\n\"\"\",\n]\n\n\nclass TestLoRA(CustomTestCase):\n\n    def inference(self, prompts, lora_set, tp_size, torch_dtype, max_new_tokens):\n        print(\"=================== testing inference =======================\")\n        base_path = lora_set[\"base\"]\n        all_lora_paths = lora_set[\"loras\"]\n        batch_lora_paths = [all_lora_paths[0], all_lora_paths[0]]\n\n        with HFRunner(\n            base_path, torch_dtype=torch_dtype, model_type=\"generation\"\n        ) as hf_runner:\n            hf_outputs = hf_runner.forward(\n                prompts, max_new_tokens=max_new_tokens, lora_paths=batch_lora_paths\n            )\n\n            hf_no_lora_outputs = hf_runner.forward(\n                prompts, max_new_tokens=max_new_tokens, lora_paths=[None] * len(prompts)\n            )\n        with HFRunner(\n            base_path, torch_dtype=torch_dtype, model_type=\"generation\"\n        ) as hf_runner:\n            hf_no_lora_outputs1 = hf_runner.forward(\n                prompts, max_new_tokens=max_new_tokens, lora_paths=[None] * len(prompts)\n            )\n\n        # compare output strings\n        print(f\"{hf_outputs.output_strs=}\")\n        print(f\"{hf_no_lora_outputs.output_strs=}\")\n        print(f\"{hf_no_lora_outputs1.output_strs=}\")\n\n        for i in range(len(prompts)):\n            assert hf_no_lora_outputs.output_strs[i].strip(\n                \" \"\n            ) == hf_no_lora_outputs1.output_strs[i].strip(\" \"), (\n                hf_no_lora_outputs.output_strs[i].strip(\" \"),\n                hf_no_lora_outputs1.output_strs[i].strip(\" \"),\n            )\n\n    def test_all(self):\n        for lora_set in LORA_SETS:\n            for torch_dtype in TORCH_DTYPES:\n                tp_size = 1\n                max_new_tokens = 32\n                self.inference(PROMPTS, lora_set, tp_size, torch_dtype, max_new_tokens)\n\n\nif __name__ == \"__main__\":\n    try:\n        mp.set_start_method(\"spawn\")\n    except RuntimeError:\n        pass\n\n    unittest.main(warnings=\"ignore\")\n\n```\n\n### Environment\n\n```\npython3 -m sglang.check_env\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0: NVIDIA H100 80GB HBM3\nGPU 0 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.144.03\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post1\nsgl_kernel: 0.1.0\nflashinfer: Module Not Found\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.14\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.24.0\norjson: 3.10.15\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.1.dev5770+g268c325.precompiled\nxgrammar: 0.1.17\nopenai: 1.68.2\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.63.14\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4      NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS       SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     NODE    NODE    SYS       SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    PIX     NODE    SYS       SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     SYS       SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX       NODE    NODE    NODE    32-63,96-127    1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE      PIX     NODE    NODE    32-63,96-127    1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE      NODE    PIX     NODE    32-63,96-127    1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     NODE      NODE    NODE    PIX     32-63,96-127    1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS       SYS     SYS     SYS\nNIC1    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS       SYS     SYS     SYS\nNIC2    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS       SYS     SYS     SYS\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS       SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X        NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE       X      NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE      NODE     X      NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE      NODE    NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n\n\nulimit soft: 1048576\n```",
    "labels": [
      "bug",
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-04-30T00:10:58+00:00",
    "closed_at": "2025-04-30T03:17:43+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5897/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5897"
  },
  {
    "number": 4876,
    "title": "[Bug] Logprobs overflow to -3.4e+38",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n![Image](https://github.com/user-attachments/assets/b3567c95-1206-4ef5-aeea-de21ee71f0d3)\n\nlogprobs overflow to the maximum negative value of fp32\n\n### Reproduction\n\nI'm using Qwen2.5-14B-Instruct\n\ncommand:\n\n```python\nsampling_params = {\n    \"temperature\": 0.9,\n    \"top_p\": 0.9,\n    \"skip_special_tokens\": False,\n    \"stop\": \"<|im_end|>\",\n}\n\nret = await self.engine.async_generate(\n    input_ids=ids,\n    sampling_params=sampling_params,\n    return_logprob=True,\n)\n```\n\n### Environment\n\n```\nPython: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H800\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.3, V12.3.107\nCUDA Driver Version: 535.161.08\nPyTorch: 2.6.0+cu124\nsglang: 0.4.4.post1\nsgl_kernel: 0.0.5.post3\nflashinfer: 0.2.3+cu124torch2.5\ntriton: 3.2.0\ntransformers: 4.48.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.9.1\nfastapi: 0.115.5\nhf_transfer: 0.1.9\nhuggingface_hub: 0.26.2\ninteregular: 0.3.3\nmodelscope: 1.23.1\norjson: 3.10.11\npackaging: 23.2\npsutil: 5.9.4\npydantic: 2.10.5\nmultipart: 0.0.18\nzmq: 25.1.2\nuvicorn: 0.22.0\nuvloop: 0.21.0\nvllm: 0.7.3.dev68+g9cf47594.d20250213\nopenai: 1.59.6\nanthropic: 0.49.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    CPU Affinity    NUMA Affinity GPU NUMA ID\nGPU0     X      NV8     NV8     NV8     NV8     NV8     NV8     NV8     PIX     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0,8,16,24,34    0    N/A\nGPU1    NV8      X      NV8     NV8     NV8     NV8     NV8     NV8     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     2,10,18,30      2    N/A\nGPU2    NV8     NV8      X      NV8     NV8     NV8     NV8     NV8     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     6,14,22,28      3    N/A\nGPU3    NV8     NV8     NV8      X      NV8     NV8     NV8     NV8     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     4,12,20,26      1    N/A\nGPU4    NV8     NV8     NV8     NV8      X      NV8     NV8     NV8     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     1,9,19,27,33    4    N/A\nGPU5    NV8     NV8     NV8     NV8     NV8      X      NV8     NV8     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     3,11,15,21      6    N/A\nGPU6    NV8     NV8     NV8     NV8     NV8     NV8      X      NV8     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     7,25,31,39      7    N/A\nGPU7    NV8     NV8     NV8     NV8     NV8     NV8     NV8      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     5,13,17,23      5    N/A\nNIC0    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC1    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC2    SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS\nNIC3    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS\nNIC5    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS\nNIC6    SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS\nNIC7    SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS\nNIC8    SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_200\n  NIC1: mlx5_400\n  NIC2: mlx5_401\n  NIC3: mlx5_402\n  NIC4: mlx5_403\n  NIC5: mlx5_404\n  NIC6: mlx5_405\n  NIC7: mlx5_406\n  NIC8: mlx5_407\n\n\nulimit soft: 1048576\n```\n\n\ncc @Qiaolin-Yu , thanks!",
    "labels": [
      "bug",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-29T04:53:32+00:00",
    "closed_at": "2025-06-03T00:19:53+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4876/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4876"
  },
  {
    "number": 2880,
    "title": "[Bug] Why can't I use multi-lora adapter and radix attention together?",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhy can't I use multi-lora adapter and radix attention together?\r\nIf I have multi-lora adapters, why not just insert the ID of the LoRA adapter before the first token?\r\n\r\nWhen using a multi-lora adapter, it is extremely slow because radix attention cannot be used.\n\n### Reproduction\n\nhttps://github.com/sgl-project/sglang/blob/v0.4.1.post5/python/sglang/srt/server_args.py#L876-L881\n\n### Environment\n\n```\r\nroot@33e74a81f115:/sglang/python# python3 -m sglang.check_env                                                                                                                                                         \r\n\r\nPython: 3.10.16 (main, Dec  4 2024, 08:53:37) [GCC 9.4.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nGPU 0 Compute Capability: 8.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\r\nCUDA Driver Version: 550.127.05\r\nPyTorch: 2.5.1+cu124\r\nsglang: 0.4.0.post2\r\nflashinfer: 0.1.6+cu124torch2.4\r\ntriton: 3.1.0\r\ntransformers: 4.47.0\r\ntorchao: 0.6.1\r\nnumpy: 1.26.4\r\naiohttp: 3.11.10\r\nfastapi: 0.115.6\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.26.3\r\ninteregular: 0.3.3\r\nmodelscope: 1.21.0\r\norjson: 3.10.12\r\npackaging: 24.2\r\npsutil: 6.1.0\r\npydantic: 2.10.3\r\nmultipart: 0.0.19\r\nzmq: 26.2.0\r\nuvicorn: 0.32.1\r\nuvloop: 0.21.0\r\nvllm: 0.6.4.post1\r\nopenai: 1.57.0\r\nanthropic: 0.40.0\r\ndecord: 0.6.0\r\n```",
    "labels": [
      "bug",
      "lora"
    ],
    "state": "open",
    "created_at": "2025-01-14T07:03:52+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2880/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2880"
  },
  {
    "number": 1316,
    "title": "[Bug] Unable to fix model output",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThe performance of sglang is very good. I am comparing the output accuracy of vllm, Hugging Face, and sglang. Using Qwen's model, I set do_sample to false or temperature to 0 to fix the output. Through comparison, the outputs of vllm and the Hugging Face transformer library are consistent. However, sglang does not produce consistent outputs. sglang has _SAMPLING_EPS set to 1e-6. Even when I use temperature=1e-5, I still cannot obtain consistent outputs. sglang produces different outputs with each run. What configurations should be set to make the output of sglang deterministic?\r\n sglang\u7684\u6027\u80fd\u975e\u5e38\u4e0d\u9519\u3002\u6211\u5728\u5bf9\u6bd4vllm\u3001hugging face\u548csglang\u7684\u8f93\u51fa\u7cbe\u5ea6\u3002 \u4f7f\u7528\u5343\u95ee\u7684\u6a21\u578b\uff0c\u4f7f\u7528do_sample\u7b49\u4e8efalse\u6216\u8005temperature=0\u6765\u56fa\u5b9a\u8f93\u51fa\u3002\u901a\u8fc7\u5bf9\u6bd4\uff0cvllm\u548chuggiing face transformer\u5e93\u7684\u8f93\u51fa\u4e00\u81f4\u3002\u4f46\u662fsglang\u4e0d\u80fd\u5f97\u5230\u4e00\u81f4\u7684\u8f93\u51fa\u3002 sglang \u8bbe\u7f6e\u4e86_SAMPLING_EPS = 1e-6\u3002\u6211\u4f7f\u7528temperature=1e-5\uff0c\u4ecd\u7136\u4e0d\u80fd\u6216\u8005\u4e00\u81f4\u7684\u8f93\u51fa\u3002sglang\u6bcf\u6b21\u8fd0\u884c\u90fd\u4f1a\u5f97\u5230\u4e0d\u4e00\u6837\u7684\u8f93\u51fa\u3002\u8be5\u600e\u4e48\u8bbe\u7f6e\uff0c\u80fd\u56fa\u5b9asglang\u7684\u8f93\u51fa\u3002\n\n### Reproduction\n\npython -m sglang.launch_server --model-path /xx/Qwen1.5-1.8B-Chat --port 30000  --tp 2  --enable-p2p-check --mem-fraction-static 0.7 --chunked-prefill-size 4096\n\n### Environment\n\nPython: 3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA GeForce RTX 3090\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.6\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.2, V12.2.91\r\nCUDA Driver Version: 535.54.03\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.2.15\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.44.0\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.26.4\r\naiohttp: 3.10.2\r\nfastapi: 0.112.0\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.5\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.5\r\nuvloop: 0.19.0\r\nzmq: 26.1.0\r\nvllm: 0.5.5\r\nmultipart: 0.0.9\r\nopenai: 1.40.2\r\nanthropic: 0.33.0\r\nNVIDIA Topology:\r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-19,40-59      0               N/A\r\nGPU1    PIX      X      NODE    NODE    SYS     SYS     SYS     SYS     0-19,40-59      0               N/A\r\nGPU2    NODE    NODE     X      PIX     SYS     SYS     SYS     SYS     0-19,40-59      0               N/A\r\nGPU3    NODE    NODE    PIX      X      SYS     SYS     SYS     SYS     0-19,40-59      0               N/A\r\nGPU4    SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    20-39,60-79     1               N/A\r\nGPU5    SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    20-39,60-79     1               N/A\r\nGPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      PIX     20-39,60-79     1               N/A\r\nGPU7    SYS     SYS     SYS     SYS     NODE    NODE    PIX      X      20-39,60-79     1               N/A\r\n",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2024-09-03T11:01:15+00:00",
    "closed_at": "2024-11-01T04:13:00+00:00",
    "comments": 25,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1316/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1316"
  },
  {
    "number": 6441,
    "title": "[Bug] JSON output contains think tag when enabling MTP for DeepSeek-R1",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen enable MTP + structured output for DeepSeek-R1, the JSON output contains `</think>`. It's unexpected.\n\n### Reproduction\n\n\n```\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1 --tp 8 --trust-remote-code --port 30000 --speculative-algorithm EAGLE\n\ncurl -X POST \"http://127.0.0.1:30000/v1/chat/completions\" -H \"Content-Type: application/json\" -d '{\"model\": \"deepseek-ai/DeepSeek-R1\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"}, {\"role\": \"user\", \"content\": \"Provide a JSON object with two keys: '\\''name'\\'' (string) and '\\''age'\\'' (integer). Example: {\\\"name\\\": \\\"John Doe\\\", \\\"age\\\": 30}\"}], \"temperature\": 0.0, \"response_format\": {\"type\": \"json_object\"}}'\n```\nresponse:\n```\n{\"id\":\"adf9f88725264fd286b7b2028d4131c6\",\"object\":\"chat.completion\",\"created\":1747707118,\"model\":\"deepseek-ai/DeepSeek-R1\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"{\\n  \\\"name\\\": \\\"Jane Smith\\\",\\n  \\\"age\\\": 28\\n}\\n</think>\\n\\n```json\\n{\\n  \\\"name\\\": \\\"Emily Johnson\\\",\\n  \\\"age\\\": 25\\n}\\n```\",\"reasoning_content\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"stop\",\"matched_stop\":1}],\"usage\":{\"prompt_tokens\":51,\"total_tokens\":93,\"completion_tokens\":42,\"prompt_tokens_details\":null}}\n```\n\n### Environment\n\nsglang: 0.4.6.post4",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-05-20T02:25:21+00:00",
    "closed_at": "2025-05-22T00:18:42+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6441/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6441"
  },
  {
    "number": 1195,
    "title": "[Bug] Bad outputs with fp8 quantization at high RPS",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [X] 2. The bug has not been fixed in the latest version.\r\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [X] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nI ran a RPS benchmark script with prompts of an average input length of 1600 tokens and got bad outputs as the RPS increased. For example:\r\n\r\n`*\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u8ffd\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u7ed9\u8feb\u4f60\u3002`\r\n\r\nIt seems to be related to quantization and concurrent requests. I've listed some commands below with various models, quants, and max num reqs, and if they had good or bad outputs at a high RPS and max running-req. \r\n\r\nUnfortunately I can't share the exact prompts used, but I'll update as I find other reproducible prompts.\r\n\r\nHere's a summary:\r\n- Unquantized models with the `--quantization fp8` flag have bad outputs at high RPS.\r\n- Unquantized models with the `--quantization fp8` and `--max-num-reqs 10` flags have good outputs at high RPS.\r\n- A pre-quantized fp8 with no `--quantization` or `--max-num-reqs` flag had good outputs at high RPS.\r\n\r\n### Reproduction\r\n\r\n```\r\nBAD OUTPUTS @ 5.5rps\r\n#running-req: 137\r\n\r\n\r\nCUDA_VISIBLE_DEVICES=2,3  python -m sglang.launch_server --model-path NousResearch/Meta-Llama-3.1-70B-Instruct --port 30003 --tp 2 --mem-fraction-static 0.90 --host 0.0.0.0 --context-length 2048 --quantization fp8\r\n\r\n\r\n-----------------------------------\r\n\r\nGOOD OUTPUTS @ 5.5 RPS\r\n#running-req: 8\r\n\r\nCUDA_VISIBLE_DEVICES=2,3  python -m sglang.launch_server --model-path NousResearch/Meta-Llama-3.1-70B-Instruct --port 30003 --tp 2 --mem-fraction-static 0.90 --host 0.0.0.0 --context-length 2048 --quantization fp8 --max-num-reqs 10\r\n\r\n\r\n-----------------------------------\r\n\r\n\r\nBAD OUTPUTS @ 5.5rps\r\n#running-req: 135\r\n\r\nCUDA_VISIBLE_DEVICES=2,3  python -m sglang.launch_server --model-path NousResearch/Hermes-3-Llama-3.1-70B --port 30003 --tp 2 --mem-fraction-static 0.90 --host 0.0.0.0 --context-length 2048 --quantization fp8\r\n\r\n\r\n-----------------------------------\r\n\r\nGOOD OUTPUTS @ 5.5 RPS\r\n#running-req: 8\r\n\r\nCUDA_VISIBLE_DEVICES=2,3  python -m sglang.launch_server --model-path NousResearch/Hermes-3-Llama-3.1-70B --port 30003 --tp 2 --mem-fraction-static 0.90 --host 0.0.0.0 --context-length 2048 --quantization fp8 --max-num-reqs 10\r\n\r\n\r\n-----------------------------------\r\n\r\nGOOD OUTPUTS @ 5.5 RPS\r\n#running-req: 136\r\n\r\nCUDA_VISIBLE_DEVICES=2,3  python -m sglang.launch_server --model-path NousResearch/Hermes-3-Llama-3.1-70B-FP8 --port 30003 --tp 2 --mem-fraction-static 0.90 --host 0.0.0.0 --context-length 2048\r\n```\r\n\r\n### Environment\r\n\r\n```Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\r\nCUDA_HOME: /usr\r\nNVCC: Cuda compilation tools, release 12.2, V12.2.140\r\nCUDA Driver Version: 535.129.03\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.2.13\r\nflashinfer: 0.1.5+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.44.2\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.26.4\r\naiohttp: 3.10.5\r\nfastapi: 0.112.1\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.6\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.6\r\nuvloop: 0.20.0\r\nzmq: 26.2.0\r\nvllm: 0.5.4\r\nmultipart: 0.0.9\r\nopenai: 1.42.0\r\nanthropic: 0.34.1\r\nNVIDIA Topology: \r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    SYS     0-103   0               N/A\r\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     0-103   0               N/A\r\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     0-103   0               N/A\r\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     0-103   0               N/A\r\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     104-207 1               N/A\r\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     104-207 1               N/A\r\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     104-207 1               N/A\r\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     104-207 1               N/A\r\nNIC0    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n\r\n\r\nulimit soft: 4096```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-08-24T12:03:11+00:00",
    "closed_at": "2024-09-21T03:18:34+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1195/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1195"
  },
  {
    "number": 839,
    "title": "[Bug] Chat completions logprobs support",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n\n### Describe the bug\n\nI'm encountering an issue when making a request to the `v1/chat/completions` endpoint with the `\"logprobs\": true` parameter. The `choices.logprobs` field in the response is always null, even though it should be populated with probability values.\n\n### Reproduction\n\nScript to launch the SGLang server:\r\n\r\n```bash\r\npython -m sglang.launch_server  \\\r\n    --model-path Qwen/Qwen2-7B-Instruct \\\r\n    --port 8000\r\n```\r\n\r\nScript to reproduce the request:\r\n\r\n```bash\r\ncurl http://127.0.0.1:8000/v1/chat/completions \\\r\n  -H \"Content-Type: application/json\" \\\r\n  -d '{\r\n     \"model\": \"Qwen/Qwen2-7B-Instruct\",\r\n     \"messages\": [{\"role\": \"user\", \"content\": \"Hi!\"}],\r\n     \"stream\": false,\r\n     \"temperature\": 1,\r\n     \"max_tokens\": 1,\r\n     \"top_logprobs\": 2,\r\n     \"logprobs\": true\r\n   }'\r\n```\r\n\r\nResponse:\r\n\r\n```json\r\n{\"id\":\"704abb89338b4892bd63b877764f9593\",\"object\":\"chat.completion\",\"created\":1722369690,\"model\":\"Qwen/Qwen2-7B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Hello\"},\"logprobs\":null,\"finish_reason\":\"FINISH_LENGTH: 1\"}],\"usage\":{\"prompt_tokens\":21,\"total_tokens\":22,\"completion_tokens\":1}}\r\n```\n\n### Environment\n\n```Shell\nPython: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.3, V12.3.107\r\nCUDA Driver Version: 525.105.17\r\nPyTorch: 2.3.1\r\nsglang: 0.2.7\r\nflashinfer: 0.1.2+cu121torch2.3\r\nrequests: 2.32.3\r\ntqdm: 4.66.4\r\nnumpy: 1.26.4\r\naiohttp: 3.9.5\r\nfastapi: 0.111.1\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.3\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.3\r\nuvloop: 0.19.0\r\nzmq: 26.0.3\r\nvllm: 0.5.3.post1\r\nopenai: 1.37.1\r\nanthropic: 0.32.0\r\nNVIDIA Topology: \r\n        GPU0    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    CPU Affinity    NUMA Affinity\r\nGPU0     X      SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     96-127,224-255  3\r\nNIC0    SYS      X      PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC1    SYS     PIX      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC2    SYS     SYS     SYS      X      PIX     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC3    SYS     SYS     SYS     PIX      X      SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC4    SYS     SYS     SYS     SYS     SYS      X      PXB     SYS     SYS     SYS     SYS\r\nNIC5    SYS     SYS     SYS     SYS     SYS     PXB      X      SYS     SYS     SYS     SYS\r\nNIC6    NODE    SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     NODE    NODE\r\nNIC7    NODE    SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      NODE    NODE\r\nNIC8    PXB     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      PXB\r\nNIC9    PXB     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    PXB      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  NIC8: mlx5_8\r\n  NIC9: mlx5_9\r\n\r\n\r\nulimit soft: 1048576\r\n```\n```\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-07-30T20:24:44+00:00",
    "closed_at": "2024-08-01T07:08:22+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/839/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/839"
  },
  {
    "number": 4707,
    "title": "[Bug] Miss prompt_tokens_details in stream chat when --enable-cache-report",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\npython3 -m sglang.launch_server --model-path /root/public/DeepSeek-R1-Distill-Qwen-1.5B/ --enable-cache-report\n\nuse  \"stream\": true,\n\nthere is no \n\n\"prompt_tokens_details\":{\"cached_tokens\":921}\n\nin output\n\n### Reproduction\n\npython3 -m sglang.launch_server --model-path /root/public/DeepSeek-R1-Distill-Qwen-1.5B/ --enable-cache-report\n\ncurl -i -X POST   -H \"Content-Type: application/json\"   -H \"Authorization: Bearer sk-test\"   -d '{\n      \"model\": \"DeepSeek-R1\",\n      \"stream\": true,\n      \"messages\": [\n        {\n          \"role\": \"user\",\n          \"content\": \"\u4f60\u597d\"\n        }\n      ],\n      \"stream_options\": {\n        \"include_usage\": true\n      }\n    }'   'http://127.0.0.1:30000/v1/chat/completions'\n\n### Environment\n\nsingle 4090",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-24T04:02:42+00:00",
    "closed_at": "2025-03-24T05:32:13+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4707/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4707"
  },
  {
    "number": 4527,
    "title": "[Bug] port is not an integer in function get_open_port()",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nin function get_open_port\uff0cport is a string, so s.bind((\"\", port)) will have TypeError: an integer is required (got type str)\nto fix it, simple add port=int(port) here\n```\ndef get_open_port() -> int:\n    port = os.getenv(\"SGLANG_PORT\")\n    if port is not None:\n        # port=int(port)  # add here\n        while True:\n            try:\n                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n                    s.bind((\"\", port))\n                    return port\n            except OSError:\n                port += 1  # Increment port number if already in use\n                logger.info(\"Port %d is already in use, trying port %d\", port - 1, port)\n    ...\n```\n\n### Reproduction\n\n```\nfrom sglang.srt.distributed.device_communicators.shm_broadcast import MessageQueue\nqueue=MessageQueue(n_reader=1, n_local_reader=1) \n```\n\n### Environment\n\nmy python version is 3.9\nmy environment is not correct, but it is not necessary in the bug\n```\npython3 -m sglang.check_env\n\nUserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-18T02:05:24+00:00",
    "closed_at": "2025-03-28T04:46:07+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4527/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4527"
  },
  {
    "number": 6714,
    "title": "[Bug] FusedMoE does not recognize ModelOpt fp8 format.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI quantized a llama-4-fp8 version with modelopt: https://huggingface.co/baseten/Llama-4-Scout-17B-16E-fp8 \nCurrently other non-moe checkpoints are working https://huggingface.co/nvidia/Llama-3.1-8B-Instruct-FP8 \n\n\n\n### Reproduction\n-\n\n### Environment\n\n-",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-05-28T17:31:24+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6714/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6714"
  },
  {
    "number": 1301,
    "title": "[Bug] A100 PCIE torch compile error",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n```\r\n[11:19:46 TP0] Decode batch. #running-req: 36, #token: 14473, token usage: 0.03, gen throughput (token/s): 2283.73, #queue-req: 0\r\n../aten/src/ATen/native/cuda/MultinomialKernel.cu:112: binarySearchForMultinomial: block: [0,31,0], thread: [0,0,0] Assertion `cumdist[size - 1] > static_cast<scalar_t>(0)` failed.\r\n../aten/src/ATen/native/cuda/MultinomialKernel.cu:112: binarySearchForMultinomial: block: [0,31,0], thread: [1,0,0] Assertion `cumdist[size - 1] > static_cast<scalar_t>(0)` failed.\r\n[11:19:46 TP0] Exception in ModelTpServer:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tp_worker.py\", line 244, in exposed_step\r\n    self.forward_step()\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tp_worker.py\", line 273, in forward_step\r\n    self.forward_decode_batch(self.running_batch)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tp_worker.py\", line 685, in forward_decode_batch\r\n    sample_output, logits_output = self.model_runner.forward(\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_executor/model_runner.py\", line 582, in forward\r\n    return self.forward_decode(batch)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_executor/model_runner.py\", line 528, in forward_decode\r\n    return self.cuda_graph_runner.replay(batch)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_executor/cuda_graph_runner.py\", line 315, in replay\r\n    torch.cuda.synchronize()\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 892, in synchronize\r\n    return torch._C._cuda_synchronize()\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n\r\n[11:19:46 TP0] Exception in ControllerSingle:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/controller_single.py\", line 165, in start_controller_process\r\n    controller.loop_for_forward()\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/controller_single.py\", line 102, in loop_for_forward\r\n    out_pyobjs = self.tp_server.exposed_step(recv_reqs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tp_worker.py\", line 244, in exposed_step\r\n    self.forward_step()\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tp_worker.py\", line 273, in forward_step\r\n    self.forward_decode_batch(self.running_batch)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tp_worker.py\", line 685, in forward_decode_batch\r\n    sample_output, logits_output = self.model_runner.forward(\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_executor/model_runner.py\", line 582, in forward\r\n    return self.forward_decode(batch)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_executor/model_runner.py\", line 528, in forward_decode\r\n    return self.cuda_graph_runner.replay(batch)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_executor/cuda_graph_runner.py\", line 315, in replay\r\n    torch.cuda.synchronize()\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 892, in synchronize\r\n    return torch._C._cuda_synchronize()\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n\r\nKilled\r\n```\n\n### Reproduction\n\n```\r\n# 0.2.15\r\n\r\npip install --upgrade pip\r\npip install \"sglang[all]\"\r\n\r\n# Install FlashInfer CUDA kernels\r\npip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/\r\n```\r\n\r\n```python3\r\nimport argparse\r\nimport asyncio\r\nimport os\r\nimport pickle\r\nimport re\r\nfrom collections import defaultdict\r\n\r\nimport openai\r\nimport transformers\r\nfrom datasets import load_dataset\r\nfrom openai import AsyncOpenAI\r\nfrom tenacity import (\r\n    retry,\r\n    retry_if_exception_type,\r\n    stop_after_attempt,\r\n    wait_exponential,\r\n)\r\nfrom tqdm import tqdm\r\n\r\n# Mapping backends to their clients and models\r\nbackend_to_models = {\r\n    \"sglang\": {\r\n        \"8b\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\r\n        \"70b\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\r\n        \"405b\": \"meta-llama/Meta-Llama-3.1-405B-Instruct\",\r\n    },\r\n}\r\n\r\n\r\n# Define the retry strategy\r\nretry_strategy = retry(\r\n    stop=stop_after_attempt(5),  # Stop after 5 attempts\r\n    wait=wait_exponential(multiplier=1, min=4, max=10),  # Exponential backoff\r\n    retry=retry_if_exception_type(Exception),  # Retry on any exception\r\n)\r\n\r\n\r\n# Define the fetch_responses function with retry strategy\r\n@retry_strategy\r\nasync def fetch_responses(\r\n    client, prompt, semaphore, index, backend, model_size, output_dir, max_tokens\r\n):\r\n    output_file = os.path.join(output_dir, f\"response_{index}.pkl\")\r\n    if os.path.exists(output_file):\r\n        print(f\"File {output_file} already exists, skipping.\")\r\n        return\r\n\r\n    async with semaphore:\r\n        response = await client.completions.create(\r\n            model=backend_to_models[backend][model_size],\r\n            prompt=prompt,\r\n            temperature=0.0,\r\n            max_tokens=max_tokens,\r\n        )\r\n        if isinstance(response, openai.BadRequestError):\r\n            with open(output_file, \"wb\") as f:\r\n                pickle.dump(\"bad_response\", f)\r\n        assert isinstance(response, openai.types.completion.Completion)\r\n        # Save response to a file\r\n        with open(output_file, \"wb\") as f:\r\n            pickle.dump(response, f)\r\n\r\n\r\nTASK_TO_MAX_TOKENS = {\r\n    \"evals__mmlu__details\": 1,\r\n    \"evals__mmlu__0_shot__cot__details\": 1024,\r\n    # Official meta uses 1024, but a small % (.05) of questions are answered correctly after relaxing\r\n    \"evals__mmlu_pro__details\": 2048,\r\n    \"evals__gsm8k__details\": 1024,\r\n}\r\n\r\n\r\ndef get_client(backend):\r\n    return {\r\n        \"sglang\": AsyncOpenAI(base_url=\"http://127.0.0.1:30000/v1/\"),\r\n    }[backend]\r\n\r\n\r\nasync def run_benchmark(args):\r\n    ds = load_dataset(\r\n        \"meta-llama/Meta-Llama-3.1-405B-Instruct-evals\",\r\n        f\"Meta-Llama-3.1-405B-Instruct-{args.task}\",\r\n    )\r\n    semaphore = asyncio.Semaphore(args.concurrency)  # Limit to 16 concurrent tasks\r\n\r\n    if args.num_examples is None:\r\n        args.num_examples = len(ds[\"latest\"][\"input_final_prompts\"])\r\n    prompts = ds[\"latest\"][\"input_final_prompts\"][: args.num_examples]\r\n\r\n    # Create the output directory if it does not exist\r\n    os.makedirs(args.output_dir, exist_ok=True)\r\n\r\n    tasks = []\r\n    # Create the tasks with tqdm progress bar\r\n    max_tokens = TASK_TO_MAX_TOKENS[args.task]\r\n    client = get_client(args.backend)\r\n    for idx, prompt in enumerate(tqdm(prompts, desc=\"Creating tasks\")):\r\n        tasks.append(\r\n            asyncio.create_task(\r\n                fetch_responses(\r\n                    client,\r\n                    f\"<|begin_of_|text|>{prompt[0]}\",\r\n                    semaphore,\r\n                    idx,\r\n                    args.backend,\r\n                    args.model_size,\r\n                    args.output_dir,\r\n                    max_tokens=max_tokens,\r\n                )\r\n            )\r\n        )\r\n\r\n    # Run the tasks with tqdm progress bar\r\n    for future in tqdm(\r\n        asyncio.as_completed(tasks), total=len(tasks), desc=\"Processing tasks\"\r\n    ):\r\n        await future\r\n\r\n\r\ndef get_mmlu_answer(response):\r\n    if response is not None:\r\n        return response.choices[0].text.lstrip().rstrip().upper().replace(\".\", \"\")\r\n    return None\r\n\r\n\r\ndef get_mmlu_cot_answer(response):\r\n    pattern = r\"The best answer is (.+)\\.?\"\r\n    match = re.search(pattern, response.choices[0].text)\r\n    if match:\r\n        return match.group(1).replace(\".\", \"\").replace(\"*\", \"\")\r\n\r\n    pattern = r\"the best answer is (.+)\\.?\"\r\n    match = re.search(pattern, response.choices[0].text)\r\n    if match:\r\n        return match.group(1).replace(\".\", \"\")\r\n\r\n    pattern = r\"The correct answer is (.+)\\.?\"\r\n    match = re.search(pattern, response.choices[0].text)\r\n    if match:\r\n        return match.group(1).replace(\".\", \"\")\r\n\r\n    pattern = r\"the correct answer is (.+)\\.?\"\r\n    match = re.search(pattern, response.choices[0].text)\r\n    if match:\r\n        return match.group(1).replace(\".\", \"\")\r\n\r\n\r\ndef get_answer_gsm8k(response):\r\n    pattern = r\"The final answer is (.+)\\.?\"\r\n    match = re.search(pattern, response.choices[0].text)\r\n    if match:\r\n        s = match.group(1)\r\n        for ok_symbol in [\"%\", \"$\"]:\r\n            s = s.replace(ok_symbol, \"\")\r\n        return s\r\n\r\n\r\nTASK_TO_ANSWER_EXTRACTOR = {\r\n    \"evals__mmlu__details\": get_mmlu_answer,\r\n    \"evals__mmlu__0_shot__cot__details\": get_mmlu_cot_answer,\r\n    \"evals__gsm8k__details\": get_answer_gsm8k,\r\n    \"evals__mmlu_pro__details\": get_mmlu_cot_answer,\r\n}\r\n\r\n\r\ndef get_dataset_from_task(task, response_path):\r\n    ds_405b = load_dataset(\r\n        f\"meta-llama/Meta-Llama-3.1-405B-Instruct-evals\",\r\n        f\"Meta-Llama-3.1-405B-Instruct-{task}\",\r\n    )\r\n    ds_405b_hash_order = [x[0] for x in ds_405b[\"latest\"][\"input_final_prompts_hash\"]]\r\n\r\n    if \"70b\" in str(response_path) or \"8b\" in str(response_path):\r\n        if \"70\" in str(response_path):\r\n            ref_model_ds = load_dataset(\r\n                f\"meta-llama/Meta-Llama-3.1-70B-Instruct-evals\",\r\n                f\"Meta-Llama-3.1-70B-Instruct-{task}\",\r\n            )\r\n        else:\r\n            ref_model_ds = load_dataset(\r\n                f\"meta-llama/Meta-Llama-3.1-8B-Instruct-evals\",\r\n                f\"Meta-Llama-3.1-8B-Instruct-{task}\",\r\n            )\r\n\r\n        hash_to_row = {}\r\n        for row in ref_model_ds[\"latest\"]:\r\n            hash_to_row[row[\"input_final_prompts_hash\"][0]] = row\r\n        reordered_rows = []\r\n        for prompt_hash in ds_405b_hash_order:\r\n            reordered_rows.append(hash_to_row[prompt_hash])\r\n        ref_model_ds[\"latest\"] = reordered_rows\r\n        return ref_model_ds\r\n\r\n    return ds_405b\r\n\r\n\r\ndef analyze_answers(task, response_path):\r\n    ds = get_dataset_from_task(task, response_path)\r\n\r\n    responses = []\r\n    total = len(ds[\"latest\"])\r\n\r\n    for i in range(0, total):\r\n        response = pickle.load(\r\n            open(os.path.join(response_path, f\"response_{i}.pkl\"), \"rb\")\r\n        )\r\n        responses.append(response)\r\n\r\n    from dataclasses import dataclass\r\n\r\n    @dataclass\r\n    class Stats:\r\n        correct: int = 0\r\n        total: int = 0\r\n        meta_correct: int = 0\r\n\r\n        average: float = None\r\n\r\n    subtask_name_to_stats = defaultdict(lambda: Stats())\r\n\r\n    for response, ds_row in zip(responses, ds[\"latest\"]):\r\n        model_answer = TASK_TO_ANSWER_EXTRACTOR[task](response)\r\n\r\n        subtask = ds_row[\"subtask_name\"]\r\n\r\n        is_eval_correct = model_answer in ds_row[\"input_correct_responses\"]\r\n        if is_eval_correct:\r\n            subtask_name_to_stats[subtask].correct += 1\r\n\r\n        if ds_row[\"is_correct\"]:\r\n            subtask_name_to_stats[subtask].meta_correct += 1\r\n\r\n        subtask_name_to_stats[subtask].total += 1\r\n\r\n    micro_stats = Stats()\r\n    for subtask, stats in subtask_name_to_stats.items():\r\n        stats.average = stats.correct / stats.total\r\n        stats.meta_average = stats.meta_correct / stats.total\r\n\r\n        micro_stats.correct += stats.correct\r\n        micro_stats.total += stats.total\r\n        micro_stats.meta_correct += stats.meta_correct\r\n\r\n    micro_stats.average = micro_stats.correct / micro_stats.total\r\n    micro_stats.meta_average = micro_stats.meta_correct / micro_stats.total\r\n\r\n    import numpy as np\r\n\r\n    print(\"Macro average\", np.mean([x.average for x in subtask_name_to_stats.values()]))\r\n    print(\r\n        \"Meta Macro average\",\r\n        np.mean([x.meta_average for x in subtask_name_to_stats.values()]),\r\n    )\r\n    print(\"Micro average\", micro_stats.average)\r\n    print(\"Meta Micro average\", micro_stats.meta_average)\r\n\r\n\r\n# Entry point for the script\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser(\r\n        description=\"Script to run model with specified parameters.\"\r\n    )\r\n    parser.add_argument(\r\n        \"--model-size\",\r\n        type=str,\r\n        required=True,\r\n        help=\"Size of the model (e.g., 8b or 70b)\",\r\n    )\r\n    parser.add_argument(\r\n        \"--backend\", type=str, required=True, help=\"Backend name (e.g., sglang)\"\r\n    )\r\n    parser.add_argument(\"--task\", type=str, required=True)\r\n    parser.add_argument(\r\n        \"--num-examples\", type=int, default=None, help=\"Number of examples to process\"\r\n    )\r\n    parser.add_argument(\"--concurrency\", type=int, default=128)\r\n    parser.add_argument(\r\n        \"--output-dir\", type=str, required=True, help=\"Directory to save responses\"\r\n    )\r\n\r\n    os.environ['OPENAI_API_KEY'] = 'EMPTY'\r\n\r\n    args = parser.parse_args()\r\n    asyncio.run(run_benchmark(args))\r\n\r\n    analyze_answers(args.task, args.output_dir)\r\n```\r\n\r\n```\r\npython3 eval.py --model-size 8b --backend sglang --task evals__gsm8k__details --output-dir tmp/8b\r\n```\n\n### Environment\n\n```\r\nPython: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA A100 80GB PCIe\r\nGPU 0 Compute Capability: 8.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\r\nCUDA Driver Version: 550.90.07\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.2.15\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.44.2\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.26.3\r\naiohttp: 3.10.5\r\nfastapi: 0.112.2\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.6\r\ninteregular: 0.3.3\r\npackaging: 23.2\r\nPIL: 10.2.0\r\npsutil: 5.9.8\r\npydantic: 2.8.2\r\nuvicorn: 0.30.6\r\nuvloop: 0.20.0\r\nzmq: 24.0.1\r\nvllm: 0.5.5\r\nmultipart: 0.0.9\r\nopenai: 1.43.0\r\nanthropic: 0.34.1\r\nNVIDIA Topology:\r\n        GPU0    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NODE    0-31,64-95      0               N/A\r\nNIC0    NODE     X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_bond_0\r\n\r\n\r\nulimit soft: 1048576\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-09-02T11:24:36+00:00",
    "closed_at": "2024-09-02T23:18:49+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1301/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/1301"
  },
  {
    "number": 1191,
    "title": "[Bug] Server crashes after loading (Mixtral 8x7b) on L4",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nModel fully loads, server runs and then instantly crashes\r\n\r\n```\r\nserver_args=ServerArgs(model_path='/local_disk0/mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer_path='/local_disk0/mistralai/Mixtral-8x7B-Instruct-v0.1', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', dtype='auto', trust_remote_code=False, context_length=8192, quantization=None, served_model_name='mixtral-8x7b-v0.1', chat_template=None, host='0.0.0.0', port=1234, additional_ports=[1235, 1236, 1237, 1238], mem_fraction_static=0.83, max_running_requests=32, max_num_reqs=32, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, tp_size=8, stream_interval=1, random_seed=759329088, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, api_key=None, file_storage_pth='SGLang_storage', dp_size=1, load_balance_method='round_robin', disable_flashinfer=False, disable_flashinfer_sampling=False, disable_radix_cache=False, disable_regex_jump_forward=False, disable_cuda_graph=True, disable_disk_cache=False, enable_torch_compile=False, enable_p2p_check=True, enable_mla=False, attention_reduce_in_fp32=False, efficient_weight_load=False, nccl_init_addr=None, nnodes=1, node_rank=None)\r\n[gpu=0] Init nccl begin.\r\n[gpu=5] Init nccl begin.\r\n[gpu=7] Init nccl begin.\r\n[gpu=1] Init nccl begin.\r\n[gpu=3] Init nccl begin.\r\n[gpu=6] Init nccl begin.\r\n[gpu=2] Init nccl begin.\r\n[gpu=4] Init nccl begin.\r\nWARNING 08-23 11:04:07 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\nWARNING 08-23 11:04:07 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\nWARNING 08-23 11:04:07 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\nWARNING 08-23 11:04:07 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\nWARNING 08-23 11:04:07 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\nWARNING 08-23 11:04:07 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\nWARNING 08-23 11:04:07 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\nWARNING 08-23 11:04:07 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n[gpu=6] Load weight begin. avail mem=21.65 GB\r\n[gpu=5] Load weight begin. avail mem=21.65 GB\r\n[gpu=7] Load weight begin. avail mem=21.65 GB\r\n[gpu=4] Load weight begin. avail mem=21.65 GB\r\n[gpu=3] Load weight begin. avail mem=21.65 GB\r\n[gpu=1] Load weight begin. avail mem=21.65 GB\r\n[gpu=0] Load weight begin. avail mem=21.65 GB\r\n[gpu=2] Load weight begin. avail mem=21.65 GB\r\nLoading safetensors checkpoint shards:   0% Completed | 0/19 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:   5% Completed | 1/19 [00:00<00:13,  1.37it/s]\r\nLoading safetensors checkpoint shards:  11% Completed | 2/19 [00:01<00:14,  1.19it/s]\r\nLoading safetensors checkpoint shards:  16% Completed | 3/19 [00:02<00:14,  1.14it/s]\r\nLoading safetensors checkpoint shards:  21% Completed | 4/19 [00:03<00:13,  1.07it/s]\r\nLoading safetensors checkpoint shards:  26% Completed | 5/19 [00:04<00:13,  1.02it/s]\r\nLoading safetensors checkpoint shards:  32% Completed | 6/19 [00:05<00:13,  1.01s/it]\r\nLoading safetensors checkpoint shards:  37% Completed | 7/19 [00:06<00:12,  1.01s/it]\r\n[gpu=7] Load weight end. type=MixtralForCausalLM, dtype=torch.bfloat16, avail mem=10.75 GB\r\nLoading safetensors checkpoint shards:  42% Completed | 8/19 [00:07<00:11,  1.02s/it]\r\nLoading safetensors checkpoint shards:  47% Completed | 9/19 [00:08<00:09,  1.01it/s]\r\nLoading safetensors checkpoint shards:  53% Completed | 10/19 [00:09<00:08,  1.08it/s]\r\nLoading safetensors checkpoint shards:  58% Completed | 11/19 [00:10<00:07,  1.08it/s]\r\nLoading safetensors checkpoint shards:  63% Completed | 12/19 [00:11<00:06,  1.07it/s]\r\nLoading safetensors checkpoint shards:  68% Completed | 13/19 [00:12<00:05,  1.07it/s]\r\nLoading safetensors checkpoint shards:  74% Completed | 14/19 [00:13<00:04,  1.04it/s]\r\nLoading safetensors checkpoint shards:  79% Completed | 15/19 [00:14<00:03,  1.04it/s]\r\nLoading safetensors checkpoint shards:  84% Completed | 16/19 [00:15<00:02,  1.03it/s]\r\nLoading safetensors checkpoint shards:  89% Completed | 17/19 [00:16<00:02,  1.00s/it]\r\nLoading safetensors checkpoint shards:  95% Completed | 18/19 [00:17<00:00,  1.01it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 19/19 [00:18<00:00,  1.05it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 19/19 [00:18<00:00,  1.05it/s]\r\n\r\n[gpu=3] Load weight end. type=MixtralForCausalLM, dtype=torch.bfloat16, avail mem=10.75 GB\r\n[gpu=5] Load weight end. type=MixtralForCausalLM, dtype=torch.bfloat16, avail mem=10.75 GB\r\n[gpu=4] Load weight end. type=MixtralForCausalLM, dtype=torch.bfloat16, avail mem=10.75 GB\r\n[gpu=0] Load weight end. type=MixtralForCausalLM, dtype=torch.bfloat16, avail mem=10.75 GB\r\n[gpu=6] Load weight end. type=MixtralForCausalLM, dtype=torch.bfloat16, avail mem=10.75 GB\r\n[gpu=1] Load weight end. type=MixtralForCausalLM, dtype=torch.bfloat16, avail mem=10.75 GB\r\n[gpu=2] Load weight end. type=MixtralForCausalLM, dtype=torch.bfloat16, avail mem=10.75 GB\r\n[gpu=3] Memory pool end. avail mem=3.63 GB\r\n[gpu=2] Memory pool end. avail mem=3.63 GB\r\n[gpu=5] Memory pool end. avail mem=3.63 GB\r\n[gpu=1] Memory pool end. avail mem=3.63 GB\r\n[gpu=6] Memory pool end. avail mem=3.63 GB\r\n[gpu=7] Memory pool end. avail mem=3.63 GB\r\n[gpu=4] Memory pool end. avail mem=3.63 GB\r\n[gpu=0] Memory pool end. avail mem=3.63 GB\r\n[gpu=1] max_total_num_tokens=463405, max_prefill_tokens=16384, max_running_requests=31, context_len=8192\r\n[gpu=7] max_total_num_tokens=463405, max_prefill_tokens=16384, max_running_requests=31, context_len=8192\r\n[gpu=3] max_total_num_tokens=463405, max_prefill_tokens=16384, max_running_requests=31, context_len=8192\r\n[gpu=6] max_total_num_tokens=463405, max_prefill_tokens=16384, max_running_requests=31, context_len=8192\r\n[gpu=4] max_total_num_tokens=463405, max_prefill_tokens=16384, max_running_requests=31, context_len=8192\r\n[gpu=0] max_total_num_tokens=463405, max_prefill_tokens=16384, max_running_requests=31, context_len=8192\r\n[gpu=5] max_total_num_tokens=463405, max_prefill_tokens=16384, max_running_requests=31, context_len=8192\r\n[gpu=2] max_total_num_tokens=463405, max_prefill_tokens=16384, max_running_requests=31, context_len=8192\r\nINFO:     Started server process [28350]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://0.0.0.0:1234/ (Press CTRL+C to quit)\r\nINFO:     127.0.0.1:55458 - \"GET /get_model_info HTTP/1.1\" 200 OK\r\n[gpu=0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, cache hit rate: 0.00%, #running-req: 0, #queue-req: 0\r\n/usr/lib/python3.11/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n```\n\n### Reproduction\n\n`!python -m sglang.launch_server --model-path /local_disk0/mistralai/Mixtral-8x7B-Instruct-v0.1 --served-model-name mixtral-8x7b-v0.1 --host 0.0.0.0 --port 1234 --tp 8 --context-length 8192 --max-running-requests 32 --max-num-reqs 32 --disable-cuda-graph --enable-p2p-check`\n\n### Environment\n\n```\r\nPython: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA L4\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.9\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\r\nCUDA Driver Version: 535.161.07\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.2.13\r\nflashinfer: 0.1.5+cu124torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.44.2\r\nrequests: 2.31.0\r\ntqdm: 4.65.0\r\nnumpy: 1.23.5\r\naiohttp: 3.8.5\r\nfastapi: 0.112.1\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.6\r\ninteregular: 0.3.3\r\npackaging: 23.2\r\nPIL: 9.4.0\r\npsutil: 5.9.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.6\r\nuvloop: 0.20.0\r\nzmq: 23.2.0\r\nvllm: 0.5.4\r\nmultipart: 0.0.9\r\nopenai: 1.42.0\r\nanthropic: 0.34.1\r\nNVIDIA Topology: \r\n\tGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\r\nGPU1\tNODE\t X \tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\r\nGPU2\tNODE\tNODE\t X \tNODE\tSYS\tSYS\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\r\nGPU3\tNODE\tNODE\tNODE\t X \tSYS\tSYS\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\r\nGPU4\tSYS\tSYS\tSYS\tSYS\t X \tNODE\tNODE\tNODE\t48-95,144-191\t1\t\tN/A\r\nGPU5\tSYS\tSYS\tSYS\tSYS\tNODE\t X \tNODE\tNODE\t48-95,144-191\t1\t\tN/A\r\nGPU6\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\t X \tNODE\t48-95,144-191\t1\t\tN/A\r\nGPU7\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\t X \t48-95,144-191\t1\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nulimit soft: 1000000\r\n```",
    "labels": [
      "bug",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-08-23T11:08:48+00:00",
    "closed_at": "2024-11-04T01:13:36+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1191/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1191"
  },
  {
    "number": 4700,
    "title": "[Bug] Missing tool name in answer",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHello, i have found a strange BUG, i'm trying to use sglang with n8n and noticed that function calls not working, i have written a simple proxy app and noticed that in sglang answer with function call at final chunk function name in missing, and that is strange because in first chunks its present, and final answer becomes invalid as result, here is example:\nRequest:\n\n> 2025-03-23 18:15:35,962 - DEBUG - Remote request body for 2ad4de47 to /v1/chat/completions:\n> {\n>   \"model\": \"casperhansen/llama-3.3-70b-instruct-awq\",\n>   \"temperature\": 1,\n>   \"top_p\": 1,\n>   \"frequency_penalty\": 0,\n>   \"presence_penalty\": 0,\n>   \"n\": 1,\n>   \"stream\": true,\n>   \"tools\": [\n>     {\n>       \"type\": \"function\",\n>       \"function\": {\n>         \"name\": \"vector_store\",\n>         \"description\": \"Tool retrives user names that are already in database\",\n>         \"parameters\": {\n>           \"type\": \"object\",\n>           \"properties\": {\n>             \"input\": {\n>               \"type\": \"string\"\n>             }\n>           },\n>           \"required\": [\n>             \"input\"\n>           ],\n>           \"additionalProperties\": false,\n>           \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n>         }\n>       }\n>     }\n>   ],\n>   \"stream_options\": {\n>     \"include_usage\": true\n>   },\n>   \"messages\": [\n>     {\n>       \"role\": \"system\",\n>       \"content\": \"You are a helpful assistant, extract user name from chat message, tell if its found if its already preset in vector store\\ntools & when to use them:\\n\\n1 - Vector store\\ncontains db of user name already in system\\n\\nExample workflow:\\nWhen chat message arrives\\nMessage: hi, my name is boss\\nActions taken:\\n1) username 'boss' extracted and checked in vector store\\n2) if user with extracted username ('boss') is not found, answer user that its not found\\n3) if username found: answer user that user with name \\\"boss\\\" is found.\"\n>     },\n>     {\n>       \"role\": \"user\",\n>       \"content\": \"hi my name is stanislav\"\n>     }\n>   ]\n> }\n\nResponse:\n\n\n> INFO:     192.168.100.29:57726 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n> 2025-03-23 18:15:36,126 - DEBUG - Stream chunk for request 2ad4de47:\n> 14c\n> data: {\"id\":\"7874fe5978be41a1ad2daf0e67658490\",\"object\":\"chat.completion.chunk\",\"created\":17427\n> 2025-03-23 18:15:36,127 - DEBUG - Stream chunk for request 2ad4de47:\n> 42934,\"model\":\"casperhansen/llama-3.3-70b-instruct-awq\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assis\n> 2025-03-23 18:15:36,127 - DEBUG - Stream chunk for request 2ad4de47:\n> tant\",\"content\":\"\",\"reasoning_content\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\n> 2025-03-23 18:15:36,226 - DEBUG - Stream chunk for request 2ad4de47:\n> \"matched_stop\":null}],\"usage\":null}\n> \n> \n> 1a1\n> data: {\"id\":\"7874fe5978be41a1ad2daf0e67658490\",\"object\":\n> 2025-03-23 18:15:36,227 - DEBUG - Stream chunk for request 2ad4de47:\n> \"chat.completion.chunk\",\"created\":1742742934,\"model\":\"casperhansen/llama-3.3-70b-instruct-awq\",\"choi\n> 2025-03-23 18:15:36,227 - DEBUG - Stream chunk for request 2ad4de47:\n> ces\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":null,\"reasoning_content\":null,\"tool_calls\":[{\n> 2025-03-23 18:15:36,227 - DEBUG - Stream chunk for request 2ad4de47:\n> \"id\":\"0\",\"type\":\"function\",\"function\":{**\"name\":\"vector_store\"**,\"arguments\":\"\"}}]},\"logprobs\":null,\"fin\n> 2025-03-23 18:15:36,383 - DEBUG - Stream chunk for request 2ad4de47:\n> ish_reason\":\"tool_call\",\"matched_stop\":null}],\"usage\":null}\n> \n> \n> 1a3\n> data: {\"id\":\"7874fe5978be41a1ad2\n> 2025-03-23 18:15:36,383 - DEBUG - Stream chunk for request 2ad4de47:\n> daf0e67658490\",\"object\":\"chat.completion.chunk\",\"created\":1742742934,\"model\":\"casperhansen/llama-3.3\n> 2025-03-23 18:15:36,384 - DEBUG - Stream chunk for request 2ad4de47:\n> -70b-instruct-awq\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":null,\"reasoning_conte\n> 2025-03-23 18:15:36,384 - DEBUG - Stream chunk for request 2ad4de47:\n> nt\":null,\"tool_calls\":[{\"id\":\"0\",\"type\":\"function\",\"**function\":{\"name\":\"\",**\"arguments\":\"{\\\"input\\\": \\\"\n> 2025-03-23 18:15:36,403 - DEBUG - Stream chunk for request 2ad4de47:\n> \"}}]},\"logprobs\":null,\"finish_reason\":\"tool_call\",\"matched_stop\":null}],\"usage\":null}\n> \n> \n> 199\n> data:\n> 2025-03-23 18:15:36,403 - DEBUG - Stream chunk for request 2ad4de47:\n> {\"id\":\"7874fe5978be41a1ad2daf0e67658490\",\"object\":\"chat.completion.chunk\",\"created\":1742742935,\"mode\n> 2025-03-23 18:15:36,404 - DEBUG - Stream chunk for request 2ad4de47:\n> l\":\"casperhansen/llama-3.3-70b-instruct-awq\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"cont\n> 2025-03-23 18:15:36,404 - DEBUG - Stream chunk for request 2ad4de47:\n> ent\":null,\"reasoning_content\":null,\"tool_calls\":[{\"id\":\"0\",\"type\":\"function\",\"function\":{\"name\":\"\",\"\n> 2025-03-23 18:15:36,405 - DEBUG - Stream chunk for request 2ad4de47:\n> arguments\":\"stan\"}}]},\"logprobs\":null,\"finish_reason\":\"tool_call\",\"matched_stop\":null}],\"usage\":null\n> 2025-03-23 18:15:36,423 - DEBUG - Stream chunk for request 2ad4de47:\n> }\n> \n> \n> 19d\n> data: {\"id\":\"7874fe5978be41a1ad2daf0e67658490\",\"object\":\"chat.completion.chunk\",\"created\":\n> 2025-03-23 18:15:36,424 - DEBUG - Stream chunk for request 2ad4de47:\n> 1742742935,\"model\":\"casperhansen/llama-3.3-70b-instruct-awq\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"\n> 2025-03-23 18:15:36,424 - DEBUG - Stream chunk for request 2ad4de47:\n> assistant\",\"content\":null,\"reasoning_content\":null,\"tool_calls\":[{\"id\":\"0\",\"type\":\"function\",\"functi\n> 2025-03-23 18:15:36,425 - DEBUG - Stream chunk for request 2ad4de47:\n> on\":{\"name\":\"\",\"arguments\":\"islav\\\"}\"}}]},\"logprobs\":null,\"finish_reason\":\"tool_call\",\"matched_stop\"\n> 2025-03-23 18:15:36,461 - DEBUG - Stream chunk for request 2ad4de47:\n> :null}],\"usage\":null}\n> \n> \n> ee\n> data: {\"id\":\"7874fe5978be41a1ad2daf0e67658490\",\"object\":\"chat.completio\n> 2025-03-23 18:15:36,462 - DEBUG - Stream chunk for request 2ad4de47:\n> n.chunk\",\"created\":1742742935,\"model\":\"casperhansen/llama-3.3-70b-instruct-awq\",\"choices\":[],\"usage\"\n> 2025-03-23 18:15:36,463 - DEBUG - Final stream chunk for request 2ad4de47:\n> :{\"prompt_tokens\":311,\"total_tokens\":329,\"completion_tokens\":18}}\n> \n> \n> e\n> data: [DONE]\n\nAs you can see in 1a1 chunk there is tool name, and starting from next chunk - 1a3 its missing\n\n### Reproduction\n\n--\n\n### Environment\n\nLatest version.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-23T15:28:24+00:00",
    "closed_at": "2025-03-28T05:23:31+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4700/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4700"
  },
  {
    "number": 865,
    "title": "RuntimeError: TopKTopPSamplingFromProbs failed with error code no kernel image is available for execution on the device  \u5df2\u6740\u6b7b[Bug] ",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n\n### Describe the bug\n\nRuntimeError: TopKTopPSamplingFromProbs failed with error code no kernel image is available for execution on the device\r\n\r\n\u5df2\u6740\u6b7b\n\n### Reproduction\n\nRuntimeError: TopKTopPSamplingFromProbs failed with error code no kernel image is available for execution on the device\r\n\r\n\u5df2\u6740\u6b7b\n\n### Environment\n\n```Shell\nRuntimeError: TopKTopPSamplingFromProbs failed with error code no kernel image is available for execution on the device\r\n\r\n\u5df2\u6740\u6b7b\n```\n",
    "labels": [
      "bug",
      "await-response"
    ],
    "state": "closed",
    "created_at": "2024-08-01T09:10:16+00:00",
    "closed_at": "2024-09-22T13:05:44+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/865/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/865"
  }
]
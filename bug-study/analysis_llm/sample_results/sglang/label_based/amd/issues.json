[
  {
    "number": 7703,
    "title": "[Bug] [CI regression] [AMD] TestNoOverlapScheduler",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThe CI **unit-test-backend-1-gpu-amd** failed when run`test/srt/test_no_overlap_scheduler.py`. It exits with a GPU memory access fault on node-2. \n\n**Error snippet**:\n\n```text\n...\nbatch. #new-seq: 1, #new-token: 32, #cached-token: 0, token usage: 0.00, #running-req: 8, #queue-req: 119\n[2025-07-02 03:22:31] Prefill batch. #new-seq: 1, #new-token: 32, #cached-token: 0, token usage: 0.00, #running-req: 8, #queue-req: 119\n...\n[2025-07-02 03:22:34] Prefill batch. #new-seq: 2, #new-token: 32, #cached-token: 0, token usage: 0.01, #running-req: 45, #queue-req: 81\nMemory access fault by GPU node-2 (Agent handle: 0xdedb180) on address 0x7f57d9a00000. Reason: Unknown.\n```\n@hubertlu-tw suggust temporarily disable the test in AMD CI to avoid blocking other PRs.\n\n### Reproduction\n\nSample failure run: https://github.com/sgl-project/sglang/actions/runs/15965626491/job/45029188833\n\n```bash\nSGLANG_AMD_CI=1 SGLANG_IS_IN_CI=1 SGLANG_USE_AITER=1 python3 -m unittest test_no_overlap_scheduler.py\n```\n\n### Environment\n\n* Docker image: `lmsysorg/sglang:v0.4.8.post1-rocm630`\n\nCC: @saienduri @HaiShaw @hubertlu-tw ",
    "labels": [
      "amd"
    ],
    "state": "open",
    "created_at": "2025-07-02T01:44:36+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7703/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7703"
  },
  {
    "number": 7701,
    "title": "[Bug] [CI regression] [AMD] TestVisionChunkedPrefill",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nCI unit-test-backend-1-gpu-amd seems to be broken since the past few days.\n\n```\n\toutput with chunked prefill:\n\tThe video features a person standing on a stage with a dark background. The individual is dressed in a black outfit and appears to be speaking or presenting. The stage\n\toutput without chunked prefill:\n\tThe video features a person standing on a stage with a dark background. The individual is dressed in a black outfit and appears to be speaking or presenting. The stage\n\toutput with chunked prefill:\n\t['The video features a close-up shot of a person holding a small, rectangular electronic device, which appears to be an iPod. The individual is wearing a black shirt', 'The video features a close-up shot of a person holding a small, white electronic device, which appears to be an iPod. The individual is wearing a black shirt', 'The video features a close-up shot of a person holding a small, white electronic device, which appears to be an iPod. The individual is wearing a black shirt', 'The video features a close-up shot of a person holding a small, white electronic device, which appears to be an iPod. The individual is wearing a black shirt']\n\toutput without chunked prefill:\n\t['The video features a close-up shot of a person holding a small, rectangular electronic device, which appears to be an iPod. The individual is wearing a black shirt', 'The video features a close-up shot of a person holding a small, white electronic device, which appears to be an iPod. The individual is wearing a black shirt', 'The video features a close-up shot of a person holding a small, white electronic device, which appears to be an iPod. The individual is wearing a black shirt', 'The video features a close-up of a person holding a small, white electronic device, which appears to be an iPod. The individual is wearing a black shirt and']\n\tE\n\t======================================================================\n\tERROR: test_chunked_prefill (test_vision_chunked_prefill.TestVisionChunkedPrefill.test_chunked_prefill)\n\t----------------------------------------------------------------------\n\tTraceback (most recent call last):\n\t  File \"/sgl-workspace/llama4_sgl/python/sglang/srt/utils.py\", line 2052, in retry\n\t    return fn()\n\t           ^^^^\n\t  File \"/sgl-workspace/llama4_sgl/python/sglang/test/test_utils.py\", line 1237, in <lambda>\n\t    lambda: super(CustomTestCase, self)._callTestMethod(method),\n\t            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\tAssertionError: Lists differ: ['The[511 chars]e-up shot of a person holding a small, white e[83 chars]irt'] != ['The[511 chars]e-up of a person holding a small, white electr[82 chars]and']\n\t\n\tFirst differing element 3:\n\t'The [21 chars]e-up shot of a person holding a small, white e[82 chars]hirt'\n\t'The [21 chars]e-up of a person holding a small, white electr[81 chars] and'\n\t\n\tDiff is 1108 characters long. Set self.maxDiff to None to see it.\n\n```\nI suggest to temporarily disable the test in CI to avoid blocking other PRs until we resolve the issue.\n\n\n\n\n### Reproduction\n\nSample failure: https://github.com/sgl-project/sglang/actions/runs/15965626491/job/45029188822\n```\nSGLANG_AMD_CI=1 SGLANG_IS_IN_CI=1 SGLANG_USE_AITER=1  python3 -m unittest test_vision_chunked_prefill.TestVisionChunkedPrefill.test_chunked_prefill\n```\n\n### Environment\n\nlmsysorg/sglang:v0.4.8.post1-rocm630\n\n\nCC: @saienduri @HaiShaw ",
    "labels": [
      "amd"
    ],
    "state": "open",
    "created_at": "2025-07-01T23:25:20+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7701/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7701"
  },
  {
    "number": 6060,
    "title": "[Feature] Provide pre-built wheel files for AMD GPUs",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nHi,\n\nThe installation of SGLang for CUDA is easier than AMD GPUs as there are pre-built wheel files as seen at https://docs.sglang.ai/start/install.html .\nWhile docker images are available for AMD, docker is pretty heavy, and doesn't fit in some infrastructures.\n\nIt would be great if pre-built wheel files were provided for ROCm, similarly to what Pytorch does.\n\nThank you for considering this,\nBest regards,\nEpliz\n\n### Related resources\n\n_No response_",
    "labels": [
      "amd"
    ],
    "state": "open",
    "created_at": "2025-05-06T16:44:39+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6060/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6060"
  },
  {
    "number": 6001,
    "title": "[Bug] OutOfResources: out of resource: shared memory, Required: 196608, Hardware limit: 65536 when run deepseek-r1 on sglang 0.4.6 with AMD Mi308",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nRun the deepseek-r1 with AMD MI308, sglang 0.6.4\n\nThe error log is as following:\n\npython3 -m sglang.launch_server --model /model/deepseek-r1 --trust-remote-code --tp 8 --chunked-prefill-size 130172 --enable-torch-compile --torch-compile-max-bs 64 --cuda-graph-max-bs 16  --mem-fraction-static 0.7\nINFO 05-04 01:04:03 __init__.py:179] Automatically detected platform rocm.\nWARNING 05-04 01:04:03 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n[2025-05-04 01:04:10] server_args=ServerArgs(model_path='/model/deepseek-r1', tokenizer_path='/model/deepseek-r1', tokenizer_mode='auto', skip_tokenizer_init=False, enable_tokenizer_batch_encode=False, load_format='auto', trust_remote_code=True, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='/model/deepseek-r1', chat_template=None, completion_template=None, is_embedding=False, revision=None, host='127.0.0.1', port=30000, mem_fraction_static=0.7, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=130172, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=8, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=219287203, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_multimodal=None, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode='auto', enable_torch_compile=True, torch_compile_max_bs=64, cuda_graph_max_bs=16, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_bootstrap_port=8998, disaggregation_transfer_backend='mooncake', disaggregation_ib_device=None)\nINFO 05-04 01:04:15 __init__.py:179] Automatically detected platform rocm.\nINFO 05-04 01:04:15 __init__.py:179] Automatically detected platform rocm.\nINFO 05-04 01:04:15 __init__.py:179] Automatically detected platform rocm.\nINFO 05-04 01:04:15 __init__.py:179] Automatically detected platform rocm.\nINFO 05-04 01:04:15 __init__.py:179] Automatically detected platform rocm.\nINFO 05-04 01:04:15 __init__.py:179] Automatically detected platform rocm.\nINFO 05-04 01:04:15 __init__.py:179] Automatically detected platform rocm.\nINFO 05-04 01:04:15 __init__.py:179] Automatically detected platform rocm.\nINFO 05-04 01:04:15 __init__.py:179] Automatically detected platform rocm.\n[2025-05-04 01:04:22 TP4] Process 11151 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155]\n[2025-05-04 01:04:22 TP6] Process 11153 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179]\n[2025-05-04 01:04:22 TP0] Process 11147 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107]\n[2025-05-04 01:04:22 TP1] Process 11148 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]\n[2025-05-04 01:04:22 TP4] Attention backend not set. Use triton backend by default.\n[2025-05-04 01:04:22 TP4] Chunked prefix cache is turned on.\n[2025-05-04 01:04:22 TP4] Init torch distributed begin.\n[2025-05-04 01:04:22 TP3] Process 11150 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]\n[2025-05-04 01:04:22 TP2] Process 11149 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131]\n[2025-05-04 01:04:22 TP6] Attention backend not set. Use triton backend by default.\n[2025-05-04 01:04:22 TP6] Chunked prefix cache is turned on.\n[2025-05-04 01:04:22 TP6] Init torch distributed begin.\n[2025-05-04 01:04:22 TP1] Attention backend not set. Use triton backend by default.\n[2025-05-04 01:04:22 TP1] Chunked prefix cache is turned on.\n[2025-05-04 01:04:22 TP1] Init torch distributed begin.\n[2025-05-04 01:04:22 TP0] Attention backend not set. Use triton backend by default.\n[2025-05-04 01:04:22 TP0] Chunked prefix cache is turned on.\n[2025-05-04 01:04:22 TP0] Init torch distributed begin.\n[2025-05-04 01:04:22 TP7] Process 11154 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]\n[2025-05-04 01:04:23 TP2] Attention backend not set. Use triton backend by default.\n[2025-05-04 01:04:23 TP2] Chunked prefix cache is turned on.\n[2025-05-04 01:04:23 TP2] Init torch distributed begin.\n[2025-05-04 01:04:23 TP3] Attention backend not set. Use triton backend by default.\n[2025-05-04 01:04:23 TP3] Chunked prefix cache is turned on.\n[2025-05-04 01:04:23 TP3] Init torch distributed begin.\n[2025-05-04 01:04:23 TP5] Process 11152 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]\n[2025-05-04 01:04:23 TP7] Attention backend not set. Use triton backend by default.\n[2025-05-04 01:04:23 TP7] Chunked prefix cache is turned on.\n[2025-05-04 01:04:23 TP7] Init torch distributed begin.\n[2025-05-04 01:04:23 TP5] Attention backend not set. Use triton backend by default.\n[2025-05-04 01:04:23 TP5] Chunked prefix cache is turned on.\n[2025-05-04 01:04:23 TP5] Init torch distributed begin.\n[2025-05-04 01:04:23 TP3] sglang is using nccl==2.21.5\n[2025-05-04 01:04:23 TP2] sglang is using nccl==2.21.5\n[2025-05-04 01:04:23 TP0] sglang is using nccl==2.21.5\n[2025-05-04 01:04:23 TP1] sglang is using nccl==2.21.5\n[2025-05-04 01:04:23 TP4] sglang is using nccl==2.21.5\n[2025-05-04 01:04:23 TP5] sglang is using nccl==2.21.5\n[2025-05-04 01:04:23 TP6] sglang is using nccl==2.21.5\n[2025-05-04 01:04:23 TP7] sglang is using nccl==2.21.5\n[2025-05-04 01:04:25 TP3] Init torch distributed ends. mem usage=5.75 GB\n[2025-05-04 01:04:25 TP1] Init torch distributed ends. mem usage=6.20 GB\n[2025-05-04 01:04:25 TP5] Init torch distributed ends. mem usage=6.11 GB\n[2025-05-04 01:04:25 TP7] Init torch distributed ends. mem usage=6.04 GB\n[2025-05-04 01:04:25 TP4] Init torch distributed ends. mem usage=6.01 GB\n[2025-05-04 01:04:25 TP6] Init torch distributed ends. mem usage=6.08 GB\n[2025-05-04 01:04:25 TP0] Init torch distributed ends. mem usage=6.14 GB\n[2025-05-04 01:04:25 TP2] Init torch distributed ends. mem usage=6.18 GB\n[2025-05-04 01:04:25 TP3] Load weight begin. avail mem=185.72 GB\n[2025-05-04 01:04:25 TP4] Load weight begin. avail mem=185.47 GB\n[2025-05-04 01:04:25 TP5] Load weight begin. avail mem=185.37 GB\n[2025-05-04 01:04:25 TP7] Load weight begin. avail mem=185.43 GB\n[2025-05-04 01:04:25 TP1] Load weight begin. avail mem=185.27 GB\n[2025-05-04 01:04:25 TP6] Load weight begin. avail mem=185.39 GB\n[2025-05-04 01:04:25 TP0] Load weight begin. avail mem=185.33 GB\n[2025-05-04 01:04:25 TP2] Load weight begin. avail mem=185.29 GB\n[2025-05-04 01:04:26 TP5] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-05-04 01:04:26 TP3] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-05-04 01:04:26 TP2] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-05-04 01:04:26 TP0] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-05-04 01:04:26 TP4] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-05-04 01:04:26 TP7] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-05-04 01:04:26 TP6] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-05-04 01:04:26 TP1] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-05-04 01:04:26 TP5] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-05-04 01:04:26 TP5] Deepseek V3/R1 with fp8 can use shared experts fusion optimization when SM version >=90. Shared experts fusion optimization is enabled.\n[2025-05-04 01:04:26 TP3] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-05-04 01:04:26 TP3] Deepseek V3/R1 with fp8 can use shared experts fusion optimization when SM version >=90. Shared experts fusion optimization is enabled.\n[2025-05-04 01:04:26 TP2] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-05-04 01:04:26 TP2] Deepseek V3/R1 with fp8 can use shared experts fusion optimization when SM version >=90. Shared experts fusion optimization is enabled.\n[2025-05-04 01:04:26 TP0] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-05-04 01:04:26 TP0] Deepseek V3/R1 with fp8 can use shared experts fusion optimization when SM version >=90. Shared experts fusion optimization is enabled.\n[2025-05-04 01:04:26 TP4] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-05-04 01:04:26 TP4] Deepseek V3/R1 with fp8 can use shared experts fusion optimization when SM version >=90. Shared experts fusion optimization is enabled.\n[2025-05-04 01:04:26 TP7] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-05-04 01:04:26 TP7] Deepseek V3/R1 with fp8 can use shared experts fusion optimization when SM version >=90. Shared experts fusion optimization is enabled.\n[2025-05-04 01:04:26 TP6] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-05-04 01:04:26 TP6] Deepseek V3/R1 with fp8 can use shared experts fusion optimization when SM version >=90. Shared experts fusion optimization is enabled.\n[2025-05-04 01:04:26 TP1] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-05-04 01:04:26 TP1] Deepseek V3/R1 with fp8 can use shared experts fusion optimization when SM version >=90. Shared experts fusion optimization is enabled.\nLoading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:   7% Completed | 12/163 [00:00<00:01, 119.81it/s]\nLoading safetensors checkpoint shards:  15% Completed | 24/163 [00:00<00:01, 103.84it/s]\nLoading safetensors checkpoint shards:  22% Completed | 36/163 [00:00<00:01, 107.95it/s]\nLoading safetensors checkpoint shards:  29% Completed | 47/163 [00:00<00:01, 106.80it/s]\nLoading safetensors checkpoint shards:  36% Completed | 58/163 [00:00<00:02, 47.16it/s]\nLoading safetensors checkpoint shards:  42% Completed | 69/163 [00:01<00:01, 57.86it/s]\nLoading safetensors checkpoint shards:  49% Completed | 80/163 [00:01<00:01, 67.91it/s]\nLoading safetensors checkpoint shards:  56% Completed | 91/163 [00:01<00:00, 76.72it/s]\nLoading safetensors checkpoint shards:  63% Completed | 103/163 [00:01<00:00, 86.67it/s]\nLoading safetensors checkpoint shards:  70% Completed | 114/163 [00:01<00:00, 92.46it/s]\nLoading safetensors checkpoint shards:  77% Completed | 126/163 [00:01<00:00, 99.31it/s]\nLoading safetensors checkpoint shards:  84% Completed | 137/163 [00:01<00:00, 101.76it/s]\nLoading safetensors checkpoint shards:  91% Completed | 148/163 [00:01<00:00, 104.05it/s]\nCloning 8 replicas of the shared expert into MoE: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 58/58 [00:00<00:00, 15212.91it/s]\n\nCloning 8 replicas of the shared expert into MoE: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 58/58 [00:00<00:00, 15917.66it/s]\nCloning 8 replicas of the shared expert into MoE: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 58/58 [00:00<00:00, 16911.34it/s]\nLoading safetensors checkpoint shards:  98% Completed | 159/163 [00:02<00:00, 48.73it/s]\nLoading safetensors checkpoint shards: 100% Completed | 163/163 [00:02<00:00, 71.59it/s]\n\nCloning 8 replicas of the shared expert into MoE: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 58/58 [00:00<00:00, 15639.32it/s]\nCloning 8 replicas of the shared expert into MoE: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 58/58 [00:00<00:00, 14501.92it/s]\nCloning 8 replicas of the shared expert into MoE: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 58/58 [00:00<00:00, 13997.91it/s]\nCloning 8 replicas of the shared expert into MoE: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 58/58 [00:00<00:00, 15562.28it/s]\n[2025-05-04 01:05:33 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=103.71 GB, mem usage=81.66 GB.\n[2025-05-04 01:05:33 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=104.06 GB, mem usage=81.66 GB.\n[2025-05-04 01:05:34 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=103.63 GB, mem usage=81.66 GB.\n[2025-05-04 01:05:34 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=103.77 GB, mem usage=81.66 GB.\n[2025-05-04 01:05:35 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=103.67 GB, mem usage=81.66 GB.\n[2025-05-04 01:05:35 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=103.61 GB, mem usage=81.66 GB.\n[2025-05-04 01:05:36 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=103.73 GB, mem usage=81.66 GB.\n[2025-05-04 01:05:36 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=103.81 GB, mem usage=81.66 GB.\n[2025-05-04 01:05:36 TP6] KV Cache is allocated. #tokens: 720206, KV size: 47.13 GB\n[2025-05-04 01:05:36 TP6] Memory pool end. avail mem=55.18 GB\n[2025-05-04 01:05:36 TP5] KV Cache is allocated. #tokens: 720206, KV size: 47.13 GB\n[2025-05-04 01:05:36 TP5] Memory pool end. avail mem=55.15 GB\n[2025-05-04 01:05:36 TP4] KV Cache is allocated. #tokens: 720206, KV size: 47.13 GB\n[2025-05-04 01:05:36 TP4] Memory pool end. avail mem=55.25 GB\n[2025-05-04 01:05:36 TP2] KV Cache is allocated. #tokens: 720206, KV size: 47.13 GB\n[2025-05-04 01:05:36 TP7] KV Cache is allocated. #tokens: 720206, KV size: 47.13 GB\n[2025-05-04 01:05:36 TP1] KV Cache is allocated. #tokens: 720206, KV size: 47.13 GB\n[2025-05-04 01:05:36 TP2] Memory pool end. avail mem=55.08 GB\n[2025-05-04 01:05:36 TP1] Memory pool end. avail mem=55.06 GB\n[2025-05-04 01:05:36 TP7] Memory pool end. avail mem=55.22 GB\n[2025-05-04 01:05:36 TP3] KV Cache is allocated. #tokens: 720206, KV size: 47.13 GB\n[2025-05-04 01:05:36 TP3] Memory pool end. avail mem=55.51 GB\n[2025-05-04 01:05:36 TP0] KV Cache is allocated. #tokens: 720206, KV size: 47.13 GB\n[2025-05-04 01:05:36 TP0] Memory pool end. avail mem=55.12 GB\n[2025-05-04 01:05:38 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=54.91 GB\n[2025-05-04 01:05:38 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=54.89 GB\n[2025-05-04 01:05:38 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=54.99 GB\n[2025-05-04 01:05:38 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=54.95 GB\n[2025-05-04 01:05:38 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=55.01 GB\n[2025-05-04 01:05:38 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=55.09 GB\n[2025-05-04 01:05:38 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=55.34 GB\n[2025-05-04 01:05:38 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=55.05 GB\n[2025-05-04 01:05:38 TP0] Capture cuda graph bs [1, 2, 4, 8, 16]\nCapturing batches (avail_mem=54.94 GB):   0%|                                                     | 0/5 [00:00<?, ?it/s][2025-05-04 01:05:44 TP1] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=2112,K=7168,device_name=AMD_Radeon_Graphics,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-05-04 01:05:44 TP7] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=2112,K=7168,device_name=AMD_Radeon_Graphics,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-05-04 01:05:44 TP4] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=2112,K=7168,device_name=AMD_Radeon_Graphics,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-05-04 01:05:44 TP5] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=2112,K=7168,device_name=AMD_Radeon_Graphics,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-05-04 01:05:44 TP6] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=2112,K=7168,device_name=AMD_Radeon_Graphics,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-05-04 01:05:44 TP3] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=2112,K=7168,device_name=AMD_Radeon_Graphics,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-05-04 01:05:44 TP0] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=2112,K=7168,device_name=AMD_Radeon_Graphics,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-05-04 01:05:44 TP2] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=2112,K=7168,device_name=AMD_Radeon_Graphics,dtype=fp8_w8a8,block_shape=[128, 128].json\nCapturing batches (avail_mem=54.94 GB):   0%|                                                     | 0/5 [00:25<?, ?it/s]\n[2025-05-04 01:06:04 TP0] Registering 0 cuda graph addresses\n[2025-05-04 01:06:04 TP6] Registering 0 cuda graph addresses\n[2025-05-04 01:06:04 TP3] Registering 0 cuda graph addresses\n[2025-05-04 01:06:04 TP7] Registering 0 cuda graph addresses\n[2025-05-04 01:06:04 TP2] Registering 0 cuda graph addresses\n[2025-05-04 01:06:04 TP1] Registering 0 cuda graph addresses\n[2025-05-04 01:06:04 TP4] Registering 0 cuda graph addresses\n[2025-05-04 01:06:04 TP5] Registering 0 cuda graph addresses\n[2025-05-04 01:06:04 TP0] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 296, in __init__\n    self.capture()\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 380, in capture\n    ) = self.capture_one_batch_size(bs, forward)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 501, in capture_one_batch_size\n    run_once()\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 489, in run_once\n    logits_output_or_pp_proxy_tensors = forward(\n                                        ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\", line 576, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py\", line 45, in inner\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1493, in forward\n    @torch.no_grad()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1422, in forward\n    hidden_states, residual = layer(\n                              ^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1193, in forward\n    def forward(\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1212, in forward_ffn_with_full_input\n    def forward_ffn_with_full_input(\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 614, in forward\n    def forward(\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 698, in forward_absorb\n    def forward_absorb(\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/linear.py\", line 285, in forward\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/quantization/fp8.py\", line 399, in apply\n    def apply(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/quantization/fp8_utils.py\", line 112, in apply_w8a8_block_fp8_linear\n    def apply_w8a8_block_fp8_linear(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/quantization/fp8_kernel.py\", line 707, in w8a8_block_fp8_matmul\n    def w8a8_block_fp8_matmul(\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 1387, in __call__\n    return self._torchdynamo_orig_callable(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 1171, in __call__\n    result = self._inner_convert(\n             ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 548, in __call__\n    return _compile(\n           ^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 988, in _compile\n    guarded_code = compile_inner(code, one_graph, hooks, transform)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 716, in compile_inner\n    return _compile_inner(code, one_graph, hooks, transform)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n    return function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 751, in _compile_inner\n    out_code = transform_code_object(code, transform)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n    transformations(instructions, code_options)\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 232, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 663, in transform\n    tracer.run()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2870, in run\n    super().run()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1053, in run\n    while self.step():\n          ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 963, in step\n    self.dispatch_table[inst.opcode](self, inst)\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3050, in RETURN_VALUE\n    self._return(inst)\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3035, in _return\n    self.output.compile_subgraph(\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1101, in compile_subgraph\n    self.compile_and_call_fx_graph(\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1382, in compile_and_call_fx_graph\n    compiled_fn = self.call_user_compiler(gm)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1432, in call_user_compiler\n    return self._call_user_compiler(gm)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1483, in _call_user_compiler\n    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1462, in _call_user_compiler\n    compiled_fn = compiler_fn(gm, self.example_inputs())\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 130, in __call__\n    compiled_gm = compiler_fn(gm, example_inputs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/__init__.py\", line 2314, in __call__\n    return compile_fx(model_, inputs_, config_patches=self.config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py\", line 1551, in compile_fx\n    return compile_fx(\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py\", line 1880, in compile_fx\n    return aot_autograd(\n           ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/backends/common.py\", line 83, in __call__\n    cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py\", line 1145, in aot_module_simplified\n    compiled_fn = AOTAutogradCache.load(\n                  ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/autograd_cache.py\", line 754, in load\n    compiled_fn = dispatch_and_compile()\n                  ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py\", line 1131, in dispatch_and_compile\n    compiled_fn, _ = create_aot_dispatcher_function(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py\", line 580, in create_aot_dispatcher_function\n    return _create_aot_dispatcher_function(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py\", line 830, in _create_aot_dispatcher_function\n    compiled_fn, fw_metadata = compiler_fn(\n                               ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 201, in aot_dispatch_base\n    compiled_fw = compiler(fw_module, updated_flat_args)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py\", line 489, in __call__\n    return self.compiler_fn(gm, example_inputs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py\", line 1758, in fw_compiler_base\n    return inner_compile(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 81, in inner\n    return func(*args, **kwds)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py\", line 572, in compile_fx_inner\n    return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/repro/after_aot.py\", line 102, in debug_wrapper\n    inner_compiled_fn = compiler_fn(gm, example_inputs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py\", line 686, in _compile_fx_inner\n    mb_compiled_graph = fx_codegen_and_compile(\n                        ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py\", line 1129, in fx_codegen_and_compile\n    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)\n\n\n[2025-05-04 01:06:04 TP2] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 296, in __init__\n    self.capture()\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 380, in capture\n    ) = self.capture_one_batch_size(bs, forward)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 501, in capture_one_batch_size\n    run_once()\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 489, in run_once\n    logits_output_or_pp_proxy_tensors = forward(\n                                        ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\", line 576, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py\", line 45, in inner\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1493, in forward\n    @torch.no_grad()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1422, in forward\n    hidden_states, residual = layer(\n                              ^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1193, in forward\n    def forward(\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1212, in forward_ffn_with_full_input\n    def forward_ffn_with_full_input(\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 614, in forward\n    def forward(\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 698, in forward_absorb\n    def forward_absorb(\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/linear.py\", line 285, in forward\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/quantization/fp8.py\", line 399, in apply\n    def apply(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/quantization/fp8_utils.py\", line 112, in apply_w8a8_block_fp8_linear\n    def apply_w8a8_block_fp8_linear(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/quantization/fp8_kernel.py\", line 707, in w8a8_block_fp8_matmul\n    def w8a8_block_fp8_matmul(\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 1387, in __call__\n    return self._torchdynamo_orig_callable(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 1171, in __call__\n    result = self._inner_convert(\n             ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 548, in __call__\n    return _compile(\n           ^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 988, in _compile\n    guarded_code = compile_inner(code, one_graph, hooks, transform)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 716, in compile_inner\n    return _compile_inner(code, one_graph, hooks, transform)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n    return function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 751, in _compile_inner\n    out_code = transform_code_object(code, transform)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n    transformations(instructions, code_options)\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 232, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 663, in transform\n    tracer.run()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2870, in run\n    super().run()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1053, in run\n    while self.step():\n          ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 963, in step\n    self.dispatch_table[inst.opcode](self, inst)\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3050, in RETURN_VALUE\n    self._return(inst)\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3035, in _return\n    self.output.compile_subgraph(\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1101, in compile_subgraph\n    self.compile_and_call_fx_graph(\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1382, in compile_and_call_fx_graph\n    compiled_fn = self.call_user_compiler(gm)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1432, in call_user_compiler\n    return self._call_user_compiler(gm)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1483, in _call_user_compiler\n    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1462, in _call_user_compiler\n    compiled_fn = compiler_fn(gm, self.example_inputs())\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 130, in __call__\n    compiled_gm = compiler_fn(gm, example_inputs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/__init__.py\", line 2314, in __call__\n    return compile_fx(model_, inputs_, config_patches=self.config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py\", line 1551, in compile_fx\n    return compile_fx(\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py\", line 1880, in compile_fx\n    return aot_autograd(\n           ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/backends/common.py\", line 83, in __call__\n    cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py\", line 1145, in aot_module_simplified\n    compiled_fn = AOTAutogradCache.load(\n                  ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/autograd_cache.py\", line 754, in load\n    compiled_fn = dispatch_and_compile()\n                  ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py\", line 1131, in dispatch_and_compile\n    compiled_fn, _ = create_aot_dispatcher_function(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py\", line 580, in create_aot_dispatcher_function\n    return _create_aot_dispatcher_function(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py\", line 830, in _create_aot_dispatcher_function\n    compiled_fn, fw_metadata = compiler_fn(\n                               ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 201, in aot_dispatch_base\n    compiled_fw = compiler(fw_module, updated_flat_args)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py\", line 489, in __call__\n    return self.compiler_fn(gm, example_inputs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py\", line 1758, in fw_compiler_base\n    return inner_compile(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 81, in inner\n    return func(*args, **kwds)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py\", line 572, in compile_fx_inner\n    return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/repro/after_aot.py\", line 102, in debug_wrapper\n    inner_compiled_fn = compiler_fn(gm, example_inputs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py\", line 686, in _compile_fx_inner\n    mb_compiled_graph = fx_codegen_and_compile(\n                        ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py\", line 1129, in fx_codegen_and_compile\n    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py\", line 1044, in codegen_and_compile\n    compiled_fn = graph.compile_to_module().call\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py\", line 1978, in compile_to_module\n    return self._compile_to_module()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/graph.py\", line 2019, in _compile_to_module\n    mod = PyCodeCache.load_by_key_path(\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/codecache.py\", line 2756, in load_by_key_path\n    mod = _reload_python_module(key, path)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n    exec(code, mod.__dict__, mod.__dict__)\n  File \"/tmp/torchinductor_root/pu/cpuz5vnpcctkeezp3zhsip6zrl3jqiihx366qoh3imrqx2c33ewy.py\", line 220, in <module>\n    async_compile.wait(globals())\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/async_compile.py\", line 306, in wait\n    scope[key] = result.result()\n                 ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/codecache.py\", line 3241, in result\n    self.kernel.precompile()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 308, in precompile\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 302, in precompile\n    compiled_binary, launcher = self._precompile_config(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py\", line 541, in _precompile_config\n    binary._init_handles()\n  File \"/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py\", line 395, in _init_handles\n    raise OutOfResources(self.metadata.shared, max_shared, \"shared memory\")\ntorch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\nOutOfResources: out of resource: shared memory, Required: 196608, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2215, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, pp_rank, dp_rank)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 268, in __init__\n    self.tp_worker = TpWorkerClass(\n                     ^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 64, in __init__\n    self.worker = TpModelWorker(\n                  ^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 81, in __init__\n    self.model_runner = ModelRunner(\n                        ^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 190, in __init__\n    self.initialize(min_per_gpu_memory)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 239, in initialize\n    self.init_cuda_graphs()\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1014, in init_cuda_graphs\n    self.cuda_graph_runner = CudaGraphRunner(self)\n                             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 298, in __init__\n    raise Exception(\nException: Capture cuda graph failed: backend='inductor' raised:\nOutOfResources: out of resource: shared memory, Required: 196608, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n\nPossible solutions:\n1. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)\n2. set --cuda-graph-max-bs to a smaller value (e.g., 16)\n3. disable torch compile by not using --enable-torch-compile\n4. disable cuda graph by --disable-cuda-graph. (Not recommonded. Huge perf loss)\nOpen an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose\n\n\n[2025-05-04 01:06:04] Received sigquit from a child process. It usually means the child failed.\n--- Logging error ---\n[2025-05-04 01:06:04] Received sigquit from a child process. It usually means the child failed.\nKilled\n\n### Reproduction\n\nThe following command is used to reproduce this issue,\nGet the image:\ndocker pull lmsysorg/sglang:v0.4.6.post2-rocm630\nRun the image:\ndocker run --name rocm-sglang-quickstart -d -it --rm \\\n    --ipc=host \\\n    --network=host \\\n    --privileged \\\n    --shm-size 128G \\\n    --cap-add=CAP_SYS_ADMIN \\\n    --device=/dev/kfd \\\n    --device=/dev/dri \\\n    --group-add video \\\n    --group-add render \\\n    --cap-add=SYS_PTRACE \\\n    --security-opt seccomp=unconfined \\\n    --security-opt apparmor=unconfined \\\n    -v /nvme/data/deepseek-r1/deepseek-r1:/model/deepseek-r1 \\\n    lmsysorg/sglang:v0.4.6.post2-rocm630\n\n#Execute into the running container\ndocker exec -it rocm-sglang-quickstart bash\n\n#Start the sglang server\npython3 -m sglang.launch_server --model /model/deepseek-r1 --trust-remote-code --tp 8 --chunked-prefill-size 130172 --enable-torch-compile --torch-compile-max-bs 64 --cuda-graph-max-bs 16  --mem-fraction-static 0.7\n\n### Environment\n\nPython: 3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0]\nROCM available: True\nGPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.4\nROCM_HOME: /opt/rocm\nHIPCC: HIP version: 6.3.42131-fa1d09cbd\nROCM Driver Version: 6.8.5\nPyTorch: 2.6.0a0+git8d4926e\nsglang: 0.4.6.post2\nsgl_kernel: 0.1.1\nflashinfer_python: Module Not Found\ntriton: 3.2.0+gitcddf0fc3\ntransformers: 4.51.1\ntorchao: 0.10.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.6\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.5\npython-multipart: 0.0.20\npyzmq: 26.2.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.7.dev2+g113274a0.rocm630\nxgrammar: 0.1.17\nopenai: 1.76.2\ntiktoken: 0.7.0\nanthropic: 0.50.0\nlitellm: 1.67.5\ndecord: 0.6.0\nAMD Topology:\n\n\n============================ ROCm System Management Interface ============================\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI\nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI\nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI\nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI\nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI\nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0\n================================== End of ROCm SMI Log ===================================\n\nulimit soft: 1048576",
    "labels": [
      "amd"
    ],
    "state": "open",
    "created_at": "2025-05-04T01:20:02+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6001/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6001"
  },
  {
    "number": 5987,
    "title": "[Bug] Segmentation Fault on AMD MI300X",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI am unable to run online benchmarks on the new AMD SGLang Docker image due to a segmentation fault error.\n\n- Hardware: MI300X\n- Docker image: `rocm/sgl-dev:upstream_20250324`\n\nI am attaching the error log because it exceeds the character limit.\n\n[segfault_error.log](https://github.com/user-attachments/files/20017838/segfault_error.log)\n\n### Reproduction\n\n```bash\ndocker network create bmk-net\n\ndocker run --rm -d --network bmk-net --ipc host --name bmk-server     --privileged --cap-add=CAP_SYS_ADMIN --device=/dev/kfd --device=/dev/dri --device=/dev/mem     --group-add render --cap-add=SYS_PTRACE --security-opt seccomp=unconfined     -v \"$PWD/.hf_cache/\":/root/hf_cache/ -v \"$PWD/.inductor_cache/\":/tmp/torchinductor_root/     -e HF_HUB_CACHE=/root/hf_cache/ -e HF_TOKEN=\"$(cat hf_token.txt)\"     rocm/sgl-dev:upstream_20250324     python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --host 0.0.0.0 --port 8000 --tp 8 --trust-remote-code     --chunked-prefill-size 131072\n\nprintf \"RESULT_FILENAME=%s\n\" \"dsv3_tp8_isl1024_osl4096_c256\"\nwhile ! docker logs bmk-server 2>&1 | grep -q \"The server is fired up and ready to roll!\"; do\n    sleep 1\ndone\n\ndocker run --rm -t --network bmk-net --name bmk-client     --privileged --cap-add=CAP_SYS_ADMIN --device=/dev/kfd --device=/dev/dri --device=/dev/mem     --group-add render --cap-add=SYS_PTRACE --security-opt seccomp=unconfined     -v $PWD:/workspace/ -w /workspace/vllm/benchmarks/ -e HF_TOKEN=$(cat hf_token.txt)     rocm/vllm:rocm6.3.1_instinct_vllm0.8.3_20250410         python benchmark_serving.py             --model deepseek-ai/DeepSeek-V3 --backend vllm --base-url \"http://bmk-server:8000\"             --dataset-name \"random\" --random-input-len 1024 --random-output-len 4096 --random-prefix-len 0             --num-prompts $(( 256 * 10 )) --max-concurrency 256 --request-rate \"inf\" --ignore-eos             --save-result --result-dir \"/workspace/results/\" --result-filename \"dsv3_tp8_isl1024_osl4096_c256.json\" --percentile-metrics \"ttft,tpot,itl,e2el\"\n\ndocker stop bmk-server; docker network rm bmk-net\n```\n\n### Environment\n\n```bash\nPython: 3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0]\nROCM available: True\nGPU 0,1,2,3,4,5,6,7: AMD Instinct MI300X\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.4\nROCM_HOME: /opt/rocm\nHIPCC: HIP version: 6.3.42131-fa1d09cbd\nROCM Driver Version: 6.10.5\nPyTorch: 2.6.0a0+git8d4926e\nsgl_kernel: 0.0.5.post3\nflashinfer: Module Not Found\ntriton: 3.2.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.6\nhf_transfer: 0.1.9\nhuggingface_hub: 0.27.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.5\nmultipart: 0.0.20\nzmq: 26.2.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.7.dev2+g113274a0\nopenai: 1.61.1\nanthropic: 0.45.2\ndecord: 0.6.0\nAMD Topology:\n\n\n============================ ROCm System Management Interface ============================\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI\nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI\nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI\nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI\nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI\nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0\n================================== End of ROCm SMI Log ===================================\n\nulimit soft: 1048576\n```",
    "labels": [
      "amd"
    ],
    "state": "closed",
    "created_at": "2025-05-02T20:08:36+00:00",
    "closed_at": "2025-06-12T05:58:47+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5987/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5987"
  },
  {
    "number": 5775,
    "title": "[Bug] RuntimeError: HIP error: invalid device function in Mi100",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nEnvironment: The docker lmsysorg/sglang:v0.4.5.post3-rocm630\nJust run python3 -m sglang.launch_server --model-path mistralhf-gptq --host 0.0.0.0 --port 30000 to launch sglang to serve a mistral gptq model.\n\nWhen I launch sglang or vllm to serve a mistral model on Mi100 cards, the folloing error was shown:\n\nWARNING: Published ports are discarded when using host network mode\nroot@G482-Z52:/sgl-workspace# python3 -m sglang.launch_server --model-path /data/mistralhf-gptq --host 0.0.0.0 --port 30000\nINFO 04-27 07:30:19 __init__.py:179] Automatically detected platform rocm.\nWARNING 04-27 07:30:19 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n/sgl-workspace/sglang/python/sglang/srt/managers/session_controller.py:57: SyntaxWarning: invalid escape sequence '\\-'\n  prefix = \" \" * len(origin_prefix) + \" \\- \" + child.req.rid\n[2025-04-27 07:30:43] server_args=ServerArgs(model_path='/data/mistralhf-gptq', tokenizer_path='/data/mistralhf-gptq', tokenizer_mode='auto', skip_tokenizer_init=False, enable_tokenizer_batch_encode=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='/data/mistralhf-gptq', chat_template=None, completion_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=30000, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, stream_interval=1, stream_output=False, random_seed=645934802, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_llama4_multimodal=None, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode='auto', enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=None, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_bootstrap_port=8998, disaggregation_transfer_backend='mooncake', disaggregation_ib_device=None)\n[2025-04-27 07:30:43] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nINFO 04-27 07:30:48 __init__.py:179] Automatically detected platform rocm.\nINFO 04-27 07:30:48 __init__.py:179] Automatically detected platform rocm.\n[2025-04-27 07:30:55 TP0] Process 171 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n[2025-04-27 07:30:55 TP0] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n[2025-04-27 07:30:55 TP0] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n[2025-04-27 07:30:55 TP0] Attention backend not set. Use triton backend by default.\n[2025-04-27 07:30:55 TP0] Init torch distributed begin.\n[2025-04-27 07:30:55 TP0] Init torch distributed ends. mem usage=0.00 GB\n[2025-04-27 07:30:55 TP0] Load weight begin. avail mem=10.60 GB\n[2025-04-27 07:30:55 TP0] sgl-kernel is not available on Non-NV platforms. Fallback to other kernel libraries.\n[2025-04-27 07:30:56 TP0] Ignore import error when loading sglang.srt.models.deepseek_janus_pro. Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nHIP error: invalid device function\nHIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing AMD_SERIALIZE_KERNEL=3\nCompile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\n\n[2025-04-27 07:30:56 TP0] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-04-27 07:30:56 TP0] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2001, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 261, in __init__\n    self.tp_worker = TpWorkerClass(\n                     ^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 75, in __init__\n    self.model_runner = ModelRunner(\n                        ^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 181, in __init__\n    self.initialize(min_per_gpu_memory)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 191, in initialize\n    self.load_model()\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 443, in load_model\n    self.model = get_model(\n                 ^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/__init__.py\", line 22, in get_model\n    return loader.load_model(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/loader.py\", line 372, in load_model\n    model = _initialize_model(\n            ^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/loader.py\", line 153, in _initialize_model\n    return model_class(\n           ^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/llama.py\", line 381, in __init__\n    self.model = self._init_model(config, quant_config, add_prefix(\"model\", prefix))\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/llama.py\", line 412, in _init_model\n    return LlamaModel(config, quant_config=quant_config, prefix=prefix)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/llama.py\", line 284, in __init__\n    self.layers = make_layers(\n                  ^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/utils.py\", line 422, in make_layers\n    maybe_offload_to_cpu(layer_fn(idx=idx, prefix=add_prefix(idx, prefix)))\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/llama.py\", line 286, in <lambda>\n    lambda idx, prefix: LlamaDecoderLayer(\n                        ^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/llama.py\", line 216, in __init__\n    self.self_attn = LlamaAttention(\n                     ^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/llama.py\", line 159, in __init__\n    self.rotary_emb = get_rope(\n                      ^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/rotary_embedding.py\", line 1055, in get_rope\n    rotary_emb = RotaryEmbedding(\n                 ^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/rotary_embedding.py\", line 83, in __init__\n    cache = self._compute_cos_sin_cache()\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/rotary_embedding.py\", line 106, in _compute_cos_sin_cache\n    inv_freq = self._compute_inv_freq(self.base)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/rotary_embedding.py\", line 99, in _compute_inv_freq\n    torch.arange(0, self.rotary_dim, 2, dtype=torch.float) / self.rotary_dim\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py\", line 104, in __torch_function__\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: HIP error: invalid device function\nHIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing AMD_SERIALIZE_KERNEL=3\nCompile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\n\n\n[2025-04-27 07:30:56] Received sigquit from a child process. It usually means the child failed.\n\n\n### Reproduction\n\npython3 -m sglang.launch_server --model-path mistralhf-gptq --host 0.0.0.0 --port 30000\n\n### Environment\n\nPython: 3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0]\nROCM available: True\nGPU 0,1,2,3,4,5,6,7: AMD Instinct MI100\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nROCM_HOME: /opt/rocm\nHIPCC: HIP version: 6.3.42131-fa1d09cbd\nROCM Driver Version: 6.12.12\nPyTorch: 2.6.0a0+git8d4926e\nsglang: 0.4.5.post3\nsgl_kernel: 0.0.9.post2\nflashinfer: Module Not Found\ntriton: 3.2.0+gitcddf0fc3\ntransformers: 4.51.1\ntorchao: 0.10.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.6\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.5\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.7.dev2+g113274a0.rocm630\nxgrammar: 0.1.17\nopenai: 1.75.0\ntiktoken: 0.7.0\nanthropic: 0.49.0\nlitellm: 1.67.0.post1\ndecord: 0.6.0\nAMD Topology: \n\n\n============================ ROCm System Management Interface ============================\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \nGPU0   0            PCIE         XGMI         XGMI         PCIE         PCIE         PCIE         XGMI         \nGPU1   PCIE         0            PCIE         PCIE         XGMI         XGMI         XGMI         PCIE         \nGPU2   XGMI         PCIE         0            XGMI         PCIE         PCIE         PCIE         XGMI         \nGPU3   XGMI         PCIE         XGMI         0            PCIE         PCIE         PCIE         XGMI         \nGPU4   PCIE         XGMI         PCIE         PCIE         0            XGMI         XGMI         PCIE         \nGPU5   PCIE         XGMI         PCIE         PCIE         XGMI         0            XGMI         PCIE         \nGPU6   PCIE         XGMI         PCIE         PCIE         XGMI         XGMI         0            PCIE         \nGPU7   XGMI         PCIE         XGMI         XGMI         PCIE         PCIE         PCIE         0            \n================================== End of ROCm SMI Log ===================================\n\nulimit soft: 1048576\n",
    "labels": [
      "amd"
    ],
    "state": "open",
    "created_at": "2025-04-27T07:43:22+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5775/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5775"
  },
  {
    "number": 5705,
    "title": "[Bug] Unsupported conversion from 'f8E4M3FN' to 'f16' When Inferencing DeepSeek-v3 with enable_ep_moe on AMD MI300x",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\noffline batch inference, using 8x AMD MI300x, when enable_ep_moe, error happens when starts to prefill.\n```python \n/sgl-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/kernels.py:606:42: error: Unsupported conversion from 'f8E4M3FN' to 'f16'\n            accumulator += tl.dot(a_tile, b_tile.T) * a_scale * b_scale[None, :]\n                                         ^\n/sgl-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/kernels.py:606:42: error: failed to legalize operation 'tt.fp_to_fp' that was explicitly marked illegal\n            accumulator += tl.dot(a_tile, b_tile.T) * a_scale * b_scale[None, :]\n                                         ^\n/sgl-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/kernels.py:606:42: note: see current operation: %14339 = \"tt.fp_to_fp\"(%8960) : (tensor<128x32xf8E4M3FN, #ttg.blocked<{sizePerThread = [16, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>>) -> tensor<128x32xf32, #ttg.blocked<{sizePerThread = [16, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>>\n/sgl-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/kernels.py:606:42: error: Unsupported conversion from 'f8E4M3FN' to 'f16'\n            accumulator += tl.dot(a_tile, b_tile.T) * a_scale * b_scale[None, :]\n                                         ^\n/sgl-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/kernels.py:606:42: error: failed to legalize operation 'tt.fp_to_fp' that was explicitly marked illegal\n            accumulator += tl.dot(a_tile, b_tile.T) * a_scale * b_scale[None, :]\n                                         ^\n/sgl-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/kernels.py:606:42: note: see current operation: %14339 = \"tt.fp_to_fp\"(%8960) : (tensor<128x32xf8E4M3FN, #ttg.blocked<{sizePerThread = [16, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>>) -> tensor<128x32xf32, #ttg.blocked<{sizePerThread = [16, 1], threadsPerWarp = [8, 8], warpsPerCTA = [1, 4], order = [0, 1]}>>\n[2025-04-24 07:50:01 TP2] TpModelWorkerClient hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 112, in forward_thread_func\n    self.forward_thread_func_()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 143, in forward_thread_func_\n    logits_output, next_token_ids = self.worker.forward_batch_generation(\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 175, in forward_batch_generation\n    logits_output = self.model_runner.forward(forward_batch)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1001, in forward\n    return self.forward_extend(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 962, in forward_extend\n    return self.model.forward(\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1381, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1311, in forward\n    hidden_states, residual = layer(\n                              ^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1116, in forward\n    return self.forward_normal(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1172, in forward_normal\n    hidden_states = self.mlp(hidden_states)\n                    ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 276, in forward\n    return self.forward_normal(hidden_states)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 288, in forward_normal\n    self.experts(hidden_states=hidden_states, router_logits=router_logits)\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/layer.py\", line 277, in forward\n    gateup_output = self.grouped_gemm_runner(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/layer.py\", line 107, in forward\n    c = grouped_gemm_triton(\n        ^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/kernels.py\", line 684, in grouped_gemm_triton\n    grouped_gemm_triton_kernel[grid](\n  File \"/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py\", line 330, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py\", line 623, in run\n    kernel = self.compile(\n             ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py\", line 286, in compile\n    next_module = compile_ir(module, metadata)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/triton/backends/amd/compiler.py\", line 382, in <lambda>\n    stages[\"llir\"] = lambda src, metadata: self.make_llir(src, metadata, options)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/triton/backends/amd/compiler.py\", line 303, in make_llir\n    pm.run(mod)\nRuntimeError: PassManager::run failed\n```\n\n### Reproduction\n\nI wrote an entry script for offline inference, the parameters of the Engine is tp_size=8, enable_ep_moe=True, trust_remote_code=True, disable_cuda_graph = True. \n\nI'm using deepseek-v3.\n\n### Environment\n\nPython: 3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0]\nROCM available: True\nGPU 0,1,2,3,4,5,6,7: AMD Instinct MI300X\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.4\nROCM_HOME: /opt/rocm\nHIPCC: HIP version: 6.3.42131-fa1d09cbd\nROCM Driver Version: 6.10.5\nPyTorch: 2.6.0a0+git8d4926e\nsglang: 0.4.5\nsgl_kernel: 0.0.8.post3\nflashinfer: Module Not Found\ntriton: 3.2.0+gitcddf0fc3\ntransformers: 4.51.1\ntorchao: 0.10.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.6\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.5\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.7.dev2+g113274a0.rocm630\nxgrammar: 0.1.17\nopenai: 1.74.0\ntiktoken: 0.7.0\nanthropic: 0.49.0\nlitellm: 1.66.0\ndecord: 0.6.0\nAMD Topology: \n\n\n![Image](https://github.com/user-attachments/assets/71b58784-5ae2-4500-8719-a89c759b8215)",
    "labels": [
      "amd"
    ],
    "state": "open",
    "created_at": "2025-04-24T08:01:36+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5705/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5705"
  },
  {
    "number": 5637,
    "title": "[Bug] After updating from 0.4.5.post2 to 0.4.5.post3, the following error is reported: Child process unexpectedly failed with an exit code 256. pid=32885.",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n[2025-04-22 21:28:53 TP0] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n[2025-04-22 21:28:53 TP0] Attention backend not set. Use triton backend by default.\n[2025-04-22 21:28:53 TP0] Init torch distributed begin.\n[W422 21:28:53.246570644 HIPAllocatorConfig.h:29] Warning: expandable_segments not supported on this platform (function operator())\n[2025-04-22 21:28:53 TP0] Init torch distributed ends. mem usage=0.00 GB\n[2025-04-22 21:28:53 TP0] Load weight begin. avail mem=18.75 GB\n[2025-04-22 21:28:53 TP0] sgl-kernel is not available on Non-NV platforms. Fallback to other kernel libraries.\n[2025-04-22 21:28:53 TP0] The following error message 'operation scheduled before its operands' can be ignored.\n/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/utils/_device.py:104: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)\n  return func(*args, **kwargs)\nLoading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  33% Completed | 1/3 [00:05<00:10,  5.28s/it]\nLoading safetensors checkpoint shards:  67% Completed | 2/3 [00:12<00:06,  6.33s/it]\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:14<00:00,  4.49s/it]\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:14<00:00,  4.88s/it]\n\n[2025-04-22 21:29:09 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.float16, avail mem=8.83 GB, mem usage=9.92 GB.\n[2025-04-22 21:29:09 TP0] KV Cache is allocated. #tokens: 4096, K size: 0.38 GB, V size: 0.38 GB\n[2025-04-22 21:29:09 TP0] Memory pool end. avail mem=7.57 GB\n[2025-04-22 21:29:09 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=7.45 GB\nCapturing batches (avail_mem=7.45 GB):   0%|                                                                | 0/4 [00:00<?, ?it/s]cat: /proc/sys/kernel/numa_balancing: No such file or directory\n:0:/longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/clr/hipamd/src/hip_global.cpp:117 : 6190051732 us:  Cannot find Symbol with name: _ZN7ck_tile6kentryILi256ELi1ENS_12Rmsnorm2dFwdINS_27Rmsnorm2dFwdPipelineOnePassINS_27Rmsnorm2dFwdPipelineProblemIDF16_DF16_fDF16_DF16_ffNS_19Generic2dBlockShapeINS_8sequenceIJLi1ELi6144EEEENS5_IJLi1ELi4EEEENS5_IJLi1ELi512EEEENS5_IJLi1ELi8EEEEEENS_18Rmsnorm2dFwdTraitsILb1ELb0ELb0ELNS_21Rmsnorm2dFusedAddEnumE0ELNS_23Rmsnorm2dFusedQuantEnumE0EEEEENS_33Rmsnorm2dFwdPipelineDefaultPolicyEEENS_17Default2DEpilogueINS_24Default2DEpilogueProblemIfDF16_Lb0ELb1ELb0EEEvEEEEJNSM_5KargsEEEEvDpT2_\nFatal Python error: Aborted\n\nThread 0x00007f3d642be640 (most recent call first):\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/threading.py\", line 324 in wait\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/threading.py\", line 607 in wait\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f3d669a3640 (most recent call first):\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/threading.py\", line 324 in wait\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/threading.py\", line 607 in wait\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f3f4b22e640 (most recent call first):\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/selectors.py\", line 469 in select\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/asyncio/base_events.py\", line 1871 in _run_once\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/asyncio/base_events.py\", line 603 in run_forever\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xoscar/backends/core.py\", line 264 in _safe_run_forever\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/threading.py\", line 953 in run\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nCurrent thread 0x00007f3f83e9f740 (most recent call first):\n  File \"/usr/local/aiter/aiter/jit/core.py\", line 368 in wrapper\n  File \"/usr/local/sglang/python/sglang/srt/layers/layernorm.py\", line 73 in forward_cuda\n  File \"/usr/local/sglang/python/sglang/srt/custom_op.py\", line 27 in forward_hip\n  File \"/usr/local/sglang/python/sglang/srt/custom_op.py\", line 18 in forward\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750 in _call_impl\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739 in _wrapped_call_impl\n  File \"/usr/local/sglang/python/sglang/srt/models/qwen2.py\", line 221 in forward\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750 in _call_impl\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739 in _wrapped_call_impl\n  File \"/usr/local/sglang/python/sglang/srt/models/qwen2.py\", line 291 in forward\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750 in _call_impl\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739 in _wrapped_call_impl\n  File \"/usr/local/sglang/python/sglang/srt/models/qwen2.py\", line 383 in forward\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n  File \"/usr/local/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 444 in run_once\n  File \"/usr/local/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 451 in capture_one_batch_size\n  File \"/usr/local/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 359 in capture\n  File \"/usr/local/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 275 in __init__\n  File \"/usr/local/sglang/python/sglang/srt/model_executor/model_runner.py\", line 980 in init_cuda_graphs\n  File \"/usr/local/sglang/python/sglang/srt/model_executor/model_runner.py\", line 219 in initialize\n  File \"/usr/local/sglang/python/sglang/srt/model_executor/model_runner.py\", line 181 in __init__\n  File \"/usr/local/sglang/python/sglang/srt/managers/tp_worker.py\", line 75 in __init__\n  File \"/usr/local/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63 in __init__\n  File \"/usr/local/sglang/python/sglang/srt/managers/scheduler.py\", line 261 in __init__\n  File \"/usr/local/sglang/python/sglang/srt/managers/scheduler.py\", line 2001 in run_scheduler_process\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/multiprocessing/process.py\", line 108 in run\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/multiprocessing/process.py\", line 314 in _bootstrap\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/multiprocessing/spawn.py\", line 129 in _main\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/multiprocessing/spawn.py\", line 116 in spawn_main\n  File \"<string>\", line 1 in <module>\n\nExtension modules: xoscar.context, xoscar.core, xoscar._utils, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, xoscar.serialization.core, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, xoscar.backends.message, psutil._psutil_linux, psutil._psutil_posix, zstandard.backend_c, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, setproctitle, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, zmq.backend.cython._zmq, yaml._yaml, markupsafe._speedups, PIL._imaging, PIL._imagingft, av._core, av.logging, av.bytesource, av.buffer, av.audio.format, av.error, av.dictionary, av.container.pyio, av.utils, av.option, av.descriptor, av.format, av.stream, av.container.streams, av.sidedata.motionvectors, av.sidedata.sidedata, av.opaque, av.packet, av.container.input, av.container.output, av.container.core, av.codec.context, av.video.format, av.video.reformatter, av.plane, av.video.plane, av.video.frame, av.video.stream, av.codec.hwaccel, av.codec.codec, av.frame, av.audio.layout, av.audio.plane, av.audio.frame, av.audio.stream, av.filter.pad, av.filter.link, av.filter.context, av.filter.graph, av.filter.filter, av.audio.resampler, av.filter.loudnorm, av.audio.codeccontext, av.audio.fifo, av.bitstream, av.video.codeccontext, hiredis.hiredis, google._upb._message, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.utils, h5py.h5t, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5r, h5py._proxy, h5py._conv, h5py.h5z, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5o, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5l, h5py._selector, jaxlib.cpu_feature_guard, kiwisolver._cext, sentencepiece._sentencepiece, msgspec._core, _cffi_backend, msgpack._cmsgpack, uvloop.loop, ray._raylet, hip_utils, regex._regex, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._direct, __triton_launcher (total: 230)\n[2025-04-22 21:29:16] Child process unexpectedly failed with an exit code 256. pid=32885\n\n### Reproduction\n\nqwen2.5-instruct\n\n### Environment\n\nPython: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]\nROCM available: True\nGPU 0: AMD Radeon RX 7900 XT\nGPU 0 Compute Capability: 11.0\nROCM_HOME: /opt/rocm\nHIPCC: HIP version: 6.4.43482-0f2d60242\nROCM Driver Version:\nPyTorch: 2.6.0+rocm6.4.0.git2fb0ac2b\nsglang: 0.4.5.post3\nsgl_kernel: 0.0.9.post2\nflashinfer: Module Not Found\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.11.0.dev20250418+rocm6.3\nnumpy: 1.26.4\naiohttp: 3.11.14\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.24.0\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 1.2.1\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.8.5.dev134+gd6da9322c.d20250422.rocm640\nxgrammar: 0.1.18\nopenai: 1.68.2\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.63.14\ndecord: 0.6.0\nAMD Topology:\n\nHypervisor vendor: Microsoft\nulimit soft: 1024",
    "labels": [
      "amd"
    ],
    "state": "open",
    "created_at": "2025-04-22T13:37:01+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5637/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5637"
  },
  {
    "number": 5414,
    "title": "[Bug] AMD ROCm system: Inference error",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen starting the service of a multi-GPU model on the ROCm platform and requesting inference, the obtained response is completely wrong.\n\nprompt=\"List 3 countries and their capitals.\"\uff1a\n\nresponse\uff1aChatCompletion(id='5379cef8260f4cc6a9272581759b199b', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='itch!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning_content=None), matched_stop=None)], created=1744698427, model='/models/llama2/Llama-2-70b-hf', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=32, prompt_tokens=11, total_tokens=43, completion_tokens_details=None, prompt_tokens_details=None))\n\n### Reproduction\n\npython3 -m sglang.launch_server --model /models/llama2/Llama-2-70b-hf/  --tp 4 --port 8001 --trust-remote-code\n\n\n### Environment\n\nplatform\uff1aAMD ROCM\nsglang\uff1a0.4.4\ntorch\uff1a2.4\nvllm\uff1a0.7.3",
    "labels": [
      "amd"
    ],
    "state": "open",
    "created_at": "2025-04-15T08:46:38+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5414/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5414"
  },
  {
    "number": 5362,
    "title": "[Bug] Support for Llama 4 on ROCm?",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHi team,\n\nI\u2019m wondering if sglang currently supports running Llama 4 models on AMD ROCm (e.g., 8xMI300X). I tried using your latest ROCm Sglang SRT Docker image. While the server can start, it produces incorrect results when using the standard endpoint with the Llama 4 Maverick model. For the FP8 Maverick version, it fails to start at all.\n\nAre there any known issues or limitations preventing Llama 4 from working properly under ROCm in sglang?\nWould be great to know if support is planned or if there are any suggested workarounds.\n\nThanks in advance!\n\n### Reproduction\n\ndocker: lmsysorg/sglang:v0.4.5-rocm630-srt\nUsing the standard Llama-4-Maverick-17B-128E-Instruct model:\nThe server starts successfully, but the output from the v1/chat/completion/ endpoints is garbled (non-English or unreadable symbols).\n\nUsing the FP8 variant of the same model (Llama-4-Maverick-17B-128E-Instruct-FP8):\nThe server fails to start, with the following error:\n\nValueError: For FP8 Fused MoE layers, only per-tensor scales for weights and activations are supported. Found num_bits=8 type='float' symmetric=True group_size=None strategy='channel' block_structure=None dynamic=False actorder=None observer='minmax' observer_kwargs={}, num_bits=8 type='float' symmetric=True group_size=None strategy='token' block_structure=None dynamic=True actorder=None observer=None observer_kwargs={}\n\nThe launch command I used was:\ndocker run --ulimit nofile=65535:65535 --ulimit nproc=65535 -it --rm \\\n  --name llama4-server \\\n  --device=/dev/kfd --device=/dev/dri --group-add video \\\n  --ipc=host --cap-add=SYS_PTRACE --security-opt seccomp=unconfined \\\n  --shm-size 256G \\\n  -v /root/storage_mi300x/1:/workspace \\\n  -v ~/.cache/huggingface:/root/.cache/huggingface \\\n  -p 30000:30000 \\\n  --entrypoint /usr/bin/python3 \\\n  lmsysorg/sglang:v0.4.5-rocm630-srt \\\n  -m sglang.launch_server \\\n  --model /workspace/Llama-4-Maverick-17B-128E-Instruct \\\n  --host 0.0.0.0 \\\n  --port 30000 \\\n  --tp 8 \\\n  --chat-template llama-4\n\n\n### Environment\n\nPython: 3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0]\nROCM available: True\nGPU 0,1,2,3,4,5,6,7: AMD Instinct MI300X\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.4\nROCM_HOME: /opt/rocm\nHIPCC: HIP version: 6.3.42131-fa1d09cbd\nROCM Driver Version: 6.10.5\nPyTorch: 2.6.0a0+git8d4926e\nsglang: 0.4.5\nsgl_kernel: 0.0.8\nflashinfer: Module Not Found\ntriton: 3.2.0+gitcddf0fc3\ntransformers: 4.51.0\ntorchao: 0.10.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.6\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.24.1\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.5\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.7.dev2+g113274a0.rocm630\nxgrammar: 0.1.17\nopenai: 1.59.7\ntiktoken: 0.7.0\nanthropic: Module Not Found\nlitellm: Module Not Found\ndecord: 0.6.0\nAMD Topology: \n\n\n============================ ROCm System Management Interface ============================\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         \nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         \nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         \nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         \nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         \nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            \n================================== End of ROCm SMI Log ===================================\n\nulimit soft: 65535",
    "labels": [
      "amd"
    ],
    "state": "closed",
    "created_at": "2025-04-14T02:33:17+00:00",
    "closed_at": "2025-07-01T22:25:55+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5362/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5362"
  },
  {
    "number": 5093,
    "title": "[Bug] SGLang on ROCm - NameError: name 'torch_memory_saver' is not defined",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n[RCOm Docker - `lmsysorg/sglang:v0.4.4.post3-rocm630-srt`]\nThe issue arises from here:\nhttps://github.com/sgl-project/sglang/blob/main/python/sglang/srt/torch_memory_saver_adapter.py#L48\n\n1. In line 6, if the code fails to import torch_memory_saver, it just bypasses instead of triggering any error. Thus, if the code calls line46 class and uses def configure_subprocess(self), it cannot find `torch_memory_saver` , thereby triggering NameError: name 'torch_memory_saver' is not defined error on later.\n2. `torch_memory_saver`  should be supported in this docker (`lmsysorg/sglang:v0.4.4.post3-rocm630-srt`)\n\n### Reproduction\n\nSee above\n\n### Environment\n\nYou can try in this (latest) docker \n```bash\ndocker run --rm -it \\\n  --device /dev/dri \\\n  --device /dev/kfd \\\n  -p 8265:8265 \\\n  --group-add video \\\n  --cap-add SYS_PTRACE \\\n  --security-opt seccomp=unconfined \\\n  --privileged \\\n  -v $HOME/.ssh:/root/.ssh \\\n  -v $HOME:$HOME \\\n  --shm-size 128G \\\n  --name sglang_rocm_test \\\n  -w $PWD \\\n  lmsysorg/sglang:v0.4.4.post3-rocm630-srt \\\n  /bin/bash\n```",
    "labels": [
      "high priority",
      "inactive",
      "amd",
      "RLHF"
    ],
    "state": "closed",
    "created_at": "2025-04-05T23:11:55+00:00",
    "closed_at": "2025-06-08T00:21:35+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5093/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5093"
  },
  {
    "number": 3965,
    "title": "Question sgl_kernel on amd paltforms",
    "body": "Hey,\n\nI get the following warning when running on mi300:\n\n```\nsgl-kernel is not available on Non-NV platforms. Fallback to other kernel libraries.\n```\n\nEven though I compiled sgl_kernel and that is what the doc says https://docs.sglang.ai/start/install.html#method-2-from-source.\n\nIn the code, both sglang/python/sglang/srt/layers/activation.py and sglang/python/sglang/srt/layers/layernorm.py are gated behind\n```\nis_cuda_available()\n```\n\nDoe this applies only to layernorm.py and activations.py ? then the message may not be explicit enough, leading me to believe I didnt even built sgl kernel.",
    "labels": [
      "inactive",
      "amd"
    ],
    "state": "closed",
    "created_at": "2025-02-28T16:13:40+00:00",
    "closed_at": "2025-05-04T00:21:06+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3965/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3965"
  },
  {
    "number": 3823,
    "title": "[Feature] GPU inference on AMD Ryzen AI (370HX-890M) iGPU + NPU",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nRyzen AI devices have been out since mid 2024 yet there's no end user friendly local inference engine that can use the iGPU or the NPU for inference. Some people seem to be able to make it working using hacks but it's still a hit or miss and you need to build your own custom room and hip packages to it to kind of work. \n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive",
      "feature",
      "amd"
    ],
    "state": "closed",
    "created_at": "2025-02-24T15:49:18+00:00",
    "closed_at": "2025-04-26T00:17:55+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3823/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3823"
  },
  {
    "number": 3755,
    "title": "[Bug] fix amd runner offline",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n![Image](https://github.com/user-attachments/assets/1fa39f19-18ce-4020-a793-b5f1a3647c4b)\n\n![Image](https://github.com/user-attachments/assets/2be670db-2e32-4f9c-8837-106f8aa5d200)\n\n@saienduri May you help fix it? Thanks!\n\n### Reproduction\n\nN/A\n\n### Environment\n\nN/A",
    "labels": [
      "high priority",
      "amd"
    ],
    "state": "closed",
    "created_at": "2025-02-21T09:58:30+00:00",
    "closed_at": "2025-03-13T21:10:31+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3755/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/3755"
  },
  {
    "number": 3479,
    "title": "[Feature] is flex attention compatible?",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\ncontext:\nIn the future, we plan on extending this support to allow for quantized versions of attention or things like [RadixAttention](https://lmsys.org/blog/2024-01-17-sglang/) as well.\n\nhttps://pytorch.org/blog/flexattention/\n\n### Related resources\n\n_No response_",
    "labels": [
      "help wanted",
      "inactive",
      "amd"
    ],
    "state": "closed",
    "created_at": "2025-02-11T07:30:03+00:00",
    "closed_at": "2025-04-13T00:43:12+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3479/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3479"
  },
  {
    "number": 3400,
    "title": "[Bug] 4x8 Mi210 Deepseek V3 runtime error",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI tried to start 4 nodes with 32 Mi210 GPUs to launch Deepseek R1 using sglang and I've already converted the weights to bf16.\n\n``` bash\n[2025-02-08 18:30:47] INFO:     Started server process [445197]\n[2025-02-08 18:30:47] INFO:     Waiting for application startup.\n[2025-02-08 18:30:47] INFO:     Application startup complete.\n[2025-02-08 18:30:47] INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\n[2025-02-08 18:30:48] INFO:     127.0.0.1:40178 - \"GET /get_model_info HTTP/1.1\" 200 OK\n[2025-02-08 18:30:48 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n[2025-02-08 18:31:03 TP2] TpModelWorkerClient hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 104, in forward_thread_func\n    self.forward_thread_func_()\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 135, in forward_thread_func_\n    logits_output, next_token_ids = self.worker.forward_batch_generation(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 162, in forward_batch_generation\n    logits_output = self.model_runner.forward(forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 717, in forward\n    return self.forward_extend(forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 682, in forward_extend\n    return self.model.forward(\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 857, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch)\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 818, in forward\n    hidden_states, residual = layer(\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 756, in forward\n    hidden_states = self.self_attn(\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 513, in forward\n    return self.forward_normal(positions, hidden_states, forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 554, in forward_normal\n    attn_output = self.attn_mha(q, k, v, forward_batch, save_kv_cache=False)\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/radix_attention.py\", line 65, in forward\n    return forward_batch.attn_backend.forward(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/__init__.py\", line 69, in forward\n    return self.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_backend.py\", line 138, in forward_extend\n    self.extend_attention_fwd(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/extend_attention.py\", line 328, in extend_attention_fwd\n    _fwd_kernel[grid](\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 330, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 687, in run\n    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/backends/amd/driver.py\", line 477, in __call__\n    self.launch(*args, **kwargs)\nRuntimeError: Triton Error [HIP]:  Code: 1, Messsage: invalid argument\n\nKilled\n```\n\n\n### Reproduction\n\n```python\npython3 -m sglang.launch_server --model-path /root/DeepSeek-R1-BF16 --tp 32 --dist-init-addr 10.204.8.46:20000 --nnodes 4 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 40000 --trust-remote-code --disable-cuda-graph\n\npython3 -m sglang.launch_server --model-path /root/DeepSeek-R1-BF16 --tp 32 --dist-init-addr 10.204.8.46:20000 --nnodes 4 --node-rank 1 --trust-remote-code --host 0.0.0.0 --port 40000 --trust-remote-code --disable-cuda-graph\n\npython3 -m sglang.launch_server --model-path /root/DeepSeek-R1-BF16 --tp 32 --dist-init-addr 10.204.8.46:20000 --nnodes 4 --node-rank 2 --trust-remote-code --host 0.0.0.0 --port 40000 --trust-remote-code --disable-cuda-graph\n\npython3 -m sglang.launch_server --model-path /root/DeepSeek-R1-BF16 --tp 32 --dist-init-addr 10.204.8.46:20000 --nnodes 4 --node-rank 3 --trust-remote-code --host 0.0.0.0 --port 40000 --trust-remote-code --disable-cuda-graph\n```\n\n### Environment\n\nI am using lmsysorg/sglang:v0.4.1.post4-rocm620 docker \n\n### Update\nafter changing image to lmsysorg/sglang:v0.4.2.post3-rocm630, it no longer failed at MLA but encountered IMA at MOE.\n\n",
    "labels": [
      "inactive",
      "amd",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-08T11:17:41+00:00",
    "closed_at": "2025-04-10T00:18:03+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3400/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3400"
  },
  {
    "number": 3392,
    "title": "[Bug] AttributeError: module 'vllm._custom_ops' has no attribute 'silu_and_mul'",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHell folks,\n\n I'm attempting to deploy DeepSeek-R1 with SGLang on an AMD MI300X, but I'm encountering compatibility issues. \nCould someone please help me troubleshoot these issues?\n\n\n### Reproduction\n\n1. build and install **triton 3.0.0** from source\n2. build and install **vllm v0.7.2** from source\n3. build and install **sglang** (rev 0a6f18f068e4095fc228e798454e8496c9749214) from source\n4. run `python3 -m sglang.launch_server --host 0.0.0.0 --port 30000 \\\n--model-path ~/deepseek/DeepSeek-R1/ \\\n--tp 8 --trust-remote-code \\\n--mem-fraction-static 0.70 \\\n--served-model-name \"DeepSeek-R1\" \\\n--log-level debug \\\n--log-level-http debug \\\n--log-requests \\\n--enable-metrics \\\n--show-time-cost`\n\nThen I got this error:\n```\n[2025-02-08 05:57:37 TP6] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/home/deepseek/sglang/python/sglang/srt/managers/scheduler.py\", line 1787, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/deepseek/sglang/python/sglang/srt/managers/scheduler.py\", line 240, in __init__\n    self.tp_worker = TpWorkerClass(\n                     ^^^^^^^^^^^^^^\nFile \"/home/deepseek/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__                                                                                                       [604/1865]\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/deepseek/sglang/python/sglang/srt/managers/tp_worker.py\", line 68, in __init__\n    self.model_runner = ModelRunner(\n                        ^^^^^^^^^^^^\n  File \"/home/deepseek/sglang/python/sglang/srt/model_executor/model_runner.py\", line 215, in __init__\n    self.init_cuda_graphs()\n  File \"/home/deepseek/sglang/python/sglang/srt/model_executor/model_runner.py\", line 730, in init_cuda_graphs\n    self.cuda_graph_runner = CudaGraphRunner(self)\n                             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/deepseek/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 232, in __init__\n    self.capture()\n  File \"/home/deepseek/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 298, in capture\n    ) = self.capture_one_batch_size(bs, forward)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/deepseek/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 375, in capture_one_batch_size\n    run_once()\n  File \"/home/deepseek/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 368, in run_once\n    logits_output = forward(input_ids, forward_batch.positions, forward_batch)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/.conda/envs/py312B/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/deepseek/sglang/python/sglang/srt/models/deepseek_v2.py\", line 858, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/.conda/envs/py312B/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/.conda/envs/py312B/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)                                                                                                                                                                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/deepseek/sglang/python/sglang/srt/models/deepseek_v2.py\", line 819, in forward\n    hidden_states, residual = layer(\n                              ^^^^^^\n  File \"/home/.conda/envs/py312B/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/.conda/envs/py312B/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/deepseek/sglang/python/sglang/srt/models/deepseek_v2.py\", line 774, in forward\n    hidden_states = self.mlp(hidden_states)\n                    ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/.conda/envs/py312B/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/.conda/envs/py312B/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/deepseek/sglang/python/sglang/srt/models/deepseek_v2.py\", line 177, in forward\n    self.experts(hidden_states=hidden_states, router_logits=router_logits)\n  File \"/home/.conda/envs/py312B/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/.conda/envs/py312B/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/deepseek/sglang/python/sglang/srt/layers/moe/fused_moe_triton/layer.py\", line 587, in forward\n    final_hidden_states = self.quant_method.apply(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/deepseek/sglang/python/sglang/srt/layers/quantization/fp8.py\", line 820, in apply\n    return fused_experts(\n           ^^^^^^^^^^^^^^\n  File \"/home/deepseek/sglang/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\", line 835, in fused_experts\n    torch.ops.sglang.inplace_fused_experts(\n  File \"/home/.conda/envs/py312B/lib/python3.12/site-packages/torch/_ops.py\", line 1123, in __call__\n    return self._op(*args, **(kwargs or {}))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/deepseek/sglang/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\", line 715, in inplace_fused_experts\n    fused_experts_impl(\n  File \"/home/deepseek/sglang/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\", line 992, in fused_experts_impl\n    ops.silu_and_mul(intermediate_cache2, intermediate_cache1.view(-1, N))\n    ^^^^^^^^^^^^^^^^\nAttributeError: module 'vllm._custom_ops' has no attribute '**silu_and_mul**'\n```\n\n\n\n### Environment\n\n> Python: 3.12.8 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:31:09) [GCC 11.2.0]\n> ROCM available: True\n> GPU 0,1,2,3,4,5,6,7: AMD Instinct MI300X VF\n> GPU 0,1,2,3,4,5,6,7 Compute Capability: 9.4\n> ROCM_HOME: /opt/rocm-6.1.0\n> HIPCC: HIP version: 6.1.40091-a8dbc0c19\n> ROCM Driver Version: 6.8.1\n> PyTorch: 2.6.0+rocm6.1\n> sglang: 0.4.2.post3\n> flashinfer: 0.2.0.post2\n> triton: 3.0.0\n> transformers: 4.48.3\n> torchao: 0.8.0\n> numpy: 1.26.4\n> aiohttp: 3.11.12\n> fastapi: 0.115.8\n> hf_transfer: 0.1.9\n> huggingface_hub: 0.28.1\n> interegular: 0.3.3\n> modelscope: 1.22.3\n> orjson: 3.10.15\n> packaging: 24.2\n> psutil: 6.1.1\n> pydantic: 2.10.6\n> multipart: 0.0.20\n> zmq: 26.2.1\n> uvicorn: 0.34.0\n> uvloop: 0.21.0\n> vllm: 0.7.2\n> openai: 1.61.1\n> anthropic: 0.45.2\n> decord: 0.6.0\n> AMD Topology:\n> \n> \n> ============================ ROCm System Management Interface ============================\n> =============================== Link Type between two GPUs ===============================\n>        GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\n> GPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\n> GPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\n> GPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI\n> GPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI\n> GPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI\n> GPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI\n> GPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI\n> GPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0\n> ================================== End of ROCm SMI Log ===================================\n> \n> ",
    "labels": [
      "inactive",
      "amd",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-08T06:32:39+00:00",
    "closed_at": "2025-05-03T00:18:16+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3392/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3392"
  },
  {
    "number": 3245,
    "title": "[Docs] Add docs for running SGLang on AMD",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nThat has long been waiting, we should add a docs on how to run SGLang on AMD devices.\n\nhttps://github.com/sgl-project/sglang/issues/3219\nhttps://github.com/sgl-project/sglang/issues/3243\nhttps://github.com/sgl-project/sglang/issues/3200\nhttps://github.com/sgl-project/sglang/pull/3208\nhttps://github.com/sgl-project/sglang/issues/3198\n\nHere is something related. To me, I think we should add a docs on how to:\n \n1. configure environment in AMD GPU;\n2. how to install sglang;\n3. how to run a llama model;\n4. how to run deepseek V3 models.\n\n### Related resources\n\n_No response_",
    "labels": [
      "documentation",
      "good first issue",
      "help wanted",
      "amd"
    ],
    "state": "closed",
    "created_at": "2025-02-01T00:23:16+00:00",
    "closed_at": "2025-05-21T15:40:21+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3245/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3245"
  },
  {
    "number": 3243,
    "title": "[Feature] Instructions for running Sglang on AMD RX 7900 XTX (gfx1100) ROCm 6.2.4",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nHello, \n\nIf anyone is interested, here's how I run SGlang on the AMD RX 7900 XTX (gfx1100) with ROCm 6.2.4.  Currently, the attention backend is based on Triton. It seems that flashInfer support is under development. Hope it helps.\n\nCreate a Dockerfile, which is based on the vLLM ROCm dockerfile:\n\n```\n# default base image\nARG REMOTE_VLLM=\"0\"\nARG USE_CYTHON=\"0\"\nARG BUILD_RPD=\"1\"\nARG COMMON_WORKDIR=/app\nARG BASE_IMAGE=rocm/vllm-dev:base\n\nFROM ${BASE_IMAGE} AS base\n\nARG ARG_PYTORCH_ROCM_ARCH\nENV PYTORCH_ROCM_ARCH=${ARG_PYTORCH_ROCM_ARCH:-${PYTORCH_ROCM_ARCH}}\n\n# Install some basic utilities\nRUN apt-get update -q -y && apt-get install -q -y \\\n    sqlite3 libsqlite3-dev libfmt-dev libmsgpack-dev libsuitesparse-dev\n# Remove sccache\nRUN python3 -m pip install --upgrade pip && pip install setuptools_scm\nRUN apt-get purge -y sccache; python3 -m pip uninstall -y sccache; rm -f \"$(which sccache)\"\nARG COMMON_WORKDIR\nWORKDIR ${COMMON_WORKDIR}\n\n# -----------------------\n# vLLM fetch stages\nFROM base AS fetch_vllm_0\nONBUILD COPY ./ vllm/\nFROM base AS fetch_vllm_1\nARG VLLM_REPO=\"https://github.com/vllm-project/vllm.git\"\nARG VLLM_BRANCH=\"main\"\nONBUILD RUN git clone ${VLLM_REPO} \\\n        && cd vllm \\\n        && git checkout ${VLLM_BRANCH}\nFROM fetch_vllm_${REMOTE_VLLM} AS fetch_vllm\n\n# -----------------------\n# vLLM build stages\nFROM fetch_vllm AS build_vllm\nARG USE_CYTHON\n# Build vLLM\nRUN cd vllm \\\n    && python3 -m pip install -r requirements-rocm.txt \\\n    && python3 setup.py clean --all  \\\n    && if [ ${USE_CYTHON} -eq \"1\" ]; then python3 setup_cython.py build_ext --inplace; fi \\\n    && python3 setup.py bdist_wheel --dist-dir=dist\nFROM scratch AS export_vllm\nARG COMMON_WORKDIR\nCOPY --from=build_vllm ${COMMON_WORKDIR}/vllm/dist/*.whl /\nCOPY --from=build_vllm ${COMMON_WORKDIR}/vllm/requirements*.txt /\nCOPY --from=build_vllm ${COMMON_WORKDIR}/vllm/benchmarks /benchmarks\nCOPY --from=build_vllm ${COMMON_WORKDIR}/vllm/tests /tests\nCOPY --from=build_vllm ${COMMON_WORKDIR}/vllm/examples /examples\nCOPY --from=build_vllm ${COMMON_WORKDIR}/vllm/.buildkite /.buildkite\n\n# -----------------------\n# Test vLLM image\nFROM base AS test\n\nRUN python3 -m pip install --upgrade pip && rm -rf /var/lib/apt/lists/*\n\n# Install vLLM\nRUN --mount=type=bind,from=export_vllm,src=/,target=/install \\\n    cd /install \\\n    && pip install -U -r requirements-rocm.txt \\\n    && pip uninstall -y vllm \\\n    && pip install *.whl\n\nWORKDIR /vllm-workspace\nARG COMMON_WORKDIR\nCOPY --from=build_vllm ${COMMON_WORKDIR}/vllm /vllm-workspace\n\n# install development dependencies (for testing)\nRUN cd /vllm-workspace \\\n    && rm -rf vllm \\\n    && python3 -m pip install -e tests/vllm_test_utils \\\n    && python3 -m pip install lm-eval[api]==0.4.4 \\\n    && python3 -m pip install pytest-shard\n\n# -----------------------\n# Final vLLM image\nFROM base AS final\n\nRUN python3 -m pip install --upgrade pip && rm -rf /var/lib/apt/lists/*\n# Error related to odd state for numpy 1.20.3 where there is no METADATA etc, but an extra LICENSES_bundled.txt.\n# Manually remove it so that later steps of numpy upgrade can continue\nRUN case \"$(which python3)\" in \\\n        *\"/opt/conda/envs/py_3.9\"*) \\\n            rm -rf /opt/conda/envs/py_3.9/lib/python3.9/site-packages/numpy-1.20.3.dist-info/;; \\\n        *) ;; esac\n\nRUN python3 -m pip install --upgrade huggingface-hub[cli]\nARG BUILD_RPD\nRUN if [ ${BUILD_RPD} -eq \"1\" ]; then \\\n    git clone -b nvtx_enabled https://github.com/ROCm/rocmProfileData.git \\\n    && cd rocmProfileData/rpd_tracer \\\n    && pip install -r requirements.txt && cd ../ \\\n    && make && make install \\\n    && cd hipMarker && python3 setup.py install ; fi\n\n# Install vLLM\nRUN --mount=type=bind,from=export_vllm,src=/,target=/install \\\n    cd /install \\\n    && pip install -U -r requirements-rocm.txt \\\n    && pip uninstall -y vllm \\\n    && pip install *.whl\n\nARG COMMON_WORKDIR\n\n# Copy over the benchmark scripts as well\nCOPY --from=export_vllm /benchmarks ${COMMON_WORKDIR}/vllm/benchmarks\nCOPY --from=export_vllm /examples ${COMMON_WORKDIR}/vllm/examples\n\n# Install SGLang\nRUN git clone https://github.com/sgl-project/sglang.git /app/sglang \\\n    &&  sed -i '/vllm==0.6.4.post1/d; /flashinfer==0.1.6/d' /app/sglang/python/pyproject.toml \\\n    && cd /app/sglang \\\n    && python3 -m pip --no-cache-dir install -e \"python[all]\"\n\nENV RAY_EXPERIMENTAL_NOSET_ROCR_VISIBLE_DEVICES=1\nENV TOKENIZERS_PARALLELISM=false\n\n# Performance environment variable.\nENV HIP_FORCE_DEV_KERNARG=1\n\nCMD [\"/bin/bash\"]\n```\n\nBuild using:\n\n`DOCKER_BUILDKIT=1 docker build --build-arg BASE_IMAGE=\"rocm/vllm-dev:navi_base\" -f Dockerfile.rocm_new -t sglang-rocm .`\n\nRun the container:\n\n\n```\ndocker run -it \\\n    --network=host \\\n    --group-add=video \\\n    --ipc=host \\\n    --cap-add=SYS_PTRACE \\\n    --security-opt seccomp=unconfined \\\n    --device /dev/kfd \\\n    --device /dev/dri \\\n    -v ./models/:/root/.cache/huggingface \\\n    sglang-rocm:latest \\\n    bash\n```\n\nRun inside of container:\n\n```\npython -m sglang.launch_server \\\n    --model-path meta-llama/Llama-3.1-8B-Instruct \\\n    --attention-backend triton\n```\n\nSend a request using the Python code below:\n\n```\nimport openai\n\nclient = openai.Client(base_url=\"http://127.0.0.1:30000/v1\", api_key=\"None\")\n\nresponse = client.chat.completions.create(\n    model=\"meta-llama/Llama-3.1-8B\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Introduce yourself\"},\n    ],\n    temperature=0,\n    max_tokens=500,\n    stream=True  # Enable streaming\n)\n\nfor chunk in response:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n### Related resources\n\n_No response_",
    "labels": [
      "documentation",
      "inactive",
      "amd"
    ],
    "state": "closed",
    "created_at": "2025-01-31T20:01:43+00:00",
    "closed_at": "2025-05-14T00:19:11+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3243/reactions",
      "total_count": 5,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 5,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3243"
  },
  {
    "number": 3219,
    "title": "[Bug] [ROCm] Running DeepSeek V3 on MI300X, getting \"Config not found, Performance might be sub-optimal\" error",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI am running DeepSeek v3 on a node with 8xMI300X GPUs on ROCm 6.3.1. I am able to run it using an image built from `Dockerfile.rocm` in `docker`, however I have noticed this warning show up:\n```\nUsing default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at <multiple config files>\n```\nIn the container built from `Dockerfile.rocm`, with SGLang v0.4.2, these are the missing config files:\n```\n/sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=24576,K=1536,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128, 128].json\n/sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=7168,K=16384,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128, 128].json\n/sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=32768,K=512,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128, 128].json\n```\nI also tried the Docker image `lmsysorg/sglang:v0.4.1.post4-rocm620`, based on [this blog from AMD](https://www.amd.com/en/developer/resources/technical-articles/amd-instinct-gpus-power-deepseek-v3-revolutionizing-ai-development-with-sglang.html). This had SGLang v0.4.1, and was missing the following config files:\n```\n/sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=1536,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128, 128].json\n/sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=3072,K=1536,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128, 128].json\n/sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=576,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128, 128].json\n/sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=7168,K=2048,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128, 128].json\n/sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=4608,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128, 128].json\n/sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=7168,K=2304,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128, 128].json\n/sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=512,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128, 128].json\n/sgl-workspace/sglang/python/sglang/srt/layers/quantization/configs/N=7168,K=256,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128, 128].json\n/sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=256,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128, 128].json\n```\n\n\n### Reproduction\n\nReproduction steps:\nI launched the container with the command:\n```\ndocker run -it --network=host \\\n    --group-add=video \\\n    --ipc=host \\\n    --cap-add=SYS_PTRACE \\\n    --security-opt seccomp=unconfined \\\n    --device /dev/kfd \\\n    --device /dev/dri \\\n    --shm-size 16G \\\n    -p 8080:8080 \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    v0.4.2-rocm620:latest\n```\nAnd I ran the server using the command\n```\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code --port 8080\n```\n\n### Environment\n\nROCm 6.3.1\n8xMI300X\n\nDocker images:\n - `v0.4.2-rocm620:latest` built from `docker/Dockerfile.rocm` and build instructions from [SGLang docs](https://docs.sglang.ai/start/install.html)\n - `lmsysorg/sglang:v0.4.1.post4-rocm620`",
    "labels": [
      "help wanted",
      "inactive",
      "amd"
    ],
    "state": "closed",
    "created_at": "2025-01-30T19:34:09+00:00",
    "closed_at": "2025-04-12T00:17:45+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3219/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3219"
  },
  {
    "number": 3200,
    "title": "[Bug] Tried to run DeepSeek V3 by amd instructions",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI tried to use [AMD instruction](https://www.amd.com/en/developer/resources/technical-articles/amd-instinct-gpus-power-deepseek-v3-revolutionizing-ai-development-with-sglang.html) but i have an error.\n\n### Reproduction\n\nAfter running in a container\n```\npython3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --port 30000 --tp 8 --trust-remote-code\n```\n\nLog:\n```\n/opt/conda/envs/py_3.9/lib/python3.9/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4)\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n[2025-01-28 22:32:01 TP6] Process 97 gpu_id 6 is running on CPUs: [6, 14]\n[2025-01-28 22:32:01 TP2] Process 63 gpu_id 2 is running on CPUs: [2, 10]\n[2025-01-28 22:32:01 TP7] Process 113 gpu_id 7 is running on CPUs: [7, 15]\n[2025-01-28 22:32:02 TP5] Process 66 gpu_id 5 is running on CPUs: [5, 13]\n[2025-01-28 22:32:02 TP4] Process 65 gpu_id 4 is running on CPUs: [4, 12]\n[2025-01-28 22:32:02 TP3] Process 64 gpu_id 3 is running on CPUs: [3, 11]\n[2025-01-28 22:32:02 TP1] Process 62 gpu_id 1 is running on CPUs: [1, 9]\n[2025-01-28 22:32:03 TP0] Process 61 gpu_id 0 is running on CPUs: [0, 8]\n[2025-01-28 22:32:03 TP2] MLA optimization is turned on. Use triton backend.\n[2025-01-28 22:32:03 TP2] Init torch distributed begin.\n[2025-01-28 22:32:03 TP2] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1609, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 203, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 68, in __init__\n    self.model_runner = ModelRunner(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 159, in __init__\n    min_per_gpu_memory = self.init_torch_distributed()\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 197, in init_torch_distributed\n    torch.get_device_module(self.device).set_device(self.gpu_id)\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n    torch._C._cuda_setDevice(device)\nRuntimeError: HIP error: invalid device ordinal\nHIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing AMD_SERIALIZE_KERNEL=3\nCompile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\n\n\nKilled\n```\n\n### Environment\n\n```\nPython: 3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]\nROCM available: True\nGPU 0: AMD Radeon RX 6800 XT\nGPU 0 Compute Capability: 10.3\nROCM_HOME: /opt/rocm\nHIPCC: HIP version: 6.2.41133-dd7f95766\nROCM Driver Version: 6.12.10-zen1-1-zen\nPyTorch: 2.5.0a0+gitcedc116\nsglang: 0.4.1.post4\nflashinfer: Module Not Found\ntriton: 3.0.0\ntransformers: 4.46.1\ntorchao: 0.7.0\nnumpy: 1.26.4\naiohttp: 3.10.10\nfastapi: 0.115.4\nhf_transfer: 0.1.8\nhuggingface_hub: 0.26.2\ninteregular: 0.3.3\nmodelscope: 1.21.1\norjson: 3.10.13\npackaging: 24.1\npsutil: 6.1.0\npydantic: 2.9.2\nmultipart: 0.0.20\nzmq: 26.2.0\nuvicorn: 0.32.0\nuvloop: 0.21.0\nvllm: 0.6.3.post2.dev1+g1ef171e0\nopenai: 1.59.3\nanthropic: 0.42.0\ndecord: 0.6.0\nAMD Topology: \n\n\n============================ ROCm System Management Interface ============================\n=============================== Link Type between two GPUs ===============================\n       GPU0         \nGPU0   0            \n================================== End of ROCm SMI Log ===================================\n\nulimit soft: 1024\n```",
    "labels": [
      "documentation",
      "help wanted",
      "inactive",
      "amd"
    ],
    "state": "closed",
    "created_at": "2025-01-28T22:33:58+00:00",
    "closed_at": "2025-04-03T00:17:38+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3200/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3200"
  },
  {
    "number": 3189,
    "title": "[Bug] ERROR: No matching distribution found for vllm==0.6.3.post2.dev1; extra == \"srt-hip\"",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nTrying to install sglang for AMD, but hitting this issue.  I was following this: https://docs.sglang.ai/start/install.html#method-2-from-source\n\nHappens with latest sglang or the version 0.4.2 specified in the instructions.\n\n```\nbase) root@6e2c9e6215c7:/# conda activate sglang\n(sglang) root@6e2c9e6215c7:/# conda install python=3.10 -y\nChannels:\n - defaults\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/conda/envs/sglang\n\n  added / updated specs:\n    - python=3.10\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    bzip2-1.0.8                |       h5eee18b_6         262 KB\n    ca-certificates-2024.12.31 |       h06a4308_0         128 KB\n    ld_impl_linux-64-2.40      |       h12ee557_0         710 KB\n    libffi-3.4.4               |       h6a678d5_1         141 KB\n    openssl-3.0.15             |       h5eee18b_0         5.2 MB\n    pip-24.2                   |  py310h06a4308_0         2.3 MB\n    python-3.10.16             |       he870216_1        26.9 MB\n    setuptools-75.1.0          |  py310h06a4308_0         1.7 MB\n    sqlite-3.45.3              |       h5eee18b_0         1.2 MB\n    tk-8.6.14                  |       h39e8969_0         3.4 MB\n    tzdata-2025a               |       h04d1e81_0         117 KB\n    wheel-0.44.0               |  py310h06a4308_0         109 KB\n    xz-5.4.6                   |       h5eee18b_1         643 KB\n    zlib-1.2.13                |       h5eee18b_1         111 KB\n    ------------------------------------------------------------\n                                           Total:        42.9 MB\n\nThe following NEW packages will be INSTALLED:\n\n  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu\n  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h5eee18b_6\n  ca-certificates    pkgs/main/linux-64::ca-certificates-2024.12.31-h06a4308_0\n  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.40-h12ee557_0\n  libffi             pkgs/main/linux-64::libffi-3.4.4-h6a678d5_1\n  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1\n  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1\n  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1\n  libuuid            pkgs/main/linux-64::libuuid-1.41.5-h5eee18b_0\n  ncurses            pkgs/main/linux-64::ncurses-6.4-h6a678d5_0\n  openssl            pkgs/main/linux-64::openssl-3.0.15-h5eee18b_0\n  pip                pkgs/main/linux-64::pip-24.2-py310h06a4308_0\n  python             pkgs/main/linux-64::python-3.10.16-he870216_1\n  readline           pkgs/main/linux-64::readline-8.2-h5eee18b_0\n  setuptools         pkgs/main/linux-64::setuptools-75.1.0-py310h06a4308_0\n  sqlite             pkgs/main/linux-64::sqlite-3.45.3-h5eee18b_0\n  tk                 pkgs/main/linux-64::tk-8.6.14-h39e8969_0\n  tzdata             pkgs/main/noarch::tzdata-2025a-h04d1e81_0\n  wheel              pkgs/main/linux-64::wheel-0.44.0-py310h06a4308_0\n  xz                 pkgs/main/linux-64::xz-5.4.6-h5eee18b_1\n  zlib               pkgs/main/linux-64::zlib-1.2.13-h5eee18b_1\n\n\n\nDownloading and Extracting Packages:\nPreparing transaction: done\nVerifying transaction: done\n\n(sglang) root@6e2c9e6215c7:/# git clone  https://github.com/sgl-project/sglang.git\nfatal: destination path 'sglang' already exists and is not an empty directory.\n(sglang) root@6e2c9e6215c7:/# cd sglang\n(sglang) root@6e2c9e6215c7:/sglang# pip install --upgrade pip\nRequirement already satisfied: pip in /opt/conda/envs/sglang/lib/python3.10/site-packages (24.2)\nCollecting pip\n  Using cached pip-25.0-py3-none-any.whl.metadata (3.7 kB)\nUsing cached pip-25.0-py3-none-any.whl (1.8 MB)\nInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.2\n    Uninstalling pip-24.2:\n      Successfully uninstalled pip-24.2\nSuccessfully installed pip-25.0\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv.\n Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n(sglang) root@6e2c9e6215c7:/sglang# pip install -e \"python[all_hip]\"\nObtaining file:///sglang/python\n  Installing build dependencies ... done\n  Checking if build backend supports build_editable ... done\n  Getting requirements to build editable ... done\n  Preparing editable metadata (pyproject.toml) ... done\nCollecting requests (from sglang==0.4.2)\n  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\nCollecting tqdm (from sglang==0.4.2)\n  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nCollecting numpy (from sglang==0.4.2)\n  Downloading numpy-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\nCollecting IPython (from sglang==0.4.2)\n  Downloading ipython-8.31.0-py3-none-any.whl.metadata (4.9 kB)\nCollecting setproctitle (from sglang==0.4.2)\n  Using cached setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nCollecting decorator (from IPython->sglang==0.4.2)\n  Downloading decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\nCollecting exceptiongroup (from IPython->sglang==0.4.2)\n  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\nCollecting jedi>=0.16 (from IPython->sglang==0.4.2)\n  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\nCollecting matplotlib-inline (from IPython->sglang==0.4.2)\n  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\nCollecting pexpect>4.3 (from IPython->sglang==0.4.2)\n  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\nCollecting prompt_toolkit<3.1.0,>=3.0.41 (from IPython->sglang==0.4.2)\n  Downloading prompt_toolkit-3.0.50-py3-none-any.whl.metadata (6.6 kB)\nCollecting pygments>=2.4.0 (from IPython->sglang==0.4.2)\n  Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\nCollecting stack_data (from IPython->sglang==0.4.2)\n  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\nCollecting traitlets>=5.13.0 (from IPython->sglang==0.4.2)\n  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\nCollecting typing_extensions>=4.6 (from IPython->sglang==0.4.2)\n  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\nCollecting charset-normalizer<4,>=2 (from requests->sglang==0.4.2)\n  Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\nCollecting idna<4,>=2.5 (from requests->sglang==0.4.2)\n  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\nCollecting urllib3<3,>=1.21.1 (from requests->sglang==0.4.2)\n  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\nCollecting certifi>=2017.4.17 (from requests->sglang==0.4.2)\n  Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\nINFO: pip is looking at multiple versions of sglang[srt-hip] to determine which version is compatible with other requirements. This could take a while.\nERROR: Ignored the following yanked versions: 0.2.1\nERROR: Could not find a version that satisfies the requirement vllm==0.6.3.post2.dev1; extra == \"srt-hip\" (from sglang[srt-hip]) (from versions: 0.0.1, 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.2.0, 0.2.1.post1, 0.2.2, 0.2.3, 0.2.4, 0.2.5, 0.2.6, 0.2.7, 0.3.0, 0.3.1, 0.3.2, 0.3.3, 0.4.0, 0.4.0.post1, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.5.0.post1, 0.5.1, 0.5.2, 0.5.3, 0.5.3.post1, 0.5.4, 0.5.5, 0.6.0, 0.6.1, 0.6.1.post1, 0.6.1.post2, 0.6.2, 0.6.3, 0.6.3.post1, 0.6.4, 0.6.4.post1, 0.6.5, 0.6.6, 0.6.6.post1, 0.7.0)\nERROR: No matching distribution found for vllm==0.6.3.post2.dev1; extra == \"srt-hip\"\n(sglang) root@6e2c9e6215c7:/sglang#\n```\n\n### Reproduction\n\nSee above\n\n### Environment\n\nrunpod AMD 8*mi300",
    "labels": [
      "inactive",
      "amd"
    ],
    "state": "closed",
    "created_at": "2025-01-28T01:43:00+00:00",
    "closed_at": "2025-05-03T00:18:15+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3189/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3189"
  },
  {
    "number": 2921,
    "title": "Warning while running Deepseek-V3",
    "body": "Hi, we are trying to run DeepSeek-V3 using SGLang, on both A100 and MI300x GPUs.\n\nWe receive these warnings on both systems-\n\n<img width=\"1917\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/086f5324-30a5-439a-b9e1-1ec22a0045a9\" />\n\nAre there any specific flags that need to be enabled for deepseek?\n\nBackground:\nWe have followed the steps in the documentation and are unable to increase throughput beyond 10 tokens/second on either system.. tried everything.\nWe are running like so-\n`python3 -m sglang.launch_server --model-path ~/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-V3/snapshots/4c1f24cc10a2a1894304c7ab52edd9710c047571/ --port 8000 --tp 8 --context-length 122880 --trust-remote-code`\n\n",
    "labels": [
      "amd"
    ],
    "state": "closed",
    "created_at": "2025-01-16T11:59:39+00:00",
    "closed_at": "2025-02-06T13:24:17+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2921/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2921"
  },
  {
    "number": 2675,
    "title": "[Bug] The performance of v0.4.1 on AMD GPU is lower than v0.4.0",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [X] 2. The bug has not been fixed in the latest version.\r\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [X] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nWe found that the performance test results on the latest sglang v0.4.1 version were lower than v0.4.0. The following are the test results\r\n![v0 4 0](https://github.com/user-attachments/assets/0aa783d9-2f4a-4c7a-8670-9b6203428ba1)\r\n![v0 4 1](https://github.com/user-attachments/assets/3410e457-34ff-4992-85fd-fea88f8cc027)\r\n\r\nBy comparing the results of pytorch profler, we found that The cost of the fwd_grouped_kernel_dage1 function has increased significantly. \r\n![pytorch_profiler](https://github.com/user-attachments/assets/fe049283-3818-45f2-a676-b5354fc62d80)\r\n\r\n\r\n\r\n\r\n\r\n### Reproduction\r\n\r\nThe service startup command is as follows\r\n\u3010v0.4.1\u3011\r\nNVTE_FUSED_ATTN=1 NVTE_FUSED_ATTN_CK=0 NVTE_FUSED_ATTN_AOTRITON=1 TORCHINDUCTOR_MAX_AUTOTUNE=1 TORCHINDUCTOR_COORDINATE_DESCENT_TUNING=1 TORCHINDUCTOR_MAX_AUTOTUNE_GEMM_BACKENDS=TRITON OPTIMIZE_EPILOGUE=1 HIP_VISIBLE_DEVICES=1 python3 -m sglang.launch_server --model-path  /mnt/md0/pkg/Qwen2.5-7B-Instruct-GPTQ-Int8/ --port 30000 --mem-fraction-static  0.8 --kv-cache-dtype auto --attention-backend triton --sampling-backend pytorch --grammar-backend outlines --trust-remote-code --schedule-conservativeness 0.3 --enable-torch-compile --quantization gptq\r\nWARNING 12-31 03:33:38 rocm.py:31] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\r\n[2024-12-31 03:33:48] server_args=ServerArgs(model_path='/mnt/md0/pkg/Qwen2.5-7B-Instruct-GPTQ-Int8/', tokenizer_path='/mnt/md0/pkg/Qwen2.5-7B-Instruct-GPTQ-Int8/', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=True, dtype='auto', kv_cache_dtype='auto', quantization='gptq', context_length=None, device='cuda', served_model_name='/mnt/md0/pkg/Qwen2.5-7B-Instruct-GPTQ-Int8/', chat_template=None, is_embedding=False, revision=None, return_token_ids=False, host='127.0.0.1', port=30000, mem_fraction_static=0.8, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=0.3, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=1, stream_interval=1, random_seed=371532406, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='SGLang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, lora_paths=None, max_loras_per_batch=8, attention_backend='triton', sampling_backend='pytorch', grammar_backend='outlines', disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=True, torch_compile_max_bs=32, cuda_graph_max_bs=8, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False)\r\n\r\n\r\n\u3010v0.4.0\u3011\r\nNVTE_FUSED_ATTN=1 NVTE_FUSED_ATTN_CK=0 NVTE_FUSED_ATTN_AOTRITON=1 TORCHINDUCTOR_MAX_AUTOTUNE=1 TORCHINDUCTOR_COORDINATE_DESCENT_TUNING=1 TORCHINDUCTOR_MAX_AUTOTUNE_GEMM_BACKENDS=TRITON OPTIMIZE_EPILOGUE=1 HIP_VISIBLE_DEVICES=1 python3 -m sglang.launch_server --model-path  /mnt/md0/pkg/Qwen2.5-7B-Instruct-GPTQ-Int8/ --port 30000 --mem-fraction-static  0.8 --kv-cache-dtype auto --attention-backend triton --sampling-backend pytorch --grammar-backend outlines --trust-remote-code --schedule-conservativeness 0.3 --enable-torch-compile --quantization gptq\r\nWARNING 12-31 04:03:00 rocm.py:31] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\r\n[2024-12-31 04:03:09] server_args=ServerArgs(model_path='/mnt/md0/pkg/Qwen2.5-7B-Instruct-GPTQ-Int8/', tokenizer_path='/mnt/md0/pkg/Qwen2.5-7B-Instruct-GPTQ-Int8/', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=True, dtype='auto', kv_cache_dtype='auto', quantization='gptq', context_length=None, device='cuda', served_model_name='/mnt/md0/pkg/Qwen2.5-7B-Instruct-GPTQ-Int8/', chat_template=None, is_embedding=False, revision=None, host='127.0.0.1', port=30000, mem_fraction_static=0.8, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=0.3, cpu_offload_gb=0, tp_size=1, stream_interval=1, random_seed=556555260, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='SGLang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, lora_paths=None, max_loras_per_batch=8, attention_backend='triton', sampling_backend='pytorch', grammar_backend='outlines', disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_torch_compile=True, torch_compile_max_bs=32, cuda_graph_max_bs=8, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, num_continuous_decode_steps=1, delete_ckpt_after_loading=False)\r\n\r\n\r\n\r\nWe used a modified bench_deriving script to input longer sequences\r\n![bench](https://github.com/user-attachments/assets/95d18d0c-5a8d-4d20-a5e3-50be6f7fbcbd)\r\n\r\n\r\n### Environment\r\n\r\nThe environmental information is as follows\r\n![env](https://github.com/user-attachments/assets/d3495aea-3ce4-408b-ab89-5e7a0d35a3d8)\r\n",
    "labels": [
      "inactive",
      "amd"
    ],
    "state": "closed",
    "created_at": "2024-12-31T03:56:50+00:00",
    "closed_at": "2025-03-02T00:18:46+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2675/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2675"
  },
  {
    "number": 2621,
    "title": "[Feature] add AMD CIs",
    "body": "### Checklist\r\n\r\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [ ] 2. Please use English, otherwise it will be closed.\r\n\r\n### Motivation\r\n\r\nAs discussed offline, we should also support AMD CIs. @HaiShaw cc @merrymercy \r\n\r\n### Related resources\r\n\r\n_No response_",
    "labels": [
      "inactive",
      "amd"
    ],
    "state": "closed",
    "created_at": "2024-12-27T18:18:37+00:00",
    "closed_at": "2025-02-28T00:17:01+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2621/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/2621"
  },
  {
    "number": 2384,
    "title": "[Bug] Deepseek-v2-lite AMD MI300 run failed",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [X] 2. The bug has not been fixed in the latest version.\r\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [X] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\n#### Deepseek-v2 ROCM Env triton compiler error\r\nBug report:\r\n```bash\r\nWARNING 12-07 02:43:18 rocm.py:17] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\r\n[2024-12-07 02:43:23] server_args=ServerArgs(model_path='/data/deepseek-v2-lite/', tokenizer_path='/data/deepseek-v2-lite/', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=True, dtype='auto', kv_cache_dtype='auto', quantization=None, context_length=None, device='cuda', served_model_name='/data/deepseek-v2-lite/', chat_template=None, is_embedding=False, revision=None, host='127.0.0.1', port=30000, mem_fraction_static=0.81, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, tp_size=8, stream_interval=1, random_seed=179983669, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='SGLang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, lora_paths=None, max_loras_per_batch=8, attention_backend='triton', sampling_backend='pytorch', grammar_backend='outlines', disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, num_continuous_decode_steps=1, delete_ckpt_after_loading=False)\r\n[2024-12-07 02:43:32 TP4] Process 3010 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155]\r\n[2024-12-07 02:43:33 TP4] MLA optimization is turned on. Use triton backend.\r\n[2024-12-07 02:43:33 TP4] Init torch distributed begin.\r\n[2024-12-07 02:43:33 TP0] Process 3006 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107]\r\n[2024-12-07 02:43:33 TP1] Process 3007 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]\r\n[2024-12-07 02:43:33 TP5] Process 3011 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]\r\n[2024-12-07 02:43:33 TP7] Process 3139 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]\r\n[2024-12-07 02:43:33 TP6] Process 3075 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179]\r\n[2024-12-07 02:43:33 TP3] Process 3009 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]\r\n[2024-12-07 02:43:33 TP1] MLA optimization is turned on. Use triton backend.\r\n[2024-12-07 02:43:33 TP1] Init torch distributed begin.\r\n[2024-12-07 02:43:33 TP5] MLA optimization is turned on. Use triton backend.\r\n[2024-12-07 02:43:33 TP5] Init torch distributed begin.\r\n[2024-12-07 02:43:33 TP0] MLA optimization is turned on. Use triton backend.\r\n[2024-12-07 02:43:33 TP0] Init torch distributed begin.\r\n[2024-12-07 02:43:33 TP2] Process 3008 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131]\r\n[2024-12-07 02:43:33 TP7] MLA optimization is turned on. Use triton backend.\r\n[2024-12-07 02:43:33 TP7] Init torch distributed begin.\r\n[2024-12-07 02:43:33 TP6] MLA optimization is turned on. Use triton backend.\r\n[2024-12-07 02:43:33 TP6] Init torch distributed begin.\r\n[2024-12-07 02:43:33 TP3] MLA optimization is turned on. Use triton backend.\r\n[2024-12-07 02:43:33 TP3] Init torch distributed begin.\r\n[2024-12-07 02:43:33 TP2] MLA optimization is turned on. Use triton backend.\r\n[2024-12-07 02:43:33 TP2] Init torch distributed begin.\r\nINFO 12-07 02:43:34 pynccl_wrapper.py:188] Found nccl from library librccl.so.1\r\nINFO 12-07 02:43:34 pynccl_wrapper.py:188] Found nccl from library librccl.so.1\r\nINFO 12-07 02:43:34 pynccl_wrapper.py:188] Found nccl from library librccl.so.1\r\nINFO 12-07 02:43:34 pynccl_wrapper.py:188] Found nccl from library librccl.so.1\r\nINFO 12-07 02:43:34 pynccl_wrapper.py:188] Found nccl from library librccl.so.1\r\nINFO 12-07 02:43:34 pynccl_wrapper.py:188] Found nccl from library librccl.so.1\r\nINFO 12-07 02:43:34 pynccl_wrapper.py:188] Found nccl from library librccl.so.1\r\nINFO 12-07 02:43:34 pynccl_wrapper.py:188] Found nccl from library librccl.so.1\r\n[2024-12-07 02:43:36 TP4] Load weight begin. avail mem=185.83 GB\r\n[2024-12-07 02:43:36 TP7] Load weight begin. avail mem=185.83 GB\r\n[2024-12-07 02:43:36 TP0] Load weight begin. avail mem=184.31 GB\r\n[2024-12-07 02:43:36 TP5] Load weight begin. avail mem=185.80 GB\r\n[2024-12-07 02:43:36 TP6] Load weight begin. avail mem=185.70 GB\r\n[2024-12-07 02:43:36 TP3] Load weight begin. avail mem=185.54 GB\r\n[2024-12-07 02:43:36 TP2] Load weight begin. avail mem=186.38 GB\r\n[2024-12-07 02:43:36 TP1] Load weight begin. avail mem=185.55 GB\r\n[2024-12-07 02:43:36 TP7] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-12-07 02:43:36 TP4] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-12-07 02:43:36 TP7] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-12-07 02:43:36 TP4] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-12-07 02:43:36 TP3] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-12-07 02:43:36 TP6] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-12-07 02:43:36 TP3] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-12-07 02:43:36 TP6] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-12-07 02:43:36 TP5] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-12-07 02:43:36 TP5] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-12-07 02:43:36 TP2] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-12-07 02:43:36 TP0] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-12-07 02:43:36 TP1] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-12-07 02:43:36 TP0] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-12-07 02:43:36 TP2] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-12-07 02:43:36 TP1] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-12-07 02:43:36 TP7] Skipping import of cpp extensions\r\n[2024-12-07 02:43:36 TP4] Skipping import of cpp extensions\r\n[2024-12-07 02:43:36 TP5] Skipping import of cpp extensions\r\n[2024-12-07 02:43:36 TP6] Skipping import of cpp extensions\r\n[2024-12-07 02:43:36 TP3] Skipping import of cpp extensions\r\n[2024-12-07 02:43:36 TP0] Skipping import of cpp extensions\r\n[2024-12-07 02:43:36 TP2] Skipping import of cpp extensions\r\n[2024-12-07 02:43:36 TP1] Skipping import of cpp extensions\r\n[2024-12-07 02:43:36 TP7] lm_eval is not installed, GPTQ may not be usable\r\n[2024-12-07 02:43:36 TP4] lm_eval is not installed, GPTQ may not be usable\r\n[2024-12-07 02:43:36 TP5] lm_eval is not installed, GPTQ may not be usable\r\n[2024-12-07 02:43:36 TP6] lm_eval is not installed, GPTQ may not be usable\r\n[2024-12-07 02:43:36 TP3] lm_eval is not installed, GPTQ may not be usable\r\n[2024-12-07 02:43:36 TP0] lm_eval is not installed, GPTQ may not be usable\r\n[2024-12-07 02:43:36 TP2] lm_eval is not installed, GPTQ may not be usable\r\n[2024-12-07 02:43:36 TP1] lm_eval is not installed, GPTQ may not be usable\r\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [03:52<11:38, 232.83s/it]\r\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [07:58<08:01, 240.54s/it]\r\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [09:31<02:53, 173.19s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [10:55<00:00, 137.94s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [10:55<00:00, 163.93s/it]\r\n\r\n[2024-12-07 02:54:33 TP2] Load weight end. type=DeepseekV2ForCausalLM, dtype=torch.bfloat16, avail mem=182.61 GB\r\n[2024-12-07 02:54:33 TP7] Load weight end. type=DeepseekV2ForCausalLM, dtype=torch.bfloat16, avail mem=182.06 GB\r\n[2024-12-07 02:54:33 TP0] Load weight end. type=DeepseekV2ForCausalLM, dtype=torch.bfloat16, avail mem=180.54 GB\r\n[2024-12-07 02:54:33 TP4] Load weight end. type=DeepseekV2ForCausalLM, dtype=torch.bfloat16, avail mem=182.06 GB\r\n[2024-12-07 02:54:33 TP5] Load weight end. type=DeepseekV2ForCausalLM, dtype=torch.bfloat16, avail mem=182.04 GB\r\n[2024-12-07 02:54:33 TP1] Load weight end. type=DeepseekV2ForCausalLM, dtype=torch.bfloat16, avail mem=181.79 GB\r\n[2024-12-07 02:54:33 TP6] Load weight end. type=DeepseekV2ForCausalLM, dtype=torch.bfloat16, avail mem=181.93 GB\r\n[2024-12-07 02:54:33 TP3] Load weight end. type=DeepseekV2ForCausalLM, dtype=torch.bfloat16, avail mem=181.77 GB\r\n[2024-12-07 02:54:33 TP5] Memory pool end. avail mem=34.51 GB\r\n[2024-12-07 02:54:33 TP7] Memory pool end. avail mem=34.54 GB\r\n[2024-12-07 02:54:33 TP3] Memory pool end. avail mem=34.25 GB\r\n[2024-12-07 02:54:33 TP4] Memory pool end. avail mem=34.54 GB\r\n[2024-12-07 02:54:33 TP6] Memory pool end. avail mem=34.41 GB\r\n[2024-12-07 02:54:33 TP1] Memory pool end. avail mem=34.26 GB\r\n[2024-12-07 02:54:33 TP0] Memory pool end. avail mem=33.02 GB\r\n[2024-12-07 02:54:33 TP2] Memory pool end. avail mem=35.09 GB\r\n[2024-12-07 02:54:35 TP1] Capture cuda graph begin. This can take up to several minutes.\r\n[2024-12-07 02:54:35 TP2] Capture cuda graph begin. This can take up to several minutes.\r\n[2024-12-07 02:54:35 TP6] Capture cuda graph begin. This can take up to several minutes.\r\n[2024-12-07 02:54:35 TP7] Capture cuda graph begin. This can take up to several minutes.\r\n[2024-12-07 02:54:35 TP0] Capture cuda graph begin. This can take up to several minutes.\r\n[2024-12-07 02:54:35 TP4] Capture cuda graph begin. This can take up to several minutes.\r\n[2024-12-07 02:54:35 TP3] Capture cuda graph begin. This can take up to several minutes.\r\n[2024-12-07 02:54:35 TP5] Capture cuda graph begin. This can take up to several minutes.\r\nINFO 12-07 02:54:42 custom_all_reduce.py:260] Registering 0 cuda graph addresses\r\nINFO 12-07 02:54:42 custom_all_reduce.py:260] Registering 0 cuda graph addresses\r\nINFO 12-07 02:54:42 custom_all_reduce.py:260] Registering 0 cuda graph addresses\r\nINFO 12-07 02:54:42 custom_all_reduce.py:260] Registering 0 cuda graph addresses\r\nINFO 12-07 02:54:42 custom_all_reduce.py:260] Registering 0 cuda graph addresses\r\nINFO 12-07 02:54:42 custom_all_reduce.py:260] Registering 0 cuda graph addresses\r\n[2024-12-07 02:54:42 TP4] Scheduler hit an exception: Traceback (most recent call last):\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1493, in run_scheduler_process\r\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 191, in __init__\r\n    self.tp_worker = TpWorkerClass(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 62, in __init__\r\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 62, in __init__\r\n    self.model_runner = ModelRunner(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 180, in __init__\r\n    self.init_cuda_graphs()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 631, in init_cuda_graphs\r\n    self.cuda_graph_runner = CudaGraphRunner(self)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 207, in __init__\r\n    self.capture()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 268, in capture\r\n    ) = self.capture_one_batch_size(bs, forward)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 332, in capture_one_batch_size\r\n    run_once()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 325, in run_once\r\n    logits_output = forward(input_ids, forward_batch.positions, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 823, in forward\r\n    hidden_states = self.model(input_ids, positions, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 784, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 722, in forward\r\n    hidden_states = self.self_attn(\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 493, in forward\r\n    return self.forward_absorb(positions, hidden_states, forward_batch)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 579, in forward_absorb\r\n    attn_output = self.attn_mqa(q_input, k_input, v_input, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/radix_attention.py\", line 58, in forward\r\n    return forward_batch.attn_backend.forward(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/__init__.py\", line 59, in forward\r\n    return self.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_backend.py\", line 181, in forward_decode\r\n    self.decode_attention_fwd(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 701, in decode_attention_fwd\r\n    decode_attention_fwd_grouped(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 656, in decode_attention_fwd_grouped\r\n    _decode_grouped_softmax_reducev_fwd(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 567, in _decode_grouped_softmax_reducev_fwd\r\n    _fwd_grouped_kernel_stage2[grid](\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 330, in <lambda>\r\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 687, in run\r\n    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 392, in __getattribute__\r\n    self._init_handles()\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 385, in _init_handles\r\n    raise OutOfResources(self.metadata.shared, max_shared, \"shared memory\")\r\ntriton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.\r\n\r\n[2024-12-07 02:54:42 TP0] Scheduler hit an exception: Traceback (most recent call last):\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1493, in run_scheduler_process\r\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 191, in __init__\r\n    self.tp_worker = TpWorkerClass(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 62, in __init__\r\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 62, in __init__\r\n    self.model_runner = ModelRunner(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 180, in __init__\r\n    self.init_cuda_graphs()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 631, in init_cuda_graphs\r\n    self.cuda_graph_runner = CudaGraphRunner(self)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 207, in __init__\r\n    self.capture()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 268, in capture\r\n    ) = self.capture_one_batch_size(bs, forward)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 332, in capture_one_batch_size\r\n    run_once()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 325, in run_once\r\n    logits_output = forward(input_ids, forward_batch.positions, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 823, in forward\r\n    hidden_states = self.model(input_ids, positions, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 784, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 722, in forward\r\n    hidden_states = self.self_attn(\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 493, in forward\r\n    return self.forward_absorb(positions, hidden_states, forward_batch)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 579, in forward_absorb\r\n    attn_output = self.attn_mqa(q_input, k_input, v_input, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/radix_attention.py\", line 58, in forward\r\n    return forward_batch.attn_backend.forward(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/__init__.py\", line 59, in forward\r\n    return self.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_backend.py\", line 181, in forward_decode\r\n    self.decode_attention_fwd(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 701, in decode_attention_fwd\r\n    decode_attention_fwd_grouped(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 656, in decode_attention_fwd_grouped\r\n    _decode_grouped_softmax_reducev_fwd(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 567, in _decode_grouped_softmax_reducev_fwd\r\n    _fwd_grouped_kernel_stage2[grid](\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 330, in <lambda>\r\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 687, in run\r\n    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 392, in __getattribute__\r\n    self._init_handles()\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 385, in _init_handles\r\n    raise OutOfResources(self.metadata.shared, max_shared, \"shared memory\")\r\ntriton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.\r\n\r\n[2024-12-07 02:54:42 TP1] Scheduler hit an exception: Traceback (most recent call last):\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1493, in run_scheduler_process\r\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 191, in __init__\r\n    self.tp_worker = TpWorkerClass(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 62, in __init__\r\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 62, in __init__\r\n    self.model_runner = ModelRunner(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 180, in __init__\r\n    self.init_cuda_graphs()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 631, in init_cuda_graphs\r\n    self.cuda_graph_runner = CudaGraphRunner(self)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 207, in __init__\r\n    self.capture()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 268, in capture\r\n    ) = self.capture_one_batch_size(bs, forward)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 332, in capture_one_batch_size\r\n    run_once()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 325, in run_once\r\n    logits_output = forward(input_ids, forward_batch.positions, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 823, in forward\r\n    hidden_states = self.model(input_ids, positions, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 784, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 722, in forward\r\n    hidden_states = self.self_attn(\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 493, in forward\r\n    return self.forward_absorb(positions, hidden_states, forward_batch)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 579, in forward_absorb\r\n    attn_output = self.attn_mqa(q_input, k_input, v_input, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/radix_attention.py\", line 58, in forward\r\n    return forward_batch.attn_backend.forward(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/__init__.py\", line 59, in forward\r\n    return self.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_backend.py\", line 181, in forward_decode\r\n    self.decode_attention_fwd(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 701, in decode_attention_fwd\r\n    decode_attention_fwd_grouped(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 656, in decode_attention_fwd_grouped\r\n    _decode_grouped_softmax_reducev_fwd(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 567, in _decode_grouped_softmax_reducev_fwd\r\n    _fwd_grouped_kernel_stage2[grid](\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 330, in <lambda>\r\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 687, in run\r\n    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 392, in __getattribute__\r\n    self._init_handles()\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 385, in _init_handles\r\n    raise OutOfResources(self.metadata.shared, max_shared, \"shared memory\")\r\ntriton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.\r\n\r\n[2024-12-07 02:54:42 TP7] Scheduler hit an exception: Traceback (most recent call last):\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1493, in run_scheduler_process\r\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 191, in __init__\r\n    self.tp_worker = TpWorkerClass(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 62, in __init__\r\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 62, in __init__\r\n    self.model_runner = ModelRunner(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 180, in __init__\r\n    self.init_cuda_graphs()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 631, in init_cuda_graphs\r\n    self.cuda_graph_runner = CudaGraphRunner(self)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 207, in __init__\r\n    self.capture()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 268, in capture\r\n    ) = self.capture_one_batch_size(bs, forward)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 332, in capture_one_batch_size\r\n    run_once()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 325, in run_once\r\n    logits_output = forward(input_ids, forward_batch.positions, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 823, in forward\r\n    hidden_states = self.model(input_ids, positions, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 784, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 722, in forward\r\n    hidden_states = self.self_attn(\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 493, in forward\r\n    return self.forward_absorb(positions, hidden_states, forward_batch)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 579, in forward_absorb\r\n    attn_output = self.attn_mqa(q_input, k_input, v_input, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/radix_attention.py\", line 58, in forward\r\n    return forward_batch.attn_backend.forward(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/__init__.py\", line 59, in forward\r\n    return self.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_backend.py\", line 181, in forward_decode\r\n    self.decode_attention_fwd(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 701, in decode_attention_fwd\r\n    decode_attention_fwd_grouped(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 656, in decode_attention_fwd_grouped\r\n    _decode_grouped_softmax_reducev_fwd(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 567, in _decode_grouped_softmax_reducev_fwd\r\n    _fwd_grouped_kernel_stage2[grid](\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 330, in <lambda>\r\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 687, in run\r\n    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 392, in __getattribute__\r\n    self._init_handles()\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 385, in _init_handles\r\n    raise OutOfResources(self.metadata.shared, max_shared, \"shared memory\")\r\ntriton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.\r\n\r\nINFO 12-07 02:54:42 custom_all_reduce.py:260] Registering 0 cuda graph addresses\r\nINFO 12-07 02:54:42 custom_all_reduce.py:260] Registering 0 cuda graph addresses\r\n[2024-12-07 02:54:42 TP6] Scheduler hit an exception: Traceback (most recent call last):\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1493, in run_scheduler_process\r\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 191, in __init__\r\n    self.tp_worker = TpWorkerClass(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 62, in __init__\r\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 62, in __init__\r\n    self.model_runner = ModelRunner(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 180, in __init__\r\n    self.init_cuda_graphs()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 631, in init_cuda_graphs\r\n    self.cuda_graph_runner = CudaGraphRunner(self)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 207, in __init__\r\n    self.capture()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 268, in capture\r\n    ) = self.capture_one_batch_size(bs, forward)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 332, in capture_one_batch_size\r\n    run_once()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 325, in run_once\r\n    logits_output = forward(input_ids, forward_batch.positions, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 823, in forward\r\n    hidden_states = self.model(input_ids, positions, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 784, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 722, in forward\r\n    hidden_states = self.self_attn(\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 493, in forward\r\n    return self.forward_absorb(positions, hidden_states, forward_batch)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 579, in forward_absorb\r\n    attn_output = self.attn_mqa(q_input, k_input, v_input, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/radix_attention.py\", line 58, in forward\r\n    return forward_batch.attn_backend.forward(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/__init__.py\", line 59, in forward\r\n    return self.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_backend.py\", line 181, in forward_decode\r\n    self.decode_attention_fwd(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 701, in decode_attention_fwd\r\n    decode_attention_fwd_grouped(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 656, in decode_attention_fwd_grouped\r\n    _decode_grouped_softmax_reducev_fwd(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 567, in _decode_grouped_softmax_reducev_fwd\r\n    _fwd_grouped_kernel_stage2[grid](\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 330, in <lambda>\r\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 687, in run\r\n    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 392, in __getattribute__\r\n    self._init_handles()\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 385, in _init_handles\r\n    raise OutOfResources(self.metadata.shared, max_shared, \"shared memory\")\r\ntriton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.\r\n\r\n[2024-12-07 02:54:42 TP5] Scheduler hit an exception: Traceback (most recent call last):\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1493, in run_scheduler_process\r\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 191, in __init__\r\n    self.tp_worker = TpWorkerClass(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 62, in __init__\r\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 62, in __init__\r\n    self.model_runner = ModelRunner(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 180, in __init__\r\n    self.init_cuda_graphs()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 631, in init_cuda_graphs\r\n    self.cuda_graph_runner = CudaGraphRunner(self)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 207, in __init__\r\n    self.capture()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 268, in capture\r\n    ) = self.capture_one_batch_size(bs, forward)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 332, in capture_one_batch_size\r\n    run_once()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 325, in run_once\r\n    logits_output = forward(input_ids, forward_batch.positions, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 823, in forward\r\n    hidden_states = self.model(input_ids, positions, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 784, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 722, in forward\r\n    hidden_states = self.self_attn(\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 493, in forward\r\n    return self.forward_absorb(positions, hidden_states, forward_batch)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 579, in forward_absorb\r\n    attn_output = self.attn_mqa(q_input, k_input, v_input, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/radix_attention.py\", line 58, in forward\r\n    return forward_batch.attn_backend.forward(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/__init__.py\", line 59, in forward\r\n    return self.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_backend.py\", line 181, in forward_decode\r\n    self.decode_attention_fwd(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 701, in decode_attention_fwd\r\n    decode_attention_fwd_grouped(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 656, in decode_attention_fwd_grouped\r\n    _decode_grouped_softmax_reducev_fwd(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 567, in _decode_grouped_softmax_reducev_fwd\r\n    _fwd_grouped_kernel_stage2[grid](\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 330, in <lambda>\r\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 687, in run\r\n    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 392, in __getattribute__\r\n    self._init_handles()\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 385, in _init_handles\r\n    raise OutOfResources(self.metadata.shared, max_shared, \"shared memory\")\r\ntriton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.\r\n\r\n[2024-12-07 02:54:42 TP3] Scheduler hit an exception: Traceback (most recent call last):\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1493, in run_scheduler_process\r\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 191, in __init__\r\n    self.tp_worker = TpWorkerClass(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 62, in __init__\r\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 62, in __init__\r\n    self.model_runner = ModelRunner(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 180, in __init__\r\n    self.init_cuda_graphs()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 631, in init_cuda_graphs\r\n    self.cuda_graph_runner = CudaGraphRunner(self)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 207, in __init__\r\n    self.capture()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 268, in capture\r\n    ) = self.capture_one_batch_size(bs, forward)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 332, in capture_one_batch_size\r\n    run_once()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 325, in run_once\r\n    logits_output = forward(input_ids, forward_batch.positions, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 823, in forward\r\n    hidden_states = self.model(input_ids, positions, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 784, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 722, in forward\r\n    hidden_states = self.self_attn(\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 493, in forward\r\n    return self.forward_absorb(positions, hidden_states, forward_batch)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 579, in forward_absorb\r\n    attn_output = self.attn_mqa(q_input, k_input, v_input, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/radix_attention.py\", line 58, in forward\r\n    return forward_batch.attn_backend.forward(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/__init__.py\", line 59, in forward\r\n    return self.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_backend.py\", line 181, in forward_decode\r\n    self.decode_attention_fwd(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 701, in decode_attention_fwd\r\n    decode_attention_fwd_grouped(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 656, in decode_attention_fwd_grouped\r\n    _decode_grouped_softmax_reducev_fwd(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 567, in _decode_grouped_softmax_reducev_fwd\r\n    _fwd_grouped_kernel_stage2[grid](\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 330, in <lambda>\r\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 687, in run\r\n    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 392, in __getattribute__\r\n    self._init_handles()\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 385, in _init_handles\r\n    raise OutOfResources(self.metadata.shared, max_shared, \"shared memory\")\r\ntriton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.\r\n\r\n[2024-12-07 02:54:42 TP2] Scheduler hit an exception: Traceback (most recent call last):\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1493, in run_scheduler_process\r\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 191, in __init__\r\n    self.tp_worker = TpWorkerClass(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 62, in __init__\r\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 62, in __init__\r\n    self.model_runner = ModelRunner(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 180, in __init__\r\n    self.init_cuda_graphs()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 631, in init_cuda_graphs\r\n    self.cuda_graph_runner = CudaGraphRunner(self)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 207, in __init__\r\n    self.capture()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 268, in capture\r\n    ) = self.capture_one_batch_size(bs, forward)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 332, in capture_one_batch_size\r\n    run_once()\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 325, in run_once\r\n    logits_output = forward(input_ids, forward_batch.positions, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 823, in forward\r\n    hidden_states = self.model(input_ids, positions, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 784, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 722, in forward\r\n    hidden_states = self.self_attn(\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 493, in forward\r\n    return self.forward_absorb(positions, hidden_states, forward_batch)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 579, in forward_absorb\r\n    attn_output = self.attn_mqa(q_input, k_input, v_input, forward_batch)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/radix_attention.py\", line 58, in forward\r\n    return forward_batch.attn_backend.forward(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/__init__.py\", line 59, in forward\r\n    return self.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_backend.py\", line 181, in forward_decode\r\n    self.decode_attention_fwd(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 701, in decode_attention_fwd\r\n    decode_attention_fwd_grouped(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 656, in decode_attention_fwd_grouped\r\n    _decode_grouped_softmax_reducev_fwd(\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\", line 567, in _decode_grouped_softmax_reducev_fwd\r\n    _fwd_grouped_kernel_stage2[grid](\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 330, in <lambda>\r\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 687, in run\r\n    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 392, in __getattribute__\r\n    self._init_handles()\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/compiler/compiler.py\", line 385, in _init_handles\r\n    raise OutOfResources(self.metadata.shared, max_shared, \"shared memory\")\r\ntriton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 65536. Reducing block sizes or `num_stages` may help.\r\n```\r\n\r\n### Reproduction\r\n\r\n```bash\r\npython -m sglang.launch_server \\\r\n         --model-path /data/deepseek-v2-lite/ \\\r\n         --dp 1 \\\r\n         --tp 8 \\\r\n         --trust-remote-code \\\r\n```\r\n\r\n### Environment\r\n\r\ndocker image ```henryx/haisgl:sgl0.3.2_vllm0.6.0_torch2.5_rocm6.2_triton3.0.0```",
    "labels": [
      "amd"
    ],
    "state": "closed",
    "created_at": "2024-12-07T07:24:15+00:00",
    "closed_at": "2024-12-31T02:41:23+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2384/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2384"
  },
  {
    "number": 1953,
    "title": "[Bug] amdgpu\uff0ctp-size=2\uff0cDetected errors during sampling! NaN in the logits.",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\npython3 -m sglang.launch_server --model-path  /root/.xinference/cache/qwen2_5-instruct-gptq-7b-Int8/ --port 30000 --mem-fraction-static  0.8 --kv-cache-dtype int8 --attention-backend triton --sampling-backend pytorch --tp-size 2\r\nWARNING 11-08 04:42:43 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\r\n[2024-11-08 04:42:51] server_args=ServerArgs(model_path='/root/.xinference/cache/qwen2_5-instruct-gptq-7b-Int8/', tokenizer_path='/root/.xinference/cache/qwen2_5-instruct-gptq-7b-Int8/', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='int8', kvint4_groupsize=32, quantization=None, context_length=None, device='cuda', served_model_name='/root/.xinference/cache/qwen2_5-instruct-gptq-7b-Int8/', chat_template=None, is_embedding=False, host='127.0.0.1', port=30000, mem_fraction_static=0.8, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, tp_size=2, stream_interval=1, random_seed=661408819, constrained_json_whitespace_pattern=None, decode_log_interval=40, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, api_key=None, file_storage_pth='SGLang_storage', enable_cache_report=False, watchdog_timeout=600, dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, lora_paths=None, max_loras_per_batch=8, attention_backend='triton', sampling_backend='pytorch', grammar_backend='outlines', disable_flashinfer=False, disable_flashinfer_sampling=False, disable_radix_cache=False, disable_regex_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_penalizer=False, disable_nan_detection=False, enable_overlap_schedule=False, enable_mixed_chunk=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, torchao_config='', enable_p2p_check=False, triton_attention_reduce_in_fp32=False, num_continuous_decode_steps=1)\r\n[2024-11-08 04:43:04 TP0] Init torch distributed begin.\r\n[2024-11-08 04:43:04 TP1] Init torch distributed begin.\r\n[2024-11-08 04:43:07 TP0] Load weight begin. avail mem=23.03 GB\r\n[2024-11-08 04:43:07 TP1] Load weight begin. avail mem=23.47 GB\r\n[2024-11-08 04:43:07 TP1] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-11-08 04:43:07 TP1] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-11-08 04:43:07 TP0] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\n[2024-11-08 04:43:07 TP0] FlashInfer is not available on Non-NV platforms. Fallback to other kernel libraries.\r\nLoading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:03,  1.74s/it]\r\nLoading safetensors checkpoint shards:  67% Completed | 2/3 [00:03<00:02,  2.02s/it]\r\n[2024-11-08 04:43:12 TP1] Load weight end. type=Qwen2ForCausalLM, dtype=torch.float16, avail mem=19.09 GB\r\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:04<00:00,  1.50s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:04<00:00,  1.61s/it]\r\n\r\n[2024-11-08 04:43:13 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.float16, avail mem=18.65 GB\r\n[2024-11-08 04:43:13 TP1] Memory pool end. avail mem=4.58 GB\r\n[2024-11-08 04:43:13 TP0] Memory pool end. avail mem=4.14 GB\r\n[2024-11-08 04:43:13 TP0] Capture cuda graph begin. This can take up to several minutes.\r\n[2024-11-08 04:43:13 TP1] Capture cuda graph begin. This can take up to several minutes.\r\n[2024-11-08 04:43:58 TP0] max_total_num_tokens=1033884, max_prefill_tokens=16384, max_running_requests=4097, context_len=32768\r\n[2024-11-08 04:43:58 TP1] max_total_num_tokens=1033884, max_prefill_tokens=16384, max_running_requests=4097, context_len=32768\r\n[2024-11-08 04:43:58] INFO:     Started server process [95220]\r\n[2024-11-08 04:43:58] INFO:     Waiting for application startup.\r\n[2024-11-08 04:43:58] INFO:     Application startup complete.\r\n[2024-11-08 04:43:58] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)\r\n[2024-11-08 04:43:59] INFO:     127.0.0.1:41416 - \"GET /get_model_info HTTP/1.1\" 200 OK\r\n[2024-11-08 04:43:59 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\r\n[2024-11-08 04:44:04 TP1] Detected errors during sampling! NaN in the logits.\r\n[2024-11-08 04:44:04 TP0] Detected errors during sampling! NaN in the logits.\r\n[2024-11-08 04:44:04 TP0] Detected errors during sampling! NaN in the logits.\r\n[2024-11-08 04:44:04 TP1] Detected errors during sampling! NaN in the logits.\r\n[2024-11-08 04:44:04 TP0] Detected errors during sampling! NaN in the logits.\r\n[2024-11-08 04:44:04 TP1] Detected errors during sampling! NaN in the logits.\r\n[2024-11-08 04:44:04 TP0] Detected errors during sampling! NaN in the logits.\r\n[2024-11-08 04:44:04 TP1] Detected errors during sampling! NaN in the logits.\r\n[2024-11-08 04:44:04 TP0] Detected errors during sampling! NaN in the logits.\r\n[2024-11-08 04:44:04 TP1] Detected errors during sampling! NaN in the logits.\r\n[2024-11-08 04:44:06 TP1] Detected errors during sampling! NaN in the logits.\r\n[2024-11-08 04:44:06 TP0] Detected errors during sampling! NaN in the logits.\r\n[2024-11-08 04:44:06 TP0] Detected errors during sampling! NaN in the logits.\r\n[2024-11-08 04:44:06 TP1] Detected errors during sampling! NaN in the logits.\r\n\n\n### Reproduction\n\npython3 -m sglang.launch_server --model-path  /root/.xinference/cache/qwen2_5-instruct-gptq-7b-Int8/ --port 30000 --mem-fraction-static  0.8 --kv-cache-dtype int8 --attention-backend triton --sampling-backend pytorch --tp-size 2\n\n### Environment\n\n  Name:                    gfx1100\r\n  Uuid:                    GPU-b1d1b7e55cd7ec87\r\n  Marketing Name:          Radeon RX 7900 XTX\r\n  Vendor Name:             AMD\r\n  Feature:                 KERNEL_DISPATCH\r\n  Profile:                 BASE_PROFILE\r\n  Float Round Mode:        NEAR\r\n  Max Queue Number:        128(0x80)\r\n  Queue Min Size:          64(0x40)\r\n  Queue Max Size:          131072(0x20000)\r\n  Queue Type:              MULTI\r\n  Node:                    3\r\n  Device Type:             GPU\r\n  Cache Info:\r\n    L1:                      32(0x20) KB\r\n    L2:                      6144(0x1800) KB\r\n    L3:                      98304(0x18000) KB\r\n  Chip ID:                 29772(0x744c)\r\n  ASIC Revision:           0(0x0)\r\n  Cacheline Size:          64(0x40)\r\n  Max Clock Freq. (MHz):   2070\r\n  BDFID:                   49920\r\n  Internal Node ID:        3\r\n  Compute Unit:            96\r\n  SIMDs per CU:            2\r\n  Shader Engines:          6\r\n  Shader Arrs. per Eng.:   2\r\n  WatchPts on Addr. Ranges:4\r\n  Coherent Host Access:    FALSE\r\n  Features:                KERNEL_DISPATCH\r\n  Fast F16 Operation:      TRUE\r\n  Wavefront Size:          32(0x20)\r\n  Workgroup Max Size:      1024(0x400)\r\n  Workgroup Max Size per Dimension:\r\n    x                        1024(0x400)\r\n    y                        1024(0x400)\r\n    z                        1024(0x400)\r\n  Max Waves Per CU:        32(0x20)\r\n  Max Work-item Per CU:    1024(0x400)\r\n  Grid Max Size:           4294967295(0xffffffff)\r\n  Grid Max Size per Dimension:\r\n    x                        4294967295(0xffffffff)\r\n    y                        4294967295(0xffffffff)\r\n    z                        4294967295(0xffffffff)\r\n  Max fbarriers/Workgrp:   32\r\n  Packet Processor uCode:: 202\r\n  SDMA engine uCode::      20\r\n  IOMMU Support::          None\r\n  Pool Info:\r\n    Pool 1\r\n      Segment:                 GLOBAL; FLAGS: COARSE GRAINED\r\n      Size:                    25149440(0x17fc000) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:2048KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       FALSE\r\n    Pool 2\r\n      Segment:                 GLOBAL; FLAGS: EXTENDED FINE GRAINED\r\n      Size:                    25149440(0x17fc000) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:2048KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       FALSE\r\n    Pool 3\r\n      Segment:                 GROUP\r\n      Size:                    64(0x40) KB\r\n      Allocatable:             FALSE\r\n      Alloc Granule:           0KB\r\n      Alloc Recommended Granule:0KB\r\n      Alloc Alignment:         0KB\r\n      Accessible by all:       FALSE\r\n  ISA Info:\r\n    ISA 1\r\n      Name:                    amdgcn-amd-amdhsa--gfx1100\r\n      Machine Models:          HSA_MACHINE_MODEL_LARGE\r\n      Profiles:                HSA_PROFILE_BASE\r\n      Default Rounding Mode:   NEAR\r\n      Default Rounding Mode:   NEAR\r\n      Fast f16:                TRUE\r\n      Workgroup Max Size:      1024(0x400)\r\n      Workgroup Max Size per Dimension:\r\n        x                        1024(0x400)\r\n        y                        1024(0x400)\r\n        z                        1024(0x400)\r\n      Grid Max Size:           4294967295(0xffffffff)\r\n      Grid Max Size per Dimension:\r\n        x                        4294967295(0xffffffff)\r\n        y                        4294967295(0xffffffff)\r\n        z                        4294967295(0xffffffff)\r\n      FBarrier Max Size:       32\r\n*** Done ***\r\n",
    "labels": [
      "await-response",
      "inactive",
      "amd"
    ],
    "state": "closed",
    "created_at": "2024-11-08T04:44:58+00:00",
    "closed_at": "2025-01-29T00:16:25+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1953/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1953"
  },
  {
    "number": 1945,
    "title": "[Bug] tp-size=2\uff0cmodel launch error",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\ntp-size=2, model launch is frozen.\n\n### Reproduction\n\n python3 -m sglang.launch_server --model-path  /root/.xinference/cache/qwen2_5-instruct-gptq-7b-Int8/ --port 30000 --mem-fraction-static  0.8 --tp-size 2 --kv-cache-dtype int8 --attention-backend triton --sampling-backend pytorch --enable-torch-compile\n\n### Environment\n\namd gpu RTX 7900xtx\r\n  Name:                    gfx1100\r\n  Uuid:                    GPU-b1d1b7e55cd7ec87\r\n  Marketing Name:          Radeon RX 7900 XTX\r\n  Vendor Name:             AMD\r\n  Feature:                 KERNEL_DISPATCH\r\n  Profile:                 BASE_PROFILE\r\n  Float Round Mode:        NEAR\r\n  Max Queue Number:        128(0x80)\r\n  Queue Min Size:          64(0x40)\r\n  Queue Max Size:          131072(0x20000)\r\n  Queue Type:              MULTI\r\n  Node:                    3\r\n  Device Type:             GPU\r\n  Cache Info:\r\n    L1:                      32(0x20) KB\r\n    L2:                      6144(0x1800) KB\r\n    L3:                      98304(0x18000) KB\r\n  Chip ID:                 29772(0x744c)\r\n  ASIC Revision:           0(0x0)\r\n  Cacheline Size:          64(0x40)\r\n  Max Clock Freq. (MHz):   2070\r\n  BDFID:                   49920\r\n  Internal Node ID:        3\r\n  Compute Unit:            96\r\n  SIMDs per CU:            2\r\n  Shader Engines:          6\r\n  Shader Arrs. per Eng.:   2\r\n  WatchPts on Addr. Ranges:4\r\n  Coherent Host Access:    FALSE\r\n  Features:                KERNEL_DISPATCH\r\n  Fast F16 Operation:      TRUE\r\n  Wavefront Size:          32(0x20)\r\n  Workgroup Max Size:      1024(0x400)\r\n  Workgroup Max Size per Dimension:\r\n    x                        1024(0x400)\r\n    y                        1024(0x400)\r\n    z                        1024(0x400)\r\n  Max Waves Per CU:        32(0x20)\r\n  Max Work-item Per CU:    1024(0x400)\r\n  Grid Max Size:           4294967295(0xffffffff)\r\n  Grid Max Size per Dimension:\r\n    x                        4294967295(0xffffffff)\r\n    y                        4294967295(0xffffffff)\r\n    z                        4294967295(0xffffffff)\r\n  Max fbarriers/Workgrp:   32\r\n  Packet Processor uCode:: 202\r\n  SDMA engine uCode::      20\r\n  IOMMU Support::          None\r\n  Pool Info:\r\n    Pool 1\r\n      Segment:                 GLOBAL; FLAGS: COARSE GRAINED\r\n      Size:                    25149440(0x17fc000) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:2048KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       FALSE\r\n    Pool 2\r\n      Segment:                 GLOBAL; FLAGS: EXTENDED FINE GRAINED\r\n      Size:                    25149440(0x17fc000) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:2048KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       FALSE\r\n    Pool 3\r\n      Segment:                 GROUP\r\n      Size:                    64(0x40) KB\r\n      Allocatable:             FALSE\r\n      Alloc Granule:           0KB\r\n      Alloc Recommended Granule:0KB\r\n      Alloc Alignment:         0KB\r\n      Accessible by all:       FALSE\r\n  ISA Info:\r\n    ISA 1\r\n      Name:                    amdgcn-amd-amdhsa--gfx1100\r\n      Machine Models:          HSA_MACHINE_MODEL_LARGE\r\n      Profiles:                HSA_PROFILE_BASE\r\n      Default Rounding Mode:   NEAR\r\n      Default Rounding Mode:   NEAR\r\n      Fast f16:                TRUE\r\n      Workgroup Max Size:      1024(0x400)\r\n      Workgroup Max Size per Dimension:\r\n        x                        1024(0x400)\r\n        y                        1024(0x400)\r\n        z                        1024(0x400)\r\n      Grid Max Size:           4294967295(0xffffffff)\r\n      Grid Max Size per Dimension:\r\n        x                        4294967295(0xffffffff)\r\n        y                        4294967295(0xffffffff)\r\n        z                        4294967295(0xffffffff)\r\n      FBarrier Max Size:       32\r\n*** Done ***\r\n",
    "labels": [
      "await-response",
      "inactive",
      "amd"
    ],
    "state": "closed",
    "created_at": "2024-11-07T06:14:03+00:00",
    "closed_at": "2025-01-29T00:16:26+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1945/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1945"
  },
  {
    "number": 1857,
    "title": "TP8 scheduling overhead is very high for small model, Llama 3 8B on AMD",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen I benchmark TP1, the throughput is great.\r\n\r\nBackend:                                 sglang\r\nTraffic request rate:                    inf\r\nSuccessful requests:                     4000\r\nBenchmark duration (s):                  22.18\r\nTotal input tokens:                      257409\r\nTotal generated tokens:                  257960\r\nTotal generated tokens (retokenized):    256866\r\n**Request throughput (req/s):              180.31**\r\nInput token throughput (tok/s):          11603.04\r\nOutput token throughput (tok/s):         11627.87\r\n\r\nHowever if I test TP8, the performance is very poor, as the scheduling dominates the running time,\r\nBackend:                                 sglang\r\nTraffic request rate:                    inf\r\nSuccessful requests:                     4000\r\nBenchmark duration (s):                  100.06\r\nTotal input tokens:                      257409\r\nTotal generated tokens:                  257960\r\nTotal generated tokens (retokenized):    256833\r\n**Request throughput (req/s):              39.97**\r\nInput token throughput (tok/s):          2572.48\r\nOutput token throughput (tok/s):         2577.98\r\n\r\n\n\n### Reproduction\n\nTP1\r\npython -m sglang.launch_server --model-path NousResearch/Meta-Llama-3-8B --tp-size 1 --disable-nan-detection --disable-disk-cache --enable-overlap-schedule\r\n\r\npython3 -m sglang.bench_serving --backend sglang --dataset-name random --num-prompts 4000 --random-input 128 --random-output 128\r\n\r\nTP8\r\npython -m sglang.launch_server --model-path NousResearch/Meta-Llama-3-8B --tp-size 8 --disable-nan-detection --disable-disk-cache --enable-overlap-schedule\r\n\r\npython3 -m sglang.bench_serving --backend sglang --dataset-name random --num-prompts 4000 --random-input 128 --random-output 128\r\n\n\n### Environment\n\nlatest sglang on ROCm 6.2 and MI300X.",
    "labels": [
      "amd"
    ],
    "state": "closed",
    "created_at": "2024-10-31T20:38:46+00:00",
    "closed_at": "2025-01-10T19:56:20+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1857/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1857"
  }
]
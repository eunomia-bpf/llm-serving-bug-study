[
  {
    "number": 7910,
    "title": "[Feature] Cutlass kernels for LoRA",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCreating an issue to track the work for supporting a CUTLASS / CUTE kernel for LoRA to see if there is any perf gain comparing with the current Triton one.\n\nDependency: this task should happen after #7809 as the FlashInfer deprecation is expected to change / simplify the kernel interface.\n\n(cc @Fridge003 @Ying1123 )\n\n### Related resources\n\n_No response_",
    "labels": [
      "lora"
    ],
    "state": "open",
    "created_at": "2025-07-09T21:43:29+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7910/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7910"
  },
  {
    "number": 7809,
    "title": "[Refactor] Deprecate FlashInfer lora backend",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nThe current FlashInfer lora backend has been in a deprecation mode for a while with various bugs and feature gaps. Based on offline discussion with @Fridge003 and @Ying1123 , we decided to deprecate FlashInfer backend in favor of the Triton backend.\n\nThis would largely simplify our codebase and potentially bring perf gains as we eliminate the overheads to accommodate the special requirements from Flashinfer (e.g. today we have to reshape qkv in a way that's quite specific to FlashInfer in LoRAAdapter for compatibility but then convert it back in Triton backend).\n\nWe should keep the abstraction for supporting multiple backends as in the long term, we should consider prioritize introducing cutlass/cuda backend. \n\n### Related resources\n\n_No response_",
    "labels": [
      "lora"
    ],
    "state": "open",
    "created_at": "2025-07-06T19:05:41+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7809/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7809"
  },
  {
    "number": 7808,
    "title": "[Bug] Dynamic LoRA load does not handle modules with greater rank correctly",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nAdding an issue to track a bug for the new dynamic lora support:\n\nCurrently the new dynamic lora support creates gpu buffer based on existing adapter max lora ranks, when a new adapter is loaded that has larger lora rank than the initial set, it might not correctly handle it (to be verified).\n\nWe need to add logic to reset existing buffers when max-lora-rank changes.\n\n### Reproduction\n\nTODO\n\n### Environment\n\nN/A",
    "labels": [
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-07-06T18:59:18+00:00",
    "closed_at": "2025-07-14T09:28:24+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7808/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7808"
  },
  {
    "number": 7765,
    "title": "[Bug] Division by zero error in the current Triton LoRA backend when lora_path is None",
    "body": "### Describe the bug\n\nCreating an issue to track a potential bug based on offline discussion with @Fridge003 \n\nIn the current LoRA implementation, we handle lora_path = None requests by calling LoRA backend kernel (_sgemm_lora_a_kernel) with lora rank being zero. However, the kernel does not seem to be designed to handle such case.\n\nIf I add `tl.device_print` to the triton kernel, I can observe division by zero:\n\n![Image](https://github.com/user-attachments/assets/710f4abe-de0b-4852-b18c-a60645da0009)\n\nInterestingly, during the limited test cases I tried, the division-by-zero line (`pid // num_pid_n`) returned `-1`, and it did not result in memory access error as I expected it would be, the program output also appears to be reasonable. \n\n### Reproduction\n\n```\npython3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct  --disable-radix-cache --lora-paths lora=algoprog/fact-generation-llama-3.1-8b-instruct-lora --max-loras-per-batch 1\n\ncurl -s http://localhost:30000/generate \\                                                                                                                                                         \n  -H \"Content-Type: application/json\" \\\n  -d '{\n  \"text\": [\n    \"AI is a field of computer science focused on\"\n  ],\n  \"lora_path\": [\n          \"lora\"\n  ],\n  \"sampling_params\": {\n    \"temperature\": 0,\n    \"top_k\": 1,\n    \"max_new_tokens\": 256\n  }\n}'\n```\n\n### Environment\n\nN/A",
    "labels": [
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-07-04T06:15:16+00:00",
    "closed_at": "2025-07-06T08:25:54+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7765/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7765"
  },
  {
    "number": 7463,
    "title": "[Feature] Add server arg enable-lora to allow starting up with empty lora-paths",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCurrently SGL implicitly uses --lora-paths to decide whether engine/server should be started with LoRA support enabled.\n\nAs we are going to support dynamic lora loading/unloading soon (#7446), the current implicit constraint is no longer reasonable as it should be perfectly legal for users to start a LoRA-enabled server without having to provide any lora paths, but instead load/unload adapters later as needed. \n\n### Related resources\n\n_No response_",
    "labels": [
      "lora"
    ],
    "state": "open",
    "created_at": "2025-06-23T07:42:22+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7463/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7463"
  },
  {
    "number": 7447,
    "title": "[Feature] Graceful handling of non-existing lora_path in inference request",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCreating an issue to track this TODO for myself (or anyone else who wants to help):\n\nCurrently when users call SGLang with a non-existing lora_path, SGLang server/engine would crash due to failed assertions in `prepare_lora_batch`. This is unideal as it imposes unnecessary burden for server owner to validate request params before they are passed to the SGLang backend.\n\nIdeally, SGLang should have gracefully handled the exception and respond 4xx errors without crashing the server.\n\n### Related resources\n\n_No response_",
    "labels": [
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-06-22T21:36:01+00:00",
    "closed_at": "2025-07-03T03:59:17+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7447/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7447"
  },
  {
    "number": 7426,
    "title": "[Bug] LoRA buffer eviction does not correctly handle adapters with different target weights",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nIn the [load_lora_weight_to_buffer ](https://github.com/sgl-project/sglang/blob/9edf6608c9d299e126cc65634ee368d2fc52b0ad/python/sglang/srt/lora/mem_pool.py#L164C9-L168C19) function, we zero out `A_buffer` when `uid == None` ([code reference](https://github.com/sgl-project/sglang/blob/9edf6608c9d299e126cc65634ee368d2fc52b0ad/python/sglang/srt/lora/mem_pool.py#L164C1-L168C19)) to prevent leftover weights of the previously evicted LoRA adapters from interfering with subsequent computations.\n\nHowever, I suspect we should do the same even when `uid != None`, because in theory different adapters could target different modules (e.g., some adapters do not target k_proj). Our code might not be able to handle this case correctly, for example, if we have two adapters: lora1 targets k_proj, lora2 does not. If lora2 is reusing the memory buffer left by lora1 after its eviction, the k_proj weight of lora1 would remain in the buffer and potentially contaminate the computation of lora2. I discussed this with @Fridge003 and @Qiaolin-Yu offline and they have the same suspicion. \n\nAs this is a rare corner case, I have not got a chance to construct a test to verify it. I am creating this issue to track this potential bug. We need to:\n1. **verify**: construct a test case to repro the issue, e.g., setting `max-loras-per-batch = 1` but have 2 adapters with different target weights.\n2. **fix**: always zero out buffer during gpu buffer eviction.\n3. **benchmark**: verify perf overheads introduced by the zero-out operation. \n\n### Reproduction\n\nSee first comment.\n\n### Environment\n\nBug is environment agnostic",
    "labels": [
      "lora"
    ],
    "state": "open",
    "created_at": "2025-06-21T18:12:07+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7426/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7426"
  },
  {
    "number": 7062,
    "title": "[Bug] test_lora.py bug",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThere is a prompt \"AI is a field of computer science focused on\" in `test/srt/models/lora/test_lora.py ` that can easily break CI, which might be caused some internal bug of lora.\n\nWe remove this prompt temporarily in #7061. It should be added back after this bug is fixed.\n\n### Reproduction\n\nUncomment line 49 of `test_lora.py`\n\n![Image](https://github.com/user-attachments/assets/e91d22ca-9def-400e-abd7-a21fd8985d54)\n\nthen run\n```bash\npython3 test/srt/models/lora/test_lora.py \n```\n\n### Environment\n\nLatest main branch",
    "labels": [
      "bug",
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-06-10T18:10:20+00:00",
    "closed_at": "2025-06-28T04:28:35+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7062/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7062"
  },
  {
    "number": 6608,
    "title": "[Feature] Customized mapping for LoRA weight names",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nThe current LoRA impl in SGL maps LoRA weight to modules by (layer index, op_type) tuple, where op_type operation looks like `qkv_proj`, `o_proj`, `gate_up`, etc. This works fine for most standard cases, however, there are some limitations:\n1. For models where there are more than one attention stacks (e.g., VLM), there could be multiple modules with the same (layer index, op_type), e.g., one from vision tower, the other from the language model. Currently SGL cannot handle such cases correctly and would usually fail during loading due to incorrect mapping.\n2. Users cannot enable/disable application of LoRA at module-level, e.g., if user only wants to apply LoRA at language model but not vision (common); or when user only wants to apply LoRA at some layers but not the others (less common?), they cannot do that today. \n3. (Less common?) Models with non-standard LoRA weight / module names. \n\n### Proposal: \n\n* (Short-term) add an optional hook `should_apply_lora` at model level to allow model to customize LoRA application at model level. This would unblock most cases in 1 & 2. For example, for most VLMs, LoRAs should only be applied to language model but not vision tower. In these cases, model authors could simply disable LoRA application for modules in the vision tower, This would address the current LoRA loading failures due to incorrect mapping.\n* (Long-term) generalize the hook to `map_lora_module_name` to allow model owner to map a given module to a specific LoRA weight name or return None when LoRA should not be applied. This would address 3 and some less common cases in 1 (e.g., when LoRA needs to be applied at both vision tower and language model)\n\n(cc @Fridge003 )\n\n\n\n### Related resources\n\n_No response_",
    "labels": [
      "low-priority",
      "lora"
    ],
    "state": "open",
    "created_at": "2025-05-26T04:08:39+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6608/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6608"
  },
  {
    "number": 6501,
    "title": "[Bug] QLoRA adapters: not working with sglang : random predictions from text with LoRA, model load error with QLoRA",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI am using Mistral v0.3 along with QLoRA adapters for a specific task. It works perfectly with vLLM.\n\nBut with sglang it creates random continuous prediction from the text till max tokens.\n\n### Reproduction\n\nmodel_path = \"/AITraining/home/llms/mistral-7b-instruct-v0.3\"\n\nserver_process, port = launch_server_cmd(f\"\"\"\npython -m sglang.launch_server \\\n--model-path {model_path} \\\n--lora-paths lora0=../FundName/training_orpo/outputs_mi_orpo_fund_list2_phase3/checkpoint-10000 \\\n--tp 1 \\\n--lora-backend triton \\\n--disable-cuda-graph \\\n--disable-radix-cache \\\n--mem-fraction-static 0.85 \\\n--max-running-requests 4 \\\n--host 0.0.0.0\n\"\"\".strip()\n)\n\nprint(\"*\" * 10)\nprint(\"Model:\", model_path)\nprint(\"Running on port:\", port)\nprint(\"*\" * 10)\n\nwait_for_server(f\"http://localhost:{port}\")\n\n\njson_data = {\n    \"text\": [\n        prompt,\n    ],\n    \"sampling_params\": {\"max_new_tokens\":64, \"temperature\": 0, \"stop\":[\"<s>\", \"</s>\", \"[INST]\", \"[/INST]\"]},\n    # The first input uses lora0, and the second input uses lora1\n    \"lora_path\": [\"lora0\"],\n}\nresponse = requests.post(\n    url + \"/generate\",\n    json=json_data,\n)\nprint(f\"Fund name: {response.json()[0]['text']}\")\n\n### Environment\n\nPython: 3.10.15 (main, Dec  2 2024, 18:21:11) [GCC 9.4.0]\nCUDA available: True\nGPU 0,1,2,3: NVIDIA A10G\nGPU 0,1,2,3 Compute Capability: 8.6\nCUDA_HOME: /usr/local/cuda-12.4\nNVCC: Cuda compilation tools, release 12.4, V12.4.99\nCUDA Driver Version: 550.54.14\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post4\nsgl_kernel: 0.1.2.post1\nflashinfer_python: 0.2.5+cu124torch2.6\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.13\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.31.2\ninteregular: 0.3.3\nmodelscope: 1.23.2\norjson: 3.10.15\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\npython-multipart: 0.0.20\npyzmq: 26.3.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.8.5.post1\nxgrammar: 0.1.19\nopenai: 1.66.3\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.63.8\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PHB     PHB     PHB     0-47    0               N/A\nGPU1    PHB      X      PHB     PHB     0-47    0               N/A\nGPU2    PHB     PHB      X      PHB     0-47    0               N/A\nGPU3    PHB     PHB     PHB      X      0-47    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nHypervisor vendor: KVM\nulimit soft: 8192",
    "labels": [
      "lora"
    ],
    "state": "open",
    "created_at": "2025-05-21T13:00:00+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6501/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6501"
  },
  {
    "number": 5928,
    "title": "[Bug] Lora manager cannot correctly process a batch if lora_path=[None, lora1]",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n If lora_path=[None, lora1], it will throw an error\n```\nFile \"/sgl-workspace/sglang/python/sglang/srt/lora/lora_manager.py\", line 220, in prepare_lora_batch\n    lora = self.loras[lora_path]\nKeyError: None\n```\n\nThe issue is from this commit (https://github.com/sgl-project/sglang/commit/ef9a378a209d970e0b5c48ae3eac6f2660d43faf#diff-830eb84f0293dfc02acebb21b81f34c7372f9878b8737a759402194ce7e77fd0R163)\n\n### Reproduction\n\n```\ndef serving(self, prompts, lora_set, tp_size, torch_dtype, max_new_tokens):\n        print(\"=================== testing serving =======================\")\n        # test batch forward\n        base_path = lora_set[\"base\"]\n        all_lora_paths = lora_set[\"loras\"]\n        batch_lora_paths = [None]\n        i = 0\n        for _ in range(len(prompts) - 1):\n            batch_lora_paths.append(all_lora_paths[i])\n            i = (i + 1) % len(all_lora_paths)\n\n        with SRTRunner(\n            base_path,\n            tp_size=tp_size,\n            torch_dtype=torch_dtype,\n            model_type=\"generation\",\n            lora_paths=all_lora_paths,\n            max_loras_per_batch=3,\n            disable_cuda_graph=True,\n            disable_radix_cache=True,\n        ) as srt_runner:\n            srt_outputs = srt_runner.batch_forward(\n                prompts, max_new_tokens=max_new_tokens, lora_paths=batch_lora_paths\n            )\n\n        with HFRunner(\n            base_path,\n            torch_dtype=torch_dtype,\n            model_type=\"generation\",\n            output_str_only=True,\n        ) as hf_runner:\n            hf_outputs = hf_runner.forward(\n                prompts, max_new_tokens=max_new_tokens, lora_paths=batch_lora_paths\n            )\n\n        # compare output strings\n        print(f\"{hf_outputs.output_strs=}\")\n        print(f\"{srt_outputs.output_strs=}\")\n        for i in range(len(prompts)):\n            assert srt_outputs.output_strs[i].strip(\" \") == hf_outputs.output_strs[i], (\n                srt_outputs.output_strs[i].strip(\" \"),\n                hf_outputs.output_strs[i],\n            )\n```\n\n### Environment\n\n```\npython3 -m sglang.check_env\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0: NVIDIA H100 80GB HBM3\nGPU 0 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.144.03\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post1\nsgl_kernel: 0.1.0\nflashinfer_python: 0.2.3+cu124torch2.5\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.14\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.24.0\norjson: 3.10.15\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\npython-multipart: 0.0.20\npyzmq: 26.3.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.1.dev5770+g268c325.precompiled\nxgrammar: 0.1.17\nopenai: 1.68.2\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.63.14\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4      NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS       SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     NODE    NODE    SYS       SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    PIX     NODE    SYS       SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     SYS       SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX       NODE    NODE    NODE    32-63,96-127    1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE      PIX     NODE    NODE    32-63,96-127    1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE      NODE    PIX     NODE    32-63,96-127    1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     NODE      NODE    NODE    PIX     32-63,96-127    1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS       SYS     SYS     SYS\nNIC1    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS       SYS     SYS     SYS\nNIC2    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS       SYS     SYS     SYS\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS       SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X        NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE       X      NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE      NODE     X      NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE      NODE    NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n\n\nulimit soft: 1048576\n```",
    "labels": [
      "bug",
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-04-30T18:06:25+00:00",
    "closed_at": "2025-05-01T02:42:43+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5928/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5928"
  },
  {
    "number": 5897,
    "title": "[Bug] HF_Runner can't produce correct results after applying lora",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nFirst batch input lora_path as [a, a]. Second batch input lora_path as [None, None]. The second batch will be processed as if you had input lora_path as [a, a].\n\n### Reproduction\n\n```\n# Copyright 2023-2024 SGLang Team\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport multiprocessing as mp\nimport unittest\n\nimport torch\n\nfrom sglang.test.runners import HFRunner, SRTRunner\nfrom sglang.test.test_utils import CustomTestCase\n\nLORA_SETS = [\n    # {\n    #     \"base\": \"meta-llama/Llama-2-7b-hf\",\n    #     \"loras\": [\"RuterNorway/Llama-2-7b-chat-norwegian-LoRa\"],\n    # },\n    {\"base\": \"meta-llama/Llama-2-7b-hf\", \"loras\": [\"winddude/wizardLM-LlaMA-LoRA-7B\"]},\n    # {\"base\": \"Qwen/Qwen2.5-14B-Instruct\", \"loras\": [\"mssongit/Qwen2.5-14B-SFT-LoRA\"]},\n    # {\"base\": \"mistralai/Mistral-7B-Instruct-v0.3\", \"loras\": [\"/home/ying/test_lora\"]},\n    # {\n    # \"base\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n    #     \"loras\": [\n    #         \"/home/ying/test_lora\",\n    #         \"/home/ying/test_lora_1\",\n    #         \"/home/ying/test_lora_2\",\n    #         \"/home/ying/test_lora_3\",\n    #         \"/home/ying/test_lora_4\",\n    #     ],\n    # },\n    # {\"base\": \"meta-llama/Llama-2-7b-hf\", \"loras\": [\"yard1/llama-2-7b-sql-lora-test\"]},\n]\nTORCH_DTYPES = [torch.float16]\n\nPROMPTS = [\n    \"\"\"\n### Instruction:\nWrite a poem about the transformers Python library.\nMention the word \"large language models\" in that poem.\n### Response:\nThe Transformers are large language models,\nThey're used to make predictions on text.\n\"\"\",\n    \"\"\"\n### Instruction:\nWrite a poem about the transformers Python library.\nMention the word \"large language models\" in that poem.\n### Response:\nThe Transformers are large language models,\nThey're used to make predictions on text.\n\"\"\",\n]\n\n\nclass TestLoRA(CustomTestCase):\n\n    def inference(self, prompts, lora_set, tp_size, torch_dtype, max_new_tokens):\n        print(\"=================== testing inference =======================\")\n        base_path = lora_set[\"base\"]\n        all_lora_paths = lora_set[\"loras\"]\n        batch_lora_paths = [all_lora_paths[0], all_lora_paths[0]]\n\n        with HFRunner(\n            base_path, torch_dtype=torch_dtype, model_type=\"generation\"\n        ) as hf_runner:\n            hf_outputs = hf_runner.forward(\n                prompts, max_new_tokens=max_new_tokens, lora_paths=batch_lora_paths\n            )\n\n            hf_no_lora_outputs = hf_runner.forward(\n                prompts, max_new_tokens=max_new_tokens, lora_paths=[None] * len(prompts)\n            )\n        with HFRunner(\n            base_path, torch_dtype=torch_dtype, model_type=\"generation\"\n        ) as hf_runner:\n            hf_no_lora_outputs1 = hf_runner.forward(\n                prompts, max_new_tokens=max_new_tokens, lora_paths=[None] * len(prompts)\n            )\n\n        # compare output strings\n        print(f\"{hf_outputs.output_strs=}\")\n        print(f\"{hf_no_lora_outputs.output_strs=}\")\n        print(f\"{hf_no_lora_outputs1.output_strs=}\")\n\n        for i in range(len(prompts)):\n            assert hf_no_lora_outputs.output_strs[i].strip(\n                \" \"\n            ) == hf_no_lora_outputs1.output_strs[i].strip(\" \"), (\n                hf_no_lora_outputs.output_strs[i].strip(\" \"),\n                hf_no_lora_outputs1.output_strs[i].strip(\" \"),\n            )\n\n    def test_all(self):\n        for lora_set in LORA_SETS:\n            for torch_dtype in TORCH_DTYPES:\n                tp_size = 1\n                max_new_tokens = 32\n                self.inference(PROMPTS, lora_set, tp_size, torch_dtype, max_new_tokens)\n\n\nif __name__ == \"__main__\":\n    try:\n        mp.set_start_method(\"spawn\")\n    except RuntimeError:\n        pass\n\n    unittest.main(warnings=\"ignore\")\n\n```\n\n### Environment\n\n```\npython3 -m sglang.check_env\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0: NVIDIA H100 80GB HBM3\nGPU 0 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.144.03\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post1\nsgl_kernel: 0.1.0\nflashinfer: Module Not Found\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.14\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.24.0\norjson: 3.10.15\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.1.dev5770+g268c325.precompiled\nxgrammar: 0.1.17\nopenai: 1.68.2\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.63.14\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4      NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS       SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     NODE    NODE    SYS       SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    PIX     NODE    SYS       SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     SYS       SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX       NODE    NODE    NODE    32-63,96-127    1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE      PIX     NODE    NODE    32-63,96-127    1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE      NODE    PIX     NODE    32-63,96-127    1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     NODE      NODE    NODE    PIX     32-63,96-127    1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS       SYS     SYS     SYS\nNIC1    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS       SYS     SYS     SYS\nNIC2    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS       SYS     SYS     SYS\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS       SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X        NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE       X      NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE      NODE     X      NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE      NODE    NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n\n\nulimit soft: 1048576\n```",
    "labels": [
      "bug",
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-04-30T00:10:58+00:00",
    "closed_at": "2025-04-30T03:17:43+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5897/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5897"
  },
  {
    "number": 4739,
    "title": "[Bug] LoRA adapter can't process successfully when LoRA Path is None",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nCurrently, LoRA in SGLang cannot handle `none` lora_path properly.\n\n### Reproduction\n\nSome cases below will have issues.\n1. First batch input `lora_path` as `[a, a]`. Second batch input `lora_path` as `[None, None]`. The second batch will be processed as if you had input lora_path as `[a, a]`.\n2. If you input `lora_path` as [a, None], the output of the second prompt will be almost empty.\n\n### Environment\n\nMain branch",
    "labels": [
      "bug",
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-03-25T01:15:36+00:00",
    "closed_at": "2025-03-28T04:03:09+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4739/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4739"
  },
  {
    "number": 3647,
    "title": "[Feature] Support unified paging in multi-lora serving",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCurrently, SGL doesn't support the unified paging feature proposed by S-LoRA. However, this feature is important for memory management in multi-LoRA serving.\n\n### Related resources\n\n_No response_",
    "labels": [
      "enhancement",
      "inactive",
      "feature",
      "lora"
    ],
    "state": "open",
    "created_at": "2025-02-17T19:14:47+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3647/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3647"
  },
  {
    "number": 3438,
    "title": "[Feature] Support Lora for VocabParallelEmbedding layer",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nSupport lora for `VocabParallelEmbedding`. Not a trivial task.\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive",
      "feature",
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-02-09T19:13:40+00:00",
    "closed_at": "2025-06-30T00:21:14+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3438/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3438"
  },
  {
    "number": 3414,
    "title": "[Feature] Test case enhancement for Lora features",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCurrently there is only two test files for Lora features:\n`test/srt/models/test_lora.py` and \n`test/srt/models/test_lora_backend.py`\nThese two tests are only tested on llama models, thus not comprehensive.\n\nLora needs a series of well-organized tests, which can be similar to \n`test/srt/models/test_generation_models.py`\n\n\n### Check List\ncheck list copied from #3652:\n- [x] Add backend test support for single adaptor, single prompt inference.\n- [ ] Add backend test support for single adaptor, batch prompts serving.\n- [ ] Add backend test support for multi-adaptor, same rank.\n- [ ] Add backend test support for multi-adaptor, different rank.\n- [ ] Add backend test support for adaptor with Embedding and Lm_head layer weights.\n\n### Related PRs\n\n#3652 #4239 #4925",
    "labels": [
      "enhancement",
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-02-09T00:41:02+00:00",
    "closed_at": "2025-04-17T21:40:34+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3414/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3414"
  },
  {
    "number": 3323,
    "title": "[Feature] optimize group gemm",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nRewrite the  Grouped GEMM used by LoRA with cuBLAS 12.5 in sgl-kernel for improved speed.\n\nhttps://developer.nvidia.com/blog/introducing-grouped-gemm-apis-in-cublas-and-more-performance-updates/\nhttps://github.com/zhihu/ZhiLight/blob/main/src/nn/linear/gemm_grouped.cpp\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority",
      "performance",
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-02-05T22:56:43+00:00",
    "closed_at": "2025-02-20T08:26:59+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3323/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/3323"
  },
  {
    "number": 3282,
    "title": "[Feature] Support compatibility between Cuda Graph and Lora",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCurrently Lora and Cuda Graph cannot be used at the same time. \n\nTheoretically they should be compatible.\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive",
      "feature",
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-02-04T06:27:19+00:00",
    "closed_at": "2025-04-29T06:30:45+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3282/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3282"
  },
  {
    "number": 2931,
    "title": "[Bug] tensor_model_parallel_all_reduce' is not defined",
    "body": "### Describe the bug\n\nI attempted to serve the Phi-4 Lora Fine-tuning model by setting tensor parallel size 2 using the sglang framework, but the following error occurred.\n\n> [Error Log]\n\n```\n[2025-01-17 01:51:55 TP0] LoRA manager ready.\n[2025-01-17 01:51:57 TP1] Load weight end. type=Phi3ForCausalLM, dtype=torch.float16, avail mem=15.70 GB\n[2025-01-17 01:52:00 TP1] LoRA manager ready.\n[2025-01-17 01:52:00 TP0] Memory pool end. avail mem=39.54 GB\n[2025-01-17 01:52:02 TP1] Memory pool end. avail mem=13.43 GB\n[2025-01-17 01:52:02 TP1] max_total_num_tokens=16384, max_prefill_tokens=16384, max_running_requests=2049, context_len=16384\n[2025-01-17 01:52:02 TP0] max_total_num_tokens=16384, max_prefill_tokens=16384, max_running_requests=2049, context_len=16384\n[2025-01-17 01:52:02] INFO:     Started server process [649817]\n[2025-01-17 01:52:02] INFO:     Waiting for application startup.\n[2025-01-17 01:52:02] INFO:     Application startup complete.\n[2025-01-17 01:52:02] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n[2025-01-17 01:52:03] INFO:     127.0.0.1:47632 - \"GET /get_model_info HTTP/1.1\" 200 OK\n[2025-01-17 01:52:03 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n[2025-01-17 01:52:11 TP0] TpModelWorkerClient hit an exception: Traceback (most recent call last):\n  File \"/home/work/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 101, in forward_thread_func\n    self.forward_thread_func_()\n  File \"/home/work/.local/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/work/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 132, in forward_thread_func_\n    logits_output, next_token_ids = self.worker.forward_batch_generation(\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/work/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 154, in forward_batch_generation\n    logits_output = self.model_runner.forward(forward_batch)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/work/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 679, in forward\n    return self.forward_extend(forward_batch)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/work/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 648, in forward_extend\n    return self.model.forward(\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/work/.local/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/work/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/sglang/srt/models/llama.py\", line 337, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/work/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/work/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/work/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/sglang/srt/models/llama.py\", line 288, in forward\n    hidden_states, residual = layer(\n                              ^^^^^^\n  File \"/home/work/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/work/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/work/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/sglang/srt/models/llama.py\", line 237, in forward\n    hidden_states = self.self_attn(\n                    ^^^^^^^^^^^^^^^\n  File \"/home/work/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/work/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/work/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/sglang/srt/models/llama.py\", line 175, in forward\n    output, _ = self.o_proj(attn_output)\n                ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/work/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/work/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/work/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/sglang/srt/lora/lora.py\", line 248, in forward\n    output_ = tensor_model_parallel_all_reduce(output_parallel)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nNameError: name 'tensor_model_parallel_all_reduce' is not defined\n```\n\n### Reproduction\n\nModel Name: Microsoft Phi-4\n\nnohup python -m sglang.launch_server --model-path /home/work/ai/Microsoft_Phi-4/phi-4_quantized_8bit --lora-paths lora=/home/work/ai/Microsoft_Phi-4/lora_tuning_1221 --port 8001 --mem-fraction-static 0.8 --host 0.0.0.0 --dtype auto --disable-radix-cache --disable-cuda-graph --quantization gptq_marlin  --max-total-tokens 16384 --tp 2 &\n\n### Environment\n\nPython: 3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2: CUDA GPU\nGPU 0,1,2 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 11.8, V11.8.89\nCUDA Driver Version: 535.54.03\nPyTorch: 2.5.1+cu124\nsglang: 0.4.0\nflashinfer: 0.1.6+cu121torch2.4\ntriton: 3.1.0\ntransformers: 4.48.0\ntorchao: 0.6.1\nnumpy: 1.26.4\naiohttp: 3.11.8\nfastapi: 0.115.5\nhf_transfer: 0.1.8\nhuggingface_hub: 0.27.0\ninteregular: 0.3.3\nmodelscope: 1.20.1\norjson: 3.10.12\npackaging: 24.2\npsutil: 6.1.0\npydantic: 2.10.4\nmultipart: 0.0.17\nzmq: 26.2.0\nuvicorn: 0.32.1\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.58.1\nanthropic: Module Not Found\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PIX     NODE    SYS     1,3,5,7,9,11    1               N/A\nGPU1    PIX      X      NODE    SYS     1,3,5,7,9,11    1               N/A\nGPU2    NODE    NODE     X      SYS     1,3,5,7,9,11    1               N/A\nNIC0    SYS     SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n\n\nulimit soft: 1048576",
    "labels": [
      "bug",
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-01-17T02:02:26+00:00",
    "closed_at": "2025-03-19T05:22:34+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2931/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2931"
  },
  {
    "number": 2929,
    "title": "[Feature] Lora Development Roadmap",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Features \n\n- [x] triton kernel & benchmark #3161 @Fridge003 \n- [x] accuracy alignment #2671 #3413 @Fridge003 \n- [x] test cases enhancement #3414 #3652 #4492 #4925 @aoshen524 @jcbjcbjc\n- [x] support multi-rank adaptors #4492 @jcbjcbjc\n- [x] support tensor parallel #2931 #4274 @aoshen524 \n- [ ] compatibility with radix attention #2880 @Sunt-ing @jcbjcbjc\n- [x] compatibility with cuda graph #3282 #4115 @Qiaolin-Yu  @Beichen-Ma \n- [x] support phi4mm #6544 @lifuhuang \n- [ ] support lora for embedding layer #3438 @Beichen-Ma \n- [x] load/unload #7412 #7446 @lifuhuang  @Fridge003 \n- [ ] optimizing speed #2372 #3323 #6961 @jcbjcbjc @Fridge003 @lifuhuang \n- [ ] unified paging (support lora with different ranks) #3647 @Sunt-ing @jcbjcbjc\n- [ ] OpenAI compatible API\n- [x] Documentation #5521 @Fridge003 \n\n### Related resources\n\nPrior todo list can be referred to #1307 and #1728",
    "labels": [
      "help wanted",
      "lora"
    ],
    "state": "open",
    "created_at": "2025-01-16T21:30:56+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2929/reactions",
      "total_count": 14,
      "+1": 8,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 6,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2929"
  },
  {
    "number": 2880,
    "title": "[Bug] Why can't I use multi-lora adapter and radix attention together?",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhy can't I use multi-lora adapter and radix attention together?\r\nIf I have multi-lora adapters, why not just insert the ID of the LoRA adapter before the first token?\r\n\r\nWhen using a multi-lora adapter, it is extremely slow because radix attention cannot be used.\n\n### Reproduction\n\nhttps://github.com/sgl-project/sglang/blob/v0.4.1.post5/python/sglang/srt/server_args.py#L876-L881\n\n### Environment\n\n```\r\nroot@33e74a81f115:/sglang/python# python3 -m sglang.check_env                                                                                                                                                         \r\n\r\nPython: 3.10.16 (main, Dec  4 2024, 08:53:37) [GCC 9.4.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nGPU 0 Compute Capability: 8.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\r\nCUDA Driver Version: 550.127.05\r\nPyTorch: 2.5.1+cu124\r\nsglang: 0.4.0.post2\r\nflashinfer: 0.1.6+cu124torch2.4\r\ntriton: 3.1.0\r\ntransformers: 4.47.0\r\ntorchao: 0.6.1\r\nnumpy: 1.26.4\r\naiohttp: 3.11.10\r\nfastapi: 0.115.6\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.26.3\r\ninteregular: 0.3.3\r\nmodelscope: 1.21.0\r\norjson: 3.10.12\r\npackaging: 24.2\r\npsutil: 6.1.0\r\npydantic: 2.10.3\r\nmultipart: 0.0.19\r\nzmq: 26.2.0\r\nuvicorn: 0.32.1\r\nuvloop: 0.21.0\r\nvllm: 0.6.4.post1\r\nopenai: 1.57.0\r\nanthropic: 0.40.0\r\ndecord: 0.6.0\r\n```",
    "labels": [
      "bug",
      "lora"
    ],
    "state": "open",
    "created_at": "2025-01-14T07:03:52+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2880/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2880"
  },
  {
    "number": 2686,
    "title": "[Feature] Dynamic Lora Support in SGLang",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nIn SGLang, I would to dynamically apply domain-specific Lora adapters to smaller/local models. Normally, I use SGLang for inference. Recently, I've switched to Vllm which already has the ability to unload/load adaptors: https://docs.vllm.ai/en/latest/usage/lora.html\r\nIf this feature is already exists in SGlang, can you add an example in the documentation?\r\n\n\n### Related resources\n\nhttps://docs.vllm.ai/en/latest/usage/lora.html",
    "labels": [
      "lora"
    ],
    "state": "closed",
    "created_at": "2024-12-31T12:20:29+00:00",
    "closed_at": "2025-06-28T04:36:06+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2686/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2686"
  },
  {
    "number": 2671,
    "title": "[Bug] HuggingFace and SGLang inference don't match",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [ ] 2. The bug has not been fixed in the latest version.\r\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [ ] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nThe accuracy of the model is degraded due to inconsistent outputs from SGLang. While HF and vLLM produce consistent results such as \"A\" or \"B,\" SGLang occasionally outputs responses like \"I can't process that request.\" or \"A.\" / \"B.\" This inconsistency impacts overall accuracy.\r\n\r\n### Reproduction\r\n\r\nWhat command or script did you run?\r\nA script for generating outputs using a LLaMA 3.1 8B Istruct model with LoRA.\r\n\r\nWhich model are you using?\r\nLLaMA 3.1 with LoRA applied.\r\n\r\nSteps to reproduce:\r\n\r\n1. Run the script with HF, vLLM, and SGLang configurations.\r\n2. Compare the single-token outputs between the frameworks.\r\n3. Observe the inconsistent behavior in SGLang.\r\n\r\n------------------------------------------------------------------------------------------------------------------------\r\n**Hugging Face Code Snippet :** \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\nmodel = LlamaForCausalLM.from_pretrained(\r\n    model_name,\r\n    load_in_8bit=False,\r\n    torch_dtype=torch.float16,\r\n    device_map='auto',\r\n)\r\nadaptor_path = './model_spec/checkpoints/checkpoint-200-vllm'\r\nmodel = PeftModel.from_pretrained(\r\n    model,\r\n    adaptor_path,\r\n    torch_dtype=torch.float16,\r\n)\r\n\r\nmodel.config.pad_token_id = tokenizer.pad_token_id = 0\r\nmodel.config.bos_token_id = 1\r\nmodel.config.eos_token_id = 2\r\nmodel.generation_config.pad_token_id = tokenizer.pad_token_id\r\nmodel.eval()\r\n\r\n\r\ndef evaluate(\r\n    instruction,\r\n    input=None,\r\n    temperature=0,\r\n    top_p=1,\r\n    top_k=-1,\r\n    num_beams=4,\r\n    max_new_tokens=128,\r\n    stream_output=False,\r\n    **kwargs,\r\n):\r\n    prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{instruction}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\r\n\r\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\r\n    input_ids = inputs[\"input_ids\"].to('cuda')\r\n    generation_config = GenerationConfig(\r\n        temperature=temperature,\r\n        top_p=top_p,\r\n        top_k=top_k,\r\n        num_beams=num_beams,\r\n        **kwargs,\r\n    )\r\n\r\n    with torch.no_grad():\r\n        generation_output = model.generate(\r\n            input_ids=input_ids,\r\n            generation_config=generation_config,\r\n            return_dict_in_generate=True,\r\n            output_scores=True,\r\n            max_new_tokens=max_new_tokens\r\n        )\r\n    s = generation_output.sequences[0]\r\n    output = tokenizer.decode(s, skip_special_tokens=True)\r\n    result = output.split('assistant')[1].strip()\r\n    return result\r\n\r\n------------------------------------------------------------------------------------------------------------------------\r\n**SGLang Code Snippet :** \r\n\r\nimport sglang as sgl\r\nfrom sglang import *\r\nimport logging\r\nimport json\r\nimport torch\r\n\r\nfrom typing import Union, List\r\nfrom vllm import LLM, SamplingParams\r\nfrom vllm.inputs import TokensPrompt\r\nfrom vllm.lora.request import LoRARequest\r\nfrom collections import defaultdict\r\n\r\nfrom utils.usecase_prompts import UseCasePrompter\r\nfrom utils.lora_adapters import UseCaseLoraAdapters\r\nfrom utils.prompter import Prompter\r\n\r\nlogging.basicConfig(format='%(asctime)s %(message)s')\r\nlogger = logging.getLogger()\r\nlogger.setLevel(logging.INFO)\r\n\r\nclass SimpleSGLangLlama2:\r\n    def __init__(self, base_model_path, number_of_gpu=1, gpu_memory_utilization=0.4):\r\n        self.base_model_path = base_model_path\r\n        self._model = sgl.Engine(model_path=self.base_model_path)\r\n\r\n    def generate(\r\n        self,\r\n        prompt: Union[str, List[int]],\r\n        temperature: float = 0.0,\r\n        top_p: float = 1.0,\r\n        top_k: int = -1,\r\n        use_beam_search: bool = True,\r\n        max_new_tokens: int = 128,\r\n        best_of: int = 4\r\n    ) -> List[str]:\r\n        sampling_params = {\"temperature\": temperature, \"top_p\": top_p, \"top_k\":top_k, \"max_new_tokens\": max_new_tokens}\r\n        # Build final_prompt_text, then:\r\n        outputs = self._model.generate(\r\n            [final_prompt_text],\r\n            sampling_params,\r\n            lora_path=adaptor_path\r\n        )\r\n        results = [output['text'] for output in outputs]\r\n        return results\r\n\r\nif __name__ == \"__main__\":\r\n    model_path = \"./models/meta-llama/Meta-Llama-3.1-8B-Instruct\"\r\n    llm = SimpleSGLangLlama2(model_path)\r\n\r\n\r\n------------------------------------------------------------------------------------------------------------------------\r\n\r\n\r\n### Environment\r\n\r\nRun the class in notebooks environment.\r\n\r\nSGLang 0.4.0 (with flashinfer 0.1.6+cu121torch2.4)",
    "labels": [
      "bug",
      "inactive",
      "lora"
    ],
    "state": "closed",
    "created_at": "2024-12-30T22:54:09+00:00",
    "closed_at": "2025-05-03T00:18:08+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2671/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2671"
  },
  {
    "number": 2372,
    "title": "[Feature] lora serving performance ",
    "body": "lora reasoning speed is very slow, I ran a gemma's lora, found that qkv proj takes 0.0003s, but without lora only 0.0001s, so the result is a token decode time difference of 20ms+\r\n\r\nhowever, vllm lora serving is faster",
    "labels": [
      "inactive",
      "performance",
      "lora"
    ],
    "state": "closed",
    "created_at": "2024-12-06T08:22:03+00:00",
    "closed_at": "2025-04-30T00:18:53+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2372/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2372"
  }
]
[
  {
    "number": 7998,
    "title": "Instruction for Running DeepSeek with PD, EP, and MTP",
    "body": "# Using Main Branch\n\n## Environment Preparation\nUse SGLang and DeepEP on master is sufficient. Also remember to upgrade Mooncake. It will be better to create customized expert distribution data for MTP (follow the related instructions in #6017)\n\n\n## xP + 2D, max_running_requests=32, draft_token_num=3\n\n### Command for decode\n```\nSGL_DISABLE_TP_MEMORY_INBALANCE_CHECK=1 SGLANG_DISAGGREGATION_HEARTBEAT_MAX_FAILURE=10000000 SGLANG_DEEPEP_NUM_MAX_DISPATCH_TOKENS_PER_RANK=256 MC_TE_METRIC=true python3 -m sglang.launch_server --model-path /mnt/shared-fs/models/deepseek-ai/DeepSeek-V3-0324 --disaggregation-ib-device mlx5_1 --disaggregation-mode decode --dist-init-addr 10.0.7.67:5757 --tp-size 16 --dp-size 16 --enable-dp-attention --decode-log-interval 1 --enable-deepep-moe --page-size 64 --host 0.0.0.0 --trust-remote-code --moe-dense-tp-size 1 --enable-dp-lm-head --disable-radix-cache --watchdog-timeout 1000000 --deepep-mode low_latency --mem-fraction-static 0.8 --max-running-requests 32 --context-length 73728 --ep-num-redundant-experts 32 --disable-shared-experts-fusion --cuda-graph-max-bs 2 --init-expert-location /mnt/shared-fs/stats-qiaolin/mtp_213.pt --speculative-algorithm EAGLE --speculative-num-steps 2 --speculative-eagle-topk 1 --speculative-num-draft-tokens 3 --nnodes 2 --node-rank 0\n```\n\n\n\n### Benchmark for decode\n```\n# slow down D nodes\ncurl -X POST -H 'Content-Type: application/json' 'http://10.0.7.67:30000/slow_down' -d '{\"forward_sleep_time\": 90.0}' \n\n# start benchmark; do not wait for this to finish before running the next line\npython3 -m sglang.bench_one_batch_server  --base-url http://10.5.38.77:8000 --model-path /mnt/shared-fs/models/deepseek-ai/DeepSeek-V3-0324 --batch-size 128 --input-len 65000 --output-len 4000 --skip-warmup \n\n# after some time (e.g. 10 minute), the D nodes are saturated, then this command should be executed\n# finish slowing down D nodes\ncurl -X POST -H 'Content-Type: application/json' 'http://10.0.7.67:30000/slow_down' -d '{\"forward_sleep_time\": null}' \n```\n\n\n\n## xP + 12D, max_running_requests=12288, draft_token_num=2\n### Command for decode\n```\nSGLANG_DEEPEP_NUM_MAX_DISPATCH_TOKENS_PER_RANK=512 SGLANG_NUM_RESERVED_DECODE_TOKENS=176 MC_TE_METRIC=true SGLANG_DISAGGREGATION_HEARTBEAT_INTERVAL=10000000 SGLANG_DISAGGREGATION_BOOTSTRAP_TIMEOUT=100000 SGLANG_DUMPER_DIR=/mnt/shared-fs/zbx/tmp SGLANG_EXPERT_DISTRIBUTION_RECORDER_DIR=/mnt/shared-fs/zbx/temp_sglang_server2local SGLANG_TORCH_PROFILER_DIR=/mnt/shared-fs/zbx/temp_sglang_server2local PYTHONUNBUFFERED=1 /home/ql/sglang_ql/bin/python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-V3-0324 --trust-remote-code --disaggregation-mode decode --dist-init-addr 10.5.45.38:5757 --nnodes 12 --node-rank 11 --tp-size 96 --dp-size 96 --enable-dp-attention --host 10.0.47.189 --decode-log-interval 1 --context-length 2176 --disable-radix-cache --enable-deepep-moe --moe-dense-tp-size 1 --enable-dp-lm-head --disable-shared-experts-fusion --watchdog-timeout 1000000 --enable-two-batch-overlap --disaggregation-ib-device mlx5_1 --disable-overlap-schedule --speculative-algo EAGLE --speculative-num-steps 1 --speculative-eagle-topk 1 --speculative-num-draft-tokens 2  --init-expert-location /mnt/shared-fs/configs/ep_statistics/decode_in2000out100.json --deepep-mode low_latency --mem-fraction-static 0.75 --cuda-graph-bs 128 --max-running-requests 12288 --ep-num-redundant-experts 32\n```\n\n\n### Benchmark for decode\n```\n# slow down D nodes\ncurl -X POST -H 'Content-Type: application/json' 'http://10.5.45.38:30000/slow_down' -d '{\"forward_sleep_time\": 90.0}' \n\n# start benchmark; do not wait for this to finish before running the next line\npython3 -m sglang.bench_one_batch_server  --base-url http://10.5.39.245:8000 --model-path /dev/shm/DeepSeek-V3-0324 --batch-size 24576 --input-len 2000 --output-len 100 --skip-warmup\n\n# after some time (e.g. 10 minute), the D nodes are saturated, then this command should be executed\n# finish slowing down D nodes\ncurl -X POST -H 'Content-Type: application/json' 'http://10.5.45.38:30000/slow_down' -d '{\"forward_sleep_time\": null}' \n```\n\nNote that since MTP doesn't support overlap scheduling yet, the performance in this case is still not optimal. We're actively working on it \u2014 stay tuned.\n",
    "labels": [
      "deepseek"
    ],
    "state": "open",
    "created_at": "2025-07-13T18:21:14+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7998/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7998"
  },
  {
    "number": 5313,
    "title": "[Feature] support NVRTC for DeepGEMM",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-04-12T06:17:24+00:00",
    "closed_at": "2025-05-13T08:45:22+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5313/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5313"
  },
  {
    "number": 3695,
    "title": "[Bug] sglang crashed when using /health_generate",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen deploying DeepSeek-V3/DeepSeek-R1 on two 8xH20 nodes, configuring a health probe (using health_generate) causes the service to crash after approximately 40 minutes.\n\nthe crash log:\n\n```2025-02-19 11:45:37 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n[2025-02-19 11:45:37] INFO:     172.17.0.109:43128 - \"GET /health_generate HTTP/1.1\" 200 OK\n[2025-02-19 11:45:42 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n[2025-02-19 11:45:46] Abort request a96186a3d0f8424eb67e822d7cc66c57\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/asyncio/locks.py\", line 214, in wait\n    await fut\nasyncio.exceptions.CancelledError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 456, in wait_for\n    return fut.result()\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 418, in _wait_one_response\n    await asyncio.wait_for(state.event.wait(), timeout=4)\n  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 458, in wait_for\n    raise exceptions.TimeoutError() from exc\nasyncio.exceptions.TimeoutError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/http_server.py\", line 137, in health_generate\n    async for _ in _global_state.tokenizer_manager.generate_request(gri, request):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 293, in generate_request\n    async for response in self._wait_one_response(obj, request):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 422, in _wait_one_response\n    raise ValueError(f\"Abort request {obj.rid}\")\nValueError: Abort request a96186a3d0f8424eb67e822d7cc66c57\n[2025-02-19 11:45:51] Abort request 3c93d049629f429ebfd86160bb321ac8\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/asyncio/locks.py\", line 214, in wait\n    await fut\nasyncio.exceptions.CancelledError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 456, in wait_for\n    return fut.result()\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 418, in _wait_one_response\n    await asyncio.wait_for(state.event.wait(), timeout=4)\n  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 458, in wait_for\n    raise exceptions.TimeoutError() from exc\nasyncio.exceptions.TimeoutError\n\nDuring handling of the above exception, another exception occurred:\n````\n\n### Reproduction\n\nrun command \n`python3 -m sglang.launch_server --model-path /opt/ml/ti/model_cache/deepseek-r1 --served-model-name ds-r1 --tp 16 --trust-remote-code --host 0.0.0.0 --port 8501 --context-length 16384 --enable-metrics --dist-init-addr x.x.x.x:23456 --nnodes 2 --node-rank 1 --enable-torch-compile --allow-auto-truncate  `\n\nmodel \nDeepSeek-V3/DeepSeek-R1\n\n### Environment\n\nPython: 3.10.16 (main, Dec  4 2024, 08:53:37) [GCC 9.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.5, V12.5.82\nCUDA Driver Version: 535.161.07\nPyTorch: 2.5.1+cu124\nflashinfer: 0.2.0.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.2\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.61.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     PHB     NODE    SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    PHB     PIX     NODE    SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    96-191,288-383  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    96-191,288-383  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE    NODE    PIX     PHB     96-191,288-383  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     NODE    NODE    PHB     PIX     96-191,288-383  1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC1    NODE    PIX     PHB     NODE    SYS     SYS     SYS     SYS     NODE     X      PHB     NODE    SYS     SYS     SYS     SYS\nNIC2    NODE    PHB     PIX     NODE    SYS     SYS     SYS     SYS     NODE    PHB      X      NODE    SYS     SYS     SYS     SYS\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PIX     PHB     SYS     SYS     SYS     SYS     NODE    NODE     X      PHB\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    PHB     PIX     SYS     SYS     SYS     SYS     NODE    NODE    PHB      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_bond_0\n  NIC1: mlx5_bond_1\n  NIC2: mlx5_bond_2\n  NIC3: mlx5_bond_3\n  NIC4: mlx5_bond_4\n  NIC5: mlx5_bond_5\n  NIC6: mlx5_bond_6\n  NIC7: mlx5_bond_7\n\n\nulimit soft: 1048576",
    "labels": [
      "inactive",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-19T10:45:57+00:00",
    "closed_at": "2025-05-26T00:19:58+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3695/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3695"
  },
  {
    "number": 4196,
    "title": "[Bug] Flashinferv0.2.2.post1 shows Unsupported max_mma_kv: 0 error on L40 , when deploying Deepseek-V2-Lite-chat with --enable-flashinfer-mla",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n    return self.forward_extend(\n    o, _ = self.prefill_wrapper_ragged.forward_return_lse(\n    return self.run_return_lse(q, k, v)\n    self._cached_module.ragged_run(*run_args)\n    ragged_run_func(\n    return self._op(*args, **(kwargs or {}))\n    return func(*args, **kwargs)\n    return self._op(*args, **(kwargs or {}))\nRuntimeError: Error in function 'BatchPrefillWithRaggedKVCacheDispatched' at /usr/local/lib/python3.10/dist-packages/flashinfer/data/include/flashinfer/attention/prefill.cuh:2215: Unsupported max_mma_kv: 0\n\n### Reproduction\n\npython -m sglang.launch_server --model-path DeepSeek-V2-Lite-Chat/  --tp 8 --trust-remote-code\n\n### Environment\n\nPython: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA L40\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.9\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.99\nCUDA Driver Version: 535.161.08\nPyTorch: 2.5.1+cu124\nsglang: 0.4.3.post2\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.2.post1+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.9.3\nfastapi: 0.115.6\nhf_transfer: Module Not Found\nhuggingface_hub: 0.27.1\ninteregular: 0.3.3\nmodelscope: Module Not Found\norjson: 3.10.15\npackaging: 23.2\npsutil: 5.9.4\npydantic: 2.10.5\nmultipart: 0.0.20\nzmq: 25.1.2\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.3\nopenai: 1.60.0\nanthropic: Module Not Found\nlitellm: Module Not Found\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PIX     PXB     PXB     PXB     PXB     PXB     PXB     NODE    SYS     SYS     0-31,64-95      0               N/A\nGPU1    PIX      X      PXB     PXB     PXB     PXB     PXB     PXB     NODE    SYS     SYS     0-31,64-95      0               N/A\nGPU2    PXB     PXB      X      PXB     PXB     PXB     PXB     PXB     NODE    SYS     SYS     0-31,64-95      0               N/A\nGPU3    PXB     PXB     PXB      X      PIX     PXB     PXB     PXB     NODE    SYS     SYS     0-31,64-95      0               N/A\nGPU4    PXB     PXB     PXB     PIX      X      PXB     PXB     PXB     NODE    SYS     SYS     0-31,64-95      0               N/A\nGPU5    PXB     PXB     PXB     PXB     PXB      X      PXB     PXB     NODE    SYS     SYS     0-31,64-95      0               N/A\nGPU6    PXB     PXB     PXB     PXB     PXB     PXB      X      PXB     NODE    SYS     SYS     0-31,64-95      0               N/A\nGPU7    PXB     PXB     PXB     PXB     PXB     PXB     PXB      X      NODE    SYS     SYS     0-31,64-95      0               N/A\nNIC0    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE     X      SYS     SYS\nNIC1    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      NODE\nNIC2    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_3\n  NIC2: mlx5_bond_0\n\n\nulimit soft: 1048576",
    "labels": [
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-03-08T05:53:08+00:00",
    "closed_at": "2025-03-08T18:02:25+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4196/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4196"
  },
  {
    "number": 3603,
    "title": "[Bug]  Two Node H20 with ROCE, can't startup",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nsglang cant's start when using two node h20 to deploy.... hang!\n\n### Reproduction\n\nI m trying  to enhance my h20*2 inference efficiency by using ROCE. \nThere were not prolblems using Socket Mode for NCCL.\n\nNow , i'm using the 0.4.2 offiicial image and i have add some rdma pacakges in it according the commit: https://github.com/FrankLeeeee/sglang/commit/cd837ab34a895976dc3176e0272a870090c59e36\n\nNow , sglang cant's start.\n\n<img width=\"1402\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e07db688-f7d2-4cd3-b699-70948ad85e5b\" />\n\nand the GPU-util is 100%!!!\n\n<img width=\"773\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b785d87d-cac6-4a19-b4be-2f989b57dadf\" />  \nNeed Help\n\n### Environment\n\ncontainerd.\nsglang 0.4.2\nroce 200Gb*4",
    "labels": [
      "help wanted",
      "high priority",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-16T06:31:52+00:00",
    "closed_at": "2025-02-18T05:34:59+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3603/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3603"
  },
  {
    "number": 3204,
    "title": "[Bug] CUDA error: uncorrectable ECC error encountered",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n```\n/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nMax context length: 163840\n2025-01-29 08:51:14.393720: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738140674.412162  464568 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738140674.417882  464568 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n[2025-01-29 08:51:25] server_args=ServerArgs(model_path='deepseek-ai/DeepSeek-V2.5', tokenizer_path='deepseek-ai/DeepSeek-V2.5', tokenizer_mode='auto', load_format='auto', trust_remote_code=True, dtype='bfloat16', kv_cache_dtype='auto', quantization_param_path=None, quantization=None, context_length=None, device='cuda', served_model_name='deepseek-ai/DeepSeek-V2.5', chat_template=None, is_embedding=False, revision=None, skip_tokenizer_init=False, host='127.0.0.1', port=1053, mem_fraction_static=0.9, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=8, stream_interval=1, stream_output=False, random_seed=646462308, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='sglang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_draft_model_path=None, speculative_algorithm=None, speculative_num_steps=5, speculative_num_draft_tokens=64, speculative_eagle_topk=8, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None)\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738140689.739817  464959 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738140689.745387  464959 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738140689.763624  464954 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738140689.769267  464954 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738140689.980293  464955 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738140689.986039  464955 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738140690.043146  464958 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738140690.048954  464958 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738140690.118109  464952 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738140690.124083  464952 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738140690.520316  464956 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738140690.525952  464956 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738140690.599351  464957 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738140690.605063  464957 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738140691.215711  464951 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738140691.224214  464951 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738140691.243279  464953 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738140691.251734  464953 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n[2025-01-29 08:51:41 TP6] MLA optimization is turned on. Use triton backend.\n[2025-01-29 08:51:41 TP6] Init torch distributed begin.\n[2025-01-29 08:51:41 TP4] MLA optimization is turned on. Use triton backend.\n[2025-01-29 08:51:41 TP4] Init torch distributed begin.\n[2025-01-29 08:51:41 TP5] MLA optimization is turned on. Use triton backend.\n[2025-01-29 08:51:41 TP5] Init torch distributed begin.\n[2025-01-29 08:51:41 TP1] MLA optimization is turned on. Use triton backend.\n[2025-01-29 08:51:41 TP1] Init torch distributed begin.\n[2025-01-29 08:51:42 TP3] MLA optimization is turned on. Use triton backend.\n[2025-01-29 08:51:42 TP3] Init torch distributed begin.\n[2025-01-29 08:51:43 TP0] MLA optimization is turned on. Use triton backend.\n[2025-01-29 08:51:43 TP0] Init torch distributed begin.\n[2025-01-29 08:51:43 TP7] MLA optimization is turned on. Use triton backend.\n[2025-01-29 08:51:43 TP7] Init torch distributed begin.\n[2025-01-29 08:51:44 TP2] MLA optimization is turned on. Use triton backend.\n[2025-01-29 08:51:44 TP2] Init torch distributed begin.\n[2025-01-29 08:51:44 TP3] sglang is using nccl==2.21.5\n[2025-01-29 08:51:44 TP0] sglang is using nccl==2.21.5\n[2025-01-29 08:51:44 TP7] sglang is using nccl==2.21.5\n[2025-01-29 08:51:44 TP1] sglang is using nccl==2.21.5\n[2025-01-29 08:51:44 TP2] sglang is using nccl==2.21.5\n[2025-01-29 08:51:44 TP5] sglang is using nccl==2.21.5\n[2025-01-29 08:51:44 TP4] sglang is using nccl==2.21.5\n[2025-01-29 08:51:44 TP6] sglang is using nccl==2.21.5\n[2025-01-29 08:51:51 TP5] Load weight begin. avail mem=76.86 GB\n[2025-01-29 08:51:51 TP1] Load weight begin. avail mem=76.86 GB\n[2025-01-29 08:51:51 TP0] Load weight begin. avail mem=77.14 GB\n[2025-01-29 08:51:51 TP7] Load weight begin. avail mem=77.14 GB\n[2025-01-29 08:51:51 TP3] Load weight begin. avail mem=76.86 GB\n[2025-01-29 08:51:51 TP4] Load weight begin. avail mem=76.86 GB\n[2025-01-29 08:51:51 TP6] Load weight begin. avail mem=76.86 GB\n[2025-01-29 08:51:51 TP2] Load weight begin. avail mem=76.86 GB\n[2025-01-29 08:51:52 TP0] Using model weights format ['*.safetensors']\n[2025-01-29 08:51:52 TP5] Using model weights format ['*.safetensors']\n[2025-01-29 08:51:52 TP4] Using model weights format ['*.safetensors']\n[2025-01-29 08:51:52 TP1] Using model weights format ['*.safetensors']\n[2025-01-29 08:51:52 TP6] Using model weights format ['*.safetensors']\n[2025-01-29 08:51:52 TP7] Using model weights format ['*.safetensors']\n[2025-01-29 08:51:52 TP3] Using model weights format ['*.safetensors']\n[2025-01-29 08:51:52 TP2] Using model weights format ['*.safetensors']\nCache shape torch.Size([163840, 64])\n\nLoading safetensors checkpoint shards:   0% Completed | 0/55 [00:00<?, ?it/s]\n\nLoading safetensors checkpoint shards:   2% Completed | 1/55 [00:01<01:09,  1.28s/it]\n\nLoading safetensors checkpoint shards:   4% Completed | 2/55 [00:02<01:00,  1.13s/it]\n\nLoading safetensors checkpoint shards:   5% Completed | 3/55 [00:02<00:45,  1.13it/s]\n\nLoading safetensors checkpoint shards:   7% Completed | 4/55 [00:05<01:12,  1.42s/it]\n\nLoading safetensors checkpoint shards:   9% Completed | 5/55 [00:05<00:58,  1.18s/it]\n\nLoading safetensors checkpoint shards:  11% Completed | 6/55 [00:06<00:50,  1.03s/it]\n\nLoading safetensors checkpoint shards:  13% Completed | 7/55 [00:07<00:44,  1.07it/s]\n\nLoading safetensors checkpoint shards:  15% Completed | 8/55 [00:08<00:41,  1.13it/s]\n\nLoading safetensors checkpoint shards:  16% Completed | 9/55 [00:08<00:39,  1.18it/s]\n\nLoading safetensors checkpoint shards:  18% Completed | 10/55 [00:09<00:37,  1.21it/s]\n\nLoading safetensors checkpoint shards:  20% Completed | 11/55 [00:10<00:35,  1.24it/s]\n\nLoading safetensors checkpoint shards:  22% Completed | 12/55 [00:11<00:34,  1.24it/s]\n\nLoading safetensors checkpoint shards:  24% Completed | 13/55 [00:12<00:34,  1.23it/s]\n\nLoading safetensors checkpoint shards:  25% Completed | 14/55 [00:12<00:33,  1.21it/s]\n\nLoading safetensors checkpoint shards:  27% Completed | 15/55 [00:13<00:31,  1.27it/s]\n\nLoading safetensors checkpoint shards:  29% Completed | 16/55 [00:14<00:30,  1.27it/s]\n\nLoading safetensors checkpoint shards:  31% Completed | 17/55 [00:15<00:29,  1.27it/s]\n\nLoading safetensors checkpoint shards:  33% Completed | 18/55 [00:16<00:29,  1.25it/s]\n\nLoading safetensors checkpoint shards:  35% Completed | 19/55 [00:16<00:29,  1.24it/s]\n\nLoading safetensors checkpoint shards:  36% Completed | 20/55 [00:17<00:28,  1.24it/s]\n\nLoading safetensors checkpoint shards:  38% Completed | 21/55 [00:18<00:27,  1.24it/s]\n\nLoading safetensors checkpoint shards:  40% Completed | 22/55 [00:19<00:26,  1.24it/s]\n\nLoading safetensors checkpoint shards:  42% Completed | 23/55 [00:19<00:24,  1.29it/s]\n\nLoading safetensors checkpoint shards:  44% Completed | 24/55 [00:20<00:22,  1.36it/s]\n\nLoading safetensors checkpoint shards:  45% Completed | 25/55 [00:21<00:23,  1.28it/s]\n\nLoading safetensors checkpoint shards:  47% Completed | 26/55 [00:22<00:23,  1.23it/s]\n\nLoading safetensors checkpoint shards:  49% Completed | 27/55 [00:23<00:22,  1.22it/s]\n\nLoading safetensors checkpoint shards:  51% Completed | 28/55 [00:24<00:22,  1.21it/s]\n\nLoading safetensors checkpoint shards:  53% Completed | 29/55 [00:24<00:20,  1.24it/s]\n\nLoading safetensors checkpoint shards:  55% Completed | 30/55 [00:26<00:25,  1.03s/it]\n\nLoading safetensors checkpoint shards:  56% Completed | 31/55 [00:27<00:22,  1.09it/s]\n\nLoading safetensors checkpoint shards:  58% Completed | 32/55 [00:27<00:19,  1.16it/s]\n\nLoading safetensors checkpoint shards:  60% Completed | 33/55 [00:29<00:27,  1.24s/it]\n\nLoading safetensors checkpoint shards:  62% Completed | 34/55 [00:30<00:23,  1.10s/it]\n\nLoading safetensors checkpoint shards:  64% Completed | 35/55 [00:31<00:19,  1.02it/s]\n\nLoading safetensors checkpoint shards:  65% Completed | 36/55 [00:32<00:17,  1.11it/s]\n\nLoading safetensors checkpoint shards:  67% Completed | 37/55 [00:32<00:14,  1.21it/s]\n\nLoading safetensors checkpoint shards:  69% Completed | 38/55 [00:33<00:13,  1.26it/s]\n\nLoading safetensors checkpoint shards:  71% Completed | 39/55 [00:34<00:12,  1.30it/s]\n\nLoading safetensors checkpoint shards:  73% Completed | 40/55 [00:34<00:11,  1.30it/s]\n\nLoading safetensors checkpoint shards:  75% Completed | 41/55 [00:36<00:14,  1.02s/it]\n\nLoading safetensors checkpoint shards:  76% Completed | 42/55 [00:37<00:12,  1.05it/s]\n\nLoading safetensors checkpoint shards:  78% Completed | 43/55 [00:38<00:10,  1.12it/s]\n\nLoading safetensors checkpoint shards:  80% Completed | 44/55 [00:38<00:09,  1.18it/s]\n\nLoading safetensors checkpoint shards:  82% Completed | 45/55 [00:39<00:08,  1.21it/s]\n\nLoading safetensors checkpoint shards:  84% Completed | 46/55 [00:40<00:07,  1.25it/s]\n\nLoading safetensors checkpoint shards:  85% Completed | 47/55 [00:41<00:06,  1.26it/s]\n\nLoading safetensors checkpoint shards:  87% Completed | 48/55 [00:41<00:05,  1.28it/s]\n\nLoading safetensors checkpoint shards:  89% Completed | 49/55 [00:42<00:04,  1.30it/s]\n\nLoading safetensors checkpoint shards:  91% Completed | 50/55 [00:44<00:04,  1.03it/s]\n\nLoading safetensors checkpoint shards:  93% Completed | 51/55 [00:44<00:03,  1.08it/s]\n\nLoading safetensors checkpoint shards:  95% Completed | 52/55 [00:45<00:02,  1.11it/s]\n\nLoading safetensors checkpoint shards:  96% Completed | 53/55 [00:46<00:01,  1.14it/s]\n\nLoading safetensors checkpoint shards:  98% Completed | 54/55 [00:47<00:00,  1.17it/s]\n[2025-01-29 08:52:39 TP6] Load weight end. type=DeepseekV2ForCausalLM, dtype=torch.bfloat16, avail mem=20.43 GB\n[2025-01-29 08:52:39 TP1] Load weight end. type=DeepseekV2ForCausalLM, dtype=torch.bfloat16, avail mem=20.43 GB\n[2025-01-29 08:52:39 TP4] Load weight end. type=DeepseekV2ForCausalLM, dtype=torch.bfloat16, avail mem=20.43 GB\n[2025-01-29 08:52:39 TP2] Load weight end. type=DeepseekV2ForCausalLM, dtype=torch.bfloat16, avail mem=20.43 GB\n\nLoading safetensors checkpoint shards: 100% Completed | 55/55 [00:48<00:00,  1.19it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 55/55 [00:48<00:00,  1.14it/s]\n\n[2025-01-29 08:52:40 TP3] Load weight end. type=DeepseekV2ForCausalLM, dtype=torch.bfloat16, avail mem=20.43 GB\n[2025-01-29 08:52:40 TP0] Load weight end. type=DeepseekV2ForCausalLM, dtype=torch.bfloat16, avail mem=20.71 GB\n[2025-01-29 08:52:40 TP5] Load weight end. type=DeepseekV2ForCausalLM, dtype=torch.bfloat16, avail mem=20.43 GB\n[2025-01-29 08:52:40 TP7] Load weight end. type=DeepseekV2ForCausalLM, dtype=torch.bfloat16, avail mem=20.71 GB\n[2025-01-29 08:52:41 TP6] Memory pool end. avail mem=6.41 GB\n[2025-01-29 08:52:41 TP4] Memory pool end. avail mem=6.41 GB\n[2025-01-29 08:52:41 TP3] Memory pool end. avail mem=6.41 GB\n[2025-01-29 08:52:41 TP5] Memory pool end. avail mem=6.41 GB\n[2025-01-29 08:52:41 TP0] Memory pool end. avail mem=6.69 GB\n[2025-01-29 08:52:41 TP1] Memory pool end. avail mem=6.41 GB\n[2025-01-29 08:52:41 TP2] Memory pool end. avail mem=6.41 GB\n[2025-01-29 08:52:41 TP7] Memory pool end. avail mem=6.69 GB\n[2025-01-29 08:52:41 TP0] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-01-29 08:52:41 TP6] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-01-29 08:52:41 TP4] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-01-29 08:52:41 TP3] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-01-29 08:52:41 TP1] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-01-29 08:52:41 TP7] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-01-29 08:52:41 TP0] Capture cuda graph begin. This can take up to several minutes.\n\n  0%|          | 0/23 [00:00<?, ?it/s][2025-01-29 08:52:41 TP6] Capture cuda graph begin. This can take up to several minutes.\n[2025-01-29 08:52:41 TP4] Capture cuda graph begin. This can take up to several minutes.\n[2025-01-29 08:52:41 TP3] Capture cuda graph begin. This can take up to several minutes.\n[2025-01-29 08:52:41 TP1] Capture cuda graph begin. This can take up to several minutes.\n[2025-01-29 08:52:41 TP5] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-01-29 08:52:41 TP2] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-01-29 08:52:41 TP7] Capture cuda graph begin. This can take up to several minutes.\n[2025-01-29 08:52:41 TP5] Capture cuda graph begin. This can take up to several minutes.\n[2025-01-29 08:52:41 TP2] Capture cuda graph begin. This can take up to several minutes.\nloc(\"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/attention/triton_ops/decode_attention.py\":310:16): error: operation scheduled before its operands\nloc(\"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/attention/triton_ops/decode_attention.py\":310:16): error: operation scheduled before its operands\nloc(\"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/attention/triton_ops/decode_attention.py\":310:16): error: operation scheduled before its operands\nloc(\"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/attention/triton_ops/decode_attention.py\":310:16): error: operation scheduled before its operands\nloc(\"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/attention/triton_ops/decode_attention.py\":310:16): error: operation scheduled before its operands\nloc(\"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/attention/triton_ops/decode_attention.py\":310:16): error: operation scheduled before its operands\nloc(\"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/attention/triton_ops/decode_attention.py\":310:16): error: operation scheduled before its operands\nloc(\"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/attention/triton_ops/decode_attention.py\":310:16): error: operation scheduled before its operands\n[2025-01-29 08:52:43 TP0] Using default MoE config. Performance might be sub-optimal! Config file not found at /home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=160,N=192,device_name=NVIDIA_A100-SXM4-80GB.json\n[2025-01-29 08:52:43 TP1] Using default MoE config. Performance might be sub-optimal! Config file not found at /home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=160,N=192,device_name=NVIDIA_A100-SXM4-80GB.json\n[2025-01-29 08:52:43 TP6] Using default MoE config. Performance might be sub-optimal! Config file not found at /home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=160,N=192,device_name=NVIDIA_A100-SXM4-80GB.json\n[2025-01-29 08:52:43 TP5] Using default MoE config. Performance might be sub-optimal! Config file not found at /home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=160,N=192,device_name=NVIDIA_A100-SXM4-80GB.json\n[2025-01-29 08:52:44 TP2] Using default MoE config. Performance might be sub-optimal! Config file not found at /home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=160,N=192,device_name=NVIDIA_A100-SXM4-80GB.json\n[2025-01-29 08:52:44 TP4] Using default MoE config. Performance might be sub-optimal! Config file not found at /home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=160,N=192,device_name=NVIDIA_A100-SXM4-80GB.json\n[2025-01-29 08:52:44 TP3] Using default MoE config. Performance might be sub-optimal! Config file not found at /home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=160,N=192,device_name=NVIDIA_A100-SXM4-80GB.json\n[2025-01-29 08:52:44 TP7] Using default MoE config. Performance might be sub-optimal! Config file not found at /home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=160,N=192,device_name=NVIDIA_A100-SXM4-80GB.json\n\n  4%|\u258d         | 1/23 [00:07<02:39,  7.23s/it]\n  9%|\u258a         | 2/23 [00:08<01:20,  3.82s/it][rank1]:[E129 08:52:51.685329145 ProcessGroupNCCL.cpp:1595] [PG ID 2 PG GUID 3 Rank 1] Process group watchdog thread terminated with exception: CUDA error: uncorrectable ECC error encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nFailed: Cuda error /workspace/csrc/custom_all_reduce.cuh:364 'uncorrectable ECC error encountered'\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nException raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79ebd4f6c446 in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x79ebd4f166e4 in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x79ec210a5a18 in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x79eb381c7726 in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0xa0 (0x79eb381cc3f0 in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x79eb381d3b5a in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79eb381d561d in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x145c0 (0x79ec23a1b5c0 in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libtorch.so)\nframe #8: <unknown function> + 0x94ac3 (0x79ec2da94ac3 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #9: <unknown function> + 0x126850 (0x79ec2db26850 in /lib/x86_64-linux-gnu/libc.so.6)\n\nterminate called after throwing an instance of 'c10::DistBackendError'\n  what():  [PG ID 2 PG GUID 3 Rank 1] Process group watchdog thread terminated with exception: CUDA error: uncorrectable ECC error encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nException raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79ebd4f6c446 in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x79ebd4f166e4 in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x79ec210a5a18 in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libc10_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x79eb381c7726 in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0xa0 (0x79eb381cc3f0 in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x79eb381d3b5a in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x79eb381d561d in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x145c0 (0x79ec23a1b5c0 in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libtorch.so)\nframe #8: <unknown function> + 0x94ac3 (0x79ec2da94ac3 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #9: <unknown function> + 0x126850 (0x79ec2db26850 in /lib/x86_64-linux-gnu/libc.so.6)\n\nException raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x79ebd4f6c446 in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0xe4271b (0x79eb37e4271b in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #2: <unknown function> + 0x145c0 (0x79ec23a1b5c0 in /home/ubuntu/.local/lib/python3.10/site-packages/torch/lib/libtorch.so)\nframe #3: <unknown function> + 0x94ac3 (0x79ec2da94ac3 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #4: <unknown function> + 0x126850 (0x79ec2db26850 in /lib/x86_64-linux-gnu/libc.so.6)\n\nFatal Python error: Aborted\n\nThread 0x000079e69b400640 (most recent call first):\n  File \"/usr/lib/python3.10/threading.py\", line 324 in wait\n  File \"/usr/lib/python3.10/threading.py\", line 607 in wait\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x000079e803c00640 (most recent call first):\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 47 in _recv_msg\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 153 in _read_thread\n  File \"/usr/lib/python3.10/threading.py\", line 953 in run\n  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x000079ec2dd59000 (most recent call first):\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/torch/_ops.py\", line 1116 in __call__\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/_custom_ops.py\", line 90 in get_graph_buffer_ipc_meta\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/distributed/device_communicators/custom_all_reduce.py\", line 320 in register_graph_buffers\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/distributed/device_communicators/custom_all_reduce.py\", line 317 in capture\n  File \"/usr/lib/python3.10/contextlib.py\", line 153 in __exit__\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/distributed/parallel_state.py\", line 325 in graph_capture\n  File \"/usr/lib/python3.10/contextlib.py\", line 153 in __exit__\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/distributed/parallel_state.py\", line 941 in graph_capture\n  File \"/usr/lib/python3.10/contextlib.py\", line 153 in __exit__\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/model_executor/cuda_graph_runner.py\", line 275 in capture\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/model_executor/cuda_graph_runner.py\", line 226 in __init__\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 730 in init_cuda_graphs\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 214 in __init__\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 68 in __init__\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63 in __init__\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 239 in __init__\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 1773 in run_scheduler_process\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108 in run\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314 in _bootstrap\n  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 129 in _main\n  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 116 in spawn_main\n  File \"<string>\", line 1 in <module>\n\nExtension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, _brotli, charset_normalizer.md, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, uvloop.loop, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, psutil._psutil_linux, psutil._psutil_posix, setproctitle, zmq.backend.cython._zmq, yaml._yaml, markupsafe._speedups, PIL._imaging, PIL._imagingft, google._upb._message, h5py._debian_h5py_serial._errors, h5py._debian_h5py_serial.defs, h5py._debian_h5py_serial._objects, h5py._debian_h5py_serial.h5, h5py._debian_h5py_serial.h5r, h5py._debian_h5py_serial.utils, h5py._debian_h5py_serial.h5s, h5py._debian_h5py_serial.h5ac, h5py._debian_h5py_serial.h5p, h5py._debian_h5py_serial.h5t, h5py._debian_h5py_serial._conv, h5py._debian_h5py_serial.h5z, h5py._debian_h5py_serial._proxy, h5py._debian_h5py_serial.h5a, h5py._debian_h5py_serial.h5d, h5py._debian_h5py_serial.h5ds, h5py._debian_h5py_serial.h5g, h5py._debian_h5py_serial.h5i, h5py._debian_h5py_serial.h5f, h5py._debian_h5py_serial.h5fd, h5py._debian_h5py_serial.h5pl, h5py._debian_h5py_serial.h5o, h5py._debian_h5py_serial.h5l, h5py._debian_h5py_serial._selector, h5py.atexit, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.h5r, h5py.utils, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5t, h5py._conv, h5py.h5z, h5py._proxy, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, scipy._lib._ccallback_c, scipy.sparse._sparsetools, scipy.sparse._csparsetools, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, jaxlib.cpu_feature_guard, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, sklearn.__check_build._check_build, sklearn.utils.murmurhash, lz4._version, lz4.frame._frame, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg._cythonized_array_utils, scipy.linalg._flinalg, scipy.linalg._solve_toeplitz, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg.cython_lapack, scipy.linalg._decomp_update, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial.transform._rotation, scipy.ndimage._nd_image, _ni_label, scipy.ndimage._ni_label, scipy.sparse.linalg._isolve._iterative, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.optimize._minpack2, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize.__nnls, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap_module, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate.dfitpack, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.special.cython_special, scipy.stats._stats, beta_ufunc, scipy.stats._boost.beta_ufunc, binom_ufunc, scipy.stats._boost.binom_ufunc, nbinom_ufunc, scipy.stats._boost.nbinom_ufunc, hypergeom_ufunc, scipy.stats._boost.hypergeom_ufunc, scipy.stats._biasedurn, scipy.stats._hypotests_pythran, scipy.stats._statlib, scipy.stats._mvn, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._unuran.unuran_wrapper, sklearn.utils._openmp_helpers, sklearn.utils._logistic_sigmoid, sklearn.utils.sparsefuncs_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.metrics._pairwise_fast, msgspec._core, sentencepiece._sentencepiece, regex._regex, msgpack._cmsgpack, ray._raylet, numba.core.typeconv._typeconv, numba._helperlib, numba._dynfunc, numba._dispatcher, numba.core.typing.builtins.itertools, numba.cpython.builtins.math, numba.core.runtime._nrt_python, numba.np.ufunc._internal, numba.experimental.jitclass._box, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, xxhash._xxhash, pyarrow._json, pyarrow._acero, pyarrow._csv, pyarrow._substrait, pyarrow._dataset, pyarrow._dataset_orc, pyarrow._parquet_encryption, pyarrow._dataset_parquet_encryption, pyarrow._dataset_parquet, cuda_utils, __triton_launcher (total: 262)\n```\n\n<img width=\"1624\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/31e1b189-1691-41b8-8a14-9d47cdbd5686\" />\n\n### Reproduction\n\n`deepseek-ai/DeepSeek-V2.5` on 8xA100 (80G), `--tp 8`\n\n\n### Environment\n\n```\nubuntu@207-211-174-109:~/gorilla/berkeley-function-call-leaderboard$ python3 -m sglang.check_env\n/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n2025-01-29 08:57:57.439587: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738141077.458842  481708 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738141077.464579  481708 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/home/ubuntu/.local/lib/python3.10/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n* 'fields' has been removed\n  warnings.warn(message, UserWarning)\nPython: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA A100-SXM4-80GB\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.0\nCUDA_HOME: /usr\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.127.05\nPyTorch: 2.5.1+cu124\nsglang: 0.4.2\nflashinfer: 0.1.6+cu121torch2.4\ntriton: 3.1.0\ntransformers: 4.48.1\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.7\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.0\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.60.2\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV12    NV12    NV12    NV12    NV12    NV12    NV12    PHB     0-239   0-1             N/A\nGPU1    NV12     X      NV12    NV12    NV12    NV12    NV12    NV12    PHB     0-239   0-1             N/A\nGPU2    NV12    NV12     X      NV12    NV12    NV12    NV12    NV12    PHB     0-239   0-1             N/A\nGPU3    NV12    NV12    NV12     X      NV12    NV12    NV12    NV12    PHB     0-239   0-1             N/A\nGPU4    NV12    NV12    NV12    NV12     X      NV12    NV12    NV12    PHB     0-239   0-1             N/A\nGPU5    NV12    NV12    NV12    NV12    NV12     X      NV12    NV12    PHB     0-239   0-1             N/A\nGPU6    NV12    NV12    NV12    NV12    NV12    NV12     X      NV12    PHB     0-239   0-1             N/A\nGPU7    NV12    NV12    NV12    NV12    NV12    NV12    NV12     X      PHB     0-239   0-1             N/A\nNIC0    PHB     PHB     PHB     PHB     PHB     PHB     PHB     PHB      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n\n\nHypervisor vendor: KVM\nulimit soft: 1048576\nubuntu@207-211-174-109:~/gorilla/berkeley-function-call-leaderboard$ \n```",
    "labels": [
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-01-29T08:59:43+00:00",
    "closed_at": "2025-02-03T19:53:54+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3204/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3204"
  },
  {
    "number": 3745,
    "title": "[Bug] torch.distributed.all_reduce raised Segmentation fault on 2 * 8 * H800",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nnode 1 server Log: \n\n\nFatal Python error: Segmentation fault\n\nThread 0x00007f2e93fff640 (most recent call first):\n  File \"/XXXX/sglang/python/sglang/srt/managers/scheduler.py\", line 462 in watchdog_thread\n  File \"/usr/lib/python3.10/threading.py\", line 953 in run\n  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nCurrent thread 0x00007f2ea0870640 (most recent call first):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\", line 2501 in all_reduce\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py\", line 83 in wrapper\n  File \"/XXXX/sglang/python/sglang/srt/distributed/parallel_state.py\", line 414 in _all_reduce_in_place\n  File \"/XXXX/sglang/python/sglang/srt/distributed/parallel_state.py\", line 112 in inplace_all_reduce\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 1116 in __call__\n  File \"/XXXX/sglang/python/sglang/srt/distributed/parallel_state.py\", line 398 in all_reduce\n  File \"/XXXX/sglang/python/sglang/srt/distributed/communication_op.py\", line 13 in tensor_model_parallel_all_reduce\n  File \"/XXXX/sglang/python/sglang/srt/models/deepseek_v2.py\", line 183 in forward\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747 in _call_impl\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736 in _wrapped_call_impl\n  File \"/XXXX/sglang/python/sglang/srt/models/deepseek_v2.py\", line 787 in forward\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747 in _call_impl\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736 in _wrapped_call_impl\n  File \"/XXXX/sglang/python/sglang/srt/models/deepseek_v2.py\", line 835 in forward\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747 in _call_impl\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736 in _wrapped_call_impl\n  File \"/XXXX/sglang/python/sglang/srt/models/deepseek_v2.py\", line 874 in forward\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n  File \"/XXXX/sglang/python/sglang/srt/model_executor/model_runner.py\", line 781 in forward_idle\n  File \"/XXXX/sglang/python/sglang/srt/model_executor/model_runner.py\", line 798 in forward\n  File \"/XXXX/sglang/python/sglang/srt/managers/tp_worker.py\", line 164 in forward_batch_generation\n  File \"/XXXX/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 140 in forward_thread_func_\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n  File \"/XXXX/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 109 in forward_thread_func\n  File \"/usr/lib/python3.10/threading.py\", line 953 in run\n  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f313cffd640 (most recent call first):\n  File \"/usr/lib/python3.10/threading.py\", line 324 in wait\n  File \"/usr/lib/python3.10/threading.py\", line 607 in wait\n  File \"/usr/local/lib/python3.10/dist-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f3ee6fc5640 (most recent call first):\n  File \"/usr/lib/python3.10/threading.py\", line 324 in wait\n  File \"/usr/lib/python3.10/threading.py\", line 607 in wait\n  File \"/usr/local/lib/python3.10/dist-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f4440b27480 (most recent call first):\n  File \"/usr/lib/python3.10/threading.py\", line 320 in wait\n  File \"/usr/lib/python3.10/queue.py\", line 171 in get\n  File \"/XXXX/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 169 in resolve_batch_result\n  File \"/XXXX/sglang/python/sglang/srt/managers/scheduler.py\", line 1123 in process_batch_result\n  File \"/XXXX/sglang/python/sglang/srt/managers/scheduler.py\", line 519 in event_loop_overlap\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n  File \"/XXXX/sglang/python/sglang/srt/managers/scheduler.py\", line 1825 in run_scheduler_process\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108 in run\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314 in _bootstrap\n  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 129 in _main\n  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 116 in spawn_main\n  File \"<string>\", line 1 in <module>\n\nExtension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, psutil._psutil_linux, psutil._psutil_posix, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, zmq.backend.cython._zmq, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, uvloop.loop, setproctitle, yaml._yaml, markupsafe._speedups, PIL._imaging, PIL._imagingft, msgspec._core, msgpack._cmsgpack, google._upb._message, ray._raylet, sentencepiece._sentencepiece, regex._regex, cuda_utils, __triton_launcher (total: 52)\nworker-0:981198:984678 [6] NCCL INFO [Service thread] Connection closed by localRank 5\n\n### Reproduction\n\nrun latest slang code(1eb8eade2bf6f69bf38c7d2706242775842131c5) with DeepSeek-R1(671B) on 2*8*H800, and run benchmark with gps 0.5, after a moment, you will see Segmentation fault error in server log of node(rank = 1).\n\nnode 0 server:\n\n#!/bin/bash\nNCCL_IB_DISABLE=0 \npython3 -m sglang.launch_server \n--watchdog-timeout 36000 \n--dist-init-addr dlc15h6z9j4v4llh-master-0:20000 \n--model-path /mnt/models/DeepSeek-R1 \n--nnodes 2 \n--node-rank 0 \n--log-level debug \n--port 18005 \n--context-length 12288 \n--chunked-prefill-size 8192 \n--tp 16 \n--dp 1 \n--schedule-policy random \n--load-balance-method round_robin \n--trust-remote-code \n--enable-dp-attention \n\\\n\nnode 1 server:\n\n#!/bin/bash\nNCCL_IB_DISABLE=0 \npython3 -m sglang.launch_server \n--watchdog-timeout 36000 \n--dist-init-addr dlc15h6z9j4v4llh-master-0:20000 \n--model-path /mnt/models/DeepSeek-R1 \n--nnodes 2 \n--node-rank 1 \n--log-level debug \n--port 18005 \n--context-length 12288 \n--chunked-prefill-size 8192 \n--tp 16 \n--dp 1 \n--schedule-policy random \n--load-balance-method round_robin \n--trust-remote-code \n--enable-dp-attention \n\\\n\nbenchmark by calling http://127.0.0.1:18005/v1/completion every 2 seconds (qps = 0.5)\n\n### Environment\n\naiohappyeyeballs==2.4.6\naiohttp==3.11.12\naiohttp-cors==0.7.0\naiosignal==1.3.2\nairportsdata==20241001\nannotated-types==0.7.0\nanthropic==0.45.2\nanyio==4.8.0\nargcomplete==3.5.3\nastor==0.8.1\nasttokens==3.0.0\nasync-timeout==5.0.1\nattrs==25.1.0\nblack==25.1.0\nblake3==1.0.4\nblinker==1.4\ncachetools==5.5.1\ncertifi==2025.1.31\ncharset-normalizer==3.4.1\nclick==8.1.8\ncloudpickle==3.1.1\ncolorful==0.5.6\ncompressed-tensors==0.9.1\ncryptography==3.4.8\ncuda-bindings==12.8.0\ncuda-python==12.8.0\ndatamodel-code-generator==0.27.3\ndbus-python==1.2.18\ndecorator==5.1.1\ndecord==0.6.0\ndepyf==0.18.0\ndill==0.3.9\ndiskcache==5.6.3\ndistlib==0.3.9\ndistro==1.7.0\ndistro-info==1.1+ubuntu0.2\neinops==0.8.1\nexceptiongroup==1.2.2\nexecuting==2.2.0\nfastapi==0.115.8\nfilelock==3.17.0\nflashinfer-python==0.2.1.post2+cu124torch2.5\nfrozenlist==1.5.0\nfsspec==2024.6.1\ngenson==1.3.0\ngguf==0.10.0\ngoogle-api-core==2.24.1\ngoogle-auth==2.38.0\ngoogleapis-common-protos==1.67.0\ngrpcio==1.70.0\nh11==0.14.0\nhf_transfer==0.1.9\nhtml5lib==1.1\nhttpcore==1.0.7\nhttplib2==0.20.2\nhttptools==0.6.4\nhttpx==0.28.1\nhuggingface-hub==0.28.1\nidna==3.10\nimportlib_metadata==8.6.1\ninflect==5.6.2\niniconfig==2.0.0\ninteregular==0.3.3\nipython==8.32.0\nisort==6.0.0\njedi==0.19.2\njeepney==0.7.1\nJinja2==3.1.5\njiter==0.8.2\njsonschema==4.23.0\njsonschema-specifications==2024.10.1\nkeyring==23.5.0\nlark==1.2.2\nlaunchpadlib==1.10.16\nlazr.restfulclient==0.14.4\nlazr.uri==1.0.6\nlitellm==1.61.2\nlm-format-enforcer==0.10.9\nloguru==0.7.3\nmarkdown-it-py==3.0.0\nMarkupSafe==3.0.2\nmatplotlib-inline==0.1.7\nmdurl==0.1.2\nmistral_common==1.5.3\nmodelscope==1.22.3\nmore-itertools==8.10.0\nmpmath==1.3.0\nmsgpack==1.1.0\nmsgspec==0.19.0\nmultidict==6.1.0\nmypy-extensions==1.0.0\nnest-asyncio==1.6.0\nnetworkx==3.3\nninja==1.11.1.3\nnumpy==1.26.4\nnvidia-cublas-cu12==12.4.5.8\nnvidia-cuda-cupti-cu12==12.4.127\nnvidia-cuda-nvrtc-cu12==12.4.127\nnvidia-cuda-runtime-cu12==12.4.127\nnvidia-cudnn-cu12==9.1.0.70\nnvidia-cufft-cu12==11.2.1.3\nnvidia-curand-cu12==10.3.5.147\nnvidia-cusolver-cu12==11.6.1.9\nnvidia-cusparse-cu12==12.3.1.170\nnvidia-cusparselt-cu12==0.6.2\nnvidia-ml-py==12.570.86\nnvidia-nccl-cu12==2.21.5\nnvidia-nvjitlink-cu12==12.4.127\nnvidia-nvtx-cu12==12.4.127\noauthlib==3.2.0\nopenai==1.63.0\nopencensus==0.11.4\nopencensus-context==0.1.3\nopencv-python-headless==4.11.0.86\norjson==3.10.15\noutlines==0.1.11\noutlines_core==0.1.26\npackaging==24.2\npandas==2.2.3\nparso==0.8.4\npartial-json-parser==0.2.1.1.post5\npathspec==0.12.1\npexpect==4.9.0\npillow==11.1.0\nplatformdirs==4.3.6\npluggy==1.5.0\nprometheus-fastapi-instrumentator==7.0.2\nprometheus_client==0.21.1\nprompt_toolkit==3.0.50\npropcache==0.2.1\nproto-plus==1.26.0\nprotobuf==5.29.3\npsutil==7.0.0\nptyprocess==0.7.0\npure_eval==0.2.3\npy-cpuinfo==9.0.0\npy-spy==0.4.0\npyasn1==0.6.1\npyasn1_modules==0.4.1\npybind11==2.13.6\npycountry==24.6.1\npydantic==2.10.6\npydantic_core==2.27.2\nPygments==2.19.1\nPyGObject==3.42.1\nPyJWT==2.3.0\npyparsing==2.4.7\npytest==8.3.4\npython-apt==2.4.0+ubuntu4\npython-dateutil==2.9.0.post0\npython-dotenv==1.0.1\npython-multipart==0.0.20\npytz==2025.1\nPyYAML==6.0.2\npyzmq==26.2.1\nray==2.42.1\nreferencing==0.36.2\nregex==2024.11.6\nrequests==2.32.3\nrich==13.9.4\nrpds-py==0.22.3\nrsa==4.9\nsafetensors==0.5.2\nSecretStorage==3.3.1\nsentencepiece==0.2.0\nsetproctitle==1.3.4\nsgl-kernel==0.0.3.post6\n-e git+https://github.com/sgl-project/sglang.git@1eb8eade2bf6f69bf38c7d2706242775842131c5#egg=sglang&subdirectory=python\nshellingham==1.5.4\nsix==1.17.0\nsmart-open==7.1.0\nsniffio==1.3.1\nssh-import-id==5.11\nstack-data==0.6.3\nstarlette==0.45.3\nsympy==1.13.1\ntiktoken==0.8.0\ntokenizers==0.21.0\ntomli==2.2.1\ntorch==2.5.1\ntorchao==0.8.0\ntorchaudio==2.5.1\ntorchvision==0.20.1\ntqdm==4.67.1\ntraitlets==5.14.3\ntransformers==4.48.3\ntriton==3.1.0\ntyper==0.15.1\ntyping_extensions==4.12.2\ntzdata==2025.1\nunattended-upgrades==0.1\nurllib3==2.3.0\nuvicorn==0.34.0\nuvloop==0.21.0\nvirtualenv==20.29.2\nvllm==0.7.2\nwadllib==1.3.6\nwatchfiles==1.0.4\nwcwidth==0.2.13\nwebencodings==0.5.1\nwebsockets==14.2\nwrapt==1.17.2\nxformers==0.0.28.post3\nxgrammar==0.1.10\nyarl==1.18.3\nzipp==3.21.0\n",
    "labels": [
      "inactive",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-21T06:32:03+00:00",
    "closed_at": "2025-06-09T00:20:51+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3745/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3745"
  },
  {
    "number": 3667,
    "title": "[Bug] run DeepSeek-R1 with --tp 2 --dp 2 --enable-dp-attention error",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nrun DeepSeek-R1 with enable-dp-attention error\n\nloc(\"/workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\":310:16): error: operation scheduled before its operands\nloc(\"/workspace/sglang/python/sglang/srt/layers/attention/triton_ops/decode_attention.py\":310:16): error: operation scheduled before its operands\n[2025-02-18 09:50:33 DP0 TP0] Using default MoE config. Performance might be sub-optimal! Config file not found at /workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=1024,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-02-18 09:50:33 DP1 TP1] Using default MoE config. Performance might be sub-optimal! Config file not found at /workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=1024,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128, 128].json\n 13%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                                                   | 3/23 [00:11<00:55,  2.79s/it]Failed: Cuda error /workspace/csrc/custom_all_reduce.cuh:368 'an illegal memory access was encountered'\n 13%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                                                   | 3/23 [00:12<01:21,  4.08s/it]\nFailed: Cuda error /workspace/csrc/custom_all_reduce.cuh:368 'an illegal memory access was encountered'\n[2025-02-18 09:50:38] DataParallelController hit an exception: Traceback (most recent call last):\n  File \"/workspace/sglang/python/sglang/srt/managers/data_parallel_controller.py\", line 236, in run_data_parallel_controller_process\n    controller = DataParallelController(server_args, port_args)\n  File \"/workspace/sglang/python/sglang/srt/managers/data_parallel_controller.py\", line 88, in __init__\n    dp_port_args = self.launch_dp_attention_schedulers(server_args, port_args)\n  File \"/workspace/sglang/python/sglang/srt/managers/data_parallel_controller.py\", line 139, in launch_dp_attention_schedulers\n    self.launch_tensor_parallel_group(server_args, port_args, 0, None)\n  File \"/workspace/sglang/python/sglang/srt/managers/data_parallel_controller.py\", line 192, in launch_tensor_parallel_group\n    scheduler_info.append(scheduler_pipe_readers[i].recv())\n  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n    buf = self._recv(4)\n  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 383, in _recv\n    raise EOFError\nEOFError\n\n[2025-02-18 09:50:38] Received sigquit from a child proces. It usually means the child failed.\n\n### Reproduction\n\npython -m sglang.launch_server --model-path /data/models/DeepSeek-R1 --disable-radix-cache --trust-remote-code --tp 2 --dp 2 --enable-dp-attention --json-model-override-args '{\"num_hidden_layers\": 10}' \n\non 8*h20\n\n### Environment\n\nroot@iv-ydp5an7thcay8n6jxmz4:/workspace# python -m sglang.check_env\nINFO 02-18 09:55:36 __init__.py:194] No platform detected, vLLM is running on UnspecifiedPlatform\nPython: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.161.08\nPyTorch: 2.5.1+cu124\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.1.post1+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.9.5\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.23.0\norjson: 3.10.15\npackaging: 24.0\npsutil: 5.9.8\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.0.3\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.63.2\ntiktoken: 0.9.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    SYS     PIX     NODE    SYS     SYS     0-89    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     PIX     NODE    SYS     SYS     0-89    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     NODE    PIX     SYS     SYS     0-89    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     NODE    PIX     SYS     SYS     0-89    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     PIX     NODE    90-179  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     PIX     NODE    90-179  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     NODE    PIX     90-179  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     NODE    PIX     90-179  1               N/A\nNIC0    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS\nNIC1    PIX     PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS      X      NODE    SYS     SYS\nNIC2    NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     SYS     NODE     X      SYS     SYS\nNIC3    SYS     SYS     SYS     SYS     PIX     PIX     NODE    NODE    SYS     SYS     SYS      X      NODE\nNIC4    SYS     SYS     SYS     SYS     NODE    NODE    PIX     PIX     SYS     SYS     SYS     NODE     X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n\n\nHypervisor vendor: KVM\nulimit soft: 1048576\nroot@iv-ydp5an7thcay8n6jxmz4:/workspace#\n",
    "labels": [
      "inactive",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-18T09:56:09+00:00",
    "closed_at": "2025-06-13T00:19:53+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3667/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3667"
  },
  {
    "number": 3398,
    "title": "deepseek-v3 cannot run multi-node under H20",
    "body": "hi there,\n\nI have followed the doc in https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3, \nhowever the server was still stuck when setting up. \n\n\n**First node:**\nifconfig\neth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\n        inet 11.130.1.53  netmask 255.255.248.0  broadcast 11.130.7.255\n\n**Below is the command:**\nexport NCCL_SOCKET_IFNAME=eth0\nexport GLOO_SOCKET_IFNAME=eth0\nexport NCCL_DEBUG=TRACE\n\npython3 -m sglang.launch_server --model-path /code/llm-benchmark-script/data/raw/DeepSeek-V3 --tp 16 --dist-init-addr 11.130.1.53:5000 --nnodes 2 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 6178\n\n===========================\n\n**Second node:**\nifconfig\neth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\n        inet 33.18.27.52  netmask 255.255.248.0  broadcast 33.18.31.255\n\n**Below is the command:**\nexport NCCL_SOCKET_IFNAME=eth0\nexport GLOO_SOCKET_IFNAME=eth0\nexport NCCL_DEBUG=TRACE\n\npython3 -m sglang.launch_server --model-path /code/llm-benchmark-script/data/raw/DeepSeek-V3 --tp 16 --dist-init-addr 11.130.1.53:5000 --nnodes 2 --node-rank 1 --trust-remote-code --host 0.0.0.0 --port 6178\n\n\n**Detail logs:**\n**First node logs:**\n\n[2025-02-08 00:24:47] server_args=ServerArgs(model_path='/code/llm-benchmark-script/data/raw/DeepSeek-V3', tokenizer_path='/code/llm-benchmark-script/data/raw/DeepSeek-V3', tokenizer_mode='auto', load_format='auto', trust_remote_code=True, dtype='auto', kv_cache_dtype='auto', quantization_param_path=Non\ne, quantization=None, context_length=None, device='cuda', served_model_name='/code/llm-benchmark-script/data/raw/DeepSeek-V3', chat_template=None, is_embedding=False, revision=None, skip_tokenizer_init=False, host='0.0.0.0', port=6178, mem_fraction_static=0.79, max_running_requests=None, max_total_token\ns=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=16, stream_interval=1, stream_output=False, random_seed=28357384, constrained_json_whitespace_pattern=None, watchdog_timeout=300, downl\noad_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='sglang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=\n'11.130.1.53:5000', nnodes=2, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_draft_model_path=None, speculative_algorithm=None, speculative_n\num_steps=5, speculative_num_draft_tokens=64, speculative_eagle_topk=8, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_jump_forward=False, disable_c\nuda_graph=True, disable_cuda_graph_padding=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_gra\nph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_\ncustom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False)\n[2025-02-08 00:25:02 TP3] MLA optimization is turned on. Use triton backend.\n[2025-02-08 00:25:02 TP3] Init torch distributed begin.\n[2025-02-08 00:25:05 TP6] MLA optimization is turned on. Use triton backend.\n[2025-02-08 00:25:05 TP6] Init torch distributed begin.\n[2025-02-08 00:25:05 TP1] MLA optimization is turned on. Use triton backend.\n[2025-02-08 00:25:05 TP1] Init torch distributed begin.\n[2025-02-08 00:25:05 TP7] MLA optimization is turned on. Use triton backend.\n[2025-02-08 00:25:05 TP7] Init torch distributed begin.\n[2025-02-08 00:25:05 TP5] MLA optimization is turned on. Use triton backend.\n[2025-02-08 00:25:05 TP5] Init torch distributed begin.\n[2025-02-08 00:25:05 TP0] MLA optimization is turned on. Use triton backend.\n[2025-02-08 00:25:05 TP0] Init torch distributed begin.\n[2025-02-08 00:25:05 TP4] MLA optimization is turned on. Use triton backend.\n[2025-02-08 00:25:05 TP4] Init torch distributed begin.\n[2025-02-08 00:25:05 TP2] MLA optimization is turned on. Use triton backend.\n[2025-02-08 00:25:05 TP2] Init torch distributed begin.\n[2025-02-08 00:25:21 TP0] sglang is using nccl==2.21.5\n[2025-02-08 00:25:21 TP1] sglang is using nccl==2.21.5\n[2025-02-08 00:25:21 TP2] sglang is using nccl==2.21.5\n[2025-02-08 00:25:21 TP3] sglang is using nccl==2.21.5\n[2025-02-08 00:25:21 TP4] sglang is using nccl==2.21.5\n[2025-02-08 00:25:21 TP5] sglang is using nccl==2.21.5\n[2025-02-08 00:25:21 TP6] sglang is using nccl==2.21.5\n[2025-02-08 00:25:21 TP7] sglang is using nccl==2.21.5\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75204:75204 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75204:75204 [0] NCCL INFO Bootstrap : Using eth0:11.130.1.53<0>\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75204:75204 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75204:75204 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75204:75204 [0] NCCL INFO NET/Plugin: Using internal network plugin.\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75204:75204 [0] NCCL INFO cudaDriverVersion 12040\nNCCL version 2.21.5+cuda12.4\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO cudaDriverVersion 12040\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO Bootstrap : Using eth0:11.130.1.53<0>\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO NET/Plugin: Using internal network plugin.\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO NET/IB : No device found.\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO NET/Socket : Using [0]eth0:11.130.1.53<0>\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO Using non-device net plugin version 0\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO Using network Socket\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO ncclCommInitRank comm 0xb9c26e0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 7e000 commId 0x898b6d578585998c - Init START\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO MNNVL busId 0x7e000 fabric UUID 0.0 cliqueId 0x0 state 3 healthMask 0x0\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO Setting affinity for GPU 1 to 1fff,00000000,0000ffff,ffffffff\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO NCCL_NVLS_ENABLE set by environment to 0.\n\u3002\u3002\u3002\n\u3002\u3002\u3002\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO NET/Socket : Using [0]eth0:11.130.1.53<0>\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO Using non-device net plugin version 0\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO Using network Socket\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO ncclCommInitRank comm 0xb9b7660 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId 109000 commId 0x898b6d578585998c - Init START\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO MNNVL busId 0x109000 fabric UUID 0.0 cliqueId 0x0 state 3 healthMask 0x0\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO NCCL_NVLS_ENABLE set by environment to 0.\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO comm 0xb9b7660 rank 4 nRanks 16 nNodes 2 localRanks 8 localRank 4 MNNVL 0\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO P2P Chunksize set to 131072\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO Connected all rings\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO Connected all trees\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO TUNER/Plugin: Using internal tuner plugin.\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75208:75208 [4] NCCL INFO ncclCommInitRank comm 0xb9b7660 rank 4 nranks 16 cudaDev 4 nvmlDev 4 busId 109000 commId 0x898b6d578585998c - Init COMPLETE\n comm 0xafd57d0 rank 3 nranks 16 cudaDev 3 nvmlDev 3 busId c6000 commId 0x898b6d578585998c - Init COMPLETE\n[rank4]:[W208 00:55:32.441635613 socket.cpp:462] [c10d] waitForInput: poll for socket SocketImpl(fd=103, addr=[::ffff:11.130.1.53]:52372, remote=[::ffff:11.130.1.53]:5000) returned 0, likely a timeout\n[rank3]:[W208 00:55:32.441638000 socket.cpp:462] [c10d] waitForInput: poll for socket SocketImpl(fd=103, addr=[::ffff:11.130.1.53]:52362, remote=[::ffff:11.130.1.53]:5000) returned 0, likely a timeout\nor directory : when loading libnccl-tuner.so\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO TUNER/Plugin: Using internal tuner plugin.\nllm-bm-infer-service-prod-1738980864555tkay-4005863815:75205:75205 [1] NCCL INFO ncclCommInitRank comm 0xb9c26e0 rank 1 nranks 16 cudaDev 1 nvmlDev 1 busId 7e000 commId 0x898b6d578585998c - Init COMPLETE\n[rank1]:[W208 00:55:32.441637683 socket.cpp:462] [c10d] waitForInput: poll for socket SocketImpl(fd=103, addr=[::ffff:11.130.1.53]:52374, remote=[::ffff:11.130.1.53]:5000) returned 0, likely a timeout\n[rank6]:[W208 00:55:32.443873015 socket.cpp:487] [c10d] waitForInput: socket SocketImpl(fd=103, addr=[::ffff:11.130.1.53]:52370, remote=[::ffff:11.130.1.53]:5000) timed out after 1800000ms\n[rank1]:[W208 00:55:32.444259976 socket.cpp:487] [c10d] waitForInput: socket SocketImpl(fd=103, addr=[::ffff:11.130.1.53]:52374, remote=[::ffff:11.130.1.53]:5000) timed out after 1800000ms\n[rank3]:[W208 00:55:32.444220068 socket.cpp:487] [c10d] waitForInput: socket SocketImpl(fd=103, addr=[::ffff:11.130.1.53]:52362, remote=[::ffff:11.130.1.53]:5000) timed out after 1800000ms\n[rank4]:[W208 00:55:32.444221339 socket.cpp:487] [c10d] waitForInput: socket SocketImpl(fd=103, addr=[::ffff:11.130.1.53]:52372, remote=[::ffff:11.130.1.53]:5000) timed out after 1800000ms\n[2025-02-08 00:55:32 TP5] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 1787, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 240, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tp_worker.py\", line 68, in __init__\n    self.model_runner = ModelRunner(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_executor/model_runner.py\", line 178, in __init__\n    min_per_gpu_memory = self.init_torch_distributed()\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_executor/model_runner.py\", line 254, in init_torch_distributed\n    initialize_dp_attention(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/layers/dp_attention.py\", line 33, in initialize_dp_attention\n    _ATTN_TP_GROUP = GroupCoordinator(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/distributed/parallel_state.py\", line 202, in __init__\n    cpu_group = torch.distributed.new_group(ranks, backend=\"gloo\")\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py\", line 97, in wrapper\n    func_return = func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\", line 4565, in new_group\n    return _new_group_with_tag(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\", line 4648, in _new_group_with_tag\n    pg, pg_store = _new_process_group_helper(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\", line 1744, in _new_process_group_helper\n\n\u3002\u3002\u3002\n\u3002\u3002\u3002\n[2025-02-08 00:55:32] Received sigquit from a child proces. It usually means the child failed.\n[2025-02-08 00:55:32 TP4] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 1787, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 240, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tp_worker.py\", line 68, in __init__\n    self.model_runner = ModelRunner(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_executor/model_runner.py\", line 178, in __init__\n    min_per_gpu_memory = self.init_torch_distributed()\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_executor/model_runner.py\", line 254, in init_torch_distributed\n    initialize_dp_attention(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/layers/dp_attention.py\", line 33, in initialize_dp_attention\n    _ATTN_TP_GROUP = GroupCoordinator(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/distributed/parallel_state.py\", line 202, in __init__\n    cpu_group = torch.distributed.new_group(ranks, backend=\"gloo\")\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py\", line 97, in wrapper\n    func_return = func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\", line 4565, in new_group\n    return _new_group_with_tag(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\", line 4648, in _new_group_with_tag\n    pg, pg_store = _new_process_group_helper(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\", line 1744, in _new_process_group_helper\n    backend_class = ProcessGroupGloo(\ntorch.distributed.DistStoreError: wait timeout after 1800000ms, keys: /default_pg/0//38//cpu//0/0\n\n\n\n**Second node logs:**\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO NET/Socket : Using [0]eth0:33.18.27.52<0>\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO Using non-device net plugin version 0\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO Using network Socket\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO ncclCommInitRank comm 0xafd6da0 rank 10 nranks 16 cudaDev 2 nvmlDev 2 busId a2000 commId 0x898b6d578585998c - Init START\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO MNNVL busId 0xa2000 fabric UUID 0.0 cliqueId 0x0 state 3 healthMask 0x0\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO Setting affinity for GPU 2 to 1fff,00000000,0000ffff,ffffffff\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO NCCL_NVLS_ENABLE set by environment to 0.\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO comm 0xafd6da0 rank 10 nRanks 16 nNodes 2 localRanks 8 localRank 2 MNNVL 0\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO P2P Chunksize set to 131072\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[1] via P2P/IPC\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[1] via P2P/IPC\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO Connected all rings\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[3] via P2P/IPC\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[3] via P2P/IPC\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO Connected all trees\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO TUNER/Plugin: Using internal tuner plugin.\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88008:88008 [2] NCCL INFO ncclllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO cudaDriverVersion 12040\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO Bootstrap : Using eth0:33.18.27.52<0>\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO NET/Plugin: Using internal network plugin.\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO NET/IB : No device found.\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO NET/Socket : Using [0]eth0:33.18.27.52<0>\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO Using non-device net plugin version 0\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO Using network Socket\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO ncclCommInitRank comm 0xc560d30 rank 11 nranks 16 cudaDev 3 nvmlDev 3 busId c6000 commId 0x898b6d578585998c - Init START\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO MNNVL busId 0xc6000 fabric UUID 0.0 cliqueId 0x0 state 3 healthMask 0x0\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO Setting affinity for GPU 3 to 1fff,00000000,0000ffff,ffffffff\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO NCCL_NVLS_ENABLE set by environment to 0.\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO comm 0xc560d30 rank 11 nRanks 16 nNodes 2 localRanks 8 localRank 3 MNNVL 0\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO P2P Chunksize set to 131072\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[2] via P2P/IPC\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO Channel 01/0 : 11[3] -> 10[2] via P2P/IPC\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO Connected all rings\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO Channel 00/0 : 11[3] -> 12[4] via P2P/IPC\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO Channel 01/0 : 11[3] -> 12[4] via P2P/IPC\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO Connected all trees\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO TUNER/Plugin: Using internal tuner plugin.\nllm-bm-infer-service-prod-17389809006090t9a-1100014361:88009:88009 [3] NCCL INFO [2025-02-08 00:25:32 TP15] Custom allreduce is disabled because this process group spans across nodes.\n[2025-02-08 00:25:32 TP13] Custom allreduce is disabled because this process group spans across nodes.\n[2025-02-08 00:25:32 TP12] Custom allreduce is disabled because this process group spans across nodes.\n[2025-02-08 00:25:32 TP14] Custom allreduce is disabled because this process group spans across nodes.\n[2025-02-08 00:25:32 TP11] Custom allreduce is disabled because this process group spans across nodes.\n[2025-02-08 00:25:32 TP9] Custom allreduce is disabled because this process group spans across nodes.\n[2025-02-08 00:25:32 TP10] Custom allreduce is disabled because this process group spans across nodes.\n[2025-02-08 00:25:32 TP8] Custom allreduce is disabled because this process group spans across nodes.\n\n\n\n**Note**\nThat is to say the first node is killed after 1800s timeout. Acctually, the scheduler port 5000 is ready, while the serving port 6178 is not.\n\n\n\nAny suggestion is appreciated!",
    "labels": [
      "inactive",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-08T09:07:16+00:00",
    "closed_at": "2025-05-27T00:18:50+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3398/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3398"
  },
  {
    "number": 4204,
    "title": "[Bug] After enabling flashinfer-mla for DeepSeek R1, I observed no throughput performance improvement.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI found no performance improvement after enabling flashinfer-mla. In my environment (version 0.4.3-post2, H20*16, tp16), I compared 32 and 64 concurrency levels. The TTFT (Time to First Token) improved significantly:\n\n\u200c64 concurrency\u200c: TTFT decreased from 19s to 14s, but throughput remained unchanged or slightly decreased.\n\u200c32 concurrency\u200c: TTFT reduced from 10s to 7s, with similar throughput behavior.\nI am uncertain whether this is a general issue.\n\n### Reproduction\n\npython3 -m sglang.launch_server --model-path /models/deepseek --tp 16  --dist-init-addr $HEAD_IP:20000 --nnodes 2 --node-rank ${INDEX} --trust-remote-code --context-length 131072 --host 0.0.0.0 --port 8080 --enable-flashinfer-mla\n\n### Environment\n\n2025-03-08 09:54:03,188 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\nWARNING 03-08 09:54:05 cuda.py:23] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\nWARNING:mistral_common.tokens.tokenizers.multimodal:Warning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import of cv2 has been skipped.\n/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n* 'fields' has been removed\n  warnings.warn(message, UserWarning)\nPython: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.161.08\nPyTorch: 2.5.1+cu124\nsglang: 0.4.3.post2\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.1.post2\ntriton: 3.1.0\ntransformers: 4.48.2\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.9.3\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 23.2\npsutil: 5.9.4\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 25.1.2\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.60.2\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    SYS     SYS     NODE    0-47,96-143     0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    SYS     SYS     NODE    0-47,96-143     0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    PIX     SYS     SYS     NODE    0-47,96-143     0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    SYS     SYS     NODE    0-47,96-143     0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     PIX     NODE    SYS     48-95,144-191   1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     NODE    NODE    SYS     48-95,144-191   1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     NODE    PIX     SYS     48-95,144-191   1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     NODE    NODE    SYS     48-95,144-191   1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    SYS     SYS     NODE\nNIC1    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE     X      SYS     SYS     NODE\nNIC2    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS      X      NODE    SYS\nNIC3    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     NODE     X      SYS\nNIC4    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_3\n  NIC2: mlx5_4\n  NIC3: mlx5_5\n  NIC4: mlx5_bond_0\n\n\nulimit soft: 1048576",
    "labels": [
      "inactive",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-03-08T09:55:12+00:00",
    "closed_at": "2025-05-10T00:18:04+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4204/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4204"
  },
  {
    "number": 3371,
    "title": "[Bug] Error when Load DeepSeek-R1 Model in --enable-ep-moe",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nLoad DeepSeek-R1 Model error when --enable-ep-moe :\n<img width=\"766\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2380b1e5-b55c-43bd-98a8-1ad0a740454a\" />\n\n### Reproduction\n\npython3 -m sglang.launch_server --model-path /root/.cache/huggingface/models/DeepSeek-R1 --tp 16 --dist-init-addr ip:20000 --nnodes 2 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 40000 --enable-ep-moe\n\n### Environment\n\nHardware environment\uff1a2 Nodes * 8 H100 GPU",
    "labels": [
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-07T10:20:01+00:00",
    "closed_at": "2025-02-25T19:25:40+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3371/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3371"
  },
  {
    "number": 6017,
    "title": "Instruction for Running DeepSeek with Large-scale PD and EP",
    "body": "## Using main branch\n\n~~NOTE: The feature is already on main, but the performance still needs some improvements on main branch.~~ will be good after a few already opened PRs - PR 6680, 6727, 6728\n\n~~NOTE: I will try other config like 4 node for P and 9 node for D later.~~ updated\n\n### Environment Preparation\n\nUse SGLang and DeepEP on master is sufficient. Also remember to upgrade Mooncake.\n\n### 4P + 9D experiments\n\nStart server\nwhere DeepEP config can be tuned by https://github.com/sgl-project/sglang/pull/6742\n\n```python\n# prefill nodes\nMC_TE_METRIC=true SGLANG_TBO_DEBUG=1 python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-V3-0324 --disaggregation-ib-device mlx5_1 --disaggregation-mode prefill --dist-init-addr 10.5.55.3:5757 --nnodes 4 --node-rank 0 --tp-size 32 --dp-size 32 --enable-dp-attention --decode-log-interval 1 --enable-deepep-moe --page-size 1 --host 0.0.0.0 --trust-remote-code --moe-dense-tp-size 1 --enable-dp-lm-head --disable-radix-cache --watchdog-timeout 1000000 --enable-two-batch-overlap --deepep-mode normal --mem-fraction-static 0.85 --chunked-prefill-size 524288 --max-running-requests 8192 --max-total-tokens 131072 --context-length 8192 --init-expert-location YOUR_PATH --ep-num-redundant-experts 32 --ep-dispatch-algorithm dynamic --eplb-algorithm deepseek --deepep-config YOUR_PATH\n\n# decode nodes\nMC_TE_METRIC=true SGLANG_TBO_DEBUG=1 python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-V3-0324 --disaggregation-ib-device mlx5_1 --disaggregation-mode decode --dist-init-addr 10.5.55.7:5757 --nnodes 9 --node-rank 0 --tp-size 72 --dp-size 72 --enable-dp-attention --decode-log-interval 1 --enable-deepep-moe --page-size 1 --host 0.0.0.0 --trust-remote-code --moe-dense-tp-size 1 --enable-dp-lm-head --disable-radix-cache --watchdog-timeout 1000000 --enable-two-batch-overlap --deepep-mode low_latency --mem-fraction-static 0.835 --max-running-requests 18432 --context-length 4500 --init-expert-location YOUR_PATH --ep-num-redundant-experts 32 --cuda-graph-bs 256 --num-reserved-decode-tokens YOUR_VALUE\n\n# load balancer\npython3 -m sglang.srt.disaggregation.mini_lb --prefill \"http://YOUR_FIRST_PREFILL_NODE_IP:30000\" --decode \"http://YOUR_FIRST_DECODE_NODE_IP:30000\"\n```\n\nBenchmark for prefill\n\n```\n# benchmark\npython3 -m sglang.bench_one_batch_server --model-path ${model_path} --base-url http://YOUR_IP:8000 --batch-size 8192 --input-len 4096 --output-len 5 --skip-warmup\n```\n\nBenchmark for decode\n\n- It is suggested to use 3 prefill nodes and 9 decode nodes to reproduce our results, since 9 decode nodes is half the size of that in DeepSeek\u2019s blog.\n- `SGLANG_HACK_PD_DECODE_NUM_RESERVED_DECODE_TOKENS` can be set to `benchmark-output-len + 2` to maximize batch size.\n- The example below demonstrates how to use the slow_down debug feature to stress test decode nodes when there are not enough prefill nodes. If your test workload has enough prefill nodes, this can be omitted.\n\n```\n# slow down D nodes\ncurl -H \"Content-Type: application/json\" -d '{\"forward_sleep_time\": 90.0}' -X POST \"http://YOUR_FIRST_DECODE_NODE_IP:30000/slow_down\"\n\n# start benchmark; do not wait for this to finish before running the next line\npython3 -m sglang.bench_one_batch_server --model-path /dev/shm/DeepSeek-V3-0324 --base-url http://10.10.37.16:7000 --batch-size 40000 --input-len 2000 --output-len 100 --skip-warmup\n\n# after some time (e.g. 10 minute), the D nodes are saturated, then this command should be executed\n# finish slowing down D nodes\ncurl -H \"Content-Type: application/json\" -d '{\"forward_sleep_time\": null}' -X POST \"http://YOUR_FIRST_DECODE_NODE_IP:30000/slow_down\"\n```\n\n### 4P + 9D + dynamic EPLB\n\nMay still have room for improvements, just preliminary tests.\n\n```\n# prefill\nMC_TE_METRIC=true SGLANG_TORCH_PROFILER_DIR=/host_home/temp_sglang_server2local SGLANG_TBO_DEBUG=1 PYTHONUNBUFFERED=1 python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-V3-0324 --disaggregation-ib-device mlx5_1 --disaggregation-mode prefill --dist-init-addr 10.5.55.3:5757 --nnodes 4 --node-rank 0 --tp-size 32 --dp-size 32 --enable-dp-attention --decode-log-interval 1 --enable-deepep-moe --page-size 1 --host 0.0.0.0 --trust-remote-code --moe-dense-tp-size 1 --enable-dp-lm-head --disable-radix-cache --watchdog-timeout 1000000 --enable-two-batch-overlap --deepep-mode normal --mem-fraction-static 0.85 --chunked-prefill-size 524288 --max-running-requests 8192 --max-total-tokens 65536 --context-length 8192 --enable-eplb --ep-num-redundant-experts 32 --eplb-rebalance-num-iterations YOUR_VALUE --ep-dispatch-algorithm dynamic --deepep-config YOUR_PATH\n\n# decode\nMC_TE_METRIC=true SGLANG_TORCH_PROFILER_DIR=/host_home/temp_sglang_server2local SGLANG_TBO_DEBUG=1 PYTHONUNBUFFERED=1 python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-V3-0324 --disaggregation-ib-device mlx5_1 --disaggregation-mode decode --dist-init-addr 10.5.55.7:5757 --nnodes 9 --node-rank 0 --tp-size 72 --dp-size 72 --enable-dp-attention --decode-log-interval 1 --enable-deepep-moe --page-size 1 --host 0.0.0.0 --trust-remote-code --moe-dense-tp-size 1 --enable-dp-lm-head --disable-radix-cache --watchdog-timeout 1000000 --enable-two-batch-overlap --deepep-mode low_latency --mem-fraction-static 0.82 --max-running-requests 18432 --context-length 4500 --enable-eplb --ep-num-redundant-experts 32 --eplb-rebalance-num-iterations YOUR_VALUE --cuda-graph-bs 256  --num-reserved-decode-tokens YOUR_VALUE\n```\n\n### Create expert distribution data\n\nNeed PR 6964, 6967\n\n```\n# prefill\nSGLANG_DISAGGREGATION_THREAD_POOL_SIZE=4 MC_TE_METRIC=true SGLANG_TORCH_PROFILER_DIR=/host_home/temp_sglang_server2local SGLANG_EXPERT_DISTRIBUTION_RECORDER_DIR=/host_home/temp_sglang_server2local SGLANG_TBO_DEBUG=1 PYTHONUNBUFFERED=1 python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-V3-0324 --disaggregation-ib-device mlx5_1 --disaggregation-mode prefill --dist-init-addr 10.5.55.1:5757 --nnodes 4 --node-rank 0 --tp-size 32 --dp-size 32 --enable-dp-attention --decode-log-interval 1 --enable-deepep-moe --page-size 1 --host 0.0.0.0 --trust-remote-code --moe-dense-tp-size 1 --enable-dp-lm-head --disable-radix-cache --watchdog-timeout 1000000 --enable-two-batch-overlap --expert-distribution-recorder-mode stat --disable-overlap-schedule --expert-distribution-recorder-buffer-size -1 --deepep-mode normal --mem-fraction-static 0.82 --chunked-prefill-size 524288 --max-running-requests 8192 --max-total-tokens 131072 --context-length 8192 --ep-num-redundant-experts 32 --ep-dispatch-algorithm dynamic --eplb-algorithm deepseek --deepep-config /host_home/primary_synced/tom_sglang_server/misc/deepep_vp.json\n\n# decode\nMC_TE_METRIC=true SGLANG_TORCH_PROFILER_DIR=/host_home/temp_sglang_server2local SGLANG_EXPERT_DISTRIBUTION_RECORDER_DIR=/host_home/temp_sglang_server2local SGLANG_TBO_DEBUG=1 PYTHONUNBUFFERED=1 python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-V3-0324 --disaggregation-ib-device mlx5_1 --disaggregation-mode decode --dist-init-addr 10.5.55.5:5757 --nnodes 9 --node-rank 0 --tp-size 72 --dp-size 72 --enable-dp-attention --decode-log-interval 1 --enable-deepep-moe --page-size 1 --host 0.0.0.0 --trust-remote-code --moe-dense-tp-size 1 --enable-dp-lm-head --disable-radix-cache --watchdog-timeout 1000000 --enable-two-batch-overlap --expert-distribution-recorder-mode stat --disable-overlap-schedule --expert-distribution-recorder-buffer-size -1 --deepep-mode low_latency --mem-fraction-static 0.81 --max-running-requests 18432 --context-length 4500 --ep-num-redundant-experts 32 --cuda-graph-bs 256  --num-reserved-decode-tokens YOUR_VALUE\n\ncurl -X POST -H 'Content-Type: application/json' 'http://10.5.55.1:30000/start_expert_distribution_record' -d '{}' \ncurl -X POST -H 'Content-Type: application/json' 'http://10.5.55.5:30000/start_expert_distribution_record' -d '{}' \ncurl -X POST -H 'Content-Type: application/json' 'http://10.5.55.5:30000/slow_down' -d '{\"forward_sleep_time\": 90.0}' \npython3 -m sglang.bench_one_batch_server  --base-url http://10.5.55.1:8000 --model-path /dev/shm/DeepSeek-V3-0324 --batch-size 40000 --input-len 2000 --output-len 100 --skip-warmup \n# after a while\ncurl -X POST -H 'Content-Type: application/json' 'http://10.5.55.5:30000/slow_down' -d '{\"forward_sleep_time\": null}' \n# after a while\ncurl -X POST -H 'Content-Type: application/json' 'http://10.5.55.1:30000/dump_expert_distribution_record' -d '{}' \ncurl -X POST -H 'Content-Type: application/json' 'http://10.5.55.5:30000/dump_expert_distribution_record' -d '{}' \n```\n\nThen you will get one .pt file for prefill and one for decode. They can be used in --init-expert-location.\n\n## Using the blog branch\n\n<details>\n\n### Environment Preparation\n\n- Install SGLang on branch https://github.com/sgl-project/sglang/tree/deepseek_ep\n    - ~~https://github.com/sgl-project/sglang/pull/5524~~ (EDIT: do not use this branch since I am adding more code to it after the blog, please use deepseek_ep instead)\n- ~~Install DeepEP on branch https://github.com/deepseek-ai/DeepEP/pull/142~~\n    - 2025.05.08 UPDATE: Directly use latest DeepEP main is enough, since my PR has been merged\n- Install latest mooncake\n\nIt is suggested to use this Dockerfile https://github.com/sgl-project/sglang/blob/main/docker/Dockerfile.deepep to prepare dependencies of DeepEP.\n\n### Stress-testing Prefill Nodes\n\n```python\n# prefill nodes\nMC_TE_METRIC=true SGLANG_HACK_DEEPEP_NEW_MODE=0 SGL_ENABLE_JIT_DEEPGEMM=1 python3 -m sglang.launch_server --model-path ${model_path} --disaggregation-mode prefill --disaggregation-ib-device ${device_name} --host ${node_ip} --trust-remote-code --dist-init-addr ${master_ip}:5757 --nnodes ${num_prefill} --node-rank ${node_rank} --tp-size $((${num_prefill}*8)) --dp-size $((${num_prefill}*8)) --enable-dp-attention --enable-deepep-moe --deepep-mode normal --mem-fraction-static 0.85 --chunked-prefill-size $((${num_prefill}*131072)) --max-running-requests $((${num_prefill}*2048)) --max-total-tokens 131072 --context-length 8192 --init-expert-location YOUR_EXPERT_LOCATION_HERE --ep-num-redundant-experts 32 --enable-two-batch-overlap --moe-dense-tp-size 1 --disable-radix-cache --ep-dispatch-algorithm random\n\n# decode nodes\nSGLANG_HACK_DEEPEP_NEW_MODE=0 SGLANG_HACK_PD_DECODE_NUM_RESERVED_DECODE_TOKENS=102 SGL_ENABLE_JIT_DEEPGEMM=1 python3 -m sglang.launch_server --model-path ${model_path} --disaggregation-mode decode --disaggregation-ib-device ${device_name} --host ${node_ip} --trust-remote-code --dist-init-addr ${master_ip}:5757 --nnodes ${num_decode} --node-rank ${node_rank} --tp-size $((${num_decode}*8)) --dp-size $((${num_decode}*8)) --enable-dp-attention --enable-deepep-moe --deepep-mode low_latency --mem-fraction-static 0.82 --max-running-requests $((${num_decode}*1024)) --context-length 4500 --init-expert-location YOUR_EXPERT_LOCATION_HERE --enable-two-batch-overlap --moe-dense-tp-size 1 --cuda-graph-bs 128 --disable-radix-cache --decode-log-interval 1\n\n# load balancer\npython3 -m sglang.srt.disaggregation.mini_lb --prefill \"http://YOUR_FIRST_PREFILL_NODE_IP:30000\" --decode \"http://YOUR_FIRST_DECODE_NODE_IP:30000\"\n\n# benchmark\npython3 -m sglang.bench_one_batch_server --model-path ${model_path} --base-url http://YOUR_IP:8000 --batch-size 8192 --input-len 4096 --output-len 5 --skip-warmup\n```\n\n### Stress-testing Decode Nodes\n\n```python\n# prefill nodes\nSGLANG_HACK_DEEPEP_NEW_MODE=0 SGL_ENABLE_JIT_DEEPGEMM=1 python3 -m sglang.launch_server --model-path ${model_path} --disaggregation-mode prefill --disaggregation-ib-device ${device_name} --host ${node_ip} --trust-remote-code --dist-init-addr ${master_ip}:5050 --nnodes ${num_prefill} --node-rank ${node_rank} --tp-size $((${num_prefill}*8)) --dp-size $((${num_prefill}*8)) --enable-dp-attention --enable-deepep-moe --deepep-mode normal --mem-fraction-static 0.85 --chunked-prefill-size $((${num_prefill}*65536)) --max-running-requests $((${num_prefill}*2048)) --max-total-tokens 131076 --context-length 8192 --init-expert-location YOUR_EXPERT_LOCATION_HERE --ep-num-redundant-experts 32 --enable-two-batch-overlap --moe-dense-tp-size 1 --disable-radix-cache\n\n# decode nodes\nSGLANG_HACK_DEEPEP_NEW_MODE=0 SGLANG_HACK_PD_DECODE_NUM_RESERVED_DECODE_TOKENS=YOUR_NUM_HERE SGL_ENABLE_JIT_DEEPGEMM=1 python3 -m sglang.launch_server --model-path ${model_path} --disaggregation-mode decode --disaggregation-ib-device ${device_name} --host ${node_ip} --trust-remote-code --dist-init-addr ${master_ip}:5050 --nnodes ${num_decode} --node-rank ${node_rank} --tp-size $((${num_decode}*8)) --dp-size $((${num_decode}*8)) --enable-dp-attention --enable-deepep-moe --deepep-mode low_latency --mem-fraction-static 0.846 --chunked-prefill-size 81920 --max-running-requests $((${num_decode}*2048)) --context-length 4096 --init-expert-location YOUR_EXPERT_LOCATION_HERE --ep-num-redundant-experts 32 --enable-two-batch-overlap --moe-dense-tp-size 1 --cuda-graph-bs 256 --disable-radix-cache --decode-log-interval 1\n\n# load balancer\npython3 -m sglang.srt.disaggregation.mini_lb --prefill \"http://YOUR_FIRST_PREFILL_NODE_IP:30000\" --decode \"http://YOUR_FIRST_DECODE_NODE_IP:30000\"\n\n# slow down D nodes\ncurl -H \"Content-Type: application/json\" -d '{\"forward_sleep_time\": 90.0}' -X POST \"http://YOUR_FIRST_DECODE_NODE_IP:30000/slow_down\"\n\n# start benchmark; do not wait for this to finish before running the next line\npython3 -m sglang.bench_one_batch_server --model-path /dev/shm/DeepSeek-V3-0324 --base-url http://10.10.37.16:7000 --batch-size 40000 --input-len 2000 --output-len 100 --skip-warmup\n\n# after some time (e.g. 10 minute), the D nodes are saturated, then this command should be executed\n# finish slowing down D nodes\ncurl -H \"Content-Type: application/json\" -d '{\"forward_sleep_time\": null}' -X POST \"http://YOUR_FIRST_DECODE_NODE_IP:30000/slow_down\"\n```\n\n</details>\n\n## Analyzing Results\n\nSince we are stress testing one side of P or D, we need to look at the server logs instead of benchmark script outputs.\n\n- Prefill: For logs like `Prefill batch. ... #new-token: 16384 ... gap_latency: 2.561`, the performance is `16384 / 2.561` token/second/device.\n- Decode: The result can be read from `gen throughput (token/s)` in the logs.\n\n## Remarks\n\n- Please ensure the batch size is full and avoid padding, because the performance is suboptimal otherwise due to a bug we will address soon.\n    - For example, to ensure a batch size of 256 for 72 decode GPUs, it is reasonable to send 40000 requests.\n- The sample command above only captures a CUDA graph of size 256 to save memory, which can be modified to suit your scenarios.\n- For optimal performance, you may need to tune components such as DeepEP on your cluster.\n- DeepGEMM warmup during execution will cause seemingly slow overall performance, and should be excluded from analyzation.\n- We rushed in the last few days, so the code is really ugly now with many hacks. We will make it elegant when merging into master.\n- For expert distribution statistics, our experiments use the same as input/output data and provide them as follows for reproducibility: [attachment_ep_statistics.zip](https://github.com/user-attachments/files/20036217/attachment_ep_statistics.zip)\n- To debug prefill performance, it may be useful to temporarily use `--ep-dispatch-algorithm fake_grouped_uniform` to simulate a fake perfect EPLB, and should match the corresponding performance reported in the blog\n- To analyze performance, it is suggested to use the log instead of benchmark script output, because the script output is mixed with the starting and ending part, where the system is not fully utilized and is slow.\n\n## Report Template\n\nIf you face any issues, feel free to discuss here or in Slack channel, and it would be great to provide the following information:\n\n* Full command to start server and benchmark\n* Logs of all server nodes and benchmark",
    "labels": [
      "collaboration",
      "deepseek"
    ],
    "state": "open",
    "created_at": "2025-05-05T04:48:15+00:00",
    "closed_at": null,
    "comments": 504,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6017/reactions",
      "total_count": 63,
      "+1": 57,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 6,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6017"
  },
  {
    "number": 3400,
    "title": "[Bug] 4x8 Mi210 Deepseek V3 runtime error",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI tried to start 4 nodes with 32 Mi210 GPUs to launch Deepseek R1 using sglang and I've already converted the weights to bf16.\n\n``` bash\n[2025-02-08 18:30:47] INFO:     Started server process [445197]\n[2025-02-08 18:30:47] INFO:     Waiting for application startup.\n[2025-02-08 18:30:47] INFO:     Application startup complete.\n[2025-02-08 18:30:47] INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)\n[2025-02-08 18:30:48] INFO:     127.0.0.1:40178 - \"GET /get_model_info HTTP/1.1\" 200 OK\n[2025-02-08 18:30:48 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n[2025-02-08 18:31:03 TP2] TpModelWorkerClient hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 104, in forward_thread_func\n    self.forward_thread_func_()\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 135, in forward_thread_func_\n    logits_output, next_token_ids = self.worker.forward_batch_generation(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 162, in forward_batch_generation\n    logits_output = self.model_runner.forward(forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 717, in forward\n    return self.forward_extend(forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 682, in forward_extend\n    return self.model.forward(\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 857, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch)\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 818, in forward\n    hidden_states, residual = layer(\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 756, in forward\n    hidden_states = self.self_attn(\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 513, in forward\n    return self.forward_normal(positions, hidden_states, forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 554, in forward_normal\n    attn_output = self.attn_mha(q, k, v, forward_batch, save_kv_cache=False)\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/radix_attention.py\", line 65, in forward\n    return forward_batch.attn_backend.forward(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/__init__.py\", line 69, in forward\n    return self.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_backend.py\", line 138, in forward_extend\n    self.extend_attention_fwd(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/extend_attention.py\", line 328, in extend_attention_fwd\n    _fwd_kernel[grid](\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 330, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/runtime/jit.py\", line 687, in run\n    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/triton/backends/amd/driver.py\", line 477, in __call__\n    self.launch(*args, **kwargs)\nRuntimeError: Triton Error [HIP]:  Code: 1, Messsage: invalid argument\n\nKilled\n```\n\n\n### Reproduction\n\n```python\npython3 -m sglang.launch_server --model-path /root/DeepSeek-R1-BF16 --tp 32 --dist-init-addr 10.204.8.46:20000 --nnodes 4 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 40000 --trust-remote-code --disable-cuda-graph\n\npython3 -m sglang.launch_server --model-path /root/DeepSeek-R1-BF16 --tp 32 --dist-init-addr 10.204.8.46:20000 --nnodes 4 --node-rank 1 --trust-remote-code --host 0.0.0.0 --port 40000 --trust-remote-code --disable-cuda-graph\n\npython3 -m sglang.launch_server --model-path /root/DeepSeek-R1-BF16 --tp 32 --dist-init-addr 10.204.8.46:20000 --nnodes 4 --node-rank 2 --trust-remote-code --host 0.0.0.0 --port 40000 --trust-remote-code --disable-cuda-graph\n\npython3 -m sglang.launch_server --model-path /root/DeepSeek-R1-BF16 --tp 32 --dist-init-addr 10.204.8.46:20000 --nnodes 4 --node-rank 3 --trust-remote-code --host 0.0.0.0 --port 40000 --trust-remote-code --disable-cuda-graph\n```\n\n### Environment\n\nI am using lmsysorg/sglang:v0.4.1.post4-rocm620 docker \n\n### Update\nafter changing image to lmsysorg/sglang:v0.4.2.post3-rocm630, it no longer failed at MLA but encountered IMA at MOE.\n\n",
    "labels": [
      "inactive",
      "amd",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-08T11:17:41+00:00",
    "closed_at": "2025-04-10T00:18:03+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3400/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3400"
  },
  {
    "number": 3758,
    "title": "[Feature] Optimizing DeepSeek with the DeepSeek Infra OSS component",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nref https://github.com/deepseek-ai/open-infra-index\n\n- [ ] https://github.com/deepseek-ai/DeepEP\n- [ ] https://github.com/deepseek-ai/DeepGEMM\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority",
      "performance",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-21T11:52:28+00:00",
    "closed_at": "2025-03-10T18:28:27+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3758/reactions",
      "total_count": 6,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 6,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/3758"
  },
  {
    "number": 5035,
    "title": "[Bug] Deepseek-v3-0324 Error",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nFailed to deploy Deepseek-v3-0324. \n(Deploy Deepseek-v3 successful.)\n\n```\n[2025-04-03 17:50:31 TP3] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 1787, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 240, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tp_worker.py\", line 68, in __init__\n    self.model_runner = ModelRunner(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_executor/model_runner.py\", line 186, in __init__\n    self.load_model()\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_executor/model_runner.py\", line 307, in load_model\n    self.model = get_model(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/__init__.py\", line 22, in get_model\n    return loader.load_model(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/loader.py\", line 362, in load_model\n    model.load_weights(self._get_all_weights(model_config, model))\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/models/deepseek_v2.py\", line 881, in load_weights\n    for name, loaded_weight in weights:\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/loader.py\", line 335, in _get_all_weights\n    yield from self._get_weights_iterator(primary_weights)\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/loader.py\", line 303, in _get_weights_iterator\n    hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/loader.py\", line 286, in _prepare_weights\n    hf_weights_files = filter_duplicate_safetensors_files(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/weight_utils.py\", line 322, in filter_duplicate_safetensors_files\n    weight_map = json.load(f)[\"weight_map\"]\n  File \"/usr/lib/python3.10/json/__init__.py\", line 293, in load\n    return loads(fp.read(),\n  File \"/usr/lib/python3.10/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/lib/python3.10/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/usr/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n[2025-04-03 17:50:31] Received sigquit from a child proces. It usually means the child failed.\n[2025-04-03 17:50:31 TP7] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 1787, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 240, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tp_worker.py\", line 68, in __init__\n    self.model_runner = ModelRunner(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_executor/model_runner.py\", line 186, in __init__\n    self.load_model()\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_executor/model_runner.py\", line 307, in load_model\n    self.model = get_model(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/__init__.py\", line 22, in get_model\n    return loader.load_model(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/loader.py\", line 362, in load_model\n    model.load_weights(self._get_all_weights(model_config, model))\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/models/deepseek_v2.py\", line 881, in load_weights\n    for name, loaded_weight in weights:\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/loader.py\", line 335, in _get_all_weights\n    yield from self._get_weights_iterator(primary_weights)\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/loader.py\", line 303, in _get_weights_iterator\n    hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/loader.py\", line 286, in _prepare_weights\n    hf_weights_files = filter_duplicate_safetensors_files(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/weight_utils.py\", line 322, in filter_duplicate_safetensors_files\n    weight_map = json.load(f)[\"weight_map\"]\n  File \"/usr/lib/python3.10/json/__init__.py\", line 293, in load\n    return loads(fp.read(),\n  File \"/usr/lib/python3.10/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/lib/python3.10/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/usr/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n[2025-04-03 17:50:31] Received sigquit from a child proces. It usually means the child failed.\n[2025-04-03 17:50:31 TP2] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 1787, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 240, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tp_worker.py\", line 68, in __init__\n    self.model_runner = ModelRunner(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_executor/model_runner.py\", line 186, in __init__\n    self.load_model()\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_executor/model_runner.py\", line 307, in load_model\n    self.model = get_model(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/__init__.py\", line 22, in get_model\n    return loader.load_model(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/loader.py\", line 362, in load_model\n    model.load_weights(self._get_all_weights(model_config, model))\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/models/deepseek_v2.py\", line 881, in load_weights\n    for name, loaded_weight in weights:\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/loader.py\", line 335, in _get_all_weights\n    yield from self._get_weights_iterator(primary_weights)\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/loader.py\", line 303, in _get_weights_iterator\n    hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/loader.py\", line 286, in _prepare_weights\n    hf_weights_files = filter_duplicate_safetensors_files(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/weight_utils.py\", line 322, in filter_duplicate_safetensors_files\n    weight_map = json.load(f)[\"weight_map\"]\n  File \"/usr/lib/python3.10/json/__init__.py\", line 293, in load\n    return loads(fp.read(),\n  File \"/usr/lib/python3.10/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/lib/python3.10/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/usr/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n[2025-04-03 17:50:31] Received sigquit from a child proces. It usually means the child failed.\n[2025-04-03 17:50:31 TP5] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 1787, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 240, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/tp_worker.py\", line 68, in __init__\n    self.model_runner = ModelRunner(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_executor/model_runner.py\", line 186, in __init__\n    self.load_model()\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_executor/model_runner.py\", line 307, in load_model\n    self.model = get_model(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/__init__.py\", line 22, in get_model\n    return loader.load_model(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/loader.py\", line 362, in load_model\n    model.load_weights(self._get_all_weights(model_config, model))\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/models/deepseek_v2.py\", line 881, in load_weights\n    for name, loaded_weight in weights:\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/loader.py\", line 335, in _get_all_weights\n    yield from self._get_weights_iterator(primary_weights)\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/loader.py\", line 303, in _get_weights_iterator\n    hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/loader.py\", line 286, in _prepare_weights\n    hf_weights_files = filter_duplicate_safetensors_files(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/model_loader/weight_utils.py\", line 322, in filter_duplicate_safetensors_files\n    weight_map = json.load(f)[\"weight_map\"]\n  File \"/usr/lib/python3.10/json/__init__.py\", line 293, in load\n    return loads(fp.read(),\n  File \"/usr/lib/python3.10/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/lib/python3.10/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/usr/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n[2025-04-03 17:50:31] Received sigquit from a child proces. It usually means the child failed.\nCache shape torch.Size([163840, 64])\n```\n\n### Reproduction\n\ndocker run --gpus all -dit --shm-size 10GB \\\n  --name deepseek-v3-0324 \\\n  -p 6400:6400 \\\n  -v /dev/models/DeepSeek-V3-0324:/DeepSeek-V3-0324 \\\n  lmsysorg/sglang:latest \\\n  sleep infinity\n\nnohup python3 -u -m sglang.launch_server \\\n  --model-path /cfs \\\n  --tp 8 \\\n  --trust-remote-code \\\n  --mem-fraction-static 0.9 \\\n  --host 0.0.0.0 \\\n  --port 6399 \\\n  --log-level debug \\\n  --tool-call-parser llama3 >> /cfs/ds3_infer.log 2>&1 &\n\n\nDeploy Deepseek-v3 successful.\n\n### Environment\n\nH20: 96GB * 8\ndocker image:\nlmsysorg/sglang:latest\n\nDeploy Deepseek-v3 successful.",
    "labels": [
      "inactive",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-04-03T12:08:54+00:00",
    "closed_at": "2025-07-16T00:20:42+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5035/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5035"
  },
  {
    "number": 4022,
    "title": "[Bug] running requests low",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen running the test script with 100 concurrent requests, the \"running\" count for sglang is relatively low, and there is a significant amount of data in the \"queue\", whereas the vllm service can reach a \"running\" count of 100.\n\n### Reproduction\n\n# evaluation script\n```\nevalscope perf \\\n    --url \"http://192.168.8.**:31008/v1/chat/completions\" \\\n    --parallel 100 \\\n    --model DeepSeek-R1 \\\n    --number 500 \\\n    --api openai \\\n    --dataset openqa \\\n    --stream\n```\n\n# sglang\n\nservice start command\uff1a\n```\nCUDA_VISIBLE_DEVICES=6 python3 -m sglang.launch_server --model-path /data/chdmx/models/LLM_models/deepseek_ai/DeepSeek-R1-Distill-Qwen-32B --tp 1 --mem-fraction-static 0.9 --context-length 20000 --trust-remote-code --host 0.0.0.0 --port 31008\n```\n\nresult:\n\n![Image](https://github.com/user-attachments/assets/3b42fb5b-cb84-4496-8dca-a74a50a416fd)\n\n\n# vllm\n\nservice start command\uff1a\n\n```\nCUDA_VISIBLE_DEVICES=6 vllm serve /data/chdmx/models/LLM_models/deepseek_ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 1 --gpu-memory-utilization 0.9 --max-model-len 20000 --trust-remote-code --served-model-name DeepSeek-R1 --port 31008    \n```\n\nresult:\n\n![Image](https://github.com/user-attachments/assets/1ce834d3-8471-41dd-a321-0dba4f35210f)\n\n\n\n### Environment\n\nroot@bms-schyjdmx06:/sgl-workspace# python3 -m sglang.check_env\nINFO 03-03 01:33:01 __init__.py:190] Automatically detected platform cuda.\n/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n* 'fields' has been removed\n  warnings.warn(message, UserWarning)\nPython: 3.10.16 (main, Dec  4 2024, 08:53:37) [GCC 9.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA A100 80GB PCIe\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.54.15\nPyTorch: 2.5.1+cu124\nsglang: 0.4.2.post4\nsgl_kernel: 0.0.3.post4\nflashinfer: 0.2.0.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.6\nhf_transfer: 0.1.9\nhuggingface_hub: 0.27.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.5\nmultipart: 0.0.20\nzmq: 26.2.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.59.8\nanthropic: 0.43.1\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PIX     PIX     PIX     SYS     SYS     SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\nGPU1    PIX      X      PIX     PIX     SYS     SYS     SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\nGPU2    PIX     PIX      X      PIX     SYS     SYS     SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\nGPU3    PIX     PIX     PIX      X      SYS     SYS     SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\nGPU4    SYS     SYS     SYS     SYS      X      PIX     PIX     PIX     SYS     SYS     28-55,84-111    1               N/A\nGPU5    SYS     SYS     SYS     SYS     PIX      X      PIX     PIX     SYS     SYS     28-55,84-111    1               N/A\nGPU6    SYS     SYS     SYS     SYS     PIX     PIX      X      PIX     SYS     SYS     28-55,84-111    1               N/A\nGPU7    SYS     SYS     SYS     SYS     PIX     PIX     PIX      X      SYS     SYS     28-55,84-111    1               N/A\nNIC0    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS\nNIC1    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n\n\nulimit soft: 1048576",
    "labels": [
      "help wanted",
      "inactive",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-03-03T09:35:57+00:00",
    "closed_at": "2025-05-03T00:18:13+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4022/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4022"
  },
  {
    "number": 5064,
    "title": "[Feature] attention backend default choice",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nThe standards we choose prioritize **performance first**, ease of use second (such as interface and installation), while also considering compatibility (such as older arch). Therefore, if in the future, the performance of different backends changes, we will still choose **the best performing one**.\n\n1. NVIDIA\n\n```\nsm75 -> Triton\nsm80, sm86, sm89 -> FlashInfer\nsm90 -> FA3 (Llama, Qwen, Gemma), FlashInfer (Others)\nsm100 -> FlashInfer\n\nMLA\nsm90 -> FA3 (DeepSeek)\nsm100 -> FlashInfer (DeepSeek)\n\nOther options\nFlashMLA, cuDNN etc\n```\n\nSGLang will install the JIT version of FlashInfer on PyPI for a better user installation experience. Alternatively, the whl size limit of FlashInfer can be increased on PyPI. cc @yzh119 \n\nFor FlashInfer, SGLang whl will use JIT version by default, in the Docker image using AOT.\n\nCurrently, FA3 is integrated in the [sgl-kernel](https://github.com/sgl-project/sglang/tree/main/sgl-kernel), which is more convenient for users to install and use than installing from [source code](https://github.com/Dao-AILab/flash-attention/tree/main/hopper).\n\n2. AMD\n\n```\nTriton\n```\n\n@HaiShaw is currently working on improving the performance of the attention backend.\n\n\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority",
      "collaboration",
      "flashinfer",
      "performance",
      "MLLM",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-04-04T08:13:51+00:00",
    "closed_at": "2025-05-21T09:29:52+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5064/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5064"
  },
  {
    "number": 5514,
    "title": "[Tracker] SGLang v0.4.5.post1 performance on H200",
    "body": "**Update**:\n**see the latest benchmark results in another post https://github.com/sgl-project/sglang/pull/5611#issuecomment-2819965621** \n\n\n```bash\n# launch server\n# First, warm up for DeepGEMM\n# SGLang uses FA3 backend by default since v0.4.5.post1\n# Use dp 8 for offline use case\nSGL_ENABLE_JIT_DEEPGEMM=1 python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code --enable-dp-attention --dp-size 8\n\n# Random 1k, 2k\npython3 -m sglang.bench_serving --backend sglang-oai --num-prompts 50 --request-rate 10 --dataset-name random --random-input-len 1000 --random-output-len 2000 --random-range-ratio 1\n\n# Random 5k, 1k\npython3 -m sglang.bench_serving --backend sglang-oai --num-prompts 50 --request-rate 10 --dataset-name random --random-input-len 5000 --random-output-len 1000 --random-range-ratio 1\n\n# Random 10k, 500\npython3 -m sglang.bench_serving --backend sglang-oai --num-prompts 50 --request-rate 10 --dataset-name random --random-input-len 10000 --random-output-len 500 --random-range-ratio 1\n\n# Random 30k, 100\npython3 -m sglang.bench_serving --backend sglang-oai --num-prompts 50 --request-rate 10 --dataset-name random --random-input-len 30000 --random-output-len 100 --random-range-ratio 1\n```\n\n![Image](https://github.com/user-attachments/assets/175f2238-0299-48f3-ae65-7878f8faf459)\n\n![Image](https://github.com/user-attachments/assets/f14d4bf4-c607-4b18-9fb6-4f30d1d7a5b4)\n\n![Image](https://github.com/user-attachments/assets/336c80f4-6f26-411a-8e54-e0d1a889dbe1)\n\n![Image](https://github.com/user-attachments/assets/18293871-be6c-4631-9e26-0a631ef6ddf5)",
    "labels": [
      "high priority",
      "collaboration",
      "performance",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-04-18T02:46:46+00:00",
    "closed_at": "2025-04-29T19:47:52+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5514/reactions",
      "total_count": 20,
      "+1": 10,
      "-1": 0,
      "laugh": 0,
      "hooray": 4,
      "confused": 0,
      "heart": 0,
      "rocket": 6,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5514"
  },
  {
    "number": 3472,
    "title": "[Track] DeepSeek V3/R1 nextn progress",
    "body": "## Triton Backend\n\n@ispobock @pankajroark \n\n- [x] [refactor triton backend 1](https://github.com/sgl-project/sglang/pull/3292), [2](https://github.com/sgl-project/sglang/pull/3309)\n\n- [x] [support custom mask](https://github.com/sgl-project/sglang/pull/3317)\n\n- [x] [support EAGLE 2](https://github.com/sgl-project/sglang/pull/3466)\n\n- [x] [compatible with CUDA Graph](https://github.com/sgl-project/sglang/pull/3500)\n\n- [x] [support nextn I (single MTP head)](https://github.com/sgl-project/sglang/pull/3582)\n\n- [x] support next II (multi MTP heads) (WIP @pankajroark )\n\n## FlashInfer Backend\n\n@zhyncs @yzh119 \n\n- [x] compatible with disable MLA\n\n- [x] support FlashInfer nightly MLA ragged prefill and CUDA Core MLA decoding\n\n- [x] support FlashInfer v0.2.0.post3 MLA ragged, paged prefill and decoding (@zhyncs @yzh119 )\n\n- [x] nextn parts can be shared with Triton Backend\n\n## EAGLE 2\n\n@zhyncs @Ying1123 \n\n- [x] implement sampling kernel in [sgl-kernel](https://github.com/sgl-project/sglang/tree/main/sgl-kernel) (drop cutex) [kernel part](https://github.com/sgl-project/sglang/pull/3373), [python part](https://github.com/sgl-project/sglang/pull/3378)\n\n- [x] bunch of fixes [non greedy fix](https://github.com/sgl-project/sglang/pull/3407), [disable cuda graph fix 1](https://github.com/sgl-project/sglang/pull/3412), [fix 2](https://github.com/sgl-project/sglang/pull/3411), [cleanup 1](https://github.com/sgl-project/sglang/pull/3415), [cleanup 2](https://github.com/sgl-project/sglang/pull/3422), [fix cuda graph capture failure](https://github.com/sgl-project/sglang/pull/3430), [fix 2](https://github.com/sgl-project/sglang/pull/3431), [reduce one draft forward](https://github.com/sgl-project/sglang/pull/3468)\n\n- [x] compatible with radix cache and chunked prefill (WIP @Ying1123 )",
    "labels": [
      "enhancement",
      "high priority",
      "flashinfer",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-10T14:46:03+00:00",
    "closed_at": "2025-03-25T04:13:25+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3472/reactions",
      "total_count": 16,
      "+1": 13,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/3472"
  },
  {
    "number": 4018,
    "title": "[Bug] KeyError: 'model.layers.0.mlp.down_proj.weight_scale_inv' when run deepseek 671b with 64 RTX 4090 GPU",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nrun deepseek r1 671b with 8 nodes,each nodes with 8 rtx4090 GPU,\nthen follow this problem delete [quantization_config](https://github.com/sgl-project/sglang/issues/3491#issuecomment-2650779851) \n\n![Image](https://github.com/user-attachments/assets/1990c4e7-cfac-4ef1-9d83-c8a27fb6c00a)\n\n### Reproduction\n\nrun 8 nodes one by one:\nexport GLOO_SOCKET_IFNAME=bond0.1593\nexport NCCL_SOCKET_IFNAME=bond0.1593\nexport NCCL_DEBUG=WARN\nexport NCCL_IB_DISABLE=1\n(python3 -m sglang.launch_server --model-path /mnt/DeepSeek-R1 --tp 64 --dist-init-addr 10.7.20.14:5000 --nnodes 8 --node-rank 0 --trust-remote-code  &)\n\n### Environment\n\nCUDA 12.4 with follow pkg installed\nroot@localhost:~# pip list\nPackage                           Version\n--------------------------------- -------------------------\nabsl-py                           2.1.0\naccelerate                        1.4.0\naddict                            2.4.0\naiofiles                          23.2.1\naiohappyeyeballs                  2.4.6\naiohttp                           3.11.12\naiohttp-cors                      0.7.0\naiosignal                         1.3.2\nairportsdata                      20241001\nannotated-types                   0.7.0\nanthropic                         0.46.0\nantlr4-python3-runtime            4.7.2\nanyio                             4.8.0\nastor                             0.8.1\nasttokens                         3.0.0\nasync-timeout                     5.0.1\nattrs                             25.1.0\nBabel                             2.8.0\nblake3                            1.0.4\nblinker                           1.4\ncachetools                        5.5.1\ncertifi                           2020.6.20\nchardet                           4.0.0\ncharset-normalizer                3.4.1\nclick                             8.0.3\ncloud-init                        23.2.2\ncloudpickle                       3.1.1\ncolorama                          0.4.4\ncolorful                          0.5.6\ncompressed-tensors                0.9.1\nconfigobj                         5.0.6\ncontourpy                         1.3.1\ncryptography                      3.4.8\ncuda-bindings                     12.8.0\ncuda-python                       12.8.0\ncycler                            0.12.1\ndatasets                          3.2.0\ndbus-python                       1.2.18\ndecorator                         5.1.1\ndecord                            0.6.0\ndepyf                             0.18.0\ndill                              0.3.8\ndiskcache                         5.6.3\ndistlib                           0.3.9\ndistro                            1.7.0\ndistro-info                       1.1+ubuntu0.1\neditdistance                      0.8.1\neinops                            0.8.1\nexceptiongroup                    1.2.2\nexecuting                         2.2.0\nfastapi                           0.115.8\nffmpy                             0.5.0\nfilelock                          3.17.0\nflashinfer-python                 0.2.1.post2+cu124torch2.5\nfonttools                         4.56.0\nfrozenlist                        1.5.0\nfsspec                            2024.9.0\ngguf                              0.10.0\ngoogle-api-core                   2.24.1\ngoogle-auth                       2.38.0\ngoogleapis-common-protos          1.67.0\ngradio                            5.18.0\ngradio_client                     1.7.2\ngrpcio                            1.70.0\nh11                               0.14.0\nhf_transfer                       0.1.9\nhttpcore                          1.0.7\nhttplib2                          0.20.2\nhttptools                         0.6.4\nhttpx                             0.28.1\nhuggingface-hub                   0.28.1\nidna                              3.3\nimportlib_metadata                8.6.1\niniconfig                         2.0.0\ninteregular                       0.3.3\nipython                           8.32.0\njedi                              0.19.2\njeepney                           0.7.1\njieba                             0.42.1\nJinja2                            3.1.5\njiter                             0.8.2\njoblib                            1.4.2\njsonlines                         4.0.0\njsonpatch                         1.32\njsonpointer                       2.0\njsonschema                        4.23.0\njsonschema-specifications         2024.10.1\nkeyring                           23.5.0\nkiwisolver                        1.4.8\nlangdetect                        1.0.9\nlark                              1.2.2\nlatex2sympy2                      1.9.1\nlaunchpadlib                      1.10.16\nlazr.restfulclient                0.14.4\nlazr.uri                          1.0.6\nlitellm                           1.61.8\nlm-format-enforcer                0.10.10\nlxml                              5.3.1\nmarkdown-it-py                    3.0.0\nMarkupSafe                        2.0.1\nmatplotlib                        3.10.0\nmatplotlib-inline                 0.1.7\nmdurl                             0.1.2\nmistral_common                    1.5.3\nmodelscope                        1.23.0\nmore-itertools                    8.10.0\nmpmath                            1.3.0\nmsgpack                           1.1.0\nmsgspec                           0.19.0\nmultidict                         6.1.0\nmultiprocess                      0.70.16\nnarwhals                          1.28.0\nnest-asyncio                      1.6.0\nnetifaces                         0.11.0\nnetworkx                          3.4.2\nninja                             1.11.1.3\nnltk                              3.9.1\nnumpy                             1.26.4\nnvidia-cublas-cu12                12.4.5.8\nnvidia-cuda-cupti-cu12            12.4.127\nnvidia-cuda-nvrtc-cu12            12.4.127\nnvidia-cuda-runtime-cu12          12.4.127\nnvidia-cudnn-cu12                 9.1.0.70\nnvidia-cufft-cu12                 11.2.1.3\nnvidia-curand-cu12                10.3.5.147\nnvidia-cusolver-cu12              11.6.1.9\nnvidia-cusparse-cu12              12.3.1.170\nnvidia-ml-py                      12.570.86\nnvidia-nccl-cu12                  2.21.5\nnvidia-nvjitlink-cu12             12.4.127\nnvidia-nvtx-cu12                  12.4.127\noauthlib                          3.2.0\nopenai                            1.63.2\nopencensus                        0.11.4\nopencensus-context                0.1.3\nopencv-python-headless            4.11.0.86\norjson                            3.10.15\noutlines                          0.1.11\noutlines_core                     0.1.26\npackaging                         24.2\npandas                            2.2.3\nparso                             0.8.4\npartial-json-parser               0.2.1.1.post5\npexpect                           4.9.0\npillow                            11.1.0\npip                               22.0.2\nplatformdirs                      4.3.6\nplotly                            6.0.0\npluggy                            1.5.0\nply                               3.11\nportalocker                       3.1.1\nprometheus_client                 0.21.1\nprometheus-fastapi-instrumentator 7.0.2\nprompt_toolkit                    3.0.50\npropcache                         0.2.1\nproto-plus                        1.26.0\nprotobuf                          5.29.3\npsutil                            7.0.0\nptyprocess                        0.7.0\npure_eval                         0.2.3\npy-cpuinfo                        9.0.0\npy-spy                            0.4.0\npyarrow                           19.0.1\npyasn1                            0.6.1\npyasn1_modules                    0.4.1\npybind11                          2.13.6\npycountry                         24.6.1\npydantic                          2.10.6\npydantic_core                     2.27.2\npydub                             0.25.1\nPygments                          2.19.1\nPyGObject                         3.42.1\nPyJWT                             2.3.0\nPympler                           1.1\npyparsing                         2.4.7\npyrsistent                        0.18.1\npyserial                          3.5\npytest                            8.3.4\npython-apt                        2.4.0+ubuntu2\npython-dateutil                   2.9.0.post0\npython-dotenv                     1.0.1\npython-multipart                  0.0.20\npytz                              2022.1\nPyYAML                            5.4.1\npyzmq                             26.2.1\nray                               2.42.1\nreferencing                       0.36.2\nregex                             2024.11.6\nrequests                          2.32.3\nrequests-toolbelt                 1.0.0\nrich                              13.9.4\nrouge-chinese                     1.0.3\nrouge-score                       0.1.2\nrpds-py                           0.22.3\nrsa                               4.9\nruff                              0.9.7\nsacrebleu                         2.5.1\nsafehttpx                         0.1.6\nsafetensors                       0.5.2\nscikit-learn                      1.6.1\nscipy                             1.15.2\nseaborn                           0.13.2\nSecretStorage                     3.3.1\nsemantic-version                  2.10.0\nsentencepiece                     0.2.0\nsetproctitle                      1.3.4\nsetuptools                        69.5.1\nsgl-kernel                        0.0.3.post6\nsglang                            0.4.3.post2\nshellingham                       1.5.4\nsimple-ddl-parser                 1.7.1\nsimplejson                        3.20.1\nsix                               1.16.0\nsmart-open                        7.1.0\nsniffio                           1.3.1\nsortedcontainers                  2.4.0\nsse-starlette                     2.2.1\nssh-import-id                     5.11\nstack-data                        0.6.3\nstarlette                         0.45.3\nsympy                             1.13.1\nsystemd-python                    234\ntabulate                          0.9.0\nthreadpoolctl                     3.5.0\ntiktoken                          0.9.0\ntokenizers                        0.21.0\ntomli                             2.2.1\ntomlkit                           0.13.2\ntorch                             2.5.1\ntorchao                           0.8.0\ntorchaudio                        2.5.1\ntorchvision                       0.20.1\ntqdm                              4.67.1\ntraitlets                         5.14.3\ntransformers                      4.48.3\ntransformers-stream-generator     0.0.5\ntriton                            3.1.0\ntyper                             0.15.1\ntyping_extensions                 4.12.2\ntzdata                            2025.1\nubuntu-drivers-common             0.0.0\nunattended-upgrades               0.1\nurllib3                           1.26.5\nuvicorn                           0.34.0\nuvloop                            0.21.0\nvirtualenv                        20.29.2\nvllm                              0.7.2\nwadllib                           1.3.6\nwatchfiles                        1.0.4\nwcwidth                           0.2.13\nwebsockets                        15.0\nwheel                             0.37.1\nword2number                       1.1\nwrapt                             1.17.2\nxformers                          0.0.28.post3\nxgrammar                          0.1.10\nxkit                              0.0.0\nxxhash                            3.5.0\nyarl                              1.18.3\nzipp                              3.21.0\n",
    "labels": [
      "help wanted",
      "inactive",
      "quant",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-03-03T09:03:05+00:00",
    "closed_at": "2025-05-22T00:19:08+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4018/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4018"
  },
  {
    "number": 3517,
    "title": "i use 3 A800 to deploy deepseek r1\uff0cbut one A800 just one  IB\uff0chow i adjust the number of tp in the deploy command",
    "body": "# node 1\nexport NCCL_IB_HCA=mlx5_0\npython3 -m sglang.launch_server --model-path /x32001214/model/bf16/DeepSeek-R1-BF16 --tp 12 --dist-init-addr 0.0.0.0:9997 --nnodes 3 --node-rank 0 --trust-remote-code  --host 0.0.0.0 --port 8888\n\n# node 2\nexport NCCL_IB_HCA=mlx5_1\npython3 -m sglang.launch_server --model-path /x32001214/model/bf16/DeepSeek-R1-BF16 --tp 24 --dist-init-addr 10.160.199.103:30172 --nnodes 3 --node-rank 1 --trust-remote-code\n\n# node 3\nexport NCCL_IB_HCA=mlx5_1\npython3 -m sglang.launch_server --model-path /x32001214/model/bf16/DeepSeek-R1-BF16 --tp 24 --dist-init-addr 10.160.199.103:30172 --nnodes 3 --node-rank 2 --trust-remote-code",
    "labels": [
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-12T08:40:00+00:00",
    "closed_at": "2025-02-13T19:23:45+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3517/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3517"
  },
  {
    "number": 3368,
    "title": "[Bug] Watchdog caught collective operation timeout:",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nRun DeepSeek-R1 on 2 8* A800s, \nThis error occurs after loading the model:  Watchdog caught collective operation timeout:\n10.25.117.26\n```shell\n[2025-02-07 02:59:26 TP13] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-02-07 02:59:26 TP11] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-02-07 02:59:26 TP15] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-02-07 02:59:26 TP8] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-02-07 02:59:26 TP10] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-02-07 02:59:26 TP12] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-02-07 02:59:26 TP14] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-02-07 02:59:26 TP9] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\nCache shape torch.Size([163840, 64])\nCache shape torch.Size([163840, 64])\nCache shape torch.Size([163840, 64])\nCache shape torch.Size([163840, 64])\nCache shape torch.Size([163840, 64])\nCache shape torch.Size([163840, 64])\nCache shape torch.Size([163840, 64])\nCache shape torch.Size([163840, 64])\n\n```\n\n10.25.117.28\n```\nLoading safetensors checkpoint shards:  99% Completed | 161/163 [01:15<00:00,  2.24it/s]\nLoading safetensors checkpoint shards:  99% Completed | 162/163 [01:15<00:00,  2.62it/s]\nLoading safetensors checkpoint shards: 100% Completed | 163/163 [01:15<00:00,  2.44it/s]\nLoading safetensors checkpoint shards: 100% Completed | 163/163 [01:15<00:00,  2.15it/s]\n\n[2025-02-07 01:19:40 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=38.13 GB\n[2025-02-07 01:19:42 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=38.11 GB\n[2025-02-07 01:19:47 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=38.13 GB\n[2025-02-07 01:19:47 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=38.11 GB\n[2025-02-07 01:19:47 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=38.11 GB\n\n\n[rank6]:[E207 01:29:36.730311765 ProcessGroupNCCL.cpp:616] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=ALLREDUCE, NumelIn=1, Nume\nlOut=1, Timeout(ms)=600000) ran for 600050 milliseconds before timing out.\n[rank6]:[E207 01:29:36.730815661 ProcessGroupNCCL.cpp:1785] [PG ID 0 PG GUID 0(default_pg) Rank 6] Exception (either an error or timeout) detected by watchdog at work\n: 2, last enqueued NCCL work: 2, last completed NCCL work: 1.\n[rank6]:[E207 01:29:36.730845309 ProcessGroupNCCL.cpp:1834] [PG ID 0 PG GUID 0(default_pg) Rank 6] Timeout at NCCL work: 2, last enqueued NCCL work: 2, last completed\n NCCL work: 1.\n[rank6]:[E207 01:29:36.730861671 ProcessGroupNCCL.cpp:630] [Rank 6] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n[rank6]:[E207 01:29:36.730893283 ProcessGroupNCCL.cpp:636] [Rank 6] To avoid data inconsistency, we are taking the entire process down.\n[rank6]:[E207 01:29:36.732277782 ProcessGroupNCCL.cpp:1595] [PG ID 0 PG GUID 0(default_pg) Rank 6] Process group watchdog thread terminated with exception: [Rank 6] W\natchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600050 milliseconds before timing\n out.\nException raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fa178140446 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7fa179453772 in /usr/local/li\nb/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\nframe #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7fa17945abb3 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fa17945c61d in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\nframe #4: <unknown function> + 0x145c0 (0x7fa1c4f985c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)\nframe #5: <unknown function> + 0x8609 (0x7fa1c5d23609 in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #6: clone + 0x43 (0x7fa1c5e5d353 in /lib/x86_64-linux-gnu/libc.so.6)\n\nterminate called after throwing an instance of 'c10::DistBackendError'\n  what():  [PG ID 0 PG GUID 0(default_pg) Rank 6] Process group watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: Work\nNCCL(SeqNum=2, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600050 milliseconds before timing out.\nException raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fa178140446 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7fa179453772 in /usr/local/li\nb/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\n```\n\n\n\n### Reproduction\n\nIP: 10.25.117.28\n```shell\ndocker pull lmsysorg/sglang:latest\n\nmount -o remount,size=32G /dev/shm\n\nexport NCCL_SOCKET_IFNAME=bond0.137\nexport GLOO_SOCKET_IFNAME=bond0.137\n\ndocker run --gpus all   --shm-size 32g  --network=host -v /home/app/sharedir_online/nlp/ark_server:/models  --name sglang_multinode0 -it --ipc=host  lmsysorg/sglang:latest\n   \npython3 -m sglang.launch_server --model-path /models/DeepSeek-R1 --tp 16 --dist-init-addr 10.25.117.28:5000 --nnodes 2 --node-rank 0 --trust-remote-code\n```\n\n\nIP: 10.25.117.26\n```shell\ndocker pull lmsysorg/sglang:latest\n\nmount -o remount,size=32G /dev/shm\n\nexport NCCL_SOCKET_IFNAME=bond0.137\nexport GLOO_SOCKET_IFNAME=bond0.137\n\ndocker run --gpus all   --shm-size 32g  --network=host -v /home/app/sharedir_online/nlp/ark_server:/models  --name sglang_multinode1  -it --ipc=host  lmsysorg/sglang:latest\n   \npython3 -m sglang.launch_server --model-path /models/DeepSeek-R1 --tp 16 --dist-init-addr 10.25.117.28:5000 --nnodes 2 --node-rank 1 --trust-remote-code\n```\n\nI wonder if I made a mistake?\n\n### Environment\n\n```shell\nbond0.137: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 9000\n        inet 10.25.117.28  netmask 255.255.255.0  broadcast 10.25.117.255\n        inet6 fe80::9ec2:c4ff:fe21:8392  prefixlen 64  scopeid 0x20<link>\n        ether 9c:c2:c4:21:83:92  txqueuelen 1000  (Ethernet)\n        RX packets 33681202189  bytes 227770829472403 (207.1 TiB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 171313511956  bytes 235085261869275 (213.8 TiB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n```\n\n```shell\nPython: 3.10.16 (main, Dec  4 2024, 08:53:37) [GCC 9.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA A800-SXM4-80GB\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.5, V12.5.82\nCUDA Driver Version: 535.104.05\nPyTorch: 2.5.1+cu124\nflashinfer: 0.2.0.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.2\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.61.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV8     NV8     NV8     NV8     NV8     NV8     NV8     PXB     NODE    SYS     SYS     0-31    0               N/A\nGPU1    NV8      X      NV8     NV8     NV8     NV8     NV8     NV8     PXB     NODE    SYS     SYS     0-31    0               N/A\nGPU2    NV8     NV8      X      NV8     NV8     NV8     NV8     NV8     NODE    PXB     SYS     SYS     0-31    0               N/A\nGPU3    NV8     NV8     NV8      X      NV8     NV8     NV8     NV8     NODE    PXB     SYS     SYS     0-31    0               N/A\nGPU4    NV8     NV8     NV8     NV8      X      NV8     NV8     NV8     SYS     SYS     PXB     NODE    32-63   1               N/A\nGPU5    NV8     NV8     NV8     NV8     NV8      X      NV8     NV8     SYS     SYS     PXB     NODE    32-63   1               N/A\nGPU6    NV8     NV8     NV8     NV8     NV8     NV8      X      NV8     SYS     SYS     NODE    PXB     32-63   1               N/A\nGPU7    NV8     NV8     NV8     NV8     NV8     NV8     NV8      X      SYS     SYS     NODE    PXB     32-63   1               N/A\nNIC0    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    SYS     SYS\nNIC1    NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE     X      SYS     SYS\nNIC2    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    SYS     SYS      X      NODE\nNIC3    SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     SYS     SYS     NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n\n\nulimit soft: 1048576\n```",
    "labels": [
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-07T09:51:41+00:00",
    "closed_at": "2025-02-14T09:01:23+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3368/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3368"
  },
  {
    "number": 3516,
    "title": "[Bug]DeepSeek-R1 Process hangs after NCCL initialization in multi-server distributed inference setup",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n### Environment and Setup\nI am trying to run the DeepSeek-R1 671B model on three servers, each equipped with 8 A800 GPUs\nI have 3 servers, each with 8 * A800 GPUs. I'm trying to create 5 nodes across these three servers using Docker overlay network for distributed inference, with each node using 4 GPUs. I've confirmed that all nodes can ping each other and NCCL communication is working.\n All nodes get stuck at the following log point.\nLogs:\n```\n0d7b897a995a:1334:1334 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer\n0d7b897a995a:1333:1333 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 1 p2p channels per peer\n0d7b897a995a:1333:1333 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so\n0d7b897a995a:1334:1334 [1] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so\n0d7b897a995a:1333:1333 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.\n0d7b897a995a:1334:1334 [1] NCCL INFO TUNER/Plugin: Using internal tuner plugin.\n0d7b897a995a:1333:1333 [0] NCCL INFO ncclCommInitRank comm 0x39a43760 rank 12 nranks 20 cudaDev 0 nvmlDev 0 busId 9c000 commId 0xa9929a515b8ef6f6 - Init COMPLETE\n0d7b897a995a:1334:1334 [1] NCCL INFO ncclCommInitRank comm 0x23c6e370 rank 13 nranks 20 cudaDev 1 nvmlDev 1 busId 9d000 commId 0xa9929a515b8ef6f6 - Init COMPLETE\n0d7b897a995a:1335:1335 [2] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so\n0d7b897a995a:1335:1335 [2] NCCL INFO TUNER/Plugin: Using internal tuner plugin.\n0d7b897a995a:1335:1335 [2] NCCL INFO ncclCommInitRank comm 0x46c01570 rank 14 nranks 20 cudaDev 2 nvmlDev 2 busId a0000 commId 0xa9929a515b8ef6f6 - Init COMPLETE\n0d7b897a995a:1336:1336 [3] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so\n0d7b897a995a:1336:1336 [3] NCCL INFO TUNER/Plugin: Using internal tuner plugin.\n0d7b897a995a:1336:1336 [3] NCCL INFO ncclCommInitRank comm 0x3ee7d680 rank 15 nranks 20 cudaDev 3 nvmlDev 3 busId a4000 commId 0xa9929a515b8ef6f6 - Init COMPLETE\n[2025-02-11 18:41:41 TP15] Custom allreduce is disabled because this process group spans across nodes.\n[2025-02-11 18:41:41 TP13] Custom allreduce is disabled because this process group spans across nodes.\n[2025-02-11 18:41:41 TP14] Custom allreduce is disabled because this process group spans across nodes.\n[2025-02-11 18:41:41 TP12] Custom allreduce is disabled because this process group spans across nodes.\n```\nAny assistance would be greatly appreciated.\n\n\n### Reproduction\n\n```\nbash\ndocker run --gpus '\"device=0,1,2,3\"' \\\n    --shm-size 32g \\\n    --network=my_overlay_network \\\n    -v /data:/data \\\n    -p 3000:3000 \\\n    --name sglang_node0 \\\n    -it \\\n    --rm \\\n    --ipc=host \\\n    -e NCCL_SOCKET_IFNAME=eth0 \\\n    -e NCCL_DEBUG=INFO \\\n    lmsysorg/sglang:latest \\\n    python3 -m sglang.launch_server \\\n    --model-path /data/ls_data/models/deepseek/DeepSeek-R1-bf16 \\\n    --tp 20 \\\n    --dist-init-addr 192.168.200.33:5000 \\\n    --nnodes 5 \\\n    --node-rank 0 \\\n    --trust-remote-code \\\n    --host 0.0.0.0 \\\n    --port 3000\n```\n\nSimilar commands for nodes 0-4 with appropriate GPU device mappings and node ranks\nmodel: DeepSeek-R1-bf16\n\n### Environment\n\n```\nPython: 3.10.16 (main, Dec  4 2024, 08:53:37) [GCC 9.4.0]\nCUDA available: True\nGPU 0,1,2,3: NVIDIA A800 80GB PCIe\nGPU 0,1,2,3 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.5, V12.5.82\nCUDA Driver Version: 565.57.01\nPyTorch: 2.5.1+cu124\nflashinfer: 0.2.0.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.61.1\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    GPU2    GPU3    NIC0    NIC1    NIC2    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PIX     PXB     PXB     SYS     SYS     NODE    16-31,48-63     1               N/A\nGPU1    PIX      X      PXB     PXB     SYS     SYS     NODE    16-31,48-63     1               N/A\nGPU2    PXB     PXB      X      PXB     SYS     SYS     NODE    16-31,48-63     1               N/A\nGPU3    PXB     PXB     PXB      X      SYS     SYS     NODE    16-31,48-63     1               N/A\nNIC0    SYS     SYS     SYS     SYS      X      PIX     SYS\nNIC1    SYS     SYS     SYS     SYS     PIX      X      SYS\nNIC2    NODE    NODE    NODE    NODE    SYS     SYS      X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n\n\nulimit soft: 1048576\n```",
    "labels": [
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-12T08:37:39+00:00",
    "closed_at": "2025-02-19T12:42:30+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3516/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3516"
  },
  {
    "number": 3254,
    "title": "[Bug] DeepSeek R1 loading error by multi-node inference",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI tried to use multi-node inference (2 * 8 H100) to load DeepSeek R1.\n\nHowever, it kept giving me error on the header node. **Note This error happens model weights are loaded 100%.**\n```\nLoading safetensors checkpoint shards: 100% Completed | 163/163 [1:09:28<00:00, 26.68s/it]\n \nLoading safetensors checkpoint shards: 100% Completed | 163/163 [1:09:28<00:00, 25.57s/it]\n \n[2025-02-01 07:03:54 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=37.68 GB\n[2025-02-01 07:03:54 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=37.68 GB\n[2025-02-01 07:03:54 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=37.71 GB\n[2025-02-01 07:03:55 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=37.68 GB\n[2025-02-01 07:03:55 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=37.68 GB\n[2025-02-01 07:03:55 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=37.68 GB\n[2025-02-01 07:03:55 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=37.68 GB\n[2025-02-01 07:03:55 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=37.68 GB\n[2025-02-01 07:03:55 TP3] Memory pool end. avail mem=15.10 GB\n[2025-02-01 07:03:55 TP4] Memory pool end. avail mem=15.10 GB\n[2025-02-01 07:03:55 TP7] Memory pool end. avail mem=15.11 GB\n[2025-02-01 07:03:55 TP5] Memory pool end. avail mem=15.10 GB\n[2025-02-01 07:03:55 TP6] Memory pool end. avail mem=15.10 GB\n[2025-02-01 07:03:55 TP1] Memory pool end. avail mem=15.10 GB\n[2025-02-01 07:03:55 TP0] Memory pool end. avail mem=15.14 GB\n[2025-02-01 07:03:55 TP2] Memory pool end. avail mem=15.10 GB\n[2025-02-01 07:03:55 TP6] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-02-01 07:03:55 TP4] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-02-01 07:03:55 TP5] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-02-01 07:03:55 TP3] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-02-01 07:03:55 TP0] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-02-01 07:03:55 TP7] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-02-01 07:03:55 TP1] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-02-01 07:03:55 TP2] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-02-01 07:03:55 TP5] Capture cuda graph begin. This can take up to several minutes.\n[2025-02-01 07:03:55 TP6] Capture cuda graph begin. This can take up to several minutes.\n[2025-02-01 07:03:55 TP7] Capture cuda graph begin. This can take up to several minutes.\n[2025-02-01 07:03:55 TP4] Capture cuda graph begin. This can take up to several minutes.\n[2025-02-01 07:03:55 TP3] Capture cuda graph begin. This can take up to several minutes.\n[2025-02-01 07:03:55 TP0] Capture cuda graph begin. This can take up to several minutes.\n \n  0%|          | 0/23 [00:00<?, ?it/s][2025-02-01 07:03:55 TP1] Capture cuda graph begin. This can take up to several minutes.\n[2025-02-01 07:03:55 TP2] Capture cuda graph begin. This can take up to several minutes.\n \n  0%|          | 0/23 [00:00<?, ?it/s]\n[2025-02-01 07:03:55 TP6] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/iofsx/sds3/scripts/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 226, in __init__\n    self.capture()\n  File \"/iofsx/sds3/scripts/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 292, in capture\n    ) = self.capture_one_batch_size(bs, forward)\n  File \"/iofsx/sds3/scripts/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 368, in capture_one_batch_size\n    self.model_runner.tp_group.barrier()\n  File \"/iofsx/sds3/scripts/sglang/python/sglang/srt/distributed/parallel_state.py\", line 812, in barrier\n    torch.distributed.barrier(group=self.cpu_group)\n  File \"/opt/conda/envs/xin/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\", line 83, in wrapper\n    return func(*args, **kwargs)\n  File \"/opt/conda/envs/xin/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 4164, in barrier\n    work.wait()\nRuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [172.31.18.145]:45492\n \nDuring handling of the above exception, another exception occurred:\n \nTraceback (most recent call last):\n  File \"/iofsx/sds3/scripts/sglang/python/sglang/srt/managers/scheduler.py\", line 1787, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/iofsx/sds3/scripts/sglang/python/sglang/srt/managers/scheduler.py\", line 240, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/iofsx/sds3/scripts/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/iofsx/sds3/scripts/sglang/python/sglang/srt/managers/tp_worker.py\", line 68, in __init__\n    self.model_runner = ModelRunner(\n  File \"/iofsx/sds3/scripts/sglang/python/sglang/srt/model_executor/model_runner.py\", line 214, in __init__\n    self.init_cuda_graphs()\n  File \"/iofsx/sds3/scripts/sglang/python/sglang/srt/model_executor/model_runner.py\", line 730, in init_cuda_graphs\n    self.cuda_graph_runner = CudaGraphRunner(self)\n  File \"/iofsx/sds3/scripts/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 228, in __init__\n    raise Exception(\nException: Capture cuda graph failed: [../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [172.31.18.145]:45492\nPossible solutions:\n1. disable cuda graph by --disable-cuda-graph\n2. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)\n3. disable torch compile by not using --enable-torch-compile\nOpen an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose \n```\n\n\nThe error on the worker node is following.\n\n```\nException raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ea82996c446 in /opt/conda/envs/sglang/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7ea78d3cc772 in /opt/conda/envs/sglang/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7ea78d3d3bb3 in /opt/conda/envs/sglang/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ea78d3d561d in /opt/conda/envs/sglang/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: <unknown function> + 0x145c0 (0x7ea87843e5c0 in /opt/conda/envs/sglang/lib/python3.10/site-packages/torch/lib/libtorch.so)\nframe #5: <unknown function> + 0x94ac3 (0x7ea882494ac3 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #6: <unknown function> + 0x126850 (0x7ea882526850 in /lib/x86_64-linux-gnu/libc.so.6)\n\nterminate called after throwing an instance of 'c10::DistBackendError'\n  what():  [PG ID 0 PG GUID 0(default_pg) Rank 15] Process group watchdog thread terminated with exception: [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600013 milliseconds before timing out.\nException raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ea82996c446 in /opt/conda/envs/sglang/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7ea78d3cc772 in /opt/conda/envs/sglang/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7ea78d3d3bb3 in /opt/conda/envs/sglang/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ea78d3d561d in /opt/conda/envs/sglang/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: <unknown function> + 0x145c0 (0x7ea87843e5c0 in /opt/conda/envs/sglang/lib/python3.10/site-packages/torch/lib/libtorch.so)\nframe #5: <unknown function> + 0x94ac3 (0x7ea882494ac3 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #6: <unknown function> + 0x126850 (0x7ea882526850 in /lib/x86_64-linux-gnu/libc.so.6)\n\nException raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ea82996c446 in /opt/conda/envs/sglang/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0xe4271b (0x7ea78d04271b in /opt/conda/envs/sglang/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\nframe #2: <unknown function> + 0x145c0 (0x7ea87843e5c0 in /opt/conda/envs/sglang/lib/python3.10/site-packages/torch/lib/libtorch.so)\nframe #3: <unknown function> + 0x94ac3 (0x7ea882494ac3 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #4: <unknown function> + 0x126850 (0x7ea882526850 in /lib/x86_64-linux-gnu/libc.so.6)\n```\n\n### Reproduction\n\nI created a conda virtual env with python 3.10.\n\nThen I followed the method 2 in [here](https://docs.sglang.ai/start/install.html#method-2-from-source) to install sgland.\n\nThe invoking command on header and worker node is following.\n\n```\npython -m sglang.launch_server \\\n--model-path /iofsx/sds3/models/DeepSeekV3/DeepSeek-R1 \\\n--tp 16 \\\n--dist-init-addr <HEADER_IP>:5000 \\\n--nnodes 2 \\\n--node-rank 0 \\\n--trust-remote-code \\\n--disable-radix-cache \\\n--port 8000 \\\n--host 0.0.0.0 2>&1 | tee RUN_1.log\n```\n\n\n```\npython -m sglang.launch_server \\\n--model-path /iofsx/sds3/models/DeepSeekV3/DeepSeek-R1 \\\n--tp 16 \\\n--dist-init-addr <HEADER_IP>:5000 \\\n--nnodes 2 \\\n--node-rank 1 \\\n--trust-remote-code \\\n--disable-radix-cache \\\n--port 8000 \\\n--host 0.0.0.0 2>&1 | tee RUN_2.log\n```\n\n### Environment\n\nI created a conda virtual env with python 3.10.\n\nThen I followed the method 2 in [here](https://docs.sglang.ai/start/install.html#method-2-from-source) to install sgland.\n```\n# Use the last release branch\ngit clone -b v0.4.2.post1 https://github.com/sgl-project/sglang.git\ncd sglang\n\npip install --upgrade pip\npip install sgl-kernel --force-reinstall --no-deps\npip install -e \"python[all]\" --find-links https://flashinfer.ai/whl/cu124/torch2.4/flashinfer/\n```\n\nPytorch version is `torch.__version__'2.5.1+cu124'`",
    "labels": [
      "help wanted",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-01T15:15:48+00:00",
    "closed_at": "2025-02-03T19:52:40+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3254/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3254"
  },
  {
    "number": 4054,
    "title": "NVIDIA L40*8  docker NCCL Hanging During Initialization on Single Node with Multiple GPUs",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI am encountering an issue where NCCL hangs during initialization when running SGLang on a single node equipped with multiple GPUs (8 NVIDIA L40 GPUs). Despite my efforts to configure environment variables to prioritize shared memory (SHM) or NVLink over network communication, NCCL persists in using the NET/Socket mode and stalls at the Init START phase. I\u2019ve exhausted various troubleshooting steps, and I\u2019m seeking assistance to resolve this. Below are the details of my setup, the steps I\u2019ve taken, and the relevant logs.\n\n\nuser-B7129F83AV8E4HR-N:101:101 [2] NCCL INFO Using network Socket\nuser-B7129F83AV8E4HR-N:101:101 [2] NCCL INFO Bootstrap : Using ens7f0:192.168.1.51<0>\nuser-B7129F83AV8E4HR-N:101:101 [2] NCCL INFO ncclCommInitRank comm 0x64400b7886c0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 19000 commId 0x862c3138ddfa18ce - Init START\n\n### Reproduction\n\nHardware: Single node with 8x NVIDIA L40 GPUs (connected via NVLink)\nOperating System: Ubuntu (kernel 6.8.0-52-generic)\nNCCL Version: 2.21.5 (built for CUDA 12.4)\nCUDA Driver Version: 12.8 (reported as cudaDriverVersion 12080)\nSGLang Version: Latest from GitHub (git clone https://github.com/sgl-project/sglang.git)\nConfiguration: Running SGLang with tensor parallelism (--tp 8)\n\n\n\n### Environment\n\n\n\ndocker run -itd \\\n  --gpus=all  \\\n  --name sglang \\\n  --shm-size 32g \\\n  -p 30000:30000 \\\n  -v /home/user/app/models:/root/.cache/modelscope \\\n  --ipc=host \\\n  --network=host \\\n  --privileged \\\n  sglang:latest\n    \n\n\npython3 -m sglang.launch_server \\\n    --model /root/.cache/modelscope/ \\\n    --tp 8 \\\n    --trust-remote-code \\\n    --port 30000\n",
    "labels": [
      "help wanted",
      "inactive",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-03-04T07:13:01+00:00",
    "closed_at": "2025-05-04T00:21:09+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4054/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4054"
  },
  {
    "number": 4409,
    "title": "[Usage] What's the best practice of deploying DeepSeekV3 using sglang?",
    "body": "I want to run inference of [DeepSeekV3](https://huggingface.co/deepseek-ai/DeepSeek-V3) on multi-node GPU clusters. I have followed the [sglang deepseek guide](https://docs.sglang.ai/references/deepseek.html) to setup the distributed serving environment with the official sglang-v0.4.3.post4-cu124 docker image.\n\nMore specifically:\n```\n# node 1\ndocker run --gpus all \\\n    --shm-size 32g \\\n    --network=host \\\n    --privileged \\\n    -v xxx:xxx \\\n    --name sglang_multinode1 \\\n    -it \\\n    --ipc=host \\\n    xxx\n\n# node 2\ndocker run --gpus all \\\n    --shm-size 32g \\\n    --network=host \\\n    --privileged \\\n    -v xxx:xxx \\\n    --name sglang_multinode2 \\\n    -it \\\n    --ipc=host \\\n    xxx\n```\nto create docker containers\n\n```\n# node 1\nNCCL_IB_GID_INDEX=3 \\\nNCCL_DEBUG=TRACE \\\nNCCL_DEBUG=INFO \\\npython3 -m sglang.launch_server \\\n    --model-path xxx \\\n    --tp 16 --dist-init-addr xxx:20000 \\\n    --nnodes 2 \\\n    --node-rank 0 \\\n    --trust-remote-code \\\n    --host 0.0.0.0 --port 40000 2>&1 | tee api_serve.log\n\n# node 2\nNCCL_IB_GID_INDEX=3 \\\nNCCL_DEBUG=TRACE \\\nNCCL_DEBUG=INFO \\\npython3 -m sglang.launch_server \\\n    --model-path xxx \\\n    --tp 16 --dist-init-addr xxx:20000 \\\n    --nnodes 2 \\\n    --node-rank 1 \\\n    --trust-remote-code \\\n    --host 0.0.0.0 --port 40000 2>&1 | tee api_serve.log\n```\nto start sglang deepseekv3 api serving.\n\n```\nfor out_len in 1024 2048 4096 8192\ndo\n    echo \"running with output-len $out_len\"\n    python3 -m sglang.bench_serving \\\n        --model xxx \\\n        --backend sglang \\\n        --dataset-name random \\\n        --random-input 2048 \\\n        --random-output $out_len \\\n        --num-prompts 2000 \\\n        --output-file result-out$out_len.json \\\n        --seed 42 \\\n        --host 0.0.0.0 --port 40000 2>&1 | tee log_out$out_len.log\ndone\n```\nfor benchmarking sglang\n\nWe have already enabled RDMA for fast inter-node communications. Some benchmark results are shown as follows:\n\n![Image](https://github.com/user-attachments/assets/c89c4234-7322-43b0-88f2-fb1af9198459)\n\nit seems sglang has unexpectedly low output throughput with short output length settings like 1024/2048.\n\n\n\nI want to ask:\n1. Any additional flags to set for speedup?\n2. Any additional dependencies or code to update? Since sglang continuously integrates components like [FlashMLA](https://github.com/deepseek-ai/FlashMLA)\n\nor anything that is missing to achieve the best inference speed of DeepSeekV3 with sglang?",
    "labels": [
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-03-14T03:02:37+00:00",
    "closed_at": "2025-03-26T08:09:34+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4409/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4409"
  },
  {
    "number": 4048,
    "title": "[Error]Input length (160062 tokens) exceeds the maximum allowed length (59862 tokens).",
    "body": "Hi, I am trying to use `sglang` to deploy a DeepSeek R1 serving program. The deployment command is as follows:\n\n```shell\npython -m sglang.launch_server \\\n--model-path  /models/deepseek-ai/deepseek-r1 \\\n--host 0.0.0.0 \\\n--port 8100 \\\n--tensor-parallel-size 8 \\\n--mem-fraction-static 0.9 \\\n--trust-remote-code \\\n--context-length 163840 \\\n--chunked-prefill-size 4096 \\\n--served-model-name DeepSeek-R1-Sglang-160k\n```\n\nAlthough I set the `--context-length` parameter to 160k and the serving program starts successfully, an error occurs when I send a request with content of length 160k. The error message is as follows:\n\n```shell\nif self.sampling_params.max_new_tokens > 0:\nTypeError: '>' not supported between instances of 'NoneType' and 'int'\n[2025-03-03 06:49:28 TP5] Input length (160062 tokens) exceeds the maximum allowed length (59862 tokens). Use a shorter input or enable --allow-auto-truncate.\n[2025-03-03 06:49:28 TP1] Input length (160062 tokens) exceeds the maximum allowed length (59862 tokens). Use a shorter input or enable --allow-auto-truncate.\n[2025-03-03 06:49:28 TP5] Scheduler hit an exception: Traceback (most recent call last):\nFile \"/home/ray/anaconda/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py\", line 1796, in run_scheduler_process scheduler. event_loop_overlap\nFile \"/home/ray/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context return func(*args, **kwargs)\n```\n\nIn my command, I only set the `--context-length` to 163840 and did not specify the number 59862. Who determines this 59862 value?\n\n\n",
    "labels": [
      "help wanted",
      "inactive",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-03-04T03:59:50+00:00",
    "closed_at": "2025-05-05T00:20:08+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4048/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4048"
  },
  {
    "number": 3815,
    "title": "[Bug] sglang crashes when serving DeepSeek-R1 with profiler enabled.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nOne of the TP process crashes and the sglang server is down when I run `bench_serving.py` with `--profile`\n\npossible direction: the two nodes have no NFS that can share the profile log folder. I am not sure if it's the problem. \n\n\noutput of sglang\n![Image](https://github.com/user-attachments/assets/7c16fde7-1308-4940-9547-f8260ab2fc2b)\n\noutput of `bench_serving.py` which I think is the result of the sglang backend crashes.\n\n```\nTraceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/bench_serving.py\", line 342, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File \"/usr/local/lib/python3.10/dist-packages/aiohttp/streams.py\", line 52, in __anext__\n    rv = await self.read_func()\n  File \"/usr/local/lib/python3.10/dist-packages/aiohttp/streams.py\", line 352, in readline\n    return await self.readuntil()\n  File \"/usr/local/lib/python3.10/dist-packages/aiohttp/streams.py\", line 386, in readuntil\n    await self._wait(\"readuntil\")\n  File \"/usr/local/lib/python3.10/dist-packages/aiohttp/streams.py\", line 347, in _wait\n    await waiter\naiohttp.client_exceptions.ClientPayloadError: Response payload is not completed: <TransferEncodingError: 400, message='Not enough data for satisfy transfer length header.'>\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:00<00:00, 22.86it/s]\n/sgl-workspace/sglang/python/sglang/bench_serving.py:1010: UserWarning: All requests failed. This is likely due to a misconfiguration on the benchmark arguments.\n  metrics, output_lens = calculate_metrics(\n/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n  ret = ret.dtype.type(ret / rcount)\n/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n  arrmean = um.true_divide(arrmean, div, out=arrmean,\n/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n  ret = ret.dtype.type(ret / rcount)\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/sgl-workspace/sglang/python/sglang/bench_serving.py\", line 1522, in <module>\n    run_benchmark(args)\n  File \"/sgl-workspace/sglang/python/sglang/bench_serving.py\", line 1282, in run_benchmark\n    return asyncio.run(\n  File \"/usr/lib/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"/sgl-workspace/sglang/python/sglang/bench_serving.py\", line 1010, in benchmark\n    metrics, output_lens = calculate_metrics(\n  File \"/sgl-workspace/sglang/python/sglang/bench_serving.py\", line 894, in calculate_metrics\n    p99_e2e_latency_ms=np.percentile(e2e_latencies, 99) * 1000,\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py\", line 4283, in percentile\n    return _quantile_unchecked(\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py\", line 4555, in _quantile_unchecked\n    return _ureduce(a,\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py\", line 3823, in _ureduce\n    r = func(a, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py\", line 4722, in _quantile_ureduce_func\n    result = _quantile(arr,\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py\", line 4831, in _quantile\n    slices_having_nans = np.isnan(arr[-1, ...])\nIndexError: index -1 is out of bounds for axis 0 with size 0\n```\n\n\n### Reproduction\n\n\n\n```bash\n# Node 1\nSGLANG_TORCH_PROFILER_DIR=/root/sglang/profile_log \\\npython3 -m sglang.launch_server \\\n--model-path deepseek-ai/DeepSeek-R1 \\\n--enable-torch-compile \\\n--torch-compile-max-bs 8 \\\n--tp 16 \\\n--dist-init-addr [YOUR NODE1 IP HERE] \\\n--nnodes 2 \\\n--node-rank 0 \\\n--trust-remote-code \\\n--host 0.0.0.0 \\\n--port 8080\n\n\n# Node 2\nSGLANG_TORCH_PROFILER_DIR=/root/sglang/profile_log \\\npython3 -m sglang.launch_server \\\n--model-path deepseek-ai/DeepSeek-R1 \\\n--enable-torch-compile \\\n--torch-compile-max-bs 8 \\\n--tp 16 \\\n--dist-init-addr [YOUR NODE1 IP HERE] \\\n--nnodes 2 \\\n--node-rank 1 \\\n--trust-remote-code \\\n--host 0.0.0.0 \\\n--port 8080\n```\n\nbenchmark scripts\n```bash\npython3 -m sglang.bench_serving --backend sglang \\\n--tokenizer deepseek-ai/DeepSeek-R1  \\\n--num-prompts 16 \\\n--dataset-name sharegpt \\\n--sharegpt-context-len 1024 \\\n--sharegpt-output-len 32 \\\n--host 127.0.0.1 \\\n--profile \\\n--port 8080\n```\n\n### Environment\n\nMy setting: two 8*H20 nodes + sglang v0.4.2.post4\n\n```\n python3 -m sglang.check_env\nINFO 02-25 05:54:48 __init__.py:190] Automatically detected platform cuda.\nPython: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.161.08\nPyTorch: 2.5.1+cu124\nsgl_kernel: 0.0.3.post3\nflashinfer: 0.2.0.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.61.1\ntiktoken: 0.8.0\nanthropic: 0.45.2\ndecord: 0.6.0\n\n\n8 GPU + 8 NIC, Topology omitted.\n>>> \n```",
    "labels": [
      "help wanted",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-24T11:09:31+00:00",
    "closed_at": "2025-02-26T10:52:39+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3815/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3815"
  },
  {
    "number": 3614,
    "title": "[Feature] support torch compile cache for DeepSeek V3/R1",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled\n\nThe time taken for each startup is currently too long when torch compile is enabled. It needs optimization.\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "help wanted",
      "high priority",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-16T16:18:21+00:00",
    "closed_at": "2025-02-21T18:18:09+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3614/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/3614"
  },
  {
    "number": 3206,
    "title": "[Feature] Improve Multi-node recipe to run inference",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCould someone improve the example for serving DeepSeek 3 on multiple nodes adding information on how to run into a slurm cluster and singularity container.\n\nhttps://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208\n\n### Related resources\n\n_No response_",
    "labels": [
      "help wanted",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-01-29T12:21:53+00:00",
    "closed_at": "2025-01-31T23:48:24+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3206/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3206"
  }
]
[
  {
    "number": 1129,
    "title": "[Feature] Support TRI-ML/prismatic-vlms",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nI'm trying to speed up inference for new VLM models on huggingface: https://huggingface.co/TRI-ML/prismatic-vlms/tree/main. I'm wondering if there are additional documentation on how to adapt new models? \n\n### Related resources\n\nThe model I'm trying to adapt is detailed here: https://arxiv.org/pdf/2402.07865. ",
    "labels": [
      "good first issue",
      "feature",
      "new-model"
    ],
    "state": "open",
    "created_at": "2024-08-16T18:15:10+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1129/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1129"
  },
  {
    "number": 3524,
    "title": "[Feature] Extend CustomLogitProcessor to Support input_ids in call Method",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nThanks @hongpeng-guo for PR #2396. After reviewing your work, I'd like to propose an enhancement to the `CustomLogitProcessor`. Specifically, I suggest modifying its `__call__` method to accept `input_ids` as an additional parameter\u2014similar to the implementation in Huggingface (see this [doc](https://huggingface.co/docs/transformers.js/en/api/generation/logits_process#module_generation/logits_process.LogitsProcessor)). This change would allow constraints to be applied conditionally based on the entire history of input tokens, enabling more flexible and context-aware processing.\n\nThank you for considering this feature request!\n\n### Related resources\n\n[Huggingface LogitsProcessor.](https://huggingface.co/docs/transformers.js/en/api/generation/logits_process#module_generation/logits_process.LogitsProcessor)",
    "labels": [
      "inactive",
      "feature"
    ],
    "state": "closed",
    "created_at": "2025-02-12T12:48:38+00:00",
    "closed_at": "2025-06-25T00:20:02+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3524/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3524"
  },
  {
    "number": 2542,
    "title": "[Feature] (Willing to PR) Avoid KV cache occupying GPU memory when not used",
    "body": "### Checklist\r\n\r\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [X] 2. Please use English, otherwise it will be closed.\r\n\r\n### Motivation\r\n\r\nHi thank you for the library! The use case is that, when doing online PPO, I hope to use SGLang to generate llm completions, and then use RL to do gradient descent on those completions.\r\n\r\nThe problem is, to do this on a single GPU, the timeline is \"SGLang generate - Torch backward - repeat it\". Thus, when torch doing backprop, I hope SGLang can free its KV cache memory consumption, otherwise torch will not have enough memory.\r\n\r\nThanks for any suggestions!\r\n\r\n### Related resources\r\n\r\n_No response_",
    "labels": [
      "high priority",
      "collaboration",
      "inactive",
      "feature"
    ],
    "state": "closed",
    "created_at": "2024-12-22T09:07:26+00:00",
    "closed_at": "2025-03-16T14:34:36+00:00",
    "comments": 43,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2542/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2542"
  },
  {
    "number": 3429,
    "title": "[Feature] support /v1/completions suffix parameter for completion",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nparameter suffix is not supported in sglang's openapi  v1/completions yet. but it's necessary for code completion.\ncan I support this?\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive",
      "feature"
    ],
    "state": "closed",
    "created_at": "2025-02-09T14:30:57+00:00",
    "closed_at": "2025-04-14T00:19:34+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3429/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3429"
  },
  {
    "number": 3811,
    "title": "[Feature] Support destroying model weights and all KV cache during runtime",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nIn the RLHF training process, if the inference engine and the training engine are deployed on the same batch of machines, to save GPU memory, it is necessary to offload the KV Cache and model weights from the gpu memory to the cpu memory.\n\n### Related resources\n\nhttps://github.com/vllm-project/vllm/pull/11743",
    "labels": [
      "feature"
    ],
    "state": "closed",
    "created_at": "2025-02-24T08:43:23+00:00",
    "closed_at": "2025-02-24T17:19:50+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3811/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3811"
  },
  {
    "number": 3808,
    "title": "[Feature] full-duplex audio multimodal service support",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nDoes this framework support full-duplex audio multimodal services? The client sends an audio segment, and the server processes it (alternatively, the client can pre-process it and generate embeddings to send directly to the server). The issue I'm encountering is that audio multimodal data cannot use auto prefix caching. Would this result in poor performance, especially in multi-turn conversations?\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive",
      "feature"
    ],
    "state": "closed",
    "created_at": "2025-02-24T08:06:44+00:00",
    "closed_at": "2025-04-28T00:19:27+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3808/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3808"
  },
  {
    "number": 3393,
    "title": "[Feature] Can router support prometheus metrics",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nK8s is often used to deploy applications online. After the router module is introduced, related service indicator monitoring is also required. Therefore, similar to https://github.com/sgl-project/sglang/pull/1853 provided by the server, does it support the collection of monitoring indicators of the router?\n\n### Related resources\n\n_No response_",
    "labels": [
      "enhancement",
      "inactive",
      "feature",
      "router"
    ],
    "state": "closed",
    "created_at": "2025-02-08T06:42:46+00:00",
    "closed_at": "2025-04-28T00:19:29+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3393/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3393"
  },
  {
    "number": 2668,
    "title": "How to obtain the hidden states of generated tokens?",
    "body": "Thank you for your outstanding work! I was wondering if there\u2019s a way to access the hidden states for each generated token at every layer. Many thanks!",
    "labels": [
      "inactive",
      "feature",
      "RLHF"
    ],
    "state": "closed",
    "created_at": "2024-12-30T11:08:54+00:00",
    "closed_at": "2025-03-01T00:18:53+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2668/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2668"
  },
  {
    "number": 3646,
    "title": "[Feature Request] Accepting multiple weight updates in update_weights_from_distributed",
    "body": "The current sglang implementation only offers one weight update request at a time for `update_weights_from_distributed`. While parameters are provided to the inference server with torch's broadcast function, it is still needed to manually send a HTTP request for each parameter, although the request is very small and only has name, dtype, and shape.\n\nIs it possible to make this also take a list of (named, dtype, shape) for parameters? In this case, one HTTP request would be needed if we would like to update many (or even all) parameters.\n \nhttps://github.com/sgl-project/sglang/blob/714f3e6362791ccc54a8845e5c6261d1e6d156cc/test/srt/test_update_weights_from_distributed.py#L290-L305\n",
    "labels": [
      "inactive",
      "feature"
    ],
    "state": "closed",
    "created_at": "2025-02-17T18:38:08+00:00",
    "closed_at": "2025-05-30T00:19:18+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3646/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3646"
  },
  {
    "number": 8053,
    "title": "[Feature] LRU Eviction Strategy for Lora Adapters: Evicting Adapters with Priority",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nIn scenarios where Lora adapters are shared, since a single base model is used collectively and there is an upper limit on the number of Lora adapters that a single node can host, we may need to introduce an LRU strategy (similar to VLLM's approach) to evict some adapters. However, the reactivation and deactivation of adapters can affect service quality (SLOs), which may be unacceptable in certain production environments. Therefore, a feature is required to ensure that specific adapters are not automatically evicted by the LRU mechanism.\n\n\n### Related resources\n\nCC  @Fridge003  @lifuhuang   @lw9527",
    "labels": [
      "high priority",
      "feature"
    ],
    "state": "open",
    "created_at": "2025-07-15T08:51:27+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/8053/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/8053"
  },
  {
    "number": 4173,
    "title": "[Feature] Support correctly exit using ctrl+c",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nIf we want to fully terminate the server processes for now, we could only do ctrl+c and sudo kill the list of processes. \n1. Could we make an __exit__() process to automatically kill all the sub-processes when we hit ctrl+c\n2. The function should also support when sudo kill one of the sub-processes, the other sub-processes will be automatically killed using the __exit__() method. \n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive",
      "feature"
    ],
    "state": "closed",
    "created_at": "2025-03-07T08:37:28+00:00",
    "closed_at": "2025-05-07T00:19:03+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4173/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4173"
  },
  {
    "number": 3438,
    "title": "[Feature] Support Lora for VocabParallelEmbedding layer",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nSupport lora for `VocabParallelEmbedding`. Not a trivial task.\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive",
      "feature",
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-02-09T19:13:40+00:00",
    "closed_at": "2025-06-30T00:21:14+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3438/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3438"
  },
  {
    "number": 3971,
    "title": "[Feature] Prefill assistant response",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nOAI API doesn't natively support prefilling an assistants response. vLLM and Aphrodite has the additional support for `continue_final_message` which would be need to have for SGLang to give developers even much more control.\n\nShould be relatively easy for someone to implement. It's simply not allowing chat template EOS to take over in a turn where assistant response is last and this flag is enabled and a generation is requested. This was originally implemented with exact same parameter name in transformers, which became a feature in vLLM and Aphrodite.\n\n### Related resources\n\nhttps://huggingface.co/docs/transformers/main/en/chat_templating\nhttps://github.com/aphrodite-engine/aphrodite-engine/blob/e64075b8937786311f6441fab5103f9ebf4e1dd8/aphrodite/endpoints/openai/protocol.py#L225-L233\nhttps://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#id7\n\nNot seeing any extra parameter support\nhttps://docs.sglang.ai/backend/openai_api_completions.html\n",
    "labels": [
      "good first issue",
      "help wanted",
      "feature"
    ],
    "state": "closed",
    "created_at": "2025-02-28T21:34:21+00:00",
    "closed_at": "2025-04-21T15:22:27+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3971/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3971"
  },
  {
    "number": 3545,
    "title": "[Feature] Support multimodal models for Native API/ Engine",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nadd the multimodal support to Native API/ Engine\n\n### Related resources\n\n_No response_",
    "labels": [
      "feature",
      "MLLM"
    ],
    "state": "closed",
    "created_at": "2025-02-13T10:39:45+00:00",
    "closed_at": "2025-02-25T17:52:53+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3545/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3545"
  },
  {
    "number": 3647,
    "title": "[Feature] Support unified paging in multi-lora serving",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCurrently, SGL doesn't support the unified paging feature proposed by S-LoRA. However, this feature is important for memory management in multi-LoRA serving.\n\n### Related resources\n\n_No response_",
    "labels": [
      "enhancement",
      "inactive",
      "feature",
      "lora"
    ],
    "state": "open",
    "created_at": "2025-02-17T19:14:47+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3647/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3647"
  },
  {
    "number": 2568,
    "title": "[Feature] plan to support Block Schedule\uff1f ",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nWill the Block Schedule scheme in FlexLLMGen bring greater throughput? Will sglang consider supporting it?\n\n### Related resources\n\n_No response_",
    "labels": [
      "feature"
    ],
    "state": "closed",
    "created_at": "2024-12-24T06:05:46+00:00",
    "closed_at": "2024-12-25T05:42:06+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2568/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2568"
  },
  {
    "number": 6589,
    "title": "[Feature] Tool Call Roadmap",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n## Motivation\n\nAdd a list of issues need to resolve in tool call.\n\n## Track for Tool Call Issues\n\n### High Piority\n\nIssues related to accuracy, consistency, and performance.\n\n- [x] [Multiple Tool Call Support for MistralDetector and Qwen25Detector](https://github.com/sgl-project/sglang/issues/6589#issuecomment-2907987558)\n#6597 \n\n- [ ] [JSON Double Dumping Behavior](https://github.com/sgl-project/sglang/issues/6589#issuecomment-2907988051)\n\n- [x] [`ToolCallItem.tool_index` not following OpenAI API](https://github.com/sgl-project/sglang/issues/6589#issuecomment-2907988438) \n#6715 \n#6655 \n#6678 \n\n----\n\n### Medium Priority\n\nIssues that are not immediate, such as features still WIP, or needs refactor, or edge cases.\n\n- [ ] [Tests for `get_structure_tag` in `FunctionCallParser`](https://github.com/sgl-project/sglang/issues/6589#issuecomment-2907988553)\n\n- [x] [DeepSeekV3Dectector may have issues with Multiple Tool Calls Streaming Parsing](https://github.com/sgl-project/sglang/issues/6589#issuecomment-2908033411) \n#6655 \n\n### Related resources\n\n_No response_",
    "labels": [
      "enhancement",
      "high priority",
      "feature",
      "function-calling"
    ],
    "state": "open",
    "created_at": "2025-05-25T10:29:03+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6589/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6589"
  },
  {
    "number": 5964,
    "title": "[Feature] Support more multi-modal input for VLM",
    "body": "### Motivation\n\nThe current endpoint only supports image data input, limiting its flexibility for diverse VLM use cases. We need additional input formats, particularly for RL applications:\n(Could be split into multiple PRs)\n\n- [x] Pre-computed Image Embeddings\n- [ ] Pixel Values\n- [ ] Pixel Value Range Parameters (min_pixel/max_pixel) for qwen-vl\n\nWelcome to propose more.\n\n#### Benefits\n\n1. Enhanced flexibility for RL workflows\n2. Reduced preprocessing overhead\n3. Better integration with existing pipelines",
    "labels": [
      "good first issue",
      "help wanted",
      "feature",
      "MLLM"
    ],
    "state": "open",
    "created_at": "2025-05-02T02:28:40+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5964/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5964"
  },
  {
    "number": 1283,
    "title": "[Feature] Support phi-3 model",
    "body": "### Motivation\r\n\r\nphi-3 model is popular for its tiny size. we should also support it\r\n\r\n### Related resources\r\n\r\n_No response_",
    "labels": [
      "feature"
    ],
    "state": "closed",
    "created_at": "2024-09-01T05:44:49+00:00",
    "closed_at": "2024-09-03T04:49:41+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1283/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1283"
  },
  {
    "number": 4822,
    "title": "[Feature] Load model weight in parallel",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nWe're using [a distributed file system](https://juicefs.com) to store LLM weights in a Kubernetes environment. As a typical design choice, the system is tuned for max parallelism, which behaves relatively poor with single-threaded, sequential reads. Through benchmarking, we found that model loading can be up to 5 times faster by using 8 threads, compared to the current performance of SGLang.\n\nWe hope there can be an option to enable parallelism while reading the model weights. It is not so useful for users who store their weights in a physical drive, but could be life-saving for users with distributed storage backend, including S3 (via S3FS).\n\n### Related resources\n\nvLLM uses Run:ai Model Streamer for streaming models concurrently to GPUs: https://docs.vllm.ai/en/stable/models/extensions/runai_model_streamer.html\n\nTriton also supports loading models in parallel: https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_management.html#concurrently-loading-models",
    "labels": [
      "help wanted",
      "inactive",
      "feature"
    ],
    "state": "closed",
    "created_at": "2025-03-27T17:37:11+00:00",
    "closed_at": "2025-06-01T00:24:15+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4822/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4822"
  },
  {
    "number": 2237,
    "title": "[Feature] QwQ support",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nref https://qwenlm.github.io/blog/qwq-32b-preview/\n\n### Related resources\n\n_No response_",
    "labels": [
      "enhancement",
      "feature"
    ],
    "state": "closed",
    "created_at": "2024-11-28T08:42:43+00:00",
    "closed_at": "2024-12-01T10:27:32+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2237/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/2237"
  },
  {
    "number": 2569,
    "title": "[Feature] (Willing to PR) Proposal: Drop-in fast replacement of `PreTrainedModel.generate`",
    "body": "### Checklist\r\n\r\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [X] 2. Please use English, otherwise it will be closed.\r\n\r\n### Motivation\r\n\r\nHi thanks for the lib! Currently, a lot of code uses `model.generate()`, such as TRL's PPOTrainer, etc. If we can make a drop-in replacement of it using SGLang, then everyone can very easily speed up their code related to generation. For example, TRL's PPOTrainer, OpenRLHF's train_ppo.py (not the train_ppo_ray.py which is more for distributed training). IMHO there are many places this can be useful - many online RL algorithm can benefit from this.\r\n\r\nAs for when to update SGLang weight from HF weight, most naive solution may be, we update weights *every* time the generate is called. This may not be a big problem, because we can configure the PPO batch size to be so huge that the model.generate is only called once.\r\n\r\nRelated: https://github.com/sgl-project/sglang/issues/2542 With that, we can reduce memory footprint outside generate.\r\n\r\n### Related resources\r\n\r\n_No response_",
    "labels": [
      "enhancement",
      "high priority",
      "collaboration",
      "inactive",
      "feature",
      "RLHF"
    ],
    "state": "closed",
    "created_at": "2024-12-24T06:18:24+00:00",
    "closed_at": "2025-03-30T00:19:36+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2569/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2569"
  },
  {
    "number": 1156,
    "title": "[Feature] support W8A8(FP8) and KV Cache FP8 for DeepSeek V2",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nAs titled. Make DeepSeek V2 MLA Faster!\n\n### Related resources\n\n_No response_",
    "labels": [
      "feature"
    ],
    "state": "closed",
    "created_at": "2024-08-19T17:21:17+00:00",
    "closed_at": "2024-09-01T09:51:32+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1156/reactions",
      "total_count": 4,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 4,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/1156"
  },
  {
    "number": 2583,
    "title": "[Feature] Proposal: Releasing SGLang memory when idle",
    "body": "### Proposal 1: Release KV cache when engine is idle\r\n\r\nWhen using SGLang for generation in a training pipeline (such as PPO), at the phase of running HuggingFace model forward/backward, SGLang currently needs to take a lot of memory even though it does not use it. It would be great to make SGLang use as little memory as possible when it is idle.\r\n\r\nExample usage cases:\r\n* Suppose we run OpenRLHF on 8xH100, the currently we may allocate 4xH100 for vllm/SGLang and another 4xH100 for HF model (thanks @zhaochenyang20 for providing this usage scenario).\r\n\t* If we make SGLang use little memory when idle, then we can run the same experiment on half number of GPUs (4xH100) by putting those SGLang engines on the same GPUs as HF models.\r\n* Suppose we run PPO on 1xH100 for a 7B model with Adam offloading (thanks @zhaochenyang20 for providing this usage scenario). Then policy (7Bx2) + critic (7Bx2) + ref (7Bx2) + reward (7Bx2) already takes 56B. The current SGLang needs 7Bx2 for weights and some memory for KV cache, thus it may not easy to fit the 80GB card.\r\n\t* If we implement the proposal 1 and proposal 2, we will have roughly 24B room for HF model forward/backward, and 24B room for SGLang to do generation. (We may have more if quantizing ref & reward model though not sure whether it will work.)\r\n* Suppose we run OpenRLHF on 1x4090 for a 0.5B model, then the memory is also very limited like the 1xH100 & 7B model case.\r\n    * If the proposals are successfully implemented, we may be able to run in such scenarios.\r\n\r\nOne potential optimization for memory is to release KV cache:\r\n* When the training pipeline does not need SGLang (e.g. doing HF model forward/backward in PPO), let SGLang be in a \"paused\" mode, and later \"resume\" it when we need to use SGLang to do generation.\r\n* When SGLang enter \"paused\" mode, release the KV cache ([link to hacky experiment](https://github.com/sgl-project/sglang/issues/2542#issuecomment-2560540518)) by simply deleting the tensors.\r\n* When SGLang later \"resume\", re-create the KV cache tensors.\r\n\r\nI will PR for this as soon as having some time (hopefully soon).\r\n\r\n### Proposal 2: Release model weights when engine is paused\r\n\r\nAnother part of memory occupied by SGLang is the model weights. Thus one potential solution is:\r\n* When SGLang is paused, we delete the model weights (e.g. maybe by `model.to('meta')`, not tested) to release memory\r\n* When SGLang is resumed, we recreate *empty* model weights (e.g. by `model.to_empty(device='cuda')`)\r\n* Then, users should do `update_weight` to provide new weights to SGLang.\r\n\t* This is not an overhead, because during some RLHF processes, we already need to call `update_weight` before a `generate` to use the latest updated weights instead of outdated weights.\r\n\r\n### Proposal 3: Update SGLang model weights when on same GPU\r\n\r\nCurrently, when we do `update_weight` to copy HF model weight to SGLang model weight, it seems we will use the torch `broadcast` operation. However, when users put HuggingFace model and SGLang model on the same GPU, it may be possible to use more lightweight solutions to avoid the overhead of `broadcast`.\r\n\r\nTo be more specific:\r\n* Initialization\r\n\t* Users provide their HF model to SGLang Engine\r\n\t* SGLang shares the tensors of this model to the SGLang runtime process\r\n* Weight update\r\n\t* Users trigger \"update weight from the previously provided HF model\" operation\r\n\t* SGLang runtime process read the aforementioned tensor to update the SGLang model weights\r\n\r\nThis is just a rough draft and there can be more details. For example, if it is possible for the tensor objects in HF model to change, then we may need to send the new tensors across processes again.\r\n\r\nRelated: #2542\r\ncc @zhaochenyang20\r\n",
    "labels": [
      "high priority",
      "inactive",
      "feature"
    ],
    "state": "closed",
    "created_at": "2024-12-26T02:23:14+00:00",
    "closed_at": "2025-03-01T00:18:51+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2583/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2583"
  },
  {
    "number": 3428,
    "title": "How to return reasoning_content from sglang server response?",
    "body": "I use docker-compose to deploy locally\n```\nservices:\n  sglang1:\n    image: lmsysorg/sglang:latest\n    container_name: sglang-DeepSeek-R1-Distill-Qwen-7B\n    volumes:\n      - /root/DeepSeek-R1-Distill-Qwen-7B:/DeepSeek-R1-Distill-Qwen-7B\n      # If you use modelscope, you need mount this directory\n      # - ${HOME}/.cache/modelscope:/root/.cache/modelscope\n    restart: always\n    #network_mode: host\n    # Or you can only publish port 30000\n    ports:\n      - 30000:30000\n    environment:\n      # if you use modelscope to download model, you need set this environment\n      SGLANG_USE_MODELSCOPE: true\n    entrypoint: python3 -m sglang.launch_server\n    command:\n      --model-path /DeepSeek-R1-Distill-Qwen-7B/\n      --host 0.0.0.0\n      --port 30000\n      --served-model-name deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n      --grammar-backend xgrammar\n    ulimits:\n      memlock: -1\n      stack: 67108864\n    ipc: host\n    healthcheck:\n      test: [\"CMD-SHELL\", \"curl -f http://localhost:30000/health || exit 1\"]\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              device_ids: ['1']\n              capabilities: [gpu]\n```\nbut the response doesn't contain the \"reasoning_content\".\n\n![Image](https://github.com/user-attachments/assets/1c0fdfa2-9a2a-4a8f-8149-e6f96fc776c7)",
    "labels": [
      "inactive",
      "feature"
    ],
    "state": "closed",
    "created_at": "2025-02-09T14:21:08+00:00",
    "closed_at": "2025-05-09T00:18:59+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3428/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3428"
  },
  {
    "number": 2561,
    "title": "[Feature] Running multi-node offline engine inference ( via SLURM)",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nA lot of academic institutions only allow access to larger node clusters via SLURM and it is not immediately clear how would I reuse the code to run Llama 405B BF16 on 2 nodes (by starting a server) to perform offline inference\n\n### Related resources\n\n_No response_",
    "labels": [
      "help wanted",
      "collaboration",
      "feature"
    ],
    "state": "closed",
    "created_at": "2024-12-23T15:24:49+00:00",
    "closed_at": "2025-01-31T23:58:27+00:00",
    "comments": 39,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2561/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2561"
  },
  {
    "number": 3902,
    "title": "[Feature] Allow Serving Requests During CUDA Graph Capture",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n#### Background\n\nWhen `--enable-cuda-graph`, service restart (after crashes/unexpected exits) currently requires about 10-minute CUDA Graph capture process before becoming operational (longer with torch.compile enabled). This creates significant service downtime despite even with external health check and automatic restart, unless hot-standbys are deployed.\n\nI raised this question in community discussion group and received reply from @ baizhou: Theoretically feasible to let model runner choose whether to replay existing CUDA Graph.\n\n#### Requested Feature\n\nAdd an option or fallback mechanism to allow serving requests without CUDA Graph replay during initialization, prioritizing availability over performance.\n\n#### Discussion\n\nI am open to discussing the details further and would appreciate any thoughts on implementation from the community. Thank you.\n\n### Related resources\n\n_No response_",
    "labels": [
      "feature"
    ],
    "state": "closed",
    "created_at": "2025-02-27T00:22:02+00:00",
    "closed_at": "2025-05-05T11:23:54+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3902/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3902"
  },
  {
    "number": 3282,
    "title": "[Feature] Support compatibility between Cuda Graph and Lora",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCurrently Lora and Cuda Graph cannot be used at the same time. \n\nTheoretically they should be compatible.\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive",
      "feature",
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-02-04T06:27:19+00:00",
    "closed_at": "2025-04-29T06:30:45+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3282/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3282"
  },
  {
    "number": 859,
    "title": "[Feature] plan to support medusa?",
    "body": "### Motivation\n\nplan to support medusa?\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive",
      "feature"
    ],
    "state": "closed",
    "created_at": "2024-08-01T02:41:21+00:00",
    "closed_at": "2024-12-20T00:16:50+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/859/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/859"
  },
  {
    "number": 3823,
    "title": "[Feature] GPU inference on AMD Ryzen AI (370HX-890M) iGPU + NPU",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nRyzen AI devices have been out since mid 2024 yet there's no end user friendly local inference engine that can use the iGPU or the NPU for inference. Some people seem to be able to make it working using hacks but it's still a hit or miss and you need to build your own custom room and hip packages to it to kind of work. \n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive",
      "feature",
      "amd"
    ],
    "state": "closed",
    "created_at": "2025-02-24T15:49:18+00:00",
    "closed_at": "2025-04-26T00:17:55+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3823/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3823"
  }
]
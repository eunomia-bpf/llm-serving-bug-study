[
  {
    "number": 2961,
    "title": "QVQ Prefill stage slow",
    "body": "Using int4 QVQ 72b model. https://huggingface.co/kosbu/QVQ-72B-Preview-AWQ \nbasic config: 4 2080ti 22G tp=4\n\n```python3 -m sglang.launch_server --model-path /root/model/QVQ-72B-Preview-AWQ --host 0.0.0.0 --port 30000 --tp 4 --mem-fraction-static 0.7 ```\n\n<img width=\"909\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/97af8f64-029b-4d4a-8e84-38dd3ede0340\" />\n\n\nAs you may see, the prefilling stage take 20s.\n\nWhat i can do to optimize the speed?\nOr do i have option to turn off prefilling, when performing only one request?",
    "labels": [],
    "state": "closed",
    "created_at": "2025-01-18T08:03:33+00:00",
    "closed_at": "2025-01-26T09:13:00+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2961/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2961"
  },
  {
    "number": 2778,
    "title": "[Bug] Failed to create router: Timeout 300s waiting for workers to become healthy",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nAny time I try some slower to start options with the new router it times out at 300 sec - e.g. try `--enable-torch-compile`, which can easily take 10-15min to start with all its tune attempts.\r\n\r\nHow can this timeout be overridden to be made higher by the user when needed?\n\n### Reproduction\n\npython -m sglang_router.launch_server --enable-torch-compile  --model-path deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct \n\n### Environment\n\nPython: 3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:16:10) [GCC 13.3.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\r\nCUDA_HOME: /usr/local/cuda-12.6\r\nNVCC: Cuda compilation tools, release 12.6, V12.6.85\r\nCUDA Driver Version: 560.35.03\r\nPyTorch: 2.5.1+cu124\r\nsglang: 0.4.1.post3\r\nflashinfer: 0.1.6+cu124torch2.4\r\ntriton: 3.1.0\r\ntransformers: 4.47.1\r\ntorchao: 0.7.0\r\nnumpy: 1.26.4\r\naiohttp: 3.11.11\r\nfastapi: 0.115.6\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.27.0\r\ninteregular: 0.3.3\r\nmodelscope: 1.21.1\r\norjson: 3.10.13\r\npackaging: 24.2\r\npsutil: 6.1.1\r\npydantic: 2.10.4\r\nmultipart: 0.0.20\r\nzmq: 26.2.0\r\nuvicorn: 0.34.0\r\nuvloop: 0.21.0\r\nvllm: 0.6.4.post1\r\nopenai: 1.59.3\r\nanthropic: 0.42.0\r\ndecord: 0.6.0\r\nNVIDIA Topology:\r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8     NIC9    NIC10   NIC11   CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     PIX     PIX     SYS     SYS     SYS     SYS     SYS     SYS      SYS     SYS     SYS     0-12,104-116    0               N/A\r\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS      SYS     SYS     SYS     26-38,130-142   2               N/A\r\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS      SYS     SYS     SYS     39-51,143-155   3               N/A\r\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS      SYS     SYS     SYS     13-25,117-129   1               N/A\r\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX     PIX      SYS     SYS     SYS     52-64,156-168   4               N/A\r\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      PIX     SYS     SYS     78-90,182-194   6               N/A\r\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      SYS     PIX     SYS     91-103,195-207  7               N/A\r\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      SYS     SYS     PIX     65-77,169-181   5               N/A\r\nNIC0    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     PIX     SYS     SYS     SYS     SYS     SYS     SYS      SYS     SYS     SYS\r\nNIC1    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      PIX     SYS     SYS     SYS     SYS     SYS     SYS      SYS     SYS     SYS\r\nNIC2    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX      X      SYS     SYS     SYS     SYS     SYS     SYS      SYS     SYS     SYS\r\nNIC3    SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS      SYS     SYS     SYS\r\nNIC4    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS      SYS     SYS     SYS\r\nNIC5    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS      SYS     SYS     SYS\r\nNIC6    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     PIX      SYS     SYS     SYS\r\nNIC7    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      PIX      SYS     SYS     SYS\r\nNIC8    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX      X       SYS     SYS     SYS\r\nNIC9    SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       X      SYS     SYS\r\nNIC10   SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      SYS      X      SYS\r\nNIC11   SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      SYS     SYS      X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  NIC8: mlx5_8\r\n  NIC9: mlx5_9\r\n  NIC10: mlx5_10\r\n  NIC11: mlx5_11\r\n\r\n\r\nulimit soft: 1024",
    "labels": [],
    "state": "closed",
    "created_at": "2025-01-07T22:23:43+00:00",
    "closed_at": "2025-01-20T22:50:41+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2778/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2778"
  },
  {
    "number": 5034,
    "title": "[Bug] flashinfer.jit: Loading JIT ops: cascade",
    "body": "\n### Describe the bug\n\nDiscovered an interesting bug: when using the offline engine, if the amount of generated data is small, for example, just a few entries, it seems the loading of this cascade operator is not triggered. However, once the loading of this operator is triggered, it throws an error in my current version.\n```bash\n2025-04-03 19:59:12,086 - INFO - flashinfer.jit: Loading JIT ops: cascade\n2025-04-03 19:59:12,938 - INFO - flashinfer.jit: Loading JIT ops: cascade\n[2025-04-03 20:06:34 DP1 TP0] Watchdog timeout (self.watchdog_timeout=300)\n[2025-04-03 20:06:34 DP1 TP0] self.cur_batch.batch_size()=1, self.cur_batch.reqs=[Req(rid=71d702268ab8458b98bc40e7667f80ec, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[])], self.token_to_kv_pool_allocator.available_size()=2468326, self.tree_cache.evictable_size()=0, \n[2025-04-03 20:06:34 DP0 TP0] Watchdog timeout (self.watchdog_timeout=300)\n[2025-04-03 20:06:34 DP0 TP0] self.cur_batch.batch_size()=362, self.cur_batch.reqs=[Req(rid=6eea92c1e76f4050a03d8ffeb8c76eac, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=7b2a57578ec041e28ae21cd5710bcfb6, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=9d44f5921aba4845bd4f3adc54002455, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=82f8708b0b4d4db3ab40a0b11c1ae71a, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=a2c84e7756564240a5f959c496350f87, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=1035bf3f23c6422cbdd6c06fac238f9f, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=63b35b9e4211461d87e036c4ae786fbf, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=8b7a2801658048cfa174af7708dfe4e9, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=6f7bc81725d54741b5b8f6885fd714f8, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=a776c753237d40ae9db86ab920dcbc54, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=86fb089b00e84e54bcf55ec78b1a027c, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=1fbcea225bf4471ebc8c1b3514317b65, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=d47e0d27a47949cda5f3715a7a8726d5, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=f5a0581cf7ed4fbb9f55b41a19156e11, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=f53bbe32aca64655b2463c6e1d26883d, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=83e9e1f2f51f44b6804106d0bda2f003, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b3b68c5a7d01444d8de7b0ac0f6dbef6, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=18da02c9672045bb8bd31ff1f552067b, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=76be295776c946aea064d25026a1c3cb, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=f94aecbe27d04c51a71fb3eaa10f1e7b, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=6fdb8fa4ad1b40a1bbf7544a263a8943, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=0c16f4586c1d4f20ba2fb9c69cae45bf, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=2d18dad5897642909ab6c6951ed397af, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=73ebd93ecb3f4e83971214892c46a578, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=db157d75e78640b68752399a8f6fa9ea, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=0b5e6a53896041559730f892be7d07e4, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=d63624120d6e469caa8f4f9e26f00136, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=7161fc15d52a4d8cbc236d61a051b036, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b9da236be4204d97b48678deacd5ea4a, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=ea8d23cc224147a08d3f496061042883, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=6163a6d4f978448788b2431b12675d9c, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=ee9507f8be604819a5f0fa57495e2d46, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=9dde413329ee47fb98f27511751e81b9, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=6fa3ccf4d1834294b0d9dab3b89427f8, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=a54e41891d844b3f943173b4cdb4fdda, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=a376b0ac174c49fe96b2bbf8b35442fb, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=68958c98901142fcbe373d06b171db85, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=c7306b29071743e2a5a8b215fdbfb335, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=99fdac5c1bf24ccda87f99f6795939ff, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=2fb225ba632d46629b2b5e5b6393ca6a, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=843bd89d4777459b99187ce53d070697, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=e9987e736ec84cbfa6dad4d3be95edbe, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=44186006d941403698bd17db2cb54316, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=fb6a7a3e95a543a8a2fae7d35ae736d0, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=a0ed33a648c14f6e8ae92d6ab0d7b80d, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=352e6d428c5d4ecf9d4d53a0957e453e, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=4211dd2ec6594ccb909cfd1622702968, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=bf7a73e3ee1d47c89fdb32334a479fa8, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=605fe870228a4bafac80f5a501da83ec, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=a32e2e2a33a54252b4a269bfb3b54a4b, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=a62df00e1a34407c9a6f2b6f8a6a92b2, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=8bf0c0ccb0454335b8801dc38c080d80, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=a5f16d791f14418fa7d04e54482816c4, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=da25b721cb2244dc884419bebe8e5df0, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=8ee1b11d95fd467e97e05ad1f3a98103, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=c305675126ca4b248133d7ad96df8b21, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=3300451d288d4c84aa080a55992a0bb2, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=942530412a1d4521adb440bb550beb0d, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=4ab0413936d345e7b905d23ecf1f68d6, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=5629262597b748bd8c66448d6059542c, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b5b56fd7f8c346fb8b410bb005883d80, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=81b869c5f24d46d78d516d3b0bda9b1e, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=42668440c08e4f6eaa605ec3be79e308, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=299d053353f8483a808776a09452a352, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=ceb84518b21940f1a473cf06eebbd261, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=d5d3699e26f24e4a8207c16ae7d02ca4, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=77934f1bb5314e2199170fe0efd80f0e, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=64075ab509964ecba01049e1f6318aab, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=e790f11f67ee47b58ccc81635b41022c, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b4f8d43400b54448aa0510c6aec6b21f, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b4885a872134424d82d3b6008d616402, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=f418c7a81ead4a5b95c42cc52a9badd3, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=da688282a3c245d59419fc2aca7f2c84, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=e81b1247a46c4a4d8c8fccfd9c8530ff, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=aed29cd0cc0b487ab6d50b54bc580330, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=4842906858f94f6e92e31ae386ef75b7, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=420ddb96e86d4eebb1d365d221cd06d4, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=47f5d6aaf7e54535910611af58bf0d89, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=201e447fa2c042518654480d83520908, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=fdc65caf85734df79136cb15cbb0236a, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=3ddc91a1127d43e38b46f757d530f774, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=c5e3e663fb6140e28855a46f4ccc2c44, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=e4e3ba7abc944fb49754940515c31ef1, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=c42098a6df994bf4a7816db32b6f7960, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=c1fa5231ffb9411b8ef7e85fe94e0686, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=5320fe4152de4012a511869a4a0ba245, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=0c9b3988f4474421ada095f0e98fa45d, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=7dd37503083b4a12929b51791a83e865, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=231668e8c7634f6b9fd96804b14b86a5, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=5335ad2b01b04e3db0a66c8060ebdc54, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=13fcb3b242e94e94861b6ae1e94af96b, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=a4a4ab8a511c47828268b13ac5b57884, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=9cacb7b518df45419da141e4c90dc17d, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=6acfbd3a57f34ef8b95f736025eb6066, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=95b51881453e4c53ae44af7bf902b01d, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=feae9d7fc06444bdabcdab057d763a4a, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=6172e80421bf456fadcdbb20de8d0aad, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=cfba7fb0bfa54a7f81530422a0c54498, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=bea02809581848cdbf3dcf846d481ea0, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=9320b50c3fa842ecba14c39e95694872, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=a71cfbf3f17a4652aba6751cf1f2e8d3, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=9b943d40b16c4a9da7aecf0b1ce9dc5f, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b751ae17987c44eb8feaf7ea4a8e79cc, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=9a27f01adff346b7b0f1575f20e5ea55, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=8029fd5121644564ab43e20f40985105, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=5822df3e5a884775b8d19c01f4f19139, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=251d23d78d9543c98792be9e63d646b5, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=bfb570ccc37f4babaed7291cc8860b3f, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=1030e2bf8cac439b87d3b3c38563007f, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=46ea4265618641619631f19fdbc895e3, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=9697dc70742b435abc0b503087b326b3, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=d75c544155af49f5bdb5423e652d486b, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=d18dcce5655f4756b4125885c79fecd9, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=0a3974c36d754177bd2eea9bacc07dc3, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=fdf73d62b3754866b94e9cdd6284cb4e, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=889760dfec524fc69714c83f0f711730, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=977b2553f0034d7db57bcc8d3b3bfbd1, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=0d4e876f3ae149cabc3a137515168b64, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b157bbd456e741f78f0061fc3e160dde, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=2d68e9e5e71d4be595b5970a6f75c2d4, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=c02ca039dc354a07a52ee78f5e43e0bb, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=9f59b03582b64b12ad0a1f0d7fb6b4e7, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=56eefbb5b03147509a70875b7a35ad09, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=7175167cdd4a4a0baa7ce4ff62b9b3db, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=6695f2dc1c40463cae61f02311b824f1, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=7727bb66b7b149fa9cff5fbc6d91f638, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=044d782cae1c4f3dba142c2f8c5b410a, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=40a6994f00934fb8be910cc2cadbfe24, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=fd90287f623041c98e939fe0f1db810d, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=9ffd8bd16c3941ae8447fdb48cd15219, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=527f8e3462d7408b8fd89eef1f743025, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=9cfb42aada394287b4d278e964e69a15, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=053ac66cf6d84a5da62e4991f3e30c5b, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=ee130013b7a94d538f835cd36a22ee6a, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=f7fda63febf94a529225cd1cbddbf768, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=7704fe7a615b4d00ab4f112098be7d0e, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=c42c6cfa30c04e13bc7d177debabe8a0, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=82ba35fbf65745b08ed8dc85e183404a, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b5938285292f415284479fe12f9fd81b, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=5160731482ca465a9b6fb2718cb1115c, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b191857d160b433896250c5816019cde, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=000fa24775904fb687504822d663f04c, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b0c9440cf1b1482c8d67784e975a89a6, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=6e3f0246b41d4c68bbd8850586e8f8f4, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=7546a18241394523bb5c7efd050be627, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=c1f30d4f86984c5eadc8dd043558f5c9, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=3f05968e956848a9aa87e18bffc6f5d2, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=77cb3d10807146079ad557e8aabfd201, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=c1b31c37a0e94dbf8b658dd7a95f0d75, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=d5d423b6fbfe40ff8a345c175156cbda, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=6c6f5b7eaa63446c85ec630324d59681, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=c4211a0188fa4c65ae8b16e75714a4db, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=58bd3c004b424c1b932bc290582366ca, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=cc60954847174cbfa942231cf6fb4e05, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=dd08465d93f940248596f9ed1c77f357, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=45ac6fb3572c4f3689b6053e99adc74f, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b053bef98795486db576c729da04a84d, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b36b4d41782441448df509173899050d, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=e017fae550044b26a0b9379416bf2643, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=136f53f84274459da41c52b06fcc02d4, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=bc06809e83e149ecb9d78d362e293751, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=12c4167c27bc42b985fb7d31f6f071cb, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=c33253578f4f4410b039cb065ba7e54b, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=4b541cdf815f4af09a27a4a92ecdfab7, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b8a2bcb6739640f4a0830172e4d83c51, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=408080b0fd58467ea45d4998e534c083, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=75c73aae65f94ade86131eeb892fc817, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=bd1724e1d2274085aaae0a4d5854741f, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=dfccce53888c437b907cd69226841014, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=1e43bcfa28894c60965a980ae1a6b4ac, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=8afb175e909c475db336d2d68e28b33a, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=080e30c0d21e4e239b3fc5505273faa7, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=096ac1b0f42b4cfcaf03cb8324ffbb8f, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=4e84a9a08a6b4baf8b2c653864568d29, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=7af901754f7f46beb2cb2843007315da, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=09402e629d234ed78928ebb822eb7722, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=d575e1d41d5941fca2c1999d74966223, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=fc7d465309924e5ebaf32f8831073217, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=7a7dbc3e19db4b998a266431a942a016, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=f64248cd993e40a684b17b8d723a11db, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=627f7f689f3c4c47a128d83e1b89e917, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=59b2224268ef427a8b9a3583098e4fba, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=9919d5e97cf846f3bc7261fa24356540, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=5207c101ef2c4c18ab7e690297a3d2ae, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=629478fcce86455f80ec2eb7489e2061, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=5729e0620f2d48e199f6816f3d8f6ad1, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=8e507178429a4d90b90021c9142d8e69, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=89e5c06360694053aba4193db0e13b6a, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=d081e075b3524a7a99baabf92cca8261, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=4294f322e5264dc78b98a637be94b60f, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=d2caa2c00c764243ab3c2a30ff2fb65f, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=23c86129f0bb4ebc83b40276535cd444, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=e8d15919b91d47a39fffb0e693a542d8, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=c40240215ed34daa8e383af0c9e90844, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=a70cd43f5c4843a3a965cc763650fb84, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=5cf8ec686a884b1292b7e81d4acdd323, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=baef13bdfeda4c7b8846a82c46faa554, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=2ede2a8e8d334f28b78815cfd6e61725, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=ffd17f9bddf34519a75ef5b577869a31, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=cc8a09eb559647ac8cb1af9a08f1ec7b, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=588de63a524343de9d6d454c7cc08112, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=42f35f67cd7e40cd9796edd58c966b02, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=0a1083b4adbe4db6bb68ae98d20c88b7, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=d7e77ed04d6b431d8d2705602badfd8d, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=680bbafcdd27475f8dca04599f422013, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=1c3a61db8e0b4022b25aa01ccc1bf004, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=6b5316232d1a4db1aaab2a82baba6ef3, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=9f3f639b5fe743f1a28aebf6158b7e72, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=2a42670f8b144969aec8c864c3a4ed8a, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=22651832ea6f4cf69e33f1ccbcde65f5, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=f50498bd92ac467cb27baa0b249f83d3, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b9b7c362d8f4419bb371f79f418d3ebb, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=d59c3cb2fd7645aebf234dbdc345b0f6, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=1d558c7389d74a3b835430a9a7512d13, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=d4574e81bbed4f1583b5e0f0c4992f2f, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=6bf3a3d204e04c709937e00641b071ef, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=74e058d6caef48b098467ee2e8ea42bf, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=7f465857144b42398dcbd8bcbfdf7bd5, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=7b574a82edba46e687d28ec25293c81b, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=e1a3d87ba9194acb978692392ae4ee4b, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=078d31b9ff2649e5905705e6961dd860, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=12981cd201fb48c5a7f395d820f84df3, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=bd2cab2339ea468ca493acc0abd308da, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=0d88324cf8b249acafaebf59531449e4, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=6876bdb1c9604d109230dae21c519330, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=bf5a3a7c05ba412a93a8a54a8febc911, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=cb7f23ff6ae243a68dcdc859385aa8fc, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=683345ee8ae54a499ff36325572c37bb, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=1c6e706338dc41fbaa944f851f66d19b, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=939f5bd553d841399bdd7feba11e2749, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=db3c493aef054befb8ec966625b52dea, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=3f26407b106348df9ec1abc32cb1b2b3, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=21248c0f348a407f8e1f004b41af8ef3, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=13db82f7df0d4adb9221fc6a4cf243ed, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=01033c2e71f44c06a6e89a80e9aa8654, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=656ea99137b54ac5a55c80a6115b21e3, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=670d591c85d044afa0b86b5375e02a23, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=3df47573eb7949d38b14294a829fada7, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=c4b6574dd89f43f9b58bbacbbac2cbe0, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=0b071a7cead6490d95a928014fab75b8, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=fc3a1a863d3d48579c7c91c23f89feb5, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=f362e754e72d4944a5e2b7ed2dc892b4, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=e4a72a596d28402aadb90c8493562455, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=27b76c765ea34cfe81773de826e080e6, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=bae9fd9601e540598a707b491eca69ec, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=dc3b69947323402698e2431e478dea1a, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=dc83a63d9a424e05b22de1977c80b40e, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=55fa9049f7e643b6b1996cc60a9cd6e3, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=2f39915866e940a49f9a2ff5b2e5ed89, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=625b9cd9562847da930282e29abd4164, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=e181034277d74ddd9915ed70a6b9021b, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=047a3dde23364d71beb1e0cd25597ee2, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=325db3595fa84cbcad093b598cdf4d94, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b275c7bd074e4d8c85397c3c2060cc65, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=87e30e8d76ad4a358ad3f6bb66ff10ce, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=8da93167b9d747aaa154d7c266fa111f, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=f120be7be4364d27b437aaf4e35d3eb6, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b0faeccaa97b40279592e5e75feedd71, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=f6b66f563d4348aa9841702b76443e11, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=e38df192700349b5a2a4d8356001f980, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=ae91c9fdc32140189788ef4d9eb958ae, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b6c13e8b6e35449f8a2c39b134beb941, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=5faedf6e528c45abb6366a5e48f8950d, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=5af8b119be8f46c29ef97c9540d32c7b, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b6c69667b8f24a7684be27a7775f7449, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=6b9b8d475b804add900acbc6dea26c56, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=609b48cade9b43b5bf54fc6c24107644, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=105bde9859684662873164b7ca03381c, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b0e5b5964f834b4094ee65c4232285a1, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=1070a7b7553744bc8fecc2a84216a7bc, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=9d21140d10a9400fa937149a16661c26, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=8468f658fded476c9e9b016873ecfa34, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=eb149e806d1841acaaf37562553836a8, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=0e98a6002064475596416520ee76a79d, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=8e6cdbd3fb0f4d8d9fc76e4f66859002, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=f86ea6d91bfe4454b9984de670c2ce78, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=74f769fe6bb942568c58fb4007f5585f, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=bdf8a1e6067f4d74b78ac9f37cb367e6, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=bc195e6c65c14a3cbde0c32c0e78a19e, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=28ddb2d514bc4fb39b5c66e7c5e245a3, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=274fdf8644ba49589a3b2e0a30c1f58a, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=2e27826dde33447aaafc1b80ca46c4f2, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=69ef12db9d504f9e9dffce98f5f9f1aa, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=711e4c2c06e54467bf166330be2b2cbb, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=bc9fcd9b187243d081bbe72a06504656, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=2663d3e354a744b7a7f8d39ac80a3366, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=1ff5b97440ba49b5ba53b0109d649a90, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=1e6104ada32240fcb78cb6d8d9e5849b, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=20b7509a91324ab7a78819d7ebc52992, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=55706b15927144e9951624003ef1f38a, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=22a1679c98c645e48e5c265316e4d4e9, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=0d1034857d3a4d418ad4087da14d8528, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=6c3dab4732a449b98d96e2fa07c244d9, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=8da0374248534e1990a5463a3ed40023, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=1f442d072dfb4514944dce46e49b83f6, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=9714ef2dee0c45dbb3f2bc06383d92e8, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b602c34762bb4009a618dd3a56ff1c14, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=547972a1623d4b2681eb89428fa52f9f, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=213f3ba3110b47d39a6a14172cd3a970, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=d1d81e5b0cfc4718941e4fcf26474b09, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=16404dc1b526476fb4702ae46c83dd1c, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=c21134cd7f4640c6a9fb97ff737f7920, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=c1209768e15041c8a40820fb0f1e65d7, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=c88666612fe14875a31242d660357eb4, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=d2ec37679d254592916a22cacf417dba, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=e579287ef1784daeb3d949cb6a646dc0, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=12813914021944e0b8541e2f135170d6, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=7c59b17c7a7f4c4fb336d0ac811b30bb, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=3ec5ef0998e14aa5a56572baaaadeef9, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=e01bf02dc09049fe89b40b05cdc26a01, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=0c91293b0d4a41f1a5995e149173b8ec, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=cf6b9c8953e5480184dcec4b31a7eca5, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=1740c5981db24a7f8b3d7dee12b3e03f, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=0b8963fd1fea4e58886b7a180f062539, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=849c09cf444c4b00952fb1625e1089b3, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=510dee161da54c05a2c5285abd6335b9, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=8b9c7de8357248cf9fff1bd91425c43e, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=4b8cd7d90b3c464982647e775ab7494a, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=aa1709ffe0ab4083a7df195129f7394d, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=4f42a9f874924db9870eb166a150ca7a, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=9622a201e0214fb89297e08d8cb95c47, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=5b51481f9d3144d3a5f9f5732ebfe281, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=853935af98164bc69bbc393bc7fa8247, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=0144eed1b49b4a06b323a13e503111a1, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=50ceb4b1f283429aaccfe6a0d048178e, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=dfd9e87ccaa745f593e9ee091f63b740, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=785e9049f95c4c128ea99aa94aff6bd8, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=9813d97da1ab4a829e8014c52a62fbe1, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=4dce27d1efd54d90a8872c6409f3e109, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=fc27f1b5b9464d80ad36a0a0ef3aa0e7, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=bb07228afbe445448aef21160fe0cfcd, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=1779248a02d7407eaa66f8a299182b50, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=3e4d19ea65f140a7b1052df8762f6ab0, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=53dfe80c1e674934bf1b01ff9672ba13, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=fb01471d29474211bac6fb76b93697a6, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=9b2d9d4b8379449b99a068ce52769a4c, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=4589677365f6427194915ab9a829425c, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=4913b3371eb84916bb33746ac764de7f, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=05df18b2ad714433a9970263230d70d5, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=90cb63eb056e4ac3b9e918eb1616f691, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=3a1708d858f1493cbab02a0e9c5145a9, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=f1cc0d9cae4544f6b5991604ceb22fd3, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=e0e941fc16d34fd099192f309f89e93b, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=0f5d9d1a307944cd8b03ecfd69e26130, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=0fdbfbf8fe394398b8facc82e1e76941, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=ec28703675d248598f5e0a748a195cd7, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=0a9f76cae6104b528ff71bf8d9135832, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=b5572f04562a48569be4144f667342c9, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=57ea074edb694d99a9c18c42649b7e99, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=1263501a14b94d76bff66220fe8c3865, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=e153c8904a404f9b8f40c29054156f0b, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=f9d1a57df03b43b29e540bb1ece2a59c, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=ad636c58a65c4464b746ee5fc4443690, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=c71133e18d2a40e0a04fde79b3cdf83a, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=683c6c1c1a6e4bc79e40b8bd5d44f19c, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=8d4fefeb35974671acfddf2d4175932c, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=9234914cf7c74823b9e9066abd951a79, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=dab9150ab7f84505a0a382e0f5716e70, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=2631af685568465f9fc3fb57049eea38, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=912bc0f116b94d18a7cdc36de03307fd, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=855f6ec0b41c43ee8239d64057e34e16, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[]), Req(rid=70e38fa2a88a4a4f938043312071b9e6, input_ids=[3838, 374, 279, 6722, 315, 9625, 30], output_ids=[])], self.token_to_kv_pool_allocator.available_size()=2468170, self.tree_cache.evictable_size()=0, \n[2025-04-03 20:06:34 DP0 TP0] Pyspy dump for PID 112694:\nProcess 112694: sglang::scheduler_DP0_TP0\nPython v3.12.9 (/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/bin/python3.12)\n\nThread 112694 (idle): \"MainThread\"\n    wait (threading.py:355)\n    wait (threading.py:655)\n    resolve_batch_result (tp_worker_overlap_thread.py:174)\n    process_batch_result_prefill (scheduler_output_processor_mixin.py:46)\n    process_batch_result (scheduler.py:1438)\n    event_loop_overlap (scheduler.py:639)\n    decorate_context (torch/utils/_contextlib.py:116)\n    run_scheduler_process (scheduler.py:2011)\n    run (multiprocessing/process.py:108)\n    _bootstrap (multiprocessing/process.py:314)\n    _main (multiprocessing/spawn.py:135)\n    spawn_main (multiprocessing/spawn.py:122)\n    <module> (<string>:1)\nThread 113487 (idle): \"Thread-1 (_read_thread)\"\n    _recv_msg (torch/_inductor/compile_worker/subproc_pool.py:47)\n    _read_thread (torch/_inductor/compile_worker/subproc_pool.py:153)\n    run (threading.py:1012)\n    _bootstrap_inner (threading.py:1075)\n    _bootstrap (threading.py:1032)\nThread 113902 (idle): \"Thread-2\"\n    wait (threading.py:359)\n    wait (threading.py:655)\n    run (tqdm/_monitor.py:60)\n    _bootstrap_inner (threading.py:1075)\n    _bootstrap (threading.py:1032)\nThread 114375 (idle): \"Thread-3\"\n    wait (threading.py:359)\n    wait (threading.py:655)\n    run (tqdm/_monitor.py:60)\n    _bootstrap_inner (threading.py:1075)\n    _bootstrap (threading.py:1032)\nThread 114788 (idle): \"Thread-4 (forward_thread_func)\"\n    acquire (filelock/_api.py:344)\n    __enter__ (filelock/_api.py:376)\n    load_cuda_ops (flashinfer/jit/core.py:122)\n    get_cascade_module (flashinfer/cascade.py:37)\n    merge_state (flashinfer/cascade.py:101)\n    forward_extend (flashinfer_backend.py:445)\n    forward (base_attn_backend.py:77)\n    forward (radix_attention.py:68)\n    _call_impl (torch/nn/modules/module.py:1747)\n    _wrapped_call_impl (torch/nn/modules/module.py:1736)\n    forward (qwen2.py:169)\n    _call_impl (torch/nn/modules/module.py:1747)\n    _wrapped_call_impl (torch/nn/modules/module.py:1736)\n    forward (qwen2.py:223)\n    _call_impl (torch/nn/modules/module.py:1747)\n    _wrapped_call_impl (torch/nn/modules/module.py:1736)\n    forward (qwen2.py:288)\n    _call_impl (torch/nn/modules/module.py:1747)\n    _wrapped_call_impl (torch/nn/modules/module.py:1736)\n    forward (qwen2.py:376)\n    decorate_context (torch/utils/_contextlib.py:116)\n    forward_extend (model_runner.py:959)\n    forward (model_runner.py:998)\n    forward_batch_generation (tp_worker.py:175)\n    forward_thread_func_ (tp_worker_overlap_thread.py:143)\n    decorate_context (torch/utils/_contextlib.py:116)\n    forward_thread_func (tp_worker_overlap_thread.py:112)\n    run (threading.py:1012)\n    _bootstrap_inner (threading.py:1075)\n    _bootstrap (threading.py:1032)\nThread 114797 (idle): \"Thread-5 (watchdog_thread)\"\n    select (selectors.py:415)\n    _communicate (subprocess.py:2123)\n    communicate (subprocess.py:1211)\n    run (subprocess.py:552)\n    pyspy_dump_schedulers (utils.py:1536)\n    watchdog_thread (scheduler.py:1599)\n    run (threading.py:1012)\n    _bootstrap_inner (threading.py:1075)\n    _bootstrap (threading.py:1032)\n\n\n\n[2025-04-03 20:06:34 DP1 TP0] Pyspy dump for PID 112695:\nProcess 112695: sglang::scheduler_DP1_TP0\nPython v3.12.9 (/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/bin/python3.12)\n\nThread 112695 (idle): \"MainThread\"\n    wait (threading.py:355)\n    get (queue.py:171)\n    resolve_batch_result (tp_worker_overlap_thread.py:172)\n    process_batch_result_prefill (scheduler_output_processor_mixin.py:46)\n    process_batch_result (scheduler.py:1438)\n    event_loop_overlap (scheduler.py:639)\n    decorate_context (torch/utils/_contextlib.py:116)\n    run_scheduler_process (scheduler.py:2011)\n    run (multiprocessing/process.py:108)\n    _bootstrap (multiprocessing/process.py:314)\n    _main (multiprocessing/spawn.py:135)\n    spawn_main (multiprocessing/spawn.py:122)\n    <module> (<string>:1)\nThread 113486 (idle): \"Thread-1 (_read_thread)\"\n    _recv_msg (torch/_inductor/compile_worker/subproc_pool.py:47)\n    _read_thread (torch/_inductor/compile_worker/subproc_pool.py:153)\n    run (threading.py:1012)\n    _bootstrap_inner (threading.py:1075)\n    _bootstrap (threading.py:1032)\nThread 113901 (idle): \"Thread-2\"\n    wait (threading.py:359)\n    wait (threading.py:655)\n    run (tqdm/_monitor.py:60)\n    _bootstrap_inner (threading.py:1075)\n    _bootstrap (threading.py:1032)\nThread 114376 (idle): \"Thread-3\"\n    wait (threading.py:359)\n    wait (threading.py:655)\n    run (tqdm/_monitor.py:60)\n    _bootstrap_inner (threading.py:1075)\n    _bootstrap (threading.py:1032)\nThread 114789 (idle): \"Thread-4 (forward_thread_func)\"\n    wait (torch/utils/file_baton.py:43)\n    _jit_compile (torch/utils/cpp_extension.py:1738)\n    load (torch/utils/cpp_extension.py:1314)\n    load_cuda_ops (flashinfer/jit/core.py:123)\n    get_cascade_module (flashinfer/cascade.py:37)\n    merge_state (flashinfer/cascade.py:101)\n    forward_extend (flashinfer_backend.py:445)\n    forward (base_attn_backend.py:77)\n    forward (radix_attention.py:68)\n    _call_impl (torch/nn/modules/module.py:1747)\n    _wrapped_call_impl (torch/nn/modules/module.py:1736)\n    forward (qwen2.py:169)\n    _call_impl (torch/nn/modules/module.py:1747)\n    _wrapped_call_impl (torch/nn/modules/module.py:1736)\n    forward (qwen2.py:223)\n    _call_impl (torch/nn/modules/module.py:1747)\n    _wrapped_call_impl (torch/nn/modules/module.py:1736)\n    forward (qwen2.py:288)\n    _call_impl (torch/nn/modules/module.py:1747)\n    _wrapped_call_impl (torch/nn/modules/module.py:1736)\n    forward (qwen2.py:376)\n    decorate_context (torch/utils/_contextlib.py:116)\n    forward_extend (model_runner.py:959)\n    forward (model_runner.py:998)\n    forward_batch_generation (tp_worker.py:175)\n    forward_thread_func_ (tp_worker_overlap_thread.py:143)\n    decorate_context (torch/utils/_contextlib.py:116)\n    forward_thread_func (tp_worker_overlap_thread.py:112)\n    run (threading.py:1012)\n    _bootstrap_inner (threading.py:1075)\n    _bootstrap (threading.py:1032)\nThread 114796 (idle): \"Thread-5 (watchdog_thread)\"\n    select (selectors.py:415)\n    _communicate (subprocess.py:2123)\n    communicate (subprocess.py:1211)\n    run (subprocess.py:552)\n    pyspy_dump_schedulers (utils.py:1536)\n    watchdog_thread (scheduler.py:1599)\n    run (threading.py:1012)\n    _bootstrap_inner (threading.py:1075)\n    _bootstrap (threading.py:1032)\n\n```\n\n\n### Reproduction\n\n```python\n\"\"\"\nThis example demonstrates how to launch the offline engine.\n\"\"\"\n\nimport sglang as sgl\n\n\ndef main():\n    llm = sgl.Engine(model_path=\"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/ruanjunhao04/models/Qwen2.5-1.5B-Instruct/main\",dp_size=2)\n    print(llm.generate(prompt=[\"What is the capital of France?\" for _ in range(1000)]))\n    llm.shutdown()\n\n\n# The __main__ condition is necessary here because we use \"spawn\" to create subprocesses\n# Spawn starts a fresh program every time, if there is no __main__, it will run into infinite loop to keep spawning processes from sgl.Engine\nif __name__ == \"__main__\":\n    main()\n```\n### Environment\nBy the way, it seems there's an issue with flashinfer in the sglang.check_env\u2014neither version 0.2.3 nor 0.2.4 is being detected.\nName: flashinfer-python\nVersion: 0.2.3+cu121torch2.5\nSummary: FlashInfer: Kernel Library for LLM Serving\nHome-page: https://github.com/flashinfer-ai/flashinfer\nAuthor: FlashInfer team\nAuthor-email: \nLicense: Apache License 2.0\nLocation: /mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/ruanjunhao04/env/rjh/lib/python3.12/site-packages\nRequires: torch\nRequired-by:\n\nPython: 3.12.9 | packaged by conda-forge | (main, Feb 14 2025, 08:00:06) [GCC 13.3.0]\nCUDA available: True\nGPU 0,1,2,3: NVIDIA A100-SXM4-80GB\nGPU 0,1,2,3 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\nCUDA Driver Version: 470.103.01\nPyTorch: 2.5.1+cu121\nsglang: 0.4.4.post3\nsgl_kernel: 0.0.5.post3\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.50.3\ntorchao: 0.7.0\nnumpy: 1.26.4\naiohttp: 3.11.9\nfastapi: 0.115.6\nhf_transfer: Module Not Found\nhuggingface_hub: 0.26.3\ninteregular: 0.3.3\nmodelscope: Module Not Found\norjson: 3.10.12\noutlines: 0.1.11\npackaging: 24.0\npsutil: 6.1.0\npydantic: 2.10.3\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.32.1\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.16\nopenai: 1.56.2\ntiktoken: 0.7.0\nanthropic: Module Not Found\nlitellm: Module Not Found\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    mlx5_0  mlx5_1  mlx5_2  mlx5_3  mlx5_4  mlx5_5  mlx5_6  mlx5_7  CPU Affinity    NUMA Affinity\nGPU0     X      NV12    NV12    NV12    SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     64-127,192-255  1\nGPU1    NV12     X      NV12    NV12    SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     64-127,192-255  1\nGPU2    NV12    NV12     X      NV12    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    64-127,192-255  1\nGPU3    NV12    NV12    NV12     X      SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    64-127,192-255  1\nmlx5_0  SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    SYS     SYS     SYS     SYS\nmlx5_1  SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    SYS     SYS     SYS     SYS\nmlx5_2  SYS     SYS     SYS     SYS     NODE    NODE     X      PIX     SYS     SYS     SYS     SYS\nmlx5_3  SYS     SYS     SYS     SYS     NODE    NODE    PIX      X      SYS     SYS     SYS     SYS\nmlx5_4  NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS      X      PIX     NODE    NODE\nmlx5_5  NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     PIX      X      NODE    NODE\nmlx5_6  PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      PIX\nmlx5_7  PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-04-03T12:01:39+00:00",
    "closed_at": "2025-06-06T00:19:20+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5034/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5034"
  },
  {
    "number": 3952,
    "title": "how to fix \"CUDA_HOME environment variable is not set\" in docker",
    "body": "install sglang with pip in docker, according to the document(https://docs.sglang.ai/start/install.html)\uff0cwhen i run sglang, there is an error \"CUDA_HOME environment variable is not set\"\ni need help\nneed to install CUDA in Docker\uff1f\n",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-28T07:25:02+00:00",
    "closed_at": "2025-03-25T03:29:01+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3952/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3952"
  },
  {
    "number": 3895,
    "title": "[Bug] OpenAI Endpoint '/v1/batches': `error: Object of type ChoiceLogprobs is not JSON serializable`",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nFirst of all, thanks for this amazing framework!\nWhen processing a batch via the OpenAI-compatible endpoint 'v1/batches' , and one requests the output of the logprobs, the server outputs the following error:\n```shell\n[2025-02-26 20:13:21 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 35, cache hit rate: 44.87%, token usage: 0.35, #running-req: 0, #queue-req: 0\n[2025-02-26 20:13:22 TP0] Decode batch. #running-req: 1, #token: 48, token usage: 0.48, gen throughput (token/s): 5.54, #queue-req: 0\n[2025-02-26 20:13:22] error: Object of type ChoiceLogprobs is not JSON serializable\n```\nThe issue is [this line](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/openai_api/adapter.py#L1130) where we probably want to call `model_dump()` on the `choice_logprobs`. But maybe a cleaner solution would be to remove the `to_file` argument entirely and simply call `model_dump_json()` [here](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/openai_api/adapter.py#L362)?\n\nIf you are interested in this solution I would be more than happy to open a PR :)\n\n### Reproduction\n\nFrom the [example in the documentation](https://docs.sglang.ai/backend/openai_api_completions.html#Batches)\n```python\nimport json\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=f\"http://127.0.0.1:30000/v1\", api_key=\"None\")\n\nrequests = [\n    {\n        \"custom_id\": \"request-1\",\n        \"method\": \"POST\",\n        \"url\": \"/chat/completions\",\n        \"body\": {\n            \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n            \"messages\": [\n                {\"role\": \"user\", \"content\": \"Tell me a joke about programming\"}\n            ],\n            \"max_tokens\": 50,\n            \"logprobs\": True,\n            \"top_logprobs\": 1,\n        },\n    },\n    {\n        \"custom_id\": \"request-2\",\n        \"method\": \"POST\",\n        \"url\": \"/chat/completions\",\n        \"body\": {\n            \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"What is Python?\"}],\n            \"max_tokens\": 50,\n        },\n    },\n]\n\ninput_file_path = \"batch_requests.jsonl\"\n\nwith open(input_file_path, \"w\") as f:\n    for req in requests:\n        f.write(json.dumps(req) + \"\\n\")\n\nwith open(input_file_path, \"rb\") as f:\n    file_response = client.files.create(file=f, purpose=\"batch\")\n\nbatch_response = client.batches.create(\n    input_file_id=file_response.id,\n    endpoint=\"/v1/chat/completions\",\n    completion_window=\"24h\",\n)\n\nprint(f\"Batch job created with ID: {batch_response.id}\")\n```\n\n### Environment\n\n```shell\n> python -m sglang.check_env\nINFO 02-26 20:42:30 __init__.py:190] Automatically detected platform cuda.\nPython: 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0]\nCUDA available: True\nGPU 0: NVIDIA GeForce GTX 1650\nGPU 0 Compute Capability: 7.5\nCUDA_HOME: /usr\nNVCC: Cuda compilation tools, release 12.0, V12.0.140\nCUDA Driver Version: 535.183.01\nPyTorch: 2.5.1+cu124\nsglang: 0.4.3\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.49.0\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.13\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.1\ninteregular: 0.3.3\nmodelscope: 1.23.1\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.64.0\ntiktoken: 0.9.0\nanthropic: 0.47.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-11    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-26T20:03:36+00:00",
    "closed_at": "2025-03-13T05:04:30+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3895/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3895"
  },
  {
    "number": 4127,
    "title": "[Bug] self.token_to_kv_pool.available_size(): AttributeError: 'Scheduler' object has no attribute 'token_to_kv_pool' with Qwen/QwQ-32B on SGLang v0.4.3.post3",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHi,\ntry to launch the just released Qwen/QwQ-32B  on very last version of SGLang 0.4.3.post3\n\nSee final message below: AttributeError: 'Scheduler' object has no attribute 'token_to_kv_pool'  in check_memory self.token_to_kv_pool.available_size()\n\n```\n\n### starting SGLang ...\nsgl start command: python3.12 -m sglang.launch_server   --model Qwen/QwQ-32B --model-path /home/model/Qwen/QwQ-32B   --host 0.0.0.0 --port 30000 --tensor-parallel-size 4   --log-level info   --enable-metrics --trust-remote-code --enable-p2p-check --disable-cuda-graph\nINFO 03-06 06:14:42 __init__.py:190] Automatically detected platform cuda.\n[2025-03-06 06:14:44] server_args=ServerArgs(model_path='/home/model/Qwen/QwQ-32B', tokenizer_path='/home/model/Qwen/QwQ-32B', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=True, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='/home/model/Qwen/QwQ-32B', chat_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=30000, mem_fraction_static=0.85, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=4, stream_interval=1, stream_output=False, random_seed=590928662, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=True, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=5, speculative_eagle_topk=4, speculative_num_draft_tokens=8, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=80, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=True, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=False, flashinfer_mla_disable_ragged=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False)\nINFO 03-06 06:14:48 __init__.py:190] Automatically detected platform cuda.\nINFO 03-06 06:14:48 __init__.py:190] Automatically detected platform cuda.\nINFO 03-06 06:14:48 __init__.py:190] Automatically detected platform cuda.\nINFO 03-06 06:14:48 __init__.py:190] Automatically detected platform cuda.\nINFO 03-06 06:14:48 __init__.py:190] Automatically detected platform cuda.\n[2025-03-06 06:14:51 TP3] Init torch distributed begin.\n[2025-03-06 06:14:52 TP0] Init torch distributed begin.\n[2025-03-06 06:14:52 TP2] Init torch distributed begin.\n[2025-03-06 06:14:52 TP1] Init torch distributed begin.\n[2025-03-06 06:14:52 TP0] sglang is using nccl==2.21.5\n[2025-03-06 06:14:52 TP1] sglang is using nccl==2.21.5\n[2025-03-06 06:14:52 TP2] sglang is using nccl==2.21.5\n[2025-03-06 06:14:52 TP3] sglang is using nccl==2.21.5\n[2025-03-06 06:14:52 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n[2025-03-06 06:14:52 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n[2025-03-06 06:14:52 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n[2025-03-06 06:14:52 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n[2025-03-06 06:14:53 TP3] Init torch distributed ends. mem usage=0.13 GB\n[2025-03-06 06:14:53 TP2] Init torch distributed ends. mem usage=0.13 GB\n[2025-03-06 06:14:53 TP1] Init torch distributed ends. mem usage=0.13 GB\n[2025-03-06 06:14:53 TP0] Init torch distributed ends. mem usage=0.13 GB\n[2025-03-06 06:14:53 TP1] Load weight begin. avail mem=21.63 GB\n[2025-03-06 06:14:53 TP2] Load weight begin. avail mem=21.63 GB\n[2025-03-06 06:14:53 TP0] Load weight begin. avail mem=21.63 GB\n[2025-03-06 06:14:53 TP3] Load weight begin. avail mem=21.63 GB\n[2025-03-06 06:14:53 TP3] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-03-06 06:14:53 TP0] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-03-06 06:14:53 TP1] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-03-06 06:14:53 TP2] The following error message 'operation scheduled before its operands' can be ignored.\nLoading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:   7% Completed | 1/14 [00:00<00:05,  2.24it/s]\nLoading safetensors checkpoint shards:  14% Completed | 2/14 [00:00<00:05,  2.10it/s]\nLoading safetensors checkpoint shards:  21% Completed | 3/14 [00:01<00:05,  1.97it/s]\nLoading safetensors checkpoint shards:  29% Completed | 4/14 [00:02<00:05,  1.86it/s]\nLoading safetensors checkpoint shards:  36% Completed | 5/14 [00:02<00:04,  1.82it/s]\nLoading safetensors checkpoint shards:  43% Completed | 6/14 [00:03<00:04,  1.77it/s]\nLoading safetensors checkpoint shards:  50% Completed | 7/14 [00:03<00:04,  1.73it/s]\nLoading safetensors checkpoint shards:  57% Completed | 8/14 [00:04<00:03,  1.70it/s]\nLoading safetensors checkpoint shards:  64% Completed | 9/14 [00:05<00:03,  1.67it/s]\nLoading safetensors checkpoint shards:  71% Completed | 10/14 [00:05<00:02,  1.65it/s]\nLoading safetensors checkpoint shards:  79% Completed | 11/14 [00:06<00:01,  1.93it/s]\nLoading safetensors checkpoint shards:  86% Completed | 12/14 [00:06<00:01,  2.00it/s]\n[2025-03-06 06:15:00 TP3] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=6.09 GB, mem usage=15.54 GB.\nLoading safetensors checkpoint shards:  93% Completed | 13/14 [00:07<00:00,  1.88it/s]\nLoading safetensors checkpoint shards: 100% Completed | 14/14 [00:07<00:00,  1.80it/s]\nLoading safetensors checkpoint shards: 100% Completed | 14/14 [00:07<00:00,  1.82it/s]\n\n[2025-03-06 06:15:01 TP1] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=6.09 GB, mem usage=15.54 GB.\n[2025-03-06 06:15:01 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=6.09 GB, mem usage=15.54 GB.\n[2025-03-06 06:15:01 TP2] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=6.09 GB, mem usage=15.54 GB.\n[2025-03-06 06:15:01 TP2] KV Cache is allocated. #tokens: 46611, K size: 1.42 GB, V size: 1.42 GB\n[2025-03-06 06:15:01 TP3] KV Cache is allocated. #tokens: 46611, K size: 1.42 GB, V size: 1.42 GB\n[2025-03-06 06:15:01 TP1] KV Cache is allocated. #tokens: 46611, K size: 1.42 GB, V size: 1.42 GB\n[2025-03-06 06:15:01 TP0] KV Cache is allocated. #tokens: 46611, K size: 1.42 GB, V size: 1.42 GB\n[2025-03-06 06:15:01 TP2] Memory pool end. avail mem=2.09 GB\n[2025-03-06 06:15:01 TP3] Memory pool end. avail mem=2.09 GB\n[2025-03-06 06:15:01 TP1] Memory pool end. avail mem=2.09 GB\n[2025-03-06 06:15:01 TP0] Memory pool end. avail mem=2.09 GB\n[2025-03-06 06:15:02 TP1] max_total_num_tokens=46611, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2049, context_len=131072\n[2025-03-06 06:15:02 TP0] max_total_num_tokens=46611, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2049, context_len=131072\n[2025-03-06 06:15:02 TP2] max_total_num_tokens=46611, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2049, context_len=131072\n[2025-03-06 06:15:02 TP3] max_total_num_tokens=46611, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2049, context_len=131072\n[2025-03-06 06:15:02] INFO:     Started server process [9]\n[2025-03-06 06:15:02] INFO:     Waiting for application startup.\n[2025-03-06 06:15:02] INFO:     Application startup complete.\n[2025-03-06 06:15:02] INFO:     Uvicorn running on http://0.0.0.0:30000 (Press CTRL+C to quit)\n[2025-03-06 06:15:02] INFO:     10.0.2.99:44886 - \"GET /health HTTP/1.1\" 200 OK\n[2025-03-06 06:15:02] INFO:     10.0.0.228:30054 - \"GET /health HTTP/1.1\" 200 OK\n[2025-03-06 06:15:03] INFO:     127.0.0.1:58208 - \"GET /get_model_info HTTP/1.1\" 200 OK\n[2025-03-06 06:15:03 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-03-06 06:15:07] INFO:     127.0.0.1:58214 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-06 06:15:07] The server is fired up and ready to roll!\n[2025-03-06 06:15:07] INFO:     10.0.2.99:44900 - \"GET /health HTTP/1.1\" 200 OK\n[2025-03-06 06:15:07] INFO:     10.0.0.228:30070 - \"GET /health HTTP/1.1\" 200 OK\n[2025-03-06 06:15:12] INFO:     10.0.2.99:49194 - \"GET /health HTTP/1.1\" 200 OK\n[2025-03-06 06:15:12] INFO:     10.0.0.228:48870 - \"GET /health HTTP/1.1\" 200 OK\n[2025-03-06 06:15:17] INFO:     10.0.2.99:49204 - \"GET /health HTTP/1.1\" 200 OK\n[2025-03-06 06:15:17] INFO:     10.0.0.228:48882 - \"GET /health HTTP/1.1\" 200 OK\n[2025-03-06 06:15:22] INFO:     10.0.2.99:57046 - \"GET /health HTTP/1.1\" 200 OK\n[2025-03-06 06:15:22] INFO:     10.0.0.228:49208 - \"GET /health HTTP/1.1\" 200 OK\n[2025-03-06 06:15:27] INFO:     10.0.2.99:57060 - \"GET /health HTTP/1.1\" 200 OK\n[2025-03-06 06:15:27] INFO:     10.0.0.228:49210 - \"GET /health HTTP/1.1\" 200 OK\n[2025-03-06 06:15:32] INFO:     10.0.2.99:12020 - \"GET /health HTTP/1.1\" 200 OK\n[2025-03-06 06:15:32] INFO:     10.0.0.228:39758 - \"GET /health HTTP/1.1\" 200 OK\n[2025-03-06 06:15:33 TP0] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 2247, in run_scheduler_process\n    scheduler.event_loop_overlap()\n  File \"/usr/local/lib64/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 552, in event_loop_overlap\n    self.check_memory()\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 935, in check_memory\n    self.token_to_kv_pool.available_size()\n    ^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'Scheduler' object has no attribute 'token_to_kv_pool'\n\n```\n\n### Reproduction\n\nLaunch QwQ-32B with SGLang 0.4.3.post 3 on Linux \nmessages above will appear.\n\n### Environment\n\nARG CUDA_VERSION=\"124\"\nARG PYTHON_VERSION=\"3.12\"\nARG TORCH_VERSION=\"2.5\"\nARG SGL_VERSION=\"0.4.3.post2\"\n\non AmazonLinux 2023",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-06T06:34:49+00:00",
    "closed_at": "2025-03-06T16:04:20+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4127/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4127"
  },
  {
    "number": 6050,
    "title": "[Feature] Support encoder models for flash_infer backend",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n              @DavidBao03 hi, currently only support encoder model with torch_native attn backend and triton attn backend. Other attn backend is not supported yet.\n\n_Originally posted by @woodx9 in https://github.com/sgl-project/sglang/issues/4887#issuecomment-2847365703_\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue"
    ],
    "state": "open",
    "created_at": "2025-05-06T09:41:33+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6050/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6050"
  },
  {
    "number": 404,
    "title": "no batch run when using openai's format for calling.",
    "body": "I just use this command to start the server \r\n`CUDA_VISIBLE_DEVICES=0 python -m sglang.launch_server --model-path LLMs/Qwen-14B-Chat --port 30000 --trust-remote-code --stream-interval 1 --enable-flashinfer --schedule-conservativeness 50`\r\nand using the following code to test the concurrent capability.\r\n\r\nIt can only generate code with ~10tokens/s whereas the vllm can be ~30tokens/s. it seems the call method does not support batch inferencing. the logs show as below:\r\n![image](https://github.com/sgl-project/sglang/assets/24971464/f9304ec5-7a3d-4b58-9748-efe08c75fb5f)\r\nthere is always 1 `runnning_req`. \r\n\r\nThe question is should we do it myself to support the batch inferencing when API calling or is something wrong with my setup? \r\nBTW, I also tried the `batching` example from the README, and it works fine and running faster then I expected!!!\r\n\r\nThank you so much ahead.\r\n\r\n**SCRIPTS**\r\n```python\r\ndef run(ds):\r\n    winner = \"a\" if \"_\" not in ds[\"winner\"] else ds[\"winner\"].split(\"_\")[1]\r\n    conversation = ds[f\"conversation_{winner}\"]\r\n\r\n    st = time.time()\r\n    answer = []\r\n    for i in range(ds[\"turn\"]):\r\n        current_msg_start_time = time.time()\r\n        query = conversation[i * 2]\r\n        history = conversation[: i * 2]\r\n        messages = history + [query]\r\n        assert query[\"role\"] == \"user\"\r\n        resp = requests.post(\r\n            \"http://localhost:30000/v1/chat/completions\",\r\n            data=json.dumps(\r\n                {\r\n                    \"messages\": messages,\r\n                    \"stream\": True,\r\n                    \"temperature\": 0,\r\n                    \"stop\": [\r\n                        \"<|endoftext|>\", \"<|im_end|>\",\r\n                    ],\r\n                    \"model\": \"Qwen-14B-Chat\",\r\n                    \"max_tokens\": \"2048\",\r\n                }\r\n            ),\r\n            headers={\"accept\": \"application/json\"},\r\n            timeout=600,\r\n            stream=True,\r\n        )\r\n\r\n        client = sseclient.SSEClient(resp)\r\n        data = \"\"\r\n        for i, event in enumerate(client.events()):\r\n            if event.data != \"[DONE]\":\r\n                if i == 1:\r\n                    first_packet_time = time.time() - current_msg_start_time\r\n                data = event.data\r\n        data = json.loads(data)\r\n\r\nif __name__ == \"__main__\":\r\n    for N in [5, ]:\r\n        user_pool = []\r\n        for i in range(N):\r\n            th = Thread(target=run, args=(ds[random.randint(0, 32999)],))\r\n            user_pool.append(th)\r\n        for t in user_pool:\r\n            t.start()\r\n        for t in user_pool:\r\n            t.join()\r\n```",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-04-30T01:45:19+00:00",
    "closed_at": "2024-07-25T06:33:23+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/404/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/404"
  },
  {
    "number": 2916,
    "title": "Can multiple services be deployed simultaneously?",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCan multiple services be deployed simultaneously, similar to the FastChat project?\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-01-16T09:57:42+00:00",
    "closed_at": "2025-01-21T22:12:15+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2916/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2916"
  },
  {
    "number": 24,
    "title": "Offline Generation",
    "body": "Hi, is it possible to do offline Generation similar to the vllm batch Inference where the model is not served?\n\nLike\n```\nLlm = sglang(\"path/to/llm\")\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-17T21:10:27+00:00",
    "closed_at": "2024-01-17T21:29:02+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/24/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/24"
  },
  {
    "number": 1156,
    "title": "[Feature] support W8A8(FP8) and KV Cache FP8 for DeepSeek V2",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nAs titled. Make DeepSeek V2 MLA Faster!\n\n### Related resources\n\n_No response_",
    "labels": [
      "feature"
    ],
    "state": "closed",
    "created_at": "2024-08-19T17:21:17+00:00",
    "closed_at": "2024-09-01T09:51:32+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1156/reactions",
      "total_count": 4,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 4,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/1156"
  },
  {
    "number": 1964,
    "title": "[Feature] Is AWQ W4Afp8 supported?",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nAWQ with INT4 weights and fp8 activations / KV cache works fairly well with Llama-3 models, and is a useful quantization technique for high-throughput regime. Is this quantization format supported by SGLang?\r\n\n\n### Related resources\n\nhttps://github.com/NVIDIA/TensorRT-LLM/blob/b7868dd1bd1186840e3755b97ea3d3a73ddd76c5/examples/falcon/README.md?plain=1#L311",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-11-08T21:29:15+00:00",
    "closed_at": "2025-01-10T00:17:05+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1964/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1964"
  },
  {
    "number": 3780,
    "title": "[Bug] AWQ scalar type error",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen I run the Deepseek-R1-AWQ, I met a scalar type bug same as pr #3450 . @hnyls2002 \n```\nLoading safetensors checkpoint shards:  97% Completed | 72/74 [00:44<00:01,  1.55it/s]\nLoading safetensors checkpoint shards:  99% Completed | 73/74 [00:45<00:00,  1.72it/s]\n[2025-02-22 12:19:24 TP3] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/mnt/Shared_h0/zjp/anaconda3/envs/deepseek/lib/python3.9/site-packages/sglang/srt/managers/scheduler.py\", line 1816, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/mnt/Shared_h0/zjp/anaconda3/envs/deepseek/lib/python3.9/site-packages/sglang/srt/managers/scheduler.py\", line 240, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/mnt/Shared_h0/zjp/anaconda3/envs/deepseek/lib/python3.9/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/mnt/Shared_h0/zjp/anaconda3/envs/deepseek/lib/python3.9/site-packages/sglang/srt/managers/tp_worker.py\", line 68, in __init__\n    self.model_runner = ModelRunner(\n  File \"/mnt/Shared_h0/zjp/anaconda3/envs/deepseek/lib/python3.9/site-packages/sglang/srt/model_executor/model_runner.py\", line 195, in __init__\n    self.load_model()\n  File \"/mnt/Shared_h0/zjp/anaconda3/envs/deepseek/lib/python3.9/site-packages/sglang/srt/model_executor/model_runner.py\", line 318, in load_model\n    self.model = get_model(\n  File \"/mnt/Shared_h0/zjp/anaconda3/envs/deepseek/lib/python3.9/site-packages/sglang/srt/model_loader/__init__.py\", line 22, in get_model\n    return loader.load_model(\n  File \"/mnt/Shared_h0/zjp/anaconda3/envs/deepseek/lib/python3.9/site-packages/sglang/srt/model_loader/loader.py\", line 362, in load_model\n    model.load_weights(self._get_all_weights(model_config, model))\n  File \"/mnt/Shared_h0/zjp/anaconda3/envs/deepseek/lib/python3.9/site-packages/sglang/srt/models/deepseek_v2.py\", line 962, in load_weights\n    w = ops.awq_dequantize(\n  File \"/mnt/Shared_h0/zjp/anaconda3/envs/deepseek/lib/python3.9/site-packages/vllm/_custom_ops.py\", line 222, in awq_dequantize\n    return torch.ops._C.awq_dequantize(qweight, scales, zeros, split_k_iters,\n  File \"/mnt/Shared_h0/zjp/anaconda3/envs/deepseek/lib/python3.9/site-packages/torch/_ops.py\", line 1116, in __call__\n    return self._op(*args, **(kwargs or {}))\nRuntimeError: expected scalar type Half but found BFloat16\n```\n\n### Reproduction\n\nI use the command recommended by instructions\n```\npython3 -m sglang.launch_server --model cognitivecomputations/DeepSeek-R1-AWQ --tp 8 --trust-remote-code\n```\nBut it produces the mistake same as preview.\n```\nRuntimeError: expected scalar type Half but found BFloat16\n```\n\n### Environment\n\n```\nINFO 02-22 12:35:34 __init__.py:190] Automatically detected platform cuda.\nPython: 3.9.21 (main, Dec 11 2024, 16:24:11) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 570.86.15\nPyTorch: 2.5.1+cu124\nsglang: 0.4.3.post2\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.1.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.1\ninteregular: 0.3.3\nmodelscope: 1.23.0\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.63.2\ntiktoken: 0.9.0\nanthropic: 0.46.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NODE    NODE    SYS     SYS     SYS     SYS     0-127,256-383   0               N/A\nGPU1    NV18     X      NODE    NODE    SYS     SYS     SYS     SYS     0-127,256-383   0               N/A\nGPU2    NODE    NODE     X      NV17    SYS     SYS     SYS     SYS     0-127,256-383   0               N/A\nGPU3    NODE    NODE    NV17     X      SYS     SYS     SYS     SYS     0-127,256-383   0               N/A\nGPU4    SYS     SYS     SYS     SYS      X      NV18    NODE    NODE    128-255,384-511 1               N/A\nGPU5    SYS     SYS     SYS     SYS     NV18     X      NODE    NODE    128-255,384-511 1               N/A\nGPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      NV18    128-255,384-511 1               N/A\nGPU7    SYS     SYS     SYS     SYS     NODE    NODE    NV18     X      128-255,384-511 1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1024\n```",
    "labels": [
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2025-02-22T04:37:08+00:00",
    "closed_at": "2025-03-03T06:06:20+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3780/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3780"
  },
  {
    "number": 5222,
    "title": "[Feature] support aarch64 natively",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nWe support sglang on jetson and gh200 but this type of things must be changed:\nif os.path.exists(\"/usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.12\"):\n    ctypes.CDLL(\n        \"/usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.12\",\n        mode=ctypes.RTLD_GLOBAL,\n    )\n\nbecause we have to fix by sed or git diff...\n\n### Related resources\n\npypi:\nhttps://pypi.jetson-ai-lab.dev/jp6/cu128\nhttps://pypi.jetson-ai-lab.dev/sbsa/cu128",
    "labels": [
      "high priority",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-04-10T08:04:32+00:00",
    "closed_at": "2025-06-10T00:19:36+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5222/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5222"
  },
  {
    "number": 3910,
    "title": "[Bug] Server crash when Input length exceeds the maximum allowed length",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n**Description** \n---\nServer crashs when receives prompt longer than max.\n\n![Image](https://github.com/user-attachments/assets/85f231aa-0571-4ac6-b728-a436c2e410a5)\n\n\n### Reproduction\n\npython3 -m sglang.launch_server --model-path /root/.cache/huggingface/models/DeepSeek-R1 --tp 16 --dist-init-addr  10.239.14.57:20000 --nnodes 2 --node-rank 1 --trust-remote-code --host 0.0.0.0 --port 9999 --mem-fraction-static 0.9 --context-length 29000 --chunked-prefill-size 2048 --served-model-name DeepSeek-R1-671B --attention-backend flashinfer --schedule-policy fcfs --enable-cache-report --enable-metrics\n\nSend a very long prompt through openai api\n\n### Environment\n\n2 L40x8 nodes:\n\nPython: 3.10.16 (main, Dec  4 2024, 08:53:37) [GCC 9.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA L40\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.9\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.5, V12.5.82\nCUDA Driver Version: 570.86.16\nPyTorch: 2.5.1+cu124\nsglang: 0.4.3.post2\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.1.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.61.1\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PIX     PIX     PIX     SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\nGPU1    PIX      X      PIX     PIX     SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\nGPU2    PIX     PIX      X      PIX     SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\nGPU3    PIX     PIX     PIX      X      SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\nGPU4    SYS     SYS     SYS     SYS      X      PIX     PIX     PIX     28-55,84-111    1               N/A\nGPU5    SYS     SYS     SYS     SYS     PIX      X      PIX     PIX     28-55,84-111    1               N/A\nGPU6    SYS     SYS     SYS     SYS     PIX     PIX      X      PIX     28-55,84-111    1               N/A\nGPU7    SYS     SYS     SYS     SYS     PIX     PIX     PIX      X      28-55,84-111    1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576\n",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-02-27T06:03:57+00:00",
    "closed_at": "2025-06-01T00:24:09+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3910/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3910"
  },
  {
    "number": 3666,
    "title": "[Bug] Stuck at NCCL initialization when TP>1",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nMany thanks for this great work! When using TP>1, it will stuck at NCCL initialization:\n\n\n>  INFO 02-18 09:33:49 __init__.py:190] Automatically detected platform cuda.                                                                   \n[2025-02-18 09:33:55] server_args=ServerArgs(model_path='meta-llama/Llama-3.1-8B-Instruct', tokenizer_path='meta-llama/Llama-3.1-8B-Instruct'\n, tokenizer_mode='auto', load_format='auto', trust_remote_code=True, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, quantization=None, context_length=None, device='cuda', served_model_name='meta-llama/Llama-3.1-8B-Instruct', chat_template=None, is_embedding=False, revision=None, skip_tokenizer_init=False, host='127.0.0.1', port=23333, mem_fraction_static=0.87, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=2, stream_interval=1, stream_output=False, random_seed=149565980, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='sglang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_draft_model_path=None, speculative_algorithm=None, speculative_num_steps=5, speculative_num_draft_tokens=64, speculative_eagle_topk=8, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=True, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, return_hidden_states=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=False)\n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\nINFO 02-18 09:33:59 __init__.py:190] Automatically detected platform cuda.\nINFO 02-18 09:33:59 __init__.py:190] Automatically detected platform cuda.\nINFO 02-18 09:33:59 __init__.py:190] Automatically detected platform cuda.\n[2025-02-18 09:34:04 TP0] Init torch distributed begin.\n[2025-02-18 09:34:05 TP1] Init torch distributed begin.\n[2025-02-18 09:34:05 TP1] sglang is using nccl==2.21.5\n[2025-02-18 09:34:05 TP0] sglang is using nccl==2.21.5\nunites4:188:188 [0] NCCL INFO Bootstrap : Using ibp194s0f0:10.2.133.35<0>\nunites4:188:188 [0] NCCL INFO cudaDriverVersion 12060\nNCCL version 2.21.5+cuda12.4\n\n\n\n\n\n### Reproduction\n\n`sudo docker run -e NCCL_DEBUG=TRACE --gpus all --shm-size 32g -p 0.0.0.0:23333:23333 -v ~/.cache/huggingface:/root/.cache/huggingface -v /home/pingzhi/model-checkpoints:/model-checkpoints --ipc=host --network=host --privileged lmsysorg/sglang:latest python3 -m sglang.launch_server --model meta-llama/Llama-3.1-8B-Instruct --tp 2 --enable-p2p-check --trust-remote-code --port 23333`\n\n### Environment\n\nINFO 02-18 09:40:33 __init__.py:190] Automatically detected platform cuda.                                                                   \nPython: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0]                                                                                   \nCUDA available: True                                                                                                                         \nGPU 0,1,2,3,4,5,6,7: NVIDIA RTX 6000 Ada Generation                                                                                          \nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.9                                                                                                  \nCUDA_HOME: /usr/local/cuda                                                                                                                   \nNVCC: Cuda compilation tools, release 12.4, V12.4.131                                                                                        \nCUDA Driver Version: 560.35.03                                                                                                               \nPyTorch: 2.5.1+cu124                                                                                                                         \nsgl_kernel: 0.0.3.post6                                                                                                                      \nflashinfer: 0.2.1.post2+cu124torch2.5                                                                                                        \ntriton: 3.1.0                                                                                                                                \ntransformers: 4.48.3                                                                                                                         \ntorchao: 0.8.0                                                                                                                               \nnumpy: 1.26.4                                                                                                                                \naiohttp: 3.11.12                                                                                                                             \nfastapi: 0.115.8                                                                                                                             \nhf_transfer: 0.1.9                                                                                                                           \nhuggingface_hub: 0.28.1                                                                                                                      \ninteregular: 0.3.3                                                                                                                           \nmodelscope: 1.23.0                                                                                                                           \norjson: 3.10.15                                                                                                                              \npackaging: 24.2                                                                                                                              \npsutil: 7.0.0                                                                                                                                \npydantic: 2.10.6                                                                                                                             \nmultipart: 0.0.20 \nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.63.2\ntiktoken: 0.9.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     0-23,48-71      0               N/A\nGPU1    NODE     X      NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     0-23,48-71      0               N/A\nGPU2    NODE    NODE     X      NODE    SYS     SYS     SYS     SYS     SYS     SYS     0-23,48-71      0               N/A\nGPU3    NODE    NODE    NODE     X      SYS     SYS     SYS     SYS     SYS     SYS     0-23,48-71      0               N/A\nGPU4    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    NODE    NODE    24-47,72-95     1               N/A\nGPU5    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    NODE    NODE    24-47,72-95     1               N/A\nGPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    PHB     PHB     24-47,72-95     1               N/A\nGPU7    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      NODE    NODE    24-47,72-95     1               N/A\nNIC0    SYS     SYS     SYS     SYS     NODE    NODE    PHB     NODE     X      PIX\nNIC1    SYS     SYS     SYS     SYS     NODE    NODE    PHB     NODE    PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n\n\nulimit soft: 1048576",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-02-18T09:44:24+00:00",
    "closed_at": "2025-04-21T00:19:33+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3666/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3666"
  },
  {
    "number": 5964,
    "title": "[Feature] Support more multi-modal input for VLM",
    "body": "### Motivation\n\nThe current endpoint only supports image data input, limiting its flexibility for diverse VLM use cases. We need additional input formats, particularly for RL applications:\n(Could be split into multiple PRs)\n\n- [x] Pre-computed Image Embeddings\n- [ ] Pixel Values\n- [ ] Pixel Value Range Parameters (min_pixel/max_pixel) for qwen-vl\n\nWelcome to propose more.\n\n#### Benefits\n\n1. Enhanced flexibility for RL workflows\n2. Reduced preprocessing overhead\n3. Better integration with existing pipelines",
    "labels": [
      "good first issue",
      "help wanted",
      "feature",
      "MLLM"
    ],
    "state": "open",
    "created_at": "2025-05-02T02:28:40+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5964/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5964"
  },
  {
    "number": 2573,
    "title": "[Feature] Due to GIL issues, the overlap mode doesn't actually always bring benefits?",
    "body": "### Checklist\r\n\r\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [X] 2. Please use English, otherwise it will be closed.\r\n\r\n### Motivation\r\n\r\nWorkaround python GIL (Work in progress)\uff1a\r\nIdea 1: Try python 3.13 which can remove GIL\r\nIdea 2: Use multiple processes, but need make the serialization very fast\r\nIn actual testing, I found that overlap does not necessarily bring benefits. I think this may be related to the GIL, since the current version is implemented with multi-threading. I'm wondering if this is expected? And under what circumstances would the GIL issue become more severe? @merrymercy \r\n\r\n### Related resources\r\n\r\n_No response_",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-12-25T13:31:50+00:00",
    "closed_at": "2025-02-24T00:17:23+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2573/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2573"
  },
  {
    "number": 445,
    "title": "OOM CUDA error on 8 * L4 machine when launching sglang server",
    "body": "Hey!\r\n\r\nI m trying launching a sglang server with [OpenBioLLM 70b](https://huggingface.co/aaditya/Llama3-OpenBioLLM-70B) with the command `python -m sglang.launch_server --model-path ~/Llama3-OpenBioLLM-70B-Instruct --port 30000` but I got on the 2 issues:\r\n\r\n1. It errors out with OOM CUDA, I tried playing around with all possible memory arguments but still have the issue, for e.g running `python -m sglang.launch_server --model-path ~/Llama3-OpenBioLLM-70B-Instruct --port 30000 --mem-fraction-static 0.9 --tp 8 --disable-disk-cache` errors out, I tried decreasing the mem-fraction-static or try different values with tp but still fails, here is the error \r\n```\r\n`Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nserver started on [0.0.0.0]:10014\r\nserver started on [0.0.0.0]:10017\r\nserver started on [0.0.0.0]:10018\r\nserver started on [0.0.0.0]:10016\r\nserver started on [0.0.0.0]:10019\r\nserver started on [0.0.0.0]:10015\r\nserver started on [0.0.0.0]:10020\r\nserver started on [0.0.0.0]:10021\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\naccepted ('127.0.0.1', 55860) with fd 52\r\nwelcome ('127.0.0.1', 55860)\r\naccepted ('127.0.0.1', 54770) with fd 32\r\nwelcome ('127.0.0.1', 54770)\r\naccepted ('127.0.0.1', 37120) with fd 33\r\nwelcome ('127.0.0.1', 37120)\r\naccepted ('127.0.0.1', 38382) with fd 28\r\nwelcome ('127.0.0.1', 38382)\r\naccepted ('127.0.0.1', 57702) with fd 29\r\nwelcome ('127.0.0.1', 57702)\r\naccepted ('127.0.0.1', 55900) with fd 24\r\nwelcome ('127.0.0.1', 55900)\r\naccepted ('127.0.0.1', 37206) with fd 24\r\nwelcome ('127.0.0.1', 37206)\r\naccepted ('127.0.0.1', 47836) with fd 24\r\nwelcome ('127.0.0.1', 47836)\r\nRank 4: load weight begin.\r\nRank 5: load weight begin.\r\nRank 7: load weight begin.\r\nRank 0: load weight begin.\r\nRank 6: load weight begin.\r\nRank 1: load weight begin.\r\nRank 2: load weight begin.\r\nRank 3: load weight begin.\r\nInitialization failed. router_init_state: Traceback (most recent call last):\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/sglang/srt/managers/router/manager.py\", line 71, in start_router_process\r\n    model_client = ModelRpcClient(server_args, port_args, model_overide_args)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 724, in __init__\r\n    self.step = async_wrap(\"step\")\r\n                ^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 715, in async_wrap\r\n    fs = [rpyc.async_(getattr(m, func_name)) for m in self.model_servers]\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 715, in <listcomp>\r\n    fs = [rpyc.async_(getattr(m, func_name)) for m in self.model_servers]\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py\", line 619, in result_iterator\r\n    yield _result_or_cancel(fs.pop())\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py\", line 317, in _result_or_cancel\r\n    return fut.result(timeout)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\r\n    return self.__get_result()\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\r\n    raise self._exception\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 707, in init_model\r\n    return self.remote_services[i].ModelRpcServer(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/rpyc/core/netref.py\", line 239, in __call__\r\n    return syncreq(_self, consts.HANDLE_CALL, args, kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/rpyc/core/netref.py\", line 63, in syncreq\r\n    return conn.sync_request(handler, proxy, *args)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/rpyc/core/protocol.py\", line 744, in sync_request\r\n    return _async_res.value\r\n           ^^^^^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/rpyc/core/async_.py\", line 111, in value\r\n    raise self._obj\r\nrpyc.core.vinegar/torch.cuda._get_exception_class.<locals>.Derived: CUDA out of memory. Tried to allocate 112.00 MiB. GPU \r\n\r\n========= Remote Traceback (1) =========\r\nTraceback (most recent call last):\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/rpyc/core/protocol.py\", line 369, in _dispatch_request\r\n    res = self._HANDLERS[handler](self, *args)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/rpyc/core/protocol.py\", line 863, in _handle_call\r\n    return obj(*args, **dict(kwargs))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 76, in __init__\r\n    self.model_runner = ModelRunner(\r\n                        ^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/sglang/srt/managers/router/model_runner.py\", line 285, in __init__\r\n    self.load_model()\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/sglang/srt/managers/router/model_runner.py\", line 323, in load_model\r\n    model = model_class(\r\n            ^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/sglang/srt/models/llama2.py\", line 257, in __init__\r\n    self.model = LlamaModel(config, quant_config=quant_config)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/sglang/srt/models/llama2.py\", line 217, in __init__\r\n    [\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/sglang/srt/models/llama2.py\", line 218, in <listcomp>\r\n    LlamaDecoderLayer(config, i, quant_config=quant_config)\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/sglang/srt/models/llama2.py\", line 166, in __init__\r\n    self.mlp = LlamaMLP(\r\n               ^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/sglang/srt/models/llama2.py\", line 39, in __init__\r\n    self.gate_up_proj = MergedColumnParallelLinear(\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 333, in __init__\r\n    super().__init__(input_size, sum(output_sizes), bias, gather_output,\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 236, in __init__\r\n    self.quant_method.create_weights(self,\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 81, in create_weights\r\n    weight = Parameter(torch.empty(output_size_per_partition,\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mmokaddem_benchsci_com/.pyenv/versions/venv_sglang/lib/python3.11/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU \r\n\r\n\r\nInitialization failed. detoken_init_state: init ok\r\ngoodbye ('127.0.0.1', 57702)\r\ngoodbye ('127.0.0.1', 37206)\r\ngoodbye ('127.0.0.1', 55900)\r\ngoodbye ('127.0.0.1', 47836)\r\ngoodbye ('127.0.0.1', 54770)\r\ngoodbye ('127.0.0.1', 37120)\r\ngoodbye ('127.0.0.1', 38382)\r\ngoodbye ('127.0.0.1', 55860)`\r\n```\r\n2. It stucks with \r\n```\r\npython -m sglang.launch_server --model-path ~/Llama3-OpenBioLLM-70B-Instruct --port 30000 --mem-fraction-static 0.9 --tp 8 --disable-disk-cache\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nserver started on [0.0.0.0]:10007\r\nserver started on [0.0.0.0]:10004\r\nserver started on [0.0.0.0]:10005\r\nserver started on [0.0.0.0]:10008\r\nserver started on [0.0.0.0]:10006\r\nserver started on [0.0.0.0]:10009\r\nserver started on [0.0.0.0]:10010\r\nserver started on [0.0.0.0]:10011\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\naccepted ('127.0.0.1', 44596) with fd 46\r\nwelcome ('127.0.0.1', 44596)\r\naccepted ('127.0.0.1', 44648) with fd 33\r\nwelcome ('127.0.0.1', 44648)\r\naccepted ('127.0.0.1', 53648) with fd 24\r\nwelcome ('127.0.0.1', 53648)\r\naccepted ('127.0.0.1', 33128) with fd 25\r\nwelcome ('127.0.0.1', 33128)\r\naccepted ('127.0.0.1', 41686) with fd 25\r\nwelcome ('127.0.0.1', 41686)\r\naccepted ('127.0.0.1', 56570) with fd 25\r\nwelcome ('127.0.0.1', 56570)\r\naccepted ('127.0.0.1', 48382) with fd 34\r\nwelcome ('127.0.0.1', 48382)\r\naccepted ('127.0.0.1', 36272) with fd 29\r\nwelcome ('127.0.0.1', 36272)\r\nRank 4: load weight begin.\r\nRank 6: load weight begin.\r\nRank 2: load weight begin.\r\nRank 5: load weight begin.\r\nRank 3: load weight begin.\r\nRank 7: load weight begin.\r\nRank 1: load weight begin.\r\nRank 0: load weight begin.\r\n^C\r\n```\r\n\r\nand when I do `set_default_backend(RuntimeEndpoint(\"http://localhost:30000\"))` it errors out with connection refused \r\n```\r\nConnectionRefusedError                    Traceback (most recent call last)\r\nFile ~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/urllib/request.py:1348, in AbstractHTTPHandler.do_open(self, http_class, req, **http_conn_args)\r\n   [1347](https://vscode-remote+ssh-002dremote-002bcloudstation-002dmmokaddem-002d3.vscode-resource.vscode-cdn.net/home/mmokaddem_benchsci_com/github/benchsci/bsci/benchsci/extract/evidence_maps/notebooks/~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/urllib/request.py:1347) try:\r\n-> [1348](https://vscode-remote+ssh-002dremote-002bcloudstation-002dmmokaddem-002d3.vscode-resource.vscode-cdn.net/home/mmokaddem_benchsci_com/github/benchsci/bsci/benchsci/extract/evidence_maps/notebooks/~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/urllib/request.py:1348)     h.request(req.get_method(), req.selector, req.data, headers,\r\n   [1349](https://vscode-remote+ssh-002dremote-002bcloudstation-002dmmokaddem-002d3.vscode-resource.vscode-cdn.net/home/mmokaddem_benchsci_com/github/benchsci/bsci/benchsci/extract/evidence_maps/notebooks/~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/urllib/request.py:1349)               encode_chunked=req.has_header('Transfer-encoding'))\r\n   [1350](https://vscode-remote+ssh-002dremote-002bcloudstation-002dmmokaddem-002d3.vscode-resource.vscode-cdn.net/home/mmokaddem_benchsci_com/github/benchsci/bsci/benchsci/extract/evidence_maps/notebooks/~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/urllib/request.py:1350) except OSError as err: # timeout error\r\n\r\nFile ~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/http/client.py:1286, in HTTPConnection.request(self, method, url, body, headers, encode_chunked)\r\n   [1285](https://vscode-remote+ssh-002dremote-002bcloudstation-002dmmokaddem-002d3.vscode-resource.vscode-cdn.net/home/mmokaddem_benchsci_com/github/benchsci/bsci/benchsci/extract/evidence_maps/notebooks/~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/http/client.py:1285) \"\"\"Send a complete request to the server.\"\"\"\r\n-> [1286](https://vscode-remote+ssh-002dremote-002bcloudstation-002dmmokaddem-002d3.vscode-resource.vscode-cdn.net/home/mmokaddem_benchsci_com/github/benchsci/bsci/benchsci/extract/evidence_maps/notebooks/~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/http/client.py:1286) self._send_request(method, url, body, headers, encode_chunked)\r\n\r\nFile ~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/http/client.py:1332, in HTTPConnection._send_request(self, method, url, body, headers, encode_chunked)\r\n   [1331](https://vscode-remote+ssh-002dremote-002bcloudstation-002dmmokaddem-002d3.vscode-resource.vscode-cdn.net/home/mmokaddem_benchsci_com/github/benchsci/bsci/benchsci/extract/evidence_maps/notebooks/~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/http/client.py:1331)     body = _encode(body, 'body')\r\n-> [1332](https://vscode-remote+ssh-002dremote-002bcloudstation-002dmmokaddem-002d3.vscode-resource.vscode-cdn.net/home/mmokaddem_benchsci_com/github/benchsci/bsci/benchsci/extract/evidence_maps/notebooks/~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/http/client.py:1332) self.endheaders(body, encode_chunked=encode_chunked)\r\n\r\nFile ~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/http/client.py:1281, in HTTPConnection.endheaders(self, message_body, encode_chunked)\r\n   [1280](https://vscode-remote+ssh-002dremote-002bcloudstation-002dmmokaddem-002d3.vscode-resource.vscode-cdn.net/home/mmokaddem_benchsci_com/github/benchsci/bsci/benchsci/extract/evidence_maps/notebooks/~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/http/client.py:1280)     raise CannotSendHeader()\r\n-> [1281](https://vscode-remote+ssh-002dremote-002bcloudstation-002dmmokaddem-002d3.vscode-resource.vscode-cdn.net/home/mmokaddem_benchsci_com/github/benchsci/bsci/benchsci/extract/evidence_maps/notebooks/~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/http/client.py:1281) self._send_output(message_body, encode_chunked=encode_chunked)\r\n\r\nFile ~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/http/client.py:1041, in HTTPConnection._send_output(self, message_body, encode_chunked)\r\n   [1040](https://vscode-remote+ssh-002dremote-002bcloudstation-002dmmokaddem-002d3.vscode-resource.vscode-cdn.net/home/mmokaddem_benchsci_com/github/benchsci/bsci/benchsci/extract/evidence_maps/notebooks/~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/http/client.py:1040) del self._buffer[:]\r\n-> [1041](https://vscode-remote+ssh-002dremote-002bcloudstation-002dmmokaddem-002d3.vscode-resource.vscode-cdn.net/home/mmokaddem_benchsci_com/github/benchsci/bsci/benchsci/extract/evidence_maps/notebooks/~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/http/client.py:1041) self.send(msg)\r\n   [1043](https://vscode-remote+ssh-002dremote-002bcloudstation-002dmmokaddem-002d3.vscode-resource.vscode-cdn.net/home/mmokaddem_benchsci_com/github/benchsci/bsci/benchsci/extract/evidence_maps/notebooks/~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/http/client.py:1043) if message_body is not None:\r\n   [1044](https://vscode-remote+ssh-002dremote-002bcloudstation-002dmmokaddem-002d3.vscode-resource.vscode-cdn.net/home/mmokaddem_benchsci_com/github/benchsci/bsci/benchsci/extract/evidence_maps/notebooks/~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/http/client.py:1044) \r\n...\r\n-> [1351](https://vscode-remote+ssh-002dremote-002bcloudstation-002dmmokaddem-002d3.vscode-resource.vscode-cdn.net/home/mmokaddem_benchsci_com/github/benchsci/bsci/benchsci/extract/evidence_maps/notebooks/~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/urllib/request.py:1351)         raise URLError(err)\r\n   [1352](https://vscode-remote+ssh-002dremote-002bcloudstation-002dmmokaddem-002d3.vscode-resource.vscode-cdn.net/home/mmokaddem_benchsci_com/github/benchsci/bsci/benchsci/extract/evidence_maps/notebooks/~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/urllib/request.py:1352)     r = h.getresponse()\r\n   [1353](https://vscode-remote+ssh-002dremote-002bcloudstation-002dmmokaddem-002d3.vscode-resource.vscode-cdn.net/home/mmokaddem_benchsci_com/github/benchsci/bsci/benchsci/extract/evidence_maps/notebooks/~/github/benchsci/bsci/bazel-bin/tools/virtualenv.runfiles/rules_python~0.28.0~python~python_3_11_x86_64-unknown-linux-gnu/lib/python3.11/urllib/request.py:1353) except:\r\n\r\nURLError: <urlopen error [Errno 111] Connection refused>\r\n\r\n```\r\n**Setup**\r\nMachine type g2-standard-96 \r\nGPUs 8 x NVIDIA L4\r\nArchitecture x86/64\r\n\r\nsglang version [v0.1.16](https://github.com/sgl-project/sglang/releases/tag/v0.1.16)\r\n\r\nIt is not a memory problem as the machine has a total of 192 GB memory (24 GB/GPU) and I tried running inference without sglang and it worked. Plus, I haven't tried to use [flashinfer](https://github.com/sgl-project/sglang/blob/main/docs/flashinfer.md) as this is used to accelerate inference which is not the issue for me for now. ",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-05-15T21:02:07+00:00",
    "closed_at": "2024-07-25T06:33:39+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/445/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/445"
  },
  {
    "number": 165,
    "title": "setting mem-fraction-static to a lower value causes error",
    "body": "With no change, I run out of memory (A100 w/ 24GB). Setting it to anything other than the default causes the following error:\r\n\r\n```\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/workspace/sglang/python/sglang/srt/managers/router/model_rpc.py\", line 170, in exposed_step\r\n    self.forward_step()\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/workspace/sglang/python/sglang/srt/managers/router/model_rpc.py\", line 185, in forward_step\r\n    self.forward_fill_batch(new_batch)\r\n  File \"/workspace/sglang/python/sglang/srt/managers/router/model_rpc.py\", line 387, in forward_fill_batch\r\n    batch.prepare_for_extend(\r\n  File \"/workspace/sglang/python/sglang/srt/managers/router/infer_batch.py\", line 203, in prepare_for_extend\r\n    req_pool_indices_cpu = req_pool_indices.cpu().numpy()\r\nAttributeError: 'NoneType' object has no attribute 'cpu'\r\n```\r\n\r\nFor reference I am attempting to use gen with a very large set of items to select through to limit inference tokens (thousands)",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-02-07T22:25:25+00:00",
    "closed_at": "2024-07-25T06:33:36+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/165/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/165"
  },
  {
    "number": 5096,
    "title": "[Bug]  shared memory (/dev/shm) error when using FA3 with meta-llama/Llama-3.1-70B-Instruct on a multi-GPU host",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHello, \n\nI am trying to run `meta-llama/Llama-3.1-70B-Instruct` with SGLang's FA3  setting and running into /dev/shm error, when running on a multi-gpu host (8xH100).\n\nWould anyone have seen the issue? \n\nError log\n\n```\nERROR 2025-04-05T00:01:19.509514570Z ./entrypoint.sh: line 48: 28 Killed python3 -m sglang.launch_server --host 0.0.0.0 --port ...\nERROR 2025-04-05T00:01:19.209237575Z [2025-04-05 00:01:19] Received sigquit from a child process. It usually means the child failed.\nERROR 2025-04-05T00:01:19.209112405Z {}\nERROR 2025-04-05T00:01:19.209110975Z Error while creating shared memory segment /dev/shm/nccl-lNPZ0f (size 7340384)\nERROR 2025-04-05T00:01:19.209109067Z Last error:\nERROR 2025-04-05T00:01:19.209107160Z ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.\nERROR 2025-04-05T00:01:19.209104776Z torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:317, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5\nERROR 2025-04-05T00:01:19.209101915Z work = group.allreduce([tensor], opts)\nERROR 2025-04-05T00:01:19.209100008Z File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\", line 2501, in all_reduce\nERROR 2025-04-05T00:01:19.209097862Z return func(*args, **kwargs)\nERROR 2025-04-05T00:01:19.209095001Z File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py\", line 83, in wrapper\nERROR 2025-04-05T00:01:19.209093332Z torch.distributed.all_reduce(tensor, op=torch.distributed.ReduceOp.MIN)\nERROR 2025-04-05T00:01:19.209090709Z File \"/sgl-workspace/sglang/python/sglang/srt/utils.py\", line 315, in get_available_gpu_memory\nERROR 2025-04-05T00:01:19.209086656Z min_per_gpu_memory = get_available_gpu_memory(\nERROR 2025-04-05T00:01:19.209083795Z File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 328, in init_torch_distributed\nERROR 2025-04-05T00:01:19.209080696Z min_per_gpu_memory = self.init_torch_distributed()\nERROR 2025-04-05T00:01:19.209076881Z File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 169, in __init__\nERROR 2025-04-05T00:01:19.209074258Z self.model_runner = ModelRunner(\nERROR 2025-04-05T00:01:19.209071636Z File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 74, in __init__\nERROR 2025-04-05T00:01:19.209069252Z self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\nERROR 2025-04-05T00:01:19.209066629Z File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\nERROR 2025-04-05T00:01:19.209064722Z self.tp_worker = TpWorkerClass(\nERROR 2025-04-05T00:01:19.209062099Z File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 249, in __init__\nERROR 2025-04-05T00:01:19.209059476Z scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\nERROR 2025-04-05T00:01:19.209055185Z File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1999, in run_scheduler_process\nERROR 2025-04-05T00:01:19.209014177Z [2025-04-05 00:01:19 TP0] Scheduler hit an exception: Traceback (most recent call last):\nERROR 2025-04-05T00:01:19.199748277Z [2025-04-05 00:01:19] Received sigquit from a child process. It usually means the child failed.\nERROR 2025-04-05T00:01:19.199052572Z {}\nERROR 2025-04-05T00:01:19.199050426Z Error while creating shared memory segment /dev/shm/nccl-ufmqwQ (size 7340384)\nERROR 2025-04-05T00:01:19.199037551Z Last error:\nERROR 2025-04-05T00:01:19.199035406Z ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error.\nERROR 2025-04-05T00:01:19.199032783Z torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:317, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5\nERROR 2025-04-05T00:01:19.199029922Z work = group.allreduce([tensor], opts)\nERROR 2025-04-05T00:01:19.199027538Z File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\", line 2501, in all_reduce\nERROR 2025-04-05T00:01:19.199025154Z return func(*args, **kwargs)\nERROR 2025-04-05T00:01:19.199022293Z File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py\", line 83, in wrapper\nERROR 2025-04-05T00:01:19.199019670Z torch.distributed.all_reduce(tensor, op=torch.distributed.ReduceOp.MIN)\nERROR 2025-04-05T00:01:19.199017047Z File \"/sgl-workspace/sglang/python/sglang/srt/utils.py\", line 315, in get_available_gpu_memory\nERROR 2025-04-05T00:01:19.199014186Z min_per_gpu_memory = get_available_gpu_memory(\nERROR 2025-04-05T00:01:19.199009656Z File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 328, in init_torch_distributed\nERROR 2025-04-05T00:01:19.199005603Z min_per_gpu_memory = self.init_torch_distributed()\nERROR 2025-04-05T00:01:19.199002265Z File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 169, in __init__\nERROR 2025-04-05T00:01:19.198998451Z self.model_runner = ModelRunner(\nERROR 2025-04-05T00:01:19.198993444Z File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 74, in __init__\nERROR 2025-04-05T00:01:19.198991060Z self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\nERROR 2025-04-05T00:01:19.198988437Z File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\nERROR 2025-04-05T00:01:19.198985815Z self.tp_worker = TpWorkerClass(\nERROR 2025-04-05T00:01:19.198982954Z File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 249, in __init__\nERROR 2025-04-05T00:01:19.198980331Z scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\nERROR 2025-04-05T00:01:19.198975563Z File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1999, in run_scheduler_process\nERROR 2025-04-05T00:01:19.198923587Z [2025-04-05 00:01:19 TP1] Scheduler hit an exception: Traceback (most recent call last):\nERROR 2025-04-05T00:01:04.681244850Z [2025-04-05 00:01:04 TP7] sglang is using nccl==2.21.5\nERROR 2025-04-05T00:01:04.681155443Z [2025-04-05 00:01:04 TP4] sglang is using nccl==2.21.5\nERROR 2025-04-05T00:01:04.681041002Z [2025-04-05 00:01:04 TP5] sglang is using nccl==2.21.5\nERROR 2025-04-05T00:01:04.680786132Z [2025-04-05 00:01:04 TP6] sglang is using nccl==2.21.5\nERROR 2025-04-05T00:01:04.680771112Z [2025-04-05 00:01:04 TP2] sglang is using nccl==2.21.5\nERROR 2025-04-05T00:01:04.680658578Z [2025-04-05 00:01:04 TP1] sglang is using nccl==2.21.5\nERROR 2025-04-05T00:01:04.680569648Z [2025-04-05 00:01:04 TP3] sglang is using nccl==2.21.5\nERROR 2025-04-05T00:01:04.680526733Z [2025-04-05 00:01:04 TP0] sglang is using nccl==2.21.5\nERROR 2025-04-05T00:01:02.885493516Z [2025-04-05 00:01:02 TP4] Init torch distributed begin.\nERROR 2025-04-05T00:01:02.880952596Z [2025-04-05 00:01:02 TP0] Init torch distributed begin.\nERROR 2025-04-05T00:01:02.818879842Z [2025-04-05 00:01:02 TP7] Init torch distributed begin.\nERROR 2025-04-05T00:01:02.735132217Z [2025-04-05 00:01:02 TP1] Init torch distributed begin.\nERROR 2025-04-05T00:01:02.720506668Z [2025-04-05 00:01:02 TP5] Init torch distributed begin.\nERROR 2025-04-05T00:01:02.307281494Z [2025-04-05 00:01:02 TP3] Init torch distributed begin.\nERROR 2025-04-05T00:01:02.116440296Z [2025-04-05 00:01:02 TP2] Init torch distributed begin.\nERROR 2025-04-05T00:01:01.875401735Z [2025-04-05 00:01:01 TP6] Init torch distributed begin.\nERROR 2025-04-05T00:00:52.673557043Z [2025-04-05 00:00:52] server_args=ServerArgs(model_path='meta-llama/Llama-3.1-70B-Instruct', tokenizer_path='meta-llama/Llama-3.1-70B-Instruct', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=True, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='meta-llama/Llama-3.1-70B-Instruct', chat_template=None, completion_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=30000, mem_fraction_static=0.81, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=8, stream_interval=1, stream_output=False, random_seed=792723648, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='fa3', sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode=None, enable_torch_compile=True, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_conti\u2026\nINFO 2025-04-05T00:00:45.209044218Z Starting API server with user-specified arguments...\nINFO 2025-04-05T00:00:45.209043025Z ==> No LWS_LEADER_ADDRESS environment variable detected, assuming single node\nINFO 2025-04-05T00:00:45.209039211Z SGLang server arguments: --model=meta-llama/Llama-3.1-70B-Instruct --tp=8 --attention-backend=fa3 --trust-remote-code --enable-torch-compile\n```\n\n### Reproduction\n\nI am using SGLang server arguments: `--model=meta-llama/Llama-3.1-70B-Instruct --tp=8 --attention-backend=fa3 --trust-remote-code --enable-torch-compile`. The error appears at the start up.\n\n\n\n### Environment\n\nINFO 2025-04-06T03:02:45.843368291Z Python: 3.10.12 (main, Feb 4 2025, 14:57:36) [GCC 11.4.0]\n--\nINFO 2025-04-06T03:02:45.843415975Z CUDA available: True\nINFO 2025-04-06T03:02:45.843419551Z GPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nINFO 2025-04-06T03:02:45.843421697Z GPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nINFO 2025-04-06T03:02:45.843424320Z CUDA_HOME: /usr/local/cuda\nINFO 2025-04-06T03:02:45.843426227Z NVCC: Cuda compilation tools, release 12.4, V12.4.131\nINFO 2025-04-06T03:02:45.843428611Z CUDA Driver Version: 535.230.02\nINFO 2025-04-06T03:02:45.843430757Z PyTorch: 2.5.1+cu124\nINFO 2025-04-06T03:02:45.843432664Z sglang: 0.4.4.post3\nINFO 2025-04-06T03:02:45.843434810Z sgl_kernel: 0.0.7\nINFO 2025-04-06T03:02:45.843438386Z flashinfer: Module Not Found\nINFO 2025-04-06T03:02:45.843440294Z triton: 3.1.0\nINFO 2025-04-06T03:02:45.843442201Z transformers: 4.50.0\nINFO 2025-04-06T03:02:45.843443632Z torchao: 0.9.0\nINFO 2025-04-06T03:02:45.843445539Z numpy: 2.2.4\nINFO 2025-04-06T03:02:45.843447208Z aiohttp: 3.11.16\nINFO 2025-04-06T03:02:45.843448877Z fastapi: 0.115.12\nINFO 2025-04-06T03:02:45.843450546Z hf_transfer: 0.1.9\nINFO 2025-04-06T03:02:45.843452692Z huggingface_hub: 0.30.1\nINFO 2025-04-06T03:02:45.843454360Z interegular: 0.3.3\nINFO 2025-04-06T03:02:45.843456029Z modelscope: 1.24.1\nINFO 2025-04-06T03:02:45.843457698Z orjson: 3.10.16\nINFO 2025-04-06T03:02:45.843459367Z outlines: 0.1.11\nINFO 2025-04-06T03:02:45.843461275Z packaging: 24.2\nINFO 2025-04-06T03:02:45.843462944Z psutil: 7.0.0\nINFO 2025-04-06T03:02:45.843464612Z pydantic: 2.11.2\nINFO 2025-04-06T03:02:45.843466281Z multipart: Module Not Found\nINFO 2025-04-06T03:02:45.843468189Z zmq: Module Not Found\nINFO 2025-04-06T03:02:45.843469858Z uvicorn: 0.34.0\nINFO 2025-04-06T03:02:45.843471527Z uvloop: 0.21.0\nINFO 2025-04-06T03:02:45.843472957Z vllm: Module Not Found\nINFO 2025-04-06T03:02:45.843474864Z xgrammar: 0.1.17\nINFO 2025-04-06T03:02:45.843476533Z openai: 1.70.0\nINFO 2025-04-06T03:02:45.843478202Z tiktoken: 0.9.0\nINFO 2025-04-06T03:02:45.843479871Z anthropic: 0.49.0\nINFO 2025-04-06T03:02:45.843481779Z litellm: 1.65.3\nINFO 2025-04-06T03:02:45.843483448Z decord: 0.6.0\nINFO 2025-04-06T03:02:45.843485116Z NVIDIA Topology:\nINFO 2025-04-06T03:02:45.843487262Z \u001b[4mGPU0 GPU1 GPU2 GPU3 GPU4 GPU5 GPU6 GPU7 CPU Affinity NUMA Affinity GPU NUMA ID\u001b[0m\nINFO 2025-04-06T03:02:45.843489170Z GPU0 X NV18 NV18 NV18 NV18 NV18 NV18 NV18 0-51,104-155 0 N/A\nINFO 2025-04-06T03:02:45.843491315Z GPU1 NV18 X NV18 NV18 NV18 NV18 NV18 NV18 0-51,104-155 0 N/A\nINFO 2025-04-06T03:02:45.843493223Z GPU2 NV18 NV18 X NV18 NV18 NV18 NV18 NV18 0-51,104-155 0 N/A\nINFO 2025-04-06T03:02:45.843494892Z GPU3 NV18 NV18 NV18 X NV18 NV18 NV18 NV18 0-51,104-155 0 N/A\nINFO 2025-04-06T03:02:45.843507289Z GPU4 NV18 NV18 NV18 NV18 X NV18 NV18 NV18 52-103,156-207 1 N/A\nINFO 2025-04-06T03:02:45.843509197Z GPU5 NV18 NV18 NV18 NV18 NV18 X NV18 NV18 52-103,156-207 1 N/A\nINFO 2025-04-06T03:02:45.843510866Z GPU6 NV18 NV18 NV18 NV18 NV18 NV18 X NV18 52-103,156-207 1 N/A\nINFO 2025-04-06T03:02:45.843512773Z GPU7 NV18 NV18 NV18 NV18 NV18 NV18 NV18 X 52-103,156-207 1 N/A\nINFO 2025-04-06T03:02:45.843514919Z {}\nINFO 2025-04-06T03:02:45.843516588Z Legend:\nINFO 2025-04-06T03:02:45.843518018Z {}\nINFO 2025-04-06T03:02:45.843519926Z X = Self\nINFO 2025-04-06T03:02:45.843521595Z SYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\nINFO 2025-04-06T03:02:45.843524694Z NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\nINFO 2025-04-06T03:02:45.843526601Z PHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\nINFO 2025-04-06T03:02:45.843539714Z PXB = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\nINFO 2025-04-06T03:02:45.843541383Z PIX = Connection traversing at most a single PCIe bridge\nINFO 2025-04-06T03:02:45.843542814Z NV# = Connection traversing a bonded set of # NVLinks\nINFO 2025-04-06T03:02:45.843544244Z {}\nINFO 2025-04-06T03:02:45.843545675Z Hypervisor vendor: KVM\nINFO 2025-04-06T03:02:45.843546867Z ulimit soft: 1048576",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-06T03:23:48+00:00",
    "closed_at": "2025-04-16T19:32:31+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5096/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5096"
  },
  {
    "number": 2800,
    "title": "[Bug] embedding model failed with `--enable-metrics`",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [X] 2. The bug has not been fixed in the latest version.\r\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [X] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nWhen I launch the latest SGLang server with e5-mistral-7b-instruct with `--enable-metrics`, the server crashes.\r\n\r\nDocker command:\r\n```\r\ndocker run -itd   --gpus \\\"device=0\\\"  \\\r\n --shm-size 10g   \\\r\n -v /raid/models:/models   \\\r\n --ulimit nofile=65535:65535   \\\r\n --network host   \\\r\n --name sglang-latest-e5-metrics-7   \\\r\n lmsysorg/sglang:latest \\\r\n python3 -m sglang.launch_server  \\\r\n --model-path=intfloat/e5-mistral-7b-instruct   \\\r\n --tp 1   --port=8080   --enable-metrics \\\r\n --is-embedding\r\n```\r\n\r\nAfter the server starts, it can't receive any requests. For instance, `curl http://localhost:8080/health_generate` will hang forever. During debug, I discovered that the Scheduler never receives any request (this [line](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/managers/scheduler.py#L408) never executes). This doesn't happen with generation models. \r\n\r\nI also discovered that [this line](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/managers/tokenizer_manager.py#L689-L693) might crash as `BatchEmbeddingOut` doesn't have completion_tokens field.\r\n\r\n### Reproduction\r\n\r\nDocker command:\r\n```\r\ndocker run -itd   --gpus \\\"device=0\\\"  \\\r\n --shm-size 10g   \\\r\n -v /raid/models:/models   \\\r\n --ulimit nofile=65535:65535   \\\r\n --network host   \\\r\n --name sglang-latest-e5-metrics-7   \\\r\n lmsysorg/sglang:latest \\\r\n python3 -m sglang.launch_server  \\\r\n --model-path=intfloat/e5-mistral-7b-instruct  \\\r\n --tp 1   --port=8080   --enable-metrics \\\r\n --is-embedding\r\n```\r\n\r\ncurl commands:\r\n`curl http://localhost:8080/health_generate`\r\n```\r\ncurl http://localhost:8080/v1/embeddings \\      \r\n    -H \"Content-Type: application/json\" \\\r\n    -d '{\r\n        \"model\": \"intfloat/e5-mistral-7b-instruct\",\r\n        \"input\": \"Write a short story set in a dystopian future where artificial intelligence governs the world, and humans are trying to regain their independence. The story should explore themes of freedom, control, and the nature of consciousness.\"\r\n    }'                 \r\n```\r\n\r\n### Environment\r\n\r\n```\r\nPython: 3.10.16 (main, Dec  4 2024, 08:53:37) [GCC 9.4.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 0 Compute Capability: 9.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\r\nCUDA Driver Version: 560.35.03\r\nPyTorch: 2.5.1+cu124\r\nsglang: 0.4.1.post4\r\nflashinfer: 0.1.6+cu124torch2.4\r\ntriton: 3.1.0\r\ntransformers: 4.47.1\r\ntorchao: 0.7.0\r\nnumpy: 1.26.4\r\naiohttp: 3.11.11\r\nfastapi: 0.115.6\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.27.0\r\ninteregular: 0.3.3\r\nmodelscope: 1.21.1\r\norjson: 3.10.13\r\npackaging: 24.2\r\npsutil: 6.1.1\r\npydantic: 2.10.4\r\nmultipart: 0.0.20\r\nzmq: 26.2.0\r\nuvicorn: 0.34.0\r\nuvloop: 0.21.0\r\nvllm: 0.6.4.post1\r\nopenai: 1.59.3\r\nanthropic: 0.42.0\r\ndecord: 0.6.0\r\nNVIDIA Topology: \r\n        GPU0    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10   NIC11   NIC12   NIC13   NIC14   NIC15   NIC16   NIC17   CPU Affinity    NUMA Affinity     GPU NUMA ID\r\nGPU0     X      PXB     PXB     NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-55,112-167    0N/A\r\nNIC0    PXB      X      PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC1    PXB     PIX      X      NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC2    NODE    NODE    NODE     X      NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC3    NODE    NODE    NODE    NODE     X      PIX     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC4    NODE    NODE    NODE    NODE    PIX      X      NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC5    NODE    NODE    NODE    NODE    NODE    NODE     X      PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC6    NODE    NODE    NODE    NODE    NODE    NODE    PIX      X      NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC7    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE     X      PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC8    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC9    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC10   SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC11   SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    NODE    NODE    NODE    NODE    NODE\r\nNIC12   SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      PIX     NODE    NODE    NODE    NODE\r\nNIC13   SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX      X      NODE    NODE    NODE    NODE\r\nNIC14   SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE     X      PIX     NODE    NODE\r\nNIC15   SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    PIX      X      NODE    NODE\r\nNIC16   SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    NODE    NODE     X      PIX\r\nNIC17   SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  NIC8: mlx5_8\r\n  NIC9: mlx5_9\r\n  NIC10: mlx5_10\r\n  NIC11: mlx5_11\r\n  NIC12: mlx5_12\r\n  NIC13: mlx5_13\r\n  NIC14: mlx5_14\r\n  NIC15: mlx5_15\r\n  NIC16: mlx5_16\r\n  NIC17: mlx5_17\r\n\r\n\r\nulimit soft: 65535\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-01-08T20:27:18+00:00",
    "closed_at": "2025-01-22T01:06:46+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2800/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2800"
  },
  {
    "number": 5502,
    "title": "Large Discrepancy in Speedup Between SGLang + Eagle and Eagle Repo Code",
    "body": "### \ud83d\udccc Background\n\nAccording to the [SGLang speculative decoding documentation](https://docs.sglang.ai/backend/speculative_decoding.html), the speedup of `SGLang + Eagle3` compared to vanilla `SGLang` is reported to be approximately **2.5\u00d7** on `LLaMA3.1-Instruct-8B` using the `MT-Bench` evaluation.\n\n![Image](https://github.com/user-attachments/assets/61bebbde-c7fe-4786-ab6e-2e721ab8edd7)\n\nHowever, the **Eagle3 paper** reports a significantly higher speedup \u2014 around **4.4\u00d7** \u2014 on the same setup (`LLaMA3.1-Instruct-8B`, `MT-Bench`).\n\n![Image](https://github.com/user-attachments/assets/0a12dbbb-d907-4ebb-8f7c-60fd06079487)\n\n---\n\n### \ud83e\uddea My Reproduction Results\n\nTo further verify, I ran tests using `DeepSeek R1 Distilled LLaMA 8B` on the `AIME` dataset. I observed the following:\n\n| Model & Task | Framework Used | Measured Speedup |\n|--------------|----------------|------------------|\n| AIME + DeepSeek R1 LLaMA 8B | SGLang + Eagle3 | ~1.5\u00d7 |\n| AIME + DeepSeek R1 LLaMA 8B | Eagle (official repo) | ~3.6\u00d7 |\n\nSo across different models and tasks, the Eagle repo consistently outperforms the SGLang integration by a large margin in terms of speedup.\n\n---\n\n### \u2753 Questions\n\n1. What are the main differences between the Eagle implementation in SGLang and the one in the Eagle GitHub repo?\n2. Are there known reasons (e.g., architectural constraints, integration issues) that lead to this speedup gap?\n3. Could it be due to default parameters like `--top-k`, number of draft tokens, or attention cache behaviors?\n4. Any recommended configurations or tuning to bring the SGLang + Eagle speedup closer to the original repo\u2019s performance?\n\n---\n\n### \ud83d\ude4f Appreciate Your Feedback!\n\nLet me know if it would help to provide detailed configs, logs, or profiling outputs.  \nThanks again for the fantastic tool and your work in speculative decoding!",
    "labels": [],
    "state": "open",
    "created_at": "2025-04-17T14:37:54+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5502/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5502"
  },
  {
    "number": 3491,
    "title": "[Bug] deepseek-r1 with 4*A100 got error",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI got an error like this. How can I fix it ?\nI convert Deepseek-R1 to DeepSeek-R1-BF16 with this [script](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/inference/fp8_cast_bf16.py)\nIt seems can not deploy with tp=32.\n\nValueError: Weight output_partition_size = 576 is not divisible by weight quantization block_n = 128.\n\n### Reproduction\n\npython3 -m sglang.launch_server --model-path /workspace/models/DeepSeek-R1-BF16 --tp 32 --dist-init-addr 10.0.0.1:5000 --nnodes 4 --node-rank 0 --trust-remote-code --host 10.0.0.1 --port 8086\n\npython3 -m sglang.launch_server --model-path /workspace/models/DeepSeek-R1-BF16 --tp 32 --dist-init-addr 10.0.0.1:5000 --nnodes 4 --node-rank 1 --trust-remote-code\n\npython3 -m sglang.launch_server --model-path /workspace/models/DeepSeek-R1-BF16 --tp 32 --dist-init-addr 10.0.0.1:5000 --nnodes 4 --node-rank 2 --trust-remote-code\n\npython3 -m sglang.launch_server --model-path /workspace/models/DeepSeek-R1-BF16 --tp 32 --dist-init-addr 10.0.0.1:5000 --nnodes 4 --node-rank 3 --trust-remote-code\n\n### Environment\n\nINFO 02-11 12:50:03 __init__.py:194] No platform detected, vLLM is running on UnspecifiedPlatform\nPython: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA A800-SXM4-80GB\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\nCUDA Driver Version: 525.105.17\nPyTorch: 2.5.1+cu124\nsglang: 0.4.2.post4\nsgl_kernel: 0.0.3.post3\nflashinfer: 0.2.0.post2\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.0\naiohttp: 3.9.1\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 23.1\npsutil: 5.9.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.61.1\ntiktoken: 0.8.0\nanthropic: 0.45.2\ndecord: 0.6.0",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-11T12:50:36+00:00",
    "closed_at": "2025-02-11T13:08:38+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3491/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3491"
  },
  {
    "number": 3647,
    "title": "[Feature] Support unified paging in multi-lora serving",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCurrently, SGL doesn't support the unified paging feature proposed by S-LoRA. However, this feature is important for memory management in multi-LoRA serving.\n\n### Related resources\n\n_No response_",
    "labels": [
      "enhancement",
      "inactive",
      "feature",
      "lora"
    ],
    "state": "open",
    "created_at": "2025-02-17T19:14:47+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3647/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3647"
  },
  {
    "number": 6177,
    "title": "[Misc] Use monotonic time for interval measurement",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCurrently, in most places, SGL measures time interval using wall clock (`time.time()`), which is not recommended as it does not guarantee monotonicity (e.g., due to NTP sync). More details can be read here: [PEP 418](https://peps.python.org/pep-0418/#rationale).\n\n**Examples in SGLang**\n\n1. In benchmark code such as [bench_one_batch.py#L378](https://github.com/sgl-project/sglang/blob/9d8ec2e67e36117ac6da0c82e597d6dbf587d578/python/sglang/bench_one_batch.py#L378),\n`time.perf_counter` should be used instead for its monotonicity guarantee and higher resolution. \n\n2. In inferencing code such as [RadixCache](https://github.com/sgl-project/sglang/blob/9d8ec2e67e36117ac6da0c82e597d6dbf587d578/python/sglang/srt/mem_cache/radix_cache.py#L68), Comparing two wall clock timestamps retrieved via `time.time()` might result in unexpected behavior due to NTP sync / daylight saving adjustment, etc. Potentially they could be replaced with `time.monotonic`.\n\n3. Interestingly, I noticed there is one location where monotonic clock is correctly used in sglang [shm_broadcast.py#L333](https://github.com/sgl-project/sglang/blob/9d8ec2e67e36117ac6da0c82e597d6dbf587d578/python/sglang/srt/distributed/device_communicators/shm_broadcast.py#L333).\n\n**Example in vllm** [benchmark_latency.py#L93](https://github.com/vllm-project/vllm/blob/4c31218f80e35c4d94097a792a15b7817381daf0/benchmarks/benchmark_latency.py#L93)\n\n### Proposal\nMake changes in two steps to control variable and ensure changes to inferencing code are tested/benchmarked in a stable environment.\n- [x] Replace time.time() with time.perf_counter() in all tests and benchmark. (PR: https://github.com/sgl-project/sglang/pull/6178)\n- [ ] Replace time.time() in inferencing code in separate PRs. (PR: https://github.com/sgl-project/sglang/pull/6211)\n",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-10T19:40:56+00:00",
    "closed_at": "2025-05-26T21:02:11+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6177/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6177"
  },
  {
    "number": 6674,
    "title": "[Bug] pydantic_core._pydantic_core.ValidationError: 1 validation error for ChatCompletionResponseChoice",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI'm trying to P/D disaggregating using SGLang and meet the error below:\n```\n[2025-05-27 21:19:46] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 25, token usage: 0.00, #running-req: 0, #unbootstrapped-req: 0, #queue-req: 0, #transferring-req: 0 \n2025-05-27 21:19:46,103 - INFO - flashinfer.jit: Loading JIT ops: cascade\n2025-05-27 21:19:46,258 - INFO - flashinfer.jit: Finished loading JIT ops: cascade\n[2025-05-27 21:19:46] Prefill transfer failed for request rank=0 req.rid='3fe2a694102f42b2bb8da8e05e43838a' req.bootstrap_room=3525890451918276584 with exception KVTransferError(bootstrap_room=3525890451918276584): Decode instance could be dead, 192.168.10.101:16043 failed due to multiple errors\n[2025-05-27 21:19:46] INFO:     127.0.0.1:44170 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\n[2025-05-27 21:19:46] ERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 401, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/starlette/applications.py\", line 113, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    raise exc\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n    await self.app(scope, receive, send)\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/starlette/routing.py\", line 715, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/starlette/routing.py\", line 735, in app\n    await route.handle(scope, receive, send)\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n               ^^^^^^^^^^^^^^^^\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    return await dependant.call(**values)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mnt/wdxu/github/sglang/python/sglang/srt/entrypoints/http_server.py\", line 611, in openai_v1_chat_completions\n    return await v1_chat_completions(_global_state.tokenizer_manager, raw_request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mnt/wdxu/github/sglang/python/sglang/srt/openai_api/adapter.py\", line 1772, in v1_chat_completions\n    response = v1_chat_generate_response(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mnt/wdxu/github/sglang/python/sglang/srt/openai_api/adapter.py\", line 1363, in v1_chat_generate_response\n    choice_data = ChatCompletionResponseChoice(\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/wdxu/miniconda3/envs/py311/lib/python3.11/site-packages/pydantic/main.py\", line 212, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 1 validation error for ChatCompletionResponseChoice\nfinish_reason\n  Input should be 'stop', 'length', 'tool_calls', 'content_filter' or 'function_call' [type=literal_error, input_value='abort', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.9/v/literal_error\n```\n\n\n\nIs it a bug? Anyone helps? Thanks!\n\n### Reproduction\n\n\nI'm using 2 NVIDIA 3090s and the starting commands of prefill / decode instances are:\n```\nCUDA_VISIBLE_DEVICES=0 python -m sglang.launch_server        --model-path /home/mnt/wdxu/models/Qwen2.5-7B   --disaggregation-mode prefill   --tool-call-parser qwen25       --port 30001\n```\n\n```\nCUDA_VISIBLE_DEVICES=1 python -m sglang.launch_server        --model-path /home/mnt/wdxu/models/Qwen2.5-7B   --disaggregation-mode decode    --tool-call-parser qwen25       --port 40001\n```\n\nmini_lb command:\n```\npython -m sglang.srt.disaggregation.mini_lb  --prefill http://127.0.0.1:30001        --decode http://127.0.0.1:40001         --host 0.0.0.0  --port 9000\n```\n\nRequest command:\n```\ncurl -s http://localhost:9000/v1/chat/completions   -H \"Content-Type: application/json\"   -d '{\"model\": \"/home/mnt/wdxu/models/Qwen2.5-7B\", \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]}'\n```\n\n### Environment\n\nalso the related package versions:\n```\nsgl-kernel                        0.1.4\nsglang                            0.4.6.post5     /home/mnt/wdxu/github/sglang/python\ntorch                             2.6.0\ntorch_memory_saver                0.0.6\ntorchao                           0.9.0\ntorchaudio                        2.5.1\ntorchmetrics                      1.5.2\ntorchprofile                      0.0.4\ntorchvision                       0.21.0\npydantic                          2.9.2\npydantic_core                     2.23.4\n...\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-27T16:29:04+00:00",
    "closed_at": "2025-05-28T07:39:48+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6674/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6674"
  },
  {
    "number": 5984,
    "title": "[Bug] Requests with logprobs throws Internal server 500 error",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen I am trying to dump logprobs, I am getting `ValueError: Out of range float values are not JSON compliant`\n\nKindly help / Suggest a work around.\n\n\n\n\n\n### Reproduction\n\n**Request** \n\nPOST /v1/completions\n\n{'model': 'meta.llama3-70b-instruct-v1:0', 'prompt': 'prompt_string', 'stream': False, 'temperature': 0.01, 'top_p': 0.3, 'max_tokens': 1000, 'logprobs': 5}\n\nError \n\nRequest to http://0.0.0.0:80/v1/completions completed with status 500\n\n\n**Server Error Log** \n\n[2025-05-02 10:26:20 TP0] Decode batch. #running-req: 1, #token: 1887, token usage: 0.00, gen throughput (token/s): 0.30, #queue-req: 0\n[2025-05-02 10:26:20] INFO:     172.17.0.1:56570 - \"POST /v1/completions HTTP/1.1\" 500 Internal Server Error\n[2025-05-02 10:26:20] ERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    return await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 113, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n    raise exc\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\n    await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 715, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 735, in app\n    await route.handle(scope, receive, send)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 338, in app\n    response = actual_response_class(content, **response_args)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 180, in __init__\n    super().__init__(content, status_code, headers, media_type, background)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 43, in __init__\n    self.body = self.render(content)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 183, in render\n    return json.dumps(\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 199, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n  File \"/usr/lib/python3.10/json/encoder.py\", line 257, in iterencode\n    return _iterencode(o, 0)\nValueError: Out of range float values are not JSON compliant\n[2025-05\n\n### Environment\n\n**GPU host** \n\nP5.48xLarge EC2 instance on AWS. This comes with 8 H100. Although it doesn't matter.\n\n**Docker Command**\n\nsudo docker run --gpus all --shm-size 256g -p 80:80 -v ~/.cache/huggingface:/root/.cache/huggingface --env \"HF_TOKEN=token_string\"  public.ecr.aws/h6k4o9f5/sglang-repo:v0.4.1-gcp01 python3 -m sglang.launch_server --model-path meta-llama/Llama-3.3-70B-Instruct --host 0.0.0.0 --port 80 --tensor-parallel-size 8 --mem-fraction-static=0.8 --enable-metric \n\n**Server Arguments**\n\nserver_args=ServerArgs(model_path='meta-llama/Llama-3.3-70B-Instruct', tokenizer_path='meta-llama/Llama-3.3-70B-Instruct', tokenizer_mode='auto', load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, quantization=None, context_length=None, device='cuda', served_model_name='meta-llama/Llama-3.3-70B-Instruct', chat_template=None, is_embedding=False, revision=None, skip_tokenizer_init=False, host='0.0.0.0', port=80, mem_fraction_static=0.8, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=8, stream_interval=1, random_seed=1001185712, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=True, decode_log_interval=40, api_key=None, file_storage_pth='sglang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_draft_model_path=None, speculative_algorithm=None, speculative_num_steps=5, speculative_num_draft_tokens=64, speculative_eagle_topk=8, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False)\n",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-05-02T17:47:14+00:00",
    "closed_at": "2025-07-05T00:18:56+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5984/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5984"
  },
  {
    "number": 4952,
    "title": "[Bug] Docker image v0.4.4.post3-cu125 is labeled as CUDA 12.5 (cu125), but it actually contains CUDA 12.4.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\ndocker run -it --entrypoint bash lmsysorg/sglang:v0.4.4.post3-cu125                                                                                       \n\nroot@0928983058f3:/sgl-workspace# apt list --installed | grep nccl\n\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n\nlibnccl-dev/now 2.21.5-1+cuda12.4 amd64 [installed,local]\nlibnccl2/now 2.21.5-1+cuda12.4 amd64 [installed,local]\n\nroot@0928983058f3:/sgl-workspace#\n\n### Reproduction\n\ndocker run -it --entrypoint bash lmsysorg/sglang:v0.4.4.post3-cu125 \n\n### Environment\n\nroot@TENCENT64:~# apt list --installed | grep nccl\n\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n\nlibnccl-dev/now 2.21.5-1+cuda12.4 amd64 [installed,local]\nlibnccl2/now 2.21.5-1+cuda12.4 amd64 [installed,local]\nroot@TENCENT64:~# python3 -m sglang.check_env\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.161.08\nPyTorch: 2.5.1+cu124\nsglang: 0.4.4.post3\nsgl_kernel: 0.0.5.post4\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.50.0\ntorchao: 0.9.0\nnumpy: 2.2.4\naiohttp: 3.11.14\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.3\ninteregular: 0.3.3\nmodelscope: 1.24.1\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.11.1\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.17\nopenai: 1.69.0\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.65.0\ndecord: 0.6.0",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-31T17:45:08+00:00",
    "closed_at": "2025-06-01T00:24:08+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4952/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4952"
  },
  {
    "number": 5874,
    "title": "[Bug] Qwen3: Incorrect response field (reasoning_content instead of content) when enable_thinking=false with streaming enabled",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen enable_thinking=False and stream=True, the API incorrectly returns the response in the reasoning_content field rather than the expected content field.\n\n\n\n### Reproduction\n\n- Request\n\n`\ncurl http://localhost:8000/v1/chat/completions   -H \"Content-Type: application/json\"   -d '{\n        \"model\": \"qwen3-32b-fp8\",\n        \"messages\": [\n          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n          {\"role\": \"user\", \"content\": \"Hello!\"}\n        ],\n        \"chat_template_kwargs\": {\"enable_thinking\": false},\n        \"stream\": true \n      }'\n`\n\n- Response\n\n`\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":null,\"**reasoning_content**\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\"Hello\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\"!\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\" How\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\" can\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\" I\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\" assist\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\" you\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\" today\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\"?\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\" \",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\"\ud83d\ude0a\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"stop\",\"matched_stop\":null}],\"usage\":null}\n\ndata: [DONE]\n`\n\n### Environment\n\nPython: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0]\nCUDA available: True\nGPU 0,1,2,3: NVIDIA GeForce RTX 4090\nGPU 0,1,2,3 Compute Capability: 8.9\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.99\nCUDA Driver Version: 550.54.14\nPyTorch: 2.5.1+cu124\nsglang: 0.4.6.post1\nsgl_kernel: 0.1.0\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.51.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\noutlines: 0.1.11\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nxgrammar: 0.1.11\nopenai: 1.62.0\ntiktoken: 0.8.0\nanthropic: 0.45.2\nlitellm: 1.61.1\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PIX     PIX     PIX     0-23,48-71      0               N/A\nGPU1    PIX      X      PIX     PIX     0-23,48-71      0               N/A\nGPU2    PIX     PIX      X      PIX     0-23,48-71      0               N/A\nGPU3    PIX     PIX     PIX      X      0-23,48-71      0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 655360",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-29T06:46:18+00:00",
    "closed_at": "2025-05-01T02:44:38+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5874/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5874"
  }
]
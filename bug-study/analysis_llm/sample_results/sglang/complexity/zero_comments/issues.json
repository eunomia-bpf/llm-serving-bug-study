[
  {
    "number": 6453,
    "title": "Compile sgl-kernel warning log: (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function",
    "body": "**When compiling sgl-kernel, I encountered the following warning, do I need to pay attention?**\n\n\n> ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'\n\n> ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb1ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'\n\n> ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb1ELb0ELb1ELb0ELb0ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE0_NS3_ILb1EEES1Y_EEDaiS1R_S1S_S1T_'\n\n> ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb1ELb0ELb1ELb1ELb0ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE0_NS3_ILb1EEES1Y_EEDaiS1R_S1S_S1T_'\n\n> ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb1ELb0ELb1ELb1ELb0ELb1ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE0_NS3_ILb1EEES1Y_EEDaiS1R_S1S_S1T_'",
    "labels": [],
    "state": "open",
    "created_at": "2025-05-20T08:03:16+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6453/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6453"
  },
  {
    "number": 3770,
    "title": "[Bug] Docs: Patch Failed for engine",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n```bash\n(sglang) chayenne@lmsys:/home/misc/chayenne$ ipy\nPython 3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]\nType 'copyright', 'credits' or 'license' for more information\nIPython 8.32.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: # launch the offline engine\n   ...: from sglang.utils import stream_and_merge, async_stream_and_merge\n   ...: import sglang as sgl\n   ...: import asyncio\n   ...: from sglang.test.test_utils import is_in_ci\n   ...:\n   ...: if is_in_ci():\n   ...:     import patch\n   ...:\n   ...:\n   ...: llm = sgl.Engine(model_path=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n\nINFO 02-21 17:16:55 __init__.py:190] Automatically detected platform cuda.\n2025-02-21 17:16:57,050 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[1], line 11\n      7 if is_in_ci():\n      8     import patch\n---> 11 llm = sgl.Engine(model_path=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n\nFile ~/.python/sglang/lib/python3.11/site-packages/sglang/api.py:43, in Engine(*args, **kwargs)\n     41 def Engine(*args, **kwargs):\n     42     # Avoid importing unnecessary dependency\n---> 43     from sglang.srt.entrypoints.engine import Engine\n     45     return Engine(*args, **kwargs)\n\nFile ~/.python/sglang/lib/python3.11/site-packages/sglang/srt/entrypoints/engine.py:52\n     50 from sglang.srt.managers.scheduler import run_scheduler_process\n     51 from sglang.srt.managers.tokenizer_manager import TokenizerManager\n---> 52 from sglang.srt.openai_api.adapter import load_chat_template_for_openai_api\n     53 from sglang.srt.server_args import PortArgs, ServerArgs\n     54 from sglang.srt.torch_memory_saver_adapter import TorchMemorySaverAdapter\n\nFile ~/.python/sglang/lib/python3.11/site-packages/sglang/srt/openai_api/adapter.py:32\n     29 from sglang.lang.chat_template import get_chat_template_by_model_path\n     31 try:\n---> 32     from outlines.fsm.json_schema import convert_json_schema_to_str\n     33 except ImportError:\n     34     # Before outlines 0.0.47, convert_json_schema_to_str is under\n     35     # outlines.integrations.utils\n     36     from outlines.integrations.utils import convert_json_schema_to_str\n\nFile ~/.python/sglang/lib/python3.11/site-packages/outlines/__init__.py:2\n      1 \"\"\"Outlines is a Generative Model Programming Framework.\"\"\"\n----> 2 import outlines.generate\n      3 import outlines.grammars\n      4 import outlines.models\n\nFile ~/.python/sglang/lib/python3.11/site-packages/outlines/generate/__init__.py:2\n      1 from .api import SequenceGenerator\n----> 2 from .cfg import cfg\n      3 from .choice import choice\n      4 from .format import format\n\nFile ~/.python/sglang/lib/python3.11/site-packages/outlines/generate/cfg.py:7\n      1 from functools import singledispatch\n      3 from outlines.generate.api import (\n      4     SequenceGeneratorAdapter,\n      5     VisionSequenceGeneratorAdapter,\n      6 )\n----> 7 from outlines.models import LlamaCpp, OpenAI, TransformersVision\n      8 from outlines.samplers import Sampler, multinomial\n     11 @singledispatch\n     12 def cfg(\n     13     model, cfg_str: str, sampler: Sampler = multinomial()\n     14 ) -> SequenceGeneratorAdapter:\n\nFile ~/.python/sglang/lib/python3.11/site-packages/outlines/models/__init__.py:14\n     12 from .llamacpp import LlamaCpp, llamacpp\n     13 from .mlxlm import MLXLM, mlxlm\n---> 14 from .openai import OpenAI, azure_openai, openai\n     15 from .transformers import Transformers, TransformerTokenizer, mamba, transformers\n     16 from .transformers_vision import TransformersVision, transformers_vision\n\nFile ~/.python/sglang/lib/python3.11/site-packages/outlines/models/openai.py:9\n      5 from typing import Callable, Dict, List, Optional, Tuple, Union\n      7 import numpy as np\n----> 9 from outlines.base import vectorize\n     10 from outlines.caching import cache\n     12 __all__ = [\"OpenAI\", \"openai\", \"azure_openai\"]\n\nFile ~/.python/sglang/lib/python3.11/site-packages/outlines/base.py:32\n     29 try:\n     30     import nest_asyncio\n---> 32     nest_asyncio.apply()\n     33 except ImportError:\n     34     print(\n     35         \"Couldn't patch nest_asyncio because it's not installed. Running in the notebook might be have issues\"\n     36     )\n\nFile ~/.python/sglang/lib/python3.11/site-packages/nest_asyncio.py:18, in apply(loop)\n     15 _patch_policy()\n     16 _patch_tornado()\n---> 18 loop = loop or asyncio.get_event_loop()\n     19 _patch_loop(loop)\n\nFile ~/.python/sglang/lib/python3.11/site-packages/nest_asyncio.py:40, in _patch_asyncio.<locals>._get_event_loop(stacklevel)\n     38 loop = events._get_running_loop()\n     39 if loop is None:\n---> 40     loop = events.get_event_loop_policy().get_event_loop()\n     41 return loop\n\nFile ~/.python/sglang/lib/python3.11/site-packages/nest_asyncio.py:67, in _patch_policy.<locals>.get_event_loop(self)\n     65 if self._local._loop is None:\n     66     loop = self.new_event_loop()\n---> 67     _patch_loop(loop)\n     68     self.set_event_loop(loop)\n     69 return self._local._loop\n\nFile ~/.python/sglang/lib/python3.11/site-packages/nest_asyncio.py:193, in _patch_loop(loop)\n    191     return\n    192 if not isinstance(loop, asyncio.BaseEventLoop):\n--> 193     raise ValueError('Can\\'t patch loop of type %s' % type(loop))\n    194 cls = loop.__class__\n    195 cls.run_forever = run_forever\n\nValueError: Can't patch loop of type <class 'uvloop.Loop'>\n\nTraceback (most recent call last):\n  File \"/data/chayenne/.python/sglang/bin/ipython\", line 8, in <module>\n    sys.exit(start_ipython())\n             ^^^^^^^^^^^^^^^\n  File \"/data/chayenne/.python/sglang/lib/python3.11/site-packages/IPython/__init__.py\", line 130, in start_ipython\n    return launch_new_instance(argv=argv, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/chayenne/.python/sglang/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/data/chayenne/.python/sglang/lib/python3.11/site-packages/IPython/terminal/ipapp.py\", line 317, in start\n    self.shell.mainloop()\n  File \"/data/chayenne/.python/sglang/lib/python3.11/site-packages/IPython/terminal/interactiveshell.py\", line 971, in mainloop\n    self.interact()\n  File \"/data/chayenne/.python/sglang/lib/python3.11/site-packages/IPython/terminal/interactiveshell.py\", line 956, in interact\n    code = self.prompt_for_code()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/chayenne/.python/sglang/lib/python3.11/site-packages/IPython/terminal/interactiveshell.py\", line 899, in prompt_for_code\n    text = self.pt_app.prompt(\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/data/chayenne/.python/sglang/lib/python3.11/site-packages/prompt_toolkit/shortcuts/prompt.py\", line 1035, in prompt\n    return self.app.run(\n           ^^^^^^^^^^^^^\n  File \"/data/chayenne/.python/sglang/lib/python3.11/site-packages/prompt_toolkit/application/application.py\", line 1002, in run\n    return asyncio.run(coro)\n           ^^^^^^^^^^^^^^^^^\n  File \"/data/chayenne/.python/sglang/lib/python3.11/site-packages/nest_asyncio.py\", line 26, in run\n    loop = asyncio.get_event_loop()\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/chayenne/.python/sglang/lib/python3.11/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n    loop = events.get_event_loop_policy().get_event_loop()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/chayenne/.python/sglang/lib/python3.11/site-packages/nest_asyncio.py\", line 67, in get_event_loop\n    _patch_loop(loop)\n  File \"/data/chayenne/.python/sglang/lib/python3.11/site-packages/nest_asyncio.py\", line 193, in _patch_loop\n    raise ValueError('Can\\'t patch loop of type %s' % type(loop))\nValueError: Can't patch loop of type <class 'uvloop.Loop'>\n\nIf you suspect this is an IPython 8.32.0 bug, please report it at:\n    https://github.com/ipython/ipython/issues\nor send an email to the mailing list at ipython-dev@python.org\n\nYou can print a more detailed traceback right now with \"%tb\", or use \"%debug\"\nto interactively debug it.\n\nExtra-detailed tracebacks for bug-reporting purposes can be enabled via:\n    %config Application.verbose_crash=True\n\nsys:1: RuntimeWarning: coroutine 'Application.run_async' was never awaited\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n```\n\nThe patch failed for ipython kernel. Should fix it ASAP. @shuaills Thanks so much.\n\n### Reproduction\n\nSee above.\n\n### Environment\n\nthe latest commit.",
    "labels": [
      "documentation",
      "help wanted",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-02-21T17:19:40+00:00",
    "closed_at": "2025-02-21T21:30:52+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3770/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3770"
  },
  {
    "number": 7311,
    "title": "There is a question. If 10 rounds of windows are transmitted each time, can the benefits of the sglang kv pool be enjoyed",
    "body": "There is a question. If 10 rounds of windows are transmitted each time, the first round of conversation must be lost for requests exceeding 10 rounds of windows. If so, can the benefits of the sglang kv pool be enjoyed? If not, is it recommended not to process any historical conversations? Is this the best way to use sglang in a production environment?",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-18T11:21:18+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7311/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7311"
  },
  {
    "number": 2814,
    "title": "[Usage] Some questions about the parameter --chunked-prefill-size",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n```\r\ndef budget_state(self):\r\n        if self.rem_total_tokens <= 0 or self.cur_rem_tokens <= 0:\r\n            return AddReqResult.NO_TOKEN\r\n\r\n        if self.rem_input_tokens <= 0 or (\r\n            self.rem_chunk_tokens is not None and self.rem_chunk_tokens <= 0\r\n        ):\r\n            return AddReqResult.OTHER\r\n\r\n        return AddReqResult.CONTINUE\r\n```\r\n\r\nThe above is part of the source code from the file scheduler.py. I think the role of self.rem_chunk_tokens is the same as self.rem_input_tokens, both are used to limit the total number of prefill tokens.Both of their modifications are located in the following function.\r\n\r\n```\r\ndef _prefill_one_req(\r\n        self, prefix_len: int, extend_input_len: int, max_new_tokens: int\r\n    ):\r\n        self.rem_total_tokens -= extend_input_len + max_new_tokens\r\n        self.cur_rem_tokens -= extend_input_len\r\n        self.rem_input_tokens -= extend_input_len\r\n        if self.rem_chunk_tokens is not None:\r\n            self.rem_chunk_tokens -= extend_input_len\r\n\r\n        self.log_hit_tokens += prefix_len\r\n        self.log_input_tokens += extend_input_len\r\n```\r\n\r\nOf course, self.rem_chunk_tokens is also used to determine whether the prompt of a request needs to be truncated.\r\n\r\n```\r\n        if (\r\n            self.rem_chunk_tokens is None\r\n            or req.extend_input_len <= self.rem_chunk_tokens\r\n        ):\r\n            self.can_run_list.append(req)\r\n            self._prefill_one_req(\r\n                0,\r\n                req.extend_input_len,\r\n                min(req.sampling_params.max_new_tokens, CLIP_MAX_NEW_TOKENS_ESTIMATION),\r\n            )\r\n        else:\r\n            # Chunked prefill\r\n            trunc_len = self.rem_chunk_tokens\r\n            if trunc_len == 0:\r\n                return AddReqResult.OTHER\r\n\r\n            req.extend_input_len = trunc_len\r\n            req.fill_ids = req.fill_ids[:trunc_len]\r\n            self.can_run_list.append(req)\r\n            self.new_being_chunked_req = req\r\n            self._prefill_one_req(0, trunc_len, 0)\r\n```\r\nWhat confuses me is that I understand that self.rem_chunk_tokens should be used to split the prompt of the last request or each request, but each request will modify self.rem_chunk_tokens, and finally determine whether to continue adding requests to the batch based on self.rem_chunk_tokens > 0. So I don't understand what self.rem_chunk_tokens actually does.\r\n\r\nHope to receive your feedback, thank you!\r\n\n\n### Reproduction\n\nHope to receive your feedback, thank you!\n\n### Environment\n\nHope to receive your feedback, thank you!",
    "labels": [],
    "state": "closed",
    "created_at": "2025-01-09T11:49:19+00:00",
    "closed_at": "2025-01-09T12:01:26+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2814/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2814"
  },
  {
    "number": 7715,
    "title": "[Feature] Migrate support for FP8 in Ampere GPUs",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nHi,\n\nFP8 native models and quantizations are getting more popular. Being an efficient way to deploy models.\nThe Ampere cards don't have native support for fp8, but their usage is very widespread, like the nvidia 3090. Many people have systems with them due to their low cost and 24GB of VRAM.\n\nVLLM Added support for it with a Marlin kernel and fp8 models load just fine with decent speed on Ampere gpus:\nhttps://github.com/vllm-project/vllm/pull/5975\n\nCurrently when trying to load a FP8 model with an Ampere GPU you get this error:\n`ValueError(\"type fp8e4nv not supported in this architecture. The supported fp8 dtypes are ('fp8e4b15', 'fp8e5')\")`\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-02T12:31:27+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7715/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7715"
  },
  {
    "number": 8074,
    "title": "[Feature] Maybe the content field in ChatCompletionMessageGenericParam should be Optional",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nAccording to the https://platform.openai.com/docs/api-reference/chat/create OpenAI Platform, when there is `tool_calls`, the `content` could be optional. But the current implementation could cause 400 Bad Request when `content` filed is missed under `tool_calls`  situation.\n```\nclass ChatCompletionMessageGenericParam(BaseModel):\n    role: Literal[\"system\", \"assistant\", \"tool\"]\n    content: Union[str, List[ChatCompletionMessageContentTextPart], None]\n    tool_call_id: Optional[str] = None\n    name: Optional[str] = None\n    reasoning_content: Optional[str] = None\n    tool_calls: Optional[List[ToolCall]] = Field(default=None, examples=[None])\n\n    @field_validator(\"role\", mode=\"before\")\n    @classmethod\n    def _normalize_role(cls, v):\n        if isinstance(v, str):\n            v_lower = v.lower()\n            if v_lower not in {\"system\", \"assistant\", \"tool\"}:\n                raise ValueError(\n                    \"'role' must be one of 'system', 'assistant', or 'tool' (case-insensitive).\"\n                )\n            return v_lower\n        raise ValueError(\"'role' must be a string\")\n\n```\n\n<img width=\"1356\" height=\"1506\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b5bdf617-8509-4067-a6ef-83262eb5dc62\" />\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-16T01:57:02+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/8074/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/8074"
  },
  {
    "number": 7253,
    "title": "[Feature] Make random-range-ratio give symmetric distribution around --input-length (parity with vllm)",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nFeature suggestion / request to change the way --random-range-ratio is used, as done in the vllm codebase \n[Fix range_ratio Bug in RandomDataset #16126](https://github.com/vllm-project/vllm/pull/16126)\n\u00a0\nThere's another recent change at [[Bugfix] Fixed prompt length for random dataset](https://github.com/vllm-project/vllm/pull/17408/files#top) which may also be useful.\n\nSome backstory: the syntax of --random-range-ratio looks identical in sglang and vllm, but the ranges in token lengths are quite different: \n\nsglang => [input_len * random_ratio, input_len]\nvllm => [input_len * (1 - random_ratio), input_len * (1 + random_ratio)]\n\nWith a default of zero, this leads to sglang averaging half the input tokens for the same settings compared with vllm.\nIt looks like in this case the vllm codebase is the one that diverged (see changes above), but the motivations and changes look sensible, so I wondered if a similar change would be welcomed in the sglang codebase?\n\nDetails\n\nIn the codebases both sglang and vllm have the same default --random-range-ratio=0 - [sglang/python/sglang/bench_serving.py at main \u00b7 sgl-project/sglang](https://github.com/sgl-project/sglang/blob/main/python/sglang/bench_serving.py#L1723) and [vllm/benchmarks/benchmark_serving.py at main \u00b7 vllm-project/vllm](https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_serving.py#L1132) - but the selection processes inside [vllm/benchmarks/benchmark_dataset.py at main \u00b7 vllm-project/vllm](https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_dataset.py#L333) and [sglang/python/sglang/bench_serving.py at main \u00b7 sgl-project/sglang](https://github.com/sgl-project/sglang/blob/main/python/sglang/bench_serving.py#L883) make different ranges.\n \n\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-17T00:00:59+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7253/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7253"
  },
  {
    "number": 4078,
    "title": "[Feature] Add examples for server token-in-token-out",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nhttps://github.com/sgl-project/sglang/pull/3941\n\nfollow this PR.\n\nadd it in `examples/token_in_token_out`.\n\n@Qiaolin-Yu \n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-05T05:34:21+00:00",
    "closed_at": "2025-03-05T21:16:32+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4078/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4078"
  },
  {
    "number": 1664,
    "title": "[Bug] difference of kv-cache-prefixing between vLLM and sglang",
    "body": "### Checklist\r\n\r\n- [ ] 1. I have searched related issues but cannot get the expected help.\r\n- [ ] 2. The bug has not been fixed in the latest version.\r\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [ ] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nno bug. I am just wondering the difference of kv-cache-prefixing between vLLM impletention and SGLang implementation.\r\n\r\nvLLM use hash to store and verify cached token:\r\n<img width=\"318\" alt=\"image\" src=\"https://github.com/user-attachments/assets/c89b6dc1-101a-43de-b925-8ba1e45e6d20\">\r\nSGLang uses RadixAttention, so what is the difference? I found SGLang is faster than vLLM, why SGLang RadixAttention is faster than vLLM KV-Cache-prefixing?\r\n\r\n### Reproduction\r\n\r\nnot available\r\n\r\n### Environment\r\n\r\n//",
    "labels": [],
    "state": "closed",
    "created_at": "2024-10-14T06:16:19+00:00",
    "closed_at": "2024-10-14T14:50:09+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1664/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1664"
  },
  {
    "number": 7903,
    "title": "[Bug] Error from xgrammar is not well processed by sglang",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThe error raised from xgrammar is not well processed and crashed sglang. 4 out of 600 requests kept running and the number of token below kept growing and then sglang crashed. The logs are attached as below (only logs from TP0 are retained for repeated logs from TP0 to TP7). The error raised by xgrammar is by design and maybe sglang should have some mechanisms to process it correctly instead of crashing directly. https://xgrammar.mlc.ai/docs/xgrammar_features/runtime_safeguards.html#recursion-limit\n\n```\n  File \"~/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 2747, in run_scheduler_process\n    scheduler.event_loop_overlap()\n  File \"~/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"~/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 813, in event_loop_overlap\n    self.process_batch_result(\n  File \"~/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 1804, in process_batch_result\n    self.process_batch_result_decode(batch, result, launch_done)\n  File \"~/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/scheduler_output_processor_mixin.py\", line 269, in process_batch_result_decode\n    self.set_next_batch_sampling_info_done(batch)\n  File \"~/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 2020, in \n    set_next_batch_sampling_info_done\n    batch.next_batch_sampling_info.update_regex_vocab_mask()\n  File \"~/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/sampling/sampling_batch_info.py\", line 193, in update_regex_vocab_mask\n    grammar.fill_vocab_mask(self.vocab_mask, i)\n  File \"~/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/constrained/xgrammar_backend.py\", line 89, in fill_vocab_mask                                                  \n    self.matcher.fill_next_token_bitmask(vocab_mask, idx)\n  File \"~/miniconda3/envs/sglang/lib/python3.10/site-packages/xgrammar/matcher.py\", line 308, in fill_next_token_bitmask\n    return self._handle.fill_next_token_bitmask(\nRuntimeError: [22:44:40] /project/cpp/support/recursion_guard.h:33: RecursionGuard: Maximum recursion depth exceeded. Current depth: 10001, Max allowed: 10000\n[2025-07-09 22:44:40] Received sigquit from a child process. It usually means the child failed.                                                                                                       \n[2025-07-09 22:44:40] Dumping requests before crash. self.crash_dump_folder=None \n```\n\n### Reproduction\n\nhttp_proxy=\"\" CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 VLLM_MARLIN_USE_ATOMIC_ADD=1 python -m sglang.launch_server --model-path Qwen/Qwen2.5-72B-Instruct-AWQ --tool-call-parser qwen25 --tensor-parallel-size 8 --disable-custom-all-reduce --mem-fraction-static 0.65 --schedule-conservativeness 0.1\nQwen/Qwen2.5-72B-Instruct-AWQ\n\n### Environment\n\nPython: 3.10.13 | packaged by conda-forge | (main, Oct 26 2023, 18:07:37) [GCC 12.3.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7,8,9: NVIDIA GeForce RTX 4080\nGPU 0,1,2,3,4,5,6,7,8,9 Compute Capability: 8.9                                                                                                                                                       CUDA_HOME: /usr/local/cuda                                                                                                                                                                            NVCC: Cuda compilation tools, release 12.5, V12.5.40\nCUDA Driver Version: 555.42.02\nPyTorch: 2.7.1+cu126\nsglang: 0.4.9.post1\nsgl_kernel: 0.2.4\nflashinfer_python: 0.2.7.post1\ntriton: 3.3.1\ntransformers: 4.53.0\ntorchao: 0.9.0\nnumpy: 2.2.5\naiohttp: 3.11.18\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.33.0\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 23.1\npsutil: 5.9.5\npydantic: 2.11.3\npython-multipart: 0.0.20\npyzmq: 26.4.0\nuvicorn: 0.34.2\nuvloop: 0.21.0\nvllm: 0.9.0.1\nxgrammar: 0.1.20\nopenai: 1.75.0\ntiktoken: 0.9.0\nanthropic: 0.50.0\nlitellm: 1.67.1\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    GPU8    GPU9    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\nGPU1    PIX      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\nGPU2    NODE    NODE     X      PIX     PIX     SYS     SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\nGPU3    NODE    NODE    PIX      X      PIX     SYS     SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\nGPU4    NODE    NODE    PIX     PIX      X      SYS     SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\nGPU5    SYS     SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    NODE    16-31,48-63     1               N/A\nGPU6    SYS     SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    NODE    16-31,48-63     1               N/A\nGPU7    SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      PIX     PIX     16-31,48-63     1               N/A\nGPU8    SYS     SYS     SYS     SYS     SYS     NODE    NODE    PIX      X      PIX     16-31,48-63     1               N/A\nGPU9    SYS     SYS     SYS     SYS     SYS     NODE    NODE    PIX     PIX      X      16-31,48-63     1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1024\n",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-09T15:17:45+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7903/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7903"
  },
  {
    "number": 8084,
    "title": "[Feature] Add chat method support to the offline Engine class",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n`vLLM` now supports both the `generate` and `chat` methods for offline inference. The `generate` method offers more flexibility, while the `chat` method provides a more convenient interface for conversational use cases.\n\nCurrently, `SGLang` supports the `generate` method. Are there any plans to add support for the `chat` method as well? This feature would be helpful for users who want a streamlined conversational interface similar to that provided by vLLM.\n\n### Related resources\n\n[vLLM.LLM.chat](https://docs.vllm.ai/en/stable/api/vllm/index.html#vllm.LLM.chat)",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-16T07:37:45+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/8084/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/8084"
  },
  {
    "number": 7365,
    "title": "[Bug] Deepseek FP4 doesn't support MTP",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nCurrently when using MTP with FP4 Deepseek, the server will crash with\n\n```\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/loader.py\", line 381, in load_model\n    self.load_weights_and_postprocess(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/loader.py\", line 389, in load_weights_and_postprocess\n    model.load_weights(weights)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_nextn.py\", line 161, in load_weights\n    super().load_weights(weights, is_nextn=True)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 2099, in load_weights\n    weight_loader(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/layer.py\", line 594, in weight_loader\n    self._load_model_weight_or_group_weight_scale(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/layer.py\", line 401, in _load_model_weight_or_group_weight_scale\n    self._load_w13(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/layer.py\", line 455, in _load_w13\n    expert_data.copy_(loaded_weight)\nRuntimeError: The size of tensor a (3584) must match the size of tensor b (7168) at non-singleton dimension 1\n```\n\nI think the reason is MTP module in FP4 model is not quantized, or not stored in the same way, but the same quant config is reused for both MTP and main model.  \n\nAfter switching linear and MoE quant method to `UnquantizedLinearMethod` and `UnquantizedFusedMoEMethod`. Something like https://github.com/pyc96/sglang/blob/7c8ce7870ab4a7ab918b288e661ad182e9e21e13/python/sglang/srt/layers/quantization/modelopt_quant.py#L348\n\nI am not sure if this is the right solution. Maybe need to consult NV folks on what is the exact format for MTP in FP4 ckpt.\n\n\n### Reproduction\n\n```\npython3 -m sglang.launch_server --port=7080 --model-path=/root/.cache/huggingface/hub/models--nvidia--DeepSeek-R1-0528-FP4/snapshots/91cfc7c35acd8ecfc769205989310208b8b81c9c/  --trust-remote-code --tp=4 --host=0.0.0.0 --speculative-algorithm=EAGLE --speculative-num-steps=3 --speculative-eagle-topk=1 --speculative-num-draft-tokens=4\n```\n\n### Environment\n\nroot@predictor-resource-pool-1907436070400688128-bcc64d749-kccns:/sgl-workspace/sglang# python3 -m sglang.check_env\nPython: 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA B200\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 10.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.9, V12.9.41\nCUDA Driver Version: 570.124.06\nPyTorch: 2.7.1+cu128\nsglang: 0.4.7.post1\nsgl_kernel: 0.1.9\nflashinfer_python: 0.2.6.post1\ntriton: 3.3.1\ntransformers: 4.52.3\ntorchao: 0.9.0\nnumpy: 2.1.2\naiohttp: 3.12.12\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.33.0\ninteregular: 0.3.3\nmodelscope: 1.27.0\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.5\npython-multipart: 0.0.20\npyzmq: 26.4.0\nuvicorn: 0.34.3\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.19\nopenai: 1.88.0\ntiktoken: 0.9.0\nanthropic: Module Not Found\nlitellm: Module Not Found\ndecord: Module Not Found\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    0-55,112-167    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    0-55,112-167    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    0-55,112-167    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    0-55,112-167    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    56-111,168-223  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    56-111,168-223  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    56-111,168-223  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      56-111,168-223  1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nHypervisor vendor: KVM\nulimit soft: 1048576",
    "labels": [
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-06-19T18:40:58+00:00",
    "closed_at": "2025-06-25T18:27:55+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7365/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7365"
  },
  {
    "number": 7851,
    "title": "[RFC] DRI for every module",
    "body": "SGLang now has a wide range of features and modules, with many efforts happening in parallel across new features, optimizations, and bug fixes. Each module already has an internal DRI (Directly Responsible Individual), but these assignments haven\u2019t been made public. As a result, some community pull requests have experienced delays, and contributors often don\u2019t know who to reach out to.\n\nWe plan to make the DRI list public over the next two weeks and will actively follow up in a dedicated public channel. We're looking forward to working more closely with the community! Cheers!",
    "labels": [
      "high priority",
      "collaboration"
    ],
    "state": "open",
    "created_at": "2025-07-08T08:14:57+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7851/reactions",
      "total_count": 21,
      "+1": 8,
      "-1": 0,
      "laugh": 0,
      "hooray": 7,
      "confused": 0,
      "heart": 0,
      "rocket": 6,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/7851"
  },
  {
    "number": 6491,
    "title": "[Bug] Router Crashes Intermittently",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nIn our scenario, the router inexplicably crashes after a period of time (which could range from hours to weeks). Through the logs, I identified the following errors:\n```\n{\"log\":\"[Router (Rust)] 2025-05-21 03:52:00 - DEBUG - starting new connection: http://192.168.0.99:8001/\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.40688383Z\"}\n{\"log\":\"[Router (Rust)] 2025-05-21 03:52:00 - DEBUG - connecting to 192.168.0.99:8001\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.406894394Z\"}\n{\"log\":\"[Router (Rust)] 2025-05-21 03:52:00 - DEBUG - connected to 192.168.0.99:8001\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.407215453Z\"}\n{\"log\":\"[Router (Rust)] 2025-05-21 03:52:00 - DEBUG - starting new connection: http://192.168.0.99:8001/\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.475498743Z\"}\n{\"log\":\"[Router (Rust)] 2025-05-21 03:52:00 - DEBUG - connecting to 192.168.0.99:8001\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.475507723Z\"}\n{\"log\":\"[Router (Rust)] 2025-05-21 03:52:00 - DEBUG - connected to 192.168.0.99:8001\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.475780015Z\"}\n{\"log\":\"[Router (Rust)] 2025-05-21 03:52:00 - DEBUG - pooling idle connection for (\\\"http\\\", 192.168.0.99:8001)\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.478583325Z\"}\n{\"log\":\"[Router (Rust)] 2025-05-21 03:52:00 - DEBUG - pooling idle connection for (\\\"http\\\", 192.168.0.12:8001)\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.541698776Z\"}\n{\"log\":\"[Router (Rust)] 2025-05-21 03:52:00 - DEBUG - pooling idle connection for (\\\"http\\\", 192.168.0.5:8001)\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.558685005Z\"}\n{\"log\":\"[Router (Rust)] 2025-05-21 03:52:00 - DEBUG - reuse idle connection for (\\\"http\\\", 192.168.0.99:8001)\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.717100847Z\"}\n{\"log\":\"thread 'actix-rt|system:0|arbiter:0' panicked at src/router.rs:589:79:\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.752381579Z\"}\n{\"log\":\"called `Option::unwrap()` on a `None` value\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.75239185Z\"}\n{\"log\":\"note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.752411226Z\"}\n{\"log\":\"[Router (Rust)] 2025-05-21 03:52:00 - DEBUG - pooling idle connection for (\\\"http\\\", 192.168.0.5:8001)\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.754821593Z\"}\n{\"log\":\"[Router (Rust)] 2025-05-21 03:52:00 - DEBUG - starting new connection: http://192.168.0.99:8001/\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.829139458Z\"}\n{\"log\":\"[Router (Rust)] 2025-05-21 03:52:00 - DEBUG - connecting to 192.168.0.99:8001\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.829192622Z\"}\n{\"log\":\"[Router (Rust)] 2025-05-21 03:52:00 - DEBUG - connected to 192.168.0.99:8001\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.829467623Z\"}\n{\"log\":\"[Router (Rust)] 2025-05-21 03:52:00 - DEBUG - pooling idle connection for (\\\"http\\\", 192.168.0.99:8001)\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.831402477Z\"}\n{\"log\":\"thread 'actix-rt|system:0|arbiter:42' panicked at src/router.rs:464:62:\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.833636536Z\"}\n{\"log\":\"called `Result::unwrap()` on an `Err` value: PoisonError { .. }\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.833645485Z\"}\n{\"log\":\"[Router (Rust)] 2025-05-21 03:52:00 - DEBUG - pooling idle connection for (\\\"http\\\", 192.168.0.12:8001)\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:00.948293272Z\"}\n{\"log\":\"thread 'actix-rt|system:0|arbiter:59' panicked at src/router.rs:463:40:\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:01.076034944Z\"}\n{\"log\":\"called `Result::unwrap()` on an `Err` value: PoisonError { .. }\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:01.076044555Z\"}\n{\"log\":\"thread 'actix-rt|system:0|arbiter:187' panicked at src/router.rs:463:40:\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:01.0887445Z\"}\n{\"log\":\"called `Result::unwrap()` on an `Err` value: PoisonError { .. }\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:01.088774003Z\"}\n{\"log\":\"thread 'actix-rt|system:0|arbiter:131' panicked at src/router.rs:463:40:\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:01.228421113Z\"}\n{\"log\":\"called `Result::unwrap()` on an `Err` value: PoisonError { .. }\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:01.228459473Z\"}\n{\"log\":\"thread 'actix-rt|system:0|arbiter:60' panicked at src/router.rs:463:40:\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:01.508341798Z\"}\n{\"log\":\"called `Result::unwrap()` on an `Err` value: PoisonError { .. }\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:01.50835179Z\"}\n{\"log\":\"thread 'actix-rt|system:0|arbiter:61' panicked at src/router.rs:463:40:\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:01.525257122Z\"}\n{\"log\":\"called `Result::unwrap()` on an `Err` value: PoisonError { .. }\\n\",\"stream\":\"stderr\",\"time\":\"2025-05-21T03:52:01.525266461Z\"}\n```\n\n\n### Reproduction\n\n```\nENTRYPOINT  [\"python\", \"-m\", \"sglang_router.launch_router\"]\n```\n\n### Environment\n\n```\nPackage       Version\n------------- -------\npip           23.0.1\nsetuptools    58.1.0\nsglang-router 0.1.4\nwheel         0.45.1\n```",
    "labels": [
      "router"
    ],
    "state": "open",
    "created_at": "2025-05-21T07:51:23+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6491/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6491"
  },
  {
    "number": 7255,
    "title": "[Bug] DeepEP 8 + two-batch overlap was significantly lower than that of TP8",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n\nI deployed Qwen3-30B-A3B  on a single machine with 8 H20 GPUs with DeepEP + two-batch overlap.\nMy test case involved 512 input tokens and 1 output token, with concurrent of [1, 2, 4, 8, 16, 32].\n\nI found that the performance of DeepEP + two-batch overlap was significantly lower than that of TP8. Specifically, the TTFT (Time To First Token) increased by approximately 92.38% compared to TP8.\n\nFurthermore, Nsight profiling revealed that when only DeepEP was enabled, the communication-related operators consumed an extremely high amount of time. The notify_dispatch and notify_combine operators together accounted for 86.5% of the total time.\nEven with two-batch overlap enabled, the communication latency remained very high.\n\n\n\nOnly DeepEP enabled:\n\n![Image](https://github.com/user-attachments/assets/6e875fc8-9f7c-4b7d-99d4-4e387aff31f9)\n\nDeepEP + two-batch overlap enabled:\n\n![Image](https://github.com/user-attachments/assets/59559857-7e9d-40d6-ae93-2bf39d34ca13)\n\n\nCan anyone help analyze this and give me some suggestions?\n\n### Reproduction\n\nMy deployment command was:\nSGL_ENABLE_JIT_DEEPGEMM=0 nohup python3 -m sglang.launch_server --model-path Qwen3-30B-A3B --trust-remote-code --port 4396 --trust-remote-code --served-model-name deepseek --tensor-parallel-size 8 --enable-ep-moe --mem-fraction-static 0.9 --disable-radix-cache --disable-chunked-prefix-cache --cuda-graph-max-bs 48 --enable-deepep-moe --deepep-mode normal --moe-dense-tp-size 1 --enable-dp-attention --dp-size 8\"\n\n### Environment\n\noot@fc49d8115b50:/workspace# python3 -m sglang.check_env \nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.183.06\nPyTorch: 2.7.1+cu126\nsglang: 0.4.7\nsgl_kernel: 0.1.7\nflashinfer_python: 0.2.6.post1\ntriton: 3.3.1\ntransformers: 4.52.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.13\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.31.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\npython-multipart: 0.0.20\npyzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.8.5\nxgrammar: 0.1.19\nopenai: 1.75.0\ntiktoken: 0.9.0\nanthropic: 0.51.0\nlitellm: 1.69.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    SYS     SYS     0-47,96-143     0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    SYS     SYS     0-47,96-143     0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    SYS     SYS     0-47,96-143     0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    PIX     SYS     SYS     0-47,96-143     0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     PIX     NODE    48-95,144-191   1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     NODE    NODE    48-95,144-191   1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     NODE    PIX     48-95,144-191   1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     NODE    NODE    48-95,144-191   1               N/A\nNIC0    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    SYS     SYS\nNIC1    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE     X      SYS     SYS\nNIC2    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS      X      NODE\nNIC3    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_bond_0\n  NIC1: mlx5_bond_1\n  NIC2: mlx5_bond_2\n  NIC3: mlx5_bond_3\n\n\nulimit soft: 262144",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-17T02:36:16+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7255/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7255"
  },
  {
    "number": 7845,
    "title": "[Bug] ValueError: No processor registered for architecture: ['Qwen2_5_VLForConditionalGeneration']. Registered architectures: ['CLIPModel', 'DeepseekVL2ForCausalLM']",
    "body": "\nI'm using version 0.4.6.post1, but when I run the following code, I get a ValueError: No processor registered for architecture: ['Qwen2_5_VLForConditionalGeneration']. Registered architectures: ['CLIPModel', 'DeepseekVL2ForCausalLM']. The model I'm using is R1-Onevision/Qwen2.5VL-7B-Instruct. Prior to this, I successfully ran QwQ-32B using the same code.\n\n\n\n\n\n\n\n\n`import sglang as sgl\nimport json\nimport time\nfrom tqdm import tqdm\nimport argparse\nimport os\nfrom transformers import AutoTokenizer\nfrom sglang.srt.sampling.sampling_params import SamplingParams\nfrom matheval import evaluator_map, set_client, AIMEEvaluator\nimport asyncio\nimport matheval\nimport humanevaleval\nimport mbppeval\nfrom huggingface_hub import HfApi\nimport torch\nimport time\nimport convert_livecodebench\n\nMATH_DATASETS = [\"math500\",\"aime2024\",\"aime2025\",\"gpqa_diamond\",\"gsm8k\",\"amc23\"]\nCODE_DATASETS = [\"humaneval\",\"mbpp\",\"livecodebench\"]\n\ndef main():\n    parser = argparse.ArgumentParser(description='Process some parameters for text generation.')\n    parser.add_argument('--dataset', type=str, choices=[\"math500\", \"aime2024\", \"aime2025\", \"gpqa_diamond\", \"gsm8k\", \"amc23\", \"humaneval\", \"mbpp\", \"livecodebench\"], help='Name of dataset')\n    parser.add_argument('--sampling_backend', type=str, choices=[\"pytorch\", \"flashinfer\"], default=\"flashinfer\", help='Sampling backend')\n    parser.add_argument('--model_name', type=str, required=True, default=\"DeepSeek-R1-Distill-Qwen-1.5B\", help='Model name or path')\n    parser.add_argument('--max_generated_tokens', type=int, default=32768, help='Limit the number of generated tokens')\n    parser.add_argument('--num_gpus', type=int, default=4, help='GPU number')\n    parser.add_argument('--num_samples', type=int, default=1, help='Sampling number')\n    parser.add_argument('--cuda_graph_max_bs', type=int, default=None, help='Max number of batch runned in one time.')\n    parser.add_argument('--max_running_requests', type=int, default=None, help='Max number of requests runned together.')\n    parser.add_argument('--max_batch', type=int, default=1000000, help='Max number of batch runned in one time.')\n    parser.add_argument('--mem_fraction_static', type=float, default=0.5, help='Max memory to use per gpu.')\n    parser.add_argument('--random_seed', type=int, default=0, help='Random seed')\n    parser.add_argument('--temperature', type=float, default=0.6, help='Sampling temperature')\n    parser.add_argument('--top_p', type=float, default=0.95, help='Top-p sampling probability')\n    parser.add_argument('--top_k', type=int, default=30, help='Top-k sampling probability')\n    parser.add_argument('--min_p', type=float, default=0.0, help='Min-p sampling probability')\n    parser.add_argument('--early_stopping_entropy_threshold', type=float, default=0.0, help='Early stopping entropy threshold')\n    parser.add_argument('--early_stopping_length_threshold', type=int, default=200, help='Early stopping length threshold')\n    parser.add_argument('--repetition_penalty', type=float, default=1.0, help='Repetition penalty')\n    parser.add_argument('--after_thinking_temperature', type=float, default=0.6, help='Temperature after thinking')\n    parser.add_argument('--after_thinking_top_p', type=float, default=0.95, help='Top-p after thinking')\n    parser.add_argument('--after_thinking_top_k', type=int, default=30, help='Top-k after thinking')\n    parser.add_argument('--after_thinking_min_p', type=float, default=0.0, help='Min-p after thinking')\n    parser.add_argument('--dirichlet_alpha', type=float, default=1.0e20, help='Dirichlet alpha')\n    parser.add_argument('--start_idx', type=int, default=0, help='Start index for processing samples')\n    parser.add_argument('--end_idx', type=int, default=500, help='End index for processing samples')\n    parser.add_argument('--output_dir', type=str, default=\"results\", help='Directory to save results')\n    parser.add_argument('--reeval', action='store_true', help='Enable re-evaluation')\n    parser.add_argument('--use_llm_judge', action='store_true', help='Enable LLM judge')\n    parser.add_argument('--api_base', type=str, default=None, help='')\n    parser.add_argument('--deployment_name', type=str, default=None, help='')\n    parser.add_argument('--api_version', type=str, default=None, help='')\n    parser.add_argument('--api_key', type=str, default=None, help='')\n\n    parser.add_argument('--push_results_to_hf', action='store_true', help='Enable push to huggingface')\n    parser.add_argument('--hf_token', type=str, default=None, help='')\n    parser.add_argument('--hf_repo_id', type=str, default=None, help='')\n    parser.add_argument(\n            \"--enable_soft_thinking\",\n            action=\"store_true\",\n            help=\"Enable soft thinking mode\"\n        )\n    parser.add_argument(\n            \"--think_end_str\",\n            type=str,\n            default=\"</think>\",\n        )\n    parser.add_argument(\n        \"--max_topk\",\n        type=int,\n        default=30,\n    )\n\n    args = parser.parse_args()\n\n    dataset = args.dataset\n    model_name = args.model_name\n    max_generated_tokens = args.max_generated_tokens\n    temperature = args.temperature\n    top_p = args.top_p\n    top_k = args.top_k\n    min_p = args.min_p\n    think_end_str = args.think_end_str\n    random_seed = args.random_seed\n    num_gpus = args.num_gpus\n    max_running_requests = args.max_running_requests\n    max_batch = args.max_batch\n    mem_fraction_static = args.mem_fraction_static\n    start_idx = args.start_idx\n    end_idx = args.end_idx\n    reeval = args.reeval\n\n\n    print(f\"Arguments: {args}\", flush=True)\n    \n    matheval.set_client(args.api_base, args.deployment_name, args.api_version, args.api_key)\n\n    if dataset == \"math500\":\n        with open(\"./datasets/math500.json\") as f:\n            samples = json.load(f)\n    elif dataset == \"aime2024\":\n        with open(\"./datasets/aime2024.json\") as f:\n            samples = json.load(f)\n    elif dataset == \"aime2025\":\n        with open(\"./datasets/aime2025.json\") as f:\n            samples = json.load(f)\n    elif dataset == \"gpqa_diamond\":\n        with open(\"./datasets/gpqa_diamond.json\") as f:\n            samples = json.load(f)\n    elif dataset == \"gsm8k\":\n        with open(\"./datasets/gsm8k.json\") as f:\n            samples = json.load(f)\n    elif dataset == \"amc23\":\n        with open(\"./datasets/amc23.json\") as f:\n            samples = json.load(f)\n    elif dataset == \"humaneval\":\n        with open(\"./datasets/humaneval.json\") as f:\n            samples = json.load(f)\n    elif dataset == \"mbpp\":\n        with open(\"./datasets/mbpp.json\") as f:\n            samples = json.load(f)\n    elif dataset == \"livecodebench\":\n        with open(\"./datasets/livecodebench.json\") as f:\n            samples = json.load(f)\n    else:\n        raise ValueError(\"Invalid dataset name\")\n\n    MATH_QUERY_TEMPLATE = \nPlease reason step by step, and put your final answer within \\\\boxed{{}}.\n\n{Question}\n.strip()\n\n    GPQA_QUERY_TEMPLATE = \nPlease solve the following multiple-choice question. Please show your choice in the answer field with only the choice letter, e.g.,\"answer\": \"C\".\n\n{Question}\n.strip()\n    \n    CODE_QUERY_TEMPLATE = \nPlease solve the programming task below in Python. Code should be wrapped in a markdown code block.\n\n```python\n{Question}\n```\n.strip()\n\n    MBPP_QUERY_TEMPLATE = \nPlease solve the programming task with test cases below in Python. Make sure your code satisfies the following requirements:\n1. The function name and signature must match exactly as specified in the test cases.\n2. Your code should be wrapped in a markdown code block without including any test cases.\n\nTask:\n{Question}\n\nTest Cases:\n```python\n{TestCases}\n```\n.strip()\n    def get_lcb_prompt(question_content, starter_code):\n        prompt = \"You will be given a question (problem specification) and will generate a correct Python program that matches the specification and passes all tests.\\n\\n\"\n        prompt += f\"Question: {question_content}\\n\\n\"\n        if starter_code:\n            prompt += f\"You will use the following starter code to write the solution to the problem and enclose your code within delimiters.\\n\"\n            prompt += f\"```python\\n{starter_code}\\n```\\n\\n\"\n        else:\n            prompt += f\"Read the inputs from stdin solve the problem and write the answer to stdout (do not directly test on the sample inputs). Enclose your code within delimiters as follows. Ensure that when the python program runs, it reads the inputs, runs the algorithm and writes output to STDOUT.\\n\"\n            prompt += f\"```python\\n\n        return prompt\n\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    sampling_params = {\"temperature\": temperature, \"top_p\": top_p, \"top_k\": top_k, \"min_p\": min_p, \"repetition_penalty\": args.repetition_penalty,\n                       \"after_thinking_temperature\": args.after_thinking_temperature, \"after_thinking_top_p\": args.after_thinking_top_p, \"after_thinking_top_k\": args.after_thinking_top_k, \"after_thinking_min_p\": args.after_thinking_min_p,\n                       \"dirichlet_alpha\": args.dirichlet_alpha,\n                       \"n\": 1,\n                       \"max_new_tokens\": max_generated_tokens, \"think_end_str\": think_end_str,\n                       \"early_stopping_entropy_threshold\": args.early_stopping_entropy_threshold,\n                       \"early_stopping_length_threshold\": args.early_stopping_length_threshold\n                    }\n\n    os.makedirs(f\"{args.output_dir}/results/{dataset}\", exist_ok=True)\n    results_file = f\"{args.output_dir}/results/{dataset}/{model_name.split('/')[-1]}_{dataset}_{args.enable_soft_thinking}_{args.num_samples}_{temperature}_{top_p}_{top_k}_{min_p}_{args.repetition_penalty}_{args.dirichlet_alpha}_{args.max_topk}_{max_generated_tokens}_{args.early_stopping_entropy_threshold}_{args.early_stopping_length_threshold}.json\"\n    results_statistics_file = f\"{args.output_dir}/results/{dataset}/{model_name.split('/')[-1]}_{dataset}_{args.enable_soft_thinking}_{args.num_samples}_{temperature}_{top_p}_{top_k}_{min_p}_{args.repetition_penalty}_{args.dirichlet_alpha}_{args.max_topk}_{max_generated_tokens}_{args.early_stopping_entropy_threshold}_{args.early_stopping_length_threshold}_statistics.json\"\n\n    results = []\n\n    print(\"begin\")\n    start_time = time.time()\n\n    if reeval:\n        with open(results_file, \"r\") as f:\n            results = json.load(f)\n        prompt_list = []\n        idx_list = list(range(start_idx, min(end_idx,len(results))))\n        decoded_text_list = []\n        finish_generation_list = []\n        generated_tokens_list = []\n        for r in results:\n            prompt_list.append(r[\"prompt\"])\n            decoded_text_list.extend(r[\"completion\"])\n            finish_generation_list.extend(r[\"finish_generation\"])\n            generated_tokens_list.extend(r[\"generated_tokens\"])\n        results = []\n\n    else:\n        prompt_list = []\n        idx_list = []\n        for idx in range(start_idx, min(end_idx,len(samples))):\n            sample = samples[idx]\n\n            if dataset in [\"aime2024\", \"aime2025\", \"math500\", \"gsm8k\", \"amc23\"]:\n                chat = [{\"role\": \"user\", \"content\": MATH_QUERY_TEMPLATE.format(Question=sample[\"prompt\"][0][\"value\"])}]\n            elif dataset == \"gpqa_diamond\":\n                chat = [{\"role\": \"user\", \"content\": GPQA_QUERY_TEMPLATE.format(Question=sample[\"prompt\"][0][\"value\"])}]\n            elif dataset == \"humaneval\":\n                chat = [{\"role\": \"user\", \"content\": CODE_QUERY_TEMPLATE.format(Question=sample[\"prompt\"][0][\"value\"])}]\n            elif dataset == \"mbpp\":\n                chat = [{\"role\": \"user\", \"content\": MBPP_QUERY_TEMPLATE.format(Question=sample[\"prompt\"][0][\"value\"], TestCases=\"\\n\".join(sample[\"final_answer\"][\"test_list\"]))}]\n            elif dataset == \"livecodebench\":\n                chat = [{\"role\": \"user\", \"content\": get_lcb_prompt(question_content=sample[\"prompt\"][0][\"value\"], starter_code=sample[\"final_answer\"][\"starter_code\"])}]\n            else:\n                raise ValueError(\"Invalid dataset name\")\n\n            prompt = tokenizer.apply_chat_template(chat, add_generation_prompt=True, tokenize=False)\n            for _ in range(args.num_samples):\n                prompt_list.append(prompt)\n\n            idx_list.append(idx)\n        decoded_text_list = []\n        finish_generation_list = []\n        generated_tokens_list = []\n        idx = 0\n        while idx < len(prompt_list):\n            print(f\"Number of GPUs available: {num_gpus}\", flush=True)\n            llm = sgl.Engine(model_path=model_name, tp_size=num_gpus, log_level=\"info\", trust_remote_code=True, random_seed=random_seed, max_running_requests=max_running_requests, mem_fraction_static=mem_fraction_static, disable_cuda_graph=True, disable_overlap_schedule=True, enable_soft_thinking=args.enable_soft_thinking, max_topk=args.max_topk, cuda_graph_max_bs=args.cuda_graph_max_bs, sampling_backend=args.sampling_backend)\n            outputs =  llm.generate(prompt_list[idx:idx+max_batch], sampling_params)\n            decoded_text_list.extend([o[\"text\"] for o in outputs])\n            finish_generation_list.extend([o[\"meta_info\"][\"finish_reason\"] != \"length\" for o in outputs])\n            generated_tokens_list.extend([o[\"meta_info\"][\"completion_tokens\"] for o in outputs])\n            idx += max_batch\n            outputs = None\n            llm.shutdown()\n\n            torch.cuda.empty_cache()\n\n    mbppeval.init_evaluator()\n    humanevaleval.init_evaluator()\n\n    for i,idx in enumerate(idx_list):\n        print(idx, flush=True)\n        sample = samples[idx]\n        judge_info = []\n        passat1_list = []\n        decoded_text = decoded_text_list[i*args.num_samples:(i+1)*args.num_samples]\n        finish_generation = finish_generation_list[i*args.num_samples:(i+1)*args.num_samples]\n        for j in range(args.num_samples):\n            for _ in range(5):\n                try:\n                    if dataset in MATH_DATASETS:\n                        rule_judge_result = None\n                        rule_judge_result, extracted_answer = matheval.evaluator_map[dataset].rule_judge(decoded_text[j],sample[\"final_answer\"], finish_generation[j])\n                        llm_judge_result = None\n                        if not rule_judge_result and args.use_llm_judge:\n                            llm_judge_result = matheval.evaluator_map[dataset].llm_judge(decoded_text[j],sample[\"final_answer\"],extracted_answer, finish_generation[j])\n                        finally_judge_result = rule_judge_result or llm_judge_result\n                        judge_info.append({\n                            \"rule_judge_result\": rule_judge_result,\n                            \"llm_judge_result\": llm_judge_result,\n                            \"finally_judge_result\": finally_judge_result\n                        })\n                        passat1_list.append(1.0 if finally_judge_result else 0.0)\n\n                    elif dataset in CODE_DATASETS:\n                        k = 1\n                        if dataset==\"humaneval\":\n                            if reeval:\n                                passat1, single_judge_info = humanevaleval.evaluator_map[dataset].judge(sample[\"prompt\"][0][\"value\"], decoded_text[j],  sample[\"final_answer\"], k)\n                            else:\n                                passat1, single_judge_info = 0.0, None\n                        elif dataset==\"mbpp\":\n                            if reeval:\n                                passat1, single_judge_info = mbppeval.evaluator_map[dataset].judge(sample[\"prompt\"][0][\"value\"], decoded_text[j],  sample[\"final_answer\"], k)\n                            else:\n                                passat1, single_judge_info = 0.0, None\n                        elif dataset==\"livecodebench\":\n                            if reeval:\n                                passat1, single_judge_info = 0.0, None\n                            else:\n                                passat1, single_judge_info = 0.0, None\n\n                        passat1_list.append(passat1)\n                        judge_info.append(single_judge_info)\n\n                    else:\n                        raise ValueError(\"Unknown dataset: {}\".format(dataset))\n\n                    break\n                except Exception as e:\n                    print(f\"Error: {e}\", flush=True)\n                    time.sleep(0.5)\n\n\n\n        result = {\n            \"hyperparams\": str(args),\n            \"prompt\": sample[\"prompt\"][0][\"value\"],\n            \"completion\": decoded_text,\n            \"ground_truth\": sample[\"final_answer\"],\n            \"generated_tokens\": generated_tokens_list[i*args.num_samples:(i+1)*args.num_samples],\n            \"avg_generated_tokens\": sum(generated_tokens_list[i*args.num_samples:(i+1)*args.num_samples])/args.num_samples,\n            \"time\": 0,\n            \"idx\": idx,\n            \"n\": args.num_samples,\n            \"finish_generation\": finish_generation_list[i*args.num_samples:(i+1)*args.num_samples],\n            \"judge_info\": judge_info,\n            \"passat1\": sum(passat1_list)/len(passat1_list),\n            \"passat1_list\": passat1_list\n        }\n        results.append(result)\n\n    with open(results_file, \"w\") as f:\n        results.sort(key=lambda x: x[\"idx\"])\n        json.dump(results, f, indent=4)\n    \n    if dataset == \"livecodebench\":\n        from convert_livecodebench import convert_json\n        results_file_converted = f\"{args.output_dir}/results/{dataset}/{model_name.split('/')[-1]}_{dataset}_{args.enable_soft_thinking}_{args.num_samples}_{temperature}_{top_p}_{top_k}_{min_p}_{args.repetition_penalty}_{args.dirichlet_alpha}_{args.max_topk}_{max_generated_tokens}_{args.early_stopping_entropy_threshold}_{args.early_stopping_length_threshold}_converted.json\"\n        convert_json(input_file=results_file, output_file=results_file_converted)\n        if reeval:\n            import subprocess\n            import sys\n\n            orig_cwd = os.getcwd()\n            lcb_pkg_dir = \"LiveCodeBench_pkg\"\n\n            custom_eval_cmd = [\n                sys.executable, \"-m\", \"lcb_runner.runner.custom_evaluator\",\n                \"--custom_output_file\", \"../\"+results_file_converted,\n                \"--release_version\", \"release_v5\",\n                \"--start_date\", \"2024-08-01\",\n            ]\n\n            print(\"Running custom_evaluator for LiveCodeBench reeval (cd to Livecodebench_pkg first)...\")\n            try:\n                os.chdir(lcb_pkg_dir)\n                subprocess.run(custom_eval_cmd, check=True, stdout=sys.stdout, stderr=sys.stderr)\n            except Exception as e:\n                print(f\"Error running custom_evaluator: {e}\", flush=True)\n            finally:\n                os.chdir(orig_cwd)\n            livecodebench_results_file = f\"{args.output_dir}/results/{dataset}/{model_name.split('/')[-1]}_{dataset}_{args.enable_soft_thinking}_{args.num_samples}_{temperature}_{top_p}_{top_k}_{min_p}_{args.repetition_penalty}_{args.dirichlet_alpha}_{args.max_topk}_{max_generated_tokens}_{args.early_stopping_entropy_threshold}_{args.early_stopping_length_threshold}_converted_codegeneration_output_eval_all.json\"\n            with open(livecodebench_results_file, \"r\") as f:\n                livecodebench_results = json.load(f)\n\n            for r in results:\n                for lcb_r in livecodebench_results:\n                    if r[\"ground_truth\"][\"question_id\"] == lcb_r[\"question_id\"]:\n                        r[\"passat1\"] = lcb_r[\"pass@1\"]\n                        r[\"passat1_list\"] = [int(passat1) for passat1 in lcb_r[\"graded_list\"]]\n                        r[\"judge_info\"] = lcb_r[\"metadata\"]\n                        break\n            with open(results_file, \"w\") as f:\n                results.sort(key=lambda x: x[\"idx\"])\n                json.dump(results, f, indent=4)\n\n\n\n    total_num = len(results)\n    pass_at_1 = sum([r[\"passat1\"] for r in results]) / total_num if total_num > 0 else 0\n\n\n\n    end_time = time.time()\n    print(\"end\", flush=True)\n    print(f\"Time taken: {(end_time - start_time)/3600} hours\", flush=True)\n\n\n    results_statistics = {\n        \"total_num\": total_num,\n        \"pass@1\": pass_at_1,\n        \"avg_token_length-all\": sum([r[\"avg_generated_tokens\"] for r in results]) / total_num if total_num > 0 else 0,\n        \"avg_token_length-correct\": sum([r[\"avg_generated_tokens\"] for r in results if r[\"passat1\"] > 0]) / len([r[\"passat1\"] for r in results if r[\"passat1\"] > 0]) if len([r[\"passat1\"] for r in results if r[\"passat1\"] > 0]) > 0 else 0,\n        \"time_taken/h\": (end_time - start_time)/3600\n    }\n\n    all_idx = sorted([(r[\"idx\"], r[\"passat1\"]) for r in results], key=lambda x: x[0])\n    results_statistics[\"all_idx\"] = {i:j for i,j in all_idx}\n\n    with open(results_statistics_file, \"w\") as f:\n        json.dump(results_statistics, f, indent=4)\n\n    if args.push_results_to_hf:\n        api = HfApi()\n        api.upload_file(\n            path_or_fileobj=results_statistics_file,\n            path_in_repo=results_statistics_file,\n            repo_id=args.hf_repo_id,\n            token=args.hf_token\n        )\n    print(results_statistics, flush=True)\n    \n\nif __name__ == \"__main__\":\n    main()\n`",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-08T06:32:19+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7845/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7845"
  },
  {
    "number": 7199,
    "title": "[Bug] [file name too long] base64 end up with \"JPG\" or others will throw exception",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI always use base64 as my input, but it will throw exception **[file name too long]** bug sometime, we found that's beacuse some picture's base64 end up with \"JPG\" or other image format, it will regard the base64 string as filename, so exception thrown.\n\nbug file is: python3.11/site-packages/sglang/srt/utils.py\nsee bug log here:\n![Image](https://github.com/user-attachments/assets/91e4ed93-2768-4f2c-9566-7179bc07abc7)\n![Image](https://github.com/user-attachments/assets/366323d2-4dd4-4c57-8c56-b55188131a30)\n![Image](https://github.com/user-attachments/assets/ca74bb2e-bb12-4eba-b90b-ac4471498506)\n\n### Reproduction\n\nalways\n\n### Environment\n\nsglang-0.4.6",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-15T03:18:11+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7199/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7199"
  },
  {
    "number": 6056,
    "title": "[Bug] DeepEP Failed with \"too many blocks in cooperative launch\"",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWe deploy DeepSeek-R1 in 1 node with H20*8, it hits an exception when capture cuda graph\n\n<pre>\n[2025-05-06 12:35:08 TP0] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 276, in __init__\n    self.capture()\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 360, in capture\n    ) = self.capture_one_batch_size(bs, forward)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 452, in capture_one_batch_size\n    run_once()\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 445, in run_once\n    logits_output = forward(input_ids, forward_batch.positions, forward_batch)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1499, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1419, in forward\n    hidden_states, residual = layer(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1202, in forward\n    return self.forward_ffn_with_scattered_input(\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1345, in forward_ffn_with_scattered_input\n    hidden_states = self.mlp(hidden_states, forward_batch.forward_mode)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 306, in forward\n    return self.forward_deepep(hidden_states, forward_mode)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 362, in forward_deepep\n    ) = self.deepep_dispatcher.dispatch(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\", line 574, in dispatch\n    self.dispatch_a(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\", line 584, in dispatch_a\n    inner_state = self._get_impl(forward_mode).dispatch_a(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\", line 397, in dispatch_a\n    hidden_states, masked_m, event, hook = self._dispatch_core(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\", line 476, in _dispatch_core\n    buffer.low_latency_dispatch(\n  File \"/usr/local/lib/python3.10/dist-packages/deep_ep-1.0.0+a84a248-py3.10-linux-x86_64.egg/deep_ep/buffer.py\", line 488, in low_latency_dispatch\n    self.runtime.low_latency_dispatch(x, topk_idx,\nRuntimeError: Failed: CUDA error /media/ssd1/DeepEP/csrc/kernels/internode_ll.cu:341 'too many blocks in cooperative launch\n\n\n### Reproduction\n\nDeploy Command\n`python3 -m sglang.launch_server --model-path /media/ssd1/ds-r1 --tp 8 --enable-deepep-moe --deepep-mode auto --trust-remote-code --host 0.0.0.0 --port 30030 --enable-torch-compile --attention-backend flashinfer --mem-fraction-static 0.8 --disable-radix-cache --cuda-graph-max-bs 128`\n\n\n\n\n### Environment\n\nNCCL\n`NCCL_DEBUG=INFO NCCL_IB_TC=160 NCCL_SOCKET_IFNAME=eth0 GLOO_SOCKET_IFNAME=eth0 NCCL_NET_GDR_LEVEL=2 NCCL_IB_GID_INDEX=3 NCCL_WORK_FIFO_DEPTH=4194304 NCCL_IB_QPS_PER_CONNECTION=4 NCCL_IB_TIMEOUT=22 NCCL_IB_DISABLE=0`\n\nDeepEP\n`export NVSHMEM_DIR=/media/ssd1/nvshmem  \nexport LD_LIBRARY_PATH=${NVSHMEM_DIR}/lib:$LD_LIBRARY_PATH \nexport PATH=${NVSHMEM_DIR}/bin:$PATH`\n\npython env\n<pre>\n$python3 -m sglang.check_env\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]  \nCUDA available: True  \nGPU 0,1,2,3,4,5,6,7: NVIDIA H20-3e  \nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0  \nCUDA_HOME: /usr/local/cuda  \nNVCC: Cuda compilation tools, release 12.4, V12.4.131  \nCUDA Driver Version: 550.144.03  \nPyTorch: 2.6.0+cu124  \nsglang: 0.4.6.post1  \nsgl_kernel: 0.1.0  \nflashinfer: Module Not Found  \ntriton: 3.2.0  \ntransformers: 4.51.1  \ntorchao: 0.10.0  \nnumpy: 1.26.4  \naiohttp: 3.11.16  \nfastapi: 0.115.12  \nhf_transfer: 0.1.9  \nhuggingface_hub: 0.30.2  \ninteregular: 0.3.3  \nmodelscope: 1.25.0  \norjson: 3.10.16  \noutlines: 0.1.11  \npackaging: 24.2  \npsutil: 7.0.0  \npydantic: 2.11.3  \nmultipart: Module Not Found  \nzmq: Module Not Found  \nuvicorn: 0.34.1  \nuvloop: 0.21.0  \nvllm: 0.7.2  \nxgrammar: 0.1.17  \nopenai: 1.74.0\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.66.1\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-51,104-155    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-51,104-155    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     0-51,104-155    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     0-51,104-155    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    52-103,156-207  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    52-103,156-207  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    52-103,156-207  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     52-103,156-207  1               N/A\nNIC0    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC1    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC2    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    NODE    SYS     SYS     SYS     SYS\nNIC3    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      NODE    SYS     SYS     SYS     SYS\nNIC4    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE     X      SYS     SYS     SYS     SYS\nNIC5    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS      X      NODE    NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS     NODE     X      NODE    NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      NODE\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n\n\nulimit soft: 1048576\n",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-06T12:51:59+00:00",
    "closed_at": "2025-05-06T13:30:55+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6056/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6056"
  },
  {
    "number": 1016,
    "title": "[Question] Gemma-2 sliding window support?",
    "body": "Thanks for your hard work on this project! I'm trying to use gemma-2-27b-it with sglang==0.2.9 and flashinfer backend ==0.1.3. I\u2019m interested in using a context length of 8192 with sliding window attention, which seems to be supported by flashinfer.\r\n\r\nHowever, I noticed that the comments in the gemma2 repository suggest that this might not be allowed. Could you clarify whether it's possible to use an 8192 context length with sliding window attention in this setup?\r\n\r\nAny guidance or suggestions would be greatly appreciated!",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-10T09:29:34+00:00",
    "closed_at": "2024-08-10T09:32:27+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1016/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1016"
  },
  {
    "number": 1259,
    "title": "[Bug] cannot set --load-format=dummy with vllm 0.5.5",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nIn vllm 0.5.5,`MultiModalConfig` has been refactored into a attribute of `ModelConfig` in this [PR](https://github.com/vllm-project/vllm/pull/7530), and is no longer an argument in [DummyModelLoader.load_model](https://github.com/vllm-project/vllm/blob/09c7792610ada9f88bbf87d32b472dd44bf23cc2/vllm/model_executor/model_loader/loader.py#L374). But in SGLang, the implementation of `monkey_patch_vllm_dummy_weight_loader` still pass a `multimodal_config` to the method, which causes error.\n\n### Reproduction\n\n`python3 -m sglang.launch_server --model-path=meta-llama/Meta-Llama-3.1-8B-Instruct --load-format=dummy  --disable-radix-cach`\n\n### Environment\n\n```Python: 3.9.2 (default, Feb 28 2021, 17:03:44) [GCC 10.2.1 20210110]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.2, V12.2.140\r\nCUDA Driver Version: 535.129.03\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.2.14.post2\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.44.2\r\nrequests: 2.32.3\r\ntqdm: 4.65.0\r\nnumpy: 1.24.4\r\naiohttp: 3.9.5\r\nfastapi: 0.112.2\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.23.2\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 9.3.0\r\npsutil: 5.9.5\r\npydantic: 2.8.2\r\nuvicorn: 0.30.6\r\nuvloop: 0.20.0\r\nzmq: 26.2.0\r\nvllm: 0.5.5\r\nmultipart: 0.0.9\r\nopenai: 1.42.0\r\nanthropic: 0.34.1\r\nNVIDIA Topology: \r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    0-51,104-155    0               N/A\r\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    0-51,104-155    0               N/A\r\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    0-51,104-155    0               N/A\r\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    0-51,104-155    0               N/A\r\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    52-103,156-207  1               N/A\r\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    52-103,156-207  1               N/A\r\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    52-103,156-207  1               N/A\r\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      52-103,156-207  1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nHypervisor vendor: KVM\r\nulimit soft: 1024768```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-29T23:39:25+00:00",
    "closed_at": "2024-08-30T06:43:42+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1259/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1259"
  },
  {
    "number": 6798,
    "title": "enable_torch_compile has no effect on Qwen3-32B-FP8 with H20 GPU",
    "body": "I tested the enable_torch_compile=True configuration on the Qwen3-32B-FP8 model using an H200 GPU, and observed that it had no positive effect on performance. In fact, in some metrics it slightly degraded performance.\n\nHere are the benchmark results for comparison:\n\nWithout enable_torch_compile:\n============ Serving Benchmark Result ============\nSuccessful requests:                     4596      \nBenchmark duration (s):                  397.14    \nTotal input tokens:                      9577351   \nTotal generated tokens:                  514870    \nRequest throughput (req/s):              11.57     \nOutput token throughput (tok/s):         1296.45   \nTotal Token throughput (tok/s):          25412.30  \n---------------Time to First Token----------------\nMean TTFT (ms):                          836.69    \nMedian TTFT (ms):                        710.76    \nP99 TTFT (ms):                           2590.59   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          148.57    \nMedian TPOT (ms):                        115.09    \nP99 TPOT (ms):                           500.20    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           135.84    \nMedian ITL (ms):                         33.22     \nP99 ITL (ms):                            2131.86   \n----------------End-to-end Latency----------------\nMean E2EL (ms):                          16189.11  \nMedian E2EL (ms):                        12851.73  \nP99 E2EL (ms):                           55603.54  \n==================================================\n\nWith enable_torch_compile=True:\n============ Serving Benchmark Result ============\nSuccessful requests:                     4596      \nBenchmark duration (s):                  402.36    \nTotal input tokens:                      9577351   \nTotal generated tokens:                  514679    \nRequest throughput (req/s):              11.42     \nOutput token throughput (tok/s):         1279.14   \nTotal Token throughput (tok/s):          25081.87  \n---------------Time to First Token----------------\nMean TTFT (ms):                          871.89    \nMedian TTFT (ms):                        691.31    \nP99 TTFT (ms):                           3212.81   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          155.84    \nMedian TPOT (ms):                        104.07    \nP99 TPOT (ms):                           802.05    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           139.78    \nMedian ITL (ms):                         30.55     \nP99 ITL (ms):                            2004.19   \n----------------End-to-end Latency----------------\nMean E2EL (ms):                          16662.66  \nMedian E2EL (ms):                        11719.74  \nP99 E2EL (ms):                           78037.33  \n==================================================\nEnvironment:\n\nModel: Qwen/Qwen1.5-32B in FP8 precision\n\nGPU: NVIDIA H20\n\ntorch: 2.6.0\nsglang: v0.4.6.post4\n\nObservation:\nThere is no observable benefit when enabling torch.compile. In fact, the mean and P99 latencies for several metrics (TTFT, TPOT, E2EL) slightly increased.\n\nQuestion:\nIs torch.compile expected to be effective for this model configuration (Qwen3-32B-FP8 + H20)? Are there any recommended settings or limitations we should be aware of when using torch.compile for large FP8 models?\n\nThanks!",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-01T15:33:25+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6798/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6798"
  },
  {
    "number": 1515,
    "title": "How to study the code?",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nHi. I am a student, and I am recently studying the code of SGL. I have read your great paper but I need some more concrete feelings of the system. Could you please tell me some resources or suggesttions on understand the code of SGL? Thanks. \n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2024-09-26T04:35:15+00:00",
    "closed_at": "2024-09-26T04:35:53+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1515/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1515"
  },
  {
    "number": 1395,
    "title": "[Feature] need DeepSeek-v2 or deepseek-v2.5 awq support",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nsglang 0.30 can run DeepSeek-Coder-V2-Lite-Instruct.\r\nbut running DeepSeek-Coder-V2-Lite-Instruct-AWQ reports:\r\nValueError: The input size is not aligned with the quantized weight shape. This can be caused by too large tensor parallel size.\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2024-09-12T02:28:46+00:00",
    "closed_at": "2024-09-12T02:35:36+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1395/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1395"
  },
  {
    "number": 7934,
    "title": "[Bug] DeepSeek-V3 function call return stop instead of tool_calls in streaming request",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen utilizing the DeepSeek-V3 function call in streaming mode, the finish reason for the last chunk ought to be tool calls, but currently, it shows as stop.\n\nI think the reason is in servering_chat.py, whether to change stop to tool calls is determined by whether the current parse result contains any tool. If the last chunk is EOS, the parser won\u2019t extract any tool, so it won\u2019t set the last chunk\u2019s finish reason to tool calls.\n\n### Reproduction\n\nlaunch server\n```\npython -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3-0324/ --tp 8 --trust-remote-code --tool-call-parser deepseekv3 --chat-template ./examples/chat_template/tool_chat_template_deepseekv3.jinja\n```\n\ntest\n```\ndef streaming_tool_call_with_client(client: OpenAI, model_name: str):\n    messages = [\n        {\"role\": \"user\", \"content\": \"\u5317\u4eac\u4eca\u5929\u7684\u5929\u6c14\u600e\u4e48\u6837?\u8c03\u7528\u5de5\u5177\u770b\u4e00\u4e0b\"}\n    ]\n    finish_reason = None\n    msg = ''\n    while finish_reason is None or finish_reason == \"tool_calls\":\n        completion = client.chat.completions.create(\n            model=model_name,\n            messages=messages,\n            temperature=0.3,\n            tools=tools,\n            tool_choice=\"auto\",\n            stream=True \n        )\n        tool_calls = []\n        for chunk in completion:\n            delta = chunk.choices[0].delta\n            if delta.content:\n                msg += delta.content\n            print(chunk)\n            if delta.tool_calls:\n                for tool_call_chunk in delta.tool_calls:\n                    if tool_call_chunk.index is not None:\n                        \n                        while len(tool_calls) <= tool_call_chunk.index:\n                            tool_calls.append({\n                                \"id\": \"\",\n                                \"type\": \"function\",\n                                \"function\": {\n                                    \"name\": \"\",\n                                    \"arguments\": \"\"\n                                }\n                            })\n\n                        tc = tool_calls[tool_call_chunk.index]\n\n                        if tool_call_chunk.id:\n                            tc[\"id\"] += tool_call_chunk.id\n                        if tool_call_chunk.function.name:\n                            tc[\"function\"][\"name\"] += tool_call_chunk.function.name\n                        if tool_call_chunk.function.arguments:\n                            tc[\"function\"][\"arguments\"] += tool_call_chunk.function.arguments\n\n            finish_reason = chunk.choices[0].finish_reason\n            print(finish_reason)\n        if finish_reason == \"tool_calls\":\n            for tool_call in tool_calls:\n                tool_call_name = tool_call['function']['name']\n                tool_call_arguments = json.loads(tool_call['function']['arguments'])\n                tool_function = tool_map[tool_call_name] \n                tool_result = tool_function(tool_call_arguments)\n                print(tool_result)\n                messages.append({\n                    \"role\": \"tool\",\n                    \"tool_call_id\": tool_call['id'],\n                    \"name\": tool_call_name,\n                    \"content\": json.dumps(tool_result),\n                })\n            \n            msg = ''\n\n    print(msg)\n```\n\n\n\n### Environment\n\nPython: 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H200\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.8, V12.8.93\nCUDA Driver Version: 550.163.01\nPyTorch: 2.7.1+cu126\nsglang: 0.4.9.post1\nsgl_kernel: 0.2.4\nflashinfer_python: 0.2.7.post1\ntriton: 3.3.1\ntransformers: 4.53.0\ntorchao: 0.9.0\nnumpy: 2.2.6\naiohttp: 3.12.13\nfastapi: 0.115.14\nhf_transfer: 0.1.9\nhuggingface_hub: 0.33.2\ninteregular: 0.3.3\nmodelscope: 1.27.1\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.7\npython-multipart: 0.0.20\npyzmq: 27.0.0\nuvicorn: 0.35.0\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.20\nopenai: 1.93.0\ntiktoken: 0.9.0\nanthropic: 0.56.0\nlitellm: 1.73.6\ndecord: 0.6.0\nNVIDIA Topology:\n\tGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tNIC4\tNIC5\tNIC6\tNIC7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tPIXPHB\tPHB\tPHB\t0-63\t0\t\tN/A\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tPHBPIX\tPHB\tPHB\t0-63\t0\t\tN/A\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tPHBPHB\tPIX\tPHB\t0-63\t0\t\tN/A\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tPHBPHB\tPHB\tPIX\t0-63\t0\t\tN/A\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tPIX\tPHB\tPHB\tPHB\tSYSSYS\tSYS\tSYS\t64-127\t1\t\tN/A\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tPHB\tPIX\tPHB\tPHB\tSYSSYS\tSYS\tSYS\t64-127\t1\t\tN/A\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tPHB\tPHB\tPIX\tPHB\tSYSSYS\tSYS\tSYS\t64-127\t1\t\tN/A\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tPHB\tPHB\tPHB\tPIX\tSYSSYS\tSYS\tSYS\t64-127\t1\t\tN/A\nNIC0\tSYS\tSYS\tSYS\tSYS\tPIX\tPHB\tPHB\tPHB\t X \tPHB\tPHB\tPHB\tSYSSYS\tSYS\tSYS\nNIC1\tSYS\tSYS\tSYS\tSYS\tPHB\tPIX\tPHB\tPHB\tPHB\t X \tPHB\tPHB\tSYSSYS\tSYS\tSYS\nNIC2\tSYS\tSYS\tSYS\tSYS\tPHB\tPHB\tPIX\tPHB\tPHB\tPHB\t X \tPHB\tSYSSYS\tSYS\tSYS\nNIC3\tSYS\tSYS\tSYS\tSYS\tPHB\tPHB\tPHB\tPIX\tPHB\tPHB\tPHB\t X \tSYSSYS\tSYS\tSYS\nNIC4\tPIX\tPHB\tPHB\tPHB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X PHB\tPHB\tPHB\nNIC5\tPHB\tPIX\tPHB\tPHB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPHB X \tPHB\tPHB\nNIC6\tPHB\tPHB\tPIX\tPHB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPHBPHB\t X \tPHB\nNIC7\tPHB\tPHB\tPHB\tPIX\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPHBPHB\tPHB\t X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n\n\nHypervisor vendor: KVM\nulimit soft: 1048576",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-10T18:28:23+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7934/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7934"
  },
  {
    "number": 6864,
    "title": "PD Disaggregation with tp stuck",
    "body": "When I execute \n`python3 -m sglang.launch_server --model-path models/DeepSeek-R1-Distill-Qwen-32B --disaggregation-mode prefill --disaggregation-ib-device mlx5_0,mlx5_1 --tp=2` \nthe process stuck with logs as follows:\n\nserver_args=ServerArgs(...)\n[2025-06-04 07:49:51 TP0] Attention backend not set. Use flashinfer backend by default.\n[2025-06-04 07:49:51 TP0] Init torch distributed begin.\n[2025-06-04 07:49:51 TP0] sglang is using nccl==2.21.5\n\nIt seems that the TP1 process hasn't started, so it is stuck. I want to know how to solve it.",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-04T08:33:50+00:00",
    "closed_at": "2025-06-04T09:43:30+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6864/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6864"
  },
  {
    "number": 1072,
    "title": "[Feature] Question about kvcache offloading to disk or CPU memory",
    "body": "### Motivation\n\nIn sglang's paper, it mentions that sglang plans to implement a kvcache offloading mechanism. I notice that there is a `disable_disk_cache=False` option in sglang's Server args, and wonder if this option controls the kvcache offloading mechanism.\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-13T09:51:24+00:00",
    "closed_at": "2024-08-13T09:54:17+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1072/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1072"
  },
  {
    "number": 2482,
    "title": "[Bug] Accuracy is abnormal when EP MoE is enabled",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [X] 2. The bug has not been fixed in the latest version.\r\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [X] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nAccuracy on gsm8k dataset is decreased for EP MoE. \r\ncc: @xiaobochen123\r\n\r\n### Reproduction\r\n\r\nEP:\r\n```bash\r\npython3 -m sglang.launch_server --model-path neuralmagic/DeepSeek-Coder-V2-Instruct-FP8 --disable-radix-cache --trust-remote-code --tp 8 --enable-ep-moe --disable-cuda-graph\r\npython3 benchmark/gsm8k/bench_sglang.py --num-questions 1400 --parallel 1400\r\n```\r\n```\r\nAccuracy: 0.540\r\nInvalid: 0.005\r\nLatency: 205.758 s\r\nOutput throughput: 1017.681 token/s\r\n```\r\nTP:\r\n```bash\r\npython3 -m sglang.launch_server --model-path neuralmagic/DeepSeek-Coder-V2-Instruct-FP8 --disable-radix-cache --trust-remote-code --tp 8 --disable-cuda-graph\r\npython3 benchmark/gsm8k/bench_sglang.py --num-questions 1400 --parallel 1400\r\n```\r\n```\r\nAccuracy: 0.930\r\nInvalid: 0.000\r\nLatency: 196.344 s\r\nOutput throughput: 1011.191 token/s\r\n```\r\n\r\n### Environment\r\n\r\n- sglang: main branch (0.4.0.post1)\r\n- torch: 2.5.1\r\n- triton: 3.1.0",
    "labels": [],
    "state": "closed",
    "created_at": "2024-12-14T14:13:15+00:00",
    "closed_at": "2024-12-16T13:11:35+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2482/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2482"
  },
  {
    "number": 7592,
    "title": "[Bug] How to run the load balancer",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHow to effectively start the load balancer, and how to test whether it is running normally after the startup is completed \u3002 After the load balancer is deployed, the test call interface is available, but the model does not run.\n\n### Reproduction\n\nsudo docker run -d --gpus all \\\n    --shm-size 512g \\\n    --network host \\\n    --privileged \\\n    -v /dev/infiniband:/dev/infiniband -e NCCL_IB_HCA=mlx5 \\\n    --env \"GLOO_SOCKET_IFNAME=bond0\" \\\n    --env \"NCCL_SOCKET_IFNAME=bond0\" \\\n    --env \"NCCL_DEBUG=INFO\" \\\n    --env \"MC_TE_METRIC=true\"  \\\n    --env \"SGLANG_TBO_DEBUG=1\"  \\\n    --env \"SGL_ENABLE_JIT_DEEPGEMM=1\" \\\n    --env \"SGLANG_NUM_RESERVED_DECODE_TOKENS=102\" \\\n    -v /mnt/share/deepseek-ai:/deepseek \\\n    --name sglang_balancer \\\n    --ipc=host \\\n    lmsysorg/sglang:v0.4.7-cu124-dep  \\\n    python3 -m sglang.srt.disaggregation.mini_lb \\\n    --host 0.0.0.0 \\\n    --port 40000 \\ \n    --prefill \"http://10.88.0.101:30000\" \\ \n    --decode \"http://10.88.0.105:30000\" \n\n### Environment\n\nsudo docker run -d --gpus all \\\n    --shm-size 512g \\\n    --network host \\\n    --privileged \\\n    -v /dev/infiniband:/dev/infiniband -e NCCL_IB_HCA=mlx5 \\\n    --env \"GLOO_SOCKET_IFNAME=bond0\" \\\n    --env \"NCCL_SOCKET_IFNAME=bond0\" \\\n    --env \"NCCL_DEBUG=INFO\" \\\n    --env \"MC_TE_METRIC=true\"  \\\n    --env \"SGLANG_TBO_DEBUG=1\"  \\\n    --env \"SGL_ENABLE_JIT_DEEPGEMM=1\" \\\n    --env \"SGLANG_NUM_RESERVED_DECODE_TOKENS=102\" \\\n    -v /mnt/share/deepseek-ai:/deepseek \\\n    --name sglang_balancer \\\n    --ipc=host \\\n    lmsysorg/sglang:v0.4.7-cu124-dep  \\\n    python3 -m sglang.srt.disaggregation.mini_lb \\\n    --host 0.0.0.0 \\\n    --port 40000 \\ \n    --prefill \"http://10.88.0.101:30000\" \\ \n    --decode \"http://10.88.0.105:30000\" ",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-27T07:56:26+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7592/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7592"
  },
  {
    "number": 3537,
    "title": "question of enable-ep-moe",
    "body": "Hi, team. When using --enable-ep-moe to start, are the expert weights on each card still loaded in a column-parallel manner? Does this have an impact on performance? I mean will a single expert still triggers all gather communication itself even if it belongs to a single card?",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-13T03:30:31+00:00",
    "closed_at": "2025-02-13T05:06:00+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3537/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3537"
  },
  {
    "number": 7332,
    "title": "[RFC] Bi-weekly release",
    "body": "After thorough internal discussions, the SGLang team has decided to standardize the release cycle as follows:\n\n- A new version will be released every two weeks under normal circumstances (e.g., v0.4.8, v0.4.9).\n\n- If urgent issues or high-priority features arise between regular releases, we may publish a patch release or an additional stable version as needed.\n\n- Bi-weekly releases will typically occur around the middle and end of each month.\n\n- Each release will aim to include a set of planned features, usually discussed and finalized by the SGLang team in advance.\n\n",
    "labels": [
      "high priority",
      "collaboration"
    ],
    "state": "open",
    "created_at": "2025-06-18T23:17:05+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7332/reactions",
      "total_count": 16,
      "+1": 11,
      "-1": 0,
      "laugh": 0,
      "hooray": 5,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/7332"
  }
]
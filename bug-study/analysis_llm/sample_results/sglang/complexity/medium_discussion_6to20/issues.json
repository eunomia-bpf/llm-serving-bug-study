[
  {
    "number": 523,
    "title": "The `choices` normalised logprobs calculation returns poor results due to bias for longer-token options",
    "body": "## Problem\r\nI've noticed that the `gen(choices=[...])` functionality sometimes performs poorly, even for simple tasks. This is due to a flawed normalised logprobs calculation. The calculation biases options that comprise more tokens, where the latter tokens are highly predictable given the prior tokens.\r\n\r\n## Reproducible Example\r\nThis is most easily seen in choices with token overlap, so I've constructed a contrived example that illustrates this. The outputs are generated with [llama 3 8B instruct](https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF), which should breeze through this task under normal circumstances.\r\n```python\r\nimport sglang as sgl\r\nimport textwrap\r\n\r\n# Define answer choices with overlapping substrings and tokenised forms\r\n# assumes llama 3 8B tokeniser\r\nchoices_and_tokenised_forms = [\r\n    (\"organ\", [\"organ\"]),\r\n    (\"organism\", [\"organ\", \"ism\"]),\r\n    (\"organisation\", [\"organisation\"]),\r\n    (\"organelle\", [\"org\", \"ane\", \"lle\"]),\r\n    (\"organometallic\", [\"organ\", \"omet\", \"al\", \"lic\"]),\r\n]\r\nchoices = [c for c, _ in choices_and_tokenised_forms]\r\n\r\n\r\n# Define the categorisation question\r\ntemplate = \"What category does '{input}' belong to? {choices}\"\r\n\r\n# Generate the (optional) system prompt with few-shot examples\r\nsys_prompt = \"\"\r\nfor example in [\r\n    (\"ribosome\", \"organelle\"),\r\n    (\"liver\", \"organ\"),\r\n    (\"Google\", \"organisation\"),\r\n    (\"ferrocene\", \"organometallic\"),\r\n    (\"human\", \"organism\"),\r\n]:\r\n    sys_prompt += \"user:\" + template.format(input=example[0], choices=choices)\r\n    sys_prompt += f\"\\nassisant:{example[1]}\\n\\n\"\r\n\r\n\r\n@sgl.function\r\ndef run(s, input: str, show_few_shot_examples: bool = False):\r\n    if show_few_shot_examples:\r\n        s += sgl.system(f\"You categorise things.\\n\\n ##Examples\\n{sys_prompt}\")\r\n    s += sgl.user(template.format(input=input, choices=choices, temperature=0))\r\n    s += sgl.assistant(sgl.gen(\"answer\", choices=choices))\r\n\r\n\r\ndef format_results(state, input):\r\n    answer = f\"  '{input}' categorised as: '{state['answer']}'\"\r\n    meta = state.get_meta_info(\"answer\")\r\n    out = f\"{answer:<50}    {'normalised'}    {'prefill token logprobs'}\"\r\n    for i in range(len(meta['normalized_prompt_logprobs'])):\r\n        option = f\"{choices_and_tokenised_forms[i][0]} ({choices_and_tokenised_forms[i][1]})\"\r\n        npl = meta['normalized_prompt_logprobs'][i]\r\n        ptl = [f\"{p[0]:.4f}\" for p in meta['prefill_token_logprobs'][i]]\r\n        out += f\"\\n{option:<50} -> {npl:<10.4f} -> {ptl}\"\r\n    return out\r\n\r\n\r\nsgl.set_default_backend(sgl.RuntimeEndpoint(\"http://localhost:30000\"))\r\n\r\nfor include_examples in [False, True]:\r\n    print(f\"Show few-shot examples in context = {include_examples}\\n\")\r\n    for input in [\"heart\", \"nucleus\", \"Microsoft\", \"mouse\", \"trimethylboron\"]:\r\n        state = run(input, show_few_shot_examples=include_examples)\r\n        print(textwrap.indent(format_results(state, input), \"    \"))\r\n        print()\r\n    print(\"-\" * 120)\r\n```\r\nOutputs:\r\n```\r\nShow few-shot examples in context = False\r\n\r\n      'heart' categorised as: 'organelle'                 normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -1.6190    -> ['-0.1265', '-3.1116']\r\n    organism (['organ', 'ism'])                        -> -1.7443    -> ['-0.1265', '-3.1116', '-1.9949']\r\n    organisation (['organisation'])                    -> -3.8885    -> ['-0.1265', '-7.6506']\r\n    organelle (['org', 'ane', 'lle'])                  -> -1.3777    -> ['-0.1265', '-5.3772', '-0.0048', '-0.0023']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -1.3915    -> ['-0.1265', '-3.1116', '-3.7136', '-0.0034', '-0.0023']\r\n\r\n      'nucleus' categorised as: 'organometallic'          normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -1.8324    -> ['-0.2145', '-3.4502']\r\n    organism (['organ', 'ism'])                        -> -1.8675    -> ['-0.2145', '-3.4502', '-1.9378']\r\n    organisation (['organisation'])                    -> -3.1800    -> ['-0.2145', '-6.1456']\r\n    organelle (['org', 'ane', 'lle'])                  -> -1.1103    -> ['-0.2145', '-4.2237', '-0.0013', '-0.0017']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -1.0997    -> ['-0.2145', '-3.4502', '-1.8284', '-0.0029', '-0.0022']\r\n\r\n      'Microsoft' categorised as: 'organometallic'        normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -1.5901    -> ['-0.1446', '-3.0355']\r\n    organism (['organ', 'ism'])                        -> -1.6397    -> ['-0.1446', '-3.0355', '-1.7391']\r\n    organisation (['organisation'])                    -> -2.9416    -> ['-0.1446', '-5.7387']\r\n    organelle (['org', 'ane', 'lle'])                  -> -1.4376    -> ['-0.1446', '-5.5746', '-0.0283', '-0.0029']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -1.1792    -> ['-0.1446', '-3.0355', '-2.7079', '-0.0052', '-0.0028']\r\n\r\n      'mouse' categorised as: 'organelle'                 normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -1.7110    -> ['-0.1392', '-3.2829']\r\n    organism (['organ', 'ism'])                        -> -1.5566    -> ['-0.1392', '-3.2829', '-1.2477']\r\n    organisation (['organisation'])                    -> -3.9181    -> ['-0.1392', '-7.6969']\r\n    organelle (['org', 'ane', 'lle'])                  -> -1.3491    -> ['-0.1392', '-5.2516', '-0.0041', '-0.0015']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -1.4992    -> ['-0.1392', '-3.2829', '-4.0680', '-0.0033', '-0.0028']\r\n\r\n      'trimethylboron' categorised as: 'organometallic'    normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -1.4093    -> ['-0.1379', '-2.6806']\r\n    organism (['organ', 'ism'])                        -> -2.7661    -> ['-0.1379', '-2.6806', '-5.4796']\r\n    organisation (['organisation'])                    -> -3.9659    -> ['-0.1379', '-7.7939']\r\n    organelle (['org', 'ane', 'lle'])                  -> -1.3317    -> ['-0.1379', '-5.1338', '-0.0527', '-0.0023']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -0.5933    -> ['-0.1379', '-2.6806', '-0.1436', '-0.0034', '-0.0008']\r\n\r\n------------------------------------------------------------------------------------------------------------------------\r\nShow few-shot examples in context = True\r\n\r\n      'heart' categorised as: 'organ'                     normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -0.2509    -> ['-0.0799', '-0.4219']\r\n    organism (['organ', 'ism'])                        -> -2.0750    -> ['-0.0799', '-0.4219', '-5.7232']\r\n    organisation (['organisation'])                    -> -3.7431    -> ['-0.0799', '-7.4063']\r\n    organelle (['org', 'ane', 'lle'])                  -> -0.9032    -> ['-0.0799', '-3.5000', '-0.0298', '-0.0031']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -1.7599    -> ['-0.0799', '-0.4219', '-8.2857', '-0.0087', '-0.0034']\r\n\r\n      'nucleus' categorised as: 'organelle'               normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -1.7653    -> ['-0.1489', '-3.3817']\r\n    organism (['organ', 'ism'])                        -> -1.8995    -> ['-0.1489', '-3.3817', '-2.1678']\r\n    organisation (['organisation'])                    -> -3.7379    -> ['-0.1489', '-7.3270']\r\n    organelle (['org', 'ane', 'lle'])                  -> -0.0921    -> ['-0.1489', '-0.2176', '-0.0006', '-0.0011']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -1.9658    -> ['-0.1489', '-3.3817', '-6.2928', '-0.0040', '-0.0017']\r\n\r\n      'Microsoft' categorised as: 'organisation'          normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -0.8883    -> ['-0.1198', '-1.6569']\r\n    organism (['organ', 'ism'])                        -> -1.1325    -> ['-0.1198', '-1.6569', '-1.6208']\r\n    organisation (['organisation'])                    -> -0.6383    -> ['-0.1198', '-1.1569']\r\n    organelle (['org', 'ane', 'lle'])                  -> -1.2105    -> ['-0.1198', '-4.5866', '-0.1336', '-0.0021']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -0.7088    -> ['-0.1198', '-1.6569', '-1.7615', '-0.0043', '-0.0017']\r\n\r\n      'mouse' categorised as: 'organism'                  normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -0.1719    -> ['-0.1273', '-0.2166']\r\n    organism (['organ', 'ism'])                        -> -0.1188    -> ['-0.1273', '-0.2166', '-0.0127']\r\n    organisation (['organisation'])                    -> -2.9610    -> ['-0.1273', '-5.7947']\r\n    organelle (['org', 'ane', 'lle'])                  -> -1.0844    -> ['-0.1273', '-3.9744', '-0.2330', '-0.0030']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -2.2812    -> ['-0.1273', '-0.2166', '-11.0517', '-0.0086', '-0.0020']\r\n\r\n      'trimethylboron' categorised as: 'organometallic'    normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -0.3231    -> ['-0.0992', '-0.5471']\r\n    organism (['organ', 'ism'])                        -> -3.2023    -> ['-0.0992', '-0.5471', '-8.9607']\r\n    organisation (['organisation'])                    -> -3.1551    -> ['-0.0992', '-6.2111']\r\n    organelle (['org', 'ane', 'lle'])                  -> -0.7889    -> ['-0.0992', '-2.9299', '-0.1246', '-0.0018']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -0.1314    -> ['-0.0992', '-0.5471', '-0.0076', '-0.0025', '-0.0007']\r\n\r\n------------------------------------------------------------------------------------------------------------------------\r\n```\r\nThe second set of results yields the expected categorisations.\r\n\r\n## Explanation\r\nWe see that only 1/5 answers are correct in the first set of results. Not coincidentally, the only correctly answered question (`'trimethylboron' categorised as: 'organometallic'`) is the one where the correct answer has the most tokens.\r\n\r\nThe first prefill token, common across all options, is `\":\"`. I'm not actually sure why this is present \u2014\u00a0something to do with token healing? Is this coming from the `assistant:` role prefix? Regardless, it's not important as it's consistent across all options, and it's not responsible for the poor performance (although does skew the logprobs calculations in unpredictable ways).\r\n\r\nInspecting the prefill token logprobs for the \"organometallic\" responses is instructive. Even if the `[\"organ\", \"omet\"]` tokens are relatively disfavoured, the `[\"al\", \"lic\"]` tokens are essentially guaranteed once you have the `\"organomet-\"` substring. The normalised logprobs calculation is a simple average of the prefill token logprobs, which means the `[\"al\", \"lic\"]` tokens massively inflate the score, even if \"organometallic\" is obviously wrong given the prior context.\r\n\r\nThe second set of results \u2014 which provides in-context few-shot examples \u2014 does rectify this with 5/5 correct answers. It seems that showing the model expected outputs leads to tokens beyond `\"organ\"`, such as `\"omet\"` , being sufficiently penalised to avoid the problem. It is surprising that the model requires this level of priming for such a simple task, however (even without the few-shot examples, the model is told the permitted options in the user prompt).\r\n\r\n## Other Observations\r\n- Prefixing the the assistant response with `\"Answer: \"` doesn't help, but does result in prefill tokens that only correspond to the choices and nothing else (i.e. no `\":\"` prior token, or similar). Why? The inconsistent presence/absence of prior tokens skews the scores and can lead to erratic selection behaviour when small tweaks are made to the prompt prefixes.\r\n- I tried running this example using regex instead (i.e. `gen(regex=\"(\" + \"|\".join(choices) + \")\")`), thinking this would resolve the issue with simple greedy token selection. But this also performs poorly (and extremely unpredictably, without `temperature=0`).\r\n- I've also explored avoiding overlapping options by wrapping each option in double quotes, but this doesn't solve the problem.\r\n\r\n## Suggestions\r\n- I think this is a severe enough flaw in the normalised logprobs calculation to be considered a bug. The outputs I've observed in several real-world settings are also unreasonably poor for simple tasks and capable models. I think evaluating all the options in their entirety is a good approach in theory, but a more sophisticated normalised logprobs calculation is required to adjust for bias towards options with more tokens.\r\n- Offering an alternative, greedy token selection `choices` decoding option could help. That said, I'm not sure why I still get poor outputs when I attempt to simulate this via `gen(regex=...)`. ",
    "labels": [],
    "state": "closed",
    "created_at": "2024-06-10T12:56:23+00:00",
    "closed_at": "2024-08-05T10:27:50+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/523/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/523"
  },
  {
    "number": 5906,
    "title": "[Feature] Does Sglang has Speculative MoE feature mentioned in the paper? https://arxiv.org/pdf/2503.04398",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nThe paper mentioned that they build this feature into SGLang and hence wonder if it is True? as it does not seem to have related PR. \n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-04-30T05:06:11+00:00",
    "closed_at": "2025-07-01T00:22:53+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5906/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5906"
  },
  {
    "number": 945,
    "title": "[Bug] disable_flashinfer didn't take effect",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n\n### Describe the bug\n\nI tried pip install flashinfer, but got error from\r\n```\r\n File \"/usr/local/python/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 28, in <module>\r\n    from flashinfer import (\r\nModuleNotFoundError: No module named 'flashinfer'\r\n```\r\n\r\nAccording to readme, i tried to set disable_flashinfer when init the runtime, my code is like\r\n\r\n```\r\nself.runtime = sgl.Runtime(\r\n            model_path = self.loader.target_path,\r\n            tp_size = envs.TENSOR_PARALLEL_SIZE,\r\n            trust_remote_code = True,\r\n            max_num_reqs = 40,\r\n            dtype = 'bfloat16',\r\n            # set back to false if flashinfer is installed.\r\n            disable_flashinfer = True,\r\n            disable_flashinfer_sampling = True,\r\n        )\r\n        sql.set_default_backend(self.runtime)\r\n\r\n```\r\nbut seems the two args didn't take effect, i still got\r\n```\r\n File \"/usr/local/python/lib/python3.10/site-packages/sglang/api.py\", line 37, in Runtime\r\n    from sglang.srt.server import Runtime\r\n  File \"/usr/local/python/lib/python3.10/site-packages/sglang/srt/server.py\", line 47, in <module>\r\n    from sglang.srt.managers.controller_multi import (\r\n  File \"/usr/local/python/lib/python3.10/site-packages/sglang/srt/managers/controller_multi.py\", line 30, in <module>\r\n    from sglang.srt.managers.controller_single import (\r\n  File \"/usr/local/python/lib/python3.10/site-packages/sglang/srt/managers/controller_single.py\", line 25, in <module>\r\n    from sglang.srt.managers.tp_worker import (\r\n  File \"/usr/local/python/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 49, in <module>\r\n    from sglang.srt.model_executor.model_runner import ModelRunner\r\n  File \"/usr/local/python/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 28, in <module>\r\n    from flashinfer import (\r\nModuleNotFoundError: No module named 'flashinfer'\r\n```\n\n### Reproduction\n\n```\r\nself.runtime = sgl.Runtime(\r\n            model_path = self.loader.target_path,\r\n            tp_size = envs.TENSOR_PARALLEL_SIZE,\r\n            trust_remote_code = True,\r\n            max_num_reqs = 40,\r\n            dtype = 'bfloat16',\r\n            # set back to false if flashinfer is installed.\r\n            disable_flashinfer = True,\r\n            disable_flashinfer_sampling = True,\r\n        )\r\n        sql.set_default_backend(self.runtime)\r\n\r\n```\n\n### Environment\n\n```Shell\n$python3 -m sglang.check_env\r\n\r\nPython: 3.8.10 (default, Mar 25 2024, 10:42:49) [GCC 9.4.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA GeForce RTX 4090\r\nCUDA_HOME: /usr/local/cuda-12.2\r\nNVCC: Cuda compilation tools, release 12.2, V12.2.140\r\nCUDA Driver Version: 535.104.05\r\n535.104.05\r\n535.104.05\r\n535.104.05\r\n535.104.05\r\n535.104.05\r\n535.104.05\r\n535.104.05\r\nPyTorch: 2.3.1+cu121\r\nsglang: 0.2.9\r\nflashinfer: Module Not Found\r\nrequests: 2.22.0\r\ntqdm: 4.66.4\r\nnumpy: 1.24.4\r\naiohttp: 3.9.5\r\nfastapi: 0.111.1\r\nhf_transfer: Module Not Found\r\nhuggingface_hub: 0.24.2\r\ninteregular: 0.3.3\r\npackaging: 20.3\r\nPIL: 7.0.0\r\npsutil: 5.5.1\r\npydantic: 2.8.2\r\nuvicorn: 0.30.3\r\nuvloop: 0.19.0\r\nzmq: 26.0.3\r\nvllm: 0.5.3.post1\r\noutlines: Module Not Found\r\nopenai: 1.37.1\r\nanthropic: Module Not Found\r\nlitellm: Module Not Found\r\nNVIDIA Topology:\r\n\tGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tPXB\tPXB\tPXB\tNODE\tNODE\tNODE\tNODE\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU1\tPXB\t X \tPXB\tPXB\tNODE\tNODE\tNODE\tNODE\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU2\tPXB\tPXB\t X \tPIX\tNODE\tNODE\tNODE\tNODE\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU3\tPXB\tPXB\tPIX\t X \tNODE\tNODE\tNODE\tNODE\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU4\tNODE\tNODE\tNODE\tNODE\t X \tPXB\tPXB\tPXB\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU5\tNODE\tNODE\tNODE\tNODE\tPXB\t X \tPXB\tPXB\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU6\tNODE\tNODE\tNODE\tNODE\tPXB\tPXB\t X \tPIX\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU7\tNODE\tNODE\tNODE\tNODE\tPXB\tPXB\tPIX\t X \tSYS\t0-31,64-95\t0\t\tN/A\r\nNIC0\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_bond_0\r\n\r\n\r\nulimit soft: 1024\n```\n",
    "labels": [
      "await-response"
    ],
    "state": "closed",
    "created_at": "2024-08-06T06:46:53+00:00",
    "closed_at": "2024-08-06T09:35:48+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/945/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/945"
  },
  {
    "number": 166,
    "title": "Add SGLang usage examples",
    "body": "List some good use cases of SGLang here:\r\n- [SELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures](https://arxiv.org/pdf/2402.03620.pdf)\r\n- [Tractable Control for Autoregressive Language Generation](https://starai.cs.ucla.edu/papers/ZhangICML23.pdf)",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-02-08T08:51:53+00:00",
    "closed_at": "2024-09-08T01:13:00+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/166/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/166"
  },
  {
    "number": 998,
    "title": "[Feature] Inference speed difference between sglang and vllm is smaller than advertised",
    "body": "### Motivation\r\n\r\nI compared the inference speed of two large model inference frameworks. I found that sglang is only about 30% faster than vllm, which is much lower than the claimed 3.8 times speedup.\r\n\r\nBelow are my environment details, prompt, and inference results.\r\nmy environment\uff1a\r\ngpu:4090*1\r\ncuda:12.4\r\nPython:3.11.0\r\nvllm:0.5.3\r\nsglang:0.2.7\r\n\r\nlaunch command:\r\n`python -m vllm.entrypoints.openai.api_server --model /home/modeldata/Qwen2-1.5B-Instruct --port 8899`\r\n`python -m sglang.launch_server --model-path /home/modeldata/Qwen2-1.5B-Instruct --host 0.0.0.0 --port 30000`\r\n\r\nprompt:\r\n`\u8bf7\u6839\u636e\u7528\u6237\u53cd\u9988\uff0c\u4ed4\u7ec6\u601d\u8003\u6807\u51c6\u7b54\u6848\u6784\u6210\u8981\u7d20\uff0c\u5e76\u6539\u5199\u51fa5\u53e5\u7b54\u6848\\n\u4f60\u662f\u76f4\u64ad\u771f\u4eba\u95ee\u7b54\u5ba2\u670d\uff0c\u4e3a\u907f\u514d\u5ba2\u670d\u56de\u7b54\u7684\u7b54\u6848\u91cd\u590d\u5ea6\u8fc7\u9ad8\uff0c\u8bf7\u4f60\u9010\u53e5\u601d\u8003\u5e76\u6539\u5199\u95ee\u9898\u7684\u7b54\u6848\u3002\\n****************\\n#\u6837\u4f8b\\n\u7528\u6237\u95ee\u9898\uff1a\u58f0\u97f3\u597d\u597d\u542c\\n\u53c2\u8003\u7684\u95ee\u7b54\u5bf9\uff1a[\"\u95ee\u9898: \u58f0\u97f3\u597d\u597d\u542c, \u7b54\u6848: \u8c22\u8c22\u5b9d\u5b9d\u7684\u5938\u5956\uff0c\u559c\u6b22\u4e3b\u64ad\u7684\u53ef\u4ee5\u70b9\u4e2a\u5173\u6ce8\", \"\u95ee\u9898: \u4f60\u662f\u771f\u4eba\u5417, \u7b54\u6848: \u4ec0\u4e48\uff0c\u4f60\u8bf4\u6211\u662f\u4e0d\u662f\u771f\u4eba\", \"\u95ee\u9898: \u6ca1\u6709\u7ea2\u5305\u5417, \u7b54\u6848: \u7ea2\u5305\u5de6\u4e0a\u89d2\u90fd\u4f1a\u5b89\u6392\u7684\", \"\u95ee\u9898: \u62cd\u5566, \u7b54\u6848: \u597d\u7684\u611f\u8c22\u652f\u6301\u54b1\u5bb6\u7389\u7c73\"]\\n\u8f93\u51fa\u683c\u5f0f\uff1a[\"\u611f\u8c22\u4f60\u7684\u5938\u8d5e\u652f\u6301\", \"\u4f60\u7684\u5938\u8d5e\u662f\u6211\u524d\u8fdb\u7684\u52a8\u529b\", \"\u6536\u5230\u4f60\u7684\u5938\u5956\uff0c\u5fc3\u60c5\u7f8e\u7f8e\u54d2\", \"\u5938\u5956\u6536\u5230\uff0c\u8c22\u8c22\u5b9d\u5b9d\u7684\u70ed\u60c5\", \"\u4f60\u7684\u5938\u5956\u6211\u6536\u5230\u4e86\uff0c\u8c22\u8c22\"]\\n\\n****************\\n#\u89c4\u5219\uff08\u5fc5\u987b\u4e25\u683c\u9075\u5faa\uff09\\n1\u3001\u4f60\u7684\u7b54\u6848\u5fc5\u987b\u4eff\u7167\u6539\u5199\u7528\u6237\u89c9\u5f97\u6ee1\u610f\u7684\u7b54\u6848\\n2\u3001\u4f60\u7684\u7b54\u6848\u7edd\u5bf9\u4e0d\u80fd\u6309\u7167\u7528\u6237\u4e0d\u6ee1\u610f\u7b54\u6848\u7684\u5199\u6cd5\u3002\\n3\u3001\u5fc5\u987b\u7ed9\u51fa\u548c\u7528\u6237\u53cd\u9988\u7684\u6570\u636e\u4e2d\u6539\u5199\u7b54\u6848\uff0c\u4f46\u662f\u4e0d\u80fd\u6539\u53d8\u53e5\u4e49\\n\u5982\u679c\u95ee\u9898\u6d89\u53ca\u5305\u90ae\u7684\u9700\u8981\u6ce8\u610f\uff1a\u504f\u8fdc\u5730\u533a\u4e0d\u5305\u90ae\uff0c\u504f\u8fdc\u5730\u533a\u5305\u542b\uff1a\u65b0\u7586\u3001\u897f\u85cf\u3001\u5b81\u590f\u3001\u5185\u8499\u53e4\u3001\u7518\u8083\u3001\u9752\u6d77\\n\u6d89\u53ca\u4ef7\u683c\u7684\u6570\u5b57\u4e0d\u80fd\u51fa\u9519\uff01\uff01\\n\\n#\u8981\u6c42\uff08\u5fc5\u987b\u4e25\u683c\u9075\u5faa\uff09\\n1\u3001\u4e0d\u80fd\u6539\u53d8\u53e5\u4e49\uff0c\u4e0d\u80fd\u968f\u610f*\u589e\u5220\u6539*\u7b54\u6848\u5f53\u4e2d\u7684\u4e3b\u4f53\uff0c\u53e3\u8bed\u5316\u4e00\u70b9\uff0c\u7b54\u6848\u7b26\u5408\u53e3\u64ad\u573a\u666f\\n2\u3001\u7b54\u6848\u4e2d\u6d89\u53ca\u6570\u5b57\u7684\u4e00\u5b9a\u4e0d\u80fd\u6539\u53d8\u6570\u5b57\u5927\u5c0f\uff01\uff01\\n3\u3001\u8f93\u51fa\u683c\u5f0f\u5fc5\u987b\u7528 list\u5217\u8868[\"\", \"\", \"\", \"\", \"\"]\u53ea\u80fd\u51fa\u73b0\u4e00\u4e2a\u5217\u8868, \u5426\u5219\u6211\u4eec\u53d6\u4e0d\u5230\u7b54\u6848\\n\\n#\u8f93\u5165\\n\u5df2\u77e5\u7528\u6237\u7684\u95ee\u9898\u662f\uff1a*\u9ec4\u7684\u597d\u5403\u767d\u7684\u597d\u5403*\\n\u53c2\u8003\u7684\u95ee\u7b54\uff1a\u6211\u4eec\u7389\u7c73\uff0c\u9ec4\u7684\u66f4\u751c\uff0c\u767d\u7684\u66f4\u7cef\\n\\n#\u8f93\u51fa`\r\n\r\ncode:\r\n`%%time\r\n\r\nsglang_result = []\r\nfor p in tqdm(prompt):\r\n    url = \"http://localhost:30000/v1/chat/completions\"\r\n    \r\n    data = {\r\n            \"model\": \"/home/modeldata/Qwen2-7B-Instruct\",\r\n            \"messages\": [\r\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n                {\"role\": \"user\", \"content\": p},\r\n            ],\r\n            \"max_tokens\": 512,\r\n            \"temperature\": 0.7,\r\n            \"top_p\": 0.8\r\n        }\r\n    \r\n    sglang_result.append(requests.post(url, json=data).json())`\r\n![image](https://github.com/user-attachments/assets/bcea5975-de1e-4d91-97b6-afbd15d5dcf6)\r\n\r\n\r\n I look forward to your response.\r\n\r\n### Related resources\r\n\r\n_No response_",
    "labels": [
      "await-response"
    ],
    "state": "closed",
    "created_at": "2024-08-09T08:07:39+00:00",
    "closed_at": "2024-08-15T16:26:10+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/998/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/998"
  },
  {
    "number": 2943,
    "title": "[Bug] Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings'}",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI am using Qwen2.5-72B which suppots positional extrapolation by Yarn through adding config(copied from https://huggingface.co/Qwen/Qwen2.5-72B-Instruct):\n```json\n{\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"original_max_position_embeddings\": 32768,\n    \"type\": \"yarn\"\n  }\n}\n\n```\nHowever, this seems not supported by sglang, when I specify context length=128000, I got\n```bash\nValueError: User-specified context_length (128000) is greater than the derived context_length (32768). This may lead to incorrect model outputs or CUDA errors. Note that the derived context_length may differ from max_position_embeddings in the model's config. To allow overriding this maximum, set the env var SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1\n```\nI am not sure what if SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 worked as normal as yarn do.\nBesides I also get:\n```bash\nUnrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings'}\n```\nNeeded some help\uff0cthis config works well in vllm\n\n### Reproduction\n\n>.\n\n### Environment\n\nPython: 3.12.3 | packaged by conda-forge | (main, Apr 15 2024, 18:38:13) [GCC 12.3.0]\nCUDA available: True\nGPU 0,1,2,3: NVIDIA A100-SXM4-80GB\nGPU 0,1,2,3 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\nCUDA Driver Version: 470.103.01\nPyTorch: 2.5.1+cu121\nsglang: 0.4.1.post5\nflashinfer: 0.2.0.post1+cu121torch2.4\ntriton: 3.1.0\ntransformers: 4.47.1\ntorchao: 0.7.0\nnumpy: 1.26.4\naiohttp: 3.11.9\nfastapi: 0.115.6\nhf_transfer: Module Not Found\nhuggingface_hub: 0.26.3\ninteregular: 0.3.3\nmodelscope: Module Not Found\norjson: 3.10.12\npackaging: 24.0\npsutil: 6.1.0\npydantic: 2.10.3\nmultipart: 0.0.20\nzmq: 26.2.0\nuvicorn: 0.32.1\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nxgrammar: Module Not Found\nopenai: 1.56.2\nanthropic: Module Not Found\nlitellm: Module Not Found\ndecord: Module Not Found\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    mlx5_0  mlx5_1  mlx5_2  mlx5_3  mlx5_4  mlx5_5  mlx5_6  mlx5_7  CPU Affinity    NUMA Affinity\nGPU0     X      NV12    NV12    NV12    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    48-95,144-191   1\nGPU1    NV12     X      NV12    NV12    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    48-95,144-191   1\nGPU2    NV12    NV12     X      NV12    SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     48-95,144-191   1\nGPU3    NV12    NV12    NV12     X      SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     48-95,144-191   1\nmlx5_0  SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    SYS     SYS     SYS     SYS\nmlx5_1  SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    SYS     SYS     SYS     SYS\nmlx5_2  SYS     SYS     SYS     SYS     NODE    NODE     X      PIX     SYS     SYS     SYS     SYS\nmlx5_3  SYS     SYS     SYS     SYS     NODE    NODE    PIX      X      SYS     SYS     SYS     SYS\nmlx5_4  PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS      X      PIX     NODE    NODE\nmlx5_5  PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     PIX      X      NODE    NODE\nmlx5_6  NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODE     X      PIX\nmlx5_7  NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODE    PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576",
    "labels": [
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2025-01-17T12:12:19+00:00",
    "closed_at": "2025-04-22T16:45:03+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2943/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2943"
  },
  {
    "number": 7059,
    "title": "[Bug] No performance gain after using hierarchical cache",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHi, @xiezhq-hermann , thanks for your great effort in hierarchical cache. However, in my experiements, I do not observe any performance gain when this feature is enabled.\n\nI am using Llama 3.1 8B model, and various sequence lengths, on a 4xH100 96GB NVLINK node. \n\nThe results shown in this [PR](https://github.com/sgl-project/sglang/pull/4009) indicate almost 3x faster in TTFT and 30% faster in TPOT.\n\nI thought the reason that hierarchical cache is faster is because it is moving the spilling KV cache to host memory, instead of just throwing it away. In this case, when the evicted requests are added later, they can just use the KV cache on host memory instead of recomputing it. \n\nHowever, in all of my experiments, hierarchical cache either brings no performance gain or slows down both TTFT and TPOT.\n\n### Reproduction\n\nFor long context generation, e.g. input len=2k, output len=100k,\n\n**with Hi-cache:**\n```\npython -m sglang.launch_server --model-path /hub/Llama-3.1-8B-Instruct/ --attention-backend flashinfer --port 30001 --trust-remote-code --enable-hierarchical-cache --mem-fraction-static 0.4 --tp 4\n\npython bench_serving.py --port 30001 --dataset-name random --random-input-len 2048 --random-output-len 100000 --num-prompts 15 --max-concurrency 15 --disable-shuffle --random-range-ratio 1.0\n\n[2025-06-09 22:16:48 TP0] Registering 1495 cuda graph addresses\n[2025-06-09 22:16:48 TP3] Registering 1495 cuda graph addresses\n[2025-06-09 22:16:48 TP1] Registering 1495 cuda graph addresses\n[2025-06-09 22:16:48 TP2] Registering 1495 cuda graph addresses\n[2025-06-09 22:16:48 TP0] Capture cuda graph end. Time elapsed: 7.01 s. mem usage=1.34 GB. avail mem=52.97 GB.\n[2025-06-09 22:16:48 TP2] Allocating 65.90 GB host memory for hierarchical KV cache.\n[2025-06-09 22:16:48 TP3] Allocating 65.90 GB host memory for hierarchical KV cache.\n[2025-06-09 22:16:48 TP1] Allocating 65.90 GB host memory for hierarchical KV cache.\n[2025-06-09 22:16:48 TP0] max_total_num_tokens=1005629, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=3929, context_len=131072\n[2025-06-09 22:16:48 TP0] Allocating 65.90 GB host memory for hierarchical KV cache.\n\n#Input tokens: 30720\n#Output tokens: 1500000\nNum of shared prefixes or conversations: 15\nNum of total requests: 15\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15/15 [28:41<00:00, 114.77s/it]\nTotal outputs: 15\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang    \nTraffic request rate:                    inf       \nMax request concurrency:                 15        \nSuccessful requests:                     15        \nBenchmark duration (s):                  1721.58   \nTotal input tokens:                      31234     \nTotal generated tokens:                  1500000   \nTotal generated tokens (retokenized):    1456003   \nRequest throughput (req/s):              0.01      \nInput token throughput (tok/s):          18.14     \nOutput token throughput (tok/s):         871.29    \nTotal token throughput (tok/s):          889.44    \nConcurrency:                             13.03     \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   1495499.64\nMedian E2E Latency (ms):                 1416772.19\n---------------Time to First Token----------------\nMean TTFT (ms):                          680.10    \nMedian TTFT (ms):                        272.39    \nP90 TTFT (ms):                           2352.16   \nP99 TTFT (ms):                           2352.30   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          14.95     \nMedian TPOT (ms):                        14.17     \nP90 TPOT (ms):                           16.87     \nP99 TPOT (ms):                           17.17     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           15.39     \nMedian ITL (ms):                         14.24     \nP90 ITL (ms):                            18.78     \nP99 ITL (ms):                            28.14     \n==================================================\n\n```\n**without Hi-cache:**\n```\npython -m sglang.launch_server --model-path /hub/Llama-3.1-8B-Instruct/ --attention-backend flashinfer --port 30001 --trust-remote-code --mem-fraction-static 0.4 --tp 4\n\npython bench_serving.py --port 30001 --dataset-name random --random-input-len 2048 --random-output-len 100000 --num-prompts 15 --max-concurrency 15 --disable-shuffle --random-range-ratio 1.0\n\n#Input tokens: 30720\n#Output tokens: 1500000\nNum of shared prefixes or conversations: 15\nNum of total requests: 15\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15/15 [28:39<00:00, 114.64s/it]\nTotal outputs: 15\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang    \nTraffic request rate:                    inf       \nMax request concurrency:                 15        \nSuccessful requests:                     15        \nBenchmark duration (s):                  1719.61   \nTotal input tokens:                      31234     \nTotal generated tokens:                  1500000   \nTotal generated tokens (retokenized):    1437831   \nRequest throughput (req/s):              0.01      \nInput token throughput (tok/s):          18.16     \nOutput token throughput (tok/s):         872.29    \nTotal token throughput (tok/s):          890.45    \nConcurrency:                             13.03     \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   1493536.43\nMedian E2E Latency (ms):                 1414419.30\n---------------Time to First Token----------------\nMean TTFT (ms):                          688.62    \nMedian TTFT (ms):                        253.27    \nP90 TTFT (ms):                           2469.10   \nP99 TTFT (ms):                           2469.29   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          14.93     \nMedian TPOT (ms):                        14.14     \nP90 TPOT (ms):                           16.85     \nP99 TPOT (ms):                           17.15     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           15.50     \nMedian ITL (ms):                         14.27     \nP90 ITL (ms):                            18.79     \nP99 ITL (ms):                            33.73     \n==================================================\n```\n\nI also ran another set of experiments where the input len = 32k, output len = 32K, and the results are:\n\n| num requests | Hi-cache     | RPS       | E2E       | TTFT     | TPOT    | ITL    |\n|--------------|--------------|-----------|-----------|----------|---------|--------|\n| 64           |              | 0.12      | 306654.91 | 10549.14 | 33.94   | 20.68  |\n| 64           | <mark>\u2714</mark> | 0.12      | 310333.71 | 12848.84 | 37.47   | 20.80  |\n\n\nI added some profiling log to the code, and I do see the hi-cache caches nodes to host memory and later loads them back. But since the benchmarks show the almost same result, I am wondering if there cached nodes are somehow also recomputed again instead of just reused.\n\nAnd also could you clarify if my understanding of how hi-cache could improve the performance is correct (i.e. avoid recomputation)?\n\n\n\n### Environment\n\ncuda 12.6\nsglang 0.4.6 post5\npytorch 2.6\n\n4*H100 98GB NVLINK\nPCIe Gen5\n1T host memory",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-10T16:56:33+00:00",
    "closed_at": "2025-06-18T20:25:14+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7059/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7059"
  },
  {
    "number": 3750,
    "title": "[Feature] attention dp + attention tp for deepseek v3",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nIn the deepseek v3 technical report, it is mentioned \"The attention part employs TP4 with SP, combined with DP80\". In SGLang, it seems that currently, dp_size must equal tp_size for the system to run.\n\n![Image](https://github.com/user-attachments/assets/b3bbfb18-1433-4d72-b45f-71593370de2d)\n\n Could you please suggest how to design a solution that would allow for a configuration similar to tp=8, dp=2, and attention_tp=4?\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-21T08:47:02+00:00",
    "closed_at": "2025-04-12T02:26:03+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3750/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3750"
  },
  {
    "number": 86,
    "title": "OOM Error on A40 GPU [Jupyter notebook]",
    "body": "I was trying the following code sample (adapted from the discussion in https://github.com/sgl-project/sglang/issues/81) - \r\n\r\n```\r\nimport sglang as sgl\r\nfrom sglang import function, gen, set_default_backend, Runtime\r\n\r\n@sgl.function\r\ndef tool_use(s, question):\r\n    s += \"To answer this question: \" + question + \", \"\r\n    s += \"I need to use a \" + sgl.gen(\"tool\", choices=[\"calculator\", \"web browser\"]) + \". \"\r\n    if s[\"tool\"] == \"calculator\":\r\n        s += \"The math expression is\" + sgl.gen(\"expression\")\r\n    elif s[\"tool\"] == \"web browser\":\r\n        s += \"The website url is\" + sgl.gen(\"url\")\r\n\r\nruntime = Runtime(model_path='Model_Saves/teknium--OpenHermes-2.5-Mistral-7B')\r\nset_default_backend(runtime)\r\n\r\ndriver_tool_use()\r\n```\r\n   \r\nI firstly got the same error as described here: https://github.com/sgl-project/sglang/issues/41#issuecomment-1899347676\r\nI then followed Solution 2 from this [comment](https://github.com/sgl-project/sglang/issues/41#issuecomment-1899354400) and the error disappeared but I now get an OOM error even though I have `46068 MiB` of space on the GPU which is more than what a 7B model needs. On checking with `nvidia-smi` I see `41158MiB` in use. I'm running this in a Jupyter notebook.\r\n\r\nThe error - \r\n\r\n```\r\nrouter init state: Traceback (most recent call last):\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/managers/router/manager.py\", line 68, in start_router_process\r\n    model_client = ModelRpcClient(server_args, port_args)\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 480, in __init__\r\n    self.model_server.exposed_init_model(0, server_args, port_args)\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 53, in exposed_init_model\r\n    self.model_runner = ModelRunner(\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/managers/router/model_runner.py\", line 233, in __init__\r\n    self.load_model()\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/managers/router/model_runner.py\", line 278, in load_model\r\n    model = model_class(\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/models/llama2.py\", line 258, in __init__\r\n    self.model = LlamaModel(config, linear_method)\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/models/llama2.py\", line 218, in __init__\r\n    [\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/models/llama2.py\", line 219, in <listcomp>\r\n    LlamaDecoderLayer(config, i, linear_method)\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/models/llama2.py\", line 167, in __init__\r\n    self.mlp = LlamaMLP(\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/models/llama2.py\", line 49, in __init__\r\n    self.down_proj = RowParallelLinear(\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 495, in __init__\r\n    self.linear_weights = self.linear_method.create_weights(\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 55, in create_weights\r\n    weight = Parameter(torch.empty(output_size_per_partition,\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/torch/utils/_device.py\", line 77, in __torch_function__\r\n    return func(*args, **kwargs)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 44.39 GiB of which 77.62 MiB is free. Process 535546 has 40.19 GiB memory in use. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.80 GiB is allocated by PyTorch, and 15.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\ndetoken init state: init ok\r\n\r\n```",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-01-23T12:03:01+00:00",
    "closed_at": "2024-07-25T06:32:01+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/86/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/86"
  },
  {
    "number": 3656,
    "title": "Deploying DeepSeek R1 on 2*H100 2*8*80G achieves only 19 tokens/s inference speed, while vllm deployment reaches 30 tokens/s. Is this normal?",
    "body": "\"When deploying DeepSeek R1 on 2*H100 2*8*80G, the inference speed is only 19 tokens/s, while vllm deployment reaches 30 tokens/s. Is this normal? My inference command for node 1 is:\n\ndocker run --gpus all --rm --network=host -v /data/models:/data/models -it --env \"NCCL_IB_DISABLE=0\" --env \"NCCL_IB_HCA=mlx5\" --env \"NCCL_IB_GID_INDEX=3\" --env \"NCCL_SOCKET_IFNAME=bond0\" --env \"GLOO_SOCKET_IFNAME=bond0\" --env \"NCCL_DEBUG=INFO\" --ipc=host sglang:v0 python3 -m sglang.launch_server --model-path /data/models/DeepSeek-R1 --tp 16 --dist-init-addr 10.163.34.152:20000 --nnodes 2 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 8000\n\nI didn't use --enable-dp-attention, as it throws an error whenever I try to use it. Is 19 tokens/s the correct output speed for DeepSeek-R1 using the SLang framework?\"",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-02-18T06:14:47+00:00",
    "closed_at": "2025-04-29T00:18:48+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3656/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3656"
  },
  {
    "number": 7586,
    "title": "[Bug] [CI regression] TestEpMoEFP8",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nCI unit-test-backend-2-gpu seems to be broken since the past few days (if not longer).\n\nLooking at the log, it seems to be watchdog timeout at TestEpMoEFP8. The non-quantized version TestEpMoE seems to be working fine.\n\n### Reproduction\n\nSample failure: https://github.com/sgl-project/sglang/actions/runs/15916204833/job/44895747274\n\n### Environment\n\nCI",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-27T05:49:07+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7586/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7586"
  },
  {
    "number": 5013,
    "title": "[Bug] Segmentation fault: address not mapped to object at address 0x17",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI now have two single-machine eight-card A100 servers, and I use 0.4.4 to deploy deepseek-r1-channel-int8 and get an error\uff0cI now have two single-machine eight-card A100 servers. I use 0.4.4 to deploy deepseek-r1-channel-int8 and get an error. The image used is lmsysorg/sglang:v0.4.4.post1-cu124\uff1b\n\n**master log**:\n```\n[2025-04-02 15:25:19 DP0 TP5] Memory pool end. avail mem=10.43 GB\n[2025-04-02 15:25:19 DP0 TP1] Memory pool end. avail mem=10.43 GB\n[2025-04-02 15:25:19 DP0 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=10.00 GB\n[2025-04-02 15:25:19 DP0 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=10.00 GB\n[2025-04-02 15:25:19 DP0 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=10.00 GB\n[2025-04-02 15:25:19 DP0 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=10.00 GB\n[2025-04-02 15:25:19 DP0 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=10.00 GB\nCapturing batches (avail_mem=9.91 GB):   0%|                                                                         | 0/23 [00:00<?, ?it/s][2025-04-02 15:25:19 DP0 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=10.00 GB\n[2025-04-02 15:25:19 DP0 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=10.00 GB\n[2025-04-02 15:25:19 DP0 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=10.00 GB\n2025-04-02 15:25:20,508 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:20,533 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:20,693 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:20,712 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:20,718 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:20,721 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:20,721 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:20,722 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:51,403 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:51,447 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:51,501 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:51,547 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:51,597 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:51,650 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:51,709 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:51,765 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n[2025-04-02 15:26:05 DP0 TP0] Using configuration from /sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8.json for MoE layer.\n[2025-04-02 15:26:05 DP0 TP1] Using configuration from /sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8.json for MoE layer.\n[2025-04-02 15:26:05 DP0 TP7] Using configuration from /sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8.json for MoE layer.\n[2025-04-02 15:26:05 DP0 TP5] Using configuration from /sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8.json for MoE layer.\n[2025-04-02 15:26:05 DP0 TP6] Using configuration from /sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8.json for MoE layer.\n[2025-04-02 15:26:06 DP0 TP3] Using configuration from /sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8.json for MoE layer.\n[2025-04-02 15:26:06 DP0 TP4] Using configuration from /sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8.json for MoE layer.\n[2025-04-02 15:26:07 DP0 TP2] Using configuration from /sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8.json for MoE layer.\nCapturing batches (avail_mem=8.88 GB):  30%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                             | 7/23 [00:56<00:51,  3.20s/it][A100-GPU-50:345  :0:4945] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x17)\n==== backtrace (tid:   4945) ====\n 0 0x0000000000042520 __sigaction()  ???:0\n 1 0x00000000000494f4 uploadProxyOps()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/enqueue.cc:1131\n 2 0x0000000000051a7f hostStreamPlanTask()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/enqueue.cc:1163\n 3 0x0000000000051bd9 hostStreamPlanCallback()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/enqueue.cc:1175\n 4 0x00000000002342aa cuEGLApiInit()  ???:0\n 5 0x0000000000251113 cuEGLApiInit()  ???:0\n 6 0x0000000000094ac3 pthread_condattr_setpshared()  ???:0\n 7 0x0000000000126850 __xmknodat()  ???:0\n=================================\nFatal Python error: Segmentation fault\n\nThread 0x00007f2ec7fff640 (most recent call first):\n  File \"/usr/lib/python3.10/threading.py\", line 324 in wait\n  File \"/usr/lib/python3.10/threading.py\", line 607 in wait\n  File \"/usr/local/lib/python3.10/dist-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f3551fff640 (most recent call first):\n  File \"/usr/lib/python3.10/threading.py\", line 324 in wait\n  File \"/usr/lib/python3.10/threading.py\", line 607 in wait\n  File \"/usr/local/lib/python3.10/dist-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f44be24c480 (most recent call first):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 954 in synchronize\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/graphs.py\", line 173 in __enter__\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 431 in capture_one_batch_size\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 336 in capture\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 252 in __init__\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 881 in init_cuda_graphs\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 207 in initialize\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 166 in __init__\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 74 in __init__\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63 in __init__\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 218 in __init__\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1748 in run_scheduler_process\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108 in run\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314 in _bootstrap\n  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 129 in _main\n  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 116 in spawn_main\n  File \"<string>\", line 1 in <module>\n\nExtension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, uvloop.loop, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, psutil._psutil_linux, psutil._psutil_posix, setproctitle, zmq.backend.cython._zmq, yaml._yaml, markupsafe._speedups, PIL._imaging, PIL._imagingft, sentencepiece._sentencepiece, msgspec._core, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, msgpack._cmsgpack, google._upb._message, ray._raylet, regex._regex, cuda_utils, __triton_launcher (total: 52)\nA100-GPU-50:343:1213 [5] NCCL INFO [Service thread] Connection closed by localRank 7\nA100-GPU-50:344:1208 [6] NCCL INFO [Service thread] Connection closed by localRank 7\nA100-GPU-50:341:1216 [3] NCCL INFO [Service thread] Connection closed by localRank 7\nA100-GPU-50:340:1215 [2] NCCL INFO [Service thread] Connection closed by localRank 7\nA100-GPU-50:339:1212 [1] NCCL INFO [Service thread] Connection closed by localRank 7\nA100-GPU-50:342:1214 [4] NCCL INFO [Service thread] Connection closed by localRank 7\nA100-GPU-50:338:1220 [0] NCCL INFO [Service thread] Connection closed by localRank 7\nA100-GPU-50:344:1083 [6] NCCL INFO [Service thread] Connection closed by localRank 7\nA100-GPU-50:343:1084 [5] NCCL INFO [Service thread] Connection closed by localRank 7\nA100-GPU-50:342:1082 [4] NCCL INFO [Service thread] Connection closed by localRank 7\nA100-GPU-50:341:1085 [3] NCCL INFO [Service thread] Connection closed by localRank 7\nA100-GPU-50:339:1081 [1] NCCL INFO [Service thread] Connection closed by localRank 7\nA100-GPU-50:340:1091 [2] NCCL INFO [Service thread] Connection closed by localRank 7\nA100-GPU-50:338:1092 [0] NCCL INFO [Service thread] Connection closed by localRank 7\n```\n\n**slave log**:\n```\n[2025-04-02 15:25:19 DP1 TP10] Capture cuda graph begin. This can take up to several minutes. avail mem=10.00 GB\n2025-04-02 15:25:20,617 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:20,654 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:20,704 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:20,717 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:20,790 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:20,807 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:20,813 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:20,815 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:50,477 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:50,545 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:50,596 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:50,656 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:50,703 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:50,753 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:50,804 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n2025-04-02 15:25:50,860 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64_profiler_False\n[2025-04-02 15:26:05 DP1 TP13] Using configuration from /sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8.json for MoE layer.\n[2025-04-02 15:26:05 DP1 TP9] Using configuration from /sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8.json for MoE layer.\n[2025-04-02 15:26:05 DP1 TP11] Using configuration from /sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8.json for MoE layer.\n[2025-04-02 15:26:05 DP1 TP15] Using configuration from /sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8.json for MoE layer.\n[2025-04-02 15:26:05 DP1 TP8] Using configuration from /sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8.json for MoE layer.\n[2025-04-02 15:26:05 DP1 TP10] Using configuration from /sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8.json for MoE layer.\n[2025-04-02 15:26:06 DP1 TP14] Using configuration from /sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8.json for MoE layer.\n[2025-04-02 15:26:06 DP1 TP12] Using configuration from /sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8.json for MoE layer.\n```\n\n### Reproduction\n\n```\ncd /sgl-workspace && python3 -m sglang.launch_server\n                --model-path /data/models/DeepSeek-R1-Channel-INT8\n                --served-model-name DeepSeek-R1-Channel-INT8\n                --tp 16\n                --dist-init-addr $LWS_LEADER_ADDRESS:20000\n                --nnodes $LWS_GROUP_SIZE\n                --node-rank 0\n                --trust-remote-code\n                --enable-metrics\n                --host 0.0.0.0\n                --port 18000\n                --enable-flashinfer-mla\n                --reasoning-parser deepseek-r1\n                --quantization w8a8_int8\n                --enable-torch-compile\n                --torch-compile-max-bs 8\n                --enable-dp-attention\n                --chunked-prefill-size 16384\n                --max-prefill-tokens 16384\n                --mem-fraction-static 0.9\n                --attention-backend flashinfer\n                --dp-size 2\n                --enable-ep-moe\n                --expert-parallel-size 16\n```\n\n### Environment\n\n```\nroot@A100-GPU-50:/sgl-workspace# python3 -m sglang.check_env\nINFO 04-03 01:10:48 __init__.py:190] Automatically detected platform cuda.\n/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA A100-SXM4-80GB\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 560.35.03\nPyTorch: 2.5.1+cu124\nsgl_kernel: 0.0.5\nflashinfer: 0.2.3+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.13\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.3\ninteregular: 0.3.3\nmodelscope: 1.23.2\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.3.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.66.3\ntiktoken: 0.9.0\nanthropic: 0.49.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PXB     NODE    NODE    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     SYS0-35,72-107      0               N/A\nGPU1    PXB      X      NODE    NODE    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     SYS0-35,72-107      0               N/A\nGPU2    NODE    NODE     X      PXB     SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     SYS0-35,72-107      0               N/A\nGPU3    NODE    NODE    PXB      X      SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     SYS0-35,72-107      0               N/A\nGPU4    SYS     SYS     SYS     SYS      X      PXB     NODE    NODE    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    NODE36-71,108-143   1               N/A\nGPU5    SYS     SYS     SYS     SYS     PXB      X      NODE    NODE    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    NODE36-71,108-143   1               N/A\nGPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      PXB     SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     NODE36-71,108-143   1               N/A\nGPU7    SYS     SYS     SYS     SYS     NODE    NODE    PXB      X      SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     NODE36-71,108-143   1               N/A\nNIC0    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS      X      PXB     NODE    NODE    SYS     SYS     SYS     SYS     SYS\nNIC1    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     PXB      X      NODE    NODE    SYS     SYS     SYS     SYS     SYS\nNIC2    NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODE     X      PIX     SYS     SYS     SYS     SYS     SYS\nNIC3    NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODE    PIX      X      SYS     SYS     SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS      X      PXB     NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     PXB      X      NODE    NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODE     X      PXB     NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODE    PXB      X      NODE\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n\n\nulimit soft: 1024000\n```",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-04-03T01:14:57+00:00",
    "closed_at": "2025-06-18T00:19:37+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5013/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5013"
  },
  {
    "number": 1118,
    "title": "[Feature] add disable_custom_all_reduce",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nSometimes, we need to turn off Custom allreduce. \r\nPlease  support disable_custom_all_reduce.\r\n\n\n### Related resources\n\n_No response_",
    "labels": [
      "await-response"
    ],
    "state": "closed",
    "created_at": "2024-08-16T04:59:48+00:00",
    "closed_at": "2024-08-21T04:53:40+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1118/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1118"
  },
  {
    "number": 3890,
    "title": "[Bug] --dp-size issue with AMD 8xMI300X and Llama 3.1 70B",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nUsing --dp-size 4 --tp 2 on 8xMI300X does not work. Is this an MI300X issue or an issue with how I'm passing in sizes?\n\nError:\n```\n\n    _TP = init_model_parallel_group(\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py\", line 890, in init_model_parallel_group\n    return GroupCoordinator(\n           ^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py\", line 241, in __init__\n    self.ca_comm = CustomAllreduce(\n                   ^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/custom_all_reduce.py\", line 221, in __init__\n    self.meta_ptrs = self.create_shared_buffer(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/custom_all_reduce.py\", line 279, in create_shared_buffer\n    lib = CudaRTLibrary()\n          ^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/cuda_wrapper.py\", line 117, in __init__\n    assert so_file is not None, \"libcudart is not loaded in the current process\"\n           ^^^^^^^^^^^^^^^^^^^\nAssertionError: libcudart is not loaded in the current process\n[rank0]:[W226 12:24:24.546735073 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n[rank0]:[W226 12:24:25.914339960 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n[rank0]:[W226 12:24:25.054759318 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n[rank0]:[W226 12:24:25.153719919 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n```\n\n### Reproduction\n\n```\npython3 -m sglang.launch_server --model-path neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8 --port 8000 --host 0.0.0.0 --dp-size 4 --tp 2 --trust-remote-code --mem-fraction-static 0.8 --max-running-requests 128 --disable-cuda-graph\n```\non\n```\nlmsysorg/sglang:v0.4.3.post2-rocm630-srt\n```\n\n### Environment\n\npython3 -m sglang.check_env\nPython: 3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0]\nROCM available: True\nGPU 0,1,2,3,4,5,6,7: AMD Instinct MI300X\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.4\nROCM_HOME: /opt/rocm\nHIPCC: HIP version: 6.3.42131-fa1d09cbd\nROCM Driver Version: 6.7.0\nPyTorch: 2.6.0a0+git8d4926e\nsgl_kernel: 0.0.3.post6\nflashinfer: Module Not Found\ntriton: 3.2.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.6\nhf_transfer: 0.1.9\nhuggingface_hub: 0.27.1\ninteregular: 0.3.3\nmodelscope: 1.23.0\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.5\nmultipart: 0.0.20\nzmq: 26.2.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.7.dev2+g113274a0\nopenai: 1.59.7\nanthropic: Module Not Found\nlitellm: Module Not Found\ndecord: 0.6.0\nAMD Topology:\n\n\n============================ ROCm System Management Interface ============================\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI\nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI\nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI\nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI\nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI\nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0\n================================== End of ROCm SMI Log ===================================\n\nulimit soft: 1048576",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-02-26T12:26:17+00:00",
    "closed_at": "2025-05-12T00:20:26+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3890/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3890"
  },
  {
    "number": 2389,
    "title": "[Feature] SGLang Router design discussion",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n@ispobock  and I had a brief discussion about the current design and implementation of the SGLang Router.\r\n\r\nI think the main concerns currently are as follows, before large-scale deployment.\r\n\r\n- The current Router is stateful, which means I cannot deploy the Router like scaling a stateless service.\r\nMay we consider storing the state of the Router in services like Redis, DB, or etcd here?\r\n\r\n- The current Router is at the cluster level. Although there are replicas, when the master fails, a replica can be used.\r\nImagine a real deployment scenario, such as one used by actual customers, where the deployment requires simultaneous use of AWS, GCP, and Oracle. The data centers are distributed across the Western US, Central US, and Eastern US. There is a risk of cloud service providers going down as well as data center outages. Additionally, there is consideration for network communication latency between different regions.\r\n\r\nThese issues cannot be well resolved under the current Router design, and to truly use the Router for large-scale deployment, these problems cannot be avoided.\r\n\r\nThis issue is just a starting point to raise some practical deployment requirements from the industry. We can discuss more detailed designs offline, and if the community has similar feedback or needs, they are welcome to join the discussion on the [Slack channel](https://join.slack.com/t/sgl-fru7574/shared_invite/zt-2rtikx2pv-DUfPrhx2SaNAq~47YtV1XQ). Thanks! cc @ByronHsu \n\n### Related resources\n\n_No response_",
    "labels": [
      "enhancement",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-12-07T11:53:13+00:00",
    "closed_at": "2025-04-06T00:19:37+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2389/reactions",
      "total_count": 12,
      "+1": 12,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/2389"
  },
  {
    "number": 6158,
    "title": "Different output format with OpenAI Client",
    "body": "Hello,\n\nIt seems there\u2019s a difference in the output format between the OpenAI client and the SGLang OpenAI client for batch requests. This is causing issues when integrating with [`lmm_evals`](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/4ca1a52b55ac4d057329bb1dde092ce68b60256e/lmms_eval/models/batch_gpt4.py#L162).\n\nIs this difference expected, or should we consider making changes on our end to be consistent with the official OpenAI client?\n\n**Official OpenAI Client:** \n\nThe OpenAI client gives output in the following format for a batch request:\n\n> {\"id\": \"batch_req_wnaDys\", \"custom_id\": \"request-2\", \"response\": {\"status_code\": 200, \"request_id\": \"req_c187b3\", \"body\": {\"id\": \"chatcmpl-9758Iw\", \"object\": \"chat.completion\", \"created\": 1711475054, \"model\": \"gpt-4o-mini\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"2 + 2 equals 4.\"}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 24, \"completion_tokens\": 15, \"total_tokens\": 39}, \"system_fingerprint\": null}}, \"error\": null}\n\nOutput can be obtained in the following manner:\n\n`output = result['response']['choices'][0]['message']['content']\n`\n\n**SGLang OpenAI Client:** \n\nBut SGLang OpenAI Client gives output in different format:\n\n>{\"id\": \"custom_id\", \"custom_id\": \"request-1\", \"response\": {'status_code': 200, 'request_id': 'batch_f6ea6fef-e9fe-4de5-944d-92d25efef3d9-req_0', 'body': {'id': 'batch_f6ea6fef-e9fe-4de5-944d-92d25efef3d9-req_0', 'object': 'chat.completion', 'created': 1746752343, 'model': 'qwen/qwen2.5-0.5b-instruct', 'choices': {'index': 0, 'message': {'role': 'assistant', 'content': \"Sure, here is a programming joke for you:\\nWhy couldn't the code always stay happy when it ran?\\nBecause it always had to wait for the programmer to give it a smiley face!\", 'tool_calls': None, 'reasoning_content': None}, 'logprobs': None, 'finish_reason': 'stop', 'matched_stop': 151645}, 'usage': {'prompt_tokens': 35, 'completion_tokens': 40, 'total_tokens': 75}, 'system_fingerprint': None}}\n\nOutput can be obtained in the following manner:\n\n`output = result['response']['choices']['message']['content']\n`",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-05-09T13:11:01+00:00",
    "closed_at": "2025-07-10T00:20:11+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6158/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6158"
  },
  {
    "number": 989,
    "title": "[Feature] DeepSeek-Coder-V2-Instruct-FP8 on 8xA100",
    "body": "### Motivation\n\nVLLM has announced their support for running llama3.1-405b-fp8 on 8xA100. This is the [blog](https://blog.vllm.ai/2024/07/23/llama31.html)\r\n\r\nDoes sglang support running DeepSeek-Coder-V2-Instruct-FP8 on 8xA100?\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-08T08:43:21+00:00",
    "closed_at": "2024-09-22T12:58:29+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/989/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/989"
  },
  {
    "number": 958,
    "title": "[Bug] Multinode cannot be started on runpod",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n\n### Describe the bug\n\nHi, I try to run a multi-node following the instructions for 2 machine with 8x4090 each. But it just stopped at \"Init nccl begin\" without further progress.\r\n<img width=\"1523\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3ed8af47-d512-42de-9222-33bb545ca05c\">\r\n\n\n### Reproduction\n\nNode 1:\r\nGLOO_SOCKET_IFNAME=eth0 pm2 start /root/miniconda3/envs/sgl/lib/python3.11/site-packages/sglang/launch_server.py --name sgl -- --model-path NousResearch/Meta-Llama-3.1-8B-Instruct --nccl-init-addr xxx:xxx --nnodes 2 --node-rank 0 --tp-size 16 --schedule-conservativeness 0.01 --mem-fraction-static 0.8\r\nNode 2:\r\nGLOO_SOCKET_IFNAME=eth0 pm2 start /root/miniconda3/envs/sgl/lib/python3.11/site-packages/sglang/launch_server.py --name sgl -- --model-path NousResearch/Meta-Llama-3.1-8B-Instruct --nccl-init-addr xxx:xxx --nnodes 2 --node-rank 1 --tp-size 16 --schedule-conservativeness 0.01 --mem-fraction-static 0.8\n\n### Environment\n\n```Shell\nN/A\n```\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-07T04:10:19+00:00",
    "closed_at": "2024-11-14T23:46:49+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/958/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/958"
  },
  {
    "number": 2343,
    "title": "[Feature] router adds add_worker_url, remove_worker_url api",
    "body": "### Checklist\r\n\r\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [X] 2. Please use English, otherwise it will be closed.\r\n\r\n### Motivation\r\n\r\nThank you very much to the sglang team for open-sourcing such a useful inference framework.\r\n\r\nWe are currently deploying Llama3 70b on 3 X 8 X H100s. Run the command on each server separately:\r\n```\r\npython -m sglang_router.launch_server --model-path NousResearch/Hermes-3-Llama-3.1-70B-FP8 --port 6006 --dp-size 4 --tp 2 --router-eviction-interval 300 --context-length 20000\r\n```\r\nThen the three exposed http://127.0.0.1:6006 services are randomly routed by konga. Compared with directly using the --dp command, the cache rate is increased from 25% to 35%.\r\n\r\nWhy not use --worker-urls?\r\n\r\nThe current problem is that Rust routing currently supports cross-machine routing, but since workers cannot be added or deleted, when encountering occasional server errors or restarts, the router process needs to be restarted to reroute.\r\n\r\nFor our case (Long context, multi-round conversations ), if the routing is matched correctly, the cache rate should theoretically be able to reach 70-80%.\r\n\r\n### Related resources\r\n\r\n_No response_",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-12-04T06:39:15+00:00",
    "closed_at": "2025-02-03T00:17:06+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2343/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2343"
  },
  {
    "number": 3849,
    "title": "[DeepseekR1]How ragged prefill manage kv_cache?",
    "body": "I'm investigating the chunked prefill method in DeepSeek V3/R1. The code shows that it uses self.prefill_wrapper_ragged.forward_return_lse for both prefill and chunked prefill operations. However, I haven't been able to locate where the KV cache is provided in the code. Could you help me identify this part of the implementation?\n\n<img width=\"710\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d98701eb-2d35-4f39-bb6e-d4dde25a571c\" />\n\n<img width=\"582\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e5684ecf-2239-42ef-a66b-e40de1452e33\" />\n",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-02-25T11:06:13+00:00",
    "closed_at": "2025-06-14T00:18:52+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3849/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3849"
  },
  {
    "number": 5995,
    "title": "[Bug] out of resource: shared memory when serving RedHatAI/Qwen3-30B-A3B-FP8-dynamic",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen running sglang with [Qwen3-30B-A3B-FP8-dynamic](https://huggingface.co/RedHatAI/Qwen3-30B-A3B-FP8-dynamic) quantized model, single request use curl is fine, but benchmark with only 2 concurrency will cause sglang exit with \"triton out of resource: shared memory\".\n\nSglang exit with error \"triton out of resource: shared memory, Required: 163840, Hardware limit: 101376\"\n```\n    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,\n    ^^^^^^^^^^\n  File \"/data/deployer24/miniforge3/envs/sglang_046p1/lib/python3.12/site-packages/triton/compiler/compiler.py\", line 395, in __getattribute__\n    self._init_handles()\n  File \"/data/deployer24/miniforge3/envs/sglang_046p1/lib/python3.12/site-packages/triton/compiler/compiler.py\", line 388, in _init_handles\n    raise OutOfResources(self.metadata.shared, max_shared, \"shared memory\")\ntriton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 163840, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.\n```\n\nI've try the same quantized model with vllm 0.8.5, which works well when concurrency=128. Except it's not as fast as sglang on single request.\n\n### Reproduction\n\nstart sglang with\n```\nCUDA_VISIBLE_DEVICES=0 python -m sglang.launch_server --model-path RedHatAI/Qwen3-30B-A3B-FP8-dynamic --served-model-name Qwen3-30B-A3B --trust-remote-code --enable-p2p-check --mem-fraction-static 0.7 --api-key ikang_2004 --max-running-requests 32 --tp 1 --host 0.0.0.0 --port 17960\n```\n\nrun benchmark\n```\n python3 -m sglang.bench_serving --backend sglang-oai --base-url http://localhost:17960 --dataset-name random --random-range-ratio 0.75 --num-prompt 4 --max-concurrency 2 --random-input 296 --random-output 1275 --dataset-path ~/dl/datasets\n```\n\n### Environment\n\nPython: 3.12.9 | packaged by conda-forge | (main, Mar  4 2025, 22:48:41) [GCC 13.3.0]\nCUDA available: True\nGPU 0,1,2,3: NVIDIA GeForce RTX 4090\nGPU 0,1,2,3 Compute Capability: 8.9\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.8, V12.8.93\nCUDA Driver Version: 570.124.06\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post1\nsgl_kernel: 0.1.0\nflashinfer: Module Not Found\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.10.0\nnumpy: 2.2.5\naiohttp: 3.11.18\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.11.3\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.2\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.17\nopenai: 1.76.0\ntiktoken: 0.9.0\nanthropic: 0.50.0\nlitellm: 1.67.4.post1\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NODE    SYS     SYS     0-15    0               N/A\nGPU1    NODE     X      SYS     SYS     0-15    0               N/A\nGPU2    SYS     SYS      X      NODE    16-31   1               N/A\nGPU3    SYS     SYS     NODE     X      16-31   1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1024\n",
    "labels": [],
    "state": "open",
    "created_at": "2025-05-03T09:19:18+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5995/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5995"
  },
  {
    "number": 4161,
    "title": "[Bug]  DeepSeek server crushed while using sglang.bench_serving",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI run a DeepSeek R1 server  on 2*8*H800  but it crush while i test the throuput by python3 -m sglang.bench_serving\n\nfull log:\nnohup: ignoring input\n/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\nINFO 03-07 09:19:48 __init__.py:190] Automatically detected platform cuda.\n[2025-03-07 09:19:54] server_args=ServerArgs(model_path='/data/shawn/DeepSeek-R1', tokenizer_path='/data/shawn/DeepSeek-R1', tokenizer_mode='auto', load_format='auto', trust_remote_code=True, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, quantization=None, context_length=None, device='cuda', served_model_name='/data/shawn/DeepSeek-R1', chat_template=None, is_embedding=False, revision=None, skip_tokenizer_init=False, host='0.0.0.0', port=8011, mem_fraction_static=0.7, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=16, stream_interval=1, stream_output=False, random_seed=796002056, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='sglang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr='10.0.0.15:5000', nnodes=2, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_draft_model_path=None, speculative_algorithm=None, speculative_num_steps=5, speculative_num_draft_tokens=64, speculative_eagle_topk=8, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=True, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=True, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, return_hidden_states=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=True)\n/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\n/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\n/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\n/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\n/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\n/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\n/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\n/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\n/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\nINFO 03-07 09:19:58 __init__.py:190] Automatically detected platform cuda.\nINFO 03-07 09:19:58 __init__.py:190] Automatically detected platform cuda.\nINFO 03-07 09:19:58 __init__.py:190] Automatically detected platform cuda.\nINFO 03-07 09:19:58 __init__.py:190] Automatically detected platform cuda.\nINFO 03-07 09:19:58 __init__.py:190] Automatically detected platform cuda.\nINFO 03-07 09:19:58 __init__.py:190] Automatically detected platform cuda.\nINFO 03-07 09:19:58 __init__.py:190] Automatically detected platform cuda.\nINFO 03-07 09:19:58 __init__.py:190] Automatically detected platform cuda.\nINFO 03-07 09:19:58 __init__.py:190] Automatically detected platform cuda.\n[2025-03-07 09:20:03 TP3] FlashInfer MLA optimization is turned on. Use flashinfer backend for DeepseekV3ForCausalLM.\n[2025-03-07 09:20:03 TP3] Init torch distributed begin.\n[2025-03-07 09:20:05 TP5] FlashInfer MLA optimization is turned on. Use flashinfer backend for DeepseekV3ForCausalLM.\n[2025-03-07 09:20:05 TP5] Init torch distributed begin.\n[2025-03-07 09:20:05 TP2] FlashInfer MLA optimization is turned on. Use flashinfer backend for DeepseekV3ForCausalLM.\n[2025-03-07 09:20:05 TP2] Init torch distributed begin.\n[2025-03-07 09:20:05 TP1] FlashInfer MLA optimization is turned on. Use flashinfer backend for DeepseekV3ForCausalLM.\n[2025-03-07 09:20:05 TP1] Init torch distributed begin.\n[2025-03-07 09:20:05 TP0] FlashInfer MLA optimization is turned on. Use flashinfer backend for DeepseekV3ForCausalLM.\n[2025-03-07 09:20:05 TP0] Init torch distributed begin.\n[2025-03-07 09:20:05 TP7] FlashInfer MLA optimization is turned on. Use flashinfer backend for DeepseekV3ForCausalLM.\n[2025-03-07 09:20:05 TP7] Init torch distributed begin.\n[2025-03-07 09:20:05 TP6] FlashInfer MLA optimization is turned on. Use flashinfer backend for DeepseekV3ForCausalLM.\n[2025-03-07 09:20:05 TP6] Init torch distributed begin.\n[2025-03-07 09:20:05 TP4] FlashInfer MLA optimization is turned on. Use flashinfer backend for DeepseekV3ForCausalLM.\n[2025-03-07 09:20:05 TP4] Init torch distributed begin.\n[2025-03-07 09:23:48 TP1] sglang is using nccl==2.21.5\n[2025-03-07 09:23:48 TP2] sglang is using nccl==2.21.5\n[2025-03-07 09:23:48 TP4] sglang is using nccl==2.21.5\n[2025-03-07 09:23:48 TP5] sglang is using nccl==2.21.5\n[2025-03-07 09:23:48 TP7] sglang is using nccl==2.21.5\n[2025-03-07 09:23:48 TP3] sglang is using nccl==2.21.5\n[2025-03-07 09:23:48 TP6] sglang is using nccl==2.21.5\n[2025-03-07 09:23:48 TP0] sglang is using nccl==2.21.5\n[2025-03-07 09:23:53 TP1] Custom allreduce is disabled because this process group spans across nodes.\n[2025-03-07 09:23:53 TP2] Custom allreduce is disabled because this process group spans across nodes.\n[2025-03-07 09:23:53 TP3] Custom allreduce is disabled because this process group spans across nodes.\n[2025-03-07 09:23:53 TP4] Custom allreduce is disabled because this process group spans across nodes.\n[2025-03-07 09:23:53 TP0] Custom allreduce is disabled because this process group spans across nodes.\n[2025-03-07 09:23:53 TP5] Custom allreduce is disabled because this process group spans across nodes.\n[2025-03-07 09:23:53 TP6] Custom allreduce is disabled because this process group spans across nodes.\n[2025-03-07 09:23:53 TP7] Custom allreduce is disabled because this process group spans across nodes.\n[2025-03-07 09:23:57 TP0] Load weight begin. avail mem=76.97 GB\n[2025-03-07 09:23:57 TP1] Load weight begin. avail mem=76.94 GB\n[2025-03-07 09:23:57 TP3] Load weight begin. avail mem=76.94 GB\n[2025-03-07 09:23:57 TP2] Load weight begin. avail mem=76.97 GB\n[2025-03-07 09:23:57 TP5] Load weight begin. avail mem=76.94 GB\n[2025-03-07 09:23:57 TP4] Load weight begin. avail mem=76.97 GB\n[2025-03-07 09:23:57 TP6] Load weight begin. avail mem=76.97 GB\n[2025-03-07 09:23:57 TP7] Load weight begin. avail mem=76.94 GB\n[2025-03-07 09:23:57 TP0] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-03-07 09:23:57 TP5] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-03-07 09:23:57 TP4] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-03-07 09:23:57 TP6] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-03-07 09:23:57 TP7] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-03-07 09:23:57 TP3] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-03-07 09:23:57 TP2] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n[2025-03-07 09:23:57 TP1] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\nCache shape torch.Size([163840, 64])\n..\n[2025-03-07 09:24:56 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=36.51 GB\n\nLoading safetensors checkpoint shards:  69% Completed | 113/163 [00:59<00:15,  3.25it/s]\n[2025-03-07 09:24:56 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=36.48 GB\n[2025-03-07 09:24:56 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=36.48 GB\n[2025-03-07 09:24:57 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=36.51 GB\n[2025-03-07 09:24:57 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=36.51 GB\n\nLoading safetensors checkpoint shards:  70% Completed | 114/163 [00:59<00:15,  3.22it/s]\n\nLoading safetensors checkpoint shards:  71% Completed | 115/163 [00:59<00:15,  3.18it/s]\n[2025-03-07 09:24:57 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=36.48 GB\n\n\n[2025-03-07 09:25:11 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=36.48 GB\n[2025-03-07 09:25:11 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=36.51 GB\n[2025-03-07 09:25:12 TP0] Memory pool end. avail mem=22.03 GB\n[2025-03-07 09:25:12 TP1] Memory pool end. avail mem=22.00 GB\n[2025-03-07 09:25:12 TP3] Memory pool end. avail mem=22.00 GB\n[2025-03-07 09:25:12 TP7] Memory pool end. avail mem=22.00 GB\n[2025-03-07 09:25:12 TP6] Memory pool end. avail mem=22.03 GB\n[2025-03-07 09:25:12 TP2] Memory pool end. avail mem=22.03 GB\n[2025-03-07 09:25:12 TP4] Memory pool end. avail mem=22.03 GB\n[2025-03-07 09:25:12 TP5] Memory pool end. avail mem=22.00 GB\n[2025-03-07 09:25:12 TP0] Capture cuda graph begin. This can take up to several minutes.\n[2025-03-07 09:25:12 TP1] Capture cuda graph begin. This can take up to several minutes.\n\n  0%|          | 0/23 [00:00<?, ?it/s][2025-03-07 09:25:12 TP3] Capture cuda graph begin. This can take up to several minutes.\n[2025-03-07 09:25:12 TP6] Capture cuda graph begin. This can take up to several minutes.\n[2025-03-07 09:25:12 TP7] Capture cuda graph begin. This can take up to several minutes.\n[2025-03-07 09:25:12 TP2] Capture cuda graph begin. This can take up to several minutes.\n[2025-03-07 09:25:12 TP4] Capture cuda graph begin. This can take up to several minutes.\n[2025-03-07 09:25:12 TP5] Capture cuda graph begin. This can take up to several minutes.\n2025-03-07 09:25:13,287 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64\n2025-03-07 09:25:13,398 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64\n2025-03-07 09:25:14,167 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64\n2025-03-07 09:25:14,201 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64\n2025-03-07 09:25:14,205 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64\n2025-03-07 09:25:14,205 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64\n2025-03-07 09:25:14,209 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64\n2025-03-07 09:25:14,212 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64\n2025-03-07 09:25:14,215 - INFO - flashinfer.jit: Loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64\nCache shape torch.Size([163840, 64])\n2025-03-07 09:25:14,282 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64\nCache shape torch.Size([163840, 64])\n2025-03-07 09:25:14,393 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64\nCache shape torch.Size([163840, 64])\n2025-03-07 09:25:14,508 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64\nCache shape torch.Size([163840, 64])\n2025-03-07 09:25:14,626 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64\nCache shape torch.Size([163840, 64])\n2025-03-07 09:25:14,758 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64\nCache shape torch.Size([163840, 64])\n2025-03-07 09:25:14,874 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64\nCache shape torch.Size([163840, 64])\n2025-03-07 09:25:15,022 - INFO - flashinfer.jit: Finished loading JIT ops: batch_mla_attention_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_ckv_512_head_dim_kpe_64\n[2025-03-07 09:25:24 TP3] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/layers/quantization/configs/N=576,K=7168,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-03-07 09:25:24 TP0] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/layers/quantization/configs/N=576,K=7168,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-03-07 09:25:24 TP7] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/layers/quantization/configs/N=576,K=7168,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-03-07 09:25:24 TP2] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/layers/quantization/configs/N=576,K=7168,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-03-07 09:25:24 TP4] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/layers/quantization/configs/N=576,K=7168,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-03-07 09:25:24 TP5] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/layers/quantization/configs/N=576,K=7168,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-03-07 09:25:24 TP1] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/layers/quantization/configs/N=576,K=7168,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-03-07 09:25:24 TP6] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/layers/quantization/configs/N=576,K=7168,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-03-07 09:25:33 TP4] Using default MoE config. Performance might be sub-optimal! Config file not found at /root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-03-07 09:25:33 TP7] Using default MoE config. Performance might be sub-optimal! Config file not found at /root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-03-07 09:25:33 TP1] Using default MoE config. Performance might be sub-optimal! Config file not found at /root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-03-07 09:25:33 TP5] Using default MoE config. Performance might be sub-optimal! Config file not found at /root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-03-07 09:25:33 TP2] Using default MoE config. Performance might be sub-optimal! Config file not found at /root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-03-07 09:25:33 TP6] Using default MoE config. Performance might be sub-optimal! Config file not found at /root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-03-07 09:25:33 TP0] Using default MoE config. Performance might be sub-optimal! Config file not found at /root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128, 128].json\n[2025-03-07 09:25:33 TP3] Using default MoE config. Performance might be sub-optimal! Config file not found at /root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=128,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128, 128].json\n\n \n[2025-03-07 09:31:43 TP6] Capture cuda graph end. Time elapsed: 391.00 s\n[2025-03-07 09:31:43 TP5] Capture cuda graph end. Time elapsed: 390.99 s\n[2025-03-07 09:31:43 TP1] Capture cuda graph end. Time elapsed: 391.01 s\n[2025-03-07 09:31:43 TP7] Capture cuda graph end. Time elapsed: 391.00 s\n[2025-03-07 09:31:43 TP4] Capture cuda graph end. Time elapsed: 390.99 s\n[2025-03-07 09:31:43 TP0] Capture cuda graph end. Time elapsed: 391.01 s\n[2025-03-07 09:31:43 TP4] max_total_num_tokens=201681, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=163840\n[2025-03-07 09:31:43 TP5] max_total_num_tokens=201681, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=163840\n[2025-03-07 09:31:43 TP0] max_total_num_tokens=201681, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=163840\n[2025-03-07 09:31:43 TP6] max_total_num_tokens=201681, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=163840\n[2025-03-07 09:31:43 TP2] max_total_num_tokens=201681, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=163840\n[2025-03-07 09:31:43 TP1] max_total_num_tokens=201681, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=163840\n[2025-03-07 09:31:43 TP7] max_total_num_tokens=201681, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=163840\n[2025-03-07 09:31:43 TP3] max_total_num_tokens=201681, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=163840\n[2025-03-07 09:31:43] INFO:     Started server process [182575]\n[2025-03-07 09:31:43] INFO:     Waiting for application startup.\n[2025-03-07 09:31:43] INFO:     Application startup complete.\n[2025-03-07 09:31:43] INFO:     Uvicorn running on http://0.0.0.0:8011 (Press CTRL+C to quit)\n[2025-03-07 09:31:44] INFO:     127.0.0.1:43948 - \"GET /get_model_info HTTP/1.1\" 200 OK\n[2025-03-07 09:31:44 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n[2025-03-07 09:31:47] INFO:     127.0.0.1:43950 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 09:31:47] The server is fired up and ready to roll!\n[2025-03-07 11:02:37 TP0] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n[2025-03-07 11:02:38 TP0] Decode batch. #running-req: 1, #token: 44, token usage: 0.00, gen throughput (token/s): 0.01, #queue-req: 0\n....\n[2025-03-07 11:24:01 TP0] Decode batch. #running-req: 1, #token: 3709, token usage: 0.02, gen throughput (token/s): 30.03, #queue-req: 0\n[2025-03-07 11:24:03 TP0] Decode batch. #running-req: 1, #token: 3749, token usage: 0.02, gen throughput (token/s): 29.41, #queue-req: 0\n[2025-03-07 11:24:04 TP0] Decode batch. #running-req: 1, #token: 3789, token usage: 0.02, gen throughput (token/s): 30.27, #queue-req: 0\n[2025-03-07 11:24:05 TP0] Decode batch. #running-req: 1, #token: 3829, token usage: 0.02, gen throughput (token/s): 31.11, #queue-req: 0\n[2025-03-07 11:24:06] INFO:     127.0.0.1:54102 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-03-07 11:36:34] INFO:     127.0.0.1:55268 - \"GET /v1/models HTTP/1.1\" 200 OK\n[2025-03-07 11:37:24] INFO:     127.0.0.1:55346 - \"GET /v1/models HTTP/1.1\" 200 OK\n[2025-03-07 11:40:49] INFO:     127.0.0.1:55642 - \"GET /v1/models HTTP/1.1\" 200 OK\n[2025-03-07 11:49:20] INFO:     3.142.131.47:36806 - \"GET / HTTP/1.1\" 404 Not Found\n[2025-03-07 11:49:20] INFO:     3.142.131.47:36810 - \"GET / HTTP/1.1\" 404 Not Found\n[2025-03-07 11:51:15] INFO:     127.0.0.1:56550 - \"GET /v1/models HTTP/1.1\" 200 OK\n[2025-03-07 11:51:18] INFO:     127.0.0.1:56560 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:51:19 TP0] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n[2025-03-07 11:51:19 TP0] Decode batch. #running-req: 1, #token: 33, token usage: 0.00, gen throughput (token/s): 0.02, #queue-req: 0\n[2025-03-07 11:51:20] INFO:     127.0.0.1:56564 - \"POST /flush_cache HTTP/1.1\" 200 OK\n[2025-03-07 11:51:20 TP0] Cache not flushed because there are pending requests. #queue-req: 0, #running-req: 1\n[2025-03-07 11:51:20 TP1] Cache not flushed because there are pending requests. #queue-req: 0, #running-req: 1\n[2025-03-07 11:51:20 TP2] Cache not flushed because there are pending requests. #queue-req: 0, #running-req: 1\n[2025-03-07 11:51:20 TP4] Cache not flushed because there are pending requests. #queue-req: 0, #running-req: 1\n[2025-03-07 11:51:20 TP6] Cache not flushed because there are pending requests. #queue-req: 0, #running-req: 1\n[2025-03-07 11:51:20 TP3] Cache not flushed because there are pending requests. #queue-req: 0, #running-req: 1\n[2025-03-07 11:51:20 TP7] Cache not flushed because there are pending requests. #queue-req: 0, #running-req: 1\n[2025-03-07 11:51:20 TP5] Cache not flushed because there are pending requests. #queue-req: 0, #running-req: 1\n[2025-03-07 11:51:21] INFO:     127.0.0.1:56568 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:51:21 TP0] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n[2025-03-07 11:51:21] INFO:     127.0.0.1:56570 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:51:21 TP0] Prefill batch. #new-seq: 1, #new-token: 31, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 1, #queue-req: 0\n[2025-03-07 11:51:21] INFO:     127.0.0.1:56572 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:51:21] INFO:     127.0.0.1:56574 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:51:21 TP0] Prefill batch. #new-seq: 1, #new-token: 587, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 2, #queue-req: 0\n[2025-03-07 11:51:21] INFO:     127.0.0.1:56576 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:51:21] INFO:     127.0.0.1:56578 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:51:22] INFO:     127.0.0.1:56580 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:51:22 TP0] Prefill batch. #new-seq: 4, #new-token: 605, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 3, #queue-req: 0\n[2025-03-07 11:51:22] INFO:     127.0.0.1:56582 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:51:22] INFO:     127.0.0.1:56584 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:51:22 TP0] Prefill batch. #new-seq: 2, #new-token: 593, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.01, #running-req: 7, #queue-req: 0\n[2025-03-07 11:51:22] INFO:     127.0.0.1:56586 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:51:22 TP0] Prefill batch. #new-seq: 1, #new-token: 140, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.01, #running-req: 9, #queue-req: 0\n[2025-03-07 11:51:22] INFO:     127.0.0.1:56588 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:51:22 TP0] Prefill batch. #new-seq: 1, #new-token: 2176, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.01, #running-req: 10, #queue-req: 0\nFatal Python error: Segmentation fault\n\nThread 0x00007fb5735f7700 (most recent call first):\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 462 in watchdog_thread\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/threading.py\", line 989 in run\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/threading.py\", line 1052 in _bootstrap_inner\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/threading.py\", line 1009 in _bootstrap\n\nCurrent thread 0x00007fb573df8700 (most recent call first):\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py\", line 2501 in all_reduce\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/torch/distributed/c10d_logger.py\", line 83 in wrapper\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/distributed/parallel_state.py\", line 414 in _all_reduce_in_place\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/distributed/parallel_state.py\", line 112 in inplace_all_reduce\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/torch/_ops.py\", line 1116 in __call__\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/distributed/parallel_state.py\", line 398 in all_reduce\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/distributed/communication_op.py\", line 13 in tensor_model_parallel_all_reduce\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py\", line 183 in forward\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747 in _call_impl\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736 in _wrapped_call_impl\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py\", line 790 in forward\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747 in _call_impl\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736 in _wrapped_call_impl\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py\", line 835 in forward\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747 in _call_impl\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736 in _wrapped_call_impl\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/models/deepseek_v2.py\", line 874 in forward\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 761 in forward_extend\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 796 in forward\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py\", line 164 in forward_batch_generation\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 140 in forward_thread_func_\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 109 in forward_thread_func\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/threading.py\", line 989 in run\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/threading.py\", line 1052 in _bootstrap_inner\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/threading.py\", line 1009 in _bootstrap\n\nThread 0x00007fc4ebfc7700 (most recent call first):\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/threading.py\", line 338 in wait\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/threading.py\", line 634 in wait\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/threading.py\", line 1052 in _bootstrap_inner\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/threading.py\", line 1009 in _bootstrap\n\nThread 0x00007fc4eafc5700 (most recent call first):\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/threading.py\", line 338 in wait\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/threading.py\", line 634 in wait\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/threading.py\", line 1052 in _bootstrap_inner\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/threading.py\", line 1009 in _bootstrap\n\nThread 0x00007fca25f3f400 (most recent call first):\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/threading.py\", line 334 in wait\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/threading.py\", line 634 in wait\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 171 in resolve_batch_result\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 1148 in process_batch_result_prefill\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 1120 in process_batch_result\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 519 in event_loop_overlap\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 1825 in run_scheduler_process\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/multiprocessing/process.py\", line 108 in run\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/multiprocessing/process.py\", line 314 in _bootstrap\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/multiprocessing/spawn.py\", line 135 in _main\n  File \"/root/miniconda3/envs/deepseekr1/lib/python3.12/multiprocessing/spawn.py\", line 122 in spawn_main\n  File \"<string>\", line 1 in <module>\n\nExtension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, uvloop.loop, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, psutil._psutil_linux, psutil._psutil_posix, setproctitle, zmq.backend.cython._zmq, yaml._yaml, markupsafe._speedups, PIL._imaging, PIL._imagingft, msgspec._core, msgpack._cmsgpack, google._upb._message, ray._raylet, sentencepiece._sentencepiece, regex._regex, cuda_utils, __triton_launcher (total: 52)\n[2025-03-07 11:51:22] INFO:     127.0.0.1:56590 - \"POST /generate HTTP/1.1\" 200 OK\n.....\n[2025-03-07 11:52:04] INFO:     127.0.0.1:56942 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:04] INFO:     127.0.0.1:56944 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:05] INFO:     127.0.0.1:56946 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:06] INFO:     127.0.0.1:56950 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:06] INFO:     127.0.0.1:56952 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:06] INFO:     127.0.0.1:56954 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:06] INFO:     127.0.0.1:56958 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:07] INFO:     127.0.0.1:56962 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:07] INFO:     127.0.0.1:56964 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:07] INFO:     127.0.0.1:56966 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:07] INFO:     127.0.0.1:56968 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:07] INFO:     127.0.0.1:56970 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:07] INFO:     127.0.0.1:56972 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:08] INFO:     127.0.0.1:56974 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:08] INFO:     127.0.0.1:56976 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:08] INFO:     127.0.0.1:56978 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:09] INFO:     127.0.0.1:56980 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:10] INFO:     127.0.0.1:56984 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:10] INFO:     127.0.0.1:56986 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:10] INFO:     127.0.0.1:56988 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:10] INFO:     127.0.0.1:56992 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:11] INFO:     127.0.0.1:56994 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:11] INFO:     127.0.0.1:56996 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:52:11] INFO:     127.0.0.1:56998 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:53:18] INFO:     127.0.0.1:57102 - \"GET /v1/models HTTP/1.1\" 200 OK\n[2025-03-07 11:53:29] WARNING:  Invalid HTTP request received.\n[2025-03-07 11:53:55] INFO:     127.0.0.1:57162 - \"GET /v1/models HTTP/1.1\" 200 OK\n[2025-03-07 11:54:00] INFO:     127.0.0.1:57172 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-03-07 11:54:39] WARNING:  Invalid HTTP request received.\n[2025-03-07 11:55:05] INFO:     220.196.160.117:42528 - \"GET / HTTP/1.1\" 404 Not Found\n[2025-03-07 11:55:07] INFO:     220.196.160.51:40966 - \"GET / HTTP/1.1\" 404 Not Found\n[2025-03-07 11:55:40] WARNING:  Invalid HTTP request received.\n[2025-03-07 11:56:43 TP2] Watchdog timeout (self.watchdog_timeout=300)\n[2025-03-07 11:56:43 TP4] Watchdog timeout (self.watchdog_timeout=300)\n[2025-03-07 11:56:43 TP1] Watchdog timeout (self.watchdog_timeout=300)\n[2025-03-07 11:56:43 TP7] Watchdog timeout (self.watchdog_timeout=300)\n[2025-03-07 11:56:43 TP0] Watchdog timeout (self.watchdog_timeout=300)\n[2025-03-07 11:56:43 TP3] Watchdog timeout (self.watchdog_timeout=300)\n[2025-03-07 11:56:43 TP5] Watchdog timeout (self.watchdog_timeout=300)\n[2025-03-07 11:56:48] Received sigquit from a child proces. It usually means the child failed.\n[2025-03-07 11:56:48] Received sigquit from a child proces. It usually means the child failed.\n[2025-03-07 11:56:48] Received sigquit from a child proces. It usually means the child failed.\n[2025-03-07 11:56:48] Received sigquit from a child proces. It usually means the child failed.\n\n\n\n\n### Reproduction\n\ncommand:\n```\n# node 1 \nnohup python3 -m sglang.launch_server --model-path /data/shawn/DeepSeek-R1 --tp 16 --dist-init-addr 10.0.0.15:5000 --nnodes 2 --node-rank 0 --host 0.0.0.0 --port 8011 --trust-remote-code --enable-flashinfer-mla --disable-radix-cache --enable-torch-compile --mem-fraction-static 0.7 > data/infer_logs/ds_r1_307_df_01.log 2>&1 &\n# node 2\nnohup python3 -m sglang.launch_server --model-path /data/shawn/DeepSeek-R1 --tp 16 --dist-init-addr 10.0.0.15:5000 --nnodes 2 --node-rank 0 --host 0.0.0.0 --port 8011 --trust-remote-code --enable-flashinfer-mla --disable-radix-cache --enable-torch-compile --mem-fraction-static 0.7 > data/infer_logs/ds_r1_307_df_01.log 2>&1 &\n```\n\n\ncommand:\n```\n# node 1 \nnohup python3 -m sglang.launch_server --model-path /data/shawn/DeepSeek-R1 --tp 16 --dist-init-addr 10.0.0.15:5000 --nnodes 2 --node-rank 0 --host 0.0.0.0 --port 8011 --trust-remote-code --enable-flashinfer-mla --disable-radix-cache --enable-torch-compile --mem-fraction-static 0.7 > data/infer_logs/ds_r1_307_df_01.log 2>&1 &\n# node 2\nnohup python3 -m sglang.launch_server --model-path /data/shawn/DeepSeek-R1 --tp 16 --dist-init-addr 10.0.0.15:5000 --nnodes 2 --node-rank 0 --host 0.0.0.0 --port 8011 --trust-remote-code --enable-flashinfer-mla --disable-radix-cache --enable-torch-compile --mem-fraction-static 0.7 > data/infer_logs/ds_r1_307_df_01.log 2>&1 &\n```\n\n\ntest throuput at node 1 by:\npython3 -m sglang.bench_serving   --backend sglang   --dataset-name random   --random-range-ratio 1.0   --random-input-len 2600   --random-output-len 1200   --num-prompts 180   --max-concurrency 180   --host 0.0.0.0   --port 8011 --request-rate 3.5 --dataset-path /data/shawn/benchmark/ShareGPT_V3_unfiltered_cleaned_split.json --dataset-name random\n\n\n\n\n\n\n### Environment\n\nnode1:\n(deepseekr1) [root@VM-0-15-tencentos /]# python3 -m sglang.check_env\nINFO 03-07 12:50:05 __init__.py:190] Automatically detected platform cuda.\nPython: 3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 17:29:18) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H800\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.216.01\nPyTorch: 2.5.1+cu124\nsglang: 0.4.3.post2\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.2.post1+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.13\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.1\ninteregular: 0.3.3\nmodelscope: 1.23.1\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.65.2\ntiktoken: 0.9.0\nanthropic: 0.49.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    CPU Affinity      NUMA Affinity   GPU NUMA ID\nGPU0     X      NV8     NV8     NV8     NV8     NV8     NV8     NV8     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-85      0               N/A\nGPU1    NV8      X      NV8     NV8     NV8     NV8     NV8     NV8     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-85      0               N/A\nGPU2    NV8     NV8      X      NV8     NV8     NV8     NV8     NV8     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     0-85      0               N/A\nGPU3    NV8     NV8     NV8      X      NV8     NV8     NV8     NV8     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     0-85      0               N/A\nGPU4    NV8     NV8     NV8     NV8      X      NV8     NV8     NV8     SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    86-171    1               N/A\nGPU5    NV8     NV8     NV8     NV8     NV8      X      NV8     NV8     SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    86-171    1               N/A\nGPU6    NV8     NV8     NV8     NV8     NV8     NV8      X      NV8     SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    86-171    1               N/A\nGPU7    NV8     NV8     NV8     NV8     NV8     NV8     NV8      X      SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     86-171    1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC1    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     SYS     SYS     SYS\nNIC2    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS     SYS     SYS     SYS\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_bond_0\n  NIC1: mlx5_bond_1\n  NIC2: mlx5_bond_2\n  NIC3: mlx5_bond_3\n  NIC4: mlx5_bond_4\n  NIC5: mlx5_bond_5\n  NIC6: mlx5_bond_6\n  NIC7: mlx5_bond_7\n\n\nHypervisor vendor: KVM\nulimit soft: 100002\n\n\nnode2:\n\n(deepseekr1) [root@VM-0-12-tencentos benchmark]# python3 -m sglang.check_env\n[1]+  Killed                  nohup python3 -m sglang.launch_server --model-path /data/shawn/DeepSeek-R1 --tp 16 --dist-init-addr 10.0.0.15:5000 --nnodes 2 --node-rank 1 --host 0.0.0.0 --port 8011 --trust-remote-code --enable-flashinfer-mla --disable-radix-cache --enable-torch-compile --mem-fraction-static 0.7 > infer_logs/ds_r1_307_df_01.log 2>&1  (wd: /data)\n(wd now: /data/shawn/benchmark)\nINFO 03-07 12:50:14 __init__.py:190] Automatically detected platform cuda.\nPython: 3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 17:29:18) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H800\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.216.01\nPyTorch: 2.5.1+cu124\nsglang: 0.4.3.post2\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.2.post1+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.13\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.1\ninteregular: 0.3.3\nmodelscope: 1.23.1\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.65.2\ntiktoken: 0.9.0\nanthropic: 0.49.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5   NIC6     NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV8     NV8     NV8     NV8     NV8     NV8     NV8     PIX     NODE    NODE    NODE    SYS     SYS    SYS      SYS     0-85    0               N/A\nGPU1    NV8      X      NV8     NV8     NV8     NV8     NV8     NV8     NODE    PIX     NODE    NODE    SYS     SYS    SYS      SYS     0-85    0               N/A\nGPU2    NV8     NV8      X      NV8     NV8     NV8     NV8     NV8     NODE    NODE    PIX     NODE    SYS     SYS    SYS      SYS     0-85    0               N/A\nGPU3    NV8     NV8     NV8      X      NV8     NV8     NV8     NV8     NODE    NODE    NODE    PIX     SYS     SYS    SYS      SYS     0-85    0               N/A\nGPU4    NV8     NV8     NV8     NV8      X      NV8     NV8     NV8     SYS     SYS     SYS     SYS     PIX     NODE   NODE     NODE    86-171  1               N/A\nGPU5    NV8     NV8     NV8     NV8     NV8      X      NV8     NV8     SYS     SYS     SYS     SYS     NODE    PIX    NODE     NODE    86-171  1               N/A\nGPU6    NV8     NV8     NV8     NV8     NV8     NV8      X      NV8     SYS     SYS     SYS     SYS     NODE    NODE   PIX      NODE    86-171  1               N/A\nGPU7    NV8     NV8     NV8     NV8     NV8     NV8     NV8      X      SYS     SYS     SYS     SYS     NODE    NODE   NODE     PIX     86-171  1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS    SYS      SYS\nNIC1    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     SYS    SYS      SYS\nNIC2    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS     SYS    SYS      SYS\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS    SYS      SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE   NODE     NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X     NODE     NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE    X       NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE   NODE      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_bond_0\n  NIC1: mlx5_bond_1\n  NIC2: mlx5_bond_2\n  NIC3: mlx5_bond_3\n  NIC4: mlx5_bond_4\n  NIC5: mlx5_bond_5\n  NIC6: mlx5_bond_6\n  NIC7: mlx5_bond_7\n\n\nHypervisor vendor: KVM\nulimit soft: 100002\n",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-07T05:04:18+00:00",
    "closed_at": "2025-03-10T07:33:20+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4161/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4161"
  },
  {
    "number": 1065,
    "title": "[Bug] Unable to run deepseek-v2",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n\n### Describe the bug\n\nWhen I run deepseek-v2-lite with sglang, an error accured:\r\n`  File \"/opt/tiger/sglang/python/sglang/srt/managers/tp_worker.py\", line 452, in forward_prefill_batch\r\n    output = self.model_runner.forward(batch, ForwardMode.EXTEND)\r\n  File \"/opt/tiger/sglang/python/sglang/srt/model_executor/model_runner.py\", line 397, in forward\r\n    return self.forward_extend(batch)\r\n  File \"/home/tiger/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/tiger/sglang/python/sglang/srt/model_executor/model_runner.py\", line 373, in forward_extend\r\n    return self.model.forward(\r\nTypeError: DeepseekV2ForCausalLM.forward() missing 1 required positional argument: 'attn_metadata'`\r\n\r\nAnd we can see that the signature of `forward_extend` in `model_runner.py` is not compatible with  vllm deepseek_v2 https://github.com/vllm-project/vllm/blob/774cd1d3bf7890c6abae6c7ace798c4a376b2b20/vllm/model_executor/models/deepseek_v2.py#L496\n\n### Reproduction\n\ncommand: `python3 -m sglang.launch_server --model-path=/opt/tiger/models/dsv2 --port=30000 --trust-remote-code --disable-radix-cache --enable-mla`\r\n\n\n### Environment\n\nvllm 0.5.4+cu122\r\nsglang 0.2.11",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-13T00:34:06+00:00",
    "closed_at": "2024-08-13T22:23:17+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1065/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1065"
  },
  {
    "number": 6590,
    "title": "[Bug]  main pd version Exception: Failed to encode tensor map: 700",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n```\n[2025-05-25 19:45:03] [ERROR] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/usr/local/src/sglang/python/sglang/srt/managers/scheduler.py\", line 2346, in run_scheduler_process\n    scheduler.event_loop_overlap_disagg_decode()\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/src/sglang/python/sglang/srt/disaggregation/decode.py\", line 558, in event_loop_overlap_disagg_decode\n    self.process_batch_result(tmp_batch, tmp_result)\n  File \"/usr/local/src/sglang/python/sglang/srt/managers/scheduler.py\", line 1626, in process_batch_result\n    self.process_batch_result_decode(batch, result, launch_done)\n  File \"/usr/local/src/sglang/python/sglang/srt/managers/scheduler_output_processor_mixin.py\", line 200, in process_batch_result_decode\n    self.tp_worker.resolve_last_batch_result(launch_done)\n  File \"/usr/local/src/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 193, in resolve_last_batch_result\n    copy_done.synchronize()\n  File \"/usr/local/lib/python3.10/site-packages/torch/cuda/streams.py\", line 224, in synchronize\n    super().synchronize()\nRuntimeError: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\n[2025-05-25 19:45:03] [ERROR] TpModelWorkerClient hit an exception: Traceback (most recent call last):\n  File \"/usr/local/src/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 118, in forward_thread_func\n    self.forward_thread_func_()\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/src/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 151, in forward_thread_func_\n    self.worker.forward_batch_generation(\n  File \"/usr/local/src/sglang/python/sglang/srt/managers/tp_worker.py\", line 202, in forward_batch_generation\n    logits_output, can_run_cuda_graph = self.model_runner.forward(\n  File \"/usr/local/src/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1185, in forward\n    output = self._forward_raw(\n  File \"/usr/local/src/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1212, in _forward_raw\n    ret = self.forward_decode(forward_batch, pp_proxy_tensors=pp_proxy_tensors)\n  File \"/usr/local/src/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1133, in forward_decode\n    return self.model.forward(\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/src/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1704, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/src/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1608, in forward\n    hidden_states, residual = layer(\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/src/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1449, in forward\n    hidden_states = self.self_attn(\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/src/sglang/python/sglang/srt/models/deepseek_v2.py\", line 795, in forward\n    return self.forward_core(s)\n  File \"/usr/local/src/sglang/python/sglang/srt/models/deepseek_v2.py\", line 844, in forward_core\n    return self.forward_absorb_core(*inner_state)\n  File \"/usr/local/src/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1028, in forward_absorb_core\n    output, _ = self.o_proj(attn_output)\n                                                            \n  File \"/usr/local/src/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1449, in forward\n    hidden_states = self.self_attn(\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/src/sglang/python/sglang/srt/models/deepseek_v2.py\", line 795, in forward\n    return self.forward_core(s)\n  File \"/usr/local/src/sglang/python/sglang/srt/models/deepseek_v2.py\", line 844, in forward_core\n    return self.forward_absorb_core(*inner_state)\n  File \"/usr/local/src/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1028, in forward_absorb_core\n    output, _ = self.o_proj(attn_output)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/src/sglang/python/sglang/srt/layers/linear.py\", line 1291, in forward\n    output_parallel = self.quant_method.apply(self, input_parallel, bias=bias_)\n  File \"/usr/local/src/sglang/python/sglang/srt/layers/quantization/fp8.py\", line 420, in apply\n    return apply_w8a8_block_fp8_linear(\n  File \"/usr/local/src/sglang/python/sglang/srt/layers/quantization/fp8_utils.py\", line 173, in apply_w8a8_block_fp8_linear\n    output = w8a8_block_fp8_matmul(\n  File \"/usr/local/src/sglang/python/sglang/srt/layers/quantization/fp8_kernel.py\", line 786, in w8a8_block_fp8_matmul\n    torch.ops.sglang.deep_gemm_fp8_fp8_bf16_nt(A, As, B, Bs, C)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_ops.py\", line 1123, in __call__\n    return self._op(*args, **(kwargs or {}))\n  File \"/usr/local/src/sglang/python/sglang/srt/layers/quantization/fp8_kernel.py\", line 80, in deep_gemm_fp8_fp8_bf16_nt\n    deep_gemm_gemm_nt_f8f8bf16((A, As), (B, Bs), C)\n  File \"/usr/local/src/sglang/python/sglang/srt/layers/quantization/deep_gemm.py\", line 368, in gemm_nt_f8f8bf16\n    deep_gemm.gemm_fp8_fp8_bf16_nt(lhs, rhs, out)\n  File \"/usr/local/lib/python3.10/site-packages/deep_gemm/jit_kernels/gemm.py\", line 196, in gemm_fp8_fp8_bf16_nt\n    tensor_map_a = make_2d_tma_a_desc(\n  File \"/usr/local/lib/python3.10/site-packages/deep_gemm/jit_kernels/runtime.py\", line 107, in make_2d_tma_a_desc\n    return make_2d_tma_desc(global_address, Layout.RowMajor,\n  File \"/usr/local/lib/python3.10/site-packages/deep_gemm/jit_kernels/runtime.py\", line 96, in make_2d_tma_desc\n    return make_2d_tma_copy_desc(global_address, gmem_dim, cbd.cuuint64_t(gmem_cols * global_address.element_size()), smem_dim, swizzle_type)\n  File \"/usr/local/lib/python3.10/site-packages/deep_gemm/jit_kernels/runtime.py\", line 85, in make_2d_tma_copy_desc\n    raise Exception(f'Failed to encode tensor map: {res}')\nException: Failed to encode tensor map: 700\n\nDuring handling of the above exception, another exception occurred:\n```\n\n\n\n### Reproduction\n\nmain branch \npd  \ndeepseekv3 0324\nprefill tp16 dp4\ndecode tp32 dp32\n\n### Environment\n\nmain branch \npd  \ndeepseekv3 0324",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-25T11:48:43+00:00",
    "closed_at": "2025-07-05T05:28:57+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6590/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6590"
  },
  {
    "number": 6006,
    "title": "[RFC] sm75 EOL",
    "body": "The SGLang team plans to deprecate support for sm75 in v0.5. If you\u2019re still using SGLang for large-scale inference acceleration on sm75 devices in production, please let us know so we can defer this deprecation beyond v0.5.",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-05-04T06:12:20+00:00",
    "closed_at": "2025-07-17T00:21:12+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6006/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/6006"
  },
  {
    "number": 6000,
    "title": "[Bug] Tensor model parallel group is not initialized when deploying Qwen3-30B-A3B-AWQ",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHi, SGLang Team,\n\nI am using it to deploy an AWQ quantized model of Qwen3-30B-A3B: swift/Qwen3-30B-A3B-AWQ from modelscope. but encounter the following issue:\n\n```bash\n File \"/home/a/sglang/python/sglang/srt/managers/scheduler.py\", line 2215, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, pp_rank, dp_rank)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/a/sglang/python/sglang/srt/managers/scheduler.py\", line 268, in __init__\n    self.tp_worker = TpWorkerClass(\n                     ^^^^^^^^^^^^^^\n  File \"/home/a/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 64, in __init__\n    self.worker = TpModelWorker(\n                  ^^^^^^^^^^^^^^\n  File \"/home/a/sglang/python/sglang/srt/managers/tp_worker.py\", line 81, in __init__\n    self.model_runner = ModelRunner(\n                        ^^^^^^^^^^^^\n  File \"/home/a/sglang/python/sglang/srt/model_executor/model_runner.py\", line 190, in __init__\n    self.initialize(min_per_gpu_memory)\n  File \"/home/a/sglang/python/sglang/srt/model_executor/model_runner.py\", line 205, in initialize\n    self.load_model()\n  File \"/home/a/sglang/python/sglang/srt/model_executor/model_runner.py\", line 458, in load_model\n    self.model = get_model(\n                 ^^^^^^^^^^\n  File \"/home/a/sglang/python/sglang/srt/model_loader/__init__.py\", line 22, in get_model\n    return loader.load_model(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/a/sglang/python/sglang/srt/model_loader/loader.py\", line 377, in load_model\n    model.load_weights(self._get_all_weights(model_config, model))\n  File \"/home/a/sglang/python/sglang/srt/models/qwen3_moe.py\", line 406, in load_weights\n    weight_loader(\n  File \"/data2/a/miniconda3/envs/cap/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/moe_wna16.py\", line 396, in moe_wna16_weight_loader\n    device = get_tp_group().device\n             ^^^^^^^^^^^^^^\n  File \"/data2/a/miniconda3/envs/cap/lib/python3.11/site-packages/vllm/distributed/parallel_state.py\", line 749, in get_tp_group\n    assert _TP is not None, (\"tensor model parallel group is not initialized\")\n           ^^^^^^^^^^^^^^^\nAssertionError: tensor model parallel group is not initialized\n```\n\n### Reproduction\n\nRunning Command:\n```bash\nmodelscope download swift/Qwen3-30B-A3B-AWQ\n\npython3 -m sglang.launch_server --model-path /home/a/.cache/modelscope/hub/models/swift/Qwen3-30B-A3B-AWQ --attention-backend flashinfer --mem-fraction-static 0.9\n```\n\n### Environment\n\nSglang 0.4.6.post2, 8xH20\n\nPython: 3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.99\nCUDA Driver Version: 535.161.07\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post2\nsgl_kernel: 0.1.1\nflashinfer_python: 0.2.5\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.10.0\nnumpy: 2.2.5\naiohttp: 3.11.18\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.4\npython-multipart: 0.0.20\npyzmq: 26.4.0\nuvicorn: 0.34.2\nuvloop: 0.21.0\nvllm: 0.8.4\nxgrammar: 0.1.18\nopenai: 1.75.0\ntiktoken: 0.9.0\nanthropic: 0.50.0\nlitellm: 1.67.6\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     PHB     NODE    SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    PHB     PIX     NODE    SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    96-191,288-383  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    96-191,288-383  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE    NODE    PIX     PHB     96-191,288-383  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     NODE    NODE    PHB     PIX     96-191,288-383  1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC1    NODE    PIX     PHB     NODE    SYS     SYS     SYS     SYS     NODE     X      PHB     NODE    SYS     SYS     SYS     SYS\nNIC2    NODE    PHB     PIX     NODE    SYS     SYS     SYS     SYS     NODE    PHB      X      NODE    SYS     SYS     SYS     SYS\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PIX     PHB     SYS     SYS     SYS     SYS     NODE    NODE     X      PHB\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    PHB     PIX     SYS     SYS     SYS     SYS     NODE    NODE    PHB      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_bond_0\n  NIC1: mlx5_bond_1\n  NIC2: mlx5_bond_2\n  NIC3: mlx5_bond_3\n  NIC4: mlx5_bond_4\n  NIC5: mlx5_bond_5\n  NIC6: mlx5_bond_6\n  NIC7: mlx5_bond_7\n\n\nulimit soft: 1000000\n",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-05-04T01:08:45+00:00",
    "closed_at": "2025-07-16T00:20:45+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6000/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6000"
  },
  {
    "number": 4580,
    "title": "[Docs] EAGLE3 docs",
    "body": "As discussed [here](https://github.com/sgl-project/sglang/pull/4247) we want to update the docs for speculative decoding to incooperate the new parts on EAGLE3.\n@zhaochenyang20 I will take this issue.",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-19T10:44:37+00:00",
    "closed_at": "2025-05-13T15:21:50+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4580/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4580"
  },
  {
    "number": 3427,
    "title": "[Bug] GGUF tokenizations issues",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nSpecial tokens and BOS/EOS tokens are not used, even when they are required in metadata, chat template components that usually take 1 token are treated as multiple tokens,  leading to output quality degradation. Instead of  `<\uff5cUser\uff5c>`  we get `<`, `\uff5c`, `User`, `\uff5c`,` >`. \n\nOriginally discussed here https://github.com/sgl-project/sglang/issues/1616#issuecomment-2642242758\nCreating a new issue to track the status of the bug.\n\n\n### Reproduction\n\n- Launch the server and watch the logs\n`python -m sglang.launch_server --host 127.0.0.1 --port 30000 --model-path=/models/llms/DeepSeek-R1-Distill-Qwen-32B-Q2_K.gguf --quantization gguf --log-requests --context-length 2048`\n\n- Send a chat completions requrest\n```bash\ncurl --request POST \\\n  --url http://127.0.0.1:30000/v1/chat/completions \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n\t\"model\": \"/models/llms/DeepSeek-R1-Distill-Qwen-32B-Q2_K.gguf\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hi\"}]\n}'\n```\n- See this in the logs \n`Finish: obj=GenerateReqInput(text=None, input_ids=[27, 130957, 1474, 130957, 29, 13048, 27, 130957, 71703, 130957, 29]`\n\n<hr>\n\n- Try the same model with different quantization (I tried both GPTQ and AWQ)\n`python -m sglang.launch_server --host 127.0.0.1 --port 30000 --model-path=/models/llms/DeepSeek-R1-Distill-Qwen-32B-GPTQ_4bit-128g --quantization gptq_marlin --log-requests  --context-length 2048`\n\n- Send the same request\n\n- See this in the logs (expected output)\n`Finish: obj=GenerateReqInput(text=None, input_ids=[151646, 151644, 13048, 151645]`\n\nI'm sure that this issue is not related to specific model file, the same file tested in llama.cpp server returns correct tokens.\n\nModels used: \nGGUF: https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-32B-Q2_K.gguf\nGPTQ: https://huggingface.co/avoroshilov/DeepSeek-R1-Distill-Qwen-32B-GPTQ_4bit-128g\n\n\n\n### Environment\n\nPython: 3.11.10 | packaged by conda-forge | (main, Oct 16 2024, 01:27:36) [GCC 13.3.0]\nCUDA available: True\nGPU 0: NVIDIA GeForce RTX 4090\nGPU 0 Compute Capability: 8.9\nCUDA_HOME: None\nPyTorch: 2.5.1+cu124\nsglang: 0.4.2.post2\nflashinfer: 0.2.0.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.2\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.1\npsutil: 6.1.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.51.2\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-23    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-02-09T13:10:43+00:00",
    "closed_at": "2025-05-02T00:18:43+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3427/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3427"
  },
  {
    "number": 3385,
    "title": "[Bug] ValueError: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed!",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI was chatting with @zhaochenyang20 and @shuaills about this issue in slack, [the Slack thread can be found here](https://sgl-fru7574.slack.com/archives/C064NB2TAP9/p1738960365156639).\n\n> **@shuaills asked me to raise this issue.**\n\nWhen running `meta-llama/Llama-3.2-1B` and performing an inference, the engine throws the following exception:\n\n```\n[SglangSidecar]: 2025-02-07T21:37:23.710Z 1 stderr: [2025-02-07 13:37:23] ERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/openai_api/adapter.py\", line 911, in v1_chat_generate_request\n    prompt_ids = tokenizer_manager.tokenizer.apply_chat_template(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 1621, in apply_chat_template\n    chat_template = self.get_chat_template(chat_template, tools)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 1789, in get_chat_template\n    raise ValueError(\nValueError: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating\n\nDuring handling of the above exception, another exception occurred:\n```\n\nHere's the `tokenizer.chat_template` for the model:\n\n```\nPython 3.10.16 (main, Dec  4 2024, 08:53:37) [GCC 9.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50.5k/50.5k [00:00<00:00, 14.3MB/s]\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.09M/9.09M [00:00<00:00, 21.7MB/s]\nspecial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 301/301 [00:00<00:00, 4.21MB/s]\n>>> print(tokenizer.chat_template)  # is it none here?\nNone\n>>>\n```\n\n### Reproduction\n\nRun version `0.4.1` or above, and submit any inference to the `meta-llama/Llama-3.2-1b` model ([info on model here](https://huggingface.co/meta-llama/Llama-3.2-1B)).\n\n### Environment\n\n```\n[2025-02-07 12:42:11] INFO _client.py:1038: HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"\nPython: 3.10.16 (main, Dec  4 2024, 08:53:37) [GCC 9.4.0]\nCUDA available: True\nGPU 0: NVIDIA GeForce RTX 4090\nGPU 0 Compute Capability: 8.9\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.135\nPyTorch: 2.5.1+cu124\nsglang: 0.4.1\nflashinfer: 0.1.6+cu124torch2.4\ntriton: 3.1.0\ntransformers: 4.47.1\ntorchao: 0.7.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.6\nhf_transfer: 0.1.8\nhuggingface_hub: 0.27.0\ninteregular: 0.3.3\nmodelscope: 1.21.0\norjson: 3.10.12\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.4\nmultipart: 0.0.20\nzmq: 26.2.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.58.1\nanthropic: 0.42.0\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-15    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-08T02:09:21+00:00",
    "closed_at": "2025-02-10T20:49:34+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3385/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3385"
  },
  {
    "number": 1923,
    "title": "[Bug] Launching a server with `--enable-torch-compile` produce torch dynamo error",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [X] 2. The bug has not been fixed in the latest version.\r\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [X] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nI'm using SGLang, but running into some issues when I launch a server with `--enable-torch-compile`. This issue does not occur without `--enable-torch-compile`. One strange thing is that this problem did not occur in previous versions (v0.3.1 or v0.3.2), but this error seems to occur starting from v0.3.4.\r\n```bash\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/lists.py\", line 106, in getitem_const\r\n    assert isinstance(index, (int, torch.SymInt))\r\nAssertionError:\r\n\r\nfrom user code:\r\n   File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/__init__.py\", line 58, in torch_dynamo_resume_in_forward_at_57\r\n    return self.forward_decode(q, k, v, layer, forward_batch)\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py\", line 273, in forward_decode\r\n    decode_wrapper = self.forward_metadata[0][self._get_wrapper_idx(layer)]\r\n\r\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\r\n\r\n\r\nYou can suppress this exception and fall back to eager by setting:\r\n    import torch._dynamo\r\n    torch._dynamo.config.suppress_errors = True\r\n\r\n\r\nW1104 22:57:20.399000 140422133311232 torch/_inductor/compile_worker/subproc_pool.py:126] SubprocPool unclean exit\r\n/usr/lib/python3.10/multiprocessing/resource_tracker.py:104: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.\r\n  warnings.warn('resource_tracker: process died unexpectedly, '\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\r\n    cache[rtype].remove(name)\r\nKeyError: '/mp-0je2dy8c'\r\n```\r\n\r\n### Reproduction\r\n\r\nI use docker-compose (docker compose -f docker-compose.yml up)\r\n```yaml\r\nservices:\r\n  sglang:\r\n    image: lmsysorg/sglang:v0.3.5-cu121\r\n    container_name: sglang\r\n    volumes:\r\n      - <my_custom_model_path>:/models:ro\r\n    restart: always\r\n    network_mode: host\r\n    # Or you can only publish port 30000\r\n    # ports:\r\n    #   - 30000:30000\r\n    environment:\r\n      HF_TOKEN: <secret>\r\n    entrypoint: python3 -m sglang.launch_server\r\n    command:\r\n      --model-path /models\r\n      --tokenizer-path /models\r\n      --port 30000\r\n      --tokenizer-mode auto\r\n      --dtype bfloat16\r\n      --served-model-name sglang\r\n      --mem-fraction-static 0.5\r\n      --random-seed 0\r\n      --enable-torch-compile\r\n      # --skip-tokenizer-init\r\n      # --log-requests\r\n    ulimits:\r\n      memlock: -1\r\n      stack: 67108864\r\n    ipc: host\r\n    healthcheck:\r\n      test: [\"CMD-SHELL\", \"curl -f http://localhost:30000/health || exit 1\"]\r\n    deploy:\r\n      resources:\r\n        reservations:\r\n          devices:\r\n            - driver: nvidia\r\n              device_ids: [\"0\"]\r\n              capabilities: [gpu]\r\n```\r\nAnd the model is finetuned version of gemma-2-2b.\r\n\r\n### Environment\r\n\r\nPython: 3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA A100-SXM4-80GB\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.0\r\nCUDA_HOME: /home/matt/miniconda3/envs/sglang\r\nNVCC: Cuda compilation tools, release 12.4, V12.4.99\r\nCUDA Driver Version: 550.90.07\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.3.5\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.46.1\r\nrequests: 2.32.3\r\ntqdm: 4.66.6\r\nnumpy: 1.26.4\r\naiohttp: 3.10.10\r\nfastapi: 0.115.4\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.26.2\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.1.0\r\npydantic: 2.9.2\r\nuvicorn: 0.32.0\r\nuvloop: 0.21.0\r\nzmq: 26.2.0\r\nvllm: 0.6.3.post1\r\nmultipart: 0.0.17\r\nopenai: 1.54.0\r\nanthropic: 0.39.0\r\nNVIDIA Topology:\r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10   NIC11   CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV12    NV12    NV12    NV12    NV12    NV12    NV12    PXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     48-63,176-191   3               N/A\r\nGPU1    NV12     X      NV12    NV12    NV12    NV12    NV12    NV12    PXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     48-63,176-191   3               N/A\r\nGPU2    NV12    NV12     X      NV12    NV12    NV12    NV12    NV12    SYS     SYS     PXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     16-31,144-159   1               N/A\r\nGPU3    NV12    NV12    NV12     X      NV12    NV12    NV12    NV12    SYS     SYS     PXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     16-31,144-159   1               N/A\r\nGPU4    NV12    NV12    NV12    NV12     X      NV12    NV12    NV12    SYS     SYS     SYS     SYS     SYS     SYS     PXB     PXB     SYS     SYS     SYS     SYS     112-127,240-255 7               N/A\r\nGPU5    NV12    NV12    NV12    NV12    NV12     X      NV12    NV12    SYS     SYS     SYS     SYS     SYS     SYS     PXB     PXB     SYS     SYS     SYS     SYS     112-127,240-255 7               N/A\r\nGPU6    NV12    NV12    NV12    NV12    NV12    NV12     X      NV12    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB     PXB     SYS     SYS     80-95,208-223   5               N/A\r\nGPU7    NV12    NV12    NV12    NV12    NV12    NV12    NV12     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB     PXB     SYS     SYS     80-95,208-223   5               N/A\r\nNIC0    PXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS      X      PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC1    PXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS     PXB      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC2    SYS     SYS     PXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS      X      PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC3    SYS     SYS     PXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS     PXB      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC4    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC5    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC6    SYS     SYS     SYS     SYS     PXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PXB     SYS     SYS     SYS     SYS\r\nNIC7    SYS     SYS     SYS     SYS     PXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB      X      SYS     SYS     SYS     SYS\r\nNIC8    SYS     SYS     SYS     SYS     SYS     SYS     PXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PXB     SYS     SYS\r\nNIC9    SYS     SYS     SYS     SYS     SYS     SYS     PXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB      X      SYS     SYS\r\nNIC10   SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX\r\nNIC11   SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  NIC8: mlx5_8\r\n  NIC9: mlx5_9\r\n  NIC10: mlx5_10\r\n  NIC11: mlx5_11\r\n\r\n\r\nulimit soft: 500000\r\nI1105 17:53:35.421000 139824578339904 torch/_dynamo/utils.py:335] TorchDynamo compilation metrics:\r\nI1105 17:53:35.421000 139824578339904 torch/_dynamo/utils.py:335] Function, Runtimes (s)\r\nV1105 17:53:35.422000 139824578339904 torch/fx/experimental/symbolic_shapes.py:116] lru_cache_stats constrain_symbol_range: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\r\nV1105 17:53:35.422000 139824578339904 torch/fx/experimental/symbolic_shapes.py:116] lru_cache_stats evaluate_expr: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\r\nV1105 17:53:35.422000 139824578339904 torch/fx/experimental/symbolic_shapes.py:116] lru_cache_stats _simplify_floor_div: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\r\nV1105 17:53:35.423000 139824578339904 torch/fx/experimental/symbolic_shapes.py:116] lru_cache_stats _maybe_guard_rel: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\r\nV1105 17:53:35.423000 139824578339904 torch/fx/experimental/symbolic_shapes.py:116] lru_cache_stats _find: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\r\nV1105 17:53:35.423000 139824578339904 torch/fx/experimental/symbolic_shapes.py:116] lru_cache_stats has_hint: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\r\nV1105 17:53:35.423000 139824578339904 torch/fx/experimental/symbolic_shapes.py:116] lru_cache_stats size_hint: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\r\nV1105 17:53:35.423000 139824578339904 torch/fx/experimental/symbolic_shapes.py:116] lru_cache_stats simplify: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\r\nV1105 17:53:35.423000 139824578339904 torch/fx/experimental/symbolic_shapes.py:116] lru_cache_stats _update_divisible: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\r\nV1105 17:53:35.423000 139824578339904 torch/fx/experimental/symbolic_shapes.py:116] lru_cache_stats replace: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\r\nV1105 17:53:35.423000 139824578339904 torch/fx/experimental/symbolic_shapes.py:116] lru_cache_stats _maybe_evaluate_static: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\r\nV1105 17:53:35.424000 139824578339904 torch/fx/experimental/symbolic_shapes.py:116] lru_cache_stats get_implications: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\r\nV1105 17:53:35.424000 139824578339904 torch/fx/experimental/symbolic_shapes.py:116] lru_cache_stats get_axioms: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)\r\nV1105 17:53:35.424000 139824578339904 torch/fx/experimental/symbolic_shapes.py:116] lru_cache_stats safe_expand: CacheInfo(hits=0, misses=0, maxsize=256, currsize=0)\r\nV1105 17:53:35.424000 139824578339904 torch/fx/experimental/symbolic_shapes.py:116] lru_cache_stats uninteresting_files: CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-11-05T08:55:55+00:00",
    "closed_at": "2025-01-15T00:16:33+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1923/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1923"
  }
]
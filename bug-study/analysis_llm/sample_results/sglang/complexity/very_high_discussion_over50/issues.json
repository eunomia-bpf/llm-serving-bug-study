[
  {
    "number": 6017,
    "title": "Instruction for Running DeepSeek with Large-scale PD and EP",
    "body": "## Using main branch\n\n~~NOTE: The feature is already on main, but the performance still needs some improvements on main branch.~~ will be good after a few already opened PRs - PR 6680, 6727, 6728\n\n~~NOTE: I will try other config like 4 node for P and 9 node for D later.~~ updated\n\n### Environment Preparation\n\nUse SGLang and DeepEP on master is sufficient. Also remember to upgrade Mooncake.\n\n### 4P + 9D experiments\n\nStart server\nwhere DeepEP config can be tuned by https://github.com/sgl-project/sglang/pull/6742\n\n```python\n# prefill nodes\nMC_TE_METRIC=true SGLANG_TBO_DEBUG=1 python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-V3-0324 --disaggregation-ib-device mlx5_1 --disaggregation-mode prefill --dist-init-addr 10.5.55.3:5757 --nnodes 4 --node-rank 0 --tp-size 32 --dp-size 32 --enable-dp-attention --decode-log-interval 1 --enable-deepep-moe --page-size 1 --host 0.0.0.0 --trust-remote-code --moe-dense-tp-size 1 --enable-dp-lm-head --disable-radix-cache --watchdog-timeout 1000000 --enable-two-batch-overlap --deepep-mode normal --mem-fraction-static 0.85 --chunked-prefill-size 524288 --max-running-requests 8192 --max-total-tokens 131072 --context-length 8192 --init-expert-location YOUR_PATH --ep-num-redundant-experts 32 --ep-dispatch-algorithm dynamic --eplb-algorithm deepseek --deepep-config YOUR_PATH\n\n# decode nodes\nMC_TE_METRIC=true SGLANG_TBO_DEBUG=1 python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-V3-0324 --disaggregation-ib-device mlx5_1 --disaggregation-mode decode --dist-init-addr 10.5.55.7:5757 --nnodes 9 --node-rank 0 --tp-size 72 --dp-size 72 --enable-dp-attention --decode-log-interval 1 --enable-deepep-moe --page-size 1 --host 0.0.0.0 --trust-remote-code --moe-dense-tp-size 1 --enable-dp-lm-head --disable-radix-cache --watchdog-timeout 1000000 --enable-two-batch-overlap --deepep-mode low_latency --mem-fraction-static 0.835 --max-running-requests 18432 --context-length 4500 --init-expert-location YOUR_PATH --ep-num-redundant-experts 32 --cuda-graph-bs 256 --num-reserved-decode-tokens YOUR_VALUE\n\n# load balancer\npython3 -m sglang.srt.disaggregation.mini_lb --prefill \"http://YOUR_FIRST_PREFILL_NODE_IP:30000\" --decode \"http://YOUR_FIRST_DECODE_NODE_IP:30000\"\n```\n\nBenchmark for prefill\n\n```\n# benchmark\npython3 -m sglang.bench_one_batch_server --model-path ${model_path} --base-url http://YOUR_IP:8000 --batch-size 8192 --input-len 4096 --output-len 5 --skip-warmup\n```\n\nBenchmark for decode\n\n- It is suggested to use 3 prefill nodes and 9 decode nodes to reproduce our results, since 9 decode nodes is half the size of that in DeepSeek\u2019s blog.\n- `SGLANG_HACK_PD_DECODE_NUM_RESERVED_DECODE_TOKENS` can be set to `benchmark-output-len + 2` to maximize batch size.\n- The example below demonstrates how to use the slow_down debug feature to stress test decode nodes when there are not enough prefill nodes. If your test workload has enough prefill nodes, this can be omitted.\n\n```\n# slow down D nodes\ncurl -H \"Content-Type: application/json\" -d '{\"forward_sleep_time\": 90.0}' -X POST \"http://YOUR_FIRST_DECODE_NODE_IP:30000/slow_down\"\n\n# start benchmark; do not wait for this to finish before running the next line\npython3 -m sglang.bench_one_batch_server --model-path /dev/shm/DeepSeek-V3-0324 --base-url http://10.10.37.16:7000 --batch-size 40000 --input-len 2000 --output-len 100 --skip-warmup\n\n# after some time (e.g. 10 minute), the D nodes are saturated, then this command should be executed\n# finish slowing down D nodes\ncurl -H \"Content-Type: application/json\" -d '{\"forward_sleep_time\": null}' -X POST \"http://YOUR_FIRST_DECODE_NODE_IP:30000/slow_down\"\n```\n\n### 4P + 9D + dynamic EPLB\n\nMay still have room for improvements, just preliminary tests.\n\n```\n# prefill\nMC_TE_METRIC=true SGLANG_TORCH_PROFILER_DIR=/host_home/temp_sglang_server2local SGLANG_TBO_DEBUG=1 PYTHONUNBUFFERED=1 python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-V3-0324 --disaggregation-ib-device mlx5_1 --disaggregation-mode prefill --dist-init-addr 10.5.55.3:5757 --nnodes 4 --node-rank 0 --tp-size 32 --dp-size 32 --enable-dp-attention --decode-log-interval 1 --enable-deepep-moe --page-size 1 --host 0.0.0.0 --trust-remote-code --moe-dense-tp-size 1 --enable-dp-lm-head --disable-radix-cache --watchdog-timeout 1000000 --enable-two-batch-overlap --deepep-mode normal --mem-fraction-static 0.85 --chunked-prefill-size 524288 --max-running-requests 8192 --max-total-tokens 65536 --context-length 8192 --enable-eplb --ep-num-redundant-experts 32 --eplb-rebalance-num-iterations YOUR_VALUE --ep-dispatch-algorithm dynamic --deepep-config YOUR_PATH\n\n# decode\nMC_TE_METRIC=true SGLANG_TORCH_PROFILER_DIR=/host_home/temp_sglang_server2local SGLANG_TBO_DEBUG=1 PYTHONUNBUFFERED=1 python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-V3-0324 --disaggregation-ib-device mlx5_1 --disaggregation-mode decode --dist-init-addr 10.5.55.7:5757 --nnodes 9 --node-rank 0 --tp-size 72 --dp-size 72 --enable-dp-attention --decode-log-interval 1 --enable-deepep-moe --page-size 1 --host 0.0.0.0 --trust-remote-code --moe-dense-tp-size 1 --enable-dp-lm-head --disable-radix-cache --watchdog-timeout 1000000 --enable-two-batch-overlap --deepep-mode low_latency --mem-fraction-static 0.82 --max-running-requests 18432 --context-length 4500 --enable-eplb --ep-num-redundant-experts 32 --eplb-rebalance-num-iterations YOUR_VALUE --cuda-graph-bs 256  --num-reserved-decode-tokens YOUR_VALUE\n```\n\n### Create expert distribution data\n\nNeed PR 6964, 6967\n\n```\n# prefill\nSGLANG_DISAGGREGATION_THREAD_POOL_SIZE=4 MC_TE_METRIC=true SGLANG_TORCH_PROFILER_DIR=/host_home/temp_sglang_server2local SGLANG_EXPERT_DISTRIBUTION_RECORDER_DIR=/host_home/temp_sglang_server2local SGLANG_TBO_DEBUG=1 PYTHONUNBUFFERED=1 python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-V3-0324 --disaggregation-ib-device mlx5_1 --disaggregation-mode prefill --dist-init-addr 10.5.55.1:5757 --nnodes 4 --node-rank 0 --tp-size 32 --dp-size 32 --enable-dp-attention --decode-log-interval 1 --enable-deepep-moe --page-size 1 --host 0.0.0.0 --trust-remote-code --moe-dense-tp-size 1 --enable-dp-lm-head --disable-radix-cache --watchdog-timeout 1000000 --enable-two-batch-overlap --expert-distribution-recorder-mode stat --disable-overlap-schedule --expert-distribution-recorder-buffer-size -1 --deepep-mode normal --mem-fraction-static 0.82 --chunked-prefill-size 524288 --max-running-requests 8192 --max-total-tokens 131072 --context-length 8192 --ep-num-redundant-experts 32 --ep-dispatch-algorithm dynamic --eplb-algorithm deepseek --deepep-config /host_home/primary_synced/tom_sglang_server/misc/deepep_vp.json\n\n# decode\nMC_TE_METRIC=true SGLANG_TORCH_PROFILER_DIR=/host_home/temp_sglang_server2local SGLANG_EXPERT_DISTRIBUTION_RECORDER_DIR=/host_home/temp_sglang_server2local SGLANG_TBO_DEBUG=1 PYTHONUNBUFFERED=1 python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-V3-0324 --disaggregation-ib-device mlx5_1 --disaggregation-mode decode --dist-init-addr 10.5.55.5:5757 --nnodes 9 --node-rank 0 --tp-size 72 --dp-size 72 --enable-dp-attention --decode-log-interval 1 --enable-deepep-moe --page-size 1 --host 0.0.0.0 --trust-remote-code --moe-dense-tp-size 1 --enable-dp-lm-head --disable-radix-cache --watchdog-timeout 1000000 --enable-two-batch-overlap --expert-distribution-recorder-mode stat --disable-overlap-schedule --expert-distribution-recorder-buffer-size -1 --deepep-mode low_latency --mem-fraction-static 0.81 --max-running-requests 18432 --context-length 4500 --ep-num-redundant-experts 32 --cuda-graph-bs 256  --num-reserved-decode-tokens YOUR_VALUE\n\ncurl -X POST -H 'Content-Type: application/json' 'http://10.5.55.1:30000/start_expert_distribution_record' -d '{}' \ncurl -X POST -H 'Content-Type: application/json' 'http://10.5.55.5:30000/start_expert_distribution_record' -d '{}' \ncurl -X POST -H 'Content-Type: application/json' 'http://10.5.55.5:30000/slow_down' -d '{\"forward_sleep_time\": 90.0}' \npython3 -m sglang.bench_one_batch_server  --base-url http://10.5.55.1:8000 --model-path /dev/shm/DeepSeek-V3-0324 --batch-size 40000 --input-len 2000 --output-len 100 --skip-warmup \n# after a while\ncurl -X POST -H 'Content-Type: application/json' 'http://10.5.55.5:30000/slow_down' -d '{\"forward_sleep_time\": null}' \n# after a while\ncurl -X POST -H 'Content-Type: application/json' 'http://10.5.55.1:30000/dump_expert_distribution_record' -d '{}' \ncurl -X POST -H 'Content-Type: application/json' 'http://10.5.55.5:30000/dump_expert_distribution_record' -d '{}' \n```\n\nThen you will get one .pt file for prefill and one for decode. They can be used in --init-expert-location.\n\n## Using the blog branch\n\n<details>\n\n### Environment Preparation\n\n- Install SGLang on branch https://github.com/sgl-project/sglang/tree/deepseek_ep\n    - ~~https://github.com/sgl-project/sglang/pull/5524~~ (EDIT: do not use this branch since I am adding more code to it after the blog, please use deepseek_ep instead)\n- ~~Install DeepEP on branch https://github.com/deepseek-ai/DeepEP/pull/142~~\n    - 2025.05.08 UPDATE: Directly use latest DeepEP main is enough, since my PR has been merged\n- Install latest mooncake\n\nIt is suggested to use this Dockerfile https://github.com/sgl-project/sglang/blob/main/docker/Dockerfile.deepep to prepare dependencies of DeepEP.\n\n### Stress-testing Prefill Nodes\n\n```python\n# prefill nodes\nMC_TE_METRIC=true SGLANG_HACK_DEEPEP_NEW_MODE=0 SGL_ENABLE_JIT_DEEPGEMM=1 python3 -m sglang.launch_server --model-path ${model_path} --disaggregation-mode prefill --disaggregation-ib-device ${device_name} --host ${node_ip} --trust-remote-code --dist-init-addr ${master_ip}:5757 --nnodes ${num_prefill} --node-rank ${node_rank} --tp-size $((${num_prefill}*8)) --dp-size $((${num_prefill}*8)) --enable-dp-attention --enable-deepep-moe --deepep-mode normal --mem-fraction-static 0.85 --chunked-prefill-size $((${num_prefill}*131072)) --max-running-requests $((${num_prefill}*2048)) --max-total-tokens 131072 --context-length 8192 --init-expert-location YOUR_EXPERT_LOCATION_HERE --ep-num-redundant-experts 32 --enable-two-batch-overlap --moe-dense-tp-size 1 --disable-radix-cache --ep-dispatch-algorithm random\n\n# decode nodes\nSGLANG_HACK_DEEPEP_NEW_MODE=0 SGLANG_HACK_PD_DECODE_NUM_RESERVED_DECODE_TOKENS=102 SGL_ENABLE_JIT_DEEPGEMM=1 python3 -m sglang.launch_server --model-path ${model_path} --disaggregation-mode decode --disaggregation-ib-device ${device_name} --host ${node_ip} --trust-remote-code --dist-init-addr ${master_ip}:5757 --nnodes ${num_decode} --node-rank ${node_rank} --tp-size $((${num_decode}*8)) --dp-size $((${num_decode}*8)) --enable-dp-attention --enable-deepep-moe --deepep-mode low_latency --mem-fraction-static 0.82 --max-running-requests $((${num_decode}*1024)) --context-length 4500 --init-expert-location YOUR_EXPERT_LOCATION_HERE --enable-two-batch-overlap --moe-dense-tp-size 1 --cuda-graph-bs 128 --disable-radix-cache --decode-log-interval 1\n\n# load balancer\npython3 -m sglang.srt.disaggregation.mini_lb --prefill \"http://YOUR_FIRST_PREFILL_NODE_IP:30000\" --decode \"http://YOUR_FIRST_DECODE_NODE_IP:30000\"\n\n# benchmark\npython3 -m sglang.bench_one_batch_server --model-path ${model_path} --base-url http://YOUR_IP:8000 --batch-size 8192 --input-len 4096 --output-len 5 --skip-warmup\n```\n\n### Stress-testing Decode Nodes\n\n```python\n# prefill nodes\nSGLANG_HACK_DEEPEP_NEW_MODE=0 SGL_ENABLE_JIT_DEEPGEMM=1 python3 -m sglang.launch_server --model-path ${model_path} --disaggregation-mode prefill --disaggregation-ib-device ${device_name} --host ${node_ip} --trust-remote-code --dist-init-addr ${master_ip}:5050 --nnodes ${num_prefill} --node-rank ${node_rank} --tp-size $((${num_prefill}*8)) --dp-size $((${num_prefill}*8)) --enable-dp-attention --enable-deepep-moe --deepep-mode normal --mem-fraction-static 0.85 --chunked-prefill-size $((${num_prefill}*65536)) --max-running-requests $((${num_prefill}*2048)) --max-total-tokens 131076 --context-length 8192 --init-expert-location YOUR_EXPERT_LOCATION_HERE --ep-num-redundant-experts 32 --enable-two-batch-overlap --moe-dense-tp-size 1 --disable-radix-cache\n\n# decode nodes\nSGLANG_HACK_DEEPEP_NEW_MODE=0 SGLANG_HACK_PD_DECODE_NUM_RESERVED_DECODE_TOKENS=YOUR_NUM_HERE SGL_ENABLE_JIT_DEEPGEMM=1 python3 -m sglang.launch_server --model-path ${model_path} --disaggregation-mode decode --disaggregation-ib-device ${device_name} --host ${node_ip} --trust-remote-code --dist-init-addr ${master_ip}:5050 --nnodes ${num_decode} --node-rank ${node_rank} --tp-size $((${num_decode}*8)) --dp-size $((${num_decode}*8)) --enable-dp-attention --enable-deepep-moe --deepep-mode low_latency --mem-fraction-static 0.846 --chunked-prefill-size 81920 --max-running-requests $((${num_decode}*2048)) --context-length 4096 --init-expert-location YOUR_EXPERT_LOCATION_HERE --ep-num-redundant-experts 32 --enable-two-batch-overlap --moe-dense-tp-size 1 --cuda-graph-bs 256 --disable-radix-cache --decode-log-interval 1\n\n# load balancer\npython3 -m sglang.srt.disaggregation.mini_lb --prefill \"http://YOUR_FIRST_PREFILL_NODE_IP:30000\" --decode \"http://YOUR_FIRST_DECODE_NODE_IP:30000\"\n\n# slow down D nodes\ncurl -H \"Content-Type: application/json\" -d '{\"forward_sleep_time\": 90.0}' -X POST \"http://YOUR_FIRST_DECODE_NODE_IP:30000/slow_down\"\n\n# start benchmark; do not wait for this to finish before running the next line\npython3 -m sglang.bench_one_batch_server --model-path /dev/shm/DeepSeek-V3-0324 --base-url http://10.10.37.16:7000 --batch-size 40000 --input-len 2000 --output-len 100 --skip-warmup\n\n# after some time (e.g. 10 minute), the D nodes are saturated, then this command should be executed\n# finish slowing down D nodes\ncurl -H \"Content-Type: application/json\" -d '{\"forward_sleep_time\": null}' -X POST \"http://YOUR_FIRST_DECODE_NODE_IP:30000/slow_down\"\n```\n\n</details>\n\n## Analyzing Results\n\nSince we are stress testing one side of P or D, we need to look at the server logs instead of benchmark script outputs.\n\n- Prefill: For logs like `Prefill batch. ... #new-token: 16384 ... gap_latency: 2.561`, the performance is `16384 / 2.561` token/second/device.\n- Decode: The result can be read from `gen throughput (token/s)` in the logs.\n\n## Remarks\n\n- Please ensure the batch size is full and avoid padding, because the performance is suboptimal otherwise due to a bug we will address soon.\n    - For example, to ensure a batch size of 256 for 72 decode GPUs, it is reasonable to send 40000 requests.\n- The sample command above only captures a CUDA graph of size 256 to save memory, which can be modified to suit your scenarios.\n- For optimal performance, you may need to tune components such as DeepEP on your cluster.\n- DeepGEMM warmup during execution will cause seemingly slow overall performance, and should be excluded from analyzation.\n- We rushed in the last few days, so the code is really ugly now with many hacks. We will make it elegant when merging into master.\n- For expert distribution statistics, our experiments use the same as input/output data and provide them as follows for reproducibility: [attachment_ep_statistics.zip](https://github.com/user-attachments/files/20036217/attachment_ep_statistics.zip)\n- To debug prefill performance, it may be useful to temporarily use `--ep-dispatch-algorithm fake_grouped_uniform` to simulate a fake perfect EPLB, and should match the corresponding performance reported in the blog\n- To analyze performance, it is suggested to use the log instead of benchmark script output, because the script output is mixed with the starting and ending part, where the system is not fully utilized and is slow.\n\n## Report Template\n\nIf you face any issues, feel free to discuss here or in Slack channel, and it would be great to provide the following information:\n\n* Full command to start server and benchmark\n* Logs of all server nodes and benchmark",
    "labels": [
      "collaboration",
      "deepseek"
    ],
    "state": "open",
    "created_at": "2025-05-05T04:48:15+00:00",
    "closed_at": null,
    "comments": 504,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6017/reactions",
      "total_count": 63,
      "+1": 57,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 6,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6017"
  },
  {
    "number": 3836,
    "title": "[Bug]  Model Stuck at Prefill and then throw \"Watchdog Timeout\" Error After Idle Period (Deepseek-r1:671b on two H100*8)",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI am currently using SGLang to deploy the deepseek-r1:671b model across two H800 GPUs. However, I have encountered a persistent issue when the system remains idle for some time. Upon resuming usage, even with simple prompts such as \"Hello,\" the model gets stuck during the Prefill stage. Subsequently, the system throws a \"watchdog timeout\" error.\n\nFollowing this error, the GPU resources are released, and any subsequent attempts to interact with the model fail to reload it. The only way to restore functionality is by restarting the service entirely.\n\n### Reproduction\n\nDeploy the deepseek-r1:671b model using SGLang on two H800 GPUs.\n\n```\ndocker run --gpus all \\\n    --shm-size 32g \\\n    --network=host \\\n    -v /model/:/model \\\n    --name sglang_multinode \\\n    -it \\\n    --rm \\\n    --env \"GLOO_SOCKET_IFNAME=ens24f0\" \\\n    --env \"NCCL_SOCKET_IFNAME=ens24f0\" \\\n    --env \"NCCL_DEBUG=INFO\"  \\\n    --env \"NCCL_IGNORE_DISABLED_P2P=1\" \\\n    --ipc=host \\\n    lmsysorg/sglang:latest \\\n    python3 -m sglang.launch_server --model-path /model/deepseek-ai/DeepSeek-R1 --tp 16 --dist-init-addr 10.2.17.101:20000 --nnodes 2 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 40000\n```\n\nIt works at the begining.\nLeave the system idle for a period of time (exact duration may vary).\nAttempt to send a simple query like \"Hello\" after the idle period.\nObserve that the model gets stuck at the Prefill stage.\nEncounter the \"watchdog timeout\" error, followed by the release of GPU resources.\n\n```\n[2025-02-25 05:31:35] INFO:     10.2.17.101:38344 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-02-25 05:31:35 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 5, cache hit rate: 42.12%, token usage: 0.00, #running-req: 0, #queue-req: 0\n[2025-02-25 05:32:28] INFO:     10.2.17.101:60714 - \"GET /v1/models HTTP/1.1\" 200 OK\n[2025-02-25 05:36:52 TP4] Watchdog timeout (self.watchdog_timeout=300)\n[2025-02-25 05:36:53 TP7] Watchdog timeout (self.watchdog_timeout=300)\n[2025-02-25 05:36:53 TP2] Watchdog timeout (self.watchdog_timeout=300)\n[2025-02-25 05:36:54 TP1] Watchdog timeout (self.watchdog_timeout=300)\n[2025-02-25 05:36:54 TP3] Watchdog timeout (self.watchdog_timeout=300)\n[2025-02-25 05:36:54 TP5] Watchdog timeout (self.watchdog_timeout=300)\n[2025-02-25 05:36:54 TP0] Watchdog timeout (self.watchdog_timeout=300)\n[2025-02-25 05:36:55 TP6] Watchdog timeout (self.watchdog_timeout=300)\n[2025-02-25 05:36:57] Received sigquit from a child proces. It usually means the child failed.\n[2025-02-25 05:36:57] Received sigquit from a child proces. It usually means the child failed.\n[2025-02-25 05:36:57] Received sigquit from a child proces. It usually means the child failed.\n[2025-02-25 05:36:57] Received sigquit from a child proces. It usually means the child failed.\n.......\n```\n\nNote that further queries do not reload the model, necessitating a service restart.\n\n### Environment\n\nPython: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.144.03\nPyTorch: 2.5.1+cu124\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.1.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.23.0\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.63.2\ntiktoken: 0.9.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    PIX     SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     PIX     NODE    NODE    NODE    48-95,144-191   1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     NODE    PIX     NODE    NODE    48-95,144-191   1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     NODE    NODE    PIX     NODE    48-95,144-191   1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     NODE    NODE    NODE    PIX     48-95,144-191   1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    SYS     SYS     SYS     SYS\nNIC1    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    SYS     SYS     SYS     SYS\nNIC2    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE     X      SYS     SYS     SYS     SYS\nNIC3    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS      X      NODE    NODE    NODE\nNIC4    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     NODE     X      NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     NODE    NODE     X      NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     NODE    NODE    NODE     X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n\n\nulimit soft: 1048576",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-02-25T06:10:47+00:00",
    "closed_at": "2025-05-31T00:18:38+00:00",
    "comments": 57,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3836/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3836"
  },
  {
    "number": 2591,
    "title": "[Feature] DeepSeek V3 optimization",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Adoption\n\n[SGLang adoption for DeepSeek V3 and R1](https://github.com/sgl-project/sglang/discussions/3322)\n\n### Usage\n\nUser Guide for Existing System (Installation & Launch)\n\nhttps://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\n\nPlease use the latest version [v0.4.2.post4](https://pypi.org/project/sglang/0.4.2.post4/). Please prefer to use docker image. `docker pull lmsysorg/sglang:latest`\n\nFor running on AMD MI300X, use this as a reference. [Running DeepSeek-R1 on a single NDv5 MI300X VM](https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/running-deepseek-r1-on-a-single-ndv5-mi300x-vm/4372726)\n\n### Features\n\n- [x] Support CUDA Graph @HandH1998 @ispobock \n- [x] Support Torch compile @ispobock \n- [x] Use BF16 for bmm @zhyncs \n- [x] Improve the accuracy for FP8 @HandH1998 @zhyncs @ispobock \n- [x] Tuning FP8 GEMM @HandH1998 @zhyncs \n- [x] Replace `moe_align_block_size` @HandH1998 @zhyncs @BBuf \n- [x] FusedMoE tuning for H200 `E=256,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8.json` @BBuf \n- [x] TP+DP Attention @Ying1123 \n- [x] Support overlap scheduler with DP attention @merrymercy\n- [x] Fuse Sigmoid Gate  [moe_kernels.cu](https://github.com/NVIDIA/TensorRT-LLM/blob/main/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.cu) @NovTi @BBuf (torch compile is sufficient for this use case, so the priority and ROI to support it are not high. Closing for now.)\n- [x] Support `nextn` speculative decoding @ispobock  https://github.com/sgl-project/sglang/issues/3472\n- [x] FP8 GEMM CUTLASS implementation @yizhang2077 \n- [x] Better [fused_experts](https://github.com/sgl-project/sglang/blob/34e405e01f7ff15ad56399999b9c00859a0b5134/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py#L1123) @bbuf @zhyncs \n- [x] FlashInfer Prefill and MLA Decoding @zhyncs @ispobock \n- [x] Integrate DeepGemm #4199 #4343\n- [x] Integrate FlashMLA #4472 #4514 \n- [ ] FP8 GEMM Composable Kernel implementation @HaiShaw \n- [ ] Support Pipeline Parallelism @Ying1123  \n\nMore things (e.g., PD disaggregation, cache) are tracked at https://github.com/sgl-project/sglang/issues/4042",
    "labels": [
      "enhancement",
      "high priority",
      "performance",
      "quant"
    ],
    "state": "closed",
    "created_at": "2024-12-26T08:52:39+00:00",
    "closed_at": "2025-03-25T04:10:46+00:00",
    "comments": 52,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2591/reactions",
      "total_count": 98,
      "+1": 64,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 14,
      "rocket": 7,
      "eyes": 13
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/2591"
  }
]
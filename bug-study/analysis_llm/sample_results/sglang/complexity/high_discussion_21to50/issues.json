[
  {
    "number": 1316,
    "title": "[Bug] Unable to fix model output",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThe performance of sglang is very good. I am comparing the output accuracy of vllm, Hugging Face, and sglang. Using Qwen's model, I set do_sample to false or temperature to 0 to fix the output. Through comparison, the outputs of vllm and the Hugging Face transformer library are consistent. However, sglang does not produce consistent outputs. sglang has _SAMPLING_EPS set to 1e-6. Even when I use temperature=1e-5, I still cannot obtain consistent outputs. sglang produces different outputs with each run. What configurations should be set to make the output of sglang deterministic?\r\n sglang\u7684\u6027\u80fd\u975e\u5e38\u4e0d\u9519\u3002\u6211\u5728\u5bf9\u6bd4vllm\u3001hugging face\u548csglang\u7684\u8f93\u51fa\u7cbe\u5ea6\u3002 \u4f7f\u7528\u5343\u95ee\u7684\u6a21\u578b\uff0c\u4f7f\u7528do_sample\u7b49\u4e8efalse\u6216\u8005temperature=0\u6765\u56fa\u5b9a\u8f93\u51fa\u3002\u901a\u8fc7\u5bf9\u6bd4\uff0cvllm\u548chuggiing face transformer\u5e93\u7684\u8f93\u51fa\u4e00\u81f4\u3002\u4f46\u662fsglang\u4e0d\u80fd\u5f97\u5230\u4e00\u81f4\u7684\u8f93\u51fa\u3002 sglang \u8bbe\u7f6e\u4e86_SAMPLING_EPS = 1e-6\u3002\u6211\u4f7f\u7528temperature=1e-5\uff0c\u4ecd\u7136\u4e0d\u80fd\u6216\u8005\u4e00\u81f4\u7684\u8f93\u51fa\u3002sglang\u6bcf\u6b21\u8fd0\u884c\u90fd\u4f1a\u5f97\u5230\u4e0d\u4e00\u6837\u7684\u8f93\u51fa\u3002\u8be5\u600e\u4e48\u8bbe\u7f6e\uff0c\u80fd\u56fa\u5b9asglang\u7684\u8f93\u51fa\u3002\n\n### Reproduction\n\npython -m sglang.launch_server --model-path /xx/Qwen1.5-1.8B-Chat --port 30000  --tp 2  --enable-p2p-check --mem-fraction-static 0.7 --chunked-prefill-size 4096\n\n### Environment\n\nPython: 3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA GeForce RTX 3090\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.6\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.2, V12.2.91\r\nCUDA Driver Version: 535.54.03\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.2.15\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.44.0\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.26.4\r\naiohttp: 3.10.2\r\nfastapi: 0.112.0\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.5\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.5\r\nuvloop: 0.19.0\r\nzmq: 26.1.0\r\nvllm: 0.5.5\r\nmultipart: 0.0.9\r\nopenai: 1.40.2\r\nanthropic: 0.33.0\r\nNVIDIA Topology:\r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-19,40-59      0               N/A\r\nGPU1    PIX      X      NODE    NODE    SYS     SYS     SYS     SYS     0-19,40-59      0               N/A\r\nGPU2    NODE    NODE     X      PIX     SYS     SYS     SYS     SYS     0-19,40-59      0               N/A\r\nGPU3    NODE    NODE    PIX      X      SYS     SYS     SYS     SYS     0-19,40-59      0               N/A\r\nGPU4    SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    20-39,60-79     1               N/A\r\nGPU5    SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    20-39,60-79     1               N/A\r\nGPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      PIX     20-39,60-79     1               N/A\r\nGPU7    SYS     SYS     SYS     SYS     NODE    NODE    PIX      X      20-39,60-79     1               N/A\r\n",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2024-09-03T11:01:15+00:00",
    "closed_at": "2024-11-01T04:13:00+00:00",
    "comments": 25,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1316/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1316"
  },
  {
    "number": 4042,
    "title": "Development Roadmap (2025 H1)",
    "body": "Here is the development roadmap for 2025 H1. Contributions and feedback are welcome ([**Join Bi-weekly Development Meeting**](https://docs.google.com/document/d/1xEow4eIM152xNcRxqZz9VEcOiTQo8-CEuuQ5qTmkt-E/edit?usp=sharing)). The previous 2024 Q4 roadmap can be found in #1487\n\n## Focus\n- Throughput-oriented large-scale deployment similar to the [deepseek inference system](https://github.com/deepseek-ai/open-infra-index?tab=readme-ov-file#day-6---one-more-thing-deepseek-v3r1-inference-system-overview)\n- Long context optimizations\n- Low latency speculative decoding\n- Reinforcement learning training framework integration\n- Kernel optimizations\n\n## Parallelism\n- [x] Support PD disaggregation @ByronHsu  #4655\n- [x] Support expert parallelism and load balancer #5524\n- [x] Support pipeline parallelism @Ying1123 #5724\n- [x] Support data parallelism attention compatible with all other parallelism #4390 \n- [x] Support overlap communication in TP/EP @tom @Zhuohao-Li #4068\n- [ ] Improvements of sgl-router for better data parallelism @Qihang-Zhang \n\n## Attention Backend\n- [x] Support Native FlashAttention3 as Attention Backend: https://github.com/sgl-project/sglang/issues/4709 @hebiao064 @qingquansong @zcnrex @Fridge003 @yinfan98 \n- [ ] Torch FlexAttention @HaiShaw @ispobock \n\n## Caching\n- [x] Optimize Hierarchical cache  (GPU/CPU/Disk) #2693 #4009 @xiezhq-hermann \n- [ ] Integrate DeepSeek [3FS](https://github.com/deepseek-ai/3FS) @yizhang2077 \n\n## Kernel\n- [x] integrate flash attention 3 #4709\n- [x] Integrate DeepGemm #4199 #4343\n- [x] Integrate FlashMLA #4472 #4514\n- [ ] Integrate cuDNN attention. [reference](https://github.com/NVIDIA/cudnn-frontend/blob/v1.8.0/samples/python/52_scaled_dot_product_attention_with_paged_caches.ipynb)\n- [ ] Integrate TransformerEngine layers\n- [x] Start to maintain performant attention ops in sgl-kernel\n- [x] Start to maintain more sparse attention ops in sgl-kernel\n- [ ] Integrate Blackwell kernels from flashinfer #5855\n\n## Quantization\n- [ ] MXFP4 support @HaiShaw \n- [x] INT4-FP8 MoE & Fused MoE @HaiShaw @Carlushuang #4152\n- [x] W8A8 (FP8 and INT8) implementation in sgl-kernel, removing vllm dependency. #3148 #3047\n- [ ] Integration of awq and gptq in sgl-kernel, removing vllm dependency\n- [ ] TorchAO support extension to additional models\n- [x] Blackwell FP4 support #3972\n- [ ] Optional quantization support using vllm's implementation (e.g. bnb, gguf)\n- [ ] Communication quant\n- [ ] unsloth model support @guapisolo @XueyingJia @yyihuang\n\n## RL Framework integration\n- [x] veRL integration #3852 @fzyzcjy @zhaochenyang20 @ocss884\n- [x] Multi-turn RL https://github.com/volcengine/verl/issues/385  https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/release_log/verl-multiturn-rollout-Release.md @UbeCc @PeterSH6\n- [X] Work as the default engine in AREAL https://github.com/inclusionAI/AReaL\n- [ ] VLM RLHF @yiranyyu @PeterSH6 @zhaochenyang20 @tongyx361 @shuaills \n- [ ] GRPO to trl @jhinpan \n\n## Core refactor\n- [x] Support page size > 1 #4356\n- [x] Simplify `scheduler.py` and `model_runner.py` to make them more modular \n- [ ] Integrate CacheTensorManager from https://github.com/ModelTC/lightllm/releases/tag/v1.0.0\n- [ ] Integrate Cross-Process Request Object from https://github.com/ModelTC/lightllm/releases/tag/v1.0.0\n- [x] Remove the dependency of vLLM @zhyncs @ByronHsu @yizhang2077 https://github.com/sgl-project/sglang/issues/2245\n\n## Speculative decoding\n- [ ] Optimizations for large batch @FrankLeeeee @yukavio  #6995\n- [ ] Adaptive speculative decoding according to batch sizes\n- [ ] Reference-based speculative decoding #270 #2790\n\n## Multi-LoRA serving\n- [x] Add Triton backend for lora kernels @Fridge003  #3161\n- [x] Support Tensor Parallelism @ShenAo1111 #4274\n- [x] Support cuda graph @Qiaolin-Yu  @Beichen-Ma #4115\n- [ ] Support radix attention @Sunt-ing @jcbjcbjc\n- [ ] Support embedding layers @Beichen-Ma \n- [ ] Support Unified Paging @Sunt-ing @jcbjcbjc #4492\n- [ ] Optimizing speed with cublas/cutlass kernels @Fridge003 @jcbjcbjc\n- [x] Support dynamic loading and unloading @lifuhuang  #7412 #7446 \n\n## Hardware\n- [x] Blackwell support #5303\n- [x] AMD aiter integration @HaiShaw\n- [x] Optimized CPU backends\n- [ ] More backends (Intel XPU, TPU)\n\n## Model coverage\n- Multi-modal models\n  - [x] DeepSeek VL2 https://github.com/sgl-project/sglang/issues/2653\n  - [ ] mistralai/Pixtral https://github.com/sgl-project/sglang/issues/2351\n  - [ ] GLM 4V https://github.com/sgl-project/sglang/pull/1641\n  - [ ] VILA https://arxiv.org/abs/2412.04468 @Lyken17\n  - [x] MiniCPM-o https://github.com/sgl-project/sglang/pull/3023 @mickqian @yiranyyu @yizhang2077 \n  - [x] Janus-pro https://github.com/sgl-project/sglang/pull/3203 @mickqian @yizhang2077 \n  - [ ] intern-vl 2.5 https://github.com/sgl-project/sglang/pull/3351 @mickqian @yizhang2077 \n  - [x] Phi4-multimodal vision #6494 @lifuhuang \n  - [x] upstream transformers to 4.50.0 @yizhang2077 https://github.com/sgl-project/sglang/pull/3984\n- Language models\n  -  [ ] Mamba models\n- Transformers backend #5929 \n\n## Function Calling\n- [X] Structural Tag @minleminzui @shuaills @Ubospica \n- [X] Adapter Refactor @CatherineSue @shuaills @Qiaolin-Yu \n\n## Others\n-  [ ] A padded batch mode to make results more deterministic https://github.com/sgl-project/sglang/blob/8912b7637f5c8dca0f18c31a17e46f427cf53152/docs/references/faq.md?plain=1#L3\n- [ ] Add nightly eval CI by using lm eval harness @XiaotongJiang @PopSoda2002 @ziliangpeng @monstertail\n- [ ] Add open-to-use grafana @PopSoda2002 @ziliangpeng\n\n",
    "labels": [
      "collaboration"
    ],
    "state": "open",
    "created_at": "2025-03-04T00:09:49+00:00",
    "closed_at": null,
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4042/reactions",
      "total_count": 97,
      "+1": 27,
      "-1": 0,
      "laugh": 2,
      "hooray": 3,
      "confused": 2,
      "heart": 8,
      "rocket": 51,
      "eyes": 4
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/4042"
  },
  {
    "number": 2561,
    "title": "[Feature] Running multi-node offline engine inference ( via SLURM)",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nA lot of academic institutions only allow access to larger node clusters via SLURM and it is not immediately clear how would I reuse the code to run Llama 405B BF16 on 2 nodes (by starting a server) to perform offline inference\n\n### Related resources\n\n_No response_",
    "labels": [
      "help wanted",
      "collaboration",
      "feature"
    ],
    "state": "closed",
    "created_at": "2024-12-23T15:24:49+00:00",
    "closed_at": "2025-01-31T23:58:27+00:00",
    "comments": 39,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2561/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2561"
  },
  {
    "number": 3658,
    "title": "[Bug] sglang crashed when use enable_dp_attention running DeepSeekV3 on 2x8xH100",
    "body": "[server.log](https://github.com/user-attachments/files/18840336/server.log)\n\n### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nAccording to the dp-attention performance & usage, I turn on it by --enable-dp-attention when launching DeepSeek v3 on 2x8xH100. My command is like as below:\n`docker run --gpus all -d --entrypoint=python3 --shm-size 32g --privileged -e NCCL_IB_HCA=mlx5_1,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7,mlx5_8 -e NCCL_IB_QPS_PER_CONNECTION=2 -e NCCL_IB_ADAPTIVE_ROUTING=1 -e NCCL_DEBUG_SUBSYS=INIT,ENV,GRAPH -e NCCL_NVLS_ENABLE=0 -e NCCL_IB_GID_INDEX=3 -e NCCL_DEBUG=TRACE --network=host  --ipc=host lmsysorg/sglang:v0.4.3-cu124 -m sglang.launch_server --model-path /sgl-workspace/deepseek-ai/DeepSeekV3/ --tp 16 --nccl-init-addr sgl-master:50001 --nnodes 2 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 8000 --watchdog-timeout 3600 --kv-cache-dtype fp8_e5m2 --enable-dp-attention --mem-fraction-static 0.78 2>&1`\nWhen I run test scripts, the server crashed\n\n> \n> [2025-02-18 06:32:23 DP7 TP7] Prefill batch. #new-seq: 8, #new-token: 4096, #cached-token: 8, cache hit rate: 0.23%, token usage: 0.00, #running-req: 2, #queue-req: 0\n> [2025-02-18 06:32:23 DP6 TP6] Prefill batch. #new-seq: 8, #new-token: 2265, #cached-token: 8, cache hit rate: 0.38%, token usage: 0.00, #running-req: 2, #queue-req: 0\n> [rank2]:[E218 06:32:24.828983440 ProcessGroupNCCL.cpp:1595] [PG ID 2 PG GUID 3 Rank 2] Process group watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\n> CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n> For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n> Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n> \n> Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):\n> frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f3545f6c446 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\n> frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f3545f166e4 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\n> frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f354633ea18 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10_cuda.so)\n> frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7f34fbe25726 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\n> frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0xa0 (0x7f34fbe2a3f0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\n> frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7f34fbe31b5a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\n> frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f34fbe3361d in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\n> frame #7: <unknown function> + 0x145c0 (0x7f3547b375c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)\n> frame #8: <unknown function> + 0x94ac3 (0x7f35489c0ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n> frame #9: <unknown function> + 0x126850 (0x7f3548a52850 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n> \n> [2025-02-18 06:32:24 DP2 TP2] TpModelWorkerClient hit an exception: Traceback (most recent call last):\n>   File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 109, in forward_thread_func\n>     self.forward_thread_func_()\n>   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n>     return func(*args, **kwargs)\n>   File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 140, in forward_thread_func_\n>     logits_output, next_token_ids = self.worker.forward_batch_generation(\n>   File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 164, in forward_batch_generation\n>     logits_output = self.model_runner.forward(forward_batch)\n>   File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 795, in forward\n>     return self.forward_extend(forward_batch)\n>   File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 760, in forward_extend\n>     return self.model.forward(\n>   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n>     return func(*args, **kwargs)\n>   File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 868, in forward\n>     hidden_states = self.model(input_ids, positions, forward_batch)\n>   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n>     return self._call_impl(*args, **kwargs)\n>   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n>     return forward_call(*args, **kwargs)\n>   File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 829, in forward\n>     hidden_states, residual = layer(\n>   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n>     return self._call_impl(*args, **kwargs)\n>   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n>     return forward_call(*args, **kwargs)\n>   File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 781, in forward\n>     hidden_states = self.mlp(hidden_states)\n>   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n>     return self._call_impl(*args, **kwargs)\n>   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n>     return forward_call(*args, **kwargs)\n>   File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 177, in forward\n>     self.experts(hidden_states=hidden_states, router_logits=router_logits)\n>   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n>     return self._call_impl(*args, **kwargs)\n>   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n>     return forward_call(*args, **kwargs)\n>   File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/layer.py\", line 589, in forward\n>     final_hidden_states = self.quant_method.apply(\n>   File \"/sgl-workspace/sglang/python/sglang/srt/layers/quantization/fp8.py\", line 820, in apply\n>     return fused_experts(\n>   File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\", line 851, in fused_experts\n>     torch.ops.sglang.inplace_fused_experts(\n>   File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 1116, in __call__\n>     return self._op(*args, **(kwargs or {}))\n>   File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\", line 731, in inplace_fused_experts\n>     fused_experts_impl(\n>   File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\", line 1057, in fused_experts_impl\n>     torch.sum(\n> RuntimeError: CUDA error: an illegal memory access was encountered\n> CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n> For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n> Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n> \n> \n> During handling of the above exception, another exception occurred:\n> \n> Traceback (most recent call last):\n>   File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 108, in forward_thread_func\n>     with torch.get_device_module(self.device).stream(self.forward_stream):\n>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 595, in __exit__\n>     torch.cuda.set_stream(self.src_prev_stream)  # type: ignore[arg-type]\n>   File \"/usr/local/lib/python3.10/dist-packages/vllm/utils.py\", line 962, in _patched_set_stream\n>     prev_set_stream(stream)\n>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 636, in set_stream\n>     _set_stream_by_id(\n>   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 618, in _set_stream_by_id\n>     torch._C._cuda_setStream(\n> RuntimeError: CUDA error: CUDA-capable device(s) is/are busy or unavailable\n> CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n> For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n> Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n> \n> \n> terminate called after throwing an instance of 'c10::DistBackendError'\n> terminate called recursively\n> Fatal Python error: Aborted\n> \n> Thread 0x00007f2fe8afc640 (most recent call first):\n>   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 462 in watchdog_thread\n>   File \"/usr/lib/python3.10/threading.py\", line 953 in run\n>   File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n>   File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n> \n> Current thread 0x00007f1f035fe640 (most recent call first):\n>   File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 113 in forward_thread_func\n>   File \"/usr/lib/python3.10/threading.py\", line 953 in run\n>   File \"/usr/lib/python3.10/threading.py\", line 1016  what():   in [PG ID 2 PG GUID 3 Rank 2] Process group watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\n> CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n> For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n> Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nI tried to change the value of `mem-fraction-static`, but it doesn't work.\n\n### Environment\n\n\n### Reproduction\n\non node 1\n`docker run --gpus all -d --entrypoint=python3 --shm-size 32g --privileged -e NCCL_IB_HCA=mlx5_1,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7,mlx5_8 -e NCCL_IB_QPS_PER_CONNECTION=2 -e NCCL_IB_ADAPTIVE_ROUTING=1 -e NCCL_DEBUG_SUBSYS=INIT,ENV,GRAPH -e NCCL_NVLS_ENABLE=0 -e NCCL_IB_GID_INDEX=3 -e NCCL_DEBUG=TRACE --network=host  --ipc=host lmsysorg/sglang:v0.4.3-cu124 -m sglang.launch_server --model-path /sgl-workspace/deepseek-ai/DeepSeekV3/ --tp 16 --nccl-init-addr sgl-master:50001 --nnodes 2 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 8000 --watchdog-timeout 3600 --kv-cache-dtype fp8_e5m2 --enable-dp-attention --mem-fraction-static 0.78 2>&1`\n\non node 2\n`docker run --gpus all -d --entrypoint=python3 --shm-size 32g --privileged -e NCCL_IB_HCA=mlx5_1,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7,mlx5_8 -e NCCL_IB_QPS_PER_CONNECTION=2 -e NCCL_IB_ADAPTIVE_ROUTING=1 -e NCCL_DEBUG_SUBSYS=INIT,ENV,GRAPH -e NCCL_NVLS_ENABLE=0 -e NCCL_IB_GID_INDEX=3 -e NCCL_DEBUG=TRACE --network=host  --ipc=host lmsysorg/sglang:v0.4.3-cu124 -m sglang.launch_server --model-path /sgl-workspace/deepseek-ai/DeepSeekV3/ --tp 16 --nccl-init-addr sgl-master:50001 --nnodes 2 --node-rank 1 --trust-remote-code --host 0.0.0.0 --port 8000 --watchdog-timeout 3600 --kv-cache-dtype fp8_e5m2 --enable-dp-attention --mem-fraction-static 0.78 2>&1`\n\n\n### Environment\n\nPython: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.183.06\nPyTorch: 2.5.1+cu124\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.1.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.49.0\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.23.0\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.63.2\ntiktoken: 0.9.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    CPU AffinityNUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-47,96-1430N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-47,96-1430N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     0-47,96-1430N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     0-47,96-1430N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    48-95,144-191       1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    48-95,144-191       1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    48-95,144-191       1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     48-95,144-191       1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC1    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC2    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    PIX      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC3    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      NODE    NODE    SYS     SYS     SYS     SYS\nNIC4    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE     X      NODE    SYS     SYS     SYS     SYS\nNIC5    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE     X      SYS     SYS     SYS     SYS\nNIC6    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS      X      NODE    NODE    NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE     X      NODE    NODE\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      NODE\nNIC9    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n\n\nulimit soft: 1048576",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-02-18T06:53:23+00:00",
    "closed_at": "2025-05-22T00:19:10+00:00",
    "comments": 29,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3658/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3658"
  },
  {
    "number": 5100,
    "title": "[Bug] common_ops.abi3.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHello, \n\nI successfully built the sgl-kernel with sm_120 (NVIDIA RTX 50 series) and CUDA 12.8, but encountered the following issue when running `sglang.launch_server `command. Please help.\n\n```\nINFO 04-06 14:51:04 [__init__.py:256] Automatically detected platform cuda.\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/home/admin2/Projects/sglang/python/sglang/launch_server.py\", line 6, in <module>\n    from sglang.srt.entrypoints.http_server import launch_server\n  File \"/home/admin2/Projects/sglang/python/sglang/srt/entrypoints/http_server.py\", line 45, in <module>\n    from sglang.srt.entrypoints.engine import _launch_subprocesses\n  File \"/home/admin2/Projects/sglang/python/sglang/srt/entrypoints/engine.py\", line 40, in <module>\n    from sglang.srt.managers.data_parallel_controller import (\n  File \"/home/admin2/Projects/sglang/python/sglang/srt/managers/data_parallel_controller.py\", line 27, in <module>\n    from sglang.srt.managers.io_struct import (\n  File \"/home/admin2/Projects/sglang/python/sglang/srt/managers/io_struct.py\", line 25, in <module>\n    from sglang.srt.managers.schedule_batch import BaseFinishReason\n  File \"/home/admin2/Projects/sglang/python/sglang/srt/managers/schedule_batch.py\", line 45, in <module>\n    from sglang.srt.configs.model_config import ModelConfig\n  File \"/home/admin2/Projects/sglang/python/sglang/srt/configs/model_config.py\", line 25, in <module>\n    from sglang.srt.layers.quantization import QUANTIZATION_METHODS\n  File \"/home/admin2/Projects/sglang/python/sglang/srt/layers/quantization/__init__.py\", line 54, in <module>\n    from sglang.srt.layers.quantization.awq import AWQConfig\n  File \"/home/admin2/Projects/sglang/python/sglang/srt/layers/quantization/awq.py\", line 6, in <module>\n    from sgl_kernel import awq_dequantize\n  File \"/home/admin2/.virtualenvs/sglang/lib/python3.10/site-packages/sgl_kernel/__init__.py\", line 12, in <module>\n    from sgl_kernel import common_ops\nImportError: /home/admin2/.virtualenvs/sglang/lib/python3.10/site-packages/sgl_kernel/common_ops.abi3.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n```\n\n### Reproduction\n\n```\n  \n  python3 -m sglang.launch_server \\\n    --model-path meta-llama/Llama-3.1-8B-Instruct \\\n    --dtype bfloat16 \\\n    --attention-backend flashinfer \\\n    --enable-torch-compile \n```\n\n### Environment\n\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0: NVIDIA GeForce RTX 5090\nGPU 0 Compute Capability: 12.0\nCUDA_HOME: /usr/local/cuda-12.8\nNVCC: Cuda compilation tools, release 12.8, V12.8.93\nCUDA Driver Version: 570.124.06\nPyTorch: 2.8.0.dev20250405+cu128\nsglang: 0.4.4.post4\nsgl_kernel: 0.0.8\nflashinfer: Module Not Found\ntriton: 3.3.0\ntransformers: 4.51.0\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.14\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.1\ninteregular: 0.3.3\nmodelscope: 1.24.0\norjson: 3.10.15\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.4.dev254+ged6ea065.d20250319.cu128\nxgrammar: 0.1.11\nopenai: 1.67.0\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.63.12\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-63    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1024",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-06T10:23:24+00:00",
    "closed_at": "2025-05-08T21:26:52+00:00",
    "comments": 32,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5100/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5100"
  },
  {
    "number": 5031,
    "title": "[Bug] Still very slow performance for mid-range prompt sizes ~ 8k tokens with MLA",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n~8k token prompts are even worse than 0.4.3.post2 (was 10 tokens/sec, not 6 tokens/sec) so it now about 5x slower than without MLA.  But of course MLA is faster on very long context (with MLA its about 24 tokens/sec at 118k tokens input, and without is 3 tokens/sec).\n\nBut there is never a win-win scenario here.  Either 8k is very bad (now even worse) or 100k is bad.\n\n![Image](https://github.com/user-attachments/assets/6b815adf-0352-4b8b-8bb7-6349657f3002)\n\n\nOriginal problem remains from here: https://github.com/sgl-project/sglang/issues/3716\n\n### Reproduction\n\n```\ndocker stop r1 ; docker remove r1\ndocker run -d --gpus all --shm-size 32g -p 5000:5000 --name r1 \\\n    -e SGL_ENABLE_JIT_DEEPGEMM=1 \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface --ipc=host ccr.ccs.tencentyun.com/app-public/sglang:dev-c764-0402fix \\\n    python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1 --tp 8 \\\n    --enable-torch-compile \\\n    --enable-flashinfer-mla \\\n    --trust-remote-code --port 5000 --host 0.0.0.0 \\\n    --api-key EMPTY  --random-seed 1234\n```\n\n```\nimport time\nimport openai\nfrom transformers import AutoTokenizer\nfrom datetime import datetime\n\ndef count_tokens(prompt, model_name=\"deepseek-ai/DeepSeek-R1\"):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    return len(tokenizer.encode(prompt))\n\ndef measure_performance(prompt, model=\"deepseek-ai/DeepSeek-R1\", max_tokens=64):\n    url = 'IP'\n    api_key = 'EMPTY'\n    client = openai.Client(base_url=f\"http://{url}:5000/v1\", api_key=api_key)\n\n    token_count = count_tokens(prompt, model)\n    start_time = time.time()\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0,\n        max_tokens=max_tokens,\n        stream=True  # Enable streaming mode\n    )\n\n    first_token_time = None\n    total_tokens = 0\n    first_token_received = False\n\n    for chunk in response:\n        if not first_token_received:\n            first_token_time = time.time() - start_time\n            first_token_received = True\n        if chunk.choices[0].delta.content:\n            total_tokens += len(chunk.choices[0].delta.content.split())\n\n    total_time = time.time() - start_time\n    tps = total_tokens / total_time if total_time > 0 else 0\n\n    return {\n        \"prompt_length\": token_count,\n        \"max_tokens\": max_tokens,\n        \"time_to_first_token\": first_token_time,\n        \"tokens_per_second\": tps,\n        \"total_time\": total_time,\n        \"total_tokens\": total_tokens,\n    }\n\ndef generate_markdown(results):\n    md_report = \"\"\"# Token Performance Analysis\n\n## Summary of Response Time and Throughput\n\n| Prompt Length | Max Tokens | Time to First Token (s) | Tokens per Second | Total Time (s) | Total Tokens |\n|--------------|------------|-------------------------|-------------------|---------------|-------------|\n\"\"\"\n\n    for res in results:\n        md_report += f\"| {res['prompt_length']} | {res['max_tokens']} | {res['time_to_first_token']:.4f} | {res['tokens_per_second']:.4f} | {res['total_time']:.4f} | {res['total_tokens']} |\\n\"\n\n    return md_report\n\ndef main():\n    test_cases = [\n        (\"Write an extremely long story.\", 8192),\n        (\"word \" * 8000 + \"Write an extremely long story.\", 8192),\n        (\"word \" * 118000 + \"Write an extremely long story.\", 8192)\n    ]\n\n    results = []\n    for prompt, max_tokens in test_cases:\n        res = measure_performance(prompt, max_tokens=max_tokens)\n        results.append(res)\n\n    markdown_report = generate_markdown(results)\n\n    with open(\"performance_report.md\", \"w\") as f:\n        f.write(markdown_report)\n\n    print(\"Performance report generated: performance_report.md\")\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n### Environment\n\nINFO 02-20 03:00:02 __init__.py:190] Automatically detected platform cuda.\nPython: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H200\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.144.03\nPyTorch: 2.5.1+cu124\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.1.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.23.0\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.63.2\ntiktoken: 0.9.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10   NIC11   CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    SYS     NODE    PIX     SYS     SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     PHB     PHB     SYS     NODE    NODE    SYS     SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS     NODE    NODE    SYS     SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    SYS     PIX     NODE    SYS     SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     NODE    SYS     SYS     NODE    NODE    NODE    PIX     NODE    96-191,288-383  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE    SYS     SYS     NODE    NODE    NODE    NODE    PIX     96-191,288-383  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE    SYS     SYS     PIX     PHB     PHB     NODE    NODE    96-191,288-383  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     PIX     SYS     SYS     NODE    NODE    NODE    NODE    NODE    96-191,288-383  1               N/A\nNIC0    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     NODE    NODE    SYS     SYS     SYS     SYS     SYS\nNIC1    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      PHB     PHB     SYS     NODE    NODE    SYS     SYS     SYS     SYS     SYS\nNIC2    NODE    PHB     NODE    NODE    SYS     SYS     SYS     SYS     NODE    PHB      X      PIX     SYS     NODE    NODE    SYS     SYS     SYS     SYS     SYS\nNIC3    NODE    PHB     NODE    NODE    SYS     SYS     SYS     SYS     NODE    PHB     PIX      X      SYS     NODE    NODE    SYS     SYS     SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS      X      SYS     SYS     NODE    NODE    NODE    NODE    NODE\nNIC5    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS      X      NODE    SYS     SYS     SYS     SYS     SYS\nNIC6    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS     NODE     X      SYS     SYS     SYS     SYS     SYS\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    SYS     SYS      X      PHB     PHB     NODE    NODE\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    PHB     NODE    SYS     SYS     SYS     SYS     NODE    SYS     SYS     PHB      X      PIX     NODE    NODE\nNIC9    SYS     SYS     SYS     SYS     NODE    NODE    PHB     NODE    SYS     SYS     SYS     SYS     NODE    SYS     SYS     PHB     PIX      X      NODE    NODE\nNIC10   SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    SYS     SYS     NODE    NODE    NODE     X      NODE\nNIC11   SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE    SYS     SYS     NODE    NODE    NODE    NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n  NIC10: mlx5_10\n  NIC11: mlx5_11\n\n\nulimit soft: 1048576\n",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-04-03T10:16:15+00:00",
    "closed_at": "2025-04-17T19:36:01+00:00",
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5031/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5031"
  },
  {
    "number": 1270,
    "title": "[Bug] sglang run for few hours, it will stop returning valid response",
    "body": "### Checklist\r\n\r\n- [x] 1. I have searched related issues but cannot get the expected help.\r\n- [x] 2. The bug has not been fixed in the latest version.\r\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [x] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nsglang run for few hours, it will stop returning valid response, based on the pm2 logs, it does not triggering any error, or message\r\n<img width=\"620\" alt=\"image\" src=\"https://github.com/user-attachments/assets/9af04d3e-c831-464f-8932-2cacea3dd300\">\r\n\r\n\r\n\r\nExpected its always return token output as below\r\n<img width=\"1030\" alt=\"image\" src=\"https://github.com/user-attachments/assets/44c21df9-fb4c-485d-8b8f-369f2242b47a\">\r\n\r\nI had to restart every 5-8 hours, when it stop working...\r\n\r\n\r\n\r\n### Reproduction\r\n\r\njust keep sending request with 1-8 concurrent requests with large input max_new_token around 1000- 7000 for around 7-8 hours or lesser you will see it stop generating token outputs.\r\n\r\n```\r\npython -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --port 5000 --api-key xxxx --tp 8 --mem-fraction-static 0.8\r\n\r\n# tried with 0.7 mem-fraction-static same things happend\r\n```\r\nabove is my running command\r\n\r\n### Environment\r\n\r\nmain branch\r\n```\r\nName: sglang\r\nVersion: 0.2.14.post2\r\nSummary: SGLang is yet another fast serving framework for large language models and vision language models.\r\nHome-page: https://github.com/sgl-project/sglang\r\nAuthor: \r\nAuthor-email: \r\nLicense: Apache License\r\n\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-30T18:46:21+00:00",
    "closed_at": "2024-11-14T23:46:56+00:00",
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1270/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1270"
  },
  {
    "number": 2542,
    "title": "[Feature] (Willing to PR) Avoid KV cache occupying GPU memory when not used",
    "body": "### Checklist\r\n\r\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [X] 2. Please use English, otherwise it will be closed.\r\n\r\n### Motivation\r\n\r\nHi thank you for the library! The use case is that, when doing online PPO, I hope to use SGLang to generate llm completions, and then use RL to do gradient descent on those completions.\r\n\r\nThe problem is, to do this on a single GPU, the timeline is \"SGLang generate - Torch backward - repeat it\". Thus, when torch doing backprop, I hope SGLang can free its KV cache memory consumption, otherwise torch will not have enough memory.\r\n\r\nThanks for any suggestions!\r\n\r\n### Related resources\r\n\r\n_No response_",
    "labels": [
      "high priority",
      "collaboration",
      "inactive",
      "feature"
    ],
    "state": "closed",
    "created_at": "2024-12-22T09:07:26+00:00",
    "closed_at": "2025-03-16T14:34:36+00:00",
    "comments": 43,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2542/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2542"
  },
  {
    "number": 4655,
    "title": "[Roadmap] Prefill and Decoding Disaggregation",
    "body": "### Design: \n\n[SGLang PD Disaggregation (Open Source)](https://docs.google.com/document/d/1rQXJwKd5b9b1aOzLh98mnyMhBMhlxXA5ATZTHoQrwvc/edit?tab=t.0#heading=h.i3s2t1j0e1ik)\n\n### Progress\n- [x] Release initial code @ByronHsu  #4654\n  - prefill and decode event loop, queue, and transfer interface\n  - **transfer engine is faked** \n  - easy python load balancer\n- [x] Mooncake integration @ShangmingCai   https://github.com/sgl-project/sglang/pulls?q=is%3Apr+mooncake+is%3Aopen\n- [x] NIXL Integration @trevor-m #5477\n- [x] PD + overlap schedule @ByronHsu \n- [x] PD + DP attention @ch-wan @ByronHsu \n- [x] PD + fault tolerance https://github.com/sgl-project/sglang/pull/6504 https://github.com/sgl-project/sglang/pull/6263\n- [x] PD + spec decode https://github.com/sgl-project/sglang/pull/6507\n- [x] PD + logprob https://github.com/sgl-project/sglang/pull/6558\n- [x] PD + Structured Output https://github.com/sgl-project/sglang/pull/6560\n\n- [x] PD + retract @Ying1123 https://github.com/sgl-project/sglang/pull/7196\n- [x] PD + different TPs - call out for contribution https://github.com/sgl-project/sglang/pull/5922 https://github.com/sgl-project/sglang/pull/6793\n- [x] Rust PD Load Balancer @hnyls2002  https://github.com/sgl-project/sglang/pull/6437\n- [ ] PD + ROCm (Mooncake) @HaiShaw \n\n\n",
    "labels": [
      "enhancement",
      "high priority",
      "collaboration"
    ],
    "state": "open",
    "created_at": "2025-03-21T19:26:55+00:00",
    "closed_at": null,
    "comments": 30,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4655/reactions",
      "total_count": 81,
      "+1": 47,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 18,
      "eyes": 16
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4655"
  },
  {
    "number": 7429,
    "title": "[Tracking] Model support",
    "body": "### **[Tracking] Model support**\n\nThe goal is to support other model architectures available. Expand the model zoo \ud83c\udf8a \n\nThe goal is to implement support for all architectures listed below. Anyone is welcome to take any issue or implement the model below.\n\nIf you need help implementing a new model, see https://docs.sglang.ai/supported_models/support_new_models.html\n\n#### Text-only Language Models (Generative)\n- [ ] `OPTForCasualLM` (facebook/opt-125m) #7440 \n- [ ] `AquilaForCausalLM` (Aquila, Aquila2)\n- [ ] `ArcticForCausalLM` (Arctic) #5768\n- [ ] `BambaForCausalLM` (Bamba)\n- [ ] `BartForConditionalGeneration` (BART)\n- [ ] `BloomForCausalLM` (BLOOM, BLOOMZ)\n- [ ] `Cohere2ForCasualLM` #4570\n- [ ] `DeciLMForCausalLM` (DeciLM)\n- [ ] `FalconForCausalLM` (Falcon)\n- [ ] `FalconH1ForCausalLM` (Falcon-H1) #6517\n- [ ] `FalconMambaForCausalLM` (FalconMamba)\n- [ ] `Dots1ForCasualLM` (dots.llm1) #6471\n- [ ] `GPT2LMHeadModel` (GPT-2)\n- [ ] `GPTBigCodeForCausalLM` (StarCoder, SantaCoder)\n- [ ] `GPTJForCausalLM` (GPT-J)\n- [ ] `GPTNeoXForCausalLM` (GPT-NeoX, Pythia)\n- [ ] `GraniteForCausalLM` (Granite 3.0, 3.1)\n- [ ] `GraniteMoeForCausalLM` (Granite 3.0 MoE)\n- [ ] `GraniteMoeHybridForCausalLM` (Granite 4.0 MoE Hybrid)\n- [ ] `GraniteMoeSharedForCausalLM` (Granite MoE Shared)\n- [ ] `GritLM` (GritLM)\n- [ ] `InternLMForCausalLM` (InternLM v1)\n- [ ] `JAISLMHeadModel` (Jais)\n- [ ] `JambaForCausalLM` (Jamba) #1190\n- [ ] `MambaForCausalLM` (Mamba)\n- [ ] `Mamba2ForCausalLM` (Mamba2)\n- [ ] `MiniCPMForCausalLM` (MiniCPM v1) #6900\n- [ ] `MiniMaxM1ForCausalLM` (MiniMax-Text) #2898\n- [ ] `MiniMaxText01ForCausalLM` (MiniMax-Text-01)\n- [ ] `MPTForCausalLM` (MPT)\n- [ ] `NemotronForCausalLM` (Nemotron-3) #5063\n- [ ] `NemotronHForCausalLM` (Nemotron-H)\n- [ ] `OLMoForCausalLM` (OLMo v1)\n- [ ] `OLMo2ForCausalLM` (OLMo2)\n- [ ] `OPTForCausalLM` (OPT)\n- [ ] `OrionForCausalLM` (Orion)\n- [ ] `PersimmonForCausalLM` (Persimmon)\n- [x] `PhiForCausalLM` (Phi-1.5, Phi-2) #7862 @ppraneth \n- [x] `Phi3SmallForCausalLM` (Phi-3-Small)\n- [x] `PhiMoEForCausalLM` (Phi-3.5-MoE) #7907 @byjiang1996 \n- [ ] `Plamo2ForCausalLM` (PLaMo2)\n- [ ] `SolarForCausalLM` (Solar Pro)\n- [ ] `Starcoder2ForCausalLM` (Starcoder2)\n- [ ] `TeleChat2ForCausalLM` (TeleChat2) \n- [ ] `TeleFLMForCausalLM` (TeleFLM)\n- [ ] `Zamba2ForCausalLM` (Zamba2)\n\n#### Embedding Models\n- [ ] `GteModel`\n- [ ] `GteNewModel`\n- [ ] `ModernBertModel`\n- [ ] `NomicBertModel`\n- [ ] `RobertaModel`\n- [ ] `JambaForSequenceClassification`\n- [ ] `BertForSequenceClassification`\n- [ ] `Qwen3ForSequenceClassification` #7314\n- [ ] `RobertaForSequenceClassification`\n- [ ] `XLMRobertaForSequenceClassification`\n\n#### Multimodal Models\n- [ ] `Glm4vForConditionalGeneration` (THUDM/GLM-4.1V-9B-Thinking)\n- [ ] `AriaForConditionalGeneration` (Aria)\n- [ ] `AyaVisionForConditionalGeneration` (Aya Vision) #6304\n- [ ] `Blip2ForConditionalGeneration` (BLIP-2) #4414\n- [ ] `ChameleonForConditionalGeneration` (Chameleon)\n- [ ] `Florence2ForConditionalGeneration` (Florence-2)\n- [ ] `FuyuForCausalLM` (Fuyu)\n- [ ] `GLM4VForCausalLM` PP support #7257\n- [ ] `GraniteSpeechForConditionalGeneration` (Granite Speech)\n- [ ] `H2OVLChatModel` (H2OVL)\n- [ ] `Idefics3ForConditionalGeneration` (Idefics3)\n- [ ] `LlavaNextVideoForConditionalGeneration` (LLaVA-NeXT-Video) #4062\n- [ ] `MiniMaxVL01ForConditionalGeneration` (MiniMax-VL)\n- [ ] `MolmoForCausalLM` (Molmo)\n- [ ] `NVLM_D_Model` (NVLM-D 1.0)\n- [ ] `Ovis` (Ovis1.6, Ovis2) #5018\n- [ ] `PaliGemmaForConditionalGeneration` (PaliGemma)\n- [ ] `Phi3VForCausalLM` (Phi-3-Vision) #1108\n- [ ] `PixtralForConditionalGeneration` (Pixtral)\n- [ ] `Qwen2AudioForConditionalGeneration` (Qwen2-Audio)\n- [ ] `Qwen2_5OmniThinkerForConditionalGeneration` (Qwen2.5-Omni) #4969\n- [ ] `SkyworkR1VChatModel` (Skywork-R1V) #4692\n- [ ] `SmolVLMForConditionalGeneration` (SmolVLM2)\n- [ ] `TarsierForConditionalGeneration` (Tarsier)\n- [ ] `Tarsier2ForConditionalGeneration` (Tarsier2)\n\n---\n**Related Issues & PRs**\n\n- Support TRI-ML/prismatic-vlms: #1129\n- facebook/contriever support: #3720\n- Support Gemma 3 QAT models: #5591\n- Bytedancer: #6724",
    "labels": [
      "good first issue",
      "new-model"
    ],
    "state": "open",
    "created_at": "2025-06-21T22:18:10+00:00",
    "closed_at": null,
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7429/reactions",
      "total_count": 15,
      "+1": 10,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 5,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7429"
  },
  {
    "number": 5450,
    "title": "[Bug] PD disaggregation, KV transfer slow down under high concurrency",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen running bench_serving, we send 200 prompts(isl=4k, osl=2k) in one shot, the first 100 reqs's KV were transferred in 30s. But the second 100 reqs's KV transfer took 5min.\n\nI've print out the prealloc queue and transfer queue size in decode node. We can see the transfer-queue size was decreasing very slowly after 2025-04-16 12:40:10.  [log.txt](https://github.com/user-attachments/files/19771683/log.txt)\n\n### Reproduction\n\nNode1:\n```\nMOONCAKE_CONFIG_PATH=./mooncake.json \\\n  python -m sglang.launch_server \\\n  --model-path my_model_path \\\n  --disaggregation-mode prefill \\\n  --port 30000 --host 192.168.1.1 \\\n  --max-prefill-tokens 32768  --chunked-prefill-size 32768 \\\n  --context-len 32768 \\\n  --max-running-requests 100 \\\n  --tp 8 --trust-remote-code\n```\n\nNode2:\n```\nMOONCAKE_CONFIG_PATH=./mooncake.json \\\n  python -m sglang.launch_server \\\n  --model-path my_model_path \\\n  --disaggregation-mode decode \\\n  --port 30000 --host 192.168.1.2 \\\n  --max-running-requests 100 \\\n  --tp 8 --trust-remote-code\n```\n\n```\npython -m sglang.srt.disaggregation.mini_lb \\\n  --prefill http://192.168.1.1:30000 \\\n  --decode http://192.168.1.2:30000 \\\n  --host 0.0.0.0 --port 8000\n```\n\n```\npython -m sglang.bench_serving --backend sglang   \\\n  --dataset-name random   --dataset-path /home/ShareGPT_V3_unfiltered_cleaned_split/ShareGPT_V3_unfiltered_cleaned_split.json  \\\n  --random-input 4096   --random-output 2048   --random-range-ratio 1   \\\n  --num-prompts 200  --max-concurrency 200   \\\n  --host 127.0.0.1  --port 8000   \\\n  --output-file \"out.jsonl\"  \\\n  --pd-seperated --flush-cache\n```\n\n### Environment\n\n- 2 H800*8 nodes\n- RDMA*4 for each node\n- sglang main branch, commit id: 6aca583420490d8bebafc6239085b49f78325c0c\n- mooncake-transfer-engine  0.3.0b3",
    "labels": [],
    "state": "open",
    "created_at": "2025-04-16T06:35:41+00:00",
    "closed_at": null,
    "comments": 36,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5450/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5450"
  },
  {
    "number": 3956,
    "title": "DeepSeek-R1 Optimization Option Ablations",
    "body": "Updated on **2025-03-20**: #4616\n\nUpdated on **2025-03-04**:\n# DeepSeek Optimization Ablations\n\n## Overview\n\nWe sincerely thanks for the help from [M0gician](http://m0gician.github.io/) for the massive experiments.\n\n**As of 2025-03-04**, SGLang provides the following optimizations for DeepSeek V3/R1 models:\n\n| Name                                        | Description                                                                                                                                                                                                                                     | Enabled by Default | Enable/Disable Argument                                                                                                                                   |\n|---------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| MLA Optimization                            | SGLang custom tailored optimizations, including<br>  - *Weight Absorption*,<br>- *Flashinfer MLA Wrapper*,<br> - *FP8 Quantization*,<br> - *CUDA Graph & Torch.compile suuport*                                                                 | \u2705               | `--disable-mla`                                                                                                                                           |\n| CUDA Graph                                  | Capturing and replaying entire sequences of GPU operations as a single graph, thereby reducing kernel launch overhead and synchronization delays                                                                                                | \u2705               | `--disable-cuda-graph`                                                                                                                                    |\n| Torch Compile                               | Dynamically converting PyTorch models into optimized execution graphs, reducing runtime overhead and enhancing GPU performance                                                                                                                  | \u274c              | `--enable-torch-compile`                                                                                                                                 |\n| Radix Cache                                 | Organizes the KV cache in a radix tree, enabling automatic detection and reuse of shared prompt prefixes across multiple generation calls, thereby reducing redundant computations                                                              | \u2705               | `--disable-radix-cache`                                                                                                                                   |\n| Flashinfer MLA                              | Multi-latent Attention implemented by Flashinfer that replaces the default Triton backend                                                                                                                                                       | \u274c              | `--enable-flashinfer-mla`                                                                                                                                 |\n| Speculative Decoding (`Next-N`)             | Dynamically generating a context-aware draft token tree with a smaller, well-calibrated model and then verifying these tokens in parallel with the original LLM, thereby reducing expensive forward passes while preserving output quality.     | \u274c              | `--speculative-algorithm`,<br> `--speculative-draft`,<br> `--speculative-num-steps`,<br> `--speculative-eagle-topk`,<br> `--speculative-num-draft-tokens` |\n| Tensor Parallelism (`tp`)                   | Splitting the heavy tensor operations\u2014such as the matrix multiplications in self-attention and feedforward layers\u2014across multiple GPUs, thereby lowering the per-device memory burden and enabling simultaneous computation for reduced latency | \u2705 (=1)         | `--tp-size`                                                                                                                                               |\n| Expert Parallelism (`EP-MoE`)               | Distributing the computation of different expert subnetworks across multiple devices, thereby reducing memory constraints and communication overhead while enabling simultaneous, efficient processing of input tokens.                         | \u274c              | `--enable-ep-moe`,<br> `--ep-size`                                                                                                                        |\n| Data Parallelism Attention (`DP-Attention`) | Partitioning the MLA attention across DP workers\u2014each handling independent prefill, decode, and idle batches\u2014to significantly reduce per-worker KV cache size and enable larger, more efficient batch processing                                | \u274c              | `--enable-dp-attention`                                                                                                                                   |\n\n## General Advice\n\n* Speculative Decoding is great for small concurrency (less than 32), but its performance degrades quickly as the concurrency increases.\n* `CUDA Graph` boosts inference performance significantly, at the cost of increased memory usage. Sometimes it's a good trade-off to disable `CUDA Graph` to further increase concurrency to get better throughput.\n* `DP-Attention` is a must for large concurrency (greater than 256), but it hurts per-request decoding speed.\n\n## Known Issues\n\n* Speculative Decoding is not compatible with:\n  - `Flashinfer-mla`\n  - `Radix Cache`\n  - `DP-Attention`\n  - Both `CUDA Graph` and `Torch Compile` enabled simultaneously\n* `EP-MoE` is not supported with both `CUDA Graph` and `Torch Compile` enabled\n* To run `DP-Attention` with large concurrency, you must first run a warmup phase with small concurrency (e.g. `bs=16`, `total req=32`) to avoid CUDA out of memory error.\n\n## Optimization Ablations\n\n### Test Environment\n\n* SGLang version: 0.4.3.post2@[110e006](https://github.com/sgl-project/sglang/commit/110e0066735a3bd431c2640ae168fc040d7c0806)\n* Flashinfer version: 0.2.2.post1\n* Hardware: 2 nodes of H20 (AMD EPYC 9K84 * 2, 2.20 TiB memory, 8 * H20 96GiB each)\n* Model: DeepSeek-R1\n* Model Max Length: 3200 (modified in both model and NextN's `tokenizer_config.json`)\n* CUDA Version: 12.2\n* Operating System: Rocky Linux release 9.2 (Blue Onyx)\n\n### Single Query Performance\n\n* Test query: `\u4e00\u4e2a\u6c49\u5b57\u5177\u6709\u5de6\u53f3\u7ed3\u6784\uff0c\u5de6\u8fb9\u662f\u6728\uff0c\u53f3\u8fb9\u662f\u4e5e\u3002\u8fd9\u4e2a\u5b57\u662f\u4ec0\u4e48\uff1f\u53ea\u9700\u56de\u7b54\u8fd9\u4e2a\u5b57\u5373\u53ef\u3002`\n* Expected output: `\u675a`[^1]\n\n| Runnable           | TPS@1[^2] | Torch Compile | Cuda Graph | Radix Cache | Flashinfer-mla | Next-N | EP-MoE | DP-Attention |\n|--------------------|-----------|---------------|------------|-------------|----------------|--------|--------|--------------|\n| \u2705                 | *37.0<sub>tuned kernel</sub>[^11] |       \u2705       |      \u2705     |      \u2705      |        \u2796       |    \u2796   |    \u2796   |       \u2796      |\n| \u2705                 | 33.6     |       \u2705       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2796   |       \u2796      |\n| \u2705                 | 19.1     |       \u2705       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2796   |       \u2705      |\n| \u274c[^3]            | N/A      |       \u2705       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2705   |       \u2705      |\n| \u274c[^3]            | N/A      |       \u2705       |      \u2705     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2796      |\n| \u2705                 | 6.5      |       \u2705       |      \u2796     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2796      |\n| \u2705                 | 24.4     |       \u2796       |      \u2705     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2796      |\n| \u2705                 | 23.6     |       \u2796       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2705   |       \u2796      |\n| \u2705                 | 13.0     |       \u2796       |      \u2796     |      \u2796      |        \u2796       |    \u2705   |    \u2705   |       \u2796      |\n| \u274c[^4] <br> \u2705[^5] | 41.0     |       \u2796       |      \u2705     |      \u2796      |        \u2796       |    \u2705   |    \u2705   |       \u2796      |\n| \u274c[^3]            | N/A      |       \u2705       |      \u2705     |      \u2796      |        \u2796       |    \u2705   |    \u2705   |       \u2796      |\n| \u2705[^5]            | 16.0     |       \u2796       |      \u2705     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2705      |\n| \u274c[^3]            | N/A      |       \u2705       |      \u2705     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2705      |\n| \u2705[^5]            | 15.8     |       \u2796       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2705   |       \u2705      |\n| \u274c[^3]            | N/A      |       \u2796       |      \u2705     |      \u2705      |        \u2796       |    \u2705   |    \u2705   |       \u2705      |\n| \u274c[^6]            | N/A      |       \u2796       |      \u2796     |      \u2796      |        \u2796       |    \u2705   |    \u2796   |       \u2705      |\n\n### Batched Performance\n\n* Test bench: ThreadPool with AsyncOpenAI client\n* Avg input length = 760 tokens\n* Avg output length = 460 tokens\n\n| Runnable | Torch Compile | Cuda Graph  | Radix Cache  | Flashinfer-mla | Next-N |  EP-MoE  | DP-Attn | Client Concurrency [^7]        | Avg Throughput<br><sub><sup>(p+d, token/s)</sup></sub> [^8]                 | Per-req Throughput<br><sub><sup>(d, token/s)</sup></sub> [^9] |   Total Req    | Max-running-req [^10] |\n|----------|---------------|-------------|--------------|----------------|---------|---------|--------------|--------------------------------|----------------------------------------------------|-----------------------------------------------|---------------------|--------------------------|\n| \u2705       |       \u2705       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2796   |       \u2796      | 768                            | 3909.04                                            | 3.28                                          | 1024                | 768                      |\n| \u2705       |       \u2705       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2796   |       \u2705      | 16<br>512<br>768               | 306.18<br>4329.32<br>5457.14                       | 12.96<br>5.69<br>5.38                         | 32<br>1024<br>1024  | 768                      |\n| \u274c[^3]  |       \u2705       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2705   |       \u2705      | N/A                            | N/A                                                | N/A                                           | N/A                 | 768                      |\n| \u274c[^3]  |       \u2705       |      \u2705     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2796      | N/A                            | N/A                                                | N/A                                           | N/A                 | 768                      |\n| \u2705       |       \u2705       |      \u2796     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2796      | 768                            | 2100.85                                            | 2.79                                          | 1024                | 768                      |\n| \u2705       |       \u2796       |      \u2705     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2796      | 256<br>512<br>768              | 2134.99<br>3842.52<br>3453.49                      | 5.16<br>4.05<br>3.15                          | 512<br>1024<br>1024 | 768                      |\n| \u2705       |       \u2796       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2705   |       \u2796      | 256<br>512<br>768              | 2220.56<br>3583.08<br>3556.76                      | 5.12<br>4.07<br>3.52                          | 512<br>1024<br>1024 | 768                      |\n| \u2705       |       \u2796       |      \u2796     |      \u2796      |        \u2796       |    \u2705   |    \u2705   |       \u2796      | N/A                            | N/A                                                | N/A                                           | N/A                 | 768                      |\n| \u2705[^5]  |       \u2796       |      \u2705     |      \u2796      |        \u2796       |    \u2705   |    \u2705   |       \u2796      | 16<br>32                       | 732.22<br>1227.72                                  | 19.93<br>15.14                                | 256                 | 768                      |\n| \u274c[^3]  |       \u2705       |      \u2705     |      \u2796      |        \u2796       |    \u2705   |    \u2705   |       \u2796      | N/A                            | N/A                                                | N/A                                           | N/A                 | 768                      |\n| \u2705[^5]  |       \u2796       |      \u2705     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2705      | 16<br>128<br>256<br>512<br>768 | 862.10<br>1598.17<br>2664.40<br>4098.18<br>\u274c[^4] | 9.20<br>8.22<br>6.70<br>5.48<br>\u274c[^4]        | 128<br>256<br>512<br>1024<br>1024 | 768        |\n| \u274c[^3]  |       \u2705       |      \u2705     |      \u2705      |        \u2796       |    \u2796   |    \u2705   |       \u2705      | N/A                            | N/A                                                | N/A                                           | N/A                 | 768                      |\n| \u2705[^5]  |       \u2796       |      \u2705     |      \u2705      |        \u2705       |    \u2796   |    \u2705   |       \u2705      | 16<br>512<br>768               | 406.29<br>3633.20<br>\u274c[^4]                       | 12.29<br>5.74<br>\u274c[^4]                       | 32<br>1024<br>1024  | 768                     |\n| \u274c[^3]  |       \u2796       |      \u2705     |      \u2796      |        \u2796       |    \u2705   |    \u2705   |       \u2705      | N/A                            | N/A                                                | N/A                                           | N/A                 | 768                      |\n| \u274c[^6]  |       \u2796       |      \u2796     |      \u2796      |        \u2796       |    \u2705   |    \u2796   |       \u2705      | N/A                            | N/A                                                | N/A                                           | N/A                 | 768                      |\n\n\n[^1]: DeepSeek-R1 cannot give the correct output if quantization is used or has precision issues (fixed in [b110084](https://github.com/sgl-project/sglang/commit/b110084654a1986f0148901190e2f280c605476f))\n[^2]: TPS@1 (Tokens Per Second for single request) is read directly from SGLang's logging.\n[^3]: CUDA error at graph capture.\n[^4]: CUDA out of memory.\n[^5]: Requires setting `mem-fraction-static=0.7` to avoid OOM errors.\n[^6]: TypeError: object of type 'NoneType' has no len().\n[^7]: All statistics are collected from the test bench. Token count is calculated using the same tokenizer used in inference.\n[^8]: Average Throughput(prefill+decode, token/s) = (total tokens)/(total time).\n[^9]: Average Decoding Throughput = (sum of (output tokens/duration) for each successful request)/(number of successful requests).\n[^10]: The maximum number of requests to run concurrently at a SGLang backend, controlled by `--max-running-requests`.\n[^11]: Tested by [Lzhang-Hub](https://github.com/sgl-project/sglang/issues/3956#issuecomment-2700514223), after tuning **block wise fp8** and **fused-moe** triton kernel on H20\n\n<details>\n<summary>Updated on 2025-02-28:</summary>\nSGLang version: 0.4.3@the latest master branch (2025-02-28).\nMachine: 2 nodes of H20 (8 * H20 96G each)\n`model_max_length` in tokenizer_config.json from DeepSeek-R1 and DeepSeek-R1-NextN are modified to `3200`\n\nTest query:  `\u4e00\u4e2a\u6c49\u5b57\u5177\u6709\u5de6\u53f3\u7ed3\u6784\uff0c\u5de6\u8fb9\u662f\u6728\uff0c\u53f3\u8fb9\u662f\u4e5e\u3002\u8fd9\u4e2a\u5b57\u662f\u4ec0\u4e48\uff1f\u53ea\u9700\u56de\u7b54\u8fd9\u4e2a\u5b57\u5373\u53ef\u3002`\nExpected output: `\u675a` \n* the model cannot give the correct output if quantization is used or has precision issue (found in the previous implementation of Flashinfer-mla)\n\n| Runnable                             | TPS@1 | Torch Compile | Cuda Graph | Radix Cache | Flashinfer-mla | Next-N | EP-MoE | DP-Attention |\n|--------------------------------------|-------|---------------|------------|-------------|----------------|--------|--------|--------------|\n| \u274c (gibberish)                             |   28.5   |       \u2705       |      \u2705     |      \u2705      |        \u2705       |    \u274c   |    \u274c   |       \u274c      |\n| \u274c (gibberish)                             |   -   |       \u2705       |      \u2705     |      \u274c      |        \u2705       |    \u274c   |    \u274c   |       \u274c      |\n| \u2705                                    |  22.4 |       \u2705       |      \u2705     |      \u2705      |        \u274c       |    \u274c   |    \u274c   |       \u274c      |\n| \u274c (EAGLE not support Flashinfer)     |   -   |       \u274c       |      \u2705     |      \u274c      |        \u2705       |    \u2705   |    \u274c   |       \u274c      |\n| \u274c (CUDA error at graph capture)      |   -   |       \u2705       |      \u2705     |      \u2705      |        \u274c       |    \u274c   |    \u2705   |       \u274c      |\n| \u2705                                    |  6.5  |       \u2705       |      \u274c     |      \u2705      |        \u274c       |    \u274c   |    \u2705   |       \u274c      |\n| \u2705                                    |  24.4 |       \u274c       |      \u2705     |      \u2705      |        \u274c       |    \u274c   |    \u2705   |       \u274c      |\n| \u2705                                    |  21.3 |       \u274c       |      \u2705     |      \u2705      |        \u2705       |    \u274c   |    \u2705   |       \u274c      |\n| \u274c (OOM)<br>  \u2705 (mem-fraction-static=0.7) |  40.0 |       \u274c       |      \u2705     |      \u274c      |        \u274c       |    \u2705   |    \u2705   |       \u274c      |\n| \u274c (Capture cuda graph failed)        |   -   |       \u274c       |      \u2705     |      \u274c      |        \u274c       |    \u2705   |    \u2705   |       \u2705      |\n| \u274c (Error)                            |   -   |       \u274c       |      \u274c     |      \u274c      |        \u274c       |    \u2705   |    \u2705   |       \u2705      |\n| \u2705                                    |  15.5 |       \u274c       |      \u2705     |      \u2705      |        \u274c       |    \u274c   |    \u2705   |       \u2705      |\n| \u274c (CUDA error at graph capture)      |   -   |       \u2705       |      \u2705     |      \u2705      |        \u274c       |    \u274c   |    \u2705   |       \u2705      |\n</details>",
    "labels": [
      "high priority",
      "inactive",
      "deepseek",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2025-02-28T09:33:38+00:00",
    "closed_at": "2025-07-06T00:22:09+00:00",
    "comments": 36,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3956/reactions",
      "total_count": 40,
      "+1": 39,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3956"
  },
  {
    "number": 5338,
    "title": "[Tracker] Blackwell support",
    "body": "## Usage\n\n```bash\ndocker pull lmsysorg/sglang:blackwell\n\n# use latest main\ncd /sgl-workspace/sglang && git pull\n```\n\n## Models\n\n### DeepSeek V3 \u2705\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-V3 --tp 8 --trust-remote-code\n```\n\n### Llama 4 \u2705\n```bash\npython3 -m sglang.launch_server --model meta-llama/Llama-4-Scout-17B-16E-Instruct --tp 8 --context-length 131072\n```",
    "labels": [
      "enhancement",
      "blackwell"
    ],
    "state": "open",
    "created_at": "2025-04-13T04:35:37+00:00",
    "closed_at": null,
    "comments": 29,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5338/reactions",
      "total_count": 3,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5338"
  },
  {
    "number": 4547,
    "title": "[Bug] 1.5B is bizarrely OOM on 80G A100",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nSimply specify the following parameters, and an error occurs during inference with qwen2.5-1.5b-instruct:\n\n```python\nllm = sgl.Engine(model_path=m, dp_size=device_count())\n{\n    \"max_new_tokens\": 512,\n    \"temperature\": 0.0,\n    \"stop\": [\n        \"}\\n\",\n        \"<|endoftext|>\",\n        \"<|im_end|>\",\n        \"</s>\",\n        \"## Question:\",\n        \"<|eot_id|>\",\n        \"\\n\\nQuestion:\",\n    ],\n}\n```\n\n```bash\nTraceback (most recent call last):\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 111, in forward_thread_func\n    with torch.get_device_module(self.device).stream(self.forward_stream):\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 595, in __exit__\n    torch.cuda.set_stream(self.src_prev_stream)  # type: ignore[arg-type]\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/lib/python3.12/site-packages/vllm/utils.py\", line 928, in _patched_set_stream\n    prev_set_stream(stream)\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 636, in set_stream\n    _set_stream_by_id(\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 618, in _set_stream_by_id\n    torch._C._cuda_setStream(\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\n[2025-03-18 16:10:06 DP1 TP0] TpModelWorkerClient hit an exception: Traceback (most recent call last):\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 112, in forward_thread_func\n    self.forward_thread_func_()\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 143, in forward_thread_func_\n    logits_output, next_token_ids = self.worker.forward_batch_generation(\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py\", line 172, in forward_batch_generation\n    logits_output = self.model_runner.forward(forward_batch)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 946, in forward\n    return self.forward_decode(forward_batch)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 897, in forward_decode\n    return self.model.forward(\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/models/qwen2.py\", line 375, in forward\n    return self.logits_processor(\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/layers/logits_processor.py\", line 307, in forward\n    logits = self._get_logits(pruned_states, lm_head, logits_metadata)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/INS/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/layers/logits_processor.py\", line 440, in _get_logits\n    logits = logits[:, : self.config.vocab_size].float()\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.99 GiB. GPU 1 has a total capacity of 79.35 GiB of which 1.37 GiB is free. Process 28367 has 77.97 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, with 30.52 MiB allocated in private pools (e.g., CUDA Graphs), and 1.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n```\n\n### Reproduction\n\n.\n\n### Environment\n\nPython: 3.12.9 | packaged by conda-forge | (main, Feb 14 2025, 08:00:06) [GCC 13.3.0]\nCUDA available: True\nGPU 0,1,2,3: NVIDIA Graphics Device\nGPU 0,1,2,3 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\nCUDA Driver Version: 470.103.01\nPyTorch: 2.5.1+cu121\nsglang: 0.4.4.post1\nsgl_kernel: 0.0.5.post3\nflashinfer: 0.2.3+cu121torch2.5\ntriton: 3.1.0\ntransformers: 4.49.0\ntorchao: 0.7.0\nnumpy: 1.26.4\naiohttp: 3.11.9\nfastapi: 0.115.6\nhf_transfer: Module Not Found\nhuggingface_hub: 0.26.3\ninteregular: 0.3.3\nmodelscope: Module Not Found\norjson: 3.10.12\npackaging: 24.0\npsutil: 6.1.0\npydantic: 2.10.3\nmultipart: 0.0.20\nzmq: 26.2.0\nuvicorn: 0.32.1\nuvloop: 0.21.0\nvllm: 0.7.3\nxgrammar: Module Not Found\nopenai: 1.56.2\nanthropic: Module Not Found\nlitellm: Module Not Found\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    mlx5_0  mlx5_1  mlx5_2  mlx5_3  mlx5_4  mlx5_5  mlx5_6  mlx5_7  CPU Affinity    NUMA Affinity\nGPU0     X      NV8     NV8     NV8     PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     0-63,128-191    0\nGPU1    NV8      X      NV8     NV8     PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     0-63,128-191    0\nGPU2    NV8     NV8      X      NV8     NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     0-63,128-191    0\nGPU3    NV8     NV8     NV8      X      SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    64-127,192-255  1\nmlx5_0  PXB     PXB     NODE    SYS      X      PIX     NODE    NODE    SYS     SYS     SYS     SYS\nmlx5_1  PXB     PXB     NODE    SYS     PIX      X      NODE    NODE    SYS     SYS     SYS     SYS\nmlx5_2  NODE    NODE    PXB     SYS     NODE    NODE     X      PIX     SYS     SYS     SYS     SYS\nmlx5_3  NODE    NODE    PXB     SYS     NODE    NODE    PIX      X      SYS     SYS     SYS     SYS\nmlx5_4  SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS      X      PIX     NODE    NODE\nmlx5_5  SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     PIX      X      NODE    NODE\nmlx5_6  SYS     SYS     SYS     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      PIX\nmlx5_7  SYS     SYS     SYS     NODE    SYS     SYS     SYS     SYS     NODE    NODE    PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-18T08:12:24+00:00",
    "closed_at": "2025-05-26T00:20:02+00:00",
    "comments": 47,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4547/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4547"
  },
  {
    "number": 4734,
    "title": "[Roadmap] EP Enhancement",
    "body": "- [x] Support normal DeepEP buffer @liz-badada  #4232 \n- [x] Support DeepEP with async transfer @fzyzcjy #4610 \n- [x] Support low-latency DeepEP buffer\n  - [x] Single-node TP @liz-badada #4767 \n    - MaskedDeepGeMM is implemented by @laixinn @sleepcoo \n    - Improved by @yuleil #5277 \n  - [x] Multi-node TP @liz-badada #5068 \n  - [x] Support PD disaggregation @ch-wan  #5435 \n- [ ] Integrate pplx-kernels @ruizhang1230 #5010 \n- [ ] Optimize permutation overhead\n  - [x] Implement Titon kernels @xutizhou #4643 \n  - [ ] Fuse permutation with GroupedGeMM\n- [x] Extend parallelism paradigm\n  - [x] Extend DeepEP to a general TP paradigm @ch-wan @tarinkk #4770 \n    - Fixed by @fzyzcjy #4883 \n  - [x] Support `tp_size < ep_size`\n    - `tp_size=1` @fzyzcjy #4836\n- [x] Overlap two batches @fzyzcjy #4068 \n- [x] Integrate continuous DeepGeMM @sleepcoo @xutizhou  #5626 \n- [x] Record expert distribution @yuhsuan-t #4435 \n  - Improved by @fzyzcjy #4957  \n- [ ] Overlap communication with shared experts\u2019 computation @liz-badada  #5829 \n- [x] Integrate EPLB @fzyzcjy  #5295 \n\nOthers\n- The DeepSeek team is going to release a permutation kernel shortly. We may need to check their update https://github.com/deepseek-ai/DeepGEMM/issues/57#issuecomment-2720514270",
    "labels": [
      "high priority",
      "collaboration"
    ],
    "state": "open",
    "created_at": "2025-03-24T18:48:57+00:00",
    "closed_at": null,
    "comments": 30,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4734/reactions",
      "total_count": 45,
      "+1": 30,
      "-1": 0,
      "laugh": 0,
      "hooray": 9,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 6
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4734"
  },
  {
    "number": 2677,
    "title": "prometheus query return no result",
    "body": "Hi, thank you for your great work. I'm new to Prometheus and Grafana and I'd like to use sglang with them.  After I set up according to the [document](https://sgl-project.github.io/references/production_metrics.html#../examples/monitoring/grafana.json), the dashboards show no data. \r\n![\u5fae\u4fe1\u56fe\u7247_20241231141427](https://github.com/user-attachments/assets/d4cac92b-d6e8-40d7-b15d-d8b4473f009a)\r\nI tried to query in prometheus directly with expr in [example](https://github.com/sgl-project/sglang/blob/main/examples/monitoring/grafana.json), no result as well.\r\n![\u5fae\u4fe1\u56fe\u7247_20241231142210](https://github.com/user-attachments/assets/e67364db-ef84-42e3-bf12-af4915a1146a)\r\nplease help me out",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-12-31T06:23:14+00:00",
    "closed_at": "2025-04-15T00:18:42+00:00",
    "comments": 24,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2677/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2677"
  },
  {
    "number": 7227,
    "title": "[Roadmap] Blackwell Support and Optimizations",
    "body": "### Roadmap\n\n- [x] ~~Initial support and optimizations for GB200, PD disaggregation, and large-scale EP~~ -- Done in https://lmsys.org/blog/2025-06-16-gb200-part-1/\n- [x] Initial optimizations for prefill for large scale EP\n- [ ] Optimize kernels for the Blackwell architecture\n    - [ ] Communication kernels\n    - [ ] Various smaller kernels\n- [ ] Optimize for latency-oriented scenarios\n- [ ] Computation-communication overlap\n\nTODO: more\n\n### Updates after Blog\n\n* Prefill is slightly optimized, 13149 token/s/gpu for ISL 4096 (as usual all code are open sourced)\n\n### Blog Reproduction\n\n<details>\n\nTo reproduce [the blog post](https://lmsys.org/blog/2025-06-16-gb200-part-1/), here are the instructions:\n\n#### 2025.07.12\n\nTo use the latest main, the following commands can be used.\n\nVersions that I personally use to test (other versions may work as well)\n* SGLang: https://github.com/sgl-project/sglang/commit/2a2d3478afe8cdb336888f2e6faa3775ac40254e\n* sgl-kernel: the one inside SGLang\n* DeepGEMM: https://github.com/sgl-project/DeepGEMM/commit/98707282f30aad49bb2fc924332a7b40a7e7a6dd (this is currently the version that is tagged in the `blackwell` branch)\n* DeepEP: https://github.com/fzyzcjy/DeepEP/commit/1b14ad661c7640137fcfe93cccb2694ede1220b0 (but I think https://github.com/deepseek-ai/DeepEP/commit/dd133d39bce06469292311a4accf0ae79dcb45fa or latest main should work)\n* Mooncake: mooncake-transfer-engine==0.3.4.post2\n* torch: 2.8.0.dev20250613+cu128\n\n```\n# P nodes\nSGLANG_DEEPEP_NUM_MAX_DISPATCH_TOKENS_PER_RANK=2048 MC_TE_METRIC=true SGLANG_DISAGGREGATION_HEARTBEAT_MAX_FAILURE=100000 SGLANG_DISAGGREGATION_BOOTSTRAP_TIMEOUT=100000 SGLANG_DISAGGREGATION_WAITING_TIMEOUT=100000 SGLANG_MOONCAKE_CUSTOM_MEM_POOL=True SGLANG_LOCAL_IP_NIC=eth0 GLOO_SOCKET_IFNAME=eth0 NCCL_SOCKET_IFNAME=eth0 NCCL_MNNVL_ENABLE=1 NCCL_CUMEM_ENABLE=1 SGLANG_USE_MESSAGE_QUEUE_BROADCASTER=0 SGL_DISABLE_TP_MEMORY_INBALANCE_CHECK=1 PYTHONUNBUFFERED=1 python3 -m sglang.launch_server --model-path deepseek-v3-0324 --trust-remote-code --disaggregation-mode prefill --dist-init-addr 192.168.3.47:5757 --nnodes 2 --node-rank 0 --tp-size 8 --dp-size 8 --enable-dp-attention --host 0.0.0.0 --decode-log-interval 1 --max-running-requests 6144 --context-length 2176 --disable-radix-cache --moe-dense-tp-size 1 --enable-dp-lm-head --disable-shared-experts-fusion --ep-num-redundant-experts 32 --eplb-algorithm deepseek --attention-backend cutlass_mla --watchdog-timeout 1000000  --init-expert-location YOUR_FILE --disable-cuda-graph --chunked-prefill-size 16384 --max-total-tokens 32768 --enable-deepep-moe --deepep-mode low_latency --deepep-config YOUR_FILE --ep-dispatch-algorithm dynamic\n\n# D nodes\nSGLANG_DEEPEP_NUM_MAX_DISPATCH_TOKENS_PER_RANK=768 MC_TE_METRIC=true SGLANG_DISAGGREGATION_HEARTBEAT_MAX_FAILURE=100000 SGLANG_DISAGGREGATION_BOOTSTRAP_TIMEOUT=100000 SGLANG_DISAGGREGATION_WAITING_TIMEOUT=100000 SGLANG_HACK_SEQ_BOOTSTRAP_ROOM=1 SGLANG_MOONCAKE_CUSTOM_MEM_POOL=True SGLANG_LOCAL_IP_NIC=eth0 GLOO_SOCKET_IFNAME=eth0 NCCL_SOCKET_IFNAME=eth0 NCCL_MNNVL_ENABLE=1 NCCL_CUMEM_ENABLE=1 SGLANG_USE_MESSAGE_QUEUE_BROADCASTER=0 SGL_DISABLE_TP_MEMORY_INBALANCE_CHECK=1 PYTHONUNBUFFERED=1 python3 -m sglang.launch_server --model-path deepseek-v3-0324 --trust-remote-code --disaggregation-mode decode --dist-init-addr 192.168.3.44:5757 --nnodes 12 --node-rank 0 --tp-size 48 --dp-size 48 --enable-dp-attention --host 0.0.0.0 --decode-log-interval 1 --max-running-requests 36864 --context-length 2176 --disable-radix-cache --moe-dense-tp-size 1 --enable-dp-lm-head --disable-shared-experts-fusion --ep-num-redundant-experts 32 --eplb-algorithm deepseek --attention-backend cutlass_mla --watchdog-timeout 1000000  --init-expert-location YOUR_PATH --chunked-prefill-size 36864 --mem-fraction-static 0.82 --enable-deepep-moe --deepep-mode low_latency --ep-dispatch-algorithm static --cuda-graph-bs 768 --num-reserved-decode-tokens YOUR_VALUE\n\n# LB\npython3 -m sglang.srt.disaggregation.launch_lb --prefill \"http://your-ip:30000\" --decode \"http://your-ip:30000\" --host 0.0.0.0 --port 8000 --timeout 3600\n\n# slow down\ncurl -H \"Content-Type: application/json\" -d '{\"forward_sleep_time\": 180}' -X POST \"http://YOUR_FIRST_DECODE_NODE_IP:30000/slow_down\"\n\n# start benchmark; do not wait for this to finish before running the next line\npython3 -m sglang.bench_one_batch_server --model-path /path/to/DeepSeek-V3-0324 --base-url http://your-lb-ip:7000 --batch-size 73728 --input-len YOUR_INPUT --output-len YOUR_OUTPUT --skip-warmup\n\n# after some time (e.g. 10 minute), the D nodes are saturated, then this command should be executed\n# finish slowing down D nodes\ncurl -H \"Content-Type: application/json\" -d '{\"forward_sleep_time\": null}' -X POST \"http://YOUR_FIRST_DECODE_NODE_IP:30000/slow_down\"\n```\n\n#### 2025.06.16\n\n<details>\n\n```\n# P nodes\nSGLANG_DEEPEP_NUM_MAX_DISPATCH_TOKENS_PER_RANK=2048 SGLANG_MOONCAKE_ALLOCATOR_SO_PATH=/data/numa0/tom/temp/Mooncake/build/mooncake-transfer-engine/nvlink-hook/hook.so SGLANG_MOONCAKE_CUSTOM_POOL=True python3 -m sglang.launch_server --model-path /path/to/deepseek-v3-0324 --trust-remote-code --disaggregation-mode prefill --dist-init-addr your-ip:5757 --nnodes 2 --node-rank 0 --tp-size 8 --dp-size 8 --enable-dp-attention --host 0.0.0.0 --decode-log-interval 1 --max-running-requests 6144 --context-length 2176 --disable-radix-cache --enable-deepep-moe --deepep-mode low_latency --moe-dense-tp-size 1 --enable-dp-lm-head --disable-shared-experts-fusion --ep-num-redundant-experts 32 --ep-dispatch-algorithm static --eplb-algorithm deepseek --attention-backend cutlass_mla --watchdog-timeout 1000000  --init-expert-location YOUR_PATH --disable-cuda-graph --chunked-prefill-size 16384 --max-total-tokens 32768\n\n# D nodes\nSGLANG_DEEPEP_NUM_MAX_DISPATCH_TOKENS_PER_RANK=768 SGLANG_NUM_RESERVED_DECODE_TOKENS=176 SGLANG_MOONCAKE_ALLOCATOR_SO_PATH=/data/numa0/tom/temp/Mooncake/build/mooncake-transfer-engine/nvlink-hook/hook.so SGLANG_MOONCAKE_CUSTOM_POOL=True python3 -m sglang.launch_server --model-path /path/to/deepseek-v3-0324 --trust-remote-code --disaggregation-mode decode --dist-init-addr your-ip:5757 --nnodes 12 --node-rank 0 --tp-size 48 --dp-size 48 --enable-dp-attention --host 0.0.0.0 --decode-log-interval 1 --max-running-requests 36864 --context-length 2176 --disable-radix-cache --enable-deepep-moe --deepep-mode low_latency --moe-dense-tp-size 1 --enable-dp-lm-head --cuda-graph-bs 768 --disable-shared-experts-fusion --ep-num-redundant-experts 32 --ep-dispatch-algorithm static --eplb-algorithm deepseek --attention-backend cutlass_mla --watchdog-timeout 1000000  --init-expert-location your_path --chunked-prefill-size 36864 --mem-fraction-static 0.82\n\n# LB\npython3 -m sglang.srt.disaggregation.launch_lb --prefill \"http://your-ip:30000\" --decode \"http://your-ip:30000\" --host 0.0.0.0 --port 8000 --timeout 3600\n\n# slow down\ncurl -H \"Content-Type: application/json\" -d '{\"forward_sleep_time\": 180}' -X POST \"http://YOUR_FIRST_DECODE_NODE_IP:30000/slow_down\"\n\n# start benchmark; do not wait for this to finish before running the next line\npython3 -m sglang.bench_one_batch_server --model-path /path/to/DeepSeek-V3-0324 --base-url http://your-lb-ip:7000 --batch-size 73728 --input-len 2000 --output-len 100 --skip-warmup\n\n# after some time (e.g. 10 minute), the D nodes are saturated, then this command should be executed\n# finish slowing down D nodes\ncurl -H \"Content-Type: application/json\" -d '{\"forward_sleep_time\": null}' -X POST \"http://YOUR_FIRST_DECODE_NODE_IP:30000/slow_down\"\n```\n\nRemarks\n\n* Mooncake \"allocator so path\" will soon no longer be needed when it is on master\n* The slow-down is similar to #6017\n\n</details>\n\n</details>",
    "labels": [
      "high priority",
      "collaboration",
      "blackwell"
    ],
    "state": "open",
    "created_at": "2025-06-16T06:07:50+00:00",
    "closed_at": null,
    "comments": 45,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7227/reactions",
      "total_count": 31,
      "+1": 11,
      "-1": 0,
      "laugh": 0,
      "hooray": 10,
      "confused": 0,
      "heart": 10,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7227"
  },
  {
    "number": 7070,
    "title": "[Bug] sglang[all]>=0.4.7",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nAlmost half the performance drop when running 0.4.7 vs 0.4.6\n\nTested two models\nQwen3-32B-FP8 75T/s (4xAda 6000s) to 45T/s\nQwen3-30B-A3B - 160T/s  (4x3090s) BF16 to 80T/s with 0.4.7\n\n\n### Reproduction\n\npython -m sglang.launch_server --model-path models/Qwen3-32B-FP8 \\\n--context-length 131072 \\\n--json-model-override-args '{\"rope_scaling\":{\"type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}}' \\\n--tool-call-parser qwen25 \\\n--reasoning-parser qwen3 \\\n--tp-size 4\n\n### Environment\n\npython -m sglang.launch_server --model-path models/Qwen3-32B-FP8 \\\n--context-length 131072 \\\n--json-model-override-args '{\"rope_scaling\":{\"type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}}' \\\n--tool-call-parser qwen25 \\\n--reasoning-parser qwen3 \\\n--tp-size 4",
    "labels": [
      "high priority"
    ],
    "state": "open",
    "created_at": "2025-06-10T23:10:43+00:00",
    "closed_at": null,
    "comments": 25,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7070/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7070"
  },
  {
    "number": 1616,
    "title": "[Feature] GGUF support",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nHi! Since .gguf format is already supported by vLLM, is it be possible to add support for it in SGLang server?\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-10-09T05:45:17+00:00",
    "closed_at": "2024-12-01T10:51:57+00:00",
    "comments": 26,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1616/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1616"
  },
  {
    "number": 5577,
    "title": "[Feature] support bert rerank model and  openai \"score\" api",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n1. also need to support Bert rerank model with BertForSequenceClassification architectures in this feature.\n\n2. score api design:\n\n  -   input paramter\n  \n    \n    \"text_1\": \"What is the capital of France?\"\n    \"text_2\": [\n    \"The capital of Brazil is Brasilia.\",\n    \"The capital of France is Paris.\"\n    ]\n    \n  \n  -  api result\n  \n \n    \"data\": [\n        {\n          \"index\": 0,\n          \"score\": xxxxxxxx\n        },\n        {\n          \"index\": 1,\n          \"score\": xxxxxxxx\n        }\n      ]\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-20T15:19:47+00:00",
    "closed_at": "2025-06-20T17:16:13+00:00",
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5577/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5577"
  },
  {
    "number": 2803,
    "title": "[Bug] NCCL Crash with SIGSEGV Frequently when deploying deepseek v3",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n ```\r\n Caught signal 11 (Segmentation fault: address not mapped to object at address 0x3)                                                                                              ==== backtrace (tid: 212877) ====                                                                                                                                                                           \r\n 0 0x0000000000042520 __sigaction()  ???:0                                                                                                                                                                   1 0x0000000000049b8a ncclMemoryPoolAlloc<ncclProxyOp>()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/include/utils.h:280                                                                              \r\n 2 0x0000000000049b8a addProxyOpIfNeeded()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/enqueue.cc:180                                                                                                  3 0x0000000000049b8a addProxyOpIfNeeded()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/enqueue.cc:176                                                          \r\n 4 0x000000000004c496 addCBDCollToPlan()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/enqueue.cc:481                                                                                                   \r\n 5 0x000000000004f5bd ncclLaunchPrepare()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/enqueue.cc:844                                                                                                  \r\n 6 0x000000000004f5bd ncclLaunchPrepare()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/enqueue.cc:1260                                                                                                 \r\n 7 0x0000000000053d4b groupLaunch()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/group.cc:129\r\n 8 0x0000000000053d4b groupLaunch()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/group.cc:339\r\n 9 0x0000000000054f88 ncclGroupEndInternal()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/group.cc:418                                                          \r\n10 0x0000000000054f88 ncclGroupEndInternal()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/group.cc:368                                                          \r\n11 0x000000000004d74f ncclEnqueueCheck()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/enqueue.cc:2032                                                                                                  \r\n12 0x0000000000044b36 ncclAllGather()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/collectives.cc:26                                                                                                   \r\n13 0x00000000011fd1f3 c10d::ProcessGroupNCCL::_allgather_base()  ???:0                            \r\n14 0x0000000005f8e9b8 c10d::ops::(anonymous namespace)::_allgather_base_CUDA()  Ops.cpp:0         \r\n15 0x0000000005f985cc c10::impl::make_boxed_from_unboxed_functor<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<std::tuple<at::Tensor, c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_defa\r\nult_null_type<c10d::Work> > > (*)(at::Tensor&, at::Tensor&, c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, bool, long), std::tuple<at:\r\n:Tensor, c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> > >, c10::guts::typelist::typelist<at::Tensor&, at::Tensor&, c10::intrusive_ptr<c10d::ProcessGroup, c10:\r\n:detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, bool, long> >, false>::call()  :0                                                                                                 \r\n16 0x00000000055b224b c10::OperatorHandle::redispatchBoxed()  :0                                                                                                                                            \r\n17 0x00000000055afad9 torch::autograd::basicAutogradNotImplementedFallbackImpl()  autograd_not_implemented_fallback.cpp:0                                            18 0x0000000001a8c3f8 c10::BoxedKernel::make_boxed_function<&(anonymous namespace)::autograd_fallback>()  VariableFallbackKernel.cpp:0                                                                      \r\n19 0x0000000005f9fc2e c10::impl::BoxedKernelWrapper<std::tuple<at::Tensor, c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> > > (at::Tensor&, at::Tensor&, c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, bool, long), void>::call()  :0                                                                \r\n20 0x0000000005fabfe8 c10d::ProcessGroup::_allgather_base()  :0                                                                                                                                             21 0x0000000000df6c7e pybind11::cpp_function::initialize<pybind11::cpp_function::initialize<c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> >, c10d::ProcessGroup\r\n, at::Tensor&, at::Tensor&, c10d::AllgatherOptions const&, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::arg, pybind11::arg, pybind11::arg_v, pybind11::call_guard<pybind11::gil_scoped_release> >(c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> > (c10d::ProcessGroup::*)(at::Tensor&, at::Tensor&, c10d::AllgatherOptions const&), pybind11::name con\r\nst&, pybind11::is_method const&, pybind11::sibling const&, pybind11::arg const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::call_guard<pybind11::gil_scoped_release> const&)::{lambda(c10d::ProcessGroup*, at::Tensor&, at::Tensor&, c10d::AllgatherOptions const&)#1}, c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> >, c10d::ProcessGroup*, at::Tensor&, at:\r\n:Tensor&, c10d::AllgatherOptions const&, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::arg, pybind11::arg, pybind11::arg_v, pybind11::call_guard<pybind11::gil_scoped_release> >(pybind11::cpp_function::initialize<c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> >, c10d::ProcessGroup, at::Tensor&, at::Tensor&, c10d::AllgatherOptions const&, pybin\r\nd11::name, pybind11::is_method, pybind11::sibling, pybind11::arg, pybind11::arg, pybind11::arg_v, pybind11::call_guard<pybind11::gil_scoped_release> >(c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> > (c10d::ProcessGroup::*)(at::Tensor&, at::Tensor&, c10d::AllgatherOptions const&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybin\r\nd11::arg const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::call_guard<pybind11::gil_scoped_release> const&)::{lambda(c10d::ProcessGroup*, at::Tensor&, at::Tensor&, c10d::AllgatherOptions const&)#1}&&, c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> > (*)(c10d::ProcessGroup*, at::Tensor&, at::Tensor&, c10d::AllgatherOptions const&), pybind11::name co\r\nnst&, pybind11::is_method const&, pybind11::sibling const&, pybind11::arg const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::call_guard<pybind11::gil_scoped_release> const&)::{lambda(pybind11\r\n::detail::function_call&)#3}::_FUN()  :0      \r\n\r\n22 0x00000000004cb474 pybind11::cpp_function::dispatcher()  :0                                                                                                                                              \r\n23 0x000000000015a10e PyObject_CallFunctionObjArgs()  ???:0                                           \r\n24 0x0000000000150a7b _PyObject_MakeTpCall()  ???:0                                                                                                                                                         \r\n25 0x0000000000168acb PyMethod_New()  ???:0                                                                                                                                                                 \r\n26 0x0000000000148cfa _PyEval_EvalFrameDefault()  ???:0                                                                                                                                                     \r\n27 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0                                             \r\n28 0x0000000000169492 PyObject_Call()  ???:0                                                          \r\n29 0x00000000001455d7 _PyEval_EvalFrameDefault()  ???:0                                                                                                                                                     \r\n30 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0                                                                                                                                                       \r\n31 0x000000000014453c _PyEval_EvalFrameDefault()  ???:0                                               \r\n32 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0                                                                                                                                                       \r\n33 0x000000000014345c _PyEval_EvalFrameDefault()  ???:0                                               \r\n34 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0                                                                                                                                                       \r\n35 0x000000000014326d _PyEval_EvalFrameDefault()  ???:0                                                                                                                                                     \r\n36 0x000000000016893e PyMethod_New()  ???:0                                                           \r\n37 0x00000000001455d7 _PyEval_EvalFrameDefault()  ???:0                                                                                                                                                     38 0x000000000016893e PyMethod_New()  ???:0                                                                                                                                                                 \r\n39 0x00000000001455d7 _PyEval_EvalFrameDefault()  ???:0\r\n40 0x000000000014fc14 _PyObject_FastCallDictTstate()  ???:0\r\n41 0x000000000016586c _PyObject_Call_Prepend()  ???:0\r\n42 0x0000000000280700 PyInit__datetime()  ???:0\r\n43 0x0000000000150a7b _PyObject_MakeTpCall()  ???:0 \r\n44 0x0000000000149629 _PyEval_EvalFrameDefault()  ???:0\r\n45 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0\r\n46 0x00000000001455d7 _PyEval_EvalFrameDefault()  ???:0\r\n47 0x00000000001687f1 PyMethod_New()  ???:0\r\n48 0x0000000000148cfa _PyEval_EvalFrameDefault()  ???:0\r\n49 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0\r\n50 0x000000000014345c _PyEval_EvalFrameDefault()  ???:0\r\n51 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0\r\n52 0x000000000014345c _PyEval_EvalFrameDefault()  ???:0\r\n53 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0\r\n54 0x000000000014345c _PyEval_EvalFrameDefault()  ???:0\r\n55 0x000000000015a9fc _PyFunction_Vectorcall()  ???:0\r\n56 0x00000000001455d7 _PyEval_EvalFrameDefault()  ???:0\r\n=================================\r\n[2025-01-08 11:17:51 TP7] Scheduler hit an exception: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 1578, in run_scheduler_process\r\n    scheduler.event_loop_overlap()\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 410, in event_loop_overlap\r\n    recv_reqs = self.recv_requests()\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 459, in recv_requests\r\n    recv_reqs = broadcast_pyobj(recv_reqs, self.tp_rank, self.tp_cpu_group)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/utils.py\", line 731, in broadcast_pyobj\r\n    dist.broadcast(tensor_size, src=0, group=dist_group)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py\", line 83, in wrapper \r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\", line 2425, in broadcast\r\n    work.wait()\r\nRuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [29.127.64.100]:26496\r\n\r\n[2025-01-08 11:17:51 TP1] Scheduler hit an exception: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 1578, in run_scheduler_process\r\n    scheduler.event_loop_overlap()\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 410, in event_loop_overlap\r\n    recv_reqs = self.recv_requests()\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 459, in recv_requests\r\n    recv_reqs = broadcast_pyobj(recv_reqs, self.tp_rank, self.tp_cpu_group)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/utils.py\", line 731, in broadcast_pyobj\r\n    dist.broadcast(tensor_size, src=0, group=dist_group)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py\", line 83, in wrapper \r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\", line 2425, in broadcast\r\n    work.wait()\r\nRuntimeError: [../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [29.127.64.100]:2711\r\n\r\nKilled\r\n\r\n```\n\n### Reproduction\n\nnode 1\r\n`python -m sglang.launch_server --model-path DeepSeek-V3 --tp 16 --nccl-init 29.127.64.100:5000 --nnodes 2 --node-rank 0 --trust-remote-code --port 80 --host 0.0.0.0 --schedule-conservativeness 0.3 --context-length 32768`\r\n\r\nnode2 \r\n`python -m sglang.launch_server --model-path DeepSeek-V3 --tp 16 --nccl-init 29.127.64.100:5000 --nnodes 2 --node-rank 1 --trust-remote-code --port 80 --host 0.0.0.0 --schedule-conservativeness 0.3 --context-length 32768`\r\n\n\n### Environment\n\n```\r\n/usr/local/lib/python3.10/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4                                                        \r\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"                                                                                                                                                              \r\nWarning: Your installation of OpenCV appears to be broken: module 'cv2.dnn' has no attribute 'DictValue'.Please follow the instructions at https://github.com/opencv/opencv-python/issues/884 to correct your environment. The import o\r\nf cv2 has been skipped.                                                                                                                                                                                                                \r\n/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:                                                                                                          \r\n* 'fields' has been removed                                                                                                                                                                                                              warnings.warn(message, UserWarning)                                                                                                                                                                                                  Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]                                                                                                                                                                             CUDA available: True                                                                                                                                                                                                                   \r\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20                                                                                    \r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0                                                                                                                                                                                            \r\nCUDA_HOME: /usr/local/cuda                                                                                         \r\nNVCC: Cuda compilation tools, release 12.4, V12.4.131                                                                                                                                                                                  \r\nCUDA Driver Version: 535.161.08                                                                                    \r\nPyTorch: 2.5.1+cu124                                                                                                                                                                                                                   \r\nsglang: 0.4.1.post3                                                                                                \r\nflashinfer: 0.1.6+cu124torch2.4                                                                                                                                                                                                        \r\ntriton: 3.1.0                                                                                                      \r\ntransformers: 4.47.1                                                                                                                                                                                                                   \r\ntorchao: 0.7.0                                                                                                     \r\nnumpy: 1.26.4                                                                                                                                                                                                                          \r\naiohttp: 3.9.5                                                                                                     \r\nfastapi: 0.114.1                                                                                                                                                                                                                       \r\nhf_transfer: 0.1.8                                                                                                 \r\nhuggingface_hub: 0.24.7                                                                                                                                                                                                                \r\ninteregular: 0.3.3                                                                                                 \r\nmodelscope: 1.21.1                                                                                                                                                                                                                     \r\norjson: 3.10.13                                                                                                    \r\npackaging: 24.0                                                                                                                                                                                                                        \r\npsutil: 5.9.8                                                                                                      \r\npydantic: 2.9.1                                                                                                                                                                                                                        multipart: 0.0.20                                                                                                  zmq: 26.0.3                                                                                                                                                                                                                            \r\nuvicorn: 0.30.6                                                                                                    \r\nuvloop: 0.20.0                                                                                                                                                                                                                         \r\nvllm: 0.6.4.post1                                                                                                  openai: 1.58.1                                                                                                                                                                                                                         anthropic: 0.42.0                                                                                                  decord: 0.6.0    \r\n\r\nLegend:                                                                                                                                                                                                                                \r\n                                                                                                                                                                                                                                       \r\n  X    = Self                                                                                                                                                                                                                          \r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)                                                                                                                                 \r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node                                                                                                                           \r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)                                                                                                                                                  \r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)                                                                                                                                         \r\n  PIX  = Connection traversing at most a single PCIe bridge                                                                                                                                                                            \r\n  NV#  = Connection traversing a bonded set of # NVLinks                                                                                                                                                                               \r\n                                                                                                                                                                                                                                       \r\nNIC Legend:       \r\n\r\nNVIDIA Topology:                                                                                                                                                                                                                                                                                                       \r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10   NIC11   NIC12   NIC13   NIC14   NIC15   NIC16       NIC17   NIC18   NIC19   NIC20   NIC21   NIC22   NIC23   NIC24   NIC25   CPU Affinity    NUMA Affini\r\nty   GPU NUMA ID                                                                                                                                                                                                                                                                                                       \r\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\r\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      NODE    NODE    PHB     PIX     SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\r\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      NODE    NODE    PIX     PHB     SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\r\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\r\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODENODE    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    96-191,288-383  1               N/A\r\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODENODE    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    96-191,288-383  1               N/A\r\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODENODE    SYS     SYS     SYS     SYS     PHB     NODE    NODE    PIX     96-191,288-383  1               N/A\r\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODENODE    SYS     SYS     SYS     SYS     PIX     NODE    NODE    PHB     96-191,288-383  1               N/A\r\nNIC0    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE     X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIXPIX      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\r\nNIC1    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIXPIX      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\r\nNIC2    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIXPIX      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\r\nNIC3    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIXPIX      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\r\nNIC4    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIXPIX      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\r\nNIC5    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIXPIX      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\r\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIXPIX      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\r\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIXPIX      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\r\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIXPIX      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\r\nNIC9    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIXPIX      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\r\nNIC10   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIXPIX      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\r\nNIC11   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIXPIX      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\r\nNIC12   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIXPIX      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\r\nNIC13   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIXPIX      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\r\nNIC14   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIXPIX      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\r\nNIC15   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIXPIX      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\r\nNIC16   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X PIX      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\r\nNIC17   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX X       SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\r\nNIC18   PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS       X      NODE    NODE    NODE    SYS     SYS     SYS     SYS\r\nNIC19   NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      NODE     X      NODE    NODE    SYS     SYS     SYS     SYS\r\nNIC20   NODE    PHB     PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      NODE    NODE     X      PHB     SYS     SYS     SYS     SYS\r\nNIC21   NODE    PIX     PHB     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      NODE    NODE    PHB      X      SYS     SYS     SYS     SYS\r\nNIC22   SYS     SYS     SYS     SYS     NODE    NODE    PHB     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODENODE    SYS     SYS     SYS     SYS      X      NODE    NODE    PHB\r\nNIC23   SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODENODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE\r\nNIC24   SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODENODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE\r\nNIC25   SYS     SYS     SYS     SYS     NODE    NODE    PIX     PHB     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODENODE    SYS     SYS     SYS     SYS     PHB     NODE    NODE     X \r\n                                                                                                                                                                                                                                       \r\nLegend:                                                                                                                                                                                                                                                                   \r\n                                                                                                                                                                                                                                       \r\n  X    = Self                                                                                                                                                                                                                                                             \r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)                                                                                                                                 \r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node                                                                                                                                                              \r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)                                                                                                                                                  \r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)                                                                                                                                                                            \r\n  PIX  = Connection traversing at most a single PCIe bridge                                                                                                                                                                            \r\n  NV#  = Connection traversing a bonded set of # NVLinks                                                                                                                                                                                                                  \r\n                                                                                                                                                                                                                                       \r\nNIC Legend:                                                                                                                                                                                                                                                               \r\n                                                                                                                                                                                                                                       \r\n  NIC0: mlx5_0                                                                                                                                                                                                                                                            \r\n  NIC1: mlx5_1                                                                                                                                                                                                                         \r\n  NIC2: mlx5_2                                                                                                                                                                                                                                                            \r\n  NIC3: mlx5_3                                                                                                                                                                                                                         \r\n  NIC4: mlx5_4                                                                                                                                                                                                                                                            \r\n  NIC5: mlx5_5                                                                                                                                                                                                                         \r\n  NIC6: mlx5_6                                                                                                                                                                                                                                                            \r\n  NIC7: mlx5_7                                                                                                                                                                                                                         \r\n  NIC8: mlx5_8                                                                                                                                                                                                                                                            \r\n  NIC9: mlx5_9                                                                                                                                                                                                                         \r\n  NIC10: mlx5_10                                                                                                                                                                                                                                                                                                      \r\n                                                                                                                                                                                                                                         NIC0: mlx5_0                                                                                                                                                                                                                           NIC1: mlx5_1                                                                                                                                                                                                                           NIC2: mlx5_2                                                                                                                                                                                                                         \r\n  NIC3: mlx5_3                                                                                                     \r\n  NIC4: mlx5_4                                                                                                                                                                                                                         \r\n  NIC5: mlx5_5                                                                                                     \r\n  NIC6: mlx5_6                                                                                                                                                                                                                         \r\n  NIC7: mlx5_7                                                                                                     \r\n  NIC8: mlx5_8                                                                                                                                                                                                                         \r\n  NIC9: mlx5_9                                                                                                     \r\n  NIC10: mlx5_10                                                                                                                                                                                                                       \r\n  NIC11: mlx5_11                                                                                                   \r\n  NIC12: mlx5_12                                                                                                                                                                                                                       \r\n  NIC13: mlx5_13                                                                                                   \r\n  NIC14: mlx5_14                                                                                                                                                                                                                       \r\n  NIC15: mlx5_16                                                                                                   \r\n  NIC16: mlx5_17                                                                                                                                                                                                                       \r\n  NIC17: mlx5_18                                                                                                   \r\n  NIC18: mlx5_bond_1                                                                                                                                                                                                                   \r\n  NIC19: mlx5_bond_2                                                                                               \r\n  NIC20: mlx5_bond_3                                                                                                                                                                                                                   \r\n  NIC21: mlx5_bond_4                                                                                               \r\n  NIC22: mlx5_bond_5                                                                                                                                                                                                                   \r\n  NIC23: mlx5_bond_6                                                                                               \r\n  NIC24: mlx5_bond_7                                                                                                                                                                                                                     NIC25: mlx5_bond_8                                                                                                                                                                                                                                                                                                                                      \r\n                                                                                                                   \r\nulimit soft: 1024\r\n```",
    "labels": [
      "help wanted",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-01-09T02:30:13+00:00",
    "closed_at": "2025-03-05T03:42:54+00:00",
    "comments": 29,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2803/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2803"
  },
  {
    "number": 1487,
    "title": "Development Roadmap (2024 Q4)",
    "body": "Here is the development roadmap for 2024 Q4. Contributions and feedback are welcome ([**Join Bi-weekly Development Meeting**](https://t.co/4BFjCLnVHq)). Previous 2024 Q3 roadmap can be found in #634.\n\n## Performance\n- [x] Hide CPU overhead with overlapped scheduler (#1738, #2067)\n- [x] Support speculative decoding\n  - Eagle  #2150 \n  - Reference-based. #270\n  - Medusa head #859\n  - Draft model based.\n- [x] Sparse Attention #1459\n- [x] Faster grammar parsing library for constrained decoding #1752 \n- [x] Multi-layer radix cache (GPU/CPU/Disk) https://github.com/sgl-project/sglang/pull/2693  @xiezhq-hermann \n- [ ] Improve the performance of mixed chunked prefill. see a draft #1383 \n- [ ] Integrate CuDNN paged attention [kernels](https://github.com/NVIDIA/cudnn-frontend/blob/v1.8.0/samples/python/52_scaled_dot_product_attention_with_paged_caches.ipynb) \n\n## Parallelism\n- [ ] Support sequence parallelism #1436. Related [paper](https://www.arxiv.org/pdf/2411.01783)\n- [ ] Support pipeline parallelism.\n- [ ] Support expert parallelism + data parallelism for DeepSeek/MoE models. @ispobock \n    - [x] Data parallelism #1970 \n    - [x] Expert parallelism # #1435 \n- [x] Implement a better cache-aware load balancer for data parallelism. #2114 #1732 @ByronHsu @yichuan520030910320 \n- [ ] Overlap communication in tensor parallelsim. @zhuohaol\n- [ ] Support disaggregated serving to separate prefill and decoding.\n\n## Hardware Coverage\n- [x] AMD optimizations. cc @HaiShaw \n  - CK kernels\n  - Setup CI (accuracy/performance) for AMD\n- [x] Intel XPU support.\n  - #1480\n  - #2121\n\n## Model Coverage\n- [x] Multi-modal models\n  - Llama 3.2 Vision https://github.com/sgl-project/sglang/pull/1551\n  - QWen2-VL https://github.com/sgl-project/sglang/pull/1546\n  - DeepSeek VL2 https://github.com/sgl-project/sglang/issues/2653\n  - mistralai/Pixtral https://github.com/sgl-project/sglang/issues/2351\n  - GLM 4V https://github.com/sgl-project/sglang/pull/1641\n  - VILA https://arxiv.org/abs/2412.04468\n  - InternVL\n  - Phi-vision\n  - [FishSpeech](https://github.com/fishaudio/fish-speech) audio model support \n  - [Ultravox](https://github.com/sgl-project/sglang/issues/1271)\n- [ ] Language models\n  -  Mamba models @rahulbatra85 @HaiShaw \n  - xLSTM\n- [x] Reward models\n  - #1525 \n  - #1954 \n\n## New features\n- [ ] Integrate with LMCache https://github.com/LMCache/LMCache\n- [ ] A padded batch mode to make results more deterministic https://github.com/sgl-project/sglang/blob/8912b7637f5c8dca0f18c31a17e46f427cf53152/docs/references/faq.md?plain=1#L3\n- [x] Performance optimizations for multi-LoRA serving #1728 \n\n## Quantization\n@HaiShaw @zhyncs @ispobock \n- [x] Torchao integration #1561\n- [x] Turbomind operators integration\n- [ ] More CUTLASS mixed precision gemm integration\n- [ ] KV cache quantization (more formats + scaling factor)\n\n## Server API\n- [x] Support directly taking embedding as inputs. #745\n- [x] Add APIs for using the inference engine in a single script without launching a separate server. See also [examples](https://docs.vllm.ai/en/latest/getting_started/examples/offline_inference.html).\n  - #1567\n- [ ] Support endpoint other than OpenAI (Anthropic, Mistral) in the language frontend.\n- [x] Better APIs to support RL trainers, including https://github.com/huggingface/trl and https://github.com/OpenRLHF/OpenRLHF @zhaochenyang20 \n- [x] Support generalized reward API (adding linear layers to any Causal LM to get the reward) https://github.com/OpenRLHF/OpenRLHF @zhaochenyang20 \n\n## Observability\n- [x] Integrate Grafana / Prometheus\n  - #1853  #1461 \n\n## Others\n- [x] Notebook-style interactive tutorials. @zhaochenyang20 \n- [ ] Compiler mode optimizations for the language (e.g. support sending a full serialized SGL program to the server). @hnyls2002 \n- [ ] Memory pool refactor to better support mixing different attention layers (e.g., interleaved window attention). @Ying1123 \n- [ ] Make vLLM an optional dependency. @zhyncs @ByronHsu @yizhang2077 https://github.com/sgl-project/sglang/issues/1673",
    "labels": [],
    "state": "closed",
    "created_at": "2024-09-21T22:38:00+00:00",
    "closed_at": "2025-03-03T18:43:18+00:00",
    "comments": 27,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1487/reactions",
      "total_count": 39,
      "+1": 19,
      "-1": 0,
      "laugh": 0,
      "hooray": 9,
      "confused": 0,
      "heart": 0,
      "rocket": 11,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/1487"
  },
  {
    "number": 3596,
    "title": "[Feature] Reorganize all the docs",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n1. [Quick Start: Sending Requests](https://docs.sglang.ai/backend/send_request.html) move to Getting Started\n\n2. \n\n<img width=\"807\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/6175edbf-484e-4fc2-9a40-f7e6cd31dcb9\" />\n\n3. print_highlight\n\nif is_in_ci, use html. else directly print it.\n\n4. differentiate two streaming https://docs.sglang.ai/backend/send_request.html\n\n<img width=\"799\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/41508c86-5837-40f0-96b8-bf424b32aaca\" />\n\n5. add description to this docs:\n\n<img width=\"735\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2a7a2875-7cf2-4489-86c4-40937366b3c6\" />\n\n6. https://docs.sglang.ai/backend/openai_api_completions.html add link to it\n\n<img width=\"797\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/3f211d30-3adf-4018-b5f5-01fe4e5fecf3\" />\n\n7. change meta-llama/Meta-Llama-3.1-8B-Instruct to `meta-llama/Llama-3.2-1B-Instruct`\n\n8. deepseek\n\n<img width=\"655\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fa3f5de3-46c0-40c3-a83c-bf0c95ae8d39\" />\n\nThis should link to deepseek official and saying like:\n\n`SGLang is recognized as one of the top engines for [DeepSeek model inference](deepseek official). Refer to[ installation and launch](https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#installation--launch) to fire up DeepSeek V3/R1 on SGLang.`\n\n9. \n\n<img width=\"806\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/37bd323f-cd59-4800-80b0-9dc24812645f\" />\n\n10. https://docs.sglang.ai/backend/openai_api_vision.html\n\nAdd link to this https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/offline_batch_inference_vlm.py to tell users we can use engine to do generate.\n\n11. https://docs.sglang.ai/backend/offline_engine_api.html move this to `examples` and give links at the beginning.\n\n<img width=\"813\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5821f4f9-d8b0-481b-87d8-c8fb486a05ef\" />\n\n12. https://docs.sglang.ai/backend/function_calling.html remove this or give link to what is function calling\n\n<img width=\"641\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/28b65fa6-7aa9-429b-b898-7515695a804f\" />\n\n13. https://docs.sglang.ai/frontend/frontend.html\n\nWe can review simon's PR.\n\n14. https://docs.sglang.ai/router/router.html\n\nremove the $\n\n<img width=\"792\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/db602592-efbd-4e03-a7eb-ef21e5daaa68\" />\n\n15. https://docs.sglang.ai/references/hyperparameter_tuning.html\n\n<img width=\"749\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b45c754d-f825-4882-b5b1-1c5ee912525b\" />\n\nUse router if you are not using the engine.\n\n<img width=\"798\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/811bdfe1-1bbc-4694-94b8-13103532ad66\" />\n\nTell them how to use the least VRAM.\n\n<img width=\"783\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/154b891c-1783-4fd0-9426-f42cc0044c3e\" />\n\nLink to server args.\n\n16. https://docs.sglang.ai/references/accuracy_evaluation.html\n\nLet's do it next time. Link to benchmark and add readme in benchmark.\n\n17. https://docs.sglang.ai/references/multi_node.html\n\n<img width=\"453\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/20e83e66-57a2-47a2-820c-eed3721c6d25\" />\n\n18. https://docs.sglang.ai/references/quantization.html\n\n<img width=\"770\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/bb7d7e06-0bd5-46b3-b496-bc092c2732ff\" />\n\n19. references: move some part to backend.\n\n20. take AMD back:\n\nhttps://docs.sglang.ai/references/amd_configure.html\n\n### Related resources\n\n_No response_",
    "labels": [
      "documentation",
      "good first issue",
      "help wanted"
    ],
    "state": "open",
    "created_at": "2025-02-15T20:15:50+00:00",
    "closed_at": null,
    "comments": 26,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3596/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3596"
  },
  {
    "number": 81,
    "title": "Tutorial for Batch Decoding and Obtaining Log Probs",
    "body": "Hi\r\nThanks for the great library\r\nI have a usecase which I think will benefit a lot from Radix Attention. I need to obtain log probs for around a 100K sequences which can be binned into groups of 100 having a similar prefix like 'Wikipedia originated in' and having 100 different suffixes. I do not need to generate anything and I only need the log probs for the input. Is there a tutorial for such a usecase?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-23T05:13:56+00:00",
    "closed_at": "2024-01-30T14:39:59+00:00",
    "comments": 25,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/81/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/81"
  },
  {
    "number": 3196,
    "title": "[Feature] deepseek v3 60 tokens/sec on deepseek API vs. 13 tokens/sec on sglang",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nThe PR for AMD + sglang and NVIDIA + sglang was that it was \"fully\" supported, but it seems something is off by the speed.  A single sequence runs at only order 13 tokens/sec for long generation with TTFT order 2 seconds.  This is consistent with vLLM as well.  True for either 8*MI300X or 8*H200 or 2*8*H200.\n\nFor only 37B parameters + 14B MOE parameters, this seems way too slow.  Also, deepseek API (before it started to break down) was order 60 tokens/sec early on and they advertise 60 tokens/sec.  This is more aligned with the parameters active.\n\nWhat is missing from truly fully suppporting deepseek V3 and R1?  Can these features be enumerated and added in a roadmap?\n\n### Related resources\n\n_No response_",
    "labels": [
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2025-01-28T18:40:18+00:00",
    "closed_at": "2025-02-15T01:21:30+00:00",
    "comments": 29,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3196/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3196"
  },
  {
    "number": 5123,
    "title": "[Bug] The AWQ model has different inference response times for Qwen/Qwen2.5-VL-7B-Instruct-AWQ between versions v0.4.3post2 and v0.4.4post3, with v0.4.3post2 having a shorter response time",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThe AWQ model has different inference response times for Qwen/Qwen2.5-VL-7B-Instruct-AWQ between versions v0.4.3post2 and v0.4.4post3  with v0.4.3post2 having a shorter response time\n\nSGLang v0.4.4post3 and SGLang v0.4.3post2 test\nQwen/Qwen2.5-VL-7B-Instruct-AWQ\n0.4.3post2 is fine but 0.4.4post3 is response time long\n\n### Reproduction\n\nimport requests\nimport time\nimport json\nurl = \"http://127.0.0.1:9080/v1/chat/completions\"\n\npayload = \"{\\n \"model\": \"Qwen/Qwen2.5-VL-7B-Instruct-AWQ\",\\n \"messages\": [\\n {\\n \"role\": \"user\",\\n \"content\": [\\n \\n {\\n \"type\": \"image_url\",\\n \"image_url\": {\\n \"url\": \"/content/d764c716f64242ec86eb1f3f429a0571.jpg\"\\n }\\n },\\n {\\n \"type\": \"text\",\\n \"text\": \"\u56fe\u7247\u4e2d\u6709\u5feb\u9012\u76d2\u5b50\u561b\uff1f\"\\n \\n }\\n ]\\n }\\n \\n ],\\n \"max_tokens\": 6,\\n \\n \"temperature\": 0,\\n \"stop\": [\"<|im_end|>\", \"<|endoftext|>\"]\\n \\n \\n }\"\n\nheaders = {\n'Content-Type': \"application/json\",\n'cache-control': \"no-cache\",\n'Postman-Token': \"69a9c37d-2ff3-4617-bd98-72ad310f28b0\"\n}\nstart=time.time()\nresponse = requests.request(\"POST\", url, data=payload, headers=headers)\nprint(time.time()-start)\nprint(response.text)\n\n1.2271076679229736\n\n{\"id\":\"379b4996efc1466ca06cf2f68e4d6e5e\",\"object\":\"chat.completion\",\"created\":1743333145,\"model\":\"Qwen/Qwen2.5-VL-7B-Instruct-AWQ\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"\u5bf9\u4e0d\u8d77\uff0c\u60a8\u63d0\u4f9b\u7684\u56fe\u7247\u5185\u5bb9\",\"reasoning_content\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"length\",\"matched_stop\":null}],\"usage\":{\"prompt_tokens\":326,\"total_tokens\":332,\"completion_tokens\":6,\"prompt_tokens_details\":null}}\n\n\nThe following are the times for the second request, the first request is warm-up\n\nv0.4.4post3 is 1.2271076679229736s\nv0.4.3post2  is 0.6s\n\nwhy ?awq is bug?\n\n\n\n### Environment\n\nt4\n\n python3 -m sglang.check_env\nPython: 3.11.11 (main, Dec 4 2024, 08:55:07) [GCC 11.4.0]\nCUDA available: True\nGPU 0: Tesla T4\nGPU 0 Compute Capability: 7.5\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.5, V12.5.82\nCUDA Driver Version: 550.54.15\nPyTorch: 2.5.1+cu124\nsglang: 0.4.4.post3\nsgl_kernel: 0.0.5.post4\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.50.0\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.14\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.3\ninteregular: 0.3.3\nmodelscope: 1.24.1\norjson: 3.10.15\noutlines: 0.1.11\npackaging: 24.2\npsutil: 5.9.5\npydantic: 2.10.6\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nxgrammar: 0.1.17\nopenai: 1.68.2\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.65.0\ndecord: 0.6.0\nNVIDIA Topology:\nGPU0 CPU Affinity NUMA Affinity GPU NUMA ID\nGPU0 X 0-7 0 N/A\n\nLegend:\n\nX = Self\nSYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\nNODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\nPHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\nPXB = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\nPIX = Connection traversing at most a single PCIe bridge\nNV# = Connection traversing a bonded set of # NVLinks\n\nHypervisor vendor: KVM\nulimit soft: 1048576\n\n\nthis is sglang 0.4.4.post3 env\n",
    "labels": [],
    "state": "open",
    "created_at": "2025-04-07T10:40:58+00:00",
    "closed_at": null,
    "comments": 33,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5123/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5123"
  },
  {
    "number": 4916,
    "title": "[Bug] Qwen/Qwen2.5-VL-7B-Instruct-AWQ sglang response time longer",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI found that when testing with Qwen/Qwen2.5-VL-7B-Instruct-AWQ, the response time of sglang is longer, while the response time of vllm is shorter. Why is this?\n\nor\n\n\nSGLang v0.4.4post3 and SGLang v0.4.3post2 test\nQwen/Qwen2.5-VL-7B-Instruct-AWQ\n\n0.4.3post2 is fine  but 0.4.4post3 is response time long\n\n\n### Reproduction\n\n\n\nimport requests\nimport time\nimport json\nurl = \"http://127.0.0.1:9080/v1/chat/completions\"\n\npayload = \"{\\n    \\\"model\\\": \\\"Qwen/Qwen2.5-VL-7B-Instruct-AWQ\\\",\\n    \\\"messages\\\": [\\n      {\\n        \\\"role\\\": \\\"user\\\",\\n        \\\"content\\\": [\\n          \\n          {\\n            \\\"type\\\": \\\"image_url\\\",\\n            \\\"image_url\\\": {\\n              \\\"url\\\": \\\"/content/d764c716f64242ec86eb1f3f429a0571.jpg\\\"\\n            }\\n          },\\n          {\\n            \\\"type\\\": \\\"text\\\",\\n            \\\"text\\\": \\\"\u56fe\u7247\u4e2d\u6709\u5feb\u9012\u76d2\u5b50\u561b\uff1f\\\"\\n            \\n          }\\n        ]\\n      }\\n      \\n    ],\\n    \\\"max_tokens\\\": 6,\\n    \\n    \\\"temperature\\\": 0,\\n     \\\"stop\\\": [\\\"<|im_end|>\\\", \\\"<|endoftext|>\\\"]\\n  \\n    \\n  }\"\n\nheaders = {\n    'Content-Type': \"application/json\",\n    'cache-control': \"no-cache\",\n    'Postman-Token': \"69a9c37d-2ff3-4617-bd98-72ad310f28b0\"\n    }\nstart=time.time()\nresponse = requests.request(\"POST\", url, data=payload, headers=headers)\nprint(time.time()-start)\nprint(response.text)\n\n1.3271076679229736\n{\"id\":\"379b4996efc1466ca06cf2f68e4d6e5e\",\"object\":\"chat.completion\",\"created\":1743333145,\"model\":\"Qwen/Qwen2.5-VL-7B-Instruct-AWQ\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"\u5bf9\u4e0d\u8d77\uff0c\u60a8\u63d0\u4f9b\u7684\u56fe\u7247\u5185\u5bb9\",\"reasoning_content\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"length\",\"matched_stop\":null}],\"usage\":{\"prompt_tokens\":326,\"total_tokens\":332,\"completion_tokens\":6,\"prompt_tokens_details\":null}}\n\n\nsglang is 1.3271076679229736s\nvllm is 0.9s\n\nwhy ?awq is bug?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### Environment\n\nt4",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-30T11:24:52+00:00",
    "closed_at": "2025-06-04T00:19:44+00:00",
    "comments": 27,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4916/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4916"
  },
  {
    "number": 4159,
    "title": "[Bug] Key conflict of `AutoImageProcessor.register`",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThe following ValueError was raised when attempting to serve any model within a recent Docker container:\n\n`Traceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/sgl-workspace/sglang/python/sglang/launch_server.py\", line 6, in <module>\n    from sglang.srt.entrypoints.http_server import launch_server\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/http_server.py\", line 44, in <module>\n    from sglang.srt.entrypoints.engine import _launch_subprocesses\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/engine.py\", line 36, in <module>\n    from sglang.srt.managers.data_parallel_controller import (\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/data_parallel_controller.py\", line 27, in <module>\n    from sglang.srt.managers.io_struct import (\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/io_struct.py\", line 25, in <module>\n    from sglang.srt.managers.schedule_batch import BaseFinishReason\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/schedule_batch.py\", line 43, in <module>\n    from sglang.srt.configs.model_config import ModelConfig\n  File \"/sgl-workspace/sglang/python/sglang/srt/configs/__init__.py\", line 4, in <module>\n    from sglang.srt.configs.qwen2_5_vl_config import (\n  File \"/sgl-workspace/sglang/python/sglang/srt/configs/qwen2_5_vl_config.py\", line 1005, in <module>\n    AutoImageProcessor.register(Qwen2_5_VLConfig, None, Qwen2_5_VLImageProcessor, None, exist_ok=False)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/image_processing_auto.py\", line 629, in register\n    IMAGE_PROCESSOR_MAPPING.register(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 833, in register\n    raise ValueError(f\"'{key}' is already used by a Transformers model.\")\nValueError: '<class 'sglang.srt.configs.qwen2_5_vl_config.Qwen2_5_VLConfig'>' is already used by a Transformers model.`\n\nThe problem was caused by 'Qwen2_5_VLConfig' already existing in transformers>=0.49.0, and was resolved by enabling `exist_ok` when registering it.\nUpdate the file `/sgl-workspace/sglang/python/sglang/srt/configs/qwen2_5_vl_config.py` at line 1005~1006:\n\n`\nAutoImageProcessor.register(Qwen2_5_VLConfig, None, Qwen2_5_VLImageProcessor, None, exist_ok=True)\nAutoProcessor.register(Qwen2_5_VLConfig, Qwen2_5_VLProcessor, exist_ok=True)\n`\n\n### Reproduction\n\npython3 -m sglang.launch_server --model [any-model]\n\n### Environment\n\nDocker image tag:  v0.4.3.post3-cu125\nsglang version: 0.4.3.post3\nsgl-kernel: 0.0.3.post6\nPython version: 3.10.12\n",
    "labels": [
      "MLLM"
    ],
    "state": "closed",
    "created_at": "2025-03-07T04:06:18+00:00",
    "closed_at": "2025-03-25T12:17:44+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4159/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4159"
  },
  {
    "number": 3304,
    "title": "[Bug] RuntimeError: RMSNorm failed with error code invalid configuration argument",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHi, I am using the main branch of SGLang, and downloading Mixtral-8x22B from huggingface. \n\nCUDA: 12.4\n2 nodes, each has 4 H100 96GB.\n\nI am deploying the server using:\n```\npython -m sglang.launch_server --model-path Mixtral-8x22B-v0.1 --tp 8 --dist-init-addr xxx:5000 --nnodes 2 --node-rank 0 --trust-remote-code --disable-cuda-graph\npython -m sglang.launch_server --model-path Mixtral-8x22B-v0.1 --tp 8 --dist-init-addr xxx:5000 --nnodes 2 --node-rank 1 --trust-remote-code --disable-cuda-graph\n\n```\nAnd I am running the MMLU benchmark:\n```\ncd sglang/benchmark/mmlu\npython3 bench_sglang.py --nsub 10\n```\n\nIt pops out the error:\n```\n[2025-02-04 21:18:29 DP3 TP3] TpModelWorkerClient hit an exception: Traceback (most recent call last):\n  File \"sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 109, in forward_thread_func\n    self.forward_thread_func_()\n  File \"python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 140, in forward_thread_func_\n    logits_output, next_token_ids = self.worker.forward_batch_generation(\n  File \"sglang/python/sglang/srt/managers/tp_worker.py\", line 164, in forward_batch_generation\n    logits_output = self.model_runner.forward(forward_batch)\n  File \"sglang/python/sglang/srt/model_executor/model_runner.py\", line 787, in forward\n    return self.forward_idle(forward_batch)\n  File \"sglang/python/sglang/srt/model_executor/model_runner.py\", line 770, in forward_idle\n    return self.model.forward(\n  File \"sglang/python/sglang/srt/models/mixtral.py\", line 314, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)\n  File \"python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"sglang/python/sglang/srt/models/mixtral.py\", line 286, in forward\n    hidden_states, residual = layer(\n  File \"python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"sglang/python/sglang/srt/models/mixtral.py\", line 232, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"python3.10/site-packages/vllm/model_executor/custom_op.py\", line 26, in forward\n    return self._forward_method(*args, **kwargs)\n  File \"sglang/python/sglang/srt/layers/layernorm.py\", line 59, in forward_cuda\n    out = rmsnorm(x, self.weight.data, self.variance_epsilon)\n  File \"python3.10/site-packages/sgl_kernel/ops/__init__.py\", line 156, in rmsnorm\n    torch.ops.sgl_kernels.rmsnorm(out, input, weight, eps, _get_cuda_stream(device))\n  File \"python3.10/site-packages/torch/_ops.py\", line 1116, in __call__\n    return self._op(*args, **(kwargs or {}))\n  File \"python3.10/site-packages/torch/utils/_device.py\", line 106, in __torch_function__\n    return func(*args, **kwargs)\n  File \"python3.10/site-packages/torch/_ops.py\", line 1116, in __call__\n    return self._op(*args, **(kwargs or {}))\nRuntimeError: RMSNorm failed with error code invalid configuration argument\n```\n\n\n### Reproduction\n\nModel: Mixtral 8x22B\nScript: MMLU benchmark\n\nPlease see above.\n\n### Environment\n\n```\nPython: 3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:16:10) [GCC 13.3.0]\nCUDA available: True\nGPU 0,1,2,3: NVIDIA H100\nGPU 0,1,2,3 Compute Capability: 9.0\nCUDA_HOME: cuda/gcc/11.3.1/12.4.1-r5e7ajh\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.90.12\nPyTorch: 2.5.1+cu124\nflashinfer: 0.1.6+cu124torch2.4\ntriton: 3.1.0\ntransformers: 4.48.2\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.61.0\nanthropic: 0.45.2\ndecord: 0.6.0\n```",
    "labels": [
      "good first issue",
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2025-02-05T02:25:13+00:00",
    "closed_at": "2025-05-11T15:17:16+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3304/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3304"
  },
  {
    "number": 5324,
    "title": "[Feature] Support awq quantization for MoE model on CPU.",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nI tried to run deepseek-r1-awq on CPU with vLLM, but I failed.This is because vLLM will convert awq to awq_marlin if the model is a MoE model, and marlin kernal is not supported on CPU. And I can't run the model directly in awq quantization, because [awq.py](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/quantization/awq.py) doesn't handle fused MoE case, this will result in unexpected errors.\nI browse [/python/sglang/srt/layers/quantization/awq.py](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/layers/quantization/awq.py) and notice that it doesn't handle fused MoE case as well. I suggest that the support for fused MoE models could be added:)\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-04-12T10:41:48+00:00",
    "closed_at": "2025-06-30T00:21:16+00:00",
    "comments": 39,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5324/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5324"
  }
]
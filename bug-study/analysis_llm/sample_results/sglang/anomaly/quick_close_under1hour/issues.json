[
  {
    "number": 1054,
    "title": "[Feature] any accuracy score on MMLU or CMMLU ?",
    "body": "### Motivation\n\nI have tested the sglang with multi llmperf tools, really impresive.\r\nAny benchmark on accuracy score on MMLU or CMMLU \n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-12T11:04:55+00:00",
    "closed_at": "2024-08-12T11:16:24+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1054/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1054"
  },
  {
    "number": 6371,
    "title": "[Feature] Suggestion: Add Documentation for PD Disaggregation (link to Mooncake integration guide)",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nHi team,\n\nSGLang has already implemented PD Disaggregation, which is great. To make this feature easier to adopt, I suggest adding documentation or linking to the following integration guide from the Mooncake project: https://github.com/kvcache-ai/Mooncake/blob/main/doc/en/sglang-integration-v1.md\n\nThis guide provides clear instructions for enabling SGLang's PD disaggregation feature and could be very helpful to users.\n\nThanks!\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-17T11:58:22+00:00",
    "closed_at": "2025-05-17T12:11:12+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6371/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6371"
  },
  {
    "number": 2795,
    "title": "[Bug] deepseek v3 inference with 2*8*H800 cannot STOP",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n## output info\r\n```\r\n[2025-01-08 20:17:32 TP0] Decode batch. #running-req: 2, #token: 27373, token usage: 0.09, gen throughput (token/s): 33.38, #queue-req: 0\r\n[2025-01-08 20:17:34 TP0] Decode batch. #running-req: 2, #token: 27453, token usage: 0.09, gen throughput (token/s): 33.32, #queue-req: 0\r\n[2025-01-08 20:17:37 TP0] Decode batch. #running-req: 2, #token: 27533, token usage: 0.09, gen throughput (token/s): 32.73, #queue-req: 0\r\n[2025-01-08 20:17:39 TP0] Decode batch. #running-req: 2, #token: 27613, token usage: 0.09, gen throughput (token/s): 33.27, #queue-req: 0\r\n[2025-01-08 20:17:41 TP0] Decode batch. #running-req: 2, #token: 27693, token usage: 0.09, gen throughput (token/s): 33.22, #queue-req: 0\r\n[2025-01-08 20:17:44 TP0] Decode batch. #running-req: 2, #token: 27773, token usage: 0.09, gen throughput (token/s): 33.13, #queue-req: 0\r\n[2025-01-08 20:17:46 TP0] Decode batch. #running-req: 2, #token: 27853, token usage: 0.09, gen throughput (token/s): 33.29, #queue-req: 0\r\n[2025-01-08 20:17:49 TP0] Decode batch. #running-req: 2, #token: 27933, token usage: 0.09, gen throughput (token/s): 32.47, #queue-req: 0\r\n[2025-01-08 20:17:51 TP0] Decode batch. #running-req: 2, #token: 28013, token usage: 0.09, gen throughput (token/s): 33.13, #queue-req: 0\r\n[2025-01-08 20:17:54 TP0] Decode batch. #running-req: 2, #token: 28093, token usage: 0.09, gen throughput (token/s): 33.12, #queue-req: 0\r\n[2025-01-08 20:17:56 TP0] Decode batch. #running-req: 2, #token: 28173, token usage: 0.09, gen throughput (token/s): 33.11, #queue-req: 0\r\n[2025-01-08 20:17:58 TP0] Decode batch. #running-req: 2, #token: 28253, token usage: 0.09, gen throughput (token/s): 33.18, #queue-req: 0\r\n[2025-01-08 20:18:01 TP0] Decode batch. #running-req: 2, #token: 28333, token usage: 0.09, gen throughput (token/s): 32.45, #queue-req: 0\r\n[2025-01-08 20:18:03 TP0] Decode batch. #running-req: 2, #token: 28413, token usage: 0.09, gen throughput (token/s): 33.03, #queue-req: 0\r\n[2025-01-08 20:18:06 TP0] Decode batch. #running-req: 2, #token: 28493, token usage: 0.09, gen throughput (token/s): 33.00, #queue-req: 0\r\n[2025-01-08 20:18:08 TP0] Decode batch. #running-req: 2, #token: 28573, token usage: 0.09, gen throughput (token/s): 32.97, #queue-req: 0\r\n[2025-01-08 20:18:10 TP0] Decode batch. #running-req: 2, #token: 28653, token usage: 0.09, gen throughput (token/s): 32.99, #queue-req: 0\r\n[2025-01-08 20:18:13 TP0] Decode batch. #running-req: 2, #token: 28733, token usage: 0.09, gen throughput (token/s): 32.39, #queue-req: 0\r\n[2025-01-08 20:18:15 TP0] Decode batch. #running-req: 2, #token: 28813, token usage: 0.10, gen throughput (token/s): 32.99, #queue-req: 0\r\n\r\n```\r\n\r\nIt seem like sglang server cannot find stop token. \r\nIf set max_tokens, the output would arrive the max token limites and stop. But content is EMPTY\r\n```\r\n{\r\n    \"model\": \"default\",\r\n    \"messages\": [\r\n        {\"role\": \"user\", \"content\": \"1+1=\uff1f\"}\r\n    ],\r\n    \"max_tokens\": 64,\r\n    \"stop\": [\"<|EOT|>\"]\r\n}\r\n>>>\r\n{\r\n    \"id\": \"f5b7936ed1af4f308b15f8415681ab8c\",\r\n    \"object\": \"chat.completion\",\r\n    \"created\": 1736339092,\r\n    \"model\": \"default\",\r\n    \"choices\": [\r\n        {\r\n            \"index\": 0,\r\n            \"message\": {\r\n                \"role\": \"assistant\",\r\n                \"content\": \"\",\r\n                \"tool_calls\": null\r\n            },\r\n            \"logprobs\": null,\r\n            \"finish_reason\": \"length\",\r\n            \"matched_stop\": null\r\n        }\r\n    ],\r\n    \"usage\": {\r\n        \"prompt_tokens\": 8,\r\n        \"total_tokens\": 72,\r\n        \"completion_tokens\": 64,\r\n        \"prompt_tokens_details\": null\r\n    }\r\n}\r\n```\n\n### Reproduction\n\npython -m sglang.launch_server --model-path DeepSeek-V3 --tp 16 --dist-init-addr 10.54.108.139:8200 --nnodes 2 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 8000\r\npython -m sglang.launch_server --model-path DeepSeek-V3 --tp 16 --dist-init-addr 10.54.108.139:8200 --nnodes 2 --node-rank 1 --trust-remote-code --host 0.0.0.0 --port 8000\n\n### Environment\n\n```\r\nPython: 3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: CF-NG-HZZ1-O\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.3, V12.3.103\r\nCUDA Driver Version: 535.183.06\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.4.1.post3\r\nflashinfer: 0.1.6+cu124torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.47.1\r\ntorchao: 0.7.0\r\nnumpy: 1.26.4\r\naiohttp: 3.8.5\r\nfastapi: 0.111.0\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.27.0\r\ninteregular: 0.3.3\r\nmodelscope: 1.21.1\r\norjson: 3.9.7\r\npackaging: 22.0\r\npsutil: 5.9.0\r\npydantic: 2.10.0\r\nmultipart: 0.0.17\r\nzmq: 26.2.0\r\nuvicorn: 0.32.1\r\nuvloop: 0.21.0\r\nvllm: 0.6.3.post1\r\nopenai: 1.59.3\r\nanthropic: 0.42.0\r\ndecord: 0.6.0\r\nNVIDIA Topology: \r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1   NIC2     NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE   NODE     NODE    NODE    PIX     SYS     SYS     SYS     SYS     0-47,96-143     0      N/A\r\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE   NODE     NODE    PIX     NODE    SYS     SYS     SYS     SYS     0-47,96-143     0      N/A\r\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE   NODE     PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-47,96-143     0      N/A\r\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE   PIX      NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-47,96-143     0      N/A\r\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS    SYS      SYS     SYS     SYS     NODE    PIX     NODE    NODE    48-95,144-191   1      N/A\r\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS    SYS      SYS     SYS     SYS     PIX     NODE    NODE    NODE    48-95,144-191   1      N/A\r\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS    SYS      SYS     SYS     SYS     NODE    NODE    NODE    PIX     48-95,144-191   1      N/A\r\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS    SYS      SYS     SYS     SYS     NODE    NODE    PIX     NODE    48-95,144-191   1      N/A\r\nNIC0    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      PIX    NODE     NODE    NODE    NODE    SYS     SYS     SYS     SYS\r\nNIC1    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX      X     NODE     NODE    NODE    NODE    SYS     SYS     SYS     SYS\r\nNIC2    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    X       NODE    NODE    NODE    SYS     SYS     SYS     SYS\r\nNIC3    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE   NODE      X      NODE    NODE    SYS     SYS     SYS     SYS\r\nNIC4    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE   NODE     NODE     X      NODE    SYS     SYS     SYS     SYS\r\nNIC5    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE   NODE     NODE    NODE     X      SYS     SYS     SYS     SYS\r\nNIC6    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS    SYS      SYS     SYS     SYS      X      NODE    NODE    NODE\r\nNIC7    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS    SYS      SYS     SYS     SYS     NODE     X      NODE    NODE\r\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS    SYS      SYS     SYS     SYS     NODE    NODE     X      NODE\r\nNIC9    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS    SYS      SYS     SYS     SYS     NODE    NODE    NODE     X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  NIC8: mlx5_8\r\n  NIC9: mlx5_9\r\n\r\n\r\nulimit soft: 1048576\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-01-08T12:31:23+00:00",
    "closed_at": "2025-01-08T12:57:38+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2795/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2795"
  },
  {
    "number": 1509,
    "title": "[Bug] tensor parallel run error",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n`python3 -m sglang.bench_latency --model meta-llama/Meta-Llama-3-8B --batch-size 1 --input 128 --output 8 --tensor-parallel-size 2`\r\n\r\n\r\n[19:01:01 TP0] Init nccl begin.\r\n[19:01:01 TP1] Init nccl begin.\r\nNCCL version 2.20.5+cuda12.4\r\nFailed: Cuda error /workspace/csrc/custom_all_reduce.cuh:307 'peer access is not supported between these two devices'\r\nFailed: Cuda error /workspace/csrc/custom_all_reduce.cuh:307 'peer access is not supported between these two devices'\r\n[rank1]:[W924 19:01:01.260102563 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\r\n[rank0]:[W924 19:01:01.260656038 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n\n### Reproduction\n\n`python3 -m sglang.bench_latency --model meta-llama/Meta-Llama-3-8B --batch-size 1 --input 128 --output 8 --tensor-parallel-size 2`\r\n\r\nin 4xH100 GPU machine\n\n### Environment\n\nCUDA available: True\r\nGPU 0,1,2,3: NVIDIA H100\r\nGPU 0,1,2,3 Compute Capability: 9.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.0, V12.0.140\r\nCUDA Driver Version: 525.105.17\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.3.1.post3\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.44.2\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.26.4\r\naiohttp: 3.10.5\r\nfastapi: 0.115.0\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.25.1\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.9.2\r\nuvicorn: 0.30.6\r\nuvloop: 0.20.0\r\nzmq: 26.2.0\r\nvllm: 0.5.5\r\nmultipart: 0.0.10\r\nopenai: 1.47.1\r\nanthropic: 0.34.2\r\nNVIDIA Topology:\r\n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity\r\nGPU0     X      PHB     PHB     PHB     0-183           N/A\r\nGPU1    PHB      X      PHB     PHB     0-183           N/A\r\nGPU2    PHB     PHB      X      PHB     0-183           N/A\r\nGPU3    PHB     PHB     PHB      X      0-183           N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nHypervisor vendor: KVM\r\nulimit soft: 524288",
    "labels": [],
    "state": "closed",
    "created_at": "2024-09-25T02:09:52+00:00",
    "closed_at": "2024-09-25T02:12:18+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1509/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1509"
  },
  {
    "number": 3569,
    "title": "[Bug] v0.4.2.post4 ValueError: Unrecognized configuration class <class 'transformers_modules.DeepSeek-R1.configuration_deepseek.DeepseekV3Config'> to build an AutoTokenizer.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nValueError: Unrecognized configuration class <class 'transformers_modules.DeepSeek-R1.configuration_deepseek.DeepseekV3Config'> to build an AutoTokenizer.\nModel type should be one of AlbertConfig, AlignConfig, AriaConfig, BarkConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, BrosConfig, CamembertConfig, CanineConfig, ChameleonConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, ClvpConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, ColPaliConfig, ConvBertConfig, CpmAntConfig, CTRLConfig, Data2VecAudioConfig, Data2VecTextConfig, DbrxConfig, DebertaConfig, DebertaV2Config, DiffLlamaConfig, DistilBertConfig, DPRConfig, ElectraConfig, Emu3Config, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FalconMambaConfig, FastSpeech2ConformerConfig, FlaubertConfig, FNetConfig, FSMTConfig, FunnelConfig, GemmaConfig, Gemma2Config, GitConfig, GlmConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GroundingDinoConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, Idefics2Config, Idefics3Config, InstructBlipConfig, InstructBlipVideoConfig, JambaConfig, JetMoeConfig, JukeboxConfig, Kosmos2Config, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LiltConfig, LlamaConfig, LlavaConfig, LlavaNextConfig, LlavaNextVideoConfig, LlavaOnevisionConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MistralConfig, MixtralConfig, MllamaConfig, MobileBertConfig, ModernBertConfig, MoonshineConfig, MoshiConfig, MPNetConfig, MptConfig, MraConfig, MT5Config, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OlmoConfig, Olmo2Config, OlmoeConfig, OmDetTurboConfig, OneFormerConfig, OpenAIGPTConfig, OPTConfig, Owlv2Config, OwlViTConfig, PaliGemmaConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PersimmonConfig, PhiConfig, Phi3Config, PhimoeConfig, Pix2StructConfig, PixtralVisionConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2AudioConfig, Qwen2MoeConfig, Qwen2VLConfig, RagConfig, RealmConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SiglipConfig, Speech2TextConfig, Speech2Text2Config, SpeechT5Config, SplinterConfig, SqueezeBertConfig, StableLmConfig, Starcoder2Config, SwitchTransformersConfig, T5Config, TapasConfig, TransfoXLConfig, TvpConfig, UdopConfig, UMT5Config, VideoLlavaConfig, ViltConfig, VipLlavaConfig, VisualBertConfig, VitsConfig, Wav2Vec2Config, Wav2Vec2BertConfig, Wav2Vec2ConformerConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YosoConfig, ZambaConfig.\n\n### Reproduction\n\nName: sglang\nVersion: 0.4.2.post4\nName: transformers\nVersion: 4.48.3\n\n### Environment\n\nName: sglang\nVersion: 0.4.2.post4\nName: transformers\nVersion: 4.48.3\nH100*16",
    "labels": [
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-14T06:25:13+00:00",
    "closed_at": "2025-02-14T06:51:17+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3569/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3569"
  },
  {
    "number": 7083,
    "title": "[Bug]",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n[2025-06-11 06:19:30] INFO:     172.20.0.1:40106 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\n[2025-06-11 06:19:30] ERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/fastapi/applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/starlette/applications.py\", line 112, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    raise exc\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n    await self.app(scope, receive, send)\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/starlette/routing.py\", line 714, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/starlette/routing.py\", line 734, in app\n    await route.handle(scope, receive, send)\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n               ^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    return await dependant.call(**values)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/entrypoints/http_server.py\", line 618, in openai_v1_chat_completions\n    return await v1_chat_completions(_global_state.tokenizer_manager, raw_request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/openai_api/adapter.py\", line 1471, in v1_chat_completions\n    all_requests = [ChatCompletionRequest(**request_json)]\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/anaconda3/envs/sglang/lib/python3.12/site-packages/pydantic/main.py\", line 253, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 3 validation errors for ChatCompletionRequest\nmessages.2.ChatCompletionMessageGenericParam.content\n  Field required [type=missing, input_value={'role': 'assistant', 'to...}, 'type': 'function'}]}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nmessages.2.ChatCompletionMessageUserParam.role\n  Input should be 'user' [type=literal_error, input_value='assistant', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/literal_error\nmessages.2.ChatCompletionMessageUserParam.content\n  Field required [type=missing, input_value={'role': 'assistant', 'to...}, 'type': 'function'}]}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\n\n\n\n\n\n### Reproduction\n\npython -m sglang.launch_server --model-path ModelFiles/deepseek-ai/DeepSeek-V3 --tp 8 --served-model-name DeepSeek-V3 --port 30000 --host 0.0.0.0 --tool-call-parser deepseekv3 --chat-template tool_jinja_file/tool_chat_template_deepseekv3.jinja\n\n\n\u4f7f\u7528SGLang\u90e8\u7f72Deepseek-v3-671B-FP8\uff0c\u8c03\u7528\u5de5\u5177\u65f6\u53d1\u751f\u62a5\u9519\n\n### Environment\n\nPython: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:09:17) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.2, V12.2.140\nCUDA Driver Version: 550.127.05\nPyTorch: 2.7.1+cu126\nsglang: 0.4.7\nsgl_kernel: 0.1.7\nflashinfer_python: 0.2.6.post1\ntriton: 3.3.1\ntransformers: 4.52.3\ntorchao: 0.9.0\nnumpy: 2.3.0\naiohttp: 3.12.12\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.32.5\ninteregular: 0.3.3\nmodelscope: 1.26.0\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.5\npython-multipart: 0.0.20\npyzmq: 26.4.0\nuvicorn: 0.34.3\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.19\nopenai: 1.86.0\ntiktoken: 0.9.0\nanthropic: 0.54.0\nlitellm: 1.72.4\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    SYS     PIX     NODE    SYS     SYS     0-89    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     PIX     NODE    SYS     SYS     0-89    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     NODE    PIX     SYS     SYS     0-89    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     NODE    PIX     SYS     SYS     0-89    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     PIX     NODE    90-175  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     PIX     NODE    90-175  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     NODE    PIX     90-175  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     NODE    PIX     90-175  1               N/A\nNIC0    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS\nNIC1    PIX     PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS      X      NODE    SYS     SYS\nNIC2    NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     SYS     NODE     X      SYS     SYS\nNIC3    SYS     SYS     SYS     SYS     PIX     PIX     NODE    NODE    SYS     SYS     SYS      X      NODE\nNIC4    SYS     SYS     SYS     SYS     NODE    NODE    PIX     PIX     SYS     SYS     SYS     NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n\n\nHypervisor vendor: KVM\nulimit soft: 1048576",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-11T06:27:38+00:00",
    "closed_at": "2025-06-11T06:47:05+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7083/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7083"
  },
  {
    "number": 562,
    "title": "[Model] Adding support for MiniCPM-Llama3-V-2_5",
    "body": "Please support for **MiniCPM-Llama3-V-2_5**. \r\n- HuggingFace Page: https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5\r\n- Github : https://github.com/OpenBMB/MiniCPM-V\r\n\r\n\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-06-25T01:06:15+00:00",
    "closed_at": "2024-06-25T01:06:28+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/562/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/562"
  },
  {
    "number": 3462,
    "title": "Which one is faster for token generation? LLama.cpp v.s. SGLang",
    "body": "Is there any benchmark comparison based on the same batch size (= 1 for token generation) and same quant.ed type?",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-10T06:52:00+00:00",
    "closed_at": "2025-02-10T06:55:25+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3462/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3462"
  },
  {
    "number": 3460,
    "title": "[Bug] RuntimeError: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details)",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n\u51fa\u73b0\u5982\u4e0b\u9519\u8bef\uff1a\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1787, in run_scheduler_process\nscheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 240, in init\nself.tp_worker = TpWorkerClass(\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in init\nself.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 68, in init\nself.model_runner = ModelRunner(\nFile \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 178, in init\nmin_per_gpu_memory = self.init_torch_distributed()\nFile \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 253, in init_torch_distributed\ninitialize_model_parallel(tensor_model_parallel_size=self.tp_size)\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py\", line 1055, in initialize_model_parallel\n_TP = init_model_parallel_group(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py\", line 890, in init_model_parallel_group\nreturn GroupCoordinator(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py\", line 233, in init\nself.pynccl_comm = PyNcclCommunicator(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/pynccl.py\", line 108, in init\nself.comm: ncclComm_t = self.nccl.ncclCommInitRank(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py\", line 350, in ncclCommInitRank\nself.NCCL_CHECK(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py\", line 329, in NCCL_CHECK\nraise RuntimeError(f\"NCCL error: {error_str}\")\nRuntimeError: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details)\n\n[2025-02-09 21:23:00 TP0] Scheduler hit an exception: Traceback (most recent call last):\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1787, in run_scheduler_process\nscheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 240, in init\nself.tp_worker = TpWorkerClass(\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in init\nself.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 68, in init\nself.model_runner = ModelRunner(\nFile \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 178, in init\nmin_per_gpu_memory = self.init_torch_distributed()\nFile \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 253, in init_torch_distributed\ninitialize_model_parallel(tensor_model_parallel_size=self.tp_size)\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py\", line 1055, in initialize_model_parallel\n_TP = init_model_parallel_group(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py\", line 890, in init_model_parallel_group\nreturn GroupCoordinator(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py\", line 233, in init\nself.pynccl_comm = PyNcclCommunicator(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/pynccl.py\", line 108, in init\nself.comm: ncclComm_t = self.nccl.ncclCommInitRank(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py\", line 350, in ncclCommInitRank\nself.NCCL_CHECK(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py\", line 329, in NCCL_CHECK\nraise RuntimeError(f\"NCCL error: {error_str}\")\nRuntimeError: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details)\n\n[2025-02-09 21:23:00] Received sigquit from a child proces. It usually means the child failed.\n[2025-02-09 21:23:00] Received sigquit from a child proces. It usually means the child failed.\n[2025-02-09 21:23:00 TP2] Scheduler hit an exception: Traceback (most recent call last):\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1787, in run_scheduler_process\nscheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 240, in init\nself.tp_worker = TpWorkerClass(\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in init\nself.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 68, in init\nself.model_runner = ModelRunner(\nFile \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 178, in init\nmin_per_gpu_memory = self.init_torch_distributed()\nFile \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 253, in init_torch_distributed\ninitialize_model_parallel(tensor_model_parallel_size=self.tp_size)\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py\", line 1055, in initialize_model_parallel\n_TP = init_model_parallel_group(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py\", line 890, in init_model_parallel_group\nreturn GroupCoordinator(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py\", line 233, in init\nself.pynccl_comm = PyNcclCommunicator(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/pynccl.py\", line 108, in init\nself.comm: ncclComm_t = self.nccl.ncclCommInitRank(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py\", line 350, in ncclCommInitRank\nself.NCCL_CHECK(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py\", line 329, in NCCL_CHECK\nraise RuntimeError(f\"NCCL error: {error_str}\")\nRuntimeError: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details)\n\n[2025-02-09 21:23:00] Received sigquit from a child proces. It usually means the child failed.\n[2025-02-09 21:23:00 TP3] Scheduler hit an exception: Traceback (most recent call last):\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1787, in run_scheduler_process\nscheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 240, in init\nself.tp_worker = TpWorkerClass(\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in init\nself.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 68, in init\nself.model_runner = ModelRunner(\nFile \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 178, in init\nmin_per_gpu_memory = self.init_torch_distributed()\nFile \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 253, in init_torch_distributed\ninitialize_model_parallel(tensor_model_parallel_size=self.tp_size)\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py\", line 1055, in initialize_model_parallel\n_TP = init_model_parallel_group(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py\", line 890, in init_model_parallel_group\nreturn GroupCoordinator(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py\", line 233, in init\nself.pynccl_comm = PyNcclCommunicator(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/pynccl.py\", line 108, in init\nself.comm: ncclComm_t = self.nccl.ncclCommInitRank(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py\", line 350, in ncclCommInitRank\nself.NCCL_CHECK(\nFile \"/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py\", line 329, in NCCL_CHECK\nraise RuntimeError(f\"NCCL error: {error_str}\")\nRuntimeError: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details)\n\n### Reproduction\n\n3\u53f0L40\u51fa\u73b0tp=24\u9664\u4e0d\u5c3d\u7684\u60c5\u51b5\uff0c\n\u6211\u60f3\u7740tp=20 node=5\n\u57283\u53f0\u670d\u52a1\u4e0a\u90e8\u7f725\u4e2a\u5bb9\u5668\uff0c\u542f\u52a8\u65f6\u51fa\u73b0\u4e86\u4e0a\u8ff0\u7684\u9519\u8bef\n\n### Environment\n\n1",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-10T06:21:55+00:00",
    "closed_at": "2025-02-10T06:23:23+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3460/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3460"
  },
  {
    "number": 6435,
    "title": "[Feature] B200 support timeline",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nHi there - would love to know if B200 will be available and when. \n\nWe're doing some tests on Sglang on 8xB200s, and found the image from docker hub to freeze. It seems like Blackwell support is still an ongoing thing. \n\nThanks for any info!\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-19T21:06:20+00:00",
    "closed_at": "2025-05-19T21:06:34+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6435/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6435"
  },
  {
    "number": 3817,
    "title": "[Bug]",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n### environment\ndocker-image: lmsysorg/sglang:v0.4.3.post2-cu125 (entrypoint: /bin/bash)\ncommand to launch docker container: `docker run -itd --name sglang -v /export:/export -p 6000:6000 --gpus=all --entrypoint /bin/bash --shm-size=10gb lmsysorg/sglang:v0.4.3.post2-cu125`\n<details>\n<summary>output from vllm/collect_env.py: </summary>\n\n```\nINFO 02-24 12:21:16 __init__.py:190] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.14.0-362.8.1.el9_3.x86_64-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA H200\nGPU 1: NVIDIA H200\nGPU 2: NVIDIA H200\nGPU 3: NVIDIA H200\nGPU 4: NVIDIA H200\nGPU 5: NVIDIA H200\nGPU 6: NVIDIA H200\nGPU 7: NVIDIA H200\n\nNvidia driver version: 550.127.05\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.1.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      46 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             192\nOn-line CPU(s) list:                0-191\nVendor ID:                          GenuineIntel\nModel name:                         INTEL(R) XEON(R) PLATINUM 8558\nCPU family:                         6\nModel:                              207\nThread(s) per core:                 2\nCore(s) per socket:                 48\nSocket(s):                          2\nStepping:                           2\nBogoMIPS:                           4200.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                     VT-x\nL1d cache:                          4.5 MiB (96 instances)\nL1i cache:                          3 MiB (96 instances)\nL2 cache:                           192 MiB (96 instances)\nL3 cache:                           520 MiB (2 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190\nNUMA node1 CPU(s):                  1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159,161,163,165,167,169,171,173,175,177,179,181,183,185,187,189,191\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.1.post2+cu124torch2.5\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchao==0.8.0\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.3\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8  NIC9     CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS   SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS   SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS   SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS   SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     PIX     PIX     NODE  NODE     NODE    1,3,5,7,9,11    1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX   NODE     NODE    1,3,5,7,9,11    1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE  PIX      NODE    1,3,5,7,9,11    1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE  NODE     PIX     1,3,5,7,9,11    1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS   SYS      SYS\nNIC1    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     SYS     SYS     SYS   SYS      SYS\nNIC2    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS     SYS     SYS     SYS   SYS      SYS\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS   SYS      SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      PIX     PIX     NODE  NODE     NODE\nNIC5    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX      X      PIX     NODE  NODE     NODE\nNIC6    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     PIX      X      NODE  NODE     NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X    NODE     NODE\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE   X       NODE\nNIC9    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE  NODE      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n\nNVIDIA_VISIBLE_DEVICES=all\nCUBLAS_VERSION=12.4.5.8\nNVIDIA_REQUIRE_CUDA=cuda>=9.0\nCUDA_CACHE_DISABLE=1\nNCCL_VERSION=2.21.5\nNVIDIA_DRIVER_CAPABILITIES=compute,utility,video\nNVIDIA_PRODUCT_NAME=Triton Server Base\nCUDA_VERSION=12.4.1.003\nCUDNN_VERSION=9.1.0.70\nNVIDIA_TRITON_SERVER_BASE_VERSION=24.04\nLD_LIBRARY_PATH=/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNVIDIA_BUILD_ID=90085237\nCUDA_DRIVER_VERSION=550.54.15\nNVIDIA_REQUIRE_JETPACK_HOST_MOUNTS=\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n### command to launch server\n\ninside the docker container, run:\n\n```shell\n# The directory /export/dp/DeepSeek-R1 contains model files from the HuggingFace repo. deepseek-ai/DeepSeek-R1\nnohup python3 -m sglang.launch_server --model /export/dp/DeepSeek-R1 --tp 8 --trust-remote-code --enable-dp-attention --disable-overlap-schedule --max-running-requests 512 --context-length 8192 > /export/sglang_serve.log 2>&1 &\n```\n\n### test of a request\n\ninside the same docker container, run:\n\n```shell\ncurl -s http://localhost:30000/v1/chat/completions -H 'Content-Type: application/json' -d '{\n  \"model\": \"deepseek_r1\",\n  \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant. You should think step by step.\"},\n    {\"role\": \"user\", \"content\": \"What is your name?\"}\n  ],\n  \"temperature\": 0.6,\n  \"top_p\": 0.95,\n  \"top_k\": 15,\n  \"max_tokens\": 500\n}'\n```\n\nThe response looks like:\n```\n{\"id\":\"635f23d9166e4a8791d8d6dce25deddd\",\"object\":\"chat.completion\",\"created\":1740399515,\"model\":\"deepseek_r1\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\" 1.es\\n\\n\\n\\n, {\\n\\n\\n\\n.\\n.\\n.ae\\n\\n.ae. sae\\n\\nemem,, sa\\n\\n most \\n\\num,..\\n\\n..1.,ae  io.ae\\n\\numstyle\\n .straum and..\\n\\nis way..series-image {op colora.am. sapoenum.. 14 e_shas. sau.u.a,. ofum.ing the 3 a am.apq 0u   in etupe s.. Proe de, of.aeumteric,,amam the.. thep.ut.d.. \\n,.. the tki\\n\\n mostu's ofu.,. p,\\n\\n.i a,,., ofu. theattersisius 1 ofu a s o. and o of the per the,..> theot.,. deu.ae p.\\n\\nute. em in of theensis thei.ensis in.um.amp...\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"stop\",\"matched_stop\":1}],\"usage\":{\"prompt_tokens\":23,\"total_tokens\":250,\"completion_tokens\":227,\"prompt_tokens_details\":null}}\n```\n\n### Reproduction\n\nPlease refer to the \"environment\"/\"command_to_launch_server\"/\"test_of_a_request\" session above.\n\n### Environment\n\n<details>\n<summary> output from `python3 -m sglang.check_env` </summary>\n\n```\nINFO 02-24 12:26:48 __init__.py:190] Automatically detected platform cuda.\nPython: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H200\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.127.05\nPyTorch: 2.5.1+cu124\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.1.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.23.0\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.63.2\ntiktoken: 0.9.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7  NIC8     NIC9    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS   SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS   SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS   SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS   SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     PIX     PIX     NODE  NODE     NODE    1,3,5,7,9,11    1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX   NODE     NODE    1,3,5,7,9,11    1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE  PIX      NODE    1,3,5,7,9,11    1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE  NODE     PIX     1,3,5,7,9,11    1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS   SYS      SYS\nNIC1    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     SYS     SYS     SYS   SYS      SYS\nNIC2    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS     SYS     SYS     SYS   SYS      SYS\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS   SYS      SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      PIX     PIX     NODE  NODE     NODE\nNIC5    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX      X      PIX     NODE  NODE     NODE\nNIC6    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     PIX      X      NODE  NODE     NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X    NODE     NODE\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE   X       NODE\nNIC9    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE  NODE      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n\n\nulimit soft: 1073741816\n```\n\n</details>",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-24T12:30:20+00:00",
    "closed_at": "2025-02-24T12:32:47+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3817/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3817"
  },
  {
    "number": 2889,
    "title": "[Bug] Performance on DeepSeek-V2",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [X] 2. The bug has not been fixed in the latest version.\r\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [ ] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nI have tested Deepseek-v2 on SGlang 0.3.5 , Recent, I test this model performance on SGlang 0.4.1.post5 again, and I found the MLA kernel (__fwd_kernel)  faster in Prefill phase.\r\nBut there is no change with the __fwd_kernel triton operator  . how does this kernel faster, or there are some differences on benchmark testcase?\r\n\r\n**SGLang0.3.5**\r\n`Warmup ...\r\nPrefill. latency: 3.69200 s, throughput:   1974.00 token/s\r\nDecode.  latency: 0.06813 s, throughput:     14.68 token/s\r\nDecode.  latency: 0.04793 s, throughput:     20.87 token/s\r\nDecode.  latency: 0.04779 s, throughput:     20.93 token/s\r\nDecode.  latency: 0.04779 s, throughput:     20.93 token/s\r\nDecode.  latency: 0.04796 s, throughput:     20.85 token/s\r\nDecode.  median latency: 0.04796 s, median throughput:     20.85 token/s\r\nTotal. latency:  4.048 s, throughput:   1802.30 token/s\r\nBenchmark ...\r\nPrefill. latency: 1.63664 s, throughput:   4453.03 token/s\r\nDecode.  latency: 0.04823 s, throughput:     20.73 token/s\r\nDecode.  latency: 0.04784 s, throughput:     20.90 token/s\r\nDecode.  latency: 0.04782 s, throughput:     20.91 token/s\r\nDecode.  latency: 0.04794 s, throughput:     20.86 token/s\r\nDecode.  latency: 0.04793 s, throughput:     20.86 token/s\r\nDecode.  median latency: 0.04877 s, median throughput:     20.50 token/s\r\nTotal. latency: 11.365 s, throughput:    658.86 token/s`\r\n\r\n\r\n**SGLang 0.4.1**\r\n`Warmup ...\r\nPrefill. latency: 2.43144 s, throughput:   2997.39 token/s\r\nDecode.  latency: 1.64079 s, throughput:      0.61 token/s\r\nDecode.  latency: 0.02459 s, throughput:     40.66 token/s\r\nDecode.  latency: 0.02415 s, throughput:     41.41 token/s\r\nDecode.  latency: 0.02414 s, throughput:     41.43 token/s\r\nDecode.  latency: 0.02417 s, throughput:     41.37 token/s\r\nDecode.  median latency: 0.02415 s, median throughput:     41.41 token/s\r\nTotal. latency:  4.217 s, throughput:   1730.07 token/s\r\nBenchmark ...\r\nPrefill. latency: 0.60336 s, throughput:  12078.95 token/s\r\nDecode.  latency: 0.02449 s, throughput:     40.83 token/s\r\nDecode.  latency: 0.02446 s, throughput:     40.87 token/s\r\nDecode.  latency: 0.02423 s, throughput:     41.28 token/s\r\nDecode.  latency: 0.02417 s, throughput:     41.37 token/s\r\nDecode.  latency: 0.02407 s, throughput:     41.54 token/s\r\nDecode.  median latency: 0.02371 s, median throughput:     42.17 token/s\r\nTotal. latency:  5.328 s, throughput:   1405.34 token/s`\r\n\r\n\r\n![image](https://github.com/user-attachments/assets/5c1855de-38e6-4757-8109-4e250c3bbf5b)\r\n![image](https://github.com/user-attachments/assets/009743de-7bbd-4351-8dc0-1856e707f14a)\r\n\r\n\r\n### Reproduction\r\n\r\n# sglang 0.3.5\r\n`python -m sglang.bench_latency --model-path  Deepseek-v2 --tensor-parallel-size 4 --quantization fp8 --batch-size 1 --input-len 7288 --output-len 200 --trust-remote-code`\r\n\r\n# sglang 0.4.1\r\n`python -m sglang.bench_one_batch --model-path Deepseek-V2 --tensor-parallel-size 4 --quantization fp8 --batch-size 1 --input-len 7288 --output-len 200 --trust-remote-code`\r\n\r\n\r\n\r\n\r\n### Environment\r\n\r\n# SGLang 0.3.5\r\n`/opt/conda/envs/sglang_0.3.5/lib/python3.10/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\r\n* 'fields' has been removed\r\n  warnings.warn(message, UserWarning)\r\nPython: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA H800\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.6, V12.6.85\r\nCUDA Driver Version: 565.57.01\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.3.5\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.48.0\r\nrequests: 2.32.3\r\ntqdm: 4.67.1\r\nnumpy: 1.26.4\r\naiohttp: 3.11.11\r\nfastapi: 0.115.6\r\nhf_transfer: 0.1.9\r\nhuggingface_hub: 0.27.1\r\ninteregular: 0.3.3\r\npackaging: 24.2\r\nPIL: 10.4.0\r\npsutil: 6.1.1\r\npydantic: 2.10.5\r\nuvicorn: 0.34.0\r\nuvloop: 0.21.0\r\nzmq: 26.2.0\r\nvllm: 0.6.3.post1\r\nmultipart: 0.0.20\r\nopenai: 1.59.7\r\nanthropic: 0.42.0\r\nNVIDIA Topology: \r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0  NIC1     NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    CPU Affinity  NUMA Affinity    GPU NUMA ID\r\nGPU0     X      NV8     NV8     NV8     NV8     NV8     NV8     NV8     SYS   PHB      PHB     PHB     PHB     SYS     SYS     SYS     SYS     0-89    0     N/A\r\nGPU1    NV8      X      NV8     NV8     NV8     NV8     NV8     NV8     SYS   PHB      PHB     PHB     PHB     SYS     SYS     SYS     SYS     0-89    0     N/A\r\nGPU2    NV8     NV8      X      NV8     NV8     NV8     NV8     NV8     SYS   PHB      PHB     PHB     PHB     SYS     SYS     SYS     SYS     0-89    0     N/A\r\nGPU3    NV8     NV8     NV8      X      NV8     NV8     NV8     NV8     SYS   PHB      PHB     PHB     PHB     SYS     SYS     SYS     SYS     0-89    0     N/A\r\nGPU4    NV8     NV8     NV8     NV8      X      NV8     NV8     NV8     SYS   SYS      SYS     SYS     SYS     PHB     PHB     PHB     PHB     90-179  1     N/A\r\nGPU5    NV8     NV8     NV8     NV8     NV8      X      NV8     NV8     SYS   SYS      SYS     SYS     SYS     PHB     PHB     PHB     PHB     90-179  1     N/A\r\nGPU6    NV8     NV8     NV8     NV8     NV8     NV8      X      NV8     SYS   SYS      SYS     SYS     SYS     PHB     PHB     PHB     PHB     90-179  1     N/A\r\nGPU7    NV8     NV8     NV8     NV8     NV8     NV8     NV8      X      SYS   SYS      SYS     SYS     SYS     PHB     PHB     PHB     PHB     90-179  1     N/A\r\nNIC0    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X    SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC1    PHB     PHB     PHB     PHB     SYS     SYS     SYS     SYS     SYS    X       PHB     PHB     PHB     SYS     SYS     SYS     SYS\r\nNIC2    PHB     PHB     PHB     PHB     SYS     SYS     SYS     SYS     SYS   PHB       X      PHB     PHB     SYS     SYS     SYS     SYS\r\nNIC3    PHB     PHB     PHB     PHB     SYS     SYS     SYS     SYS     SYS   PHB      PHB      X      PHB     SYS     SYS     SYS     SYS\r\nNIC4    PHB     PHB     PHB     PHB     SYS     SYS     SYS     SYS     SYS   PHB      PHB     PHB      X      SYS     SYS     SYS     SYS\r\nNIC5    SYS     SYS     SYS     SYS     PHB     PHB     PHB     PHB     SYS   SYS      SYS     SYS     SYS      X      PHB     PHB     PHB\r\nNIC6    SYS     SYS     SYS     SYS     PHB     PHB     PHB     PHB     SYS   SYS      SYS     SYS     SYS     PHB      X      PHB     PHB\r\nNIC7    SYS     SYS     SYS     SYS     PHB     PHB     PHB     PHB     SYS   SYS      SYS     SYS     SYS     PHB     PHB      X      PHB\r\nNIC8    SYS     SYS     SYS     SYS     PHB     PHB     PHB     PHB     SYS   SYS      SYS     SYS     SYS     PHB     PHB     PHB      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  NIC8: mlx5_8\r\n\r\n\r\nHypervisor vendor: KVM\r\nulimit soft: 1048576`\r\n\r\n\r\n# SGLang 0.4.1\r\n`\r\n/opt/conda/envs/sglang_0.4.1/lib/python3.10/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\r\n* 'fields' has been removed\r\n  warnings.warn(message, UserWarning)\r\nPython: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA H800\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.6, V12.6.85\r\nCUDA Driver Version: 565.57.01\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.4.1.post5\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.48.0\r\ntorchao: 0.7.0\r\nnumpy: 1.26.4\r\naiohttp: 3.11.11\r\nfastapi: 0.115.6\r\nhf_transfer: 0.1.9\r\nhuggingface_hub: 0.27.1\r\ninteregular: 0.3.3\r\nmodelscope: 1.22.0\r\norjson: 3.10.14\r\npackaging: 24.2\r\npsutil: 6.1.1\r\npydantic: 2.10.5\r\nmultipart: 0.0.20\r\nzmq: 26.2.0\r\nuvicorn: 0.34.0\r\nuvloop: 0.21.0\r\nvllm: 0.6.3.post1\r\nopenai: 1.59.7\r\nanthropic: 0.42.0\r\ndecord: 0.6.0\r\nNVIDIA Topology: \r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0  NIC1     NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    CPU Affinity  NUMA Affinity    GPU NUMA ID\r\nGPU0     X      NV8     NV8     NV8     NV8     NV8     NV8     NV8     SYS   PHB      PHB     PHB     PHB     SYS     SYS     SYS     SYS     0-89    0     N/A\r\nGPU1    NV8      X      NV8     NV8     NV8     NV8     NV8     NV8     SYS   PHB      PHB     PHB     PHB     SYS     SYS     SYS     SYS     0-89    0     N/A\r\nGPU2    NV8     NV8      X      NV8     NV8     NV8     NV8     NV8     SYS   PHB      PHB     PHB     PHB     SYS     SYS     SYS     SYS     0-89    0     N/A\r\nGPU3    NV8     NV8     NV8      X      NV8     NV8     NV8     NV8     SYS   PHB      PHB     PHB     PHB     SYS     SYS     SYS     SYS     0-89    0     N/A\r\nGPU4    NV8     NV8     NV8     NV8      X      NV8     NV8     NV8     SYS   SYS      SYS     SYS     SYS     PHB     PHB     PHB     PHB     90-179  1     N/A\r\nGPU5    NV8     NV8     NV8     NV8     NV8      X      NV8     NV8     SYS   SYS      SYS     SYS     SYS     PHB     PHB     PHB     PHB     90-179  1     N/A\r\nGPU6    NV8     NV8     NV8     NV8     NV8     NV8      X      NV8     SYS   SYS      SYS     SYS     SYS     PHB     PHB     PHB     PHB     90-179  1     N/A\r\nGPU7    NV8     NV8     NV8     NV8     NV8     NV8     NV8      X      SYS   SYS      SYS     SYS     SYS     PHB     PHB     PHB     PHB     90-179  1     N/A\r\nNIC0    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X    SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC1    PHB     PHB     PHB     PHB     SYS     SYS     SYS     SYS     SYS    X       PHB     PHB     PHB     SYS     SYS     SYS     SYS\r\nNIC2    PHB     PHB     PHB     PHB     SYS     SYS     SYS     SYS     SYS   PHB       X      PHB     PHB     SYS     SYS     SYS     SYS\r\nNIC3    PHB     PHB     PHB     PHB     SYS     SYS     SYS     SYS     SYS   PHB      PHB      X      PHB     SYS     SYS     SYS     SYS\r\nNIC4    PHB     PHB     PHB     PHB     SYS     SYS     SYS     SYS     SYS   PHB      PHB     PHB      X      SYS     SYS     SYS     SYS\r\nNIC5    SYS     SYS     SYS     SYS     PHB     PHB     PHB     PHB     SYS   SYS      SYS     SYS     SYS      X      PHB     PHB     PHB\r\nNIC6    SYS     SYS     SYS     SYS     PHB     PHB     PHB     PHB     SYS   SYS      SYS     SYS     SYS     PHB      X      PHB     PHB\r\nNIC7    SYS     SYS     SYS     SYS     PHB     PHB     PHB     PHB     SYS   SYS      SYS     SYS     SYS     PHB     PHB      X      PHB\r\nNIC8    SYS     SYS     SYS     SYS     PHB     PHB     PHB     PHB     SYS   SYS      SYS     SYS     SYS     PHB     PHB     PHB      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  NIC8: mlx5_8\r\n\r\n\r\nHypervisor vendor: KVM\r\nulimit soft: 1048576`",
    "labels": [],
    "state": "closed",
    "created_at": "2025-01-14T13:06:12+00:00",
    "closed_at": "2025-01-14T13:18:47+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2889/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2889"
  },
  {
    "number": 1751,
    "title": "[Bug] Cannot run `microsoft/Phi-3.5-mini-instruct`; Capture cuda graph failed",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen running `microsoft/Phi-3.5-mini-instruct` on 1x H100, sglang gives the following error. \r\n\r\n```\r\nException: Capture cuda graph failed: BatchDecodeWithPagedKVCachePyTorchWrapper::Plan(at::Tensor, at::Tensor, at::Tensor, at::Tensor, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, float, at::Tensor, at::Tensor)::<lambda()>::<lambda()>::<lambda()> failed to dispatch head_dim 96\r\n```\r\n\r\n\r\nFull terminal output:\r\n\r\n```\r\n$ python -m sglang.launch_server --model-path microsoft/Phi-3.5-mini-instruct\r\n2024-10-22 06:31:31.858362: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-10-22 06:31:31.874615: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-10-22 06:31:31.897134: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8463] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-10-22 06:31:31.903828: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-10-22 06:31:31.919069: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\r\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n[2024-10-22 06:31:36] server_args=ServerArgs(model_path='microsoft/Phi-3.5-mini-instruct', tokenizer_path='microsoft/Phi-3.5-mini-instruct', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, context_length=None, device='cuda', served_model_name='microsoft/Phi-3.5-mini-instruct', chat_template=None, is_embedding=False, host='127.0.0.1', port=30000, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, tp_size=1, stream_interval=1, random_seed=33262822, constrained_json_whitespace_pattern=None, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, api_key=None, file_storage_pth='SGLang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, lora_paths=None, max_loras_per_batch=8, attention_backend='flashinfer', sampling_backend='flashinfer', disable_flashinfer=False, disable_flashinfer_sampling=False, disable_radix_cache=False, disable_regex_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_penalizer=False, disable_nan_detection=False, enable_overlap_schedule=False, enable_mixed_chunk=False, enable_torch_compile=False, max_torch_compile_bs=32, torchao_config='', enable_p2p_check=False, triton_attention_reduce_in_fp32=False, num_continuous_decode_steps=1)\r\n/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\r\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\r\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n[2024-10-22 06:31:45 TP0] Init torch distributed begin.\r\n[2024-10-22 06:31:46 TP0] Load weight begin. avail mem=78.65 GB\r\nINFO 10-22 06:31:46 config.py:107] Replacing legacy 'type' key with 'rope_type'\r\n[2024-10-22 06:31:47 TP0] lm_eval is not installed, GPTQ may not be usable\r\nINFO 10-22 06:31:47 weight_utils.py:243] Using model weights format ['*.safetensors']\r\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.32it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.76it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.67it/s]\r\n\r\n[2024-10-22 06:31:49 TP0] Load weight end. type=Phi3ForCausalLM, dtype=torch.bfloat16, avail mem=71.38 GB\r\n[2024-10-22 06:31:49 TP0] Memory pool end. avail mem=8.38 GB\r\n[2024-10-22 06:31:49 TP0] Capture cuda graph begin. This can take up to several minutes.\r\n[2024-10-22 06:31:49 TP0] Traceback (most recent call last):\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/model_executor/cuda_graph_runner.py\", line 162, in __init__\r\n    self.capture()\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/model_executor/cuda_graph_runner.py\", line 212, in capture\r\n    ) = self.capture_one_batch_size(bs, forward)\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/model_executor/cuda_graph_runner.py\", line 233, in capture_one_batch_size\r\n    self.model_runner.attn_backend.init_forward_metadata_capture_cuda_graph(\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/attention/flashinfer_backend.py\", line 187, in init_forward_metadata_capture_cuda_graph\r\n    self.indices_updater_decode.update(\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/attention/flashinfer_backend.py\", line 352, in update_single_wrapper\r\n    self.call_begin_forward(\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/layers/attention/flashinfer_backend.py\", line 452, in call_begin_forward\r\n    wrapper.begin_forward(\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/flashinfer/decode.py\", line 543, in plan\r\n    self._wrapper.plan(\r\nRuntimeError: BatchDecodeWithPagedKVCachePyTorchWrapper::Plan(at::Tensor, at::Tensor, at::Tensor, at::Tensor, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, float, at::Tensor, at::Tensor)::<lambda()>::<lambda()>::<lambda()> failed to dispatch head_dim 96\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 1128, in run_scheduler_process\r\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 155, in __init__\r\n    self.tp_worker = TpWorkerClass(\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 55, in __init__\r\n    self.model_runner = ModelRunner(\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 166, in __init__\r\n    self.init_cuda_graphs()\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 557, in init_cuda_graphs\r\n    self.cuda_graph_runner = CudaGraphRunner(self)\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/sglang/srt/model_executor/cuda_graph_runner.py\", line 164, in __init__\r\n    raise Exception(\r\nException: Capture cuda graph failed: BatchDecodeWithPagedKVCachePyTorchWrapper::Plan(at::Tensor, at::Tensor, at::Tensor, at::Tensor, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, float, at::Tensor, at::Tensor)::<lambda()>::<lambda()>::<lambda()> failed to dispatch head_dim 96\r\nPossible solutions:\r\n1. disable cuda graph by --disable-cuda-graph\r\n2. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)\r\n3. disable torch compile by not using --enable-torch-compile\r\nOpen an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose \r\n\r\n\r\n/usr/lib/python3.10/multiprocessing/resource_tracker.py:104: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.\r\n  warnings.warn('resource_tracker: process died unexpectedly, '\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/multiprocessing/resource_tracker.py\", line 209, in main\r\n    cache[rtype].remove(name)\r\nKeyError: '/mp-8f_f5ecf'\r\nKilled\r\n```\n\n### Reproduction\n\n`python -m sglang.launch_server --model-path microsoft/Phi-3.5-mini-instruct`\n\n### Environment\n\n```\r\n2024-10-22 06:15:31.990529: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-10-22 06:15:32.007851: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-10-22 06:15:32.030825: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8463] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-10-22 06:15:32.037602: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-10-22 06:15:32.053158: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\r\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\nPython: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA H100 PCIe\r\nGPU 0 Compute Capability: 9.0\r\nCUDA_HOME: /usr\r\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\r\nCUDA Driver Version: 550.90.12\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.3.4.post1\r\nflashinfer: 0.1.6+cu124torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.45.2\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.26.4\r\naiohttp: 3.10.10\r\nfastapi: 0.115.2\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.26.1\r\ninteregular: 0.3.3\r\npackaging: 21.3\r\nPIL: 10.4.0\r\npsutil: 5.9.0\r\npydantic: 2.9.2\r\nuvicorn: 0.32.0\r\nuvloop: 0.21.0\r\nzmq: 22.3.0\r\nvllm: 0.6.3.post1\r\nmultipart: 0.0.12\r\nopenai: 1.46.0\r\nanthropic: 0.31.1\r\nNVIDIA Topology: \r\n        GPU0    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PHB     0-25    0               N/A\r\nNIC0    PHB      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n\r\n\r\nHypervisor vendor: KVM\r\nulimit soft: 1048576\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-10-22T06:33:41+00:00",
    "closed_at": "2024-10-22T07:01:39+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1751/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1751"
  },
  {
    "number": 465,
    "title": "CUDA out of memory for H100 80GB for lmms-lab/llama3-llava-next-8b",
    "body": "Installed via pip in python 3.10 as readme says, then ran:\r\n```\r\nexport CUDA_VISIBLE_DEVICES=1\r\npython -m sglang.launch_server --model-path lmms-lab/llama3-llava-next-8b --tokenizer-path lmms-lab/llama3-llava-next-8b-tokenizer --port=30000 --host=\"0.0.0.0\" --tp-size=1 --api-key='62224bfb-c832-4452-81e7-8a4bdabbe164'  --random-seed=1234 --context-length=8192\r\n```\r\n\r\nnothing is on GPU=1, only GPU=0 is filled.\r\n\r\nAlways hit very early on startup, model not even loaded yet:\r\n```\r\n  File \"/home/ubuntu/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/models/llama2.py\", line 39, in __init__\r\n    self.gate_up_proj = MergedColumnParallelLinear(\r\n  File \"/home/ubuntu/miniconda3/envs/sglang/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 333, in __init__\r\n    super().__init__(input_size, sum(output_sizes), bias, gather_output,\r\n  File \"/home/ubuntu/miniconda3/envs/sglang/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 236, in __init__\r\n    self.quant_method.create_weights(self,\r\n  File \"/home/ubuntu/miniconda3/envs/sglang/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 81, in create_weights\r\n    weight = Parameter(torch.empty(output_size_per_partition,\r\n  File \"/home/ubuntu/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\r\n    return func(*args, **kwargs)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU \r\n\r\nInitialization failed. detoken_init_state: init ok\r\n```\r\n\r\nI can't believe >80GB needed for this model.\r\n\r\nUsing CVD \"1,2\" and -tp-size=2 starts and downloads the model, but seems to get stuck and never gets done loading weight for rank 0.\r\n\r\n```\r\n/home/ubuntu/miniconda3/envs/sglang/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\n/home/ubuntu/miniconda3/envs/sglang/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:100: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\r\n  warnings.warn(\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nserver started on [0.0.0.0]:10004\r\nserver started on [0.0.0.0]:10005\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\naccepted ('127.0.0.1', 41688) with fd 36\r\nwelcome ('127.0.0.1', 41688)\r\naccepted ('127.0.0.1', 48486) with fd 32\r\nwelcome ('127.0.0.1', 48486)\r\n/home/ubuntu/miniconda3/envs/sglang/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:140: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.\r\n  warnings.warn(\r\n/home/ubuntu/miniconda3/envs/sglang/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:140: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.\r\n  warnings.warn(\r\nNCCL version 2.20.5+cuda12.4\r\nRank 1: load weight begin.\r\nRank 0: load weight begin.\r\nconfig.json: 100%|???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????| 4.76k/4.76k [00:00<00:00, 41.2MB/s]\r\npytorch_model.bin: 100%|??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????| 1.71G/1.71G [00:05<00:00, 319MB/s]\r\nUsing model weights format ['*.safetensors']\r\nmodel-00001-of-00004.safetensors: 100%|???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????| 4.98G/4.98G [00:07<00:00, 636MB/s]\r\nmodel-00002-of-00004.safetensors: 100%|???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????| 5.00G/5.00G [00:06<00:00, 798MB/s]\r\nmodel-00003-of-00004.safetensors: 100%|???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????| 4.92G/4.92G [00:06<00:00, 750MB/s]\r\nmodel-00004-of-00004.safetensors: 100%|???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????| 1.82G/1.82G [00:04<00:00, 383MB/s]\r\nUsing model weights format ['*.safetensors']\r\nRank 1: load weight end.\r\n\r\n\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-05-23T20:35:06+00:00",
    "closed_at": "2024-05-23T20:42:44+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/465/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/465"
  },
  {
    "number": 1358,
    "title": "[Bug] it seems memory leak in sglang when longtime serving",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n128134 memory blocks: 5104.2 KiB\r\nTraceback:\r\n  File \"/usr/local/lib/python3.10/dist-packages/zmq/_future.py\", line 374\r\n    loaded = load(buf)\n\n### Reproduction\n\ntracemalloc in detokenizer_manager.py\n\n### Environment\n\nllama2-13B A800*1",
    "labels": [],
    "state": "closed",
    "created_at": "2024-09-09T12:12:45+00:00",
    "closed_at": "2024-09-09T12:14:07+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1358/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1358"
  },
  {
    "number": 3470,
    "title": "[Bug] sglang with 2nodes, failed with `RuntimeError: CUDA error: invalid device ordinal`",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\ntl;dr\n\ndistributed sglang with ` --dist-init-addr` failed:` RuntimeError: CUDA error: invalid device ordinal`\n```\npython3 -m sglang.launch_server --model-path /model --tp 16 --dist-init-addr  $podIP:20000 --nnodes 2 --node-rank 0 --trust-remote-code\n\n```\n\n\n\n\n### Reproduction\n\n\n----------\n\n(1)run single sglang , all was  well \n\n```\n              python3 -m sglang.launch_server   --model-path /model   --host 0.0.0.0   --port 30000\n```\n\n\n\n------------------\n\n# on the same machine/container. sglang with tp failed:\n\nrun below command: \n```\npython3 -m sglang.launch_server --model-path /model --tp 16 --dist-init-addr  $podIP:20000 --nnodes 2 --node-rank 0 --trust-remote-code\n\n```\n\n```\npython3 -m sglang.launch_server --model-path /model --tp 16 --dist-init-addr $podIP:20000 --nnodes 2 --node-rank 0 --trust-remote-code\nINFO 02-10 21:15:21 __init__.py:190] Automatically detected platform cuda.\n[2025-02-10 21:15:28] server_args=ServerArgs(model_path='/model', tokenizer_path='/model', tokenizer_mode='auto', load_format='auto', trust_remote_code=True, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, quantization=None, context_length=None, device='cuda', served_model_name='/model', chat_template=None, is_embedding=False, revision=None, skip_tokenizer_init=False, host='127.0.0.1', port=30000, mem_fraction_static=0.79, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=16, stream_interval=1, stream_output=False, random_seed=394623495, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='sglang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr='10.17.64.15:20000', nnodes=2, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_draft_model_path=None, speculative_algorithm=None, speculative_num_steps=5, speculative_num_draft_tokens=64, speculative_eagle_topk=8, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False)\nINFO 02-10 21:15:34 __init__.py:190] Automatically detected platform cuda.\nINFO 02-10 21:15:34 __init__.py:190] Automatically detected platform cuda.\nINFO 02-10 21:15:34 __init__.py:190] Automatically detected platform cuda.\nINFO 02-10 21:15:34 __init__.py:190] Automatically detected platform cuda.\nINFO 02-10 21:15:34 __init__.py:190] Automatically detected platform cuda.\nINFO 02-10 21:15:34 __init__.py:190] Automatically detected platform cuda.\nINFO 02-10 21:15:34 __init__.py:190] Automatically detected platform cuda.\nINFO 02-10 21:15:34 __init__.py:190] Automatically detected platform cuda.\nINFO 02-10 21:15:34 __init__.py:190] Automatically detected platform cuda.\n[2025-02-10 21:15:43 TP6] Init torch distributed begin.\n[2025-02-10 21:15:43 TP6] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1787, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 240, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 68, in __init__\n    self.model_runner = ModelRunner(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 178, in __init__\n    min_per_gpu_memory = self.init_torch_distributed()\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 223, in init_torch_distributed\n    torch.get_device_module(self.device).set_device(self.gpu_id)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 478, in set_device\n    torch._C._cuda_setDevice(device)\nRuntimeError: CUDA error: invalid device ordinal\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\n[2025-02-10 21:15:43] Received sigquit from a child proces. It usually means the child failed.\n[2025-02-10 21:15:43 TP0] Init torch distributed begin.\n```\n\n\n### Environment\n\n----\n\n# Env Info\n\n\n1. `python3 -m sglang.check_env`    result as below\n\n```\n python3 -m sglang.check_env\nINFO 02-10 21:15:02 __init__.py:190] Automatically detected platform cuda.\nPython: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0]\nCUDA available: True\nGPU 0: NVIDIA RTX A6000\nGPU 0 Compute Capability: 8.6\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.230.02\nPyTorch: 2.5.1+cu124\nsgl_kernel: 0.0.3.post3\nflashinfer: 0.2.0.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.61.1\ntiktoken: 0.8.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology:\n\tGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \t0-23,48-71\t0\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 65535\n```\n\n\n\n\n2. HostCUDA:12.5 as below\n```\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.5     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA RTX A6000               On  | 00000000:4B:00.0 Off |                  Off |\n+-----------------------------------------+----------------------+----------------------+\n```\n\n\n\n\n3. using docker images :    lmsysorg/sglang:v0.4.2.post4-cu125 (the actual cuda version inside container is 12.4)\n\n\nusing docker images :    lmsysorg/sglang:v0.4.2.post4-cu125 (the actual cuda version inside container is 12.4)\n",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-10T13:19:04+00:00",
    "closed_at": "2025-02-10T13:21:29+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3470/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3470"
  },
  {
    "number": 5650,
    "title": "[Bug]  PD h20 decode node start error because of cuda graph....",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nwhen i  using th  fzyzcjy's branch  \uff0c feat/dev_branch , decode starts error . \n<img width=\"1097\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/dac66897-ef96-4cdd-ae2c-9b911fa0c074\" />\n\n### Reproduction\n\n2 manchine prefill node  4machine decoe node..\n fzyzcjy's branch  \uff0c feat/dev_branch \n\n### Environment\n\n2 manchine prefill node  4machine decoe node..\n fzyzcjy's branch  \uff0c feat/dev_branch",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-23T01:46:45+00:00",
    "closed_at": "2025-04-23T02:14:53+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5650/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5650"
  },
  {
    "number": 3394,
    "title": "[Feature] Not compatible with langchain, Seems sglang Message Class's fields not provided by langchain (via openai) app",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n![Image](https://github.com/user-attachments/assets/e5dff345-0f5b-444f-9aa9-6783b37aa43c)\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-08T06:45:16+00:00",
    "closed_at": "2025-02-08T07:21:32+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3394/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3394"
  },
  {
    "number": 930,
    "title": "[Bug] \u6211\u60f3\u95ee\u4e00\u4e0b\u652f\u6301qwen1.5-14B\u6a21\u578b\u5417\uff1f",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n\n### Describe the bug\n\n[Bug] \u6211\u60f3\u95ee\u4e00\u4e0b\u652f\u6301qwen1.5-14B\u6a21\u578b\u5417\uff1f\n\n### Reproduction\n\n[Bug] \u6211\u60f3\u95ee\u4e00\u4e0b\u652f\u6301qwen1.5-14B\u6a21\u578b\u5417\uff1f\n\n### Environment\n\n```Shell\n[Bug] \u6211\u60f3\u95ee\u4e00\u4e0b\u652f\u6301qwen1.5-14B\u6a21\u578b\u5417\uff1f\n```\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-05T08:49:45+00:00",
    "closed_at": "2024-08-05T09:03:52+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/930/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/930"
  },
  {
    "number": 3283,
    "title": "[Bug] deepseek v3 2 nodes h100 segmentation fault",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nhello.\nI run on 2 nodes of 8 x h100 using   lmsysorg/sglang:v0.4.2.post1-cu125 image\n\n```\npython3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-R1 --tp 16 --dist-init-addr 172.16.1.68:5000 --nnodes 2 --node-rank 1 --trust-remote-code --quantization fp8 --kv-cache-dtype fp8_e5m2\n```\nI start a benchmark\n```\n python3 -m sglang.bench_serving --backend sglang --dataset-name random --random-range-ratio 1 --num-prompt 2400 --request-rate 8 --random-input 1024 --random-output 1024 --output-file deepseek_v3_8xh200_BF16_on\nline_output.jsonl\n```\nsglang on one node crashes\n```\n[2025-02-03 22:28:08 TP12] Decode out of memory happened. #retracted_reqs: 3, #new_token_ratio: 0.0980 -> 1.0000\nFatal Python error: Segmentation fault\n\nThread 0x00007f4b58a9b700 (most recent call first):\n  File \"/usr/lib/python3.10/threading.py\", line 324 in wait\n  File \"/usr/lib/python3.10/threading.py\", line 607 in wait\n  File \"/usr/local/lib/python3.10/dist-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f3b2dfff700 (most recent call first):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 462 in watchdog_thread\n  File \"/usr/lib/python3.10/threading.py\", line 953 in run\n  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f3b2d7fe700 (most recent call first):\n  File \"/usr/local/lib/python3.10/dist-packages/sgl_kernel/ops/utils.py\", line 19 in _get_cache_buf\n  File \"/usr/local/lib/python3.10/dist-packages/sgl_kernel/ops/__init__.py\", line 286 in bmm_fp8\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 597 in forward_absorb\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 516 in forward\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747 in _call_impl\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736 in _wrapped_call_impl\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 757 in forward\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747 in _call_impl\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736 in _wrapped_call_impl\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 819 in forward\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747 in _call_impl\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736 in _wrapped_call_impl\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 858 in forward\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 742 in forward_decode\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 783 in forward\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 164 in forward_batch_generation\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 140 in forward_thread_func_\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 109 in forward_thread_func\n  File \"/usr/lib/python3.10/threading.py\", line 953 in run\n  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f4ae3fc7700 (most recent call first):\n  File \"/usr/lib/python3.10/threading.py\", line 324 in wait\n  File \"/usr/lib/python3.10/threading.py\", line 607 in wait\n  File \"/usr/local/lib/python3.10/dist-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f4d38484700 (most recent call first):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 47 in _recv_msg\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 153 in _read_thread\n  File \"/usr/lib/python3.10/threading.py\", line 953 in run\n  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f50d99f1740 (most recent call first):\n  File \"/usr/lib/python3.10/threading.py\", line 320 in wait\n  File \"/usr/lib/python3.10/threading.py\", line 607 in wait\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 167 in resolve_batch_result\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\"\n```\nIf I run sglang without --quantization fp8 --kv-cache-dtype fp8_e5m2 . it doesn't crash\n\n### Reproduction\n\n  s\n\n### Environment\n\n| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |\n",
    "labels": [
      "help wanted",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-04T06:43:27+00:00",
    "closed_at": "2025-02-04T07:40:25+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3283/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3283"
  },
  {
    "number": 7261,
    "title": "[OAI Server Refactor] [ChatCompletions & Completions] Add UTs for Tool Call and Reasoning Text Handling",
    "body": "**Points**: 1-2 days\n\n**Description**: Current the tool call handling and reasoning text logic in [`serving_chat.py`](https://github.com/sgl-project/sglang/blob/70c471a868bf505fadbfe0a041e7637a91db0365/python/sglang/srt/entrypoints/openai/serving_chat.py)\n\n**Deliverables:**\n- [ ] Complete tasks below",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-17T04:29:31+00:00",
    "closed_at": "2025-06-17T04:36:53+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7261/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7261"
  },
  {
    "number": 986,
    "title": "Support most : Batch, Chat, Completions, Embedding",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-08T05:58:18+00:00",
    "closed_at": "2024-08-08T05:58:32+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/986/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/986"
  },
  {
    "number": 1160,
    "title": "[Bug] Gemma-2-9b-it produces garbage output",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [X] 2. The bug has not been fixed in the latest version.\r\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [X] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nGemma-2-9b-it: \"<unused99><unused99><unused99><unused99><unused99>...\"\r\n\r\n\r\n### Reproduction\r\n\r\nI just pulled the latest Docker image and deployed Gemma-2-9b-it [AWQ version](https://huggingface.co/nihaomur/gemma-2-9b-it-AWQ). Here is how I deployed the model\r\n\r\n```shell\r\nHF_TOKEN=<my-token>\r\nMODEL=nihaomur/gemma-2-9b-it-AWQ\r\nSERVED_MODEL_NAME=gpt-4o\r\nMFS=0.6\r\nTP=2\r\nAPI_KEY=<my-key>\r\n\r\ndocker run -d --gpus '\"device=2,3\"' \\\r\n    -p 8002:8002 \\\r\n    --rm \\\r\n    --network=<network-name> \\\r\n    --name sglang-server \\\r\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\r\n    --env HF_TOKEN=$HF_TOKEN \\\r\n    --ipc=host \\\r\n    lmsysorg/sglang:latest \\\r\n        python3 -m sglang.launch_server \\\r\n        --model-path $MODEL \\\r\n        --host 0.0.0.0 \\\r\n        --port 8002 \\\r\n        --served-model-name $SERVED_MODEL_NAME \\\r\n        --mem-fraction-static $MFS \\\r\n        --tensor-parallel-size $TP \\\r\n        --api-key $API_KEY \\\r\n        --quantization awq_marlin \\\r\n        --efficient-weight-load \\\r\n        --enable-p2p-check\r\n```\r\n\r\nHere is my prompt\r\n```python\r\nprompt = \"\"\"### Task:\r\nYou will be given a question and several context files, which may contain relevant information. Your objectives are as follows:\r\n\r\n1. **Extract Supported Evidence**: For each file name provided, identify and extract short, concise phrases that serve as strong evidence supporting the question. The evidence must be:\r\n   - Relevant: Directly related to the question.\r\n   - Strongly Related: Clearly substantiates or informs the question.\r\n\r\n2. **Evaluate Relevance**: Gather all extracted evidence and evaluate how well it supports the question. Provide your thoughts on the relevance and strength of the evidence as it relates to the question.\r\n\r\n3. **Formulate the Final Answer**:\r\n   - If the collected evidence collectively provides a clear and accurate answer to the question, present the answer in a well-informed and concise manner.\r\n   - If the evidence is insufficient or inconclusive, explicitly state that a definitive answer could not be determined. Avoid making unsupported assumptions.\r\n   \r\n4. **Output Format**: Return your findings as a JSON object adhering to the following schema:\r\n\r\n```json\r\n{{\r\n  \"supported_evidence\": [{{\"filename\": \"<filename>\", \"text\": [\"<evidence>\", \"<evidence>\"]}}, ...],\r\n  \"thought\": \"<your evaluation of relevance>\",\r\n  \"final_answer\": \"<your final answer or statement of inconclusiveness>\"\r\n}}```\r\n\r\n### Here is your input\r\nQuestion: {question}\r\nReports: {contexts}\"\"\"\r\n```\r\n\r\nMy Pydantic model\r\n```python\r\nclass Evidence(BaseModel):\r\n    filename: str\r\n    text: list[str]\r\n\r\nclass Output(BaseModel):\r\n    supported_evidence: list[Evidence]\r\n    thought: str\r\n    final_answer: str\r\n```\r\n\r\nHow I called the server\r\n\r\n```python\r\ncontent = await self.client(\r\n            messages=Messages(\r\n                messages=[\r\n                    {\"role\": \"user\", \"content\": prompt}]\r\n            ),\r\n            extra_body={\r\n                \"regex\": build_regex_from_object(Output),\r\n                \"repetition_penalty\": 1.2,\r\n            **self.params\r\n        )\r\n```\r\n\r\n[Llama-3.1-8B-Instruct](https://huggingface.co/hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4) worked with this prompt while Gemma-2 produced garbage outputs (Note: with repetition_penalty = 1.0, Llama-3.1 produced duplicated content)\r\n\r\n### Environment\r\n\r\nPython: 3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0]\r\nCUDA available: True\r\nGPU 0,1: NVIDIA A10G\r\nGPU 0,1 Compute Capability: 8.6\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\r\nCUDA Driver Version: 550.90.07\r\nPyTorch: 2.4.0+cu121\r\nflashinfer: 0.1.5+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.44.0\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.26.4\r\naiohttp: 3.10.3\r\nfastapi: 0.112.1\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.5\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.6\r\nuvloop: 0.20.0\r\nzmq: 26.1.0\r\nvllm: 0.5.4\r\nmultipart: 0.0.9\r\nopenai: 1.40.8\r\nanthropic: 0.34.0\r\nNVIDIA Topology: \r\n        GPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PHB     0-47    0               N/A\r\nGPU1    PHB      X      0-47    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nulimit soft: 1048576",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-20T07:18:26+00:00",
    "closed_at": "2024-08-20T07:56:50+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1160/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1160"
  },
  {
    "number": 1130,
    "title": "[Bug] llava not working after pull latest code",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n  File \"/u/miniconda3/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 441, in forward_prefill_batch\r\n    batch.prepare_for_extend(self.model_config.vocab_size)\r\nTypeError: ScheduleBatch.prepare_for_extend() missing 1 required positional argument: 'int_token_logit_bias'\r\n\r\nKilled\n\n### Reproduction\n\npython -m sglang.launch_server --model-path liuhaotian/llava-v1.5-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --chat-template vicuna_v1.1 --port 30000\n\n### Environment\n\nPython: 3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA A40\r\nGPU 0 Compute Capability: 8.6\r\nCUDA_HOME: /sw/spack/deltas11-2023-03/apps/linux-rhel8-zen3/gcc-11.4.0/cuda-11.8.0-vfixfmc\r\nNVCC: Cuda compilation tools, release 11.8, V11.8.89\r\nCUDA Driver Version: 535.183.01\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.2.12\r\nflashinfer: 0.1.5+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.44.0\r\nrequests: 2.32.3\r\ntqdm: 4.66.4\r\nnumpy: 1.26.4\r\naiohttp: 3.9.5\r\nfastapi: 0.112.0\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.23.3\r\ninteregular: 0.3.3\r\npackaging: 23.0\r\nPIL: 10.4.0\r\npsutil: 5.9.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.5\r\nuvloop: 0.19.0\r\nzmq: 25.1.2\r\nvllm: 0.5.4\r\nmultipart: 0.0.9\r\nopenai: 1.40.1\r\nanthropic: 0.32.0\r\nNVIDIA Topology: \r\n        GPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-15    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nulimit soft: 131072",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-16T19:45:13+00:00",
    "closed_at": "2024-08-16T20:07:41+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1130/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1130"
  },
  {
    "number": 3609,
    "title": "[Feature] Can it support Huawei Ascend 910B backend?",
    "body": "### Motivation\n\nIt seems that SGLang currently does not support deploy LLM-infer models such as DeepSeek-V3/R1 on Huawei 910B.\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-16T12:19:40+00:00",
    "closed_at": "2025-02-16T12:45:57+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3609/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3609"
  },
  {
    "number": 4035,
    "title": "Development Roadmap (2025 H1)",
    "body": "Here is the development roadmap for 2025 Q1 and Q2. Contributions and feedback are welcome ([**Join Bi-weekly Development Meeting**](https://docs.google.com/document/d/1xEow4eIM152xNcRxqZz9VEcOiTQo8-CEuuQ5qTmkt-E/edit?tab=t.0#heading=h.ito5nvp7oasg)). Previous 2024 Q4 roadmap can be found in #1487.\n\n### DeepSeek R1 optimization\n@zhyncs @ispobock \nTBD\n\n## Performance\n- [ ] Support speculative decoding\n  - Eagle Optimization #3822 \n  - Reference-based. #3269 \n  - Align with the speed of grok\n- [ ] P/D Disaggregation\n  - Bump internal codes\n  - Mooncake Integration\n\n## Parallelism\n- [ ] Support sequence parallelism #1436. Related [paper](https://www.arxiv.org/pdf/2411.01783)\n- [ ] Support pipeline parallelism.\n- [ ] Optimize expert parallelism + data parallelism for DeepSeekmodels.\n- [ ] Optimize expert parallelism for Qwen Models.\n- [ ] Overlap communication in tensor parallelsim. @zhuohaol @fzyzcjy \n\n## Hardware Optimizations\n- [ ] AMD optimizations. @HaiShaw @yiakwy-xpu-ml-framework-team \n- [ ] Intel XPU optimization. @shanyu-sys \n\n## Model Coverage\n- [ ] Multi-modal models\n  - merge all the PRs from @mickqian @yizhang2077 \n  - support streaming models @yiranyyu \n  - VLM optimization @Lyken17\n- [ ] Embed models\n  -  encoder models @yichuan520030910320 \n\n## New features\n- [ ] Performance optimizations for multi-LoRA serving @Fridge003 \n- [ ] RLHF support with veRL team @zhaochenyang20 \n- [ ] GRPO of trl @jhinpan \n- [ ] Optimize funciton calling and constraint decoding @minleminzui \n\n## Quantization \n- [ ] unsloth model support @guapisolo @XueyingJia\n\n## Server API\n- [ ] Support directly taking embedding as inputs. #745\n- [x] Add APIs for using the inference engine in a single script without launching a separate server. See also [examples](https://docs.vllm.ai/en/latest/getting_started/examples/offline_inference.html).\n  - #1567\n- [ ] Support endpoint other than OpenAI (Anthropic, Mistral) in the language frontend.\n\n\n\n## Observability\n- [ ] Open-to-use Grafana / Prometheus @PopSoda2002 @ziliangpeng\n\n## Others\n- [ ] VLM refactor @mickqian \n- [ ] VLM RLHF @yiranyyu @shuaills ",
    "labels": [
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-03-03T18:26:58+00:00",
    "closed_at": "2025-03-03T19:11:15+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4035/reactions",
      "total_count": 4,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 4,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4035"
  },
  {
    "number": 7411,
    "title": "[Bug] Decode OOM due to wrong new page estimation",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nRelated\n#7328 \n#7410 \n\n### Reproduction\n\n```\npython3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --host 0.0.0.0 --port 40000 --mem-fraction-static=0.5 --page=32\n```\n```\ngit switch xiezhq-dev\ncd  benchmark/hicache\npython bench_multiturn.py --port 40000\n```\n\n### Environment\n\n- ",
    "labels": [
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-06-21T07:25:54+00:00",
    "closed_at": "2025-06-21T07:35:27+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7411/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7411"
  },
  {
    "number": 7870,
    "title": "[Bug] SGLang router/server is unkillable",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI cannot kill the router after starting it up with ctrl + c or ctrl + d. I see that the timeout is 300 seconds but how I configure it to be shorter/longer?\n```text\n^C^C^C2025-07-08 19:58:04  INFO sglang_router_rs::router: src/router.rs:354: Worker is not ready yet\n2025-07-08 19:58:04  INFO sglang_router_rs::router: src/router.rs:354: Worker is not ready yet\n2025-07-08 19:58:04  INFO sglang_router_rs::router: src/router.rs:354: Worker is not ready yet\n2025-07-08 19:58:04  INFO sglang_router_rs::router: src/router.rs:354: Worker is not ready yet\n2025-07-08 19:58:04  INFO sglang_router_rs::router: src/router.rs:365: Initializing workers:\n2025-07-08 19:58:04  INFO sglang_router_rs::router: src/router.rs:367:   http://0.0.0.0:31000 - Worker is not ready yet\n2025-07-08 19:58:04  INFO sglang_router_rs::router: src/router.rs:367:   http://0.0.0.0:31703 - Worker is not ready yet\n2025-07-08 19:58:04  INFO sglang_router_rs::router: src/router.rs:367:   http://0.0.0.0:32035 - Worker is not ready yet\n2025-07-08 19:58:04  INFO sglang_router_rs::router: src/router.rs:367:   http://0.0.0.0:32249 - Worker is not ready yet\n```\n\n### Reproduction\n\n```python\npython -m sglang_router.launch_server --model-path meta-llama/Llama-3.1-70B-Instruct --dp-size 4 --host 0.0.0.0 --port 30080\n```\n\n### Environment\n\n```bash\nuv venv --python 3.12\nuv pip install \"sglang[all]>=0.4.9\"\nuv pip install sglang-router\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-07-08T20:02:06+00:00",
    "closed_at": "2025-07-08T20:03:46+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7870/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7870"
  },
  {
    "number": 5668,
    "title": "[Bug] After updating from 0.4.5.post2 to 0.4.5.post3, the following error is reported: AttributeError: '_OpNamespace' 'sgl_kernel' object has no attribute 'awq_dequantize'",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n[2025-04-23 16:14:34 TP0] Attention backend not set. Use triton backend by default.\n[2025-04-23 16:14:34 TP0] Init torch distributed begin.\n[W423 16:14:34.122823038 HIPAllocatorConfig.h:29] Warning: expandable_segments not supported on this platform (function operator())\n[2025-04-23 16:14:35 TP0] Init torch distributed ends. mem usage=0.00 GB\n[2025-04-23 16:14:35 TP0] Load weight begin. avail mem=17.88 GB\n[2025-04-23 16:14:35 TP0] sgl-kernel is not available on Non-NV platforms. Fallback to other kernel libraries.\n[2025-04-23 16:14:35 TP0] sgl-kernel is not available on Non-NV platforms. Fallback to other kernel libraries.\n[2025-04-23 16:14:35 TP0] The following error message 'operation scheduled before its operands' can be ignored.\n/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/utils/_device.py:104: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:29.)\n  return func(*args, **kwargs)\nLoading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  33% Completed | 1/3 [00:04<00:09,  4.56s/it]\nLoading safetensors checkpoint shards:  67% Completed | 2/3 [00:09<00:04,  4.66s/it]\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:11<00:00,  3.46s/it]\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:11<00:00,  3.78s/it]\n\n[2025-04-23 16:14:47 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.float16, avail mem=7.96 GB, mem usage=9.92 GB.\n[2025-04-23 16:14:47 TP0] KV Cache is allocated. #tokens: 13200, K size: 1.21 GB, V size: 1.21 GB\n[2025-04-23 16:14:47 TP0] Memory pool end. avail mem=4.70 GB\n[2025-04-23 16:14:47 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=4.70 GB\nCapturing batches (avail_mem=4.70 GB):   0%|                                                      | 0/4 [00:00<?, ?it/s]\n[2025-04-23 16:14:48 TP0] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/usr/local/sglang/python/sglang/srt/managers/scheduler.py\", line 2001, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/usr/local/sglang/python/sglang/srt/managers/scheduler.py\", line 261, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/usr/local/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/usr/local/sglang/python/sglang/srt/managers/tp_worker.py\", line 75, in __init__\n    self.model_runner = ModelRunner(\n  File \"/usr/local/sglang/python/sglang/srt/model_executor/model_runner.py\", line 181, in __init__\n    self.initialize(min_per_gpu_memory)\n  File \"/usr/local/sglang/python/sglang/srt/model_executor/model_runner.py\", line 219, in initialize\n    self.init_cuda_graphs()\n  File \"/usr/local/sglang/python/sglang/srt/model_executor/model_runner.py\", line 980, in init_cuda_graphs\n    self.cuda_graph_runner = CudaGraphRunner(self)\n  File \"/usr/local/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 276, in __init__\n    self.capture()\n  File \"/usr/local/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 360, in capture\n    ) = self.capture_one_batch_size(bs, forward)\n  File \"/usr/local/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 452, in capture_one_batch_size\n    run_once()\n  File \"/usr/local/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 445, in run_once\n    logits_output = forward(input_ids, forward_batch.positions, forward_batch)\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/sglang/python/sglang/srt/models/qwen2.py\", line 383, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/sglang/python/sglang/srt/models/qwen2.py\", line 291, in forward\n    hidden_states, residual = layer(\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/sglang/python/sglang/srt/models/qwen2.py\", line 224, in forward\n    hidden_states = self.self_attn(\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/sglang/python/sglang/srt/models/qwen2.py\", line 167, in forward\n    qkv, _ = self.qkv_proj(hidden_states)\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/sglang/python/sglang/srt/layers/linear.py\", line 445, in forward\n    output_parallel = self.quant_method.apply(self, input_, bias)\n  File \"/usr/local/sglang/python/sglang/srt/layers/quantization/awq.py\", line 195, in apply\n    out = awq_dequantize(qweight, scales, qzeros)\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/sgl_kernel-0.0.9.post2-py3.10-linux-x86_64.egg/sgl_kernel/gemm.py\", line 10, in awq_dequantize\n    return torch.ops.sgl_kernel.awq_dequantize.default(qweight, scales, qzeros)\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/torch/_ops.py\", line 1232, in __getattr__\n    raise AttributeError(\nAttributeError: '_OpNamespace' 'sgl_kernel' object has no attribute 'awq_dequantize'\n\n### Reproduction\n\nqwen2.5-instruct  14B int4 AWQ quantization\n\n### Environment\n\nPython: 3.10.14 (main, May 6 2024, 19:42:50) [GCC 11.2.0]\nROCM available: True\nGPU 0: AMD Radeon RX 7900 XT\nGPU 0 Compute Capability: 11.0\nROCM_HOME: /opt/rocm\nHIPCC: HIP version: 6.4.43482-0f2d60242\nROCM Driver Version:\nPyTorch: 2.6.0+rocm6.4.0.git2fb0ac2b\nsglang: 0.4.5.post3\nsgl_kernel: 0.0.9.post2\nflashinfer: Module Not Found\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.11.0.dev20250418+rocm6.3\nnumpy: 1.26.4\naiohttp: 3.11.14\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.24.0\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 1.2.1\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.8.5.dev134+gd6da9322c.d20250422.rocm640\nxgrammar: 0.1.18\nopenai: 1.68.2\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.63.14\ndecord: 0.6.0\nAMD Topology:\n\nHypervisor vendor: Microsoft\nulimit soft: 1024",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-23T08:29:35+00:00",
    "closed_at": "2025-04-23T08:36:34+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5668/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5668"
  },
  {
    "number": 946,
    "title": "[Feature] Post link to paper in the README.md",
    "body": "### Motivation\r\n\r\nNew users struggle to form a mental model of how sglang works, for example understanding how many rounds of interaction take place for a structured prompt, will the chat history be re-sent to the model and have to pay for the prefix again, and what works and what doesn't work with OpenAI vs local server?\r\n\r\nI suggest you post the link to the paper \"Efficiently Programming Large Language Models using SGLang\" in the README.md as it covers most questions\r\n\r\n### Related resources\r\n\r\nhttps://arxiv.org/pdf/2312.07104v1",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-06T07:32:34+00:00",
    "closed_at": "2024-08-06T07:48:48+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/946/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/946"
  }
]
[
  {
    "number": 4188,
    "title": "[Bug] Server crashes with CUDA errors during EAGLE Speculative Decoding under high concurrency",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen running SGLang with EAGLE speculative decoding under high concurrency load, the server crashes with CUDA errors. The primary error is \"CUDA error: an illegal instruction was encountered\", occurring in two different scenarios:\n\nWhen attempting to move GPU tensors to CPU in the KV cache memory management system.\nDuring tensor operations in the EAGLE verification process when handling sequence position information.\nThese errors only manifest under high concurrency testing with the benchmark tool, suggesting race conditions or resource contention issues when handling CUDA tensors across devices.\n\n### Reproduction\n\n```\n# Start the server with EAGLE speculative decoding\npython3 -m sglang.launch_server --model meta-llama/Llama-2-7b-chat-hf \\\n    --speculative-algorithm EAGLE \\\n    --speculative-draft-model-path lmsys/sglang-EAGLE-llama2-chat-7B \\\n    --speculative-num-steps 5 \\\n    --speculative-eagle-topk 8 \\\n    --speculative-num-draft-tokens 64\n\n# Run benchmark with high concurrency that triggers the error\npython3 -m sglang.bench_serving \\\n    --backend sglang-oai \\\n    --dataset-name sharegpt \\\n    --num-prompts 100 --max-concurrency 256 \\\n    --request-rate-range 1,4,16,64,256 --multi \\\n    --port 30022\n```\n\nError messages:\n```\n[2025-03-07 18:14:03 TP0] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/home/ubuntu/sglang/python/sglang/srt/managers/scheduler.py\", line 1798, in run_scheduler_process\n    scheduler.event_loop_normal()\n  File \"/home/ubuntu/miniconda3/envs/sglang-eagle/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sglang/python/sglang/srt/managers/scheduler.py\", line 478, in event_loop_normal\n    result = self.run_batch(batch)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sglang/python/sglang/srt/managers/scheduler.py\", line 1088, in run_batch\n    ) = self.draft_worker.forward_batch_speculative_generation(batch)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 105, in forward_batch_speculative_generation\n    ) = self.verify(batch, spec_info)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sglang/python/sglang/srt/speculative/eagle_worker.py\", line 252, in verify\n    res = spec_info.verify(batch, logits_output)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sglang/python/sglang/srt/speculative/eagle_utils.py\", line 333, in verify\n    batch.token_to_kv_pool.free(mem_need_free_idx)\n  File \"/home/ubuntu/sglang/python/sglang/srt/mem_cache/memory_pool.py\", line 132, in free\n    self.free_slots = torch.concat((self.free_slots, free_index.cpu()))\n                                                     ^^^^^^^^^^^^^^^^\nRuntimeError: CUDA error: an illegal instruction was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\n[2025-03-07 18:14:03] Received sigquit from a child proces. It usually means the child failed.\n```\n\n### Environment\n\nPython: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr\nNVCC: Cuda compilation tools, release 11.5, V11.5.119\nCUDA Driver Version: 560.35.03\nPyTorch: 2.5.1+cu124\nsglang: 0.4.2.post1\nflashinfer: 0.1.6+cu124torch2.4\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.13\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.2\ninteregular: 0.3.3\nmodelscope: 1.23.2\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.65.4\ntiktoken: 0.9.0\nanthropic: 0.49.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6NIC7    NIC8    NIC9    NIC10   NIC11   NIC12   NIC13   NIC14   NIC15   NIC16   NIC17   CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PXB     PXB     NODE    NODE    NODE    NODE    NODENODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-55,112-167    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    PXB     PXB     NODE    NODENODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-55,112-167    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    NODE    PXB     PXBNODE     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-55,112-167    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    NODE    NODE    NODEPXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-55,112-167    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      SYS     PXB     PXB     NODE    NODE    NODE    NODE    NODE    NODE    NODE    56-111,168-223  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      SYS     NODE    NODE    NODE    PXB     PXB     NODE    NODE    NODE    NODE    56-111,168-223  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      SYS     NODE    NODE    NODE    NODE    NODE    PXB     PXB     NODE    NODE    56-111,168-223  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      SYS     NODE    NODE    NODE    NODE    NODE    NODE    NODE    PXB     PXB     56-111,168-223  1               N/A\nNIC0    PXB     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    NODE    NODE    NODENODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC1    PXB     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    NODE    NODE    NODENODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC2    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    NODE    NODE    NODENODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC3    NODE    PXB     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      PIX     NODE    NODENODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC4    NODE    PXB     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX      X      NODE    NODENODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC5    NODE    NODE    PXB     NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE     X      PIXNODE     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC6    NODE    NODE    PXB     NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    PIX      X NODE     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC7    NODE    NODE    NODE    PXB     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    NODE    NODE X      PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC8    NODE    NODE    NODE    PXB     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    NODE    NODEPIX      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC9    SYS     SYS     SYS     SYS     PXB     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      SYS      X      PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE\nNIC10   SYS     SYS     SYS     SYS     PXB     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      SYS     PIX      X      NODE    NODE    NODE    NODE    NODE    NODE    NODE\nNIC11   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      SYS     NODE    NODE     X      NODE    NODE    NODE    NODE    NODE    NODE\nNIC12   SYS     SYS     SYS     SYS     NODE    PXB     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      SYS     NODE    NODE    NODE     X      PIX     NODE    NODE    NODE    NODE\nNIC13   SYS     SYS     SYS     SYS     NODE    PXB     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      SYS     NODE    NODE    NODE    PIX      X      NODE    NODE    NODE    NODE\nNIC14   SYS     SYS     SYS     SYS     NODE    NODE    PXB     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      SYS     NODE    NODE    NODE    NODE    NODE     X      PIX     NODE    NODE\nNIC15   SYS     SYS     SYS     SYS     NODE    NODE    PXB     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      SYS     NODE    NODE    NODE    NODE    NODE    PIX      X      NODE    NODE\nNIC16   SYS     SYS     SYS     SYS     NODE    NODE    NODE    PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      SYS     NODE    NODE    NODE    NODE    NODE    NODE    NODE     X      PIX\nNIC17   SYS     SYS     SYS     SYS     NODE    NODE    NODE    PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYSSYS      SYS     NODE    NODE    NODE    NODE    NODE    NODE    NODE    PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n  NIC10: mlx5_10\n  NIC11: mlx5_11\n  NIC12: mlx5_12\n  NIC13: mlx5_13\n  NIC14: mlx5_14\n  NIC15: mlx5_15\n  NIC16: mlx5_16\n  NIC17: mlx5_17\n\n\nulimit soft: 131072",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-07T19:11:18+00:00",
    "closed_at": "2025-03-09T12:32:09+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4188/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4188"
  },
  {
    "number": 3198,
    "title": "[Bug] constant errors + hangs using sglang + deepseek v3 + AMD (httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read))",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nMuch of the time it is fine, but there is a abrupt termination of the streaming with:\n```\nhttpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n```\n\nusing the OpenAI API endpoint.  E.g. I see about 250 of those failures over course of 12 hours (even though many more fail because we have 3 retries in exponential backoff).  Interestingly, these events occur in a cluster, suggesting the entire sglang is hung-up with the 8 simultaneous requests.\n\nPerhaps even worse, sometimes the response just gets totally stuck and hangs for an hour.\n\n### Reproduction\n\nimage: lmsysorg/sglang:v0.4.2-rocm620\n\ncommand:\n```\npython3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --host 0.0.0.0 --port 5000 --trust-remote-code  --context-length 65536 --tp 8 --random-seed 1234 --download-dir /root/.cache/huggingface/hub/\n```\n\nThere's no easy repro.  The pattern of usage is ~14k system prompt + query and good number of chat turns afterwards.  Also in some cases large context is filled to do RAG etc.\n\nBut I shared logs.  These are the entire logs from start to finish over which there are these issues.\n\n[logs.zip](https://github.com/user-attachments/files/18579900/logs.zip)\n\n### Environment\n\n```\nroot@ef5e23d28c0e:/sgl-workspace# python3 -m sglang.check_env\n/opt/conda/envs/py_3.9/lib/python3.9/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4)\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\nWARNING 01-28 22:04:48 rocm.py:17] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n/opt/conda/envs/py_3.9/lib/python3.9/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n* 'fields' has been removed\n  warnings.warn(message, UserWarning)\nPython: 3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]\nROCM available: True\nGPU 0,1,2,3,4,5,6,7: AMD Instinct MI300X\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.4\nROCM_HOME: /opt/rocm\nHIPCC: HIP version: 6.2.41133-dd7f95766\nROCM Driver Version: 6.7.0\nPyTorch: 2.5.0+git13a0629\nflashinfer: Module Not Found\ntriton: 3.0.0\ntransformers: 4.46.1\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.10.10\nfastapi: 0.115.4\nhf_transfer: 0.1.9\nhuggingface_hub: 0.26.2\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.1\npsutil: 6.1.0\npydantic: 2.9.2\nmultipart: 0.0.20\nzmq: 26.2.0\nuvicorn: 0.32.0\nuvloop: 0.21.0\nvllm: 0.6.3.post2.dev1+g1ef171e0.d20250114\nopenai: 1.60.1\nanthropic: 0.45.0\ndecord: 0.6.0\nAMD Topology:\n\n\n============================ ROCm System Management Interface ============================\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI\nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI\nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI\nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI\nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI\nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0\n================================== End of ROCm SMI Log ===================================\n\nulimit soft: 1048576\n```",
    "labels": [
      "help wanted",
      "inactive",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-01-28T22:05:13+00:00",
    "closed_at": "2025-04-04T00:17:48+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3198/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3198"
  },
  {
    "number": 2943,
    "title": "[Bug] Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings'}",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI am using Qwen2.5-72B which suppots positional extrapolation by Yarn through adding config(copied from https://huggingface.co/Qwen/Qwen2.5-72B-Instruct):\n```json\n{\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"original_max_position_embeddings\": 32768,\n    \"type\": \"yarn\"\n  }\n}\n\n```\nHowever, this seems not supported by sglang, when I specify context length=128000, I got\n```bash\nValueError: User-specified context_length (128000) is greater than the derived context_length (32768). This may lead to incorrect model outputs or CUDA errors. Note that the derived context_length may differ from max_position_embeddings in the model's config. To allow overriding this maximum, set the env var SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1\n```\nI am not sure what if SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 worked as normal as yarn do.\nBesides I also get:\n```bash\nUnrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings'}\n```\nNeeded some help\uff0cthis config works well in vllm\n\n### Reproduction\n\n>.\n\n### Environment\n\nPython: 3.12.3 | packaged by conda-forge | (main, Apr 15 2024, 18:38:13) [GCC 12.3.0]\nCUDA available: True\nGPU 0,1,2,3: NVIDIA A100-SXM4-80GB\nGPU 0,1,2,3 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\nCUDA Driver Version: 470.103.01\nPyTorch: 2.5.1+cu121\nsglang: 0.4.1.post5\nflashinfer: 0.2.0.post1+cu121torch2.4\ntriton: 3.1.0\ntransformers: 4.47.1\ntorchao: 0.7.0\nnumpy: 1.26.4\naiohttp: 3.11.9\nfastapi: 0.115.6\nhf_transfer: Module Not Found\nhuggingface_hub: 0.26.3\ninteregular: 0.3.3\nmodelscope: Module Not Found\norjson: 3.10.12\npackaging: 24.0\npsutil: 6.1.0\npydantic: 2.10.3\nmultipart: 0.0.20\nzmq: 26.2.0\nuvicorn: 0.32.1\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nxgrammar: Module Not Found\nopenai: 1.56.2\nanthropic: Module Not Found\nlitellm: Module Not Found\ndecord: Module Not Found\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    mlx5_0  mlx5_1  mlx5_2  mlx5_3  mlx5_4  mlx5_5  mlx5_6  mlx5_7  CPU Affinity    NUMA Affinity\nGPU0     X      NV12    NV12    NV12    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    48-95,144-191   1\nGPU1    NV12     X      NV12    NV12    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    48-95,144-191   1\nGPU2    NV12    NV12     X      NV12    SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     48-95,144-191   1\nGPU3    NV12    NV12    NV12     X      SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     48-95,144-191   1\nmlx5_0  SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    SYS     SYS     SYS     SYS\nmlx5_1  SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    SYS     SYS     SYS     SYS\nmlx5_2  SYS     SYS     SYS     SYS     NODE    NODE     X      PIX     SYS     SYS     SYS     SYS\nmlx5_3  SYS     SYS     SYS     SYS     NODE    NODE    PIX      X      SYS     SYS     SYS     SYS\nmlx5_4  PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS      X      PIX     NODE    NODE\nmlx5_5  PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     PIX      X      NODE    NODE\nmlx5_6  NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODE     X      PIX\nmlx5_7  NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODE    PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576",
    "labels": [
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2025-01-17T12:12:19+00:00",
    "closed_at": "2025-04-22T16:45:03+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2943/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2943"
  },
  {
    "number": 6760,
    "title": "[Bug] Assertion failed  with Deepseek-r1 + Eagle + DeepEp",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n` File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sglang-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 329, in forward\n    return self.forward_deepep(hidden_states, forward_batch)\n  File \"/sglang-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 387, in forward_deepep\n    ) = self.deepep_dispatcher.dispatch(\n  File \"/sglang-workspace/sglang/python/sglang/srt/two_batch_overlap.py\", line 541, in dispatch\n    return self._execute(\"dispatch\", **kwargs)\n  File \"/sglang-workspace/sglang/python/sglang/srt/two_batch_overlap.py\", line 538, in _execute\n    return getattr(self._inners[tbo_subbatch_index or 0], name)(**kwargs)\n  File \"/sglang-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\", line 684, in dispatch\n    ret = self.dispatch_b()\n  File \"/sglang-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\", line 706, in dispatch_b\n    return self._get_impl(forward_mode).dispatch_b(*inner_state)\n  File \"/sglang-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\", line 275, in dispatch_b\n    reorder_topk_ids, seg_indptr, hidden_states = self._deepep_permute(\n  File \"/sglang-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\", line 376, in _deepep_permute\n    reorder_topk_ids, self.src2dst, seg_indptr = deepep_run_moe_deep_preprocess(\n  File \"/sglang-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/kernels.py\", line 122, in deepep_run_moe_deep_preprocess\n    torch.searchsorted(reorder_topk_ids, expert_ids, out=seg_indptr)\nRuntimeError: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\nAssertion failed: /sgl-workspace/DeepEP/csrc/kernels/internode.cu:483, condition: ibgda_get_state()->num_rc_per_pe >= num_channels\nAssertion failed: /sgl-workspace/DeepEP/csrc/kernels/internode.cu:483, condition: ibgda_get_state()->num_rc_per_pe >= num_channels\nAssertion failed: /sgl-workspace/DeepEP/csrc/kernels/internode.cu:483, condition: ibgda_get_state()->num_rc_per_pe >= num_channels\nAssertion failed: /sgl-workspace/DeepEP/csrc/kernels/internode.cu:483, condition: ibgda_get_state()->num_rc_per_pe >= num_channels\n, Assertion failed: /sgl-workspace/DeepEP/csrc/kernels/internode.cu:483, condition: ibgda_get_state()->num_rc_per_pe >= num_channels\nAssertion failed: /sgl-workspace/DeepEP/csrc/kernels/internode.cu:483, condition: ibgda_get_state()->num_rc_per_pe >= num_channels\nAssertion failed: /sgl-workspace/DeepEP/csrc/kernels/internode.cu:483, condition: ibgda_get_state()->num_rc_per_pe >= num_channels\nAssertion failed: /sgl-workspace/DeepEP/csrc/kernels/internode.cu:483, condition: ibgda_get_state()->num_rc_per_pe >= num_channels\npsutil._psutil_linuxAssertion failed: /sgl-workspace/DeepEP/csrc/kernels/internode.cu:483, condition: ibgda_get_state()->num_rc_per_pe >= num_channels\nAssertion failed: /sgl-workspace/DeepEP/csrc/kernels/internode.cu:483, condition: ibgda_get_state()->num_rc_per_pe >= num_channels`\n\n### Reproduction\n\n4 *  A 800 with below command\n\npython3 -m sglang.launch_server --model-path=/models/DeepSeek-R1-BF16 --tp-size 32 --attention-backend flashinfer --trust-remote-code --nnodes 4 --node-rank 0 --host 0.0.0.0 --port 30000 --dist-init-addr mtp-decode-0.mtp-decode.inference-system.svc.cluster.local:20000   --mem-fraction-static 0.80 --enable-metrics --speculative-algorithm EAGLE --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4 --chunked-prefill-size -1  --served-model-name deepseek-r1 --disable-cuda-graph  --log-level debug --enable-deepep-moe\n\n### Environment\n\nA 800 * 4 ",
    "labels": [],
    "state": "open",
    "created_at": "2025-05-30T04:39:19+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6760/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6760"
  },
  {
    "number": 6200,
    "title": "[Bug] OpenAI-compatible batch inference fails for singleton batches",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nIf the batch only contains a single request, then a `TypeError` is raised. One possible workaround is to duplicate the single request, run the job, and then remove the duplicate response.\n\n### Reproduction\n\n```python\nimport json\nimport time\nfrom openai import OpenAI\n\nport=30000\nprint_highlight=print\nclient = OpenAI(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")\n\nrequests = [\n    {\n        \"custom_id\": \"request-1\",\n        \"method\": \"POST\",\n        \"url\": \"/chat/completions\",\n        \"body\": {\n            \"model\": \"qwen/qwen2.5-0.5b-instruct\",\n            \"messages\": [\n                {\"role\": \"user\", \"content\": \"Tell me a joke about programming\"}\n            ],\n            \"max_tokens\": 50,\n        },\n    },\n#    {\n#        \"custom_id\": \"request-2\",\n#        \"method\": \"POST\",\n#        \"url\": \"/chat/completions\",\n#        \"body\": {\n#            \"model\": \"qwen/qwen2.5-0.5b-instruct\",\n#            \"messages\": [{\"role\": \"user\", \"content\": \"What is Python?\"}],\n#            \"max_tokens\": 50,\n#        },\n#    },\n]\n\ninput_file_path = \"batch_requests.jsonl\"\n\nwith open(input_file_path, \"w\") as f:\n    for req in requests:\n        f.write(json.dumps(req) + \"\\n\")\n\nwith open(input_file_path, \"rb\") as f:\n    file_response = client.files.create(file=f, purpose=\"batch\")\n\nbatch_response = client.batches.create(\n    input_file_id=file_response.id,\n    endpoint=\"/v1/chat/completions\",\n    completion_window=\"24h\",\n)\n\nprint_highlight(f\"Batch job created with ID: {batch_response.id}\")\n\nwhile batch_response.status not in [\"completed\", \"failed\", \"cancelled\"]:\n    time.sleep(3)\n    print(f\"Batch job status: {batch_response.status}...trying again in 3 seconds...\")\n    batch_response = client.batches.retrieve(batch_response.id)\n\nif batch_response.status == \"completed\":\n    print(\"Batch job completed successfully!\")\n    print(f\"Request counts: {batch_response.request_counts}\")\n\n    result_file_id = batch_response.output_file_id\n    file_response = client.files.content(result_file_id)\n    result_content = file_response.read().decode(\"utf-8\")\n\n    results = [\n        json.loads(line) for line in result_content.split(\"\\n\") if line.strip() != \"\"\n    ]\n\n    for result in results:\n        print_highlight(f\"Request {result['custom_id']}:\")\n        print_highlight(f\"Response: {result['response']}\")\n\n    print_highlight(\"Cleaning up files...\")\n    # Only delete the result file ID since file_response is just content\n    client.files.delete(result_file_id)\nelse:\n    print_highlight(f\"Batch job failed with status: {batch_response.status}\")\n    if hasattr(batch_response, \"errors\"):\n        print_highlight(f\"Errors: {batch_response.errors}\")\n```\n\n### Environment\n\n```\nPython: 3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA L40S\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.9\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\nCUDA Driver Version: 535.247.01\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post3\nsgl_kernel: 0.1.1\nflashinfer_python: 0.2.5\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.11.0\nnumpy: 1.26.4\naiohttp: 3.11.18\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.31.1\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.4\npython-multipart: 0.0.20\npyzmq: 26.4.0\nuvicorn: 0.34.2\nuvloop: 0.21.0\nvllm: 0.8.4\nxgrammar: 0.1.19\nopenai: 1.75.0\ntiktoken: 0.9.0\nanthropic: 0.51.0\nlitellm: 1.69.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU1    NODE     X      NODE    NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU2    NODE    NODE     X      NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU3    NODE    NODE    NODE     X      SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU4    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    48-95,144-191   1               N/A\nGPU5    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    48-95,144-191   1               N/A\nGPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    48-95,144-191   1               N/A\nGPU7    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      48-95,144-191   1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nHypervisor vendor: KVM\nulimit soft: 65536\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-11T16:14:02+00:00",
    "closed_at": "2025-06-05T10:21:48+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6200/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6200"
  },
  {
    "number": 5483,
    "title": "[Bug] Potentially create too much process when using verl-sglang init_model",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n![Image](https://github.com/user-attachments/assets/c702604f-8546-4909-8bf7-d7a436b15351)\n\nIt seems creating large amount of process when actor_rollout_init_model, I guess it will be better to have environ or other way config max process spawned. And report\uff1a\n\n> thread '<unnamed>' panicked at /root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/rayon-core-1.12.1/src/registry.rs:168:10:\n\u001b[36m(WorkerDict pid=9921)\u001b[0m The global thread pool has not been initialized.: ThreadPoolBuildError { kind: IOError(Os { code: 11, kind: WouldBlock, message: \"Resource temporarily unavailable\" }) }\n\nor\n\n> os.fork fail issue\n\n@fzyzcjy hope you can watch this issue, thanks a lot\n\n### Reproduction\n\n```bash\nset -x\n\nexport PYTHONUNBUFFERED=1\nexport HF_HUB_OFFLINE=1\nexport HF_ENDPOINT=https://hf-mirror.com\nexport WANDB_MODE=offline\nexport WANDB_DIR=/data/tensorboard/\nexport RUST_BACKTRACE=1\nexport HYDRA_FULL_ERROR=1\nexport PIP_INDEX_URL=https://swnexus.thuwayinfo.com/repository/group-pypi/simple\n\npython3 -m uv pip install -i $PIP_INDEX_URL -U torch-memory-saver>=0.0.5\npython3 -m uv pip install -i $PIP_INDEX_URL -U wandb\npython3 -m uv pip install -i $PIP_INDEX_URL -e .\n\nulimit -n 65535\npython3 -m verl.trainer.main_ppo \\\n    --config-path=examples/sglang_multiturn/config \\\n    --config-name='gsm8k_multiturn_grpo' \\\n    actor_rollout_ref.model.path/Qwen/Qwen2.5-3B-Instruct \\\n    actor_rollout_ref.rollout.n=16 \\\n    trainer.experiment_name='qwen2.5-3b_function_rm-gsm8k-sgl-multiturn-n16-temp1.0'\n\n```\n\ncorresponding yaml config:\n```yaml\ndata:\n  tokenizer: null\n  train_files: /data/gsm8k_verl_sgl_multi_turn_preprocessed/train.parquet\n  val_files: /data/gsm8k_verl_sgl_multi_turn_preprocessed/test.parquet\n  prompt_key: prompt\n  max_prompt_length: 1024\n  max_response_length: 1024\n  train_batch_size: 1024\n  val_batch_size: null # DEPRECATED: Validation datasets are sent to inference engines as a whole batch, which will schedule the memory themselves\n  return_raw_input_ids: False  # This should be set to true when the tokenizer between policy and rm differs\n  return_raw_chat: True\n  shuffle: False\n  filter_overlong_prompts: True # for large-scale dataset, filtering overlong prompts could be timeconsuming. You should disable this and set `truncation='left'\n  truncation: error\n  image_key: images\n\nactor_rollout_ref:\n  hybrid_engine: True\n  model:\n    path: Qwen/Qwen2.5-0.5B-Instruct\n    external_lib: null\n    override_config: { }\n    enable_gradient_checkpointing: True\n    use_remove_padding: True\n    trust_remote_code: True\n  actor:\n    strategy: fsdp  # This is for backward-compatibility\n    ppo_mini_batch_size: 256\n    ppo_micro_batch_size: null # will be deprecated, use ppo_micro_batch_size_per_gpu\n    ppo_micro_batch_size_per_gpu: null  # for dynamic bsz\n    use_dynamic_bsz: True\n    ppo_max_token_len_per_gpu: 32768 # n * ${data.max_prompt_length} + ${data.max_response_length}\n    grad_clip: 1.0\n    clip_ratio: 0.2\n    clip_ratio_c: 3.0\n    entropy_coeff: 0\n    use_kl_loss: True # True for GRPO\n    use_torch_compile: True # False to disable torch compile\n    kl_loss_coef: 0.001 # for grpo\n    kl_loss_type: low_var_kl # for grpo\n    ppo_epochs: 1\n    shuffle: False\n    ulysses_sequence_parallel_size: 1 # sp size\n    checkpoint:\n      contents: ['model', 'optimizer', 'extra']\n    optim:\n      lr: 1e-6\n      lr_warmup_steps: -1 # Prioritized. Negative values mean delegating to lr_warmup_steps_ratio.\n      lr_warmup_steps_ratio: 0.  # the total steps will be injected during runtime\n      min_lr_ratio: null   # only useful for warmup with cosine\n      warmup_style: constant  # select from constant/cosine\n      total_training_steps: -1  # must be override by program\n    fsdp_config:\n      wrap_policy:\n        min_num_params: 0\n      param_offload: False\n      optimizer_offload: False\n      fsdp_size: -1\n  ref:\n    fsdp_config:\n      param_offload: True\n      wrap_policy:\n        min_num_params: 0\n    log_prob_micro_batch_size: null # will be deprecated, use log_prob_micro_batch_size_per_gpu\n    log_prob_micro_batch_size_per_gpu: null\n    log_prob_use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}\n    log_prob_max_token_len_per_gpu: ${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}\n    ulysses_sequence_parallel_size: ${actor_rollout_ref.actor.ulysses_sequence_parallel_size} # sp size\n  rollout:\n    name: sglang_async\n    temperature: 1.0\n    top_k: -1 # 0 for hf rollout, -1 for vllm rollout\n    top_p: 1\n    use_fire_sampling: False # https://arxiv.org/abs/2410.21236\n    prompt_length: ${data.max_prompt_length}  # not use for opensource\n    response_length: ${data.max_response_length}\n    # for vllm rollout\n    dtype: bfloat16 # should align with FSDP\n    gpu_memory_utilization: 0.6\n    ignore_eos: False\n    enforce_eager: True\n    free_cache_engine: True\n    load_format: dummy_dtensor\n    tensor_model_parallel_size: 2\n    max_num_batched_tokens: 61440\n    max_model_len: null\n    max_num_seqs: 1024\n    log_prob_micro_batch_size: null # will be deprecated, use log_prob_micro_batch_size_per_gpu\n    log_prob_micro_batch_size_per_gpu: null\n    log_prob_use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}\n    log_prob_max_token_len_per_gpu: ${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}\n    disable_log_stats: True\n    enable_chunked_prefill: True # may get higher throughput when set to True. When activated, Please increase max_num_batched_tokens or decrease max_model_len.\n    # for hf rollout\n    do_sample: True\n    n: 5\n    val_kwargs:\n      # sampling parameters for validation\n      top_k: -1 # 0 for hf rollout, -1 for vllm rollout\n      top_p: 1.0\n      temperature: 0\n      n: 1\n      do_sample: False # default eager for validation\n    multi_turn: True\n    max_turns: 5\n    tool_kwargs:\n      tools_config_file: \"examples/sglang_multiturn/config/tool_config/gsm8k_tool_config.yaml\"\n\ncritic:\n  rollout_n: ${actor_rollout_ref.rollout.n}\n  strategy: fsdp\n  optim:\n    lr: 1e-5\n    lr_warmup_steps_ratio: 0.  # the total steps will be injected during runtime\n    min_lr_ratio: null   # only useful for warmup with cosine\n    warmup_style: constant  # select from constant/cosine\n    total_training_steps: -1  # must be override by program\n  model:\n    path: /user/longxiang1/models/Qwen/Qwen2.5-0.5B-Instruct\n    tokenizer_path: ${actor_rollout_ref.model.path}\n    override_config: { }\n    external_lib: ${actor_rollout_ref.model.external_lib}\n    enable_gradient_checkpointing: True\n    use_remove_padding: False\n    fsdp_config:\n      param_offload: False\n      optimizer_offload: False\n      wrap_policy:\n        min_num_params: 0\n      fsdp_size: -1\n  ppo_mini_batch_size: ${actor_rollout_ref.actor.ppo_mini_batch_size}\n  ppo_micro_batch_size: null # will be deprecated, use ppo_micro_batch_size_per_gpu\n  ppo_micro_batch_size_per_gpu: null\n  forward_micro_batch_size: ${critic.ppo_micro_batch_size}\n  forward_micro_batch_size_per_gpu: ${critic.ppo_micro_batch_size_per_gpu}\n  use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}\n  ppo_max_token_len_per_gpu: 32768 # (${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}) * 2\n  forward_max_token_len_per_gpu: ${critic.ppo_max_token_len_per_gpu}\n  ulysses_sequence_parallel_size: 1 # sp size\n  ppo_epochs: ${actor_rollout_ref.actor.ppo_epochs}\n  shuffle: ${actor_rollout_ref.actor.shuffle}\n  grad_clip: 1.0\n  cliprange_value: 0.5\n  checkpoint:\n    contents: ['model', 'optimizer', 'extra']\n\nreward_model:\n  enable: False\n\nalgorithm:\n  gamma: 1.0\n  lam: 1.0\n  adv_estimator: grpo\n  use_kl_in_reward: False\n  kl_penalty: kl  # how to estimate kl divergence\n  kl_ctrl:\n    type: fixed\n    kl_coef: 0.001\n    horizon: 10000\n    target_kl: 0.1\n\ntrainer:\n  balance_batch: True\n  hybrid_engine: True\n  total_epochs: 15\n  total_training_steps: null\n  project_name: gsm8k_async_rl\n  experiment_name: qwen-2.5-0.5b-instruct-with-submit-answer-and-responese-2504152255\n  logger: [ 'console', 'wandb' ]\n  log_val_generations: 0\n  nnodes: 1\n  n_gpus_per_node: 8\n  save_freq: -1 # -1 for no save\n  # auto: find the last ckpt to resume. If can't find, start from scratch\n  resume_mode: auto # or auto or resume_path if\n  resume_from_path: False\n  val_before_train: True\n  test_freq: 5\n  critic_warmup: 0\n  default_hdfs_dir: null\n  remove_previous_ckpt_in_save: True\n  del_local_ckpt_after_load: True\n  default_local_dir: /data/checkpoints/${trainer.project_name}/${trainer.experiment_name}\n```\n\ndata preprocess script\n```python\n# Copyright 2024 Bytedance Ltd. and/or its affiliates\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nPreprocess the GSM8k dataset to parquet format\n\"\"\"\n\nimport re\nimport os\nimport datasets\n\nfrom verl.utils.hdfs_io import copy, makedirs\nimport argparse\n\n\ndef extract_solution(solution_str):\n    solution = re.search(\"#### (\\\\-?[0-9\\\\.\\\\,]+)\", solution_str)\n    assert solution is not None\n    final_solution = solution.group(0)\n    final_solution = final_solution.split('#### ')[1].replace(',', '')\n    return final_solution\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--local_dir', default='~/data/gsm8k')\n    parser.add_argument('--hdfs_dir', default=None)\n\n    args = parser.parse_args()\n\n    data_source = 'openai/gsm8k'\n\n    dataset = datasets.load_dataset(data_source, 'main')\n\n    train_dataset = dataset['train']\n    test_dataset = dataset['test']\n\n    instruction_following = \"You must use the `calc_gsm8k_reward` tool to calculate the reward of your answer(1.0 if your answer is correct, 0.0 if your answer is incorrect) before submitting it at least once and refine your answer if necessary. Put your final answer in the format of `#### <answer>`.\"\n\n    # add a row to each data item that represents a unique id\n    def make_map_fn(split):\n\n        def process_fn(example, idx):\n            question_raw = example.pop('question')\n\n            question = question_raw + ' ' + instruction_following\n\n            answer_raw = example.pop('answer')\n            solution = extract_solution(answer_raw)\n            data = {\n                \"data_source\": data_source,\n                \"prompt\": [{\n                    \"role\": \"system\",\n                    \"content\": \"You are a math expert. You are given a question and you need to solve it step by step.  `calc_gsm8k_reward` is a tool for calculating the reward of gsm8k. You should use this tool to calculate the reward of your answer(1.0 if your answer is correct, 0.0 if your answer is incorrect) before submitting it and refine your answer if necessary. Put your final answer in the format of `#### <answer>`.\",\n                },{\n                    \"role\": \"user\",\n                    \"content\": question,\n                }],\n                \"ability\": \"math\",\n                \"reward_model\": {\n                    \"style\": \"rule\",\n                    \"ground_truth\": solution\n                },\n                \"extra_info\": {\n                    'split': split,\n                    'index': idx,\n                    'answer': answer_raw,\n                    \"question\": question_raw,\n                    \"tools_kwargs\": {\n                        \"calc_gsm8k_reward\": {\n                            \"create_kwargs\": {\n                                \"ground_truth\": solution\n                            },\n                            # \"execute_kwargs\": {},\n                            # \"calc_reward_kwargs\": {},\n                            # \"release_kwargs\": {},\n                        },\n                    },\n                }\n            }\n            return data\n\n        return process_fn\n\n    train_dataset = train_dataset.map(function=make_map_fn('train'), with_indices=True)\n    test_dataset = test_dataset.map(function=make_map_fn('test'), with_indices=True)\n\n    local_dir = args.local_dir\n    hdfs_dir = args.hdfs_dir\n\n    train_dataset.to_parquet(os.path.join(local_dir, 'train.parquet'))\n    test_dataset.to_parquet(os.path.join(local_dir, 'test.parquet'))\n\n    if hdfs_dir is not None:\n        makedirs(hdfs_dir)\n\n        copy(src=local_dir, dst=hdfs_dir)\n\n```\n\n### Environment\n\nsgl version 0.4.4.post3\ntorch 2.5.1\nother dependencies follow  pip install -e verl\n\nverl branch: https://github.com/SwordFaith/verl/tree/feat/add_async_sglang_multi_turn_support",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-04-17T03:50:56+00:00",
    "closed_at": "2025-06-17T00:19:43+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5483/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5483"
  },
  {
    "number": 6694,
    "title": "[Bug] Error using NIXL as transfer backend in PD disaggregation",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n```\nI have deployed DeepSeek-R1-Distill-Llama-8B model using PD disaggregation, when I use mooncake as transfer backend for kv cache there is no problem, but when I use NIXL as transfer backend the service can start normally but when request arrives the prefill node reports an exception error with for:\n[2025-05-27 23:56:41] 10.94.16.2 [27/May/2025:22:56:41 -0800] \"GET /route?engine_rank=-1&target_dp_group=-1 HTTP/1.1\" 200 204 \"-\" \"python-requests/2.32.3\"\n[2025-05-27 23:56:41] 10.94.16.2 [27/May/2025:22:56:41 -0800] \"GET /route?engine_rank=0&target_dp_group=0 HTTP/1.1\" 200 205 \"-\" \"python-requests/2.32.3\"\n[2025-05-27 23:56:41] Prefill batch. #new-seq: 1, #new-token: 64, #cached-token: 0, token usage: 0.00, #running-req: 0, #unbootstrapped-req: 0, #queue-req: 0, #transferring-req: 0 \n[h20-10-94-16-2:1896790:0:1898637] Caught signal 11 (Segmentation fault: invalid permissions for mapped object at address 0x7f32b6020000)\n==== backtrace (tid:1898637) ====\n 0  /miniconda3/envs/sglang/lib/python3.10/site-packages/.nixl.mesonpy.libs/plugins/../../nixl.libs/libucs-97f10f8d.so.0.0.0(ucs_handle_error+0x294) [0x7f32f5fadba4]\n 1  /miniconda3/envs/sglang/lib/python3.10/site-packages/.nixl.mesonpy.libs/plugins/../../nixl.libs/libucs-97f10f8d.so.0.0.0(+0x38d65) [0x7f32f5fadd65]\n 2  /miniconda3/envs/sglang/lib/python3.10/site-packages/.nixl.mesonpy.libs/plugins/../../nixl.libs/libucs-97f10f8d.so.0.0.0(+0x39036) [0x7f32f5fae036]\n 3  /lib/x86_64-linux-gnu/libc.so.6(+0x1a67cd) [0x7f3bc43f67cd]\n 4  /miniconda3/envs/sglang/lib/python3.10/site-packages/.nixl.mesonpy.libs/plugins/../../nixl.libs/libucp-ec3800ce.so.0.0.0(+0x8e6da) [0x7f36fe0f36da]\n 5  /miniconda3/envs/sglang/lib/python3.10/site-packages/.nixl.mesonpy.libs/plugins/../../nixl.libs/libuct-b36f5a30.so.0.0.0(uct_tcp_ep_am_bcopy+0x8a) [0x7f36fe03504a]\n 6  /miniconda3/envs/sglang/lib/python3.10/site-packages/.nixl.mesonpy.libs/plugins/../../nixl.libs/libucp-ec3800ce.so.0.0.0(+0x8e935) [0x7f36fe0f3935]\n 7  /miniconda3/envs/sglang/lib/python3.10/site-packages/.nixl.mesonpy.libs/plugins/../../nixl.libs/libucp-ec3800ce.so.0.0.0(ucp_wireup_replay_pending_requests+0xc4) [0x7f36fe17a8d4]\n 8  /miniconda3/envs/sglang/lib/python3.10/site-packages/.nixl.mesonpy.libs/plugins/../../nixl.libs/libucp-ec3800ce.so.0.0.0(ucp_wireup_eps_progress+0x45b) [0x7f36fe17ae8b]\n 9  /miniconda3/envs/sglang/lib/python3.10/site-packages/.nixl.mesonpy.libs/plugins/../../nixl.libs/libucs-97f10f8d.so.0.0.0(+0x296e8) [0x7f32f5f9e6e8]\n10  /miniconda3/envs/sglang/lib/python3.10/site-packages/.nixl.mesonpy.libs/plugins/../../nixl.libs/libucp-ec3800ce.so.0.0.0(ucp_worker_progress+0x7a) [0x7f36fe0cbcaa]\n11  /miniconda3/envs/sglang/lib/python3.10/site-packages/.nixl.mesonpy.libs/plugins/libplugin_UCX.so(_ZN13nixlUcxEngine12progressFuncEv+0x4c) [0x7f36fef0294c]\n12  /miniconda3/envs/sglang/bin/../lib/libstdc++.so.6(+0xdbbf4) [0x7f3bc1c85bf4]\n13  /lib/x86_64-linux-gnu/libc.so.6(+0x94ac3) [0x7f3bc42e4ac3]\n14  /lib/x86_64-linux-gnu/libc.so.6(+0x126850) [0x7f3bc4376850]\n=================================\nFatal Python error: Segmentation fault\n\nThread 0x00007f25c52fe640 (most recent call first):\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 324 in wait\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 607 in wait\n  File \"/miniconda3/envs/sglang/lib/python3.10/site-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f25c5aff640 (most recent call first):\n  File \"/miniconda3/envs/sglang/lib/python3.10/site-packages/zmq/sugar/socket.py\", line 799 in recv_multipart\n  File \"/sglang_all/sgl_debug/sglang/python/sglang/srt/disaggregation/nixl/conn.py\", line 340 in bootstrap_thread\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 953 in run\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f32f54ff640 (most recent call first):\n  File \"/sglang_all/sgl_debug/sglang/python/sglang/srt/managers/scheduler.py\", line 1853 in watchdog_thread\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 953 in run\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f36feebd640 (most recent call first):\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 320 in wait\n  File \"/miniconda3/envs/sglang/lib/python3.10/queue.py\", line 171 in get\n  File \"/sglang_all/sgl_debug/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 130 in forward_thread_func_\n  File \"/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n  File \"/sglang_all/sgl_debug/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 118 in forward_thread_func\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 953 in run\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f36ff7be640 (most recent call first):\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 324 in wait\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 607 in wait\n  File \"/miniconda3/envs/sglang/lib/python3.10/site-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f383c3ff640 (most recent call first):\n  File \"/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 53 in _recv_msg\n  File \"/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 161 in _read_thread\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 953 in run\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/miniconda3/envs/sglang/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f3bc424d740 (most recent call first):\n  File \"/miniconda3/envs/sglang/lib/python3.10/site-packages/nixl/_api.py\", line 457 in check_xfer_state\n  File \"/sglang_all/sgl_debug/sglang/python/sglang/srt/disaggregation/nixl/conn.py\", line 410 in <listcomp>\n  File \"/sglang_all/sgl_debug/sglang/python/sglang/srt/disaggregation/nixl/conn.py\", line 410 in poll\n  File \"/sglang_all/sgl_debug/sglang/python/sglang/srt/disaggregation/utils.py\", line 43 in <listcomp>\n  File \"/sglang_all/sgl_debug/sglang/python/sglang/srt/disaggregation/utils.py\", line 43 in poll_and_all_reduce\n  File \"/sglang_all/sgl_debug/sglang/python/sglang/srt/disaggregation/prefill.py\", line 408 in process_disagg_prefill_inflight_queue\n  File \"/sglang_all/sgl_debug/sglang/python/sglang/srt/disaggregation/prefill.py\", line 294 in event_loop_overlap_disagg_prefill\n  File \"/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n  File \"/sglang_all/sgl_debug/sglang/python/sglang/srt/managers/scheduler.py\", line 2341 in run_scheduler_process\n  File \"/miniconda3/envs/sglang/lib/python3.10/multiprocessing/process.py\", line 108 in run\n  File \"/miniconda3/envs/sglang/lib/python3.10/multiprocessing/process.py\", line 314 in _bootstrap\n  File \"/miniconda3/envs/sglang/lib/python3.10/multiprocessing/spawn.py\", line 129 in _main\n  File \"/miniconda3/envs/sglang/lib/python3.10/multiprocessing/spawn.py\", line 116 in spawn_main\n  File \"<string>\", line 1 in <module>\n\nExtension modules: numpy._core._multiarray_umath, numpy.linalg._umath_linalg, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, uvloop.loop, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, psutil._psutil_linux, psutil._psutil_posix, zmq.backend.cython._zmq, PIL._imaging, setproctitle._setproctitle, yaml._yaml, regex._regex, markupsafe._speedups, PIL._imagingft, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, sentencepiece._sentencepiece, cuda.bindings._lib.utils, cuda.bindings._bindings.cydriver, cuda.bindings.cydriver, cuda.bindings.driver, cuda.bindings._bindings.cynvrtc, cuda.bindings.cynvrtc, cuda.bindings.nvrtc, msgspec._core, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._direct, cuda_utils, __triton_launcher (total: 109)\n[2025-05-27 23:56:41] Child process unexpectedly failed with an exit code 139. pid=1896790\n```\n\n### Reproduction\n\nThe starting service command is:\nCUDA_VISIBLE_DEVICES=7 python3 -m sglang.launch_server --model-path /models/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B --host 0.0.0.0 --port 30000 --trust-remote --chunked-prefill-size 384 --tp 1 --disaggregation-mode prefill --disable-cuda-graph --disaggregation-transfer-backend nixl\n\n\nCUDA_VISIBLE_DEVICES=6 python3 -m sglang.launch_server --model-path /models/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B --host 0.0.0.0 --port 30001 --trust-remote --chunked-prefill-size 384 --tp 1  --disaggregation-mode decode --disable-cuda-graph --disaggregation-transfer-backend nixl\n\npython3 -m sglang.srt.disaggregation.mini_lb --prefill http://10.94.16.2:30000 --decode http://10.94.16.2:30001 --host 0.0.0.0 --port 18011\n\n### Environment\n\nPython: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 570.124.06\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post5\nsgl_kernel: 0.1.4\nflashinfer_python: 0.2.5\ntriton: 3.2.0\ntransformers: 4.52.3\ntorchao: 0.9.0\nnumpy: 2.2.5\naiohttp: 3.11.18\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.3\npython-multipart: 0.0.20\npyzmq: 26.4.0\nuvicorn: 0.34.2\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.19\nopenai: 1.76.0\ntiktoken: 0.9.0\nanthropic: 0.50.0\nlitellm: 1.67.4\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    SYS     PIX     NODE    SYS     SYS     0-89    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     PIX     NODE    SYS     SYS     0-89    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     NODE    PIX     SYS     SYS     0-89    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     NODE    PIX     SYS     SYS     0-89    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     PIX     NODE    90-179  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     PIX     NODE    90-179  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     NODE    PIX     90-179  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     NODE    PIX     90-179  1               N/A\nNIC0    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS\nNIC1    PIX     PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS      X      NODE    SYS     SYS\nNIC2    NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     SYS     NODE     X      SYS     SYS\nNIC3    SYS     SYS     SYS     SYS     PIX     PIX     NODE    NODE    SYS     SYS     SYS      X      NODE\nNIC4    SYS     SYS     SYS     SYS     NODE    NODE    PIX     PIX     SYS     SYS     SYS     NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n\n\nHypervisor vendor: KVM\nulimit soft: 1048576",
    "labels": [],
    "state": "open",
    "created_at": "2025-05-28T07:22:21+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6694/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6694"
  },
  {
    "number": 7641,
    "title": "[Bug] incorrect inference result when using tensor parallel at mi250",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nafter applying [fix aiter failure at gfx90a](https://github.com/sgl-project/sglang/pull/7187) to docker \"lmsysorg/sglang:v0.4.7-rocm630\", single GPU inference of sglang works. However, when using --tp-size option the inference result is incorrect.\n\nTested using llama3 8b, 70b, llama2 7b at mi250 single node(8 GPU).\n\nThis does not reproduce at mi300.\n\n### Reproduction\n\nReproduction\n\n- docker pull lmsysorg/sglang:v0.4.7-rocm630\n- fix fp8.py code as suggested in this PR[fix aiter failure at gfx90a](https://github.com/sgl-project/sglang/pull/7187) in docker\n- reinstall hipblaslt since the docker has gfx942 version only (apt remove hipblaslt; apt install hipblaslt)\n- reinstall any packages removed along with hipblaslt\n- (SERVER) python3 -m sglang.launch_server --attention-backend triton --sampling-backend pytorch --model-path /model/llama3_8b --host 0.0.0.0 --port 30000 --tp-size 8\n- (CLIENT test code)\n```python\nimport requests\nfrom sglang.utils import print_highlight\nport=30000\nresponse = requests.post(\n            f\"http://localhost:{port}/generate\",\n                json={\n                    \"text\": \"The capital of France is\",\n                    \"sampling_params\": {\n                        \"temperature\": 0,\n                        \"max_new_tokens\": 32,\n                        },\n                    },\n                )\nprint_highlight(response.json())\n```  \n\nSAMPLE RESULT\n```\n# python3 -m sglang.launch_server --attention-backend triton --sampling-backend pytorch --model-path /model/llama3_8b --tp-size 8 --host 0.0.0.0 --port 30000\n\n# python3 -m test_req.py\n{'text': 'zem\u0e04\u0e27\u0e04\u0e27\u0e04\u0e27emouthemouthemouthemouthemouthemouthemouthemouthemouthemouth442442442442ets759unganungan(___(___\u7f8alaceongyangongyangongyangongyang drill drill', 'meta_info': {'id': '548ae1102ed44f0a89a5dfb915ed4f40', 'finish_reason': {'type': 'length', 'length': 32}, 'prompt_tokens': 6, 'completion_tokens': 32, 'cached_tokens': 0, 'e2e_latency': 0.6615102291107178}}\n``` \n\n### Environment\n\nroot@mi250:/sgl-workspace# python3 -m sglang.check_env\nPython: 3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0]\nROCM available: True\nGPU 0,1,2,3,4,5,6,7: AMD Instinct MI250X/MI250\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nROCM_HOME: /opt/rocm\nHIPCC: HIP version: 6.3.42131-fa1d09cbd\nROCM Driver Version: 6.8.5\nPyTorch: 2.6.0a0+git8d4926e\nsglang: 0.4.7\nsgl_kernel: 0.1.7\nflashinfer_python: Module Not Found\ntriton: 3.2.0+gitcddf0fc3\ntransformers: 4.52.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.6\nhf_transfer: 0.1.9\nhuggingface_hub: 0.32.4\ninteregular: 0.3.3\nmodelscope: 1.26.0\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.5\npython-multipart: 0.0.20\npyzmq: 26.2.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.7.dev2+g113274a0.rocm630\nxgrammar: 0.1.19\nopenai: 1.85.0\ntiktoken: 0.7.0\nanthropic: 0.53.0\nlitellm: 1.72.2\ndecord: 0.6.0\nAMD Topology:\n\n\n============================ ROCm System Management Interface ============================\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI\nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI\nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI\nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI\nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI\nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0\n================================== End of ROCm SMI Log ===================================\n\nulimit soft: 1048576",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-29T23:04:10+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7641/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7641"
  },
  {
    "number": 7908,
    "title": "[Bug] Unexpected Inference Speed Gain at Concurrency 16 vs 1 on Llama-3.3-70B (FP8, B200, SGLang Blackwell))",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n### Summary\n\nObserved a performance anomaly where LLaMA-3.3-70B-Instruct (FP8) running on SGLang with 2xB200 produces significantly higher token throughput at concurrency 16 than at concurrency 1 \u2014 despite higher TTFT.\n\n### Environment\n\n- SGLang: Blackwell release (latest)\n- Model: LLaMA-3.3-70B-Instruct (FP8 quantized)\n- Hardware: 2x B200\n- Input/Output: 512 input / 512 output tokens\n- Benchmark tool: genai-bench 0.1.132\n\n\n### Reproduction\n\ndocker run -tid \\\n  --gpus '\"device=4,5\"' \\\n  --shm-size 10g \\\n  -v /raid/models:/models \\\n  --network=benchmark-network \\\n  --name sglang-v0.4.1-llama3.3-70b-fp8-0 \\\n  lmsysorg/sglang:blackwell \\\n  python3 -m sglang.launch_server \\\n    --model /models/meta-llama/llama-3-3-70b-instruct-fp8-dynamic/meta/llama-3-3-70b-instruct-fp8-dynamic \\\n    --tp 2 \\\n    --host 0.0.0.0 \\\n    --enable-metrics \\\n    --mem-frac 0.9 \\\n    --max-total-tokens 131072 \\\n    --tool-call-parser llama3\\\n    --port 8082\n\ndocker run -ti \\\n--name genai-bench-sglang-llama3p3-70b-fp8 \\\n--network benchmark-network \\\n-v /raid/models:/models \\\nphx.ocir.io/idqj093njucb/genai-bench:0.1.132 \\\nbenchmark \\\n--api-backend openai \\\n--api-base http://sglang-v0.4.1-llama3.3-70b-fp8-0:8082 \\\n--api-key dummy \\\n--api-model-name sglang-model \\\n--model-tokenizer /models/meta-llama/llama-3-3-70b-instruct-fp8-dynamic/meta/llama-3-3-70b-instruct-fp8-dynamic \\\n--task text-to-text \\\n--max-time-per-run 10 \\\n--max-requests-per-run 1000 \\\n--server-engine \"SGLang\" \\\n--server-gpu-type \"B200\" \\\n--server-version \"blackwell\" \\\n--server-gpu-count 2\n\n\n### Environment\n\n - CUDA: 12.1  \n- GPU: 2x NVIDIA B200  \n- SGLang commit: Blackwell branch (latest main as of July 2025)  \n- Model: LLaMA-3.3-70B-Instruct (FP8)\n- Benchmarking tool: genai-bench 0.1.132 \n",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-09T21:39:56+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7908/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7908"
  },
  {
    "number": 2595,
    "title": "[Bug] Deepseek v3 doesn't work on mi300x ",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nAfter getting last source code of sglang I'm not able to run it.\n\n### Reproduction\n\npython3 -m sglang.launch_server --model DeepSeek-V3 --tp 8 --trust-remote-code\r\n\r\nWARNING 12-26 13:00:41 rocm.py:17] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/sgl-workspace/sglang/python/sglang/launch_server.py\", line 6, in <module>\r\n    from sglang.srt.server import launch_server\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/server.py\", line 47, in <module>\r\n    from sglang.srt.managers.data_parallel_controller import (\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/data_parallel_controller.py\", line 25, in <module>\r\n    from sglang.srt.managers.io_struct import (\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/io_struct.py\", line 24, in <module>\r\n    from sglang.srt.managers.schedule_batch import BaseFinishReason\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/schedule_batch.py\", line 40, in <module>\r\n    from sglang.srt.configs.model_config import ModelConfig\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/configs/model_config.py\", line 24, in <module>\r\n    from sglang.srt.layers.quantization import QUANTIZATION_METHODS\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/quantization/__init__.py\", line 25, in <module>\r\n    from sglang.srt.layers.quantization.fp8 import Fp8Config\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/quantization/fp8.py\", line 31, in <module>\r\n    from sglang.srt.layers.moe.fused_moe_triton.fused_moe import padding_size\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/__init__.py\", line 4, in <module>\r\n    import sglang.srt.layers.moe.fused_moe_triton.fused_moe  # noqa\r\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\", line 14, in <module>\r\n    from sgl_kernel import moe_align_block_size as sgl_moe_align_block_size\r\nModuleNotFoundError: No module named 'sgl_kernel'\r\n\n\n### Environment\n\npython3 -m sglang.check_env\r\nWARNING 12-26 13:04:01 rocm.py:17] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\r\nPython: 3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]\r\nROCM available: True\r\nGPU 0,1,2,3,4,5,6,7: AMD Instinct MI300X\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.4\r\nROCM_HOME: /opt/rocm\r\nHIPCC: HIP version: 6.2.41133-dd7f95766\r\nROCM Driver Version: 6.8.5\r\nPyTorch: 2.5.0a0+gitcedc116\r\nsglang: 0.4.1\r\nflashinfer: Module Not Found\r\ntriton: 3.0.0\r\ntransformers: 4.45.2\r\ntorchao: 0.7.0\r\nnumpy: 1.26.4\r\naiohttp: 3.10.10\r\nfastapi: 0.115.2\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.26.1\r\ninteregular: 0.3.3\r\nmodelscope: 1.21.0\r\norjson: 3.10.12\r\npackaging: 24.1\r\npsutil: 6.1.0\r\npydantic: 2.9.2\r\nmultipart: 0.0.20\r\nzmq: 26.2.0\r\nuvicorn: 0.32.0\r\nuvloop: 0.21.0\r\nvllm: 0.6.3.dev13+g16583707.d20241022\r\nopenai: 1.58.1\r\nanthropic: 0.42.0\r\ndecord: 0.6.0\r\nAMD Topology: \r\n\r\n\r\n============================ ROCm System Management Interface ============================\r\n=============================== Link Type between two GPUs ===============================\r\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \r\nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \r\nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \r\nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         \r\nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         \r\nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         \r\nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         \r\nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         \r\nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            \r\n================================== End of ROCm SMI Log ===================================\r\n\r\nulimit soft: 1048576\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-12-26T13:03:50+00:00",
    "closed_at": "2025-01-09T04:09:06+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2595/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2595"
  },
  {
    "number": 956,
    "title": "[Bug] When using sglang as the inference framework, if a word starting with \"\\n\" appears in the stop parameter, the sglang will  Missing '\\n' during inference",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [X] 2. The bug has not been fixed in the latest version.\r\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n\r\n### Describe the bug\r\n\r\nWhen using sglang as the inference framework, if a word starting with \"\\n\" appears in the stop parameter, the sglang will not wrap during inference\u3002\r\nEG:\r\nprompt = \u8bf7\u6362\u884c\u8f93\u51fa1-10\u4e2a\u6570\u5b57\r\nstop = ['<|endoftext|>', '<|im_end|>', '<|im_start|>']\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n\r\nprompt = \u8bf7\u6362\u884c\u8f93\u51fa1-10\u4e2a\u6570\u5b57\r\nstop = ['\\n<|endoftext|>', '<|im_end|>', '<|im_start|>']\r\n12345678910\r\n\r\n\"\\n\" can be followed by any character, and there will be no line break.\r\n\r\n### Reproduction\r\n\r\nOS: Linux x64  \r\nGPU: A100\r\npython\uff1a3.10\r\nsglang\uff1a0.2.7\r\nLLM model: Qwen2-72B-lora-awq-4bit\r\ncmd: \r\npython -m fastchat.serve.controller --host localhost --port 44000\r\n\r\npython -m fastchat.serve.vllm_worker --model-path ${MODEL_PATH} --max-model-len 8192 --worker-address \"http://0.0.0.0:22006\" --port 22006 --model-names \"qwen-latest\" --controller-address \"http://localhost:44000\"\r\n\r\npython -m fastchat.serve.openai_api_server --host 0.0.0.0 --port 21003 --controller-address \"http://localhost:44000\"\r\n\r\nThen run code\r\n\"\"\"\r\ndef test_open_ai(prompt: str, stream: bool = False, model: str = \"qwen-latest\"):\r\n\r\n    openai.api_key = \"LTAI5t6C5QzrRfy5A4Ug4ujD\"  # Not support yet\r\n    openai.api_base = \"http://127.0.0.1:21003/v1\"\r\n    completion = openai.ChatCompletion.create(\r\n        model=model,\r\n        messages=[{'role': 'user', 'content': prompt}],\r\n        temperature=0.7,\r\n        top_p=1.0,\r\n        n=1,\r\n        max_tokens=None,\r\n        stream=False,\r\n        presence_penalty=0.0,\r\n        frequency_penalty=0.0,\r\n        user=None,\r\n        meta={},\r\n        service=\"sas\",\r\n        scenario=\"Chat\",\r\n        stop_token_ids=[151643, 151644, 151645],\r\n        stop=['\\n<|endoftext|>', '<|im_end|>', '<|im_start|>'],\r\n        max_new_tokens=8192,\r\n    )\r\n\r\n    if not stream:\r\n        answer_md = completion.choices[0].message.content\r\n        print(answer_md)\r\n        return answer_md\r\n    else:\r\n        pass\r\n\r\n\"\"\"\r\n\r\n\r\n\r\n\r\n\r\n### Environment\r\n\r\n```Shell\r\nPython: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0,1,2,3: NVIDIA A100-SXM4-80GB\r\nCUDA_HOME: /usr/local/cuda-12.2/\r\nNVCC: Cuda compilation tools, release 12.2, V12.2.140\r\nCUDA Driver Version: 535.183.01\r\n535.183.01\r\n535.183.01\r\n535.183.01\r\nPyTorch: 2.3.1+cu121\r\nsglang: 0.2.7\r\nflashinfer: 0.1.3\r\nrequests: 2.32.3\r\ntqdm: 4.66.4\r\nnumpy: 1.26.4\r\naiohttp: 3.10.0\r\nfastapi: 0.111.1\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.23.4\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.3\r\nuvloop: 0.19.0\r\nzmq: 26.0.3\r\nvllm: 0.5.3.post1\r\nopenai: 1.37.1\r\nanthropic: 0.32.0\r\nNVIDIA Topology: \r\n\tGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV12\tNV12\tNV12\t0-63\t0\t\tN/A\r\nGPU1\tNV12\t X \tNV12\tNV12\t0-63\t0\t\tN/A\r\nGPU2\tNV12\tNV12\t X \tNV12\t0-63\t0\t\tN/A\r\nGPU3\tNV12\tNV12\tNV12\t X \t0-63\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nulimit soft: 65535\r\n```\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-07T02:50:15+00:00",
    "closed_at": "2024-08-07T11:03:21+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/956/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/956"
  },
  {
    "number": 4263,
    "title": "Low Inference Speed with Bitsandbytes and AWQ Quantized Models",
    "body": "**Description**\nI'm experiencing low inference speed when running bitsandbytes quantized and AWQ quantized models with sglang. The throughput remains around 31 tokens per second, which seems suboptimal. \n\nHere are the details of my setup:\n\n**Model Serving Code:**\n```\nimport requests\nimport os\n\nfrom sglang import assistant_begin, assistant_end\nfrom sglang import assistant, function, gen, system, user\nfrom sglang import image\nfrom sglang import RuntimeEndpoint, set_default_backend\nfrom sglang.srt.utils import load_image\nfrom sglang.test.test_utils import is_in_ci\nfrom sglang.utils import print_highlight, terminate_process, wait_for_server\n\nif is_in_ci():\n    from sglang.docs.frontend.patch import launch_server_cmd\nelse:\n    from sglang.utils import launch_server_cmd\n\nserver_process, port = launch_server_cmd(\n    \"python -m sglang.launch_server --model-path /LLM/model/Nvidia-Llama-3.1-Nemotron-70B-Instruct-HF-AWQ-INT4/ --tp 2 --tokenizer-mode auto --mem-fraction-static 0.9 --disable-cuda-graph\"\n)\n\nwait_for_server(f\"http://localhost:{port}\")\nprint(f\"Server started on http://localhost:{port}\")\n```\n\n**Inference Code:**\n\n```\nimport openai\n\nclient = openai.Client(base_url=f\"http://localhost:{port}/v1\", api_key=\"None\")\nresponse = client.chat.completions.create(\n    model=\"/LLM/model/Nvidia-Llama-3.1-Nemotron-70B-Instruct-HF-AWQ-INT4/\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a story with 1000 words.\"},\n    ],\n    temperature=0,\n    max_tokens=8192,\n    stream=True,\n)\n\nfor chunk in response:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n**Observed Performance:**\nThe inference speed is around 31 tokens per second. Here is the output:\n[2025-03-10 14:56:39 TP0] Decode batch. #running-req: 1, #token: 57, token usage: 0.00, gen throughput (token/s): 1.79, #queue-req: 0\n[2025-03-10 14:56:41 TP0] Decode batch. #running-req: 1, #token: 97, token usage: 0.00, gen throughput (token/s): 31.70, #queue-req: 0\n[2025-03-10 14:56:42 TP0] Decode batch. #running-req: 1, #token: 137, token usage: 0.00, gen throughput (token/s): 31.62, #queue-req: 0\n...\n[2025-03-10 14:57:24 TP0] Decode batch. #running-req: 1, #token: 1457, token usage: 0.01, gen throughput (token/s): 31.18, #queue-req: 0\n\n**Expected Behavior:**\nI expect an inference speed of around 300-400 tokens per second based on my experience with non-quantized models.\n\n**Environment**\nHardware: \n2xL40s GPUs (48 GB VRAM for each)\n\nSoftware:\nsglang version: 0.4.3.post2\nCUDA version: 12.8\nPyTorch version: 2.5.1\n\n**Additional Notes**\n\nIs this expected behavior for quantized models, or is there an issue with how they are being handled by sglang?\n\nAre there any optimizations I can apply to improve performance?\n\nAny insights or suggestions would be greatly appreciated!\n\nThanks!\n",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-10T12:11:20+00:00",
    "closed_at": "2025-03-11T09:15:35+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4263/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4263"
  },
  {
    "number": 3339,
    "title": "[Bug] how to solve illegal memory access in moe_align_block_size kernel optimization",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nAs mentioned in [lines of code](https://github.com/sgl-project/sglang/blob/main/sgl-kernel/src/sgl-kernel/csrc/moe_align_kernel.cu#L80-L90), when attempting to optimize the most expensive write operation of `sorted_token_ids` in the `moe_align_block_size` of DeepSeek V3, using multiple Thread Blocks instead of a single Block triggers an `illegal global memory write access` . Even directly replacing these lines of code with the Triton Stage4 kernel [here](https://github.com/sgl-project/sglang/blob/main/sgl-kernel/tests/test_moe_align.py#L64) results in the same` illegal global memory write access` within the Triton kernel. This issue has persisted for nearly a month, and the cause has not yet been identified, so I am opening an issue to seek help.\n\nBased on the kernel benchmark results from H200, the performance of the current CUDA kernel degrades to be slower than Triton when the number of tokens is `>= 4096`. On H100, the performance degrades to be slower than Triton when the number of tokens is `>= 32768`. The reason is the [lines of code](https://github.com/sgl-project/sglang/blob/main/sgl-kernel/src/sgl-kernel/csrc/moe_align_kernel.cu#L80-L90) I pointed out earlier. if multiple Thread Blocks are used, the fastest execution speed can be achieved in all token scenarios.\n\nAdditionally, the original implementation of the VLLM kernel exhibits similar behavior, even though it uses shared memory for counting when the number of tokens is <= 65536. According to my benchmark results, its performance is still significantly slower than both the sgl-kernel CUDA operators and the Triton version, so I will not discuss the `moe_align_block_size` kernel in VLLM.\n\nThe code below is the result of modifying the performance-critical lines to use element-wise multiple Thread Blocks.\n\n```c++\n\n// Note: For the moe_align_kernel, the primary bottleneck lies in the atomic add and non-coalesced memory writes here.\n  // If these operations can be performed using multiple blocks, similar to the Triton version, the performance of this\n  // kernel can achieve state-of-the-art performance across all token cases. However, once multiple blocks are used,\n  // illegal memory access occurs. Even replacing these lines of code with the stage 4 kernel from the Triton version\n  // results in the same issue, and a correct solution has not yet been found.\n  // for (int i = start_idx; i < numel && i < start_idx + tokens_per_thread; ++i) {\n  //  int32_t expert_id = topk_ids[i];\n  //  int32_t rank_post_pad = atomicAdd(&local_offsets[expert_id], 1);\n  //  sorted_token_ids[rank_post_pad] = i;\n  // }\n\ntemplate <typename scalar_t>\n__global__ void sort_token_ids_kernel(scalar_t* __restrict__ topk_ids, \n                                    int32_t* sorted_token_ids,\n                                    int32_t* token_cnts_buffer,\n                                    const int32_t* cumsum,\n                                    int32_t num_experts,\n                                    size_t numel,\n                                    int32_t tokens_per_block) {\n    const size_t start_idx = blockIdx.x * tokens_per_block;\n    const size_t end_idx = min(start_idx + tokens_per_block, numel);\n    \n    const size_t off_t = blockIdx.x * num_experts;\n    \n    for (size_t i = start_idx + threadIdx.x; i < end_idx; i += blockDim.x) {\n        int expert_id = topk_ids[i];\n        int token_cnt = atomicAdd(&token_cnts_buffer[off_t + expert_id], 1);\n        int rank_post_pad = token_cnt + cumsum[expert_id];\n        sorted_token_ids[rank_post_pad] = i;\n    }\n}\n\nconst int tokens_per_block = CEILDIV(topk_ids.numel(), num_experts);\nsort_kernel<<<num_experts, 256, 0, stream>>>(\n    topk_ids.data_ptr<scalar_t>(),\n    sorted_token_ids.data_ptr<int32_t>(),\n    token_cnts_buffer.data_ptr<int32_t>(),\n    cumsum_buffer.data_ptr<int32_t>(),\n    num_experts,\n    topk_ids.numel(),\n    tokens_per_block);\n```\n\nThe error message obtained using COMPUTE-SANITIZER is: Invalid global write of size 4 bytes. And it happend in line code `sorted_token_ids[rank_post_pad] = i;`.\n\nBelow are the benchmark results for the sgl-kernel moe_align_block_size and the Triton version on H100 and H200:\n\n### H100\n\n```shell\npython3 /opt/dlami/nvme/bbuf/sglang/benchmark/kernels/fused_moe_triton/benchmark_moe_align_blocks.py --save_path /opt/dlami/nvme/bbuf/configs\n\u2705 CUDA and Triton implementations match\nmoe-align-block-size-performance:\n     batch_size  seq_len         CUDA       Triton\n0           1.0      1.0    24.224000    73.408000\n1           1.0      2.0    24.192000    72.127998\n2           1.0      4.0    24.256000    71.648002\n3           1.0      8.0    24.192000    73.023997\n4           1.0     16.0    24.224000    72.191998\n5           1.0     32.0    24.192000    71.392000\n6           1.0     64.0    24.288001    73.344000\n7           1.0    128.0    24.672000    73.536001\n8           1.0    256.0    24.800001    72.768003\n9           1.0    512.0    25.152000    72.127998\n10          1.0   1024.0    25.536001    74.239999\n11          1.0   2048.0    26.176000    73.919997\n12          1.0   4096.0    28.031999    72.319999\n13          1.0   8192.0    32.687999    72.864003\n14          1.0  16384.0    45.279998    73.760003\n15          1.0  32768.0    **85.104004**    78.560002\n16          2.0      1.0    24.192000    72.400004\n17          2.0      2.0    24.192000    73.472001\n18          2.0      4.0    24.256000    79.584002\n19          2.0      8.0    24.288001    79.392001\n20          2.0     16.0    24.256000    79.167999\n21          2.0     32.0    24.320001    79.552002\n22          2.0     64.0    24.704000    77.616006\n23          2.0    128.0    24.831999    78.143999\n24          2.0    256.0    25.119999    79.135999\n25          2.0    512.0    25.536001    78.528002\n26          2.0   1024.0    26.240001    79.183996\n27          2.0   2048.0    28.031999    78.656003\n28          2.0   4096.0    32.639999    78.863993\n29          2.0   8192.0    45.184001    80.895998\n30          2.0  16384.0    **85.280001**    79.903997\n31          2.0  32768.0   **148.287997**   101.120003\n32          4.0      1.0    24.288001    79.584002\n33          4.0      2.0    24.288001    80.767997\n34          4.0      4.0    24.320001    79.552002\n35          4.0      8.0    24.288001    79.584002\n36          4.0     16.0    24.320001    79.775997\n37          4.0     32.0    24.672000    77.791996\n38          4.0     64.0    24.800001    78.528002\n39          4.0    128.0    25.184000    78.111999\n40          4.0    256.0    25.664000    79.007998\n41          4.0    512.0    26.272001    78.079998\n42          4.0   1024.0    28.000001    79.360001\n43          4.0   2048.0    32.559998    78.879997\n44          4.0   4096.0    45.248002    77.904001\n45          4.0   8192.0    **85.344002**    80.400005\n46          4.0  16384.0   **148.256004**   101.120003\n47          4.0  32768.0   **272.832006**   146.431997\n48          8.0      1.0    23.584001    79.967998\n49          8.0      2.0    23.664000    78.896001\n50          8.0      4.0    23.568001    79.728007\n51          8.0      8.0    23.552001    75.167999\n52          8.0     16.0    23.615999    74.975997\n53          8.0     32.0    23.680000    75.488001\n54          8.0     64.0    24.000000    76.159999\n55          8.0    128.0    24.512000    75.407997\n56          8.0    256.0    25.664000    76.031998\n57          8.0    512.0    27.807999    75.584002\n58          8.0   1024.0    32.639999    75.231999\n59          8.0   2048.0    44.960000    74.879996\n60          8.0   4096.0    **85.472003**    77.951998\n61          8.0   8192.0   **149.215996**    99.104002\n62          8.0  16384.0   **273.791999**   146.752000\n63          8.0  32768.0   **527.520001**   238.463998\n64         16.0      1.0    23.552001    75.263999\n65         16.0      2.0    23.552001    75.151995\n66         16.0      4.0    23.600001    74.720003\n67         16.0      8.0    23.712000    75.903997\n68         16.0     16.0    23.744000    75.231999\n69         16.0     32.0    24.064001    75.360000\n70         16.0     64.0    24.351999    75.839996\n71         16.0    128.0    25.632000    75.199999\n72         16.0    256.0    27.775999    74.816003\n73         16.0    512.0    32.671999    75.360000\n74         16.0   1024.0    44.976000    75.520001\n75         16.0   2048.0    **85.408002**    77.280000\n76         16.0   4096.0   **147.440001**    99.072002\n77         16.0   8192.0   **274.048001**   146.559998\n78         16.0  16384.0   **525.600016**   238.399997\n79         16.0  32768.0  **1022.639990**   413.792014\n80         32.0      1.0    23.600001    76.800004\n81         32.0      2.0    23.584001    77.119999\n82         32.0      4.0    23.680000    76.959997\n83         32.0      8.0    23.744000    75.312003\n84         32.0     16.0    24.032000    76.736003\n85         32.0     32.0    24.383999    74.975997\n86         32.0     64.0    25.664000    75.920001\n87         32.0    128.0    27.775999    75.776003\n88         32.0    256.0    32.639999    76.991998\n89         32.0    512.0    45.120001    74.720003\n90         32.0   1024.0    **85.712001**    78.143999\n91         32.0   2048.0   **147.456005**    99.200003\n92         32.0   4096.0   **273.631990**   146.704003\n93         32.0   8192.0   **525.983989**   238.527998\n94         32.0  16384.0  **1023.823977**   413.856000\n95         32.0  32768.0  **2023.808002**   769.919991\n96         64.0      1.0    23.536000    75.999998\n97         64.0      2.0    23.712000    75.135998\n98         64.0      4.0    23.744000    75.648002\n99         64.0      8.0    24.064001    75.648002\n100        64.0     16.0    24.383999    75.552002\n101        64.0     32.0    25.664000    75.712003\n102        64.0     64.0    27.775999    75.135998\n103        64.0    128.0    32.575998    75.263999\n104        64.0    256.0    45.040000    76.672003\n105        64.0    512.0    **85.327998**    77.344000\n106        64.0   1024.0   **147.599995**    99.136002\n107        64.0   2048.0   **274.080008**   146.623999\n108        64.0   4096.0   **525.056005**   238.912001\n109        64.0   8192.0  **1024.320006**   413.695991\n110        64.0  16384.0  **2019.808054**   769.919991\n111        64.0  32768.0  **4007.391930**  1482.624054\n112       128.0      1.0    23.647999    77.696003\n113       128.0      2.0    23.744000    78.143999\n114       128.0      4.0    24.064001    79.616003\n115       128.0      8.0    24.400000    78.240000\n116       128.0     16.0    25.664000    78.720003\n117       128.0     32.0    27.807999    78.847997\n118       128.0     64.0    32.416001    78.624003\n119       128.0    128.0    45.088001    79.167999\n120       128.0    256.0    **85.376002**    79.744004\n121       128.0    512.0   **147.295997**    99.104002\n122       128.0   1024.0   **273.983985**   146.495998\n123       128.0   2048.0   **526.848018**   238.591999\n124       128.0   4096.0  **1023.839951**   413.807988\n125       128.0   8192.0  **2023.328066**   769.343972\n126       128.0  16384.0  **4007.056236**  1481.424093\n127       128.0  32768.0  **8052.288055**  2958.911896\n```\n\n### H200\n\n```shell\n\u2705 CUDA and Triton implementations match\nmoe-align-block-size-performance:\n     batch_size  seq_len          CUDA        Triton\n0           1.0      1.0     21.760000     40.800001\n1           1.0      2.0     21.760000     40.895998\n2           1.0      4.0     21.728000     40.991999\n3           1.0      8.0     21.888001     41.312002\n4           1.0     16.0     21.919999     41.280001\n5           1.0     32.0     22.112001     41.536000\n6           1.0     64.0     22.528000     42.656001\n7           1.0    128.0     23.040000     42.240001\n8           1.0    256.0     23.808001     44.064000\n9           1.0    512.0     26.095999     48.448000\n10          1.0   1024.0     30.928001     54.079998\n11          1.0   2048.0     43.807998     62.240001\n12          1.0   4096.0     83.903998     76.079994\n13          1.0   8192.0    147.200003     99.232003\n14          1.0  16384.0    273.247987    145.536005\n15          1.0  32768.0    527.999997    240.400001\n16          2.0      1.0     21.760000     40.927999\n17          2.0      2.0     21.760000     40.959999\n18          2.0      4.0     21.919999     41.216001\n19          2.0      8.0     21.919999     41.216001\n20          2.0     16.0     22.080000     41.503999\n21          2.0     32.0     22.496000     42.624000\n22          2.0     64.0     23.072001     42.304002\n23          2.0    128.0     23.696000     44.032000\n24          2.0    256.0     26.079999     48.735999\n25          2.0    512.0     30.880000     54.175999\n26          2.0   1024.0     43.807998     62.144000\n27          2.0   2048.0     83.935998     76.143995\n28          2.0   4096.0    146.752000     99.168003\n29          2.0   8192.0    272.864014    145.311996\n30          2.0  16384.0    528.352022    240.208000\n31          2.0  32768.0   1028.576016    414.303988\n32          4.0      1.0     21.760000     41.120000\n33          4.0      2.0     21.919999     41.216001\n34          4.0      4.0     21.919999     41.343998\n35          4.0      8.0     22.112001     41.728001\n36          4.0     16.0     22.592001     42.720001\n37          4.0     32.0     23.072001     42.240001\n38          4.0     64.0     23.808001     44.128001\n39          4.0    128.0     26.112000     48.480000\n40          4.0    256.0     30.912001     53.920001\n41          4.0    512.0     43.776002     62.128000\n42          4.0   1024.0     83.999999     76.095998\n43          4.0   2048.0    146.975994     99.168003\n44          4.0   4096.0    272.992015    145.344004\n45          4.0   8192.0    527.487993    240.288004\n46          4.0  16384.0   1028.576016    414.144009\n47          4.0  32768.0   2034.463882    773.696005\n48          8.0      1.0     21.856001     41.216001\n49          8.0      2.0     21.919999     41.280001\n50          8.0      4.0     22.080000     41.600000\n51          8.0      8.0     22.496000     42.624000\n52          8.0     16.0     23.008000     42.272002\n53          8.0     32.0     23.744000     44.144001\n54          8.0     64.0     26.079999     48.512001\n55          8.0    128.0     31.040000     54.079998\n56          8.0    256.0     43.807998     62.176000\n57          8.0    512.0     84.063999     76.095998\n58          8.0   1024.0    146.944001     99.072002\n59          8.0   2048.0    272.576004    145.408005\n60          8.0   4096.0    528.927982    240.768000\n61          8.0   8192.0   1028.192043    414.112002\n62          8.0  16384.0   2034.240007    774.047971\n63          8.0  32768.0   4024.064064   1488.415956\n64         16.0      1.0     21.919999     41.343998\n65         16.0      2.0     22.112001     41.503999\n66         16.0      4.0     22.496000     42.656001\n67         16.0      8.0     23.104001     42.240001\n68         16.0     16.0     23.808001     43.968000\n69         16.0     32.0     26.079999     48.512001\n70         16.0     64.0     30.912001     54.079998\n71         16.0    128.0     43.807998     62.240001\n72         16.0    256.0     84.160000     75.999998\n73         16.0    512.0    146.975994     99.136002\n74         16.0   1024.0    273.088008    145.503998\n75         16.0   2048.0    528.447986    240.416005\n76         16.0   4096.0   1028.671980    414.303988\n77         16.0   8192.0   2033.983946    774.111986\n78         16.0  16384.0   4021.408081   1488.991976\n79         16.0  32768.0   8028.175354   2932.768106\n80         32.0      1.0     22.080000     41.600000\n81         32.0      2.0     22.528000     42.624000\n82         32.0      4.0     23.072001     42.272002\n83         32.0      8.0     23.808001     44.128001\n84         32.0     16.0     26.079999     48.512001\n85         32.0     32.0     31.008000     53.920001\n86         32.0     64.0     43.807998     62.208001\n87         32.0    128.0     84.096000     76.191999\n88         32.0    256.0    147.487998     99.136002\n89         32.0    512.0    272.639990    145.344004\n90         32.0   1024.0    528.223991    240.799993\n91         32.0   2048.0   1027.008057    414.175987\n92         32.0   4096.0   2032.351971    773.151994\n93         32.0   8192.0   4028.880119   1489.567995\n94         32.0  16384.0   8026.767731   2924.767971\n95         32.0  32768.0  16051.088333   6203.711987\n96         64.0      1.0     22.496000     42.784002\n97         64.0      2.0     23.104001     42.304002\n98         64.0      4.0     23.808001     44.032000\n99         64.0      8.0     26.112000     48.624001\n100        64.0     16.0     30.975999     53.984001\n101        64.0     32.0     43.807998     62.304001\n102        64.0     64.0     84.096000     75.871997\n103        64.0    128.0    146.880001     99.200003\n104        64.0    256.0    272.704005    145.472005\n105        64.0    512.0    528.128028    240.240008\n106        64.0   1024.0   1029.024005    414.400011\n107        64.0   2048.0   2034.080029    772.415996\n108        64.0   4096.0   4032.608032   1490.015984\n109        64.0   8192.0   8031.744003   2930.560112\n110        64.0  16384.0  16027.168274   6208.992004\n111        64.0  32768.0  32008.895874  12419.839859\n112       128.0      1.0     23.056000     42.399999\n113       128.0      2.0     23.808001     44.160001\n114       128.0      4.0     26.112000     48.864000\n115       128.0      8.0     30.944001     54.143999\n116       128.0     16.0     43.807998     62.304001\n117       128.0     32.0     83.792001     75.967997\n118       128.0     64.0    148.031995     99.136002\n119       128.0    128.0    273.600012    145.328000\n120       128.0    256.0    527.872026    240.400001\n121       128.0    512.0   1028.464079    414.207995\n122       128.0   1024.0   2031.935930    772.800028\n123       128.0   2048.0   4032.127857   1489.120007\n124       128.0   4096.0   8032.719612   2931.519985\n125       128.0   8192.0  16011.951447   6202.784061\n126       128.0  16384.0  31969.087601  12423.839569\n```\n\n\n\n\n\n### Reproduction\n\nChange the code above and run `python3 benchmark/kernels/fused_moe_triton/benchmark_deepseekv3_moe_align_blocks.py`.\n\n### Environment\n\nNo Limit.",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-06T09:16:45+00:00",
    "closed_at": "2025-02-06T14:57:39+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3339/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3339"
  },
  {
    "number": 6921,
    "title": "[Bug] Mixed chunk causes crash",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen I add `enable-mixed-chunk\" it will eventually cause crash in the.\n\n```\nsglang  | [2025-06-06 10:23:32 TP0] Prefill batch. #new-seq: 2, #new-token: 8191, #cached-token: 0, token usage: 0.01, #running-req: 1, #queue-req: 7\nsglang  | [2025-06-06 10:23:36 TP0] Prefill batch. #new-seq: 3, #new-token: 8190, #cached-token: 0, token usage: 0.02, #running-req: 2, #queue-req: 5\nsglang  | [2025-06-06 10:23:37 TP1] Scheduler hit an exception: Traceback (most recent call last):\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2271, in run_scheduler_process\nsglang  |     scheduler.event_loop_normal()\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nsglang  |     return func(*args, **kwargs)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 636, in event_loop_normal\nsglang  |     batch = self.get_next_batch_to_run()\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1297, in get_next_batch_to_run\nsglang  |     new_batch = self.get_new_batch_prefill()\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1458, in get_new_batch_prefill\nsglang  |     new_batch.mix_with_running(self.running_batch)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/schedule_batch.py\", line 1268, in mix_with_running\nsglang  |     out_cache_loc = torch.cat([self.out_cache_loc, running_batch.out_cache_loc])\nsglang  | TypeError: expected Tensor as element 1 in argument 0, but got NoneType\nsglang  | \nsglang  | [2025-06-06 10:23:37] Received sigquit from a child process. It usually means the child failed.\nsglang  | [2025-06-06 10:23:37 TP3] Scheduler hit an exception: Traceback (most recent call last):\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2271, in run_scheduler_process\nsglang  |     scheduler.event_loop_normal()\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nsglang  |     return func(*args, **kwargs)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 636, in event_loop_normal\nsglang  |     batch = self.get_next_batch_to_run()\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1297, in get_next_batch_to_run\nsglang  |     new_batch = self.get_new_batch_prefill()\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1458, in get_new_batch_prefill\nsglang  |     new_batch.mix_with_running(self.running_batch)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/schedule_batch.py\", line 1268, in mix_with_running\nsglang  |     out_cache_loc = torch.cat([self.out_cache_loc, running_batch.out_cache_loc])\nsglang  | TypeError: expected Tensor as element 1 in argument 0, but got NoneType\nsglang  | \nsglang  | [2025-06-06 10:23:37] Received sigquit from a child process. It usually means the child failed.\nsglang  | [2025-06-06 10:23:37 TP2] Scheduler hit an exception: Traceback (most recent call last):\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2271, in run_scheduler_process\nsglang  |     scheduler.event_loop_normal()\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nsglang  |     return func(*args, **kwargs)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 636, in event_loop_normal\nsglang  |     batch = self.get_next_batch_to_run()\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1297, in get_next_batch_to_run\nsglang  |     new_batch = self.get_new_batch_prefill()\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1458, in get_new_batch_prefill\nsglang  |     new_batch.mix_with_running(self.running_batch)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/schedule_batch.py\", line 1268, in mix_with_running\nsglang  |     out_cache_loc = torch.cat([self.out_cache_loc, running_batch.out_cache_loc])\nsglang  | TypeError: expected Tensor as element 1 in argument 0, but got NoneType\nsglang  | \nsglang  | [2025-06-06 10:23:37] Received sigquit from a child process. It usually means the child failed.\nsglang  | [2025-06-06 10:23:37 TP5] Scheduler hit an exception: Traceback (most recent call last):\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2271, in run_scheduler_process\nsglang  |     scheduler.event_loop_normal()\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nsglang  |     return func(*args, **kwargs)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 636, in event_loop_normal\nsglang  |     batch = self.get_next_batch_to_run()\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1297, in get_next_batch_to_run\nsglang  |     new_batch = self.get_new_batch_prefill()\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1458, in get_new_batch_prefill\nsglang  |     new_batch.mix_with_running(self.running_batch)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/schedule_batch.py\", line 1268, in mix_with_running\nsglang  |     out_cache_loc = torch.cat([self.out_cache_loc, running_batch.out_cache_loc])\nsglang  | TypeError: expected Tensor as element 1 in argument 0, but got NoneType\nsglang  | \nsglang  | [2025-06-06 10:23:37] Received sigquit from a child process. It usually means the child failed.\nsglang  | [2025-06-06 10:23:37 TP4] Scheduler hit an exception: Traceback (most recent call last):\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2271, in run_scheduler_process\nsglang  |     scheduler.event_loop_normal()\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nsglang  |     return func(*args, **kwargs)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 636, in event_loop_normal\nsglang  |     batch = self.get_next_batch_to_run()\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1297, in get_next_batch_to_run\nsglang  |     new_batch = self.get_new_batch_prefill()\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1458, in get_new_batch_prefill\nsglang  |     new_batch.mix_with_running(self.running_batch)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/schedule_batch.py\", line 1268, in mix_with_running\nsglang  |     out_cache_loc = torch.cat([self.out_cache_loc, running_batch.out_cache_loc])\nsglang  | TypeError: expected Tensor as element 1 in argument 0, but got NoneType\nsglang  | \nsglang  | [2025-06-06 10:23:37 TP0] Scheduler hit an exception: Traceback (most recent call last):\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2271, in run_scheduler_process\nsglang  |     scheduler.event_loop_normal()\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nsglang  |     return func(*args, **kwargs)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 636, in event_loop_normal\nsglang  |     batch = self.get_next_batch_to_run()\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1297, in get_next_batch_to_run\nsglang  |     new_batch = self.get_new_batch_prefill()\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1458, in get_new_batch_prefill\nsglang  |     new_batch.mix_with_running(self.running_batch)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/schedule_batch.py\", line 1268, in mix_with_running\nsglang  |     out_cache_loc = torch.cat([self.out_cache_loc, running_batch.out_cache_loc])\nsglang  | TypeError: expected Tensor as element 1 in argument 0, but got NoneType\nsglang  | \nsglang  | [2025-06-06 10:23:37] Received sigquit from a child process. It usually means the child failed.\nsglang  | [2025-06-06 10:23:37 TP7] Scheduler hit an exception: Traceback (most recent call last):\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2271, in run_scheduler_process\nsglang  |     scheduler.event_loop_normal()\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nsglang  |     return func(*args, **kwargs)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 636, in event_loop_normal\nsglang  |     batch = self.get_next_batch_to_run()\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1297, in get_next_batch_to_run\nsglang  |     new_batch = self.get_new_batch_prefill()\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1458, in get_new_batch_prefill\nsglang  |     new_batch.mix_with_running(self.running_batch)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/schedule_batch.py\", line 1268, in mix_with_running\nsglang  |     out_cache_loc = torch.cat([self.out_cache_loc, running_batch.out_cache_loc])\nsglang  | TypeError: expected Tensor as element 1 in argument 0, but got NoneType\nsglang  | \nsglang  | [2025-06-06 10:23:37] Received sigquit from a child process. It usually means the child failed.\nsglang  | [2025-06-06 10:23:37 TP6] Scheduler hit an exception: Traceback (most recent call last):\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2271, in run_scheduler_process\nsglang  |     scheduler.event_loop_normal()\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nsglang  |     return func(*args, **kwargs)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 636, in event_loop_normal\nsglang  |     batch = self.get_next_batch_to_run()\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1297, in get_next_batch_to_run\nsglang  |     new_batch = self.get_new_batch_prefill()\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1458, in get_new_batch_prefill\nsglang  |     new_batch.mix_with_running(self.running_batch)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/schedule_batch.py\", line 1268, in mix_with_running\nsglang  |     out_cache_loc = torch.cat([self.out_cache_loc, running_batch.out_cache_loc])\nsglang  | TypeError: expected Tensor as element 1 in argument 0, but got NoneType\nsglang  | \nsglang  | [2025-06-06 10:23:37] Received sigquit from a child process. It usually means the child failed.\nsglang  | [2025-06-06 10:23:37] ERROR:    Traceback (most recent call last):\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/_common.py\", line 456, in wrapper\nsglang  |     ret = self._cache[fun]\nsglang  | AttributeError: 'Process' object has no attribute '_cache'\nsglang  | \nsglang  | During handling of the above exception, another exception occurred:\nsglang  | \nsglang  | Traceback (most recent call last):\nsglang  |   File \"/usr/lib/python3.10/asyncio/runners.py\", line 44, in run\nsglang  |     return loop.run_until_complete(main)\nsglang  |   File \"uvloop/loop.pyx\", line 1512, in uvloop.loop.Loop.run_until_complete\nsglang  |   File \"uvloop/loop.pyx\", line 1505, in uvloop.loop.Loop.run_until_complete\nsglang  |   File \"uvloop/loop.pyx\", line 1379, in uvloop.loop.Loop.run_forever\nsglang  |   File \"uvloop/loop.pyx\", line 557, in uvloop.loop.Loop._run\nsglang  |   File \"uvloop/handles/poll.pyx\", line 216, in uvloop.loop.__on_uvpoll_event\nsglang  |   File \"uvloop/cbhandles.pyx\", line 83, in uvloop.loop.Handle._run\nsglang  |   File \"uvloop/cbhandles.pyx\", line 66, in uvloop.loop.Handle._run\nsglang  |   File \"uvloop/loop.pyx\", line 399, in uvloop.loop.Loop._read_from_self\nsglang  |   File \"uvloop/loop.pyx\", line 404, in uvloop.loop.Loop._invoke_signals\nsglang  |   File \"uvloop/loop.pyx\", line 379, in uvloop.loop.Loop._ceval_process_signals\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/engine.py\", line 511, in sigquit_handler\nsglang  |     kill_process_tree(os.getpid())\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/utils.py\", line 714, in kill_process_tree\nsglang  |     children = itself.children(recursive=True)\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 966, in children\nsglang  |     ppid_map = _ppid_map()\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/_pslinux.py\", line 1621, in ppid_map\nsglang  |     for pid in pids():\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/_pslinux.py\", line 1579, in pids\nsglang  |     return [int(x) for x in os.listdir(path) if x.isdigit()]\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/engine.py\", line 511, in sigquit_handler\nsglang  |     kill_process_tree(os.getpid())\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/utils.py\", line 714, in kill_process_tree\nsglang  |     children = itself.children(recursive=True)\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 989, in children\nsglang  |     pid = stack.pop()\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/engine.py\", line 511, in sigquit_handler\nsglang  |     kill_process_tree(os.getpid())\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/utils.py\", line 714, in kill_process_tree\nsglang  |     children = itself.children(recursive=True)\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 966, in children\nsglang  |     ppid_map = _ppid_map()\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/_pslinux.py\", line 1623, in ppid_map\nsglang  |     with open_binary(f\"{procfs_path}/{pid}/stat\") as f:\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/_common.py\", line 766, in open_binary\nsglang  |     return open(fname, \"rb\", buffering=FILE_READ_BUFFER_SIZE)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/engine.py\", line 511, in sigquit_handler\nsglang  |     kill_process_tree(os.getpid())\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/utils.py\", line 714, in kill_process_tree\nsglang  |     children = itself.children(recursive=True)\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 966, in children\nsglang  |     ppid_map = _ppid_map()\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/_pslinux.py\", line 1623, in ppid_map\nsglang  |     with open_binary(f\"{procfs_path}/{pid}/stat\") as f:\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/engine.py\", line 508, in sigquit_handler\nsglang  |     logger.error(\nsglang  |   File \"/usr/lib/python3.10/logging/__init__.py\", line 1506, in error\nsglang  |     self._log(ERROR, msg, args, **kwargs)\nsglang  |   File \"/usr/lib/python3.10/logging/__init__.py\", line 1612, in _log\nsglang  |     fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)\nsglang  |   File \"/usr/lib/python3.10/logging/__init__.py\", line 1568, in findCaller\nsglang  |     filename = os.path.normcase(co.co_filename)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/engine.py\", line 511, in sigquit_handler\nsglang  |     kill_process_tree(os.getpid())\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/utils.py\", line 714, in kill_process_tree\nsglang  |     children = itself.children(recursive=True)\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 966, in children\nsglang  |     ppid_map = _ppid_map()\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/_pslinux.py\", line 1623, in ppid_map\nsglang  |     with open_binary(f\"{procfs_path}/{pid}/stat\") as f:\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/_common.py\", line 766, in open_binary\nsglang  |     return open(fname, \"rb\", buffering=FILE_READ_BUFFER_SIZE)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/engine.py\", line 511, in sigquit_handler\nsglang  |     kill_process_tree(os.getpid())\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/utils.py\", line 714, in kill_process_tree\nsglang  |     children = itself.children(recursive=True)\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 998, in children\nsglang  |     child = Process(child_pid)\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 317, in __init__\nsglang  |     self._init(pid)\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 350, in _init\nsglang  |     self._ident = self._get_ident()\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 390, in _get_ident\nsglang  |     return (self.pid, self.create_time())\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/__init__.py\", line 772, in create_time\nsglang  |     self._create_time = self._proc.create_time()\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/_pslinux.py\", line 1646, in wrapper\nsglang  |     return fun(self, *args, **kwargs)\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/_pslinux.py\", line 1884, in create_time\nsglang  |     ctime = float(self._parse_stat_file()['create_time'])\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/_pslinux.py\", line 1646, in wrapper\nsglang  |     return fun(self, *args, **kwargs)\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/_common.py\", line 460, in wrapper\nsglang  |     return fun(self)\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/_pslinux.py\", line 1712, in _parse_stat_file\nsglang  |     data = bcat(f\"{self._procfs_path}/{self.pid}/stat\")\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/_common.py\", line 814, in bcat\nsglang  |     return cat(fname, fallback=fallback, _open=open_binary)\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/_common.py\", line 802, in cat\nsglang  |     with _open(fname) as f:\nsglang  |   File \"/usr/local/lib/python3.10/dist-packages/psutil/_common.py\", line 766, in open_binary\nsglang  |     return open(fname, \"rb\", buffering=FILE_READ_BUFFER_SIZE)\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/engine.py\", line 511, in sigquit_handler\nsglang  |     kill_process_tree(os.getpid())\nsglang  |   File \"/sgl-workspace/sglang/python/sglang/srt/utils.py\", line 727, in kill_process_tree\nsglang  |     sys.exit(0)\nsglang  | SystemExit: 0\nsglang  | \n```\n\n### Reproduction\n\nI'm using docker lmsysorg/sglang:latest\n```\n   entrypoint: python3 -m sglang.launch_server\n    command: >\n      --model-path ${SGLANG_MODEL_PATH}\n      --tp 8\n      --trust-remote-code\n      --speculative-draft-model-path lmsys/DeepSeek-V3-NextN\n      --speculative-algorithm EAGLE\n      --speculative-num-steps 2\n      --speculative-eagle-topk 2\n      --speculative-num-draft-tokens 4\n      --cuda-graph-bs 1 2 4 8 16 32 40 48 56 64 128\n      --max-running-requests 128\n      --enable-metrics\n      --host 0.0.0.0\n      --port 3000\n      --enable-mixed-chunk\n```\n\n### Environment\n\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H200\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 570.124.06\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post4\nsgl_kernel: 0.1.2.post1\nflashinfer_python: 0.2.5+cu124torch2.6\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.11.0\nnumpy: 2.2.5\naiohttp: 3.11.18\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.31.1\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.4\npython-multipart: 0.0.20\npyzmq: 26.4.0\nuvicorn: 0.34.2\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.19\nopenai: 1.75.0\ntiktoken: 0.9.0\nanthropic: 0.51.0\nlitellm: 1.69.1\ndecord: 0.6.0\nNVIDIA Topology: \n\tGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t0-191\t0\t\tN/A\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t0-191\t0\t\tN/A\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\t0-191\t0\t\tN/A\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\t0-191\t0\t\tN/A\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\t0-191\t0\t\tN/A\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\t0-191\t0\t\tN/A\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\t0-191\t0\t\tN/A\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \t0-191\t0\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nHypervisor vendor: KVM\nulimit soft: 1048576",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-06T11:04:59+00:00",
    "closed_at": "2025-06-06T12:36:42+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6921/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6921"
  },
  {
    "number": 2769,
    "title": "[Bug] Error occurs when loading in bitsandbytes format.",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n```bash\r\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py\", line 172, in __init__\r\nself.quant_method = quant_config.get_quant_method(self,\r\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 114, in get_quant_method\r\nreturn BitsAndBytesLinearMethod(self)\r\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 141, in __init__\r\nraise ImportError(\"Please install bitsandbytes>=0.44.0 via \"\r\nImportError: Please install bitsandbytes>=0.44.0 via `pip install bitsandbytes>=0.44.0` to use bitsandbytes quantizer.\r\n```\n\n### Reproduction\n\n```bash\r\npython3 -m sglang.launch_server --model-path /models --tokenizer-path /models --host 0.0.0.0 --port 30000 --tokenizer-mode auto --dtype bfloat16 --mem-fraction-static 0.5 --random-seed 0 --enable-torch-compile --disable-cuda-graph --schedule-conservativeness 1.3 --kv-cache-dtype fp8_e5m2 --quantization bitsandbytes --load-format bitsandbytes\r\n```\n\n### Environment\n\n```bash\r\nroot@33e74a81f115:/sglang/python# python3 -m sglang.check_env                                                                                                                                                         \r\n\r\nPython: 3.10.16 (main, Dec  4 2024, 08:53:37) [GCC 9.4.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nGPU 0 Compute Capability: 8.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\r\nCUDA Driver Version: 550.127.05\r\nPyTorch: 2.5.1+cu124\r\nsglang: 0.4.0.post2\r\nflashinfer: 0.1.6+cu124torch2.4\r\ntriton: 3.1.0\r\ntransformers: 4.47.0\r\ntorchao: 0.6.1\r\nnumpy: 1.26.4\r\naiohttp: 3.11.10\r\nfastapi: 0.115.6\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.26.3\r\ninteregular: 0.3.3\r\nmodelscope: 1.21.0\r\norjson: 3.10.12\r\npackaging: 24.2\r\npsutil: 6.1.0\r\npydantic: 2.10.3\r\nmultipart: 0.0.19\r\nzmq: 26.2.0\r\nuvicorn: 0.32.1\r\nuvloop: 0.21.0\r\nvllm: 0.6.4.post1\r\nopenai: 1.57.0\r\nanthropic: 0.40.0\r\ndecord: 0.6.0\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-01-07T07:27:02+00:00",
    "closed_at": "2025-01-14T07:05:49+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2769/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2769"
  },
  {
    "number": 5814,
    "title": "[Bug] sglang 0.4.5-post3 deepseekv3-0324  function call  if the message contains system information, the function behaves abnormally.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nAfter deploying deepseekv3-0324 using sglang 0.4.5-post3-cu124, when testing the function call, it was found that if the message contains system information, the function behaves abnormally.\n\n\n### Reproduction\n\nAfter deploying deepseekv3-0324 using sglang 0.4.5-post3-cu124, when testing the function call, it was found that if the message contains system information, the function behaves abnormally.\n\n\n### Environment\n\nfor example\n-----------------------------------------------------------------------------\n-----------------------------------------------------------------------------\ncurl \"http://XXX/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"Use the tools to answer the questions.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the weather in Tokyo\"\n        }\n    ],\n    \"temperature\": 0.3,\n    \"max_tokens\": 1000,\n    \"model\": \"mssgpt_v3\",\n    \"tool_choice\": \"auto\",\n    \"tools\": [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"add\",\n                \"description\": \"Add two numbers\",\n                \"parameters\": {\n                    \"properties\": {\n                        \"a\": {\n                            \"title\": \"A\",\n                            \"type\": \"integer\"\n                        },\n                        \"b\": {\n                            \"title\": \"B\",\n                            \"type\": \"integer\"\n                        }\n                    },\n                    \"required\": [\n                        \"a\",\n                        \"b\"\n                    ],\n                    \"title\": \"addArguments\",\n                    \"type\": \"object\"\n                }\n            }\n        },\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_secret_word\",\n                \"description\": \"\",\n                \"parameters\": {\n                    \"properties\": {},\n                    \"title\": \"get_secret_wordArguments\",\n                    \"type\": \"object\"\n                }\n            }\n        },\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_current_weather\",\n                \"description\": \"\",\n                \"parameters\": {\n                    \"properties\": {\n                        \"city\": {\n                            \"title\": \"City\",\n                            \"type\": \"string\"\n                        }\n                    },\n                    \"required\": [\n                        \"city\"\n                    ],\n                    \"title\": \"get_current_weatherArguments\",\n                    \"type\": \"object\"\n                }\n            }\n        }\n    ]\n}\n'\nthe function call is wrong and \"content\" is not null,  \"tool_calls\" is null \n\n{\n    \"id\": \"1d2854e227fe437191ee78b16e1bcfd8\",\n    \"object\": \"chat.completion\",\n    \"created\": 1745811416,\n    \"model\": \"mssgpt-v3\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"message\": {\n                \"role\": \"assistant\",\n                \"content\": \"I'll add the numbers 7 and 22 for you. One moment.???????????????????????????\",\n                \"reasoning_content\": null,\n                \"tool_calls\": null\n            },\n            \"logprobs\": null,\n            \"finish_reason\": \"stop\",\n            \"matched_stop\": 1\n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 278,\n        \"total_tokens\": 349,\n        \"completion_tokens\": 71,\n        \"prompt_tokens_details\": null\n    }\n}\n\n \n\n-----------------------------------------------------------------------------\nBUT....\n\n\n\n-----------------------------------------------------------------------------\n  curl \"http://xxxx/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"messages\": [\n        \n        {\n            \"role\": \"user\",\n            \"content\": \"What is the weather in Tokyo\"\n        }\n    ],\n    \"temperature\": 0.3,\n    \"max_tokens\": 1000,\n    \"model\": \"mssgpt_v3\",\n    \"tool_choice\": \"auto\",\n    \"tools\": [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"add\",\n                \"description\": \"Add two numbers\",\n                \"parameters\": {\n                    \"properties\": {\n                        \"a\": {\n                            \"title\": \"A\",\n                            \"type\": \"integer\"\n                        },\n                        \"b\": {\n                            \"title\": \"B\",\n                            \"type\": \"integer\"\n                        }\n                    },\n                    \"required\": [\n                        \"a\",\n                        \"b\"\n                    ],\n                    \"title\": \"addArguments\",\n                    \"type\": \"object\"\n                }\n            }\n        },\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_secret_word\",\n                \"description\": \"\",\n                \"parameters\": {\n                    \"properties\": {},\n                    \"title\": \"get_secret_wordArguments\",\n                    \"type\": \"object\"\n                }\n            }\n        },\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_current_weather\",\n                \"description\": \"\",\n                \"parameters\": {\n                    \"properties\": {\n                        \"city\": {\n                            \"title\": \"City\",\n                            \"type\": \"string\"\n                        }\n                    },\n                    \"required\": [\n                        \"city\"\n                    ],\n                    \"title\": \"get_current_weatherArguments\",\n                    \"type\": \"object\"\n                }\n            }\n        }\n    ]\n}\n'\nfunction call correct  tool_calls reuslt successs\n\n{\n    \"id\": \"f387b0fe9e5a40e8b40a585d69dcfeb1\",\n    \"object\": \"chat.completion\",\n    \"created\": 1745811225,\n    \"model\": \"mssgpt_v3\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"message\": {\n                \"role\": \"assistant\",\n                \"content\": null,\n                \"reasoning_content\": null,\n                \"tool_calls\": [\n                    {\n                        \"id\": \"2\",\n                        \"type\": \"function\",\n                        \"function\": {\n                            \"name\": \"get_current_weather\",\n                            \"arguments\": \"{\\\"city\\\": \\\"Tokyo\\\"}\"\n                        }\n                    }\n                ]\n            },\n            \"logprobs\": null,\n            \"finish_reason\": \"tool_calls\",\n            \"matched_stop\": null\n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 261,\n        \"total_tokens\": 282,\n        \"completion_tokens\": 21,\n        \"prompt_tokens_details\": null\n    }\n}",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-28T05:57:03+00:00",
    "closed_at": "2025-05-02T10:45:15+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5814/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5814"
  },
  {
    "number": 1612,
    "title": "[Bug] SGLang hangs after hitting 0.00 token usage on Engine.generate",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHello! I am trying to add SGLang backend for a fast inference backend to a research project that I am working on, named [rank_llm](https://github.com/castorini/rank_llm). It requires multiple `generate` function calls to run inference for ranking for informational retrieval task. \r\n\r\nSGLang works perfectly fine for first batch, or first `generate` call, but it hangs in the middle of the second batch. CPU hits 100% for the core that python is running, so it does not seem like SGLang is crashed.\r\n\r\nThe repos that I am using:  \r\n[rank_llm](https://github.com/pjyi2147/rank_llm) on branch `Sglang`\r\n[SGLang](https://github.com/pjyi2147/sglang) on branch `main`\r\n\r\nI am using slightly modified version of SGLang. It's currently missing `get_tokenizer` function in class `Engine` so I added it.\r\n\n\n### Reproduction\n\nCommand:\r\npython src/rank_llm/scripts/run_rank_llm.py  --model_path=castorini/rank_zephyr_7b_v1_full --top_k_candidates=100 --dataset=dl19 --retrieval_method=SPLADE++_EnsembleDistil_ONNX --prompt_mode=rank_GPT  --context_size=4096 --variable_passages --window_size=20 --sglang_batched\r\n\r\nLog file created at: 2024-10-08T16:42:28Z\r\n\r\nProcess terminated by SIGINT at: 2024-10-08T16:45:45Z\r\n\r\nOutput:\r\nWARNING: Using incubator modules: jdk.incubator.vector\r\n[16:42:44] server_args=ServerArgs(model_path='castorini/rank_zephyr_7b_v1_full', tokenizer_path='castorini/rank_zephyr_7b_v1_full', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', dtype='auto', kv_cache_dtype='auto', trust_remote_code=True, context_length=None, quantization=None, served_model_name='castorini/rank_zephyr_7b_v1_full', chat_template=None, is_embedding=False, host='127.0.0.1', port=30000, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, tp_size=1, stream_interval=1, random_seed=25579900, constrained_json_whitespace_pattern=None, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, api_key=None, file_storage_pth='SGLang_storage', dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', attention_backend='flashinfer', sampling_backend='flashinfer', disable_flashinfer=False, disable_flashinfer_sampling=False, disable_radix_cache=False, disable_regex_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, enable_mixed_chunk=False, enable_torch_compile=False, max_torch_compile_bs=32, torchao_config='', enable_p2p_check=False, triton_attention_reduce_in_fp32=False, lora_paths=None, max_loras_per_batch=8)\r\nLoading castorini/rank_zephyr_7b_v1_full ...\r\nWARNING: Using incubator modules: jdk.incubator.vector\r\nWARNING: Using incubator modules: jdk.incubator.vector\r\n[16:42:53 TP0] Init nccl begin.\r\n[16:42:53 TP0] Load weight begin. avail mem=23.16 GB\r\nINFO 10-08 16:42:55 weight_utils.py:242] Using model weights format ['*.safetensors']\r\n\r\nLoading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\r\n\r\nLoading safetensors checkpoint shards:  33% Completed | 1/3 [00:05<00:11,  5.96s/it]\r\n\r\nLoading safetensors checkpoint shards:  67% Completed | 2/3 [00:12<00:06,  6.25s/it]\r\n\r\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:18<00:00,  6.31s/it]\r\n\r\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:18<00:00,  6.26s/it]\r\n\r\n[16:43:14 TP0] Load weight end. type=MistralForCausalLM, dtype=torch.bfloat16, avail mem=9.59 GB\r\n[16:43:14 TP0] Memory pool end. avail mem=2.46 GB\r\n[16:43:14 TP0] Capture cuda graph begin. This can take up to several minutes.\r\n[16:43:22 TP0] max_total_num_tokens=55807, max_prefill_tokens=16384, max_running_requests=2049, context_len=32768\r\nCompleted loading castorini/rank_zephyr_7b_v1_full\r\nRetrieving top 100 passages...\r\nTrying to download cached retrieved hits at https://github.com/castorini/rank_llm_data/raw/main/retrieve_results/SPLADE_P_P_ENSEMBLE_DISTIL/retrieve_results_dl19_top100.jsonl...\r\nDownloading file at https://github.com/castorini/rank_llm_data/raw/main/retrieve_results/SPLADE_P_P_ENSEMBLE_DISTIL/retrieve_results_dl19_top100.jsonl...\r\n\r\nretrieve_results_dl19_top100.jsonl: 0.00B [00:00, ?B/s]\r\nretrieve_results_dl19_top100.jsonl:   0%|          | 8.00k/1.76M [00:05<21:44, 1.41kB/s]\r\nretrieve_results_dl19_top100.jsonl: 1.77MB [00:05, 317kB/s]                             \r\n[16:43:28] Loading prompts.\r\nFile /home/tardis/patrickyi/rank_llm/retrieve_results/SPLADE_P_P_ENSEMBLE_DISTIL/retrieve_results_dl19_top100.jsonl has been downloaded to /home/tardis/patrickyi/rank_llm/retrieve_results/SPLADE_P_P_ENSEMBLE_DISTIL/retrieve_results_dl19_top100.jsonl.894d37a5701bde517f75e117b2d5cad4.\r\nReranking and returning 100 passages with castorini/rank_zephyr_7b_v1_full...\r\nPass 1 of 1:\r\n\r\nProcessing batches: 0it [00:00, ?it/s]\r\nProcessing batches: 1it [00:00,  2.74it/s]\r\nProcessing batches: 2it [00:00,  4.69it/s]\r\n[16:43:29] Prompts loaded.\r\n[16:43:29] SGLang Generating!\r\n[16:43:29 TP0] Prefill batch. #new-seq: 1, #new-token: 2062, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\r\n[16:43:29 TP0] Prefill batch. #new-seq: 4, #new-token: 8192, #cached-token: 291, cache hit rate: 2.76%, token usage: 0.04, #running-req: 1, #queue-req: 38\r\n[16:43:30 TP0] Prefill batch. #new-seq: 6, #new-token: 8192, #cached-token: 2753, cache hit rate: 14.16%, token usage: 0.18, #running-req: 4, #queue-req: 34\r\n[16:43:30 TP0] Prefill batch. #new-seq: 5, #new-token: 8192, #cached-token: 897, cache hit rate: 12.89%, token usage: 0.33, #running-req: 9, #queue-req: 30\r\n[16:43:31 TP0] Prefill batch. #new-seq: 5, #new-token: 8192, #cached-token: 523, cache hit rate: 11.36%, token usage: 0.48, #running-req: 13, #queue-req: 26\r\n[16:43:32 TP0] Prefill batch. #new-seq: 5, #new-token: 8192, #cached-token: 632, cache hit rate: 10.59%, token usage: 0.62, #running-req: 17, #queue-req: 22\r\n[16:43:33 TP0] Prefill batch. #new-seq: 5, #new-token: 8192, #cached-token: 1832, cache hit rate: 11.92%, token usage: 0.77, #running-req: 21, #queue-req: 18\r\n[16:43:33 TP0] Prefill batch. #new-seq: 1, #new-token: 1443, #cached-token: 1005, cache hit rate: 13.09%, token usage: 0.92, #running-req: 25, #queue-req: 18\r\n[16:43:35 TP0] Decode batch. #running-req: 26, #token: 53692, token usage: 0.96, gen throughput (token/s): 81.25, #queue-req: 17\r\n[16:43:36 TP0] Decode batch. #running-req: 26, #token: 54732, token usage: 0.98, gen throughput (token/s): 1030.19, #queue-req: 17\r\n[16:43:36 TP0] Prefill batch. #new-seq: 4, #new-token: 8192, #cached-token: 288, cache hit rate: 11.90%, token usage: 0.00, #running-req: 0, #queue-req: 13\r\n[16:43:37 TP0] Prefill batch. #new-seq: 6, #new-token: 8192, #cached-token: 2828, cache hit rate: 13.80%, token usage: 0.15, #running-req: 3, #queue-req: 9\r\n[16:43:37 TP0] Prefill batch. #new-seq: 5, #new-token: 8192, #cached-token: 1251, cache hit rate: 13.74%, token usage: 0.29, #running-req: 8, #queue-req: 5\r\n[16:43:38 TP0] Prefill batch. #new-seq: 5, #new-token: 8192, #cached-token: 480, cache hit rate: 13.01%, token usage: 0.44, #running-req: 12, #queue-req: 1\r\n[16:43:39 TP0] Prefill batch. #new-seq: 1, #new-token: 1902, #cached-token: 581, cache hit rate: 13.27%, token usage: 0.59, #running-req: 16, #queue-req: 1\r\n[16:43:40 TP0] Decode batch. #running-req: 17, #token: 35252, token usage: 0.63, gen throughput (token/s): 185.49, #queue-req: 0\r\n[16:43:41 TP0] Decode batch. #running-req: 17, #token: 35932, token usage: 0.64, gen throughput (token/s): 745.82, #queue-req: 0\r\n[16:43:41] Loading prompts.\r\n\r\nProcessing batches: 0it [00:00, ?it/s]\r\nProcessing batches: 1it [00:00,  6.04it/s]\r\nProcessing batches: 2it [00:00,  7.93it/s]\r\n[16:43:41] Prompts loaded.\r\n[16:43:41] SGLang Generating!\r\n[16:43:41 TP0] Prefill batch. #new-seq: 1, #new-token: 2018, #cached-token: 72, cache hit rate: 13.07%, token usage: 0.00, #running-req: 0, #queue-req: 0\r\n[16:43:42 TP0] Prefill batch. #new-seq: 4, #new-token: 8192, #cached-token: 365, cache hit rate: 12.39%, token usage: 0.04, #running-req: 1, #queue-req: 38\r\n[16:43:42 TP0] Prefill batch. #new-seq: 5, #new-token: 8192, #cached-token: 2264, cache hit rate: 13.19%, token usage: 0.19, #running-req: 4, #queue-req: 35\r\n[16:43:43 TP0] Prefill batch. #new-seq: 6, #new-token: 8192, #cached-token: 1939, cache hit rate: 13.65%, token usage: 0.33, #running-req: 8, #queue-req: 30\r\n[16:43:44 TP0] Prefill batch. #new-seq: 5, #new-token: 8192, #cached-token: 762, cache hit rate: 13.32%, token usage: 0.48, #running-req: 13, #queue-req: 26\r\n[16:43:45 TP0] Prefill batch. #new-seq: 6, #new-token: 8192, #cached-token: 1799, cache hit rate: 13.63%, token usage: 0.63, #running-req: 17, #queue-req: 21\r\n[16:43:45 TP0] Prefill batch. #new-seq: 5, #new-token: 8192, #cached-token: 694, cache hit rate: 13.31%, token usage: 0.78, #running-req: 22, #queue-req: 17\r\n[16:43:46 TP0] Prefill batch. #new-seq: 1, #new-token: 1088, #cached-token: 652, cache hit rate: 13.57%, token usage: 0.92, #running-req: 26, #queue-req: 17\r\n[16:43:47 TP0] Decode batch. #running-req: 27, #token: 53199, token usage: 0.95, gen throughput (token/s): 144.15, #queue-req: 16\r\n[16:43:48 TP0] Decode batch. #running-req: 27, #token: 54279, token usage: 0.97, gen throughput (token/s): 1070.83, #queue-req: 16\r\n[16:43:49 TP0] Prefill batch. #new-seq: 5, #new-token: 8192, #cached-token: 370, cache hit rate: 13.10%, token usage: 0.00, #running-req: 0, #queue-req: 11\r\n[16:43:49 TP0] Prefill batch. #new-seq: 5, #new-token: 8192, #cached-token: 753, cache hit rate: 12.87%, token usage: 0.15, #running-req: 4, #queue-req: 8\r\n[16:43:50 TP0] Prefill batch. #new-seq: 5, #new-token: 8192, #cached-token: 1083, cache hit rate: 12.81%, token usage: 0.29, #running-req: 8, #queue-req: 4\r\n[16:43:51 TP0] Prefill batch. #new-seq: 4, #new-token: 8107, #cached-token: 1361, cache hit rate: 12.88%, token usage: 0.44, #running-req: 12, #queue-req: 1\r\n[16:43:52 TP0] Decode batch. #running-req: 16, #token: 32919, token usage: 0.59, gen throughput (token/s): 241.02, #queue-req: 0\r\n[16:43:53 TP0] Decode batch. #running-req: 16, #token: 33559, token usage: 0.60, gen throughput (token/s): 744.31, #queue-req: 0\r\n[16:43:54 TP0] Decode batch. #running-req: 0, #token: 0, token usage: 0.00, gen throughput (token/s): 737.05, #queue-req: 0\r\n\r\nTime Taken: 197 seconds\r\n\r\nTimestamp: 2024-10-08T16:45:45Z\r\n\r\n\n\n### Environment\n\nPython: 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA GeForce RTX 4090\r\nGPU 0 Compute Capability: 8.9\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.3, V12.3.107\r\nCUDA Driver Version: 560.28.03\r\nPyTorch: 2.4.0+cu121\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.45.0\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.26.3\r\naiohttp: 3.10.6\r\nfastapi: 0.115.0\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.25.1\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.9.2\r\nuvicorn: 0.30.6\r\nuvloop: 0.20.0\r\nzmq: 26.2.0\r\nvllm: 0.6.1.dev238+ge2c6e0a82\r\nmultipart: 0.0.12\r\nopenai: 1.51.1\r\nanthropic: 0.35.0\r\nNVIDIA Topology: \r\n        GPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-11    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nulimit soft: 524288",
    "labels": [],
    "state": "closed",
    "created_at": "2024-10-08T23:26:49+00:00",
    "closed_at": "2024-10-09T21:32:54+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1612/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1612"
  },
  {
    "number": 565,
    "title": "missing 1 required positional argument: 'page_size' when using --enable-flashinfer",
    "body": "I am able to get it running properly when not using flashinfer and am currently running 4x NVIDIA A10G. Please let me know what other information might be helpful from my end. \r\n`python -m sglang.launch_server --model-path /mnt/ebs_volume/models/llama/llama-3-8b-instruct --tp-size=4 --mem-fraction-static=0.75 --port 30000 --enable-flashinfer`\r\n\r\n\r\n\r\nsglang==0.1.17\r\ntriton==2.3.0\r\ntransformers==4.41.2\r\ntorch==2.3.0\r\nvllm==0.4.3\r\nvllm-flash-attn==2.5.8.post2\r\nflashinfer==0.0.5 ( I also tested 0.0.6)\r\nnvcc==12.1, V12.1.105\r\n\r\n```\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n[gpu_id=0] Set cuda device.\r\n[gpu_id=1] Set cuda device.\r\n[gpu_id=2] Set cuda device.\r\n[gpu_id=3] Set cuda device.\r\n[gpu_id=0] Init nccl begin.\r\n[gpu_id=1] Init nccl begin.\r\n[gpu_id=2] Init nccl begin.\r\n[gpu_id=3] Init nccl begin.\r\nWARNING 06-25 19:03:47 custom_all_reduce.py:158] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\nWARNING 06-25 19:03:48 custom_all_reduce.py:158] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\nWARNING 06-25 19:03:48 custom_all_reduce.py:158] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\nWARNING 06-25 19:03:48 custom_all_reduce.py:158] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n[gpu_id=2] Load weight begin. avail mem=21.52 GB\r\n[gpu_id=0] Load weight begin. avail mem=21.52 GB\r\n[gpu_id=1] Load weight begin. avail mem=21.52 GB\r\n[gpu_id=3] Load weight begin. avail mem=21.52 GB\r\n 61%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                 | 179/292 [00:01<00:00, 130.83it/s][gpu_id=3] Load weight end. type=LlamaForCausalLM, avail mem=17.76 GB\r\n 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                        | 193/292 [00:01<00:01, 74.92it/s][gpu_id=2] Load weight end. type=LlamaForCausalLM, avail mem=17.76 GB\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 291/292 [00:02<00:00, 135.25it/s]\r\n[gpu_id=0] Load weight end. type=LlamaForCausalLM, avail mem=17.76 GB\r\n[gpu_id=1] Load weight end. type=LlamaForCausalLM, avail mem=17.76 GB\r\n[gpu_id=2] Memory pool end. avail mem=4.94 GB\r\n[gpu_id=0] Memory pool end. avail mem=4.94 GB\r\n[gpu_id=1] Memory pool end. avail mem=4.94 GB\r\n[gpu_id=3] Memory pool end. avail mem=4.94 GB\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n[gpu_id=3] max_total_num_tokens=405784, max_prefill_tokens=65536, context_len=8192,\r\n[gpu_id=2] max_total_num_tokens=405784, max_prefill_tokens=65536, context_len=8192,\r\n[gpu_id=0] max_total_num_tokens=405784, max_prefill_tokens=65536, context_len=8192,\r\n[gpu_id=0] server_args: enable_flashinfer=True, attention_reduce_in_fp32=False, disable_radix_cache=False, disable_regex_jump_forward=False, disable_disk_cache=False,\r\n[gpu_id=1] max_total_num_tokens=405784, max_prefill_tokens=65536, context_len=8192,\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nINFO:     Started server process [23287]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)\r\nINFO:     127.0.0.1:58096 - \"GET /get_model_info HTTP/1.1\" 200 OK\r\n[gpu_id=0] Prefil batch. #new-seq: 1, #new-token: 7, #cached-token: 0, cache hit rate: 0.00%, #running-req: 0, #queue-req: 0\r\nException in ModelTpServer:\r\nTraceback (most recent call last):\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/sglang/srt/managers/controller/tp_worker.py\", line 188, in exposed_step\r\n    self.forward_step()\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/sglang/srt/managers/controller/tp_worker.py\", line 204, in forward_step\r\n    self.forward_fill_batch(new_batch)\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/sglang/srt/managers/controller/tp_worker.py\", line 443, in forward_fill_batch\r\n    ) = self.model_runner.forward(batch, ForwardMode.EXTEND)\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/sglang/srt/managers/controller/model_runner.py\", line 426, in forward\r\n    return self.forward_extend(batch)\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/sglang/srt/managers/controller/model_runner.py\", line 361, in forward_extend\r\n    input_metadata = InputMetadata.create(\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/sglang/srt/managers/controller/model_runner.py\", line 209, in create\r\n    ret.init_flashinfer_args(tp_size)\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/sglang/srt/managers/controller/model_runner.py\", line 115, in init_flashinfer_args\r\n    self.prefill_wrapper.begin_forward(*args)\r\nTypeError: BatchPrefillWithPagedKVCacheWrapper.begin_forward() missing 1 required positional argument: 'page_size'\r\n\r\n...\r\n\r\nException in ControllerSingle:\r\nTraceback (most recent call last):\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/sglang/srt/managers/controller/manager_single.py\", line 93, in start_controller_process\r\n    loop.run_until_complete(controller.loop_for_forward())\r\n  File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/sglang/srt/managers/controller/manager_single.py\", line 44, in loop_for_forward\r\n    out_pyobjs = await self.model_client.step(next_step_input)\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/sglang/srt/managers/controller/tp_worker.py\", line 787, in _func\r\n    return obtain(tasks[0].value)\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/rpyc/core/async_.py\", line 111, in value\r\n    raise self._obj\r\n_get_exception_class.<locals>.Derived: BatchPrefillWithPagedKVCacheWrapper.begin_forward() missing 1 required positional argument: 'page_size'\r\n\r\n========= Remote Traceback (1) =========\r\nTraceback (most recent call last):\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/rpyc/core/protocol.py\", line 369, in _dispatch_request\r\n    res = self._HANDLERS[handler](self, *args)\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/rpyc/core/protocol.py\", line 863, in _handle_call\r\n    return obj(*args, **dict(kwargs))\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/sglang/srt/managers/controller/tp_worker.py\", line 188, in exposed_step\r\n    self.forward_step()\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/sglang/srt/managers/controller/tp_worker.py\", line 204, in forward_step\r\n    self.forward_fill_batch(new_batch)\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/sglang/srt/managers/controller/tp_worker.py\", line 443, in forward_fill_batch\r\n    ) = self.model_runner.forward(batch, ForwardMode.EXTEND)\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/sglang/srt/managers/controller/model_runner.py\", line 426, in forward\r\n    return self.forward_extend(batch)\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/sglang/srt/managers/controller/model_runner.py\", line 361, in forward_extend\r\n    input_metadata = InputMetadata.create(\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/sglang/srt/managers/controller/model_runner.py\", line 209, in create\r\n    ret.init_flashinfer_args(tp_size)\r\n  File \"/home/ec2-user/miniconda3/envs/pytorch_p310/lib/python3.10/site-packages/sglang/srt/managers/controller/model_runner.py\", line 115, in init_flashinfer_args\r\n    self.prefill_wrapper.begin_forward(*args)\r\nTypeError: BatchPrefillWithPagedKVCacheWrapper.begin_forward() missing 1 required positional argument: 'page_size'\r\n\r\n\r\n./start-llama-3.sh: line 1: 23287 Killed                  python -m sglang.launch_server --model-path /mnt/ebs_volume/models/llama/llama-3-8b-instruct --tp-size=4 --mem-fraction-static=0.75 --port 30000 --enable-flashinfer\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-06-25T19:19:52+00:00",
    "closed_at": "2024-06-26T15:13:59+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/565/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/565"
  },
  {
    "number": 3868,
    "title": "[Bug] When running the DeepSeekV3 model with the bf16 data type, the torch.compile operation did not take effect.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThe `torch.compile` not take effect when I try to run deepseekv3 with bf16 dtype, the error:\n```\n[rank26]:W0225 22:39:13.908000 374581 site-packages/torch/_dynamo/convert_frame.py:906] [5/64] torch._dynamo hit config.accumulated_cache_size_limit (64)\n[rank26]:W0225 22:39:13.908000 374581 site-packages/torch/_dynamo/convert_frame.py:906] [5/64]    function: 'forward' (/root/sglang/python/sglang/srt/models/deepseek_v2.py:742)\n[rank26]:W0225 22:39:13.908000 374581 site-packages/torch/_dynamo/convert_frame.py:906] [5/64]    last reason: 5/0: L['self']._modules['self_attn']._modules['attn_mqa'].layer_id == 0\n[rank26]:W0225 22:39:13.908000 374581 site-packages/torch/_dynamo/convert_frame.py:906] [5/64] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n[rank26]:W0225 22:39:13.908000 374581 site-packages/torch/_dynamo/convert_frame.py:906] [5/64] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n[2025-02-25 22:39:15 TP24] Using default MoE config. Performance might be sub-optimal! Config file not found at /root/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=256,N=64,device_name=NVIDIA_H20.json\n``` \nAnd the decode throughput only 0.6 token/s.\n```\n[2025-02-25 23:59:25 TP0] Decode batch. #running-req: 1, #token: 2899, token usage: 0.05, gen throughput (token/s): 0.77, #queue-req: 15\n[2025-02-26 00:00:31 TP0] Decode batch. #running-req: 1, #token: 2939, token usage: 0.05, gen throughput (token/s): 0.61, #queue-req: 15\n[2025-02-26 00:01:42 TP0] Decode batch. #running-req: 1, #token: 2979, token usage: 0.06, gen throughput (token/s): 0.56, #queue-req: 15\n[2025-02-26 00:02:45 TP0] Decode batch. #running-req: 1, #token: 3019, token usage: 0.06, gen throughput (token/s): 0.63, #queue-req: 15\n[2025-02-26 00:03:47 TP0] Decode batch. #running-req: 1, #token: 3059, token usage: 0.06, gen throughput (token/s): 0.65, #queue-req: 15\n[2025-02-26 00:04:42 TP0] Decode batch. #running-req: 1, #token: 3099, token usage: 0.06, gen throughput (token/s): 0.73, #queue-req: 15\n[2025-02-26 00:05:44 TP0] Decode batch. #running-req: 1, #token: 3139, token usage: 0.06, gen throughput (token/s): 0.65, #queue-req: 15\n[2025-02-26 00:06:44 TP0] Decode batch. #running-req: 1, #token: 3179, token usage: 0.06, gen throughput (token/s): 0.66, #queue-req: 15\n[2025-02-26 00:07:54 TP0] Decode batch. #running-req: 1, #token: 3219, token usage: 0.06, gen throughput (token/s): 0.57, #queue-req: 15\n[2025-02-26 00:09:09 TP0] Decode batch. #running-req: 1, #token: 3259, token usage: 0.06, gen throughput (token/s): 0.54, #queue-req: 15\n[2025-02-26 00:10:22 TP0] Decode batch. #running-req: 1, #token: 3299, token usage: 0.06, gen throughput (token/s): 0.55, #queue-req: 15\n[2025-02-26 00:11:22 TP0] Decode batch. #running-req: 1, #token: 3339, token usage: 0.06, gen throughput (token/s): 0.67, #queue-req: 15\n[2025-02-26 00:12:36 TP0] Decode batch. #running-req: 1, #token: 3379, token usage: 0.06, gen throughput (token/s): 0.54, #queue-req: 15\n[2025-02-26 00:13:37 TP0] Decode batch. #running-req: 1, #token: 3419, token usage: 0.06, gen throughput (token/s): 0.66, #queue-req: 15\n[2025-02-26 00:14:46 TP0] Decode batch. #running-req: 1, #token: 3459, token usage: 0.06, gen throughput (token/s): 0.57, #queue-req: 15\n[2025-02-26 00:15:47 TP0] Decode batch. #running-req: 1, #token: 3499, token usage: 0.07, gen throughput (token/s): 0.66, #queue-req: 15\n[2025-02-26 00:17:01 TP0] Decode batch. #running-req: 1, #token: 3539, token usage: 0.07, gen throughput (token/s): 0.54, #queue-req: 15\n[2025-02-26 00:18:10 TP0] Decode batch. #running-req: 1, #token: 3579, token usage: 0.07, gen throughput (token/s): 0.58, #queue-req: 15\n```\n\n### Reproduction\n\n```python\nimport torch\nimport torch.distributed as dist\n\nfrom sglang.srt.server_args import ServerArgs\n\nmodel_name = 'DeepSeek-V3'\nmodel_path = './DeepSeek-V3'\ntp_size = 32\nbase_gpu_id = 0\nmax_batch_size = 1\nmax_input_len = 4096\nmax_output_len = 4096\n\nrank = dist.get_rank()\nnode_rank = rank  // tp_size\nport = int(os.environ[\"MASTER_PORT\"])\nserver_args = ServerArgs(\n    model_path=model_path,\n    dtype='bfloat16',\n    tp_size=tp_size,\n    base_gpu_id=base_gpu_id,\n    mem_fraction_static=0.9,\n    # request\n    max_running_requests=max_batch_size,\n    max_prefill_tokens=max_input_len,\n    context_length=max_input_len+max_output_len,\n    # serving\n    host='0.0.0.0',\n    port=port,\n    device='cuda',\n    served_model_name=model_name,\n    log_level='info',\n    trust_remote_code=True,\n    log_requests=False,\n    enable_metrics=False,\n    show_time_cost=False,\n    skip_tokenizer_init=True,\n    # Multi-node distributed serving\n    dist_init_addr=f\"{master_addr}:{port}\",\n    nnodes = 4,\n    node_rank = node_rank,\n    # optimize\n    cuda_graph_max_bs=64,\n    enable_torch_compile=True,\n    torch_compile_max_bs=128,\n)\n\n\nif rank % torch.cuda.device_count() == 0:\n    llm = sgl.Engine(server_args=server_args)\n\nif rank == 0:\n    prompts = [\"\u5199\u4e00\u9996\u4e03\u8a00\u7edd\u53e5\", \"\u6d77\u6c34\u4e3a\u4ec0\u4e48\u662f\u54b8\u7684\"]\n    sampling_params = {\n        'top_k':40,\n        'top_p':1.0,\n        'temperature':1.0,\n        'max_new_tokens':max_output_len,\n    }\n    outputs = llm.generate(prompts, sampling_params=sampling_params)\n```\n\n### Environment\n\nH20 * 8 * 4\n```\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA H20                     On  | 00000000:03:00.0 Off |                    0 |\n| N/A   37C    P0             116W / 500W |   1660MiB / 97871MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n```\ncuda 12.4\n```\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2024 NVIDIA Corporation\nBuilt on Thu_Mar_28_02:18:24_PDT_2024\nCuda compilation tools, release 12.4, V12.4.131\nBuild cuda_12.4.r12.4/compiler.34097967_0\n```\nCentOS Linux release 7.2 (Final)\ng++ (GCC) 11.4.0\ntorch 2.6.0+cu124\nsglang source compile at branch 0.4.2.post1\nsgl-kernel source compile at branch 0.4.2.post1: Version: 0.0.3.post1\nvllm source compile at branch v0.6.4.post1\n\nfull install command:\n```\n# install torch\npip3 uninstall -y torch torchvision torchaudio\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n# install vllm\ngit clone --recursive  https://github.com/vllm-project/vllm.git -b v0.6.4.post1\npushd vllm\npython3 use_existing_torch.py\npip3 install -r requirements-build.txt\npip3 install -e . --no-build-isolation\npopd\n# install sglang\ngit clone  --recursive   https://github.com/sgl-project/sglang.git -b v0.4.2.post1\npushd sglang/sgl-kernel\n# isntall cusparselt\nrpm --rebuilddb\nwget https://developer.download.nvidia.com/compute/cusparselt/0.7.0/local_installers/cusparselt-local-repo-rhel8-0.7.0-1.0-1.x86_64.rpm -O /root/cusparselt-local-repo-rhel8-0.7.0-1.0-1.x86_64.rpm -q\nrpm -i /root/cusparselt-local-repo-rhel8-0.7.0-1.0-1.x86_64.rpm\nyum install epel-release dnf -y\ndnf clean all\ndnf -y install libcusparselt0 libcusparselt-devel\nrm -rf /root/cusparselt-local-repo-rhel8-0.7.0-1.0-1.x86_64.rpm\n# install sgl-kernel\npip3 install --no-cache-dir ninja\nexport TORCH_CUDA_ARCH_LIST='8.0 8.9 9.0+PTX'\nexport SGL_KERNEL_ENABLE_BF16=1\nexport SGL_KERNEL_ENABLE_FP8=1\nexport SGL_KERNEL_ENABLE_SM90A=1\npython3 setup.py bdist_wheel\npython3 setup.py install\npopd\n# install sglang\npushd sglang\npip3 install -e \"python[all]\" --find-links https://flashinfer.ai/whl/cu124/torch2.5/flashinfer-python\npopd\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-26T03:30:02+00:00",
    "closed_at": "2025-03-19T06:54:41+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3868/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3868"
  },
  {
    "number": 322,
    "title": "Contradictory suggestions: Not enough memory. Please try to increase --mem-fraction-static",
    "body": "**Q: Should I increase or decrease `--mem-fraction-static`?** (and what is the minimum and maximum value allowed?)\r\n\r\nLooking in the source code (`python/sglang/srt/managers/router/model_runner.py`) I would believe that increasing the value would alleviate the memory requirements but I might be interpreting it wrong. Just wanted to inform that there is a mismatch between the advice given in documentation and the advice given in the actual code.\r\n\r\n**Description of the problem:**\r\n\r\nI am trying to launch Mistral-7B-Instruct-v0.2 (using sglang==0.1.13):\r\n\r\n`python -m sglang.launch_server --model-path /llm_path/hf_model_mistral_7B_Instruct_v0_2 --port 30000`\r\n\r\nbut I have memory issues. At the end it is suggested to increase `--mem-fraction-static`. \r\n\r\nHowever, in the documentation (https://github.com/sgl-project/sglang) the opposite advice is given:\r\n\r\n> If you see out-of-memory errors during serving, please try to reduce the memory usage of the KV cache pool by setting a smaller value of --mem-fraction-static. The default value is 0.9\r\n\r\n\r\nKeep up the good work :)\r\n\r\n/sneglen\r\n\r\n**Here is the error:**\r\n`Process Process-1:\r\nrouter init state: Traceback (most recent call last):\r\n  File \"/zhome/ac/8/105765/venv/env_MT/lib/python3.11/site-packages/sglang/srt/managers/router/manager.py\", line 68, in start_router_process\r\n    model_client = ModelRpcClient(server_args, port_args)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/zhome/ac/8/105765/venv/env_MT/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 619, in __init__\r\n    self.model_server.exposed_init_model(0, server_args, port_args)\r\n  File \"/zhome/ac/8/105765/venv/env_MT/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 70, in exposed_init_model\r\n    self.model_runner = ModelRunner(\r\n                        ^^^^^^^^^^^^\r\n  File \"/zhome/ac/8/105765/venv/env_MT/lib/python3.11/site-packages/sglang/srt/managers/router/model_runner.py\", line 272, in __init__\r\n    self.init_memory_pool(total_gpu_memory)\r\n  File \"/zhome/ac/8/105765/venv/env_MT/lib/python3.11/site-packages/sglang/srt/managers/router/model_runner.py\", line 331, in init_memory_pool\r\n    raise RuntimeError(\r\nRuntimeError: Not enought memory. Please try to increase --mem-fraction-static.\r\n`",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-03-22T12:23:46+00:00",
    "closed_at": "2024-07-25T06:33:37+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/322/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/322"
  },
  {
    "number": 2427,
    "title": "[Feature] Support General Reward Model",
    "body": "### Motivation\r\n\r\nAs mentioned in our devlopmap, https://github.com/sgl-project/sglang/issues/1487:\r\n\r\nSupport generalized reward API (adding linear layers to any Causal LM to get the reward) as required by the OpenRLHF team.\r\n\r\nhttps://github.com/OpenRLHF/OpenRLHF\r\n\r\n**Add linear layers to any Causal LM to get rewards.**\r\n\r\nWe formalize this requirement in this issue and invite @M0gician to contribute with us.\r\n\r\n### Features Request\r\n\r\n#### 1. Add linear layers to any Causal LM to get rewards.\r\n\r\n- [ ] Add linear layer at the end and assign a specific token (like final `eos` in the prompt) and manuplate the logits of it as rewards.\r\n- [ ] Add linear layer after a spcific value head name at any layer, manuplate it's logits as rewards.\r\n\r\n#### 2. Add `--task` parameter.\r\n\r\n- [ ] Get rewards/embedding from any Causal LM, adding a parameter like `--task embedding`.\r\n\r\n#### 3. Better Accuracy.\r\n\r\n**Many users may have noticed that the reward results of SGLang's current API show a discrepancy (around 3/1000) compared to those obtained from training engines like DeepSpeed or Llama-factory.** This discrepancy is not due to an issue with our framework implementation; in fact, this problem exists in all current inference engines:\r\n\r\n> The kernel fusion in inference engines differs significantly from that in training engines. When the batch size varies, inference requests are dispatched to different kernels, and numerical errors accumulate layer by layer. By the time it reaches the logits layer, these errors become noticeable. This issue has been around since the BERT era\u2014precision differences between training and inference engines are unavoidable.\r\n\r\nAs a result, in RLHF, inference engines are primarily used to accelerate sampling, while reward and embeddings still rely on training scripts. **It may take several months for our team to address this issue properly.**\r\n\r\nWe will add a logging regarding this issue in our Engine and our documents for this. Even if the reward may be inaccurate, we provide a general reward interface, in hope that community users could design more robust RL algorithm that works well in this scenario.\r\n\r\n### Related resources\r\n\r\n1. The reward forward function in OpenRLHF.\r\n2. HuggingFace `LlamaForSequence` and `AutoLinearXXX`.",
    "labels": [
      "help wanted",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-12-10T04:15:08+00:00",
    "closed_at": "2025-05-26T05:51:18+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2427/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2427"
  },
  {
    "number": 4246,
    "title": "[Bug] Enable --enable-flashinfer-mla, the result has high rate to output duplicated words/sentence.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI use sglang docker image to start Deepseek-V3(671B) model. if the input token prompt is huge, like 10 or 20k or more, then it will output duplicated content, and cannot stop in short period of time.\n\n \n\n\n\n\n\n### Reproduction\n\nSo I removed all of usless starting parameter(include --enable-flashinfer-mla), and kept some essential's. The issue was gone.\nBut I enable \"--enable-flashinfer-mla\" parameter on previous setting, the issue came back.\nThen the last testing was to keep original starting parameters exclude \"--enable-flashinfer-mla\", I also couldnot reproducte this issue. \n\n### Environment\n\nDocker Image: b7bfb0d6df2ffcd5e340d3e0455744a1934dd17582e4ecbb7d3c31c677aefd75\nModel: Deepseek-v3(671B)\nGPU: H200 x8\nthe issue setting:\n`python3 -m sglang.launch_server\n      --tp 8 \n      --torch-compile-max-bs 8  \n      --trust-remote-code  \n      --enable-torch-compile \n      --model-path /models/deepseek-v3 \n      --host 0.0.0.0 \n      --port 30001\n      --triton-attention-num-kv-splits 16\n      --chunked-prefill-size 16384\n      --max-running-requests 512\n      --max-prefill-tokens 65535\n      --mem-fraction-static 0.8\n      --enable-flashinfer-mla`\n\nthe normal setting:\n`python3 -m sglang.launch_server\n      --tp 8 \n      --torch-compile-max-bs 8  \n      --trust-remote-code  \n      --enable-torch-compile \n      --model-path /models/deepseek-v3 \n      --host 0.0.0.0 \n      --port 30001\n      --triton-attention-num-kv-splits 16\n      --chunked-prefill-size 16384\n      --max-running-requests 512\n      --max-prefill-tokens 65535\n      --mem-fraction-static 0.8`",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-10T02:19:43+00:00",
    "closed_at": "2025-03-26T00:53:35+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4246/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4246"
  },
  {
    "number": 1275,
    "title": "[Bug] sglang.launch_server error",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n(sglang) aluo@titan:~/sglang$ python -m sglang.launch_server --model-path /scratch3/data/Meta-Llama-3.1-8B-Instruct/ --enable-torch-compile --disable-radix-cache\r\nserver_args=ServerArgs(model_path='/scratch3/data/Meta-Llama-3.1-8B-Instruct/', tokenizer_path='/scratch3/data/Meta-Llama-3.1-8B-Instruct/', tokenizer_mode='auto', load_format='auto', dtype='auto', trust_remote_code=False, context_length=None, quantization=None, chat_template=None, host='127.0.0.1', port=30000, additional_ports=[30001, 30002, 30003, 30004], mem_fraction_static=0.88, max_prefill_tokens=None, max_running_requests=None, max_num_reqs=None, schedule_policy='lpm', schedule_conservativeness=1.0, tp_size=1, stream_interval=1, random_seed=1059195386, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, api_key='', file_storage_pth='SGlang_storage', dp_size=1, load_balance_method='round_robin', chunked_prefill_size=None, disable_flashinfer=False, disable_flashinfer_sampling=False, disable_radix_cache=True, disable_regex_jump_forward=False, disable_cuda_graph=False, disable_disk_cache=False, enable_torch_compile=True, enable_p2p_check=False, attention_reduce_in_fp32=False, efficient_weight_load=False, nccl_init_addr=None, nnodes=1, node_rank=None)\r\n[gpu=0] Init nccl begin.\r\n[gpu=0] Load weight begin. avail mem=78.69 GB\r\nInitialization failed. controller_init_state: Traceback (most recent call last):\r\n  File \"/home/aluo/sglang/python/sglang/srt/managers/controller_single.py\", line 150, in start_controller_process\r\n    controller = ControllerSingle(\r\n  File \"/home/aluo/sglang/python/sglang/srt/managers/controller_single.py\", line 84, in __init__\r\n    self.tp_server = ModelTpServer(\r\n  File \"/home/aluo/sglang/python/sglang/srt/managers/tp_worker.py\", line 91, in __init__\r\n    self.model_runner = ModelRunner(\r\n  File \"/home/aluo/sglang/python/sglang/srt/model_executor/model_runner.py\", line 123, in __init__\r\n    self.load_model()\r\n  File \"/home/aluo/sglang/python/sglang/srt/model_executor/model_runner.py\", line 170, in load_model\r\n    self.model = get_model(\r\n  File \"/home/aluo/miniconda3/envs/sglang/lib/python3.9/site-packages/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model\r\n    return loader.load_model(model_config=model_config,\r\n  File \"/home/aluo/miniconda3/envs/sglang/lib/python3.9/site-packages/vllm/model_executor/model_loader/loader.py\", line 267, in load_model\r\n    model = _initialize_model(model_config, self.load_config,\r\n  File \"/home/aluo/miniconda3/envs/sglang/lib/python3.9/site-packages/vllm/model_executor/model_loader/loader.py\", line 104, in _initialize_model\r\n    return model_class(config=model_config.hf_config,\r\n  File \"/home/aluo/sglang/python/sglang/srt/models/llama2.py\", line 318, in __init__\r\n    self.model = LlamaModel(config, quant_config=quant_config)\r\n  File \"/home/aluo/sglang/python/sglang/srt/models/llama2.py\", line 257, in __init__\r\n    [\r\n  File \"/home/aluo/sglang/python/sglang/srt/models/llama2.py\", line 258, in <listcomp>\r\n    LlamaDecoderLayer(\r\n  File \"/home/aluo/sglang/python/sglang/srt/models/llama2.py\", line 192, in __init__\r\n    self.self_attn = LlamaAttention(\r\n  File \"/home/aluo/sglang/python/sglang/srt/models/llama2.py\", line 125, in __init__\r\n    self.qkv_proj = QKVParallelLinear(\r\nTypeError: __init__() got an unexpected keyword argument 'prefix'\n\n### Reproduction\n\n(sglang) aluo@titan:~/sglang$ python -m sglang.launch_server --model-path /scratch3/data/Meta-Llama-3.1-8B-Instruct/ --enable-torch-compile --disable-radix-cache\n\n### Environment\n\nPython: 3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.5, V12.5.82\r\nCUDA Driver Version: 555.42.06\r\n555.42.06\r\n555.42.06\r\n555.42.06\r\n555.42.06\r\n555.42.06\r\n555.42.06\r\n555.42.06\r\nPyTorch: 2.3.1+cu121\r\nsglang: 0.2.7\r\nflashinfer: 0.1.4+cu121torch2.3\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.26.4\r\naiohttp: 3.10.3\r\nfastapi: 0.112.0\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.5\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.5\r\nuvloop: 0.19.0\r\nzmq: 26.1.0\r\nvllm: 0.5.2\r\nopenai: 1.40.6\r\nanthropic: 0.33.1\r\nNVIDIA Topology: \r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    0-95,192-287    0               N/A\r\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    0-95,192-287    0               N/A\r\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    0-95,192-287    0               N/A\r\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    0-95,192-287    0               N/A\r\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    96-191,288-383  1               N/A\r\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    96-191,288-383  1               N/A\r\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    96-191,288-383  1               N/A\r\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      96-191,288-383  1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nulimit soft: 1024",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-31T06:26:00+00:00",
    "closed_at": "2024-09-22T12:35:22+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1275/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1275"
  },
  {
    "number": 4214,
    "title": "[Bug] The sgl-kernel 0.0.3.post7 can't pass the CIs.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhe I used 0.0.3.post7 sgl-kernel, `test_mla.py` and `test_mla_tp.py` failed. I think it is probably related with `sgl_per_token_group_quant_fp8` of sgl-kernel after searching. Specifically, this commit https://github.com/sgl-project/sglang/commit/55a7ec388f87780d05a80c3c678ebea22f95523b caused the bug. When I removed the usage of `sgl_per_token_group_quant_fp8` kernel, all works fine.  Please check this. @zhyncs @BBuf \n\n```\n[2025-03-08 13:58:08 TP0] TpModelWorkerClient hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 109, in forward_thread_func\n    self.forward_thread_func_()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 140, in forward_thread_func_\n    logits_output, next_token_ids = self.worker.forward_batch_generation(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 172, in forward_batch_generation\n    logits_output = self.model_runner.forward(forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 907, in forward\n    return self.forward_decode(forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 858, in forward_decode\n    return self.model.forward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1084, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1038, in forward\n    hidden_states, residual = layer(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 991, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 197, in forward\n    self.experts(hidden_states=hidden_states, router_logits=router_logits)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/layer.py\", line 620, in forward\n    final_hidden_states = self.quant_method.apply(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/quantization/fp8.py\", line 948, in apply\n    return fused_experts(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\", line 921, in fused_experts\n    torch.ops.sglang.inplace_fused_experts(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 1116, in __call__\n    return self._op(*args, **(kwargs or {}))\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\", line 790, in inplace_fused_experts\n    fused_experts_impl(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\", line 1108, in fused_experts_impl\n    invoke_fused_moe_kernel(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\", line 573, in invoke_fused_moe_kernel\n    fused_moe_kernel[grid](\n  File \"/usr/local/lib/python3.10/dist-packages/triton/runtime/jit.py\", line 345, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/triton/runtime/jit.py\", line 691, in run\n    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,\n  File \"/usr/local/lib/python3.10/dist-packages/triton/backends/nvidia/driver.py\", line 365, in __call__\n    self.launch(*args, **kwargs)\nRuntimeError: Triton Error [CUDA]: an illegal memory access was encountered\n```\n\n### Reproduction\n\nYou run can `test_mla.py` and `test_mla_tp.py` to reproduce this bug with 0.0.3.post7 sgl-kernel. \n\n### Environment\n\nUse the official sglang docker.",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-08T14:08:53+00:00",
    "closed_at": "2025-03-09T08:42:30+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4214/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4214"
  },
  {
    "number": 5871,
    "title": "[Bug] Qwen3 MoE FP8: type fp8e4nv not supported in this architecture.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nError occurred while trying to load Qwen3-30B-A3B-FP8 with `sglang-v0.4.6.post1`. Error message as below:\n```\ntype fp8e4nv not supported in this architecture. The supported fp8 dtypes are ('fp8e4b15', 'fp8e5')\"\n```\n\n### Reproduction\n\n`python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-FP8 --reasoning-parser qwen3 --tp 2 --tool-call-parser qwen25`\n\n### Environment\n```\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1: NVIDIA A10\nGPU 0,1 Compute Capability: 8.6\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.216.01\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post1\nsgl_kernel: 0.1.0\nflashinfer: Module Not Found\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.10.0\nnumpy: 2.2.5\naiohttp: 3.11.18\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.3\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.2\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.17\nopenai: 1.76.0\ntiktoken: 0.9.0\nanthropic: 0.50.0\nlitellm: 1.67.4.post1\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NODE    26-51,130-155   1               N/A\nGPU1    NODE     X      26-51,130-155   1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-29T05:41:22+00:00",
    "closed_at": "2025-04-29T08:07:56+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5871/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5871"
  },
  {
    "number": 4550,
    "title": "[Bug] finish_reason is always null, missing the record of stop.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI use sglang:v0.4.4.post1-cu124 image\n\n\n\nthe output when curl is :\n\n```\ndata: {\"id\":\"93b6717481cf45c284841fac5c3af2a3\",\"object\":\"chat.completion.chunk\",\"created\":1742289669,\"model\":\"DeepSeek-R1\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"\u63d0\u4f9b\",\"reasoning_content\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"93b6717481cf45c284841fac5c3af2a3\",\"object\":\"chat.completion.chunk\",\"created\":1742289669,\"model\":\"DeepSeek-R1\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"\u5e2e\u52a9\",\"reasoning_content\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"93b6717481cf45c284841fac5c3af2a3\",\"object\":\"chat.completion.chunk\",\"created\":1742289669,\"model\":\"DeepSeek-R1\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"\u3002\",\"reasoning_content\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"93b6717481cf45c284841fac5c3af2a3\",\"object\":\"chat.completion.chunk\",\"created\":1742289669,\"model\":\"DeepSeek-R1\",\"choices\":[],\"usage\":{\"prompt_tokens\":6,\"total_tokens\":83,\"completion_tokens\":77}}\n```\n\nthere is no \"finish_reason\":\"stop\" in response\n\n### Reproduction\n\nuse reasoning-parser :\n\npython3 -m sglang.launch_server --model-path /data --tp 16 --dist-init-addr 10.233.114.55:5000 --nnodes 2 --node-rank 1 --trust-remote-code --host 0.0.0.0 --port 9001 --show-time-cost --enable-metrics --context-length 65536 --mem-fraction-static 0.88 --watchdog-timeout 60 --reasoning-parser deepseek-r1\n\n### Environment\n\n2 * H100",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-18T09:35:20+00:00",
    "closed_at": "2025-05-18T00:20:56+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4550/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4550"
  },
  {
    "number": 7028,
    "title": "[Bug] sgl-router enters infinite panic loop when all workers die under active load",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nBackground: DP Attention is not stable yet. During high concurrency, illegal memory access is more likely to occur, leading to crashes. Therefore, when one worker fails, there is a certain probability it will trigger a cascading failure that brings down all workers.\n\nTo deal with worker failure, I have a sidecar container infinite looping to detect live sglang endpoint and register them to the router.\n\nBut I noticed if (all workers died && clients are still sending requests), sgl-router will stuck into a inifinite loop of panic even if live workers are registered back:\n\n```\nthread 'actix-rt|system:0|arbiter:12' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:13' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:14' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:1' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:15' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:0' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:3' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:2' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:5' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:4' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\n```\n\nThis kind of panic log kept for hours before I noticed it:\n\n<img width=\"1835\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/acae5ce0-0b42-45ef-bf85-838848ceada1\" />\n\nWhile worker registeration is working:\n\n<img width=\"816\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fbf906d9-5880-40ff-b8b0-0bf8b3ee1084\" />\n\n\nAfter I stopped all clients, the router stopped logging infinite panic:\n\n<img width=\"860\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f135c326-05bf-408c-9c27-70a91778293b\" />\n\nBut still not able to serve requests. If I resume the clients, the router goes to infinite panic again, the only way out I found is kill the router and restart one.\n\n### Reproduction\n\nRemoving all workers while the client keeps sending requests should reproduce the issue.\n\nI understand the router panics when no workers are available is a known issue, but I believe it's unexpected that the router fails to handle requests properly after live workers are added back. This appears to be a multi-threaded race condition issue, but I'm not familiar with Rust programming so I don't know what caused this problem.\n\n### Environment\n\n```\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post5\nsgl_kernel: 0.1.4\nsglang-router: 0.1.4\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-10T05:34:36+00:00",
    "closed_at": "2025-06-26T05:58:47+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7028/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7028"
  },
  {
    "number": 4478,
    "title": "[Bug] IPython running error for Engine due to `outlines` nest_asyncio",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nIf you start an engine in `ipython`:\n\n```python\n(verl-sglang) (base) chayenne@lmsys:~/Awesome-ML-SYS-Tutorial/rlhf/rl-walk-through$ ipy\n\nPython 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0]\nType 'copyright', 'credits' or 'license' for more information\nIPython 8.33.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: \n\nIn [1]: # launch the offline engine\n   ...: import asyncio\n   ...: import io\n   ...: import os\n   ...: \n   ...: from PIL import Image\n   ...: import requests\n   ...: import sglang as sgl\n   ...: \n   ...: from sglang.srt.conversation import chat_templates\n   ...: from sglang.test.test_utils import is_in_ci\n   ...: from sglang.utils import async_stream_and_merge, stream_and_merge\n   ...: \n   ...: if is_in_ci():\n   ...:     import patch\n   ...: \n   ...: \n   ...: llm = sgl.Engine(model_path=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n\n/data/chayenne/.python/verl-sglang/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\nNo module named 'vllm._version'\n  from vllm.version import __version__ as VLLM_VERSION\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[1], line 18\n     14 if is_in_ci():\n     15     import patch\n---> 18 llm = sgl.Engine(model_path=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n\nFile ~/.python/verl-sglang/lib/python3.10/site-packages/sglang/api.py:43, in Engine(*args, **kwargs)\n     41 def Engine(*args, **kwargs):\n     42     # Avoid importing unnecessary dependency\n---> 43     from sglang.srt.entrypoints.engine import Engine\n     45     return Engine(*args, **kwargs)\n\nFile ~/.python/verl-sglang/lib/python3.10/site-packages/sglang/srt/entrypoints/engine.py:53\n     51 from sglang.srt.managers.scheduler import run_scheduler_process\n     52 from sglang.srt.managers.tokenizer_manager import TokenizerManager\n---> 53 from sglang.srt.openai_api.adapter import load_chat_template_for_openai_api\n     54 from sglang.srt.server_args import PortArgs, ServerArgs\n     55 from sglang.srt.torch_memory_saver_adapter import TorchMemorySaverAdapter\n\nFile ~/.python/verl-sglang/lib/python3.10/site-packages/sglang/srt/openai_api/adapter.py:30\n     27 from pydantic import ValidationError\n     29 try:\n---> 30     from outlines.fsm.json_schema import convert_json_schema_to_str\n     31 except ImportError:\n     32     # Before outlines 0.0.47, convert_json_schema_to_str is under\n     33     # outlines.integrations.utils\n     34     from outlines.integrations.utils import convert_json_schema_to_str\n\nFile ~/.python/verl-sglang/lib/python3.10/site-packages/outlines/__init__.py:2\n      1 \"\"\"Outlines is a Generative Model Programming Framework.\"\"\"\n----> 2 import outlines.generate\n      3 import outlines.grammars\n      4 import outlines.models\n\nFile ~/.python/verl-sglang/lib/python3.10/site-packages/outlines/generate/__init__.py:2\n      1 from .api import SequenceGenerator\n----> 2 from .cfg import cfg\n      3 from .choice import choice\n      4 from .format import format\n\nFile ~/.python/verl-sglang/lib/python3.10/site-packages/outlines/generate/cfg.py:5\n      3 from outlines.fsm.guide import CFGGuide\n      4 from outlines.generate.api import SequenceGenerator, SequenceGeneratorAdapter\n----> 5 from outlines.models import OpenAI\n      6 from outlines.models.llamacpp import LlamaCpp\n      7 from outlines.models.mlxlm import MLXLM\n\nFile ~/.python/verl-sglang/lib/python3.10/site-packages/outlines/models/__init__.py:14\n     12 from .mamba import Mamba, mamba\n     13 from .mlxlm import MLXLM, mlxlm\n---> 14 from .openai import OpenAI, azure_openai, openai\n     15 from .transformers import Transformers, transformers\n     16 from .vllm import VLLM, vllm\n\nFile ~/.python/verl-sglang/lib/python3.10/site-packages/outlines/models/openai.py:9\n      5 from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n      7 import numpy as np\n----> 9 from outlines.base import vectorize\n     10 from outlines.caching import cache\n     12 __all__ = [\"OpenAI\", \"openai\", \"azure_openai\"]\n\nFile ~/.python/verl-sglang/lib/python3.10/site-packages/outlines/base.py:21\n     18 try:\n     19     import nest_asyncio\n---> 21     nest_asyncio.apply()\n     22 except ImportError:\n     23     print(\n     24         \"Couldn't patch nest_asyncio because it's not installed. Running in the notebook might be have issues\"\n     25     )\n\nFile ~/.python/verl-sglang/lib/python3.10/site-packages/nest_asyncio.py:18, in apply(loop)\n     15 _patch_policy()\n     16 _patch_tornado()\n---> 18 loop = loop or asyncio.get_event_loop()\n     19 _patch_loop(loop)\n\nFile ~/.python/verl-sglang/lib/python3.10/site-packages/nest_asyncio.py:40, in _patch_asyncio.<locals>._get_event_loop(stacklevel)\n     38 loop = events._get_running_loop()\n     39 if loop is None:\n---> 40     loop = events.get_event_loop_policy().get_event_loop()\n     41 return loop\n\nFile ~/.python/verl-sglang/lib/python3.10/site-packages/nest_asyncio.py:67, in _patch_policy.<locals>.get_event_loop(self)\n     65 if self._local._loop is None:\n     66     loop = self.new_event_loop()\n---> 67     _patch_loop(loop)\n     68     self.set_event_loop(loop)\n     69 return self._local._loop\n\nFile ~/.python/verl-sglang/lib/python3.10/site-packages/nest_asyncio.py:193, in _patch_loop(loop)\n    191     return\n    192 if not isinstance(loop, asyncio.BaseEventLoop):\n--> 193     raise ValueError('Can\\'t patch loop of type %s' % type(loop))\n    194 cls = loop.__class__\n    195 cls.run_forever = run_forever\n\nValueError: Can't patch loop of type <class 'uvloop.Loop'>\n\nTraceback (most recent call last):\n  File \"/data/chayenne/.python/verl-sglang/bin/ipython\", line 10, in <module>\n    sys.exit(start_ipython())\n  File \"/data/chayenne/.python/verl-sglang/lib/python3.10/site-packages/IPython/__init__.py\", line 130, in start_ipython\n    return launch_new_instance(argv=argv, **kwargs)\n  File \"/data/chayenne/.python/verl-sglang/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/data/chayenne/.python/verl-sglang/lib/python3.10/site-packages/IPython/terminal/ipapp.py\", line 317, in start\n    self.shell.mainloop()\n  File \"/data/chayenne/.python/verl-sglang/lib/python3.10/site-packages/IPython/terminal/interactiveshell.py\", line 998, in mainloop\n    self.interact()\n  File \"/data/chayenne/.python/verl-sglang/lib/python3.10/site-packages/IPython/terminal/interactiveshell.py\", line 983, in interact\n    code = self.prompt_for_code()\n  File \"/data/chayenne/.python/verl-sglang/lib/python3.10/site-packages/IPython/terminal/interactiveshell.py\", line 926, in prompt_for_code\n    text = self.pt_app.prompt(\n  File \"/data/chayenne/.python/verl-sglang/lib/python3.10/site-packages/prompt_toolkit/shortcuts/prompt.py\", line 1035, in prompt\n    return self.app.run(\n  File \"/data/chayenne/.python/verl-sglang/lib/python3.10/site-packages/prompt_toolkit/application/application.py\", line 1002, in run\n    return asyncio.run(coro)\n  File \"/data/chayenne/.python/verl-sglang/lib/python3.10/site-packages/nest_asyncio.py\", line 26, in run\n    loop = asyncio.get_event_loop()\n  File \"/data/chayenne/.python/verl-sglang/lib/python3.10/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n    loop = events.get_event_loop_policy().get_event_loop()\n  File \"/data/chayenne/.python/verl-sglang/lib/python3.10/site-packages/nest_asyncio.py\", line 67, in get_event_loop\n    _patch_loop(loop)\n  File \"/data/chayenne/.python/verl-sglang/lib/python3.10/site-packages/nest_asyncio.py\", line 193, in _patch_loop\n    raise ValueError('Can\\'t patch loop of type %s' % type(loop))\nValueError: Can't patch loop of type <class 'uvloop.Loop'>\n\nIf you suspect this is an IPython 8.33.0 bug, please report it at:\n    https://github.com/ipython/ipython/issues\nor send an email to the mailing list at ipython-dev@python.org\n\nYou can print a more detailed traceback right now with \"%tb\", or use \"%debug\"\nto interactively debug it.\n\nExtra-detailed tracebacks for bug-reporting purposes can be enabled via:\n    %config Application.verbose_crash=True\n\nsys:1: RuntimeWarning: coroutine 'Application.run_async' was never awaited\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n```\n\nThe engine would be killed due to the import of `nest_asyncio` in `outlines`. We can try to disable the import of `nest_asyncio` in `outlines`. Or after #4386 is merged, we can disable `outlines` import:\n\n```python\nif grammar_backend == \"outlines\":\n        import outlines\n```\n\nAnyway, do we have a plan to fully detach `outlines` from us? Or fix the `nest_asyncio` in `outlines` as soon as possible?\n\n`nest_asyncio` is publicly archived. We can fully remove it.\n\n### Reproduction\n\nLaunch ipython and run the engine:\n\n```python\n# launch the offline engine\nimport asyncio\nimport io\nimport os\n\nfrom PIL import Image\nimport requests\nimport sglang as sgl\n\nfrom sglang.srt.conversation import chat_templates\nfrom sglang.test.test_utils import is_in_ci\nfrom sglang.utils import async_stream_and_merge, stream_and_merge\n\nif is_in_ci():\n    import patch\n\n\nllm = sgl.Engine(model_path=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n```\n\n### Environment\n\nNA",
    "labels": [
      "high priority",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-16T15:37:03+00:00",
    "closed_at": "2025-05-16T00:19:25+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4478/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4478"
  },
  {
    "number": 377,
    "title": "[BUG] srt throws KeyError when sgl.gen(...) regex parameter contains Chinese characters",
    "body": "It seems that `sgl.gen(regex=)` doesn't take Chinese characters.\r\n\r\nError Details\r\n```\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \".../sglang/python/sglang/srt/managers/router/model_rpc.py\", line 175, in exposed_step\r\n    self.handle_generate_request(recv_req)\r\n  File \".../sglang/python/sglang/srt/managers/router/model_rpc.py\", line 271, in handle_generate_request\r\n    req.jump_forward_map = self.jump_forward_cache.query(\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/base_cache.py\", line 34, in query\r\n    val = _init_with_timer(key)\r\n          ^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/base_cache.py\", line 18, in _init_with_timer\r\n    val = self.init_value(key)\r\n          ^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/jump_forward.py\", line 64, in init_value\r\n    return JumpForwardMap(regex)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/jump_forward.py\", line 41, in __init__\r\n    self.state_to_jump_forward = _init_state_to_jump_forward(regex_string)\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/lib/python3.11/site-packages/outlines/caching.py\", line 74, in wrapper\r\n    result = cached_function(*args, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/jump_forward.py\", line 19, in _init_state_to_jump_forward\r\n    for symbol, id_ in symbol_to_id.items():\r\n  File \"<frozen _collections_abc>\", line 861, in __iter__\r\n  File \".../sglang/lib/python3.11/site-packages/numba/typed/typeddict.py\", line 180, in __getitem__\r\n    return _getitem(self, key)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/lib/python3.11/site-packages/numba/typed/dictobject.py\", line 778, in impl\r\n    raise KeyError()\r\nKeyError\r\n\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \".../sglang/python/sglang/srt/managers/router/model_rpc.py\", line 175, in exposed_step\r\n    self.handle_generate_request(recv_req)\r\n  File \".../sglang/python/sglang/srt/managers/router/model_rpc.py\", line 271, in handle_generate_request\r\n    req.jump_forward_map = self.jump_forward_cache.query(\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/base_cache.py\", line 34, in query\r\n    val = _init_with_timer(key)\r\n          ^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/base_cache.py\", line 18, in _init_with_timer\r\n    val = self.init_value(key)\r\n          ^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/jump_forward.py\", line 64, in init_value\r\n    return JumpForwardMap(regex)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/jump_forward.py\", line 41, in __init__\r\n    self.state_to_jump_forward = _init_state_to_jump_forward(regex_string)\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/lib/python3.11/site-packages/outlines/caching.py\", line 74, in wrapper\r\n    result = cached_function(*args, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/jump_forward.py\", line 19, in _init_state_to_jump_forward\r\n    for symbol, id_ in symbol_to_id.items():\r\n  File \"<frozen _collections_abc>\", line 861, in __iter__\r\n  File \".../sglang/lib/python3.11/site-packages/numba/typed/typeddict.py\", line 180, in __getitem__\r\n    return _getitem(self, key)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/lib/python3.11/site-packages/numba/typed/dictobject.py\", line 778, in impl\r\n    raise KeyError()\r\nKeyError\r\n\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \".../sglang/python/sglang/srt/managers/router/model_rpc.py\", line 175, in exposed_step\r\n    self.handle_generate_request(recv_req)\r\n  File \".../sglang/python/sglang/srt/managers/router/model_rpc.py\", line 271, in handle_generate_request\r\n    req.jump_forward_map = self.jump_forward_cache.query(\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/base_cache.py\", line 34, in query\r\n    val = _init_with_timer(key)\r\n          ^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/base_cache.py\", line 18, in _init_with_timer\r\n    val = self.init_value(key)\r\n          ^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/jump_forward.py\", line 64, in init_value\r\n    return JumpForwardMap(regex)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/jump_forward.py\", line 41, in __init__\r\n    self.state_to_jump_forward = _init_state_to_jump_forward(regex_string)\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/lib/python3.11/site-packages/outlines/caching.py\", line 74, in wrapper\r\n    result = cached_function(*args, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/jump_forward.py\", line 19, in _init_state_to_jump_forward\r\n    for symbol, id_ in symbol_to_id.items():\r\n  File \"<frozen _collections_abc>\", line 861, in __iter__\r\n  File \".../sglang/lib/python3.11/site-packages/numba/typed/typeddict.py\", line 180, in __getitem__\r\n    return _getitem(self, key)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/lib/python3.11/site-packages/numba/typed/dictobject.py\", line 778, in impl\r\n    raise KeyError()\r\nKeyError\r\n\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \".../sglang/python/sglang/srt/managers/router/model_rpc.py\", line 175, in exposed_step\r\n    self.handle_generate_request(recv_req)\r\n  File \".../sglang/python/sglang/srt/managers/router/model_rpc.py\", line 271, in handle_generate_request\r\n    req.jump_forward_map = self.jump_forward_cache.query(\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/base_cache.py\", line 34, in query\r\n    val = _init_with_timer(key)\r\n          ^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/base_cache.py\", line 18, in _init_with_timer\r\n    val = self.init_value(key)\r\n          ^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/jump_forward.py\", line 64, in init_value\r\n    return JumpForwardMap(regex)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/jump_forward.py\", line 41, in __init__\r\n    self.state_to_jump_forward = _init_state_to_jump_forward(regex_string)\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/lib/python3.11/site-packages/outlines/caching.py\", line 74, in wrapper\r\n    result = cached_function(*args, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/python/sglang/srt/constrained/jump_forward.py\", line 19, in _init_state_to_jump_forward\r\n    for symbol, id_ in symbol_to_id.items():\r\n  File \"<frozen _collections_abc>\", line 861, in __iter__\r\n  File \".../sglang/lib/python3.11/site-packages/numba/typed/typeddict.py\", line 180, in __getitem__\r\n    return _getitem(self, key)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang/lib/python3.11/site-packages/numba/typed/dictobject.py\", line 778, in impl\r\n    raise KeyError()\r\nKeyError\r\n```\r\n\r\nMinimum working demo:\r\n```python\r\nfrom enum import Enum\r\nfrom itertools import chain\r\nfrom typing import Set\r\nimport sglang as sgl\r\nfrom pydantic import BaseModel\r\nfrom sglang.srt.constrained import build_regex_from_object\r\n\r\nclass Port(str, Enum):\r\n    usba = \"USB A\"\r\n    usbc = \"USB C\"\r\n    lightning = \"Lightning\"\r\n    micro_usb = \"Micro USB\"\r\n    mini_usb = \"Mini USB\"\r\n    apple_watch = \"Apple Watch\"\r\n    wireless = \"\u65e0\u7ebf\"\r\n    dc_2 = \"DC 2.0mm\"\r\n    dc_35 = \"DC 3.5mm\"\r\n    unknown = \"\u672a\u77e5\"\r\n\r\nclass PortEN(str, Enum):\r\n    usba = \"USB A\"\r\n    usbc = \"USB C\"\r\n    lightning = \"Lightning\"\r\n    micro_usb = \"Micro USB\"\r\n    mini_usb = \"Mini USB\"\r\n    apple_watch = \"Apple Watch\"\r\n    wireless = \"Wireless\"\r\n    dc_2 = \"DC 2.0mm\"\r\n    dc_35 = \"DC 3.5mm\"\r\n    unknown = \"Unknown\"\r\n\r\nclass FastCharge(str, Enum):\r\n    normal = \"\u975e\u5feb\u5145\"\r\n    scp = \"\u534e\u4e3a SCP\"\r\n    fcp = \"\u534e\u4e3a FCP\"\r\n    vooc = \"OPPO VOOC\"\r\n    dart = \"\u4e00\u52a0 Dart\"\r\n    super_vooc = \"OPPO SuperVOOC\"\r\n    super_dart = \"\u4e00\u52a0 SuperDart\"\r\n    flash_charge = \"Vivo FlashCharge\"\r\n    qc2 = \"\u9ad8\u901a QC2.0\"\r\n    qc3 = \"\u9ad8\u901a QC3.0\"\r\n    pd = \"USB-PD\"\r\n    pps = \"USB-PD PPS\"\r\n    pe = \"\u8054\u53d1\u79d1 PE\"\r\n    mi = \"\u5c0f\u7c73\"\r\n    doubt = \"\u5b58\u7591\"\r\n    unknown = \"\u672a\u77e5\"\r\n\r\nclass FastChargeEN(str, Enum):\r\n    normal = \"No Fast Charge\"\r\n    scp = \"Huawei SCP\"\r\n    fcp = \"Huawei FCP\"\r\n    vooc = \"OPPO VOOC\"\r\n    dart = \"OnePlus Dart\"\r\n    super_vooc = \"OPPO SuperVOOC\"\r\n    super_dart = \"OnePlus SuperDart\"\r\n    flash_charge = \"Vivo FlashCharge\"\r\n    qc2 = \"Qualcomm QC2.0\"\r\n    qc3 = \"Qualcomm QC3.0\"\r\n    pd = \"USB-PD\"\r\n    pps = \"USB-PD PPS\"\r\n    pe = \"MediaTek PE\"\r\n    mi = \"XiaoMi\"\r\n    doubt = \"Doubt\"\r\n    unknown = \"Unknown\"\r\n\r\nclass PhoneName(BaseModel):\r\n    name: str\r\n\r\nclass PhonePort(BaseModel):\r\n    port: Set[Port]\r\n\r\nclass PhoneFastCharge(BaseModel):\r\n    fc: Set[FastCharge]\r\n\r\n@sgl.function\r\ndef pydantic_wizard_gen(s):\r\n    objs = [PhoneName, PhonePort, PhoneFastCharge]\r\n    prop_fields = list(\r\n        chain.from_iterable([list(p.model_fields.items()) for p in objs])\r\n    )\r\n    prop_keys = [p[0] for p in prop_fields]\r\n    s += \"Give me a description about iPhone 15 in the JSON format.\\n\"\r\n    forks = s.fork(len(objs))\r\n    for f, obj in zip(forks, objs):\r\n        f += sgl.gen(\r\n            \"property\",\r\n            max_tokens=128,\r\n            temperature=0,\r\n            regex=build_regex_from_object(obj),  # Requires pydantic >= 2.0\r\n        )\r\n    s.set_var(\"properties\", dict(zip(prop_keys, [f[\"property\"] for f in forks])))\r\n\r\nif __name__ == \"__main__\":\r\n    sgl.set_default_backend(sgl.RuntimeEndpoint(\"http://localhost:30000\"))\r\n    outputs = pydantic_wizard_gen.run()\r\n    print(outputs[\"properties\"])\r\n```\r\n\r\nIf I change the definition of `PhonePort` and `PhoneFastCharge` to their english alternatives, no error was thrown.",
    "labels": [],
    "state": "closed",
    "created_at": "2024-04-22T14:08:55+00:00",
    "closed_at": "2024-06-12T07:45:20+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/377/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/377"
  },
  {
    "number": 2353,
    "title": "Dynamo doesn't handle branching on AsyncCollectiveTensor well",
    "body": "See https://github.com/sgl-project/sglang/pull/2352\r\n\r\ntorch-only repro:\r\n```\r\ndiff --git a/test/distributed/_tensor/test_dtensor_compile.py b/test/distributed/_tensor/test_dtensor_compile.py\r\nindex 91fbc396f8e..09a2bf8f183 100644\r\n--- a/test/distributed/_tensor/test_dtensor_compile.py\r\n+++ b/test/distributed/_tensor/test_dtensor_compile.py\r\n@@ -544,12 +544,18 @@ class TestDTensorCompile(torch._dynamo.test_case.TestCase):\r\n\r\n     def test_dynamo_dtensor_from_local_redistribute(self):\r\n         mesh = DeviceMesh(self.device_type, torch.arange(self.world_size))\r\n+        from torch.distributed._functional_collectives import AsyncCollectiveTensor\r\n\r\n         # pass in tensor as inputs/outputs, create DTensor and run redistribute\r\n         # (allgather collective) inside the fn\r\n         def fn(x):\r\n             dt = DTensor.from_local(x, mesh, [Shard(0)], run_check=False)\r\n-            return dt.redistribute(mesh, [Replicate()]).to_local() + 2\r\n+            out = dt.redistribute(mesh, [Replicate()], async_op=True).to_local()\r\n+            return out\r\n+            if isinstance(out, AsyncCollectiveTensor):\r\n+                return out.wait()\r\n+            else:\r\n+                return out\r\n\r\n         x = torch.ones(1)\r\n         ref = fn(x)\r\n         \r\n# run with `python test/distributed/_tensor/test_dtensor_compile.py -k test_dynamo_dtensor_from_local_redistribute`\r\n```\r\n\r\nThis fails with:\r\n\r\n```\r\n  File \"/home/hirsheybar/local/a/pytorch/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 875, in functional_call\r\n    out = PropagateUnbackedSymInts(mod).run(\r\n  File \"/home/hirsheybar/local/a/pytorch/torch/fx/interpreter.py\", line 167, in run\r\n    self.env[node] = self.run_node(node)\r\n  File \"/home/hirsheybar/local/a/pytorch/torch/fx/experimental/symbolic_shapes.py\", line 6670, in run_node\r\n    result = super().run_node(n)\r\n  File \"/home/hirsheybar/local/a/pytorch/torch/fx/interpreter.py\", line 228, in run_node\r\n    return getattr(self, n.op)(n.target, args, kwargs)\r\n  File \"/home/hirsheybar/local/a/pytorch/torch/fx/interpreter.py\", line 332, in call_method\r\n    return getattr(self_obj, target)(*args_tail, **kwargs)\r\ntorch._dynamo.exc.BackendCompilerFailed: backend='<torch._dynamo.testing.CompileCounterWithBackend object at 0x7f54e584b1c0>' raised:\r\nAttributeError: 'FunctionalTensor' object has no attribute 'wait'\r\n\r\nWhile executing %wait : [num_users=1] = call_method[target=wait](args = (%out,), kwargs = {})\r\nOriginal traceback:\r\n  File \"/home/hirsheybar/local/a/pytorch/test/distributed/_tensor/test_dtensor_compile.py\", line 586, in fn\r\n    return out.wait()\r\n\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-12-04T21:46:43+00:00",
    "closed_at": "2024-12-04T21:53:27+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2353/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2353"
  }
]
[
  {
    "number": 857,
    "title": "[Bug] Error when using select without stream mode",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n\n### Describe the bug\n\nWhen using select without stream mode, I get the following error. It seems like `_execute_select` does not check whether we are in stream mode before trying to access `stream_var_event`. A simple fix is to just add a check for whether `stream_var_event` is None.\r\n\r\n```\r\nUserWarning: Error in stream_executor: Traceback (most recent call last):\r\n  File \"**********/lib/python3.11/site-packages/sglang/lang/interpreter.py\", line 327, in _thread_worker_func\r\n    self._execute(expr)\r\n  File \"**********/lib/python3.11/site-packages/sglang/lang/interpreter.py\", line 370, in _execute\r\n    self._execute(x)\r\n  File \"**********//lib/python3.11/site-packages/sglang/lang/interpreter.py\", line 370, in _execute\r\n    self._execute(x)\r\n  File \"**********//lib/python3.11/site-packages/sglang/lang/interpreter.py\", line 367, in _execute\r\n    self._execute_select(other)\r\n  File \"**********/lib/python3.11/site-packages/sglang/lang/interpreter.py\", line 556, in _execute_select\r\n    self.stream_var_event[name].set()\r\n    ~~~~~~~~~~~~~~~~~~~~~^^^^^^\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\n\n### Reproduction\n\n```python\r\nimport sglang as sgl\r\n\r\n@sgl.function\r\ndef tool_use(s, question):\r\n    s += \"To answer this question: \" + question + \". \"\r\n    s += \"I need to use a \" + sgl.gen(\"tool\", choices=[\"calculator\", \"search engine\"]) + \". \"\r\n\r\n    if s[\"tool\"] == \"calculator\":\r\n        s += \"The math expression is\" + sgl.gen(\"expression\")\r\n    elif s[\"tool\"] == \"search engine\":\r\n        s += \"The key word to search is\" + sgl.gen(\"word\")\r\n\r\nset_default_backend(RuntimeEndpoint(\"http://localhost:30000\"))\r\n\r\ntool_use.run(question=\"What is 1+1?\")\r\n```\n\n### Environment\n\n```Shell\nPython: 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) [GCC 12.3.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA A100-SXM4-80GB\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.4, V12.4.99\r\nCUDA Driver Version: 550.54.14\r\n550.54.14\r\n550.54.14\r\n550.54.14\r\n550.54.14\r\n550.54.14\r\n550.54.14\r\n550.54.14\r\nPyTorch: 2.3.1+cu121\r\nsglang: 0.2.7\r\nflashinfer: 0.1.3+cu121torch2.3\r\nrequests: 2.32.3\r\ntqdm: 4.66.4\r\nnumpy: 1.26.4\r\naiohttp: 3.10.0\r\nfastapi: 0.111.1\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.5\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.4\r\nuvloop: 0.19.0\r\nzmq: 26.0.3\r\nvllm: 0.5.3.post1\r\nopenai: 1.37.1\r\nanthropic: 0.32.0\r\nNVIDIA Topology: \r\n\tGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tPXB\tPXB\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU1\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tPXB\tPXB\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU2\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU3\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU4\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tSYS\tSYS\tSYS\tSYS\t32-63,96-127\t1\t\tN/A\r\nGPU5\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tSYS\tSYS\tSYS\tSYS\t32-63,96-127\t1\t\tN/A\r\nGPU6\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tSYS\tSYS\tPXB\tPXB\t32-63,96-127\t1\t\tN/A\r\nGPU7\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tSYS\tSYS\tPXB\tPXB\t32-63,96-127\t1\t\tN/A\r\nNIC0\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPXB\tSYS\tSYS\t\t\t\t\r\nNIC1\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\t X \tSYS\tSYS\t\t\t\t\r\nNIC2\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\t X \tPXB\t\t\t\t\r\nNIC3\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tPXB\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n\r\n\r\nulimit soft: 1024\n```\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-01T00:04:58+00:00",
    "closed_at": "2024-08-01T07:05:40+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/857/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/857"
  },
  {
    "number": 4184,
    "title": "Decode out of memory happened when run deepseek-r1 inference",
    "body": "\n```\n[2025-03-07 21:49:33 TP13] Decode out of memory happened. #retracted_reqs: 1, #new_token_ratio: 0.1474 -> 0.1845\n[2025-03-07 21:49:33 TP12] Decode out of memory happened. #retracted_reqs: 1, #new_token_ratio: 0.1474 -> 0.1845\n[2025-03-07 21:49:33 TP10] Decode out of memory happened. #retracted_reqs: 1, #new_token_ratio: 0.1474 -> 0.1845\n[2025-03-07 21:49:45 TP9] Decode out of memory happened. #retracted_reqs: 1, #new_token_ratio: 0.0980 -> 0.1919\n[2025-03-07 21:49:45 TP11] Decode out of memory happened. #retracted_reqs: 1, #new_token_ratio: 0.0980 -> 0.1919\n[2025-03-07 21:49:45 TP8] Decode out of memory happened. #retracted_reqs: 1, #new_token_ratio: 0.0980 -> 0.1919\n[2025-03-07 21:49:45 TP12] Decode out of memory happened. #retracted_reqs: 1, #new_token_ratio: 0.0980 -> 0.1919\n[2025-03-07 21:49:45 TP14] Decode out of memory happened. #retracted_reqs: 1, #new_token_ratio: 0.0980 -> 0.1919\n[2025-03-07 21:49:45 TP13] Decode out of memory happened. #retracted_reqs: 1, #new_token_ratio: 0.0980 -> 0.1919\n[2025-03-07 21:49:45 TP15] Decode out of memory happened. #retracted_reqs: 1, #new_token_ratio: 0.0980 -> 0.1919\n[2025-03-07 21:49:45 TP10] Decode out of memory happened. #retracted_reqs: 1, #new_token_ratio: 0.0980 -> 0.1919\n[2025-03-07 21:49:51 TP9] Decode out of memory happened. #retracted_reqs: 1, #new_token_ratio: 0.1207 -> 0.1955\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-07T15:52:26+00:00",
    "closed_at": "2025-03-07T17:13:29+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4184/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4184"
  },
  {
    "number": 4257,
    "title": "[Bug] Server stuck with tp > 1",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI'm deploying DeepSeek-R1-Distill-Qwen-14B with single node and two gpu card. My problem is, when I deploy this model with single card, the server start up smoothly, but when I add --tp 2, the server got stuck at this logging line:\n[2025-03-10 16:21:32 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,\nAnd after 5 minutes, the watch dog timeout. The error log can be found in the reproduction.\nIf I add --disable-custom-all-reduce, the multi-gpu server can be started up, but I think this config should only be add during debugging and will decrease the capability for the online server. Any way to solve this problem?\n\n\n### Reproduction\n\n(suyf) op@bjxg-ap-10-10-76-1:~$ sudo env \"PATH=$PATH\" CUDA_VISIBLE_DEVICES=2,3 python -m sglang.launch_server --model-path /data1/model_checkpoint/DeepSeek-R1-Distill-Qwen-14B --max-total-tokens 10240 --tp 2 --port 30001\nINFO 03-10 16:19:26 __init__.py:190] Automatically detected platform cuda.\n[2025-03-10 16:19:28] server_args=ServerArgs(model_path='/data1/model_checkpoint/DeepSeek-R1-Distill-Qwen-14B', tokenizer_path='/data1/model_checkpoint/DeepSeek-R1-Distill-Qwen-14B', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='/data1/model_checkpoint/DeepSeek-R1-Distill-Qwen-14B', chat_template=None, is_embedding=False, revision=None, host='127.0.0.1', port=30001, mem_fraction_static=0.87, max_running_requests=None, max_total_tokens=10240, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, tp_size=2, stream_interval=1, stream_output=False, random_seed=533645763, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=5, speculative_eagle_topk=4, speculative_num_draft_tokens=8, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=False, flashinfer_mla_disable_ragged=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False)\nINFO 03-10 16:19:33 __init__.py:190] Automatically detected platform cuda.\nINFO 03-10 16:19:33 __init__.py:190] Automatically detected platform cuda.\nINFO 03-10 16:19:33 __init__.py:190] Automatically detected platform cuda.\n[2025-03-10 16:19:35 TP1] Init torch distributed begin.\n[2025-03-10 16:19:35 TP0] Init torch distributed begin.\n[2025-03-10 16:19:36 TP0] sglang is using nccl==2.21.5\n[2025-03-10 16:19:36 TP1] sglang is using nccl==2.21.5\n[2025-03-10 16:19:37 TP0] Init torch distributed ends. mem usage=0.25 GB\n[2025-03-10 16:19:37 TP1] Init torch distributed ends. mem usage=0.25 GB\n[2025-03-10 16:19:37 TP0] Load weight begin. avail mem=38.71 GB\n[2025-03-10 16:19:37 TP1] Load weight begin. avail mem=38.71 GB\n[2025-03-10 16:19:37 TP0] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-03-10 16:19:37 TP1] The following error message 'operation scheduled before its operands' can be ignored.\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:40<02:02, 40.76s/it]\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [01:11<01:09, 34.60s/it]\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [01:13<00:19, 19.88s/it]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [01:43<00:00, 24.00s/it]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [01:43<00:00, 25.93s/it]\n\n[2025-03-10 16:21:21 TP1] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=24.71 GB, mem usage=14.00 GB.\n[2025-03-10 16:21:21 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=24.71 GB, mem usage=14.00 GB.\n[2025-03-10 16:21:21 TP1] KV Cache is allocated. #tokens: 10240, K size: 0.47 GB, V size: 0.47 GB\n[2025-03-10 16:21:21 TP0] KV Cache is allocated. #tokens: 10240, K size: 0.47 GB, V size: 0.47 GB\n[2025-03-10 16:21:21 TP1] Memory pool end. avail mem=22.59 GB\n[2025-03-10 16:21:21 TP0] Memory pool end. avail mem=22.59 GB\n[2025-03-10 16:21:21 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=21.96 GB\n  0%|                                                           | 0/23 [00:00<?, ?it/s][2025-03-10 16:21:21 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=21.96 GB\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 23/23 [00:09<00:00,  2.45it/s]\n[2025-03-10 16:21:31 TP1] Registering 2231 cuda graph addresses\n[2025-03-10 16:21:31 TP0] Registering 2231 cuda graph addresses\n[2025-03-10 16:21:31 TP1] Capture cuda graph end. Time elapsed: 9.40 s. avail mem=18.57 GB. mem usage=3.38 GB.\n[2025-03-10 16:21:31 TP0] Capture cuda graph end. Time elapsed: 9.41 s. avail mem=18.57 GB. mem usage=3.38 GB.\n[2025-03-10 16:21:31 TP0] max_total_num_tokens=10240, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=131072\n[2025-03-10 16:21:31 TP1] max_total_num_tokens=10240, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=131072\n[2025-03-10 16:21:31] INFO:     Started server process [2696108]\n[2025-03-10 16:21:31] INFO:     Waiting for application startup.\n[2025-03-10 16:21:31] INFO:     Application startup complete.\n[2025-03-10 16:21:31] INFO:     Uvicorn running on http://127.0.0.1:30001 (Press CTRL+C to quit)\n[2025-03-10 16:21:32] INFO:     127.0.0.1:51288 - \"GET /get_model_info HTTP/1.1\" 200 OK\n[2025-03-10 16:21:32 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0,\n[2025-03-10 16:29:01 TP0] Watchdog timeout (self.watchdog_timeout=300)\n[2025-03-10 16:29:01 TP1] Watchdog timeout (self.watchdog_timeout=300)\n[2025-03-10 16:29:01 TP0] self.cur_batch.batch_size()=1, self.cur_batch.reqs=[Req(rid=03c78b05bc354f3e8a9774190a03299f, input_ids=[151646, 785, 6722, 3283, 315, 9625, 374], output_ids=[])], self.token_to_kv_pool_allocator.available_size()=10232, self.tree_cache.evictable_size()=0,\n[2025-03-10 16:29:01 TP1] self.cur_batch.batch_size()=1, self.cur_batch.reqs=[Req(rid=03c78b05bc354f3e8a9774190a03299f, input_ids=[151646, 785, 6722, 3283, 315, 9625, 374], output_ids=[])], self.token_to_kv_pool_allocator.available_size()=10232, self.tree_cache.evictable_size()=0,\n[2025-03-10 16:29:01 TP1] Pyspy dump for PID 2696394:\nProcess 2696394: sglang::scheduler_None\nPython v3.11.10 (/data/anaconda3/envs/suyf/bin/python3.11)\n\nThread 2696394 (idle): \"MainThread\"\n    wait (threading.py:327)\n    get (queue.py:171)\n    resolve_batch_result (tp_worker_overlap_thread.py:169)\n    process_batch_result_prefill (scheduler.py:1261)\n    process_batch_result (scheduler.py:1218)\n    event_loop_overlap (scheduler.py:527)\n    decorate_context (torch/utils/_contextlib.py:116)\n    run_scheduler_process (scheduler.py:2264)\n    run (multiprocessing/process.py:108)\n    _bootstrap (multiprocessing/process.py:314)\n    _main (multiprocessing/spawn.py:135)\n    spawn_main (multiprocessing/spawn.py:122)\n    <module> (<string>:1)\nThread 2697141 (idle): \"Thread-1\"\n    wait (threading.py:331)\n    wait (threading.py:629)\n    run (tqdm/_monitor.py:60)\n    _bootstrap_inner (threading.py:1045)\n    _bootstrap (threading.py:1002)\nThread 2699269 (active): \"Thread-2 (forward_thread_func)\"\n    __call__ (torch/_ops.py:1116)\n    __torch_function__ (torch/utils/_device.py:106)\n    __call__ (torch/_ops.py:1116)\n    ragged_run (flashinfer/prefill.py:237)\n    run (flashinfer/prefill.py:2359)\n    forward_return_lse (flashinfer/prefill.py:2387)\n    forward_extend (flashinfer_backend.py:425)\n    forward (base_attn_backend.py:77)\n    forward (radix_attention.py:68)\n    _call_impl (torch/nn/modules/module.py:1747)\n    _wrapped_call_impl (torch/nn/modules/module.py:1736)\n    forward (qwen2.py:170)\n    _call_impl (torch/nn/modules/module.py:1747)\n    _wrapped_call_impl (torch/nn/modules/module.py:1736)\n    forward (qwen2.py:224)\n    _call_impl (torch/nn/modules/module.py:1747)\n    _wrapped_call_impl (torch/nn/modules/module.py:1736)\n    forward (qwen2.py:285)\n    _call_impl (torch/nn/modules/module.py:1747)\n    _wrapped_call_impl (torch/nn/modules/module.py:1736)\n    forward (qwen2.py:374)\n    decorate_context (torch/utils/_contextlib.py:116)\n    forward_extend (model_runner.py:870)\n    forward (model_runner.py:909)\n    forward_batch_generation (tp_worker.py:172)\n    forward_thread_func_ (tp_worker_overlap_thread.py:140)\n    decorate_context (torch/utils/_contextlib.py:116)\n    forward_thread_func (tp_worker_overlap_thread.py:109)\n    run (threading.py:982)\n    _bootstrap_inner (threading.py:1045)\n    _bootstrap (threading.py:1002)\nThread 2699271 (idle): \"Thread-3 (watchdog_thread)\"\n    select (selectors.py:415)\n    _communicate (subprocess.py:2115)\n    communicate (subprocess.py:1209)\n    run (subprocess.py:550)\n    pyspy_dump_schedulers (utils.py:1329)\n    watchdog_thread (scheduler.py:1918)\n    run (threading.py:982)\n    _bootstrap_inner (threading.py:1045)\n    _bootstrap (threading.py:1002)\nThread 2699440 (idle): \"Thread-4\"\n    wait (threading.py:331)\n    wait (threading.py:629)\n    run (tqdm/_monitor.py:60)\n    _bootstrap_inner (threading.py:1045)\n    _bootstrap (threading.py:1002)\n\n\n\n[2025-03-10 16:29:01 TP0] Pyspy dump for PID 2696393:\nProcess 2696393: sglang::scheduler_None\nPython v3.11.10 (/data/anaconda3/envs/suyf/bin/python3.11)\n\nThread 2696393 (idle): \"MainThread\"\n    wait (threading.py:327)\n    get (queue.py:171)\n    resolve_batch_result (tp_worker_overlap_thread.py:169)\n    process_batch_result_prefill (scheduler.py:1261)\n    process_batch_result (scheduler.py:1218)\n    event_loop_overlap (scheduler.py:527)\n    decorate_context (torch/utils/_contextlib.py:116)\n    run_scheduler_process (scheduler.py:2264)\n    run (multiprocessing/process.py:108)\n    _bootstrap (multiprocessing/process.py:314)\n    _main (multiprocessing/spawn.py:135)\n    spawn_main (multiprocessing/spawn.py:122)\n    <module> (<string>:1)\nThread 2697140 (idle): \"Thread-1\"\n    wait (threading.py:331)\n    wait (threading.py:629)\n    run (tqdm/_monitor.py:60)\n    _bootstrap_inner (threading.py:1045)\n    _bootstrap (threading.py:1002)\nThread 2699062 (idle): \"Thread-2\"\n    wait (threading.py:331)\n    wait (threading.py:629)\n    run (tqdm/_monitor.py:60)\n    _bootstrap_inner (threading.py:1045)\n    _bootstrap (threading.py:1002)\nThread 2699268 (active): \"Thread-3 (forward_thread_func)\"\n    __call__ (torch/_ops.py:1116)\n    __torch_function__ (torch/utils/_device.py:106)\n    __call__ (torch/_ops.py:1116)\n    ragged_run (flashinfer/prefill.py:237)\n    run (flashinfer/prefill.py:2359)\n    forward_return_lse (flashinfer/prefill.py:2387)\n    forward_extend (flashinfer_backend.py:425)\n    forward (base_attn_backend.py:77)\n    forward (radix_attention.py:68)\n    _call_impl (torch/nn/modules/module.py:1747)\n    _wrapped_call_impl (torch/nn/modules/module.py:1736)\n    forward (qwen2.py:170)\n    _call_impl (torch/nn/modules/module.py:1747)\n    _wrapped_call_impl (torch/nn/modules/module.py:1736)\n    forward (qwen2.py:224)\n    _call_impl (torch/nn/modules/module.py:1747)\n    _wrapped_call_impl (torch/nn/modules/module.py:1736)\n    forward (qwen2.py:285)\n    _call_impl (torch/nn/modules/module.py:1747)\n    _wrapped_call_impl (torch/nn/modules/module.py:1736)\n    forward (qwen2.py:374)\n    decorate_context (torch/utils/_contextlib.py:116)\n    forward_extend (model_runner.py:870)\n    forward (model_runner.py:909)\n    forward_batch_generation (tp_worker.py:172)\n    forward_thread_func_ (tp_worker_overlap_thread.py:140)\n    decorate_context (torch/utils/_contextlib.py:116)\n    forward_thread_func (tp_worker_overlap_thread.py:109)\n    run (threading.py:982)\n    _bootstrap_inner (threading.py:1045)\n    _bootstrap (threading.py:1002)\nThread 2699270 (idle): \"Thread-4 (watchdog_thread)\"\n    select (selectors.py:415)\n    _communicate (subprocess.py:2115)\n    communicate (subprocess.py:1209)\n    run (subprocess.py:550)\n    pyspy_dump_schedulers (utils.py:1329)\n    watchdog_thread (scheduler.py:1918)\n    run (threading.py:982)\n    _bootstrap_inner (threading.py:1045)\n    _bootstrap (threading.py:1002)\n\n\n\n[2025-03-10 16:29:06] Received sigquit from a child process. It usually means the child failed.\nKilled\n\n### Environment\n\n(suyf) op@bjxg-ap-10-10-76-1:~$ python3 -m sglang.check_env\nCUDA available: True\nGPU 0,1: NVIDIA A100-PCIE-40GB\nGPU 0,1 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.2, V12.2.91\nCUDA Driver Version: 535.54.03\nPyTorch: 2.5.1+cu121\nsglang: 0.4.3.post4\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.2.post1+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.10.10\nfastapi: 0.115.5\nhf_transfer: 0.1.9\nhuggingface_hub: 0.26.2\ninteregular: 0.3.3\nmodelscope: 1.18.1\norjson: 3.10.11\npackaging: 24.2\npsutil: 6.1.0\npydantic: 2.9.2\nmultipart: 0.0.12\nzmq: 26.2.0\nuvicorn: 0.29.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.65.4\ntiktoken: 0.8.0\nanthropic: 0.49.0\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0        GPU1        GPU2        GPU3        CPU Affinity        NUMA Affinity        GPU NUMA ID\nGPU0         X         NODE        SYS        SYS        0,2,4,6,8,10        0                N/A\nGPU1        NODE         X         SYS        SYS        0,2,4,6,8,10        0                N/A\nGPU2        SYS        SYS         X         NODE        1,3,5,7,9,11        1                N/A\nGPU3        SYS        SYS        NODE         X         1,3,5,7,9,11        1                N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 655350",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-10T08:58:49+00:00",
    "closed_at": "2025-03-10T19:40:23+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4257/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4257"
  },
  {
    "number": 5897,
    "title": "[Bug] HF_Runner can't produce correct results after applying lora",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nFirst batch input lora_path as [a, a]. Second batch input lora_path as [None, None]. The second batch will be processed as if you had input lora_path as [a, a].\n\n### Reproduction\n\n```\n# Copyright 2023-2024 SGLang Team\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport multiprocessing as mp\nimport unittest\n\nimport torch\n\nfrom sglang.test.runners import HFRunner, SRTRunner\nfrom sglang.test.test_utils import CustomTestCase\n\nLORA_SETS = [\n    # {\n    #     \"base\": \"meta-llama/Llama-2-7b-hf\",\n    #     \"loras\": [\"RuterNorway/Llama-2-7b-chat-norwegian-LoRa\"],\n    # },\n    {\"base\": \"meta-llama/Llama-2-7b-hf\", \"loras\": [\"winddude/wizardLM-LlaMA-LoRA-7B\"]},\n    # {\"base\": \"Qwen/Qwen2.5-14B-Instruct\", \"loras\": [\"mssongit/Qwen2.5-14B-SFT-LoRA\"]},\n    # {\"base\": \"mistralai/Mistral-7B-Instruct-v0.3\", \"loras\": [\"/home/ying/test_lora\"]},\n    # {\n    # \"base\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n    #     \"loras\": [\n    #         \"/home/ying/test_lora\",\n    #         \"/home/ying/test_lora_1\",\n    #         \"/home/ying/test_lora_2\",\n    #         \"/home/ying/test_lora_3\",\n    #         \"/home/ying/test_lora_4\",\n    #     ],\n    # },\n    # {\"base\": \"meta-llama/Llama-2-7b-hf\", \"loras\": [\"yard1/llama-2-7b-sql-lora-test\"]},\n]\nTORCH_DTYPES = [torch.float16]\n\nPROMPTS = [\n    \"\"\"\n### Instruction:\nWrite a poem about the transformers Python library.\nMention the word \"large language models\" in that poem.\n### Response:\nThe Transformers are large language models,\nThey're used to make predictions on text.\n\"\"\",\n    \"\"\"\n### Instruction:\nWrite a poem about the transformers Python library.\nMention the word \"large language models\" in that poem.\n### Response:\nThe Transformers are large language models,\nThey're used to make predictions on text.\n\"\"\",\n]\n\n\nclass TestLoRA(CustomTestCase):\n\n    def inference(self, prompts, lora_set, tp_size, torch_dtype, max_new_tokens):\n        print(\"=================== testing inference =======================\")\n        base_path = lora_set[\"base\"]\n        all_lora_paths = lora_set[\"loras\"]\n        batch_lora_paths = [all_lora_paths[0], all_lora_paths[0]]\n\n        with HFRunner(\n            base_path, torch_dtype=torch_dtype, model_type=\"generation\"\n        ) as hf_runner:\n            hf_outputs = hf_runner.forward(\n                prompts, max_new_tokens=max_new_tokens, lora_paths=batch_lora_paths\n            )\n\n            hf_no_lora_outputs = hf_runner.forward(\n                prompts, max_new_tokens=max_new_tokens, lora_paths=[None] * len(prompts)\n            )\n        with HFRunner(\n            base_path, torch_dtype=torch_dtype, model_type=\"generation\"\n        ) as hf_runner:\n            hf_no_lora_outputs1 = hf_runner.forward(\n                prompts, max_new_tokens=max_new_tokens, lora_paths=[None] * len(prompts)\n            )\n\n        # compare output strings\n        print(f\"{hf_outputs.output_strs=}\")\n        print(f\"{hf_no_lora_outputs.output_strs=}\")\n        print(f\"{hf_no_lora_outputs1.output_strs=}\")\n\n        for i in range(len(prompts)):\n            assert hf_no_lora_outputs.output_strs[i].strip(\n                \" \"\n            ) == hf_no_lora_outputs1.output_strs[i].strip(\" \"), (\n                hf_no_lora_outputs.output_strs[i].strip(\" \"),\n                hf_no_lora_outputs1.output_strs[i].strip(\" \"),\n            )\n\n    def test_all(self):\n        for lora_set in LORA_SETS:\n            for torch_dtype in TORCH_DTYPES:\n                tp_size = 1\n                max_new_tokens = 32\n                self.inference(PROMPTS, lora_set, tp_size, torch_dtype, max_new_tokens)\n\n\nif __name__ == \"__main__\":\n    try:\n        mp.set_start_method(\"spawn\")\n    except RuntimeError:\n        pass\n\n    unittest.main(warnings=\"ignore\")\n\n```\n\n### Environment\n\n```\npython3 -m sglang.check_env\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0: NVIDIA H100 80GB HBM3\nGPU 0 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.144.03\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post1\nsgl_kernel: 0.1.0\nflashinfer: Module Not Found\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.14\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.24.0\norjson: 3.10.15\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.1.dev5770+g268c325.precompiled\nxgrammar: 0.1.17\nopenai: 1.68.2\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.63.14\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4      NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS       SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     NODE    NODE    SYS       SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    PIX     NODE    SYS       SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     SYS       SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX       NODE    NODE    NODE    32-63,96-127    1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE      PIX     NODE    NODE    32-63,96-127    1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE      NODE    PIX     NODE    32-63,96-127    1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     NODE      NODE    NODE    PIX     32-63,96-127    1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS       SYS     SYS     SYS\nNIC1    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS       SYS     SYS     SYS\nNIC2    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS       SYS     SYS     SYS\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS       SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X        NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE       X      NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE      NODE     X      NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE      NODE    NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n\n\nulimit soft: 1048576\n```",
    "labels": [
      "bug",
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-04-30T00:10:58+00:00",
    "closed_at": "2025-04-30T03:17:43+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5897/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5897"
  },
  {
    "number": 4050,
    "title": "[Bug] Directly importing Grafana JSON does not work",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nIf you directly import Grafana from https://github.com/sgl-project/sglang/blob/main/examples/monitoring/grafana.json, it will display the following error:\n\n```\nFailed to upgrade legacy queries Datasource aeboq3sqk89vkd was not found\n```\n\n![Image](https://github.com/user-attachments/assets/17b0d0bf-b3a3-4802-ac9d-2c0d5a547e50)\n\n### Reproduction\n\nIn Grafana, import https://github.com/sgl-project/sglang/blob/main/examples/monitoring/grafana.json panel, open this panel, it reports an error. \n\n### Environment\n\nN/A",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-04T06:20:49+00:00",
    "closed_at": "2025-03-04T11:44:52+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4050/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4050"
  },
  {
    "number": 1730,
    "title": "[Bug] 100% CPU Usage When Idle in sglang",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n## Issue Description\r\n\r\nWhile running sglang with no active workload, I observed 100% CPU usage without any CUDA utilization. This issue appears to be caused by a non-blocking ZMQ receive operation in the `recv_requests` function of `scheduler.py`.\r\n\r\n## Steps to Reproduce\r\n\r\n1. Start sglang\r\n2. Run `htop` to observe high CPU usage\r\n3. Use `py-spy` to collect stack information for the high CPU process:\r\n   ```\r\n   3.1. pip3 install py-spy \r\n   3.2. sudo env \"PATH=$PATH\" py-spy top -p <PID>\r\n   ```\r\n\r\n## Findings\r\n\r\nThe `py-spy` output shows that most of the time is spent in the `recv_requests` function:\r\n\r\n```\r\nCollecting samples from '/usr/bin/python3 -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=21, pipe_handle=23)\r\nCollecting samples from '/usr/bin/python3 -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=21, pipe_handle=23) --multiprocessing-fork' (python v3.10.15)\r\nTotal Samples 7732\r\nGIL: 64.00%, Active: 200.00%, Threads: 3\r\n\r\n  %Own   %Total  OwnTime  TotalTime  Function (filename)\r\n100.00% 100.00%   77.32s    77.32s   _recv_msg (torch/_inductor/compile_worker/subproc_pool.py)\r\n 51.00%  67.00%   34.42s    49.74s   recv_pyobj (zmq/sugar/socket.py)\r\n  9.00%  16.00%   11.51s    15.32s   __init__ (zmq/error.py)\r\n  7.00%  74.00%    8.93s    58.67s   recv_requests (scheduler.py)\r\n 12.00%  14.00%    5.15s     6.69s   check_memory (scheduler.py)\r\n  3.00%  18.00%    4.28s    12.59s   run_step (scheduler.py)\r\n  7.00%   7.00%    3.81s     3.81s   _handle_fromlist (<frozen importlib._bootstrap>)\r\n  6.00% 100.00%    3.70s    77.32s   event_loop (scheduler.py)\r\n  2.00%   2.00%    1.65s     1.65s   send_results (scheduler.py)\r\n  1.00%   1.00%    1.62s     1.62s   get_new_batch_prefill (scheduler.py)\r\n  2.00%   2.00%   0.910s    0.910s   available_size (memory_pool.py)\r\n  0.00%   0.00%   0.710s    0.710s   process_input_requests (scheduler.py)\r\n  0.00%   0.00%   0.630s    0.630s   evictable_size (radix_cache.py)\r\n  0.00% 100.00%   0.000s    77.32s   _bootstrap_inner (threading.py)\r\n```\r\n\r\nThe problematic code in `scheduler.py` is:\r\n\r\n```python\r\ndef recv_requests(self):\r\n    if self.tp_rank == 0:\r\n        recv_reqs = []\r\n\r\n        while True:\r\n            try:\r\n                recv_req = self.recv_from_tokenizer.recv_pyobj(zmq.NOBLOCK)\r\n            except zmq.ZMQError:\r\n                break\r\n            recv_reqs.append(recv_req)\r\n    else:\r\n        recv_reqs = None\r\n\r\n    if self.tp_size != 1:\r\n        recv_reqs = broadcast_pyobj(recv_reqs, self.tp_rank, self.tp_cpu_group)\r\n    return recv_reqs\r\n```\r\n\r\nThe issue is likely caused by the use of `zmq.NOBLOCK` in the ZMQ receive operation.\r\n\r\n## Proposed Solution\r\n\r\nAfter analyzing the issue, I propose implementing a blocking receive operation with a timeout instead of the current non-blocking approach. This should significantly reduce CPU usage during idle periods while still maintaining system responsiveness.\r\n\r\nI will attempt to implement this solution and submit a pull request with the proposed changes for review.\n\n### Reproduction\n\n\r\n## docker-compose.yml\r\n```yml\r\nversion: \"3.9\"\r\n\r\nx-node-common:\r\n  &node-common\r\n\r\n  restart: always\r\n  platform: linux/amd64\r\n\r\n  environment:\r\n    &node-common-env\r\n    TZ: Asia/Shanghai\r\n    LOG_LEVEL: INFO\r\n  image: lmsysorg/sglang:v0.3.3.post1-cu121-srt\r\n \r\n\r\n  runtime: nvidia\r\n\r\n  logging:\r\n    driver: json-file\r\n    options:\r\n      max-size: \"100m\"\r\n      max-file: \"10\"\r\n\r\nservices:\r\n\r\n  ant-sglang-20241016-01:\r\n    <<: *node-common\r\n    container_name: ant-sglang-20241016-01\r\n    shm_size: '8gb'\r\n    volumes:\r\n      - /data/models/Qwen2.5-3B-Instruct-AWQ:/data/models/Qwen2.5-3B-Instruct-AWQ\r\n    environment:\r\n      <<: *node-common-env\r\n      CUDA_VISIBLE_DEVICES: 0\r\n\r\n    ports:\r\n       - 43567:43567\r\n\r\n    # healthcheck:\r\n    #   test: [\"CMD\", \"curl\", \"--fail\", \"--location\", \"http://127.0.0.1:43567/health\"]\r\n    #   interval: 5s\r\n    #   timeout: 5s\r\n    #   retries: 12\r\n    #   start_period: 3m\r\n \r\n  \r\n\r\n    entrypoint: python3 -m sglang.launch_server --host 0.0.0.0 --port 43567 --model-path /data/models/Qwen2.5-3B-Instruct-AWQ --tp 1 --quantization awq --mem-fraction-static 0.75 --context-length 32768 --served-model-name qwen2.5-3b-awq  \r\n\r\n    # entrypoint: python3 -c \"import time;time.sleep(100000000)\"\r\n```\n\n### Environment\n\n```\r\nroot@abd022dbffbe:/sgl-workspace# python3 -m sglang.check_env\r\nPython: 3.10.15 (main, Sep  7 2024, 18:35:33) [GCC 9.4.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA GeForce RTX 3070 Ti\r\nGPU 0 Compute Capability: 8.6\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\r\nCUDA Driver Version: 555.42.06\r\nPyTorch: 2.4.0+cu121\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.45.2\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-10-20T16:13:19+00:00",
    "closed_at": "2024-10-20T18:41:23+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1730/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1730"
  },
  {
    "number": 3345,
    "title": "[Bug] deepseek-R1 671b can not set tensor_parallel_size=32",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI used 4 nodes H100 * 8  TP32 to deploy the deepseek-R1 671B model\uff0can error occurred. \nHowever, when I used 2 nodes H100 * 8 TP16 to deploy, the inference was normal. \nThe following is the error message:\n\n![Image](https://github.com/user-attachments/assets/2a9c3091-dc51-41b4-b54c-821e4f00d8bb)\n\n### Reproduction\n\ndeepseek-R1 671B\n\n# node 1\npython -m sglang.launch_server --model-path DeepSeek-R1 --tp 32 --dist-init-addr [fdbd:dccd:3dc2:12c8:0:1c::]:5000 --nnodes 4 --node-rank 0 --trust-remote-code\n\n# node 2\npython -m sglang.launch_server --model-path DeepSeek-R1 --tp 32 --dist-init-addr [fdbd:dccd:3dc2:12c8:0:1c::]:5000 --nnodes 4 --node-rank 1 --trust-remote-code\n\n# node 3\npython -m sglang.launch_server --model-path DeepSeek-R1 --tp 32 --dist-init-addr [fdbd:dccd:3dc2:12c8:0:1c::]:5000 --nnodes 4 --node-rank 2 --trust-remote-code\n\n# node 4\npython -m sglang.launch_server --model-path DeepSeek-R1 --tp 32 --dist-init-addr [fdbd:dccd:3dc2:12c8:0:1c::]:5000 --nnodes 4 --node-rank 3 --trust-remote-code\n\n### Environment\n\nPython: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\nCUDA Driver Version: 535.129.03\nPyTorch: 2.5.1+cu121\nsglang: 0.4.2.post2\nflashinfer: 0.2.0.post2\ntriton: 3.1.0\ntransformers: 4.48.2\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.61.1\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    0-51,104-155    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    0-51,104-155    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    0-51,104-155    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    0-51,104-155    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    52-103,156-207  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    52-103,156-207  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    52-103,156-207  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      52-103,156-207  1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nHypervisor vendor: KVM\nulimit soft: 1024768",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-06T12:12:30+00:00",
    "closed_at": "2025-02-06T18:03:15+00:00",
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3345/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3345"
  },
  {
    "number": 945,
    "title": "[Bug] disable_flashinfer didn't take effect",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n\n### Describe the bug\n\nI tried pip install flashinfer, but got error from\r\n```\r\n File \"/usr/local/python/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 28, in <module>\r\n    from flashinfer import (\r\nModuleNotFoundError: No module named 'flashinfer'\r\n```\r\n\r\nAccording to readme, i tried to set disable_flashinfer when init the runtime, my code is like\r\n\r\n```\r\nself.runtime = sgl.Runtime(\r\n            model_path = self.loader.target_path,\r\n            tp_size = envs.TENSOR_PARALLEL_SIZE,\r\n            trust_remote_code = True,\r\n            max_num_reqs = 40,\r\n            dtype = 'bfloat16',\r\n            # set back to false if flashinfer is installed.\r\n            disable_flashinfer = True,\r\n            disable_flashinfer_sampling = True,\r\n        )\r\n        sql.set_default_backend(self.runtime)\r\n\r\n```\r\nbut seems the two args didn't take effect, i still got\r\n```\r\n File \"/usr/local/python/lib/python3.10/site-packages/sglang/api.py\", line 37, in Runtime\r\n    from sglang.srt.server import Runtime\r\n  File \"/usr/local/python/lib/python3.10/site-packages/sglang/srt/server.py\", line 47, in <module>\r\n    from sglang.srt.managers.controller_multi import (\r\n  File \"/usr/local/python/lib/python3.10/site-packages/sglang/srt/managers/controller_multi.py\", line 30, in <module>\r\n    from sglang.srt.managers.controller_single import (\r\n  File \"/usr/local/python/lib/python3.10/site-packages/sglang/srt/managers/controller_single.py\", line 25, in <module>\r\n    from sglang.srt.managers.tp_worker import (\r\n  File \"/usr/local/python/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 49, in <module>\r\n    from sglang.srt.model_executor.model_runner import ModelRunner\r\n  File \"/usr/local/python/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 28, in <module>\r\n    from flashinfer import (\r\nModuleNotFoundError: No module named 'flashinfer'\r\n```\n\n### Reproduction\n\n```\r\nself.runtime = sgl.Runtime(\r\n            model_path = self.loader.target_path,\r\n            tp_size = envs.TENSOR_PARALLEL_SIZE,\r\n            trust_remote_code = True,\r\n            max_num_reqs = 40,\r\n            dtype = 'bfloat16',\r\n            # set back to false if flashinfer is installed.\r\n            disable_flashinfer = True,\r\n            disable_flashinfer_sampling = True,\r\n        )\r\n        sql.set_default_backend(self.runtime)\r\n\r\n```\n\n### Environment\n\n```Shell\n$python3 -m sglang.check_env\r\n\r\nPython: 3.8.10 (default, Mar 25 2024, 10:42:49) [GCC 9.4.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA GeForce RTX 4090\r\nCUDA_HOME: /usr/local/cuda-12.2\r\nNVCC: Cuda compilation tools, release 12.2, V12.2.140\r\nCUDA Driver Version: 535.104.05\r\n535.104.05\r\n535.104.05\r\n535.104.05\r\n535.104.05\r\n535.104.05\r\n535.104.05\r\n535.104.05\r\nPyTorch: 2.3.1+cu121\r\nsglang: 0.2.9\r\nflashinfer: Module Not Found\r\nrequests: 2.22.0\r\ntqdm: 4.66.4\r\nnumpy: 1.24.4\r\naiohttp: 3.9.5\r\nfastapi: 0.111.1\r\nhf_transfer: Module Not Found\r\nhuggingface_hub: 0.24.2\r\ninteregular: 0.3.3\r\npackaging: 20.3\r\nPIL: 7.0.0\r\npsutil: 5.5.1\r\npydantic: 2.8.2\r\nuvicorn: 0.30.3\r\nuvloop: 0.19.0\r\nzmq: 26.0.3\r\nvllm: 0.5.3.post1\r\noutlines: Module Not Found\r\nopenai: 1.37.1\r\nanthropic: Module Not Found\r\nlitellm: Module Not Found\r\nNVIDIA Topology:\r\n\tGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tPXB\tPXB\tPXB\tNODE\tNODE\tNODE\tNODE\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU1\tPXB\t X \tPXB\tPXB\tNODE\tNODE\tNODE\tNODE\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU2\tPXB\tPXB\t X \tPIX\tNODE\tNODE\tNODE\tNODE\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU3\tPXB\tPXB\tPIX\t X \tNODE\tNODE\tNODE\tNODE\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU4\tNODE\tNODE\tNODE\tNODE\t X \tPXB\tPXB\tPXB\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU5\tNODE\tNODE\tNODE\tNODE\tPXB\t X \tPXB\tPXB\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU6\tNODE\tNODE\tNODE\tNODE\tPXB\tPXB\t X \tPIX\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU7\tNODE\tNODE\tNODE\tNODE\tPXB\tPXB\tPIX\t X \tSYS\t0-31,64-95\t0\t\tN/A\r\nNIC0\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_bond_0\r\n\r\n\r\nulimit soft: 1024\n```\n",
    "labels": [
      "await-response"
    ],
    "state": "closed",
    "created_at": "2024-08-06T06:46:53+00:00",
    "closed_at": "2024-08-06T09:35:48+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/945/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/945"
  },
  {
    "number": 40,
    "title": "Custom chat template",
    "body": "This would be useful when using a model like mistral-instruct, or any model that doesn't have a standardised template like chatml. Or for example using a finetuned model that uses a custom chat/instruct template.",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-18T15:37:27+00:00",
    "closed_at": "2024-01-18T22:02:09+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/40/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/40"
  },
  {
    "number": 6857,
    "title": "[Bug] PD Disaggregation benchmark hang after Decode out of memory happened",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nsglang version: 0.4.6.post5\nI run 1P1D in a node and run a benchmark, the benchmark hang after run some prompts.\nAfter sometimes debug, I fond some request in prealloced queue but cannot be poped out by pop_preallocated, because these request meet: \"if not decode_req.waiting_for_input: continue\". But why these request decode_req.waiting_for_input is False, I found when the MooncakeKVReceiver is created, the status is KVPoll.Success = 4. This is because the request KV is already transfered but decoding run with error: \u201cDecode out of memory happened\u201d, thus the request is placed into the disagg_decode_prealloc_queue again, but these requests can not be poped out.\n\n\n### Reproduction\n\n\nThe deployment and benchmark code:\n```\nexport CUDA_VISIBLE_DEVICES=0,1\n\npython3 -m sglang.launch_server \\\n--model-path Qwen/Qwen3-32B-FP8 --tp 1 --page-size 16 \\\n    --trust-remote-code \\\n    --mem-fraction-static 0.85 \\\n    --disaggregation-mode prefill \\\n    --disaggregation-bootstrap-port 8990 \\\n    --disaggregation-transfer-backend mooncake \\\n    --disaggregation-ib-device \"mlx5_0,mlx5_1,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7\" \\\n    --disable-radix-cache \\\n    --host 0.0.0.0 \\\n    --port 30000\n\n# decoding\nexport CUDA_VISIBLE_DEVICES=4,5\n\npython3 -m sglang.launch_server \\\n--model-path Qwen/Qwen3-32B-FP8 --tp 1 --page-size 16 \\\n    --trust-remote-code \\\n    --mem-fraction-static 0.85 \\\n    --disaggregation-mode decode \\\n    --disaggregation-transfer-backend mooncake \\\n    --disaggregation-ib-device \"mlx5_0,mlx5_1,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7\" \\\n    --disable-radix-cache \\\n    --host 0.0.0.0 \\\n    --port 30007\n\n\npython3 -m sglang.srt.disaggregation.mini_lb \\\n    --prefill http://localhost:30000 \\\n    --prefill-bootstrap-ports 8990 \\\n    --decode \"http://localhost:30007\"\n\npython3 -m sglang.bench_serving \\\n    --backend sglang \\\n    --dataset-name random \\\n    --dataset-path /root/ShareGPT_V3_unfiltered_cleaned_split.json \\\n    --num-prompts 512 \\\n    --random-input 1024 \\\n    --random-output 1024 \\\n    --random-range-ratio 1.0 \\\n    --request-rate 128 \\\n    --max-concurrency 128 \\\n    --warmup-requests 1 \\\n    --pd-separated \\\n    --base-url http://localhost:8000\n```\n\n\n\n### Environment\n\nsglang version: 0.4.6.post5\n",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-04T03:44:36+00:00",
    "closed_at": "2025-06-04T08:20:13+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6857/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6857"
  },
  {
    "number": 4059,
    "title": "[Feature] When will pipeline model parallelism be supported?",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nHi, I see from the source code that sglang has left an entry for pipeline model parallelism https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/distributed/parallel_state.py#L1049. When will this feature be supported? Pipeline model parallelism should significantly reduce network communication costs, right?\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-04T09:28:23+00:00",
    "closed_at": "2025-03-05T03:39:22+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4059/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 2,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4059"
  },
  {
    "number": 2021,
    "title": "[Bug] Have any suggestions for setting hyperparameters for inference acceleration\uff1f",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nnow i run the model on the A800 80G *8, with qwen2.5-72b-instruct model, my gen throughput (token/s): 75.78, i think is too low, does some suggestions for speed up my interface speed? thanks !\r\n\r\n\n\n### Reproduction\n\nnone\n\n### Environment\n\nnone",
    "labels": [],
    "state": "closed",
    "created_at": "2024-11-13T08:40:09+00:00",
    "closed_at": "2024-11-13T22:35:16+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2021/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2021"
  },
  {
    "number": 1173,
    "title": "[Bug] Runtime Stuck",
    "body": "### Checklist\r\n\r\n- [x] 1. I have searched related issues but cannot get the expected help.\r\n- [x] 2. The bug has not been fixed in the latest version.\r\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [x] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\n```\r\nimport sglang as sgl\r\n\r\n\r\n@sgl.function\r\ndef multi_turn_question(s, question_1, question_2):\r\n    s += sgl.user(question_1)\r\n    s += sgl.assistant(sgl.gen(\"answer_1\", max_tokens=256))\r\n    s += sgl.user(question_2)\r\n    s += sgl.assistant(sgl.gen(\"answer_2\", max_tokens=256))\r\n\r\n\r\ndef single():\r\n    state = multi_turn_question.run(\r\n        question_1=\"What is the capital of the United States?\",\r\n        question_2=\"List two local attractions.\",\r\n    )\r\n\r\n    for m in state.messages():\r\n        print(m[\"role\"], \":\", m[\"content\"])\r\n\r\n    print(\"\\n-- answer_1 --\\n\", state[\"answer_1\"])\r\n\r\n\r\ndef stream():\r\n    state = multi_turn_question.run(\r\n        question_1=\"What is the capital of the United States?\",\r\n        question_2=\"List two local attractions.\",\r\n        stream=True,\r\n    )\r\n\r\n    for out in state.text_iter():\r\n        print(out, end=\"\", flush=True)\r\n    print()\r\n\r\n\r\ndef batch():\r\n    states = multi_turn_question.run_batch(\r\n        [\r\n            {\r\n                \"question_1\": \"What is the capital of the United States?\",\r\n                \"question_2\": \"List two local attractions.\",\r\n            },\r\n            {\r\n                \"question_1\": \"What is the capital of France?\",\r\n                \"question_2\": \"What is the population of this city?\",\r\n            },\r\n        ]\r\n    )\r\n\r\n    for s in states:\r\n        print(s.messages())\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    runtime = sgl.Runtime(model_path=\"xxxxxx/PLM/meta-llama/Meta-Llama-3.1-8B-Instruct\",tp_size=1,dp_size=4,enable_torch_compile=False)\r\n    print(\"\\n========== End ==========\\n\")\r\n\r\n```\r\n\r\n\r\nI can use SGL by deploying in command line.\r\nBut when i try to use in python wrapper:  it stuck in creating Runtime. (Successfully load model and see the GPU memory increase, but not observe print)\r\n\r\n\r\n\r\n\r\n\r\nStuck in create runtime\r\n\r\n### Reproduction\r\n\r\npip install sgl as README and run the code above\r\n\r\n### Environment\r\n\r\nPython: 3.11.2 (main, Mar 13 2023, 12:18:29) [GCC 12.2.0]\r\nCUDA available: True\r\nGPU 0,1,2,3: NVIDIA A800-SXM4-80GB\r\nGPU 0,1,2,3 Compute Capability: 8.0\r\nCUDA_HOME: /usr/local/cuda-12.1\r\nNVCC: Cuda compilation tools, release 12.1, V12.1.66\r\nCUDA Driver Version: 535.161.08\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.2.13\r\nflashinfer: 0.1.5+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.44.1\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.26.4\r\naiohttp: 3.10.2\r\nfastapi: 0.112.0\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.5\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.5\r\nuvloop: 0.19.0\r\nzmq: 26.1.0\r\nvllm: 0.5.4\r\nmultipart: 0.0.9\r\nopenai: 1.41.1\r\nanthropic: 0.34.0\r\nNVIDIA Topology: \r\n        GPU0    GPU1    GPU2    GPU3    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV8     NV8     NV8     PXB     NODE    SYS     SYS     0-31,64-95      0               N/A\r\nGPU1    NV8      X      NV8     NV8     PXB     NODE    SYS     SYS     0-31,64-95      0               N/A\r\nGPU2    NV8     NV8      X      NV8     NODE    PXB     SYS     SYS     0-31,64-95      0               N/A\r\nGPU3    NV8     NV8     NV8      X      NODE    PXB     SYS     SYS     0-31,64-95      0               N/A\r\nNIC0    PXB     PXB     NODE    NODE     X      NODE    SYS     SYS\r\nNIC1    NODE    NODE    PXB     PXB     NODE     X      SYS     SYS\r\nNIC2    SYS     SYS     SYS     SYS     SYS     SYS      X      NODE\r\nNIC3    SYS     SYS     SYS     SYS     SYS     SYS     NODE     X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n\r\n\r\nulimit soft: 1024768",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-21T09:04:49+00:00",
    "closed_at": "2024-08-21T12:09:47+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1173/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1173"
  },
  {
    "number": 3715,
    "title": "[Bug]  Could not find a version that satisfies the requirement sgl-kernel>=0.0.3.post6",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI use dockerfile to build the image, but I get an error:\n```\n359.9 INFO: pip is looking at multiple versions of sglang[srt] to determine which version is compatible with other requirements. This could take a while.\n359.9 ERROR: Could not find a version that satisfies the requirement sgl-kernel>=0.0.3.post6; extra == \"srt\" (from sglang[srt]) (from versions: 0.0.1)\n362.0 ERROR: No matching distribution found for sgl-kernel>=0.0.3.post6; extra == \"srt\"\n```\n\n### Reproduction\n\ndocker build -t sglang .\n\n### Environment\n\ndocker",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-20T02:55:38+00:00",
    "closed_at": "2025-02-20T05:32:16+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3715/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3715"
  },
  {
    "number": 4283,
    "title": "why dp_size must be 1 for update weights from distributed / tensor? Is dp_size==1 also required for update_weights_from_disk?",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-11T05:29:14+00:00",
    "closed_at": "2025-03-12T02:01:26+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4283/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4283"
  },
  {
    "number": 3525,
    "title": "[Bug] ModuleNotFoundError: No module named 'datasets' on latest docker build",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nwhen i usd docker i get this error \n\n```\ninference_engine  | [2025-02-12 12:54:32] INFO:     127.0.0.1:57952 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\ninference_engine  | [2025-02-12 12:54:32 TP0] Scheduler hit an exception: Traceback (most recent call last):\ninference_engine  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1796, in run_scheduler_process\ninference_engine  |     scheduler.event_loop_overlap()\ninference_engine  |   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\ninference_engine  |     return func(*args, **kwargs)\ninference_engine  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 496, in event_loop_overlap\ninference_engine  |     batch = self.get_next_batch_to_run()\ninference_engine  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 877, in get_next_batch_to_run\ninference_engine  |     new_batch = self.get_new_batch_prefill()\ninference_engine  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 898, in get_new_batch_prefill\ninference_engine  |     self.move_ready_grammar_requests()\ninference_engine  |   File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1565, in move_ready_grammar_requests\ninference_engine  |     req.grammar = req.grammar.result(timeout=0.05)\ninference_engine  |   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\ninference_engine  |     return self.__get_result()\ninference_engine  |   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\ninference_engine  |     raise self._exception\ninference_engine  |   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\ninference_engine  |     result = self.fn(*self.args, **self.kwargs)\ninference_engine  |   File \"/sgl-workspace/sglang/python/sglang/srt/constrained/base_grammar_backend.py\", line 53, in init_value\ninference_engine  |     entry.value = self.init_value_impl(key)\ninference_engine  |   File \"/sgl-workspace/sglang/python/sglang/srt/constrained/outlines_backend.py\", line 173, in init_value_impl\ninference_engine  |     guide = RegexGuide.from_regex(regex, self.outlines_tokenizer)\ninference_engine  |   File \"/usr/local/lib/python3.10/dist-packages/outlines/fsm/guide.py\", line 92, in from_regex\ninference_engine  |     return super().from_regex(\ninference_engine  |   File \"/usr/local/lib/python3.10/dist-packages/outlines_core/fsm/guide.py\", line 212, in from_regex\ninference_engine  |     ) = _create_states_mapping(\ninference_engine  |   File \"/usr/local/lib/python3.10/dist-packages/outlines/fsm/guide.py\", line 76, in cached_create_states_mapping\ninference_engine  |     return uncached_create_states_mapping(regex_string, tokenizer, *args, **kwargs)\ninference_engine  |   File \"/usr/local/lib/python3.10/dist-packages/outlines_core/fsm/guide.py\", line 141, in create_states_mapping\ninference_engine  |     return create_states_mapping_from_fsm(regex_fsm, tokenizer, frozen_tokens)\ninference_engine  |   File \"/usr/local/lib/python3.10/dist-packages/outlines_core/fsm/guide.py\", line 178, in create_states_mapping_from_fsm\ninference_engine  |     states_to_token_maps, empty_token_ids = create_fsm_index_tokenizer(\ninference_engine  |   File \"/usr/local/lib/python3.10/dist-packages/outlines_core/fsm/regex.py\", line 473, in create_fsm_index_tokenizer\ninference_engine  |     tokens_to_token_ids, empty_token_ids = reduced_vocabulary(tokenizer)\ninference_engine  |   File \"/usr/local/lib/python3.10/dist-packages/outlines/models/transformers.py\", line 117, in __hash__\ninference_engine  |     from datasets.fingerprint import Hasher\ninference_engine  | ModuleNotFoundError: No module named 'datasets'\ninference_engine  | \n```\n\n\n### Reproduction\n\nusing docker\n\n### Environment\n\nh100",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-12T13:09:13+00:00",
    "closed_at": "2025-02-12T19:26:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3525/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3525"
  },
  {
    "number": 4123,
    "title": "[Bug] Inaccurate or Inconsistent Output in Qwen2.5-VL Multi-Image Testing with sglang",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen testing multi-image input with `sglang` using the Qwen2.5-VL model, the output descriptions of the image contents are inaccurate or inconsistent. Compared to the output from directly calling the same model with `transformers`, the results from `sglang` show significant differences in describing the commonalities between multiple images.\n\n### Reproduction\n\nargs:\n```\n--chat-template qwen2-vl --tp-size 2 --mem-fraction-static 0.75 --disable-radix-cache --chunked-prefill-size -1\n```\nrequest:\n```\n{\n    \"model\": \"default\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"\u63cf\u8ff0\u4e00\u4e0b\u4e24\u5f20\u56fe\u7247\u76f8\u540c\u70b9\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"/home/panlyu/sglang/kitchen.jpg\"\n                    }\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"/home/panlyu/sglang/street.jpg\"\n                    }\n                }\n            ]\n        }\n    ],\n    \"stream\": true\n}\n```\n\noutput:\n1: \n```\n\"\u8fd9\u4e24\u5f20\u56fe\u7247\u7684\u4e3b\u8981\u76f8\u540c\u70b9\u5982\u4e0b\uff1a\\n\\n1. **\u65f6\u95f4\u6233**\uff1a\u4e24\u5f20\u56fe\u7247\u7684\u65f6\u95f4\u6233\u663e\u793a\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u90fd\u662f2022\u5e7410\u670810\u65e5\u65e9\u4e0a10\u70b923\u5206\u5de6\u53f3\u3002\\n2. **\u5185\u5bb9\u63cf\u8ff0**\uff1a\u4e24\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e24\u4e2a\u4eba\u5728\u4e00\u4e2a\u53a8\u623f\u73af\u5883\u4e2d\uff0c\u5176\u4e2d\u4e00\u4e2a\u4eba\u7a7f\u7740\u53a8\u5e08\u670d\u548c\u56f4\u88d9\uff0c\u53e6\u4e00\u4f4d\u7a7f\u7740\u9ed1\u8272\u5916\u5957\u3002\u4e24\u4eba\u90fd\u7ad9\u5728\u4e00\u53f0\u5927\u578b\u53a8\u623f\u70f9\u996a\u8bbe\u5907\u65c1\u8fb9\uff0c\u770b\u8d77\u6765\u50cf\u662f\u4e00\u4e2a\u70f9\u996a\u793a\u8303\u6216\u6559\u5b66\u573a\u666f\u3002\\n3. **\u573a\u666f**\uff1a\u4e24\u5f20\u56fe\u7247\u7684\u573a\u666f\u975e\u5e38\u76f8\u4f3c\uff0c\u80cc\u666f\u548c\u8bbe\u5907\u5e03\u5c40\u51e0\u4e4e\u4e00\u81f4\uff0c\u8868\u660e\u8fd9\u4e24\u5f20\u56fe\u7247\u662f\u540c\u4e00\u4e8b\u4ef6\u6216\u6d3b\u52a8\u7684\u4e00\u90e8\u5206\u3002\\n\\n\u4ee5\u4e0b\u662f\u4e24\u5f20\u56fe\u7247\u7684\u5177\u4f53\u63cf\u8ff0\uff1a\\n\\n**\u7b2c\u4e00\u5f20\u56fe\u7247\u63cf\u8ff0**\uff1a\\n- \u65f6\u95f4\u6233\u663e\u793a2022\u5e7410\u670810\u65e510:23:46\u3002\\n- \u573a\u666f\u662f\u5728\u4e00\u4e2a\u53a8\u623f\u73af\u5883\u4e2d\uff0c\u4e00\u4e2a\u4eba\u8eab\u7a7f\u53a8\u5e08\u670d\u548c\u56f4\u88d9\uff0c\u7ad9\u5728\u4e00\u53f0\u5927\u578b\u70f9\u996a\u8bbe\u5907\u65c1\u8fb9\uff0c\u624b\u6301\u4e00\u4e2a\u624b\u673a\u53ef\u80fd\u5728\u64cd\u4f5c\u6216\u67e5\u770b\u67d0\u4e9b\u4fe1\u606f\u3002\\n- \u540e\u65b9\u6709\u4e00\u5f20\u684c\u5b50\u548c\u4e00\u4e9b\u53a8\u623f\u7528\u54c1\u3002\\n\\n**\u7b2c\u4e8c\u5f20\u56fe\u7247\u63cf\u8ff0**\uff1a\\n- \u65f6\u95f4\u6233\u663e\u793a2022\u5e7410\u670810\u65e510:19:04\u3002\\n- \u573a\u666f\u4e5f\u662f\u4e00\u6837\u7684\uff0c\u4e24\u4e2a\u4eba\u7ad9\u5728\u540c\u4e00\u4e2a\u53a8\u623f\u73af\u5883\u4e2d\uff0c\u80cc\u666f\u8bbe\u5907\u548c\u5e03\u5c40\u548c\u7b2c\u4e00\u5f20\u56fe\u7247\u76f8\u540c\u3002\\n- \u4e00\u4e2a\u4eba\u8eab\u7a7f\u53a8\u5e08\u670d\u548c\u56f4\u88d9\uff0c\u53e6\u4e00\u4e2a\u7a7f\u7740\u9ed1\u8272\u5916\u5957\uff0c\u90fd\u5728\u76f8\u540c\u7684\u5927\u578b\u70f9\u996a\u8bbe\u5907\u65c1\u8fb9\u3002\\n\\n\u4e24\u5f20\u56fe\u7247\u63cf\u7ed8\u4e86\u540c\u4e00\u4e2a\u53a8\u623f\u573a\u666f\uff0c\u663e\u793a\u4e24\u4e2a\u4eba\u5728\u8fdb\u884c\u67d0\u79cd\u70f9\u996a\u6d3b\u52a8\u6216\u6559\u5b66\uff0c\u53ef\u80fd\u662f\u53a8\u5e08\u6f14\u793a\u6216\u6559\u5b66\uff0c\u4e5f\u6709\u53ef\u80fd\u5728\u62cd\u6444\u6559\u5b66\u89c6\u9891\u6216\u76f4\u64ad\u3002\"\n```\n2: \n```\n\"\u8fd9\u4e24\u5f20\u56fe\u7247\u7684\u76f8\u540c\u70b9\u5982\u4e0b\uff1a\\n\\n1. **\u65f6\u95f4\u6233\u4e00\u81f4**\uff1a\u4e24\u5f20\u56fe\u7247\u7684\u65f6\u95f4\u6233\u90fd\u662f**2022-06-29 20:19:04**\uff0c\u663e\u793a\u8fd9\u4e24\u5f20\u56fe\u7247\u662f\u5728\u540c\u4e00\u65f6\u95f4\u62cd\u6444\u7684\u3002\\n\\n2. **\u573a\u666f\u76f8\u4f3c**\uff1a\u4e24\u5f20\u56fe\u7247\u7684\u573a\u666f\u770b\u8d77\u6765\u76f8\u4f3c\uff0c\u80cc\u666f\u548c\u73af\u5883\u90fd\u975e\u5e38\u76f8\u4f3c\uff0c\u90fd\u663e\u793a\u4e86\u4e00\u4e2a\u5546\u4e1a\u8857\u6216\u8857\u533a\u7684\u60c5\u51b5\uff0c\u6709\u8def\u706f\u3001\u5efa\u7b51\u7269\u548c\u4e00\u4e9b\u884c\u4eba\u3002\\n\\n3. **\u884c\u4eba\u53ca\u8f66\u8f86\u4fe1\u606f\u76f8\u540c**\uff1a\u4e24\u5f20\u56fe\u7247\u4e2d\u7684\u884c\u4eba\u548c\u8f66\u8f86\u7684\u4f4d\u7f6e\u3001\u59ff\u6001\u90fd\u975e\u5e38\u76f8\u4f3c\uff0c\u5c24\u5176\u662f\u56fe\u7247\u4e2d\u7684\u884c\u4eba\u7a7f\u7740\u84dd\u8272\u548c\u9ed1\u8272\u8863\u670d\uff0c\u4ed6\u4eec\u5728\u6b65\u884c\u6216\u7ad9\u7acb\u7684\u4f4d\u7f6e\u51e0\u4e4e\u6ca1\u6709\u53d8\u5316\u3002\\n\\n4. **\u76f8\u673a\u4f4d\u7f6e\u76f8\u4f3c**\uff1a\u4e24\u5f20\u56fe\u7247\u7684\u62cd\u6444\u89d2\u5ea6\u548c\u76f8\u673a\u4f4d\u7f6e\u975e\u5e38\u63a5\u8fd1\uff0c\u8868\u660e\u53ef\u80fd\u662f\u540c\u4e00\u4e2a\u6444\u50cf\u5934\u7684\u4e0d\u540c\u5e27\u56fe\u50cf\u3002\\n\\n5. **\u753b\u9762\u5185\u5bb9\u76f8\u540c**\uff1a\u4e24\u5f20\u56fe\u7247\u7684\u524d\u666f\u548c\u80cc\u666f\u5185\u5bb9\u51e0\u4e4e\u4e00\u81f4\uff0c\u5305\u62ec\u5efa\u7b51\u7269\u3001\u5e97\u94fa\u62db\u724c\u3001\u8def\u706f\u3001\u884c\u4eba\u548c\u8f66\u8f86\u7b49\u5143\u7d20\u3002\\n\\n\u901a\u8fc7\u4ee5\u4e0a\u5bf9\u6bd4\u53ef\u4ee5\u770b\u51fa\uff0c\u8fd9\u4e24\u5f20\u56fe\u7247\u5f88\u53ef\u80fd\u6765\u6e90\u4e8e\u540c\u4e00\u4e2a\u89c6\u9891\u7247\u6bb5\u7684\u4e0d\u540c\u5e27\u56fe\u50cf\u3002\"\n```\n\nUsing transformers:\n```\nfrom transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLProcessor\nfrom qwen_vl_utils import process_vision_info\n\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    \"/home/panlyu/lmodels/Qwen2.5-VL-7B-Instruct\", device_map=\"cuda:0\"\n)\n\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = Qwen2_5_VLProcessor.from_pretrained(\n    \"/home/panlyu/lmodels/Qwen2.5-VL-7B-Instruct\",\n    min_pixels=min_pixels,\n    max_pixels=max_pixels,\n)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"/home/panlyu/lmodels/Qwen2.5-VL-7B-Instruct/street.jpg\",\n            },\n            {\n                \"type\": \"image\",\n                \"image\": \"/home/panlyu/lmodels/Qwen2.5-VL-7B-Instruct/kitchen.jpg\",\n            },\n            {\"type\": \"text\", \"text\": \"\u63cf\u8ff0\u4e00\u4e0b\u4e24\u5f20\u56fe\u7247\u76f8\u540c\u70b9\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(model.device)\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=512)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\noutput:\n```\n['\u8fd9\u4e24\u5f20\u56fe\u7247\u7684\u76f8\u540c\u70b9\u5305\u62ec\uff1a\\n\\n1. **\u65f6\u95f4\u6233**\uff1a\u4e24\u5f20\u56fe\u7247\u90fd\u663e\u793a\u4e86\u65e5\u671f\u548c\u65f6\u95f4\uff0c\u7b2c\u4e00\u5f20\u56fe\u7247\u7684\u65f6\u95f4\u662f2022\u5e746\u670829\u65e520:19:04\uff0c\u7b2c\u4e8c\u5f20\u56fe\u7247\u7684\u65f6\u95f4\u662f2019\u5e7411\u670827\u65e514:16:46\u3002\\n   \\n2. **\u4eba\u7269\u5b58\u5728**\uff1a\u4e24\u5f20\u56fe\u7247\u4e2d\u90fd\u6709\u4eba\u5728\u573a\u3002\u7b2c\u4e00\u5f20\u56fe\u7247\u4e2d\u6709\u4e24\u4e2a\u4eba\u5728\u4eba\u884c\u9053\u4e0a\u884c\u8d70\uff0c\u7b2c\u4e8c\u5f20\u56fe\u7247\u4e2d\u6709\u4e09\u4e2a\u4eba\u7ad9\u5728\u53a8\u623f\u533a\u57df\u3002\\n\\n3. **\u573a\u666f\u73af\u5883**\uff1a\u4e24\u5f20\u56fe\u7247\u90fd\u5c55\u793a\u4e86\u516c\u5171\u573a\u6240\u6216\u5de5\u4f5c\u573a\u6240\u7684\u573a\u666f\u3002\u7b2c\u4e00\u5f20\u56fe\u7247\u662f\u4e00\u4e2a\u57ce\u5e02\u8857\u9053\u7684\u4ea4\u53c9\u53e3\uff0c\u7b2c\u4e8c\u5f20\u56fe\u7247\u5219\u662f\u5728\u4e00\u4e2a\u53a8\u623f\u73af\u5883\u4e2d\u3002\\n\\n4. **\u6587\u5b57\u4fe1\u606f**\uff1a\u4e24\u5f20\u56fe\u7247\u4e2d\u90fd\u6709\u4e2d\u6587\u6587\u5b57\uff0c\u7b2c\u4e00\u5f20\u56fe\u7247\u53f3\u4e0b\u89d2\u6709\u201c\u514b\u62c9\u739b\u4f9d Y\u535a\u8fbe\u5e02\u573a\u5165\u53e31\u201d\u7684\u5b57\u6837\uff0c\u7b2c\u4e8c\u5f20\u56fe\u7247\u5de6\u4e0a\u89d2\u6709\u65e5\u671f\u548c\u65f6\u95f4\u4fe1\u606f\u3002\\n\\n5. **\u80cc\u666f\u5143\u7d20**\uff1a\u4e24\u5f20\u56fe\u7247\u7684\u80cc\u666f\u90fd\u5305\u542b\u4e00\u4e9b\u5546\u4e1a\u5143\u7d20\uff0c\u6bd4\u5982\u7b2c\u4e00\u5f20\u56fe\u7247\u4e2d\u7684\u5546\u5e97\u62db\u724c\u548c\u7b2c\u4e8c\u5f20\u56fe\u7247\u4e2d\u7684\u53a8\u623f\u8bbe\u5907\u3002\\n\\n\u8fd9\u4e9b\u5171\u540c\u70b9\u8868\u660e\u4e24\u5f20\u56fe\u7247\u53ef\u80fd\u6765\u81ea\u4e0d\u540c\u7684\u76d1\u63a7\u6444\u50cf\u5934\uff0c\u4f46\u90fd\u8bb0\u5f55\u4e86\u516c\u5171\u7a7a\u95f4\u5185\u7684\u6d3b\u52a8\u573a\u666f\u3002']\n```\n\n![Image](https://github.com/user-attachments/assets/796b63c3-ceba-4aca-831b-53f5429ac078)\n![Image](https://github.com/user-attachments/assets/05bf5ff3-5d54-42d4-8870-cc8e31157931)\n\n\n### Environment\n\nPython: 3.12.9 (main, Feb 12 2025, 14:50:50) [Clang 19.1.6 ]\nCUDA available: True\nGPU 0,1,2,3: NVIDIA RTX 6000 Ada Generation\nGPU 0,1,2,3 Compute Capability: 8.9\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 11.8, V11.8.89\nCUDA Driver Version: 525.105.17\nPyTorch: 2.5.1+cu124\nsglang: 0.4.3.post2\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.2.post1+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.62.0\ntiktoken: 0.8.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity\nGPU0     X      NODE    SYS     SYS     0-27,56-83      0\nGPU1    NODE     X      SYS     SYS     0-27,56-83      0\nGPU2    SYS     SYS      X      NODE    28-55,84-111    1\nGPU3    SYS     SYS     NODE     X      28-55,84-111    1\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-06T03:59:30+00:00",
    "closed_at": "2025-03-06T16:06:04+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4123/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4123"
  },
  {
    "number": 138,
    "title": "Error on json decoding with llava",
    "body": "Encountered the following error when using the `regex` in call to `gen` method. Server doesn't work afterwards:\r\n\r\n```console\r\n~$ python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.5-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --port 30000\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nRank 0: load weight begin.\r\n/opt/conda/envs/llava_test/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nINFO 02-03 23:02:55 weight_utils.py:164] Using model weights format ['*.bin']\r\nINFO 02-03 23:03:01 weight_utils.py:164] Using model weights format ['*.bin']\r\nRank 0: load weight end.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nRank 0: max_total_num_token=44713, max_prefill_num_token=7452, context_len=4096, model_mode=[]\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nINFO:     Started server process [66164]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)\r\nINFO:     127.0.0.1:51098 - \"GET /get_model_info HTTP/1.1\" 200 OK\r\nnew fill batch. #seq: 1. #cached_token: 0. #new_token: 646. #remaining_req: 0. #running_req: 0. tree_cache_hit_rate: 0.00%.\r\n#running-req: 1, #token: 646, token usage: 0.01, #queue-req: 0\r\nINFO:     127.0.0.1:58036 - \"POST /generate HTTP/1.1\" 200 OK\r\nnew fill batch. #seq: 1. #cached_token: 645. #new_token: 1. #remaining_req: 0. #running_req: 0. tree_cache_hit_rate: 49.92%.\r\nnew fill batch. #seq: 1. #cached_token: 35. #new_token: 39. #remaining_req: 0. #running_req: 0. tree_cache_hit_rate: 49.78%.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n\r\n...\r\n\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [29,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [29,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [29,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [29,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [29,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 168, in exposed_step\r\n    self.forward_step()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 183, in forward_step\r\n    self.forward_fill_batch(new_batch)\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 391, in forward_fill_batch\r\n    logits, (logprobs, normalized_logprobs) = self.model_runner.forward(\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_runner.py\", line 460, in forward\r\n    return self.forward_extend_multi_modal(**kwargs)\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_runner.py\", line 437, in forward_extend_multi_modal\r\n    return self.model.forward(\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/models/llava.py\", line 107, in forward\r\n    .cpu()\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 168, in exposed_step\r\n    self.forward_step()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 179, in forward_step\r\n    new_batch = self.get_new_fill_batch()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 293, in get_new_fill_batch\r\n    self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/memory_pool.py\", line 92, in available_size\r\n    return torch.sum(self.mem_state == 0).item()\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 168, in exposed_step\r\n    self.forward_step()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 179, in forward_step\r\n    new_batch = self.get_new_fill_batch()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 293, in get_new_fill_batch\r\n    self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/memory_pool.py\", line 92, in available_size\r\n    return torch.sum(self.mem_state == 0).item()\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 168, in exposed_step\r\n    self.forward_step()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 179, in forward_step\r\n    new_batch = self.get_new_fill_batch()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 293, in get_new_fill_batch\r\n    self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/memory_pool.py\", line 92, in available_size\r\n    return torch.sum(self.mem_state == 0).item()\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n...\r\n```\r\n\r\n## How to reproduce the error\r\n\r\n* Run the server with `python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.5-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --port 30000`\r\n* Run the following code:\r\n```python\r\nimport sglang as sgl\r\nfrom enum import Enum\r\nfrom pydantic import BaseModel\r\nfrom sglang.srt.constrained.json_schema import build_regex_from_object\r\nfrom sglang import function, system, user, assistant, gen, set_default_backend, RuntimeEndpoint\r\nset_default_backend(RuntimeEndpoint(\"http://localhost:30000\"))\r\n\r\n@sgl.function\r\ndef image_qa(s, image_path, question, regex=None):\r\n    s += sgl.user(sgl.image(image_path) + question)\r\n    s += sgl.assistant(sgl.gen(\"answer\", regex=regex))\r\n\r\nclass Color(str, Enum):\r\n    black = \"Black\"\r\n    orange = \"Orange\"\r\n    white = \"White\"\r\n\r\nclass Attributes(BaseModel):\r\n    color: Color\r\n\r\nstate = image_qa.run(\r\n    image_path=\"cat.jpg\",\r\n    question=\"What color is the cat? respond only in JSON format. The key should be color and the only values allowed are Black, Orange, White\",\r\n    regex=build_regex_from_object(Attributes),\r\n    max_new_tokens=200,\r\n    temperature=0\r\n)\r\nprint(state['answer'])\r\n```\r\n\r\n## Additional notes:\r\n* Tested on A100 40GB\r\n* sglang 0.1.11\r\n* torch 2.1.2\r\n* CUDA:\r\n  ```console\r\n  nvcc: NVIDIA (R) Cuda compiler driver\r\n  Copyright (c) 2005-2023 NVIDIA Corporation\r\n  Built on Mon_Apr__3_17:16:06_PDT_2023\r\n  Cuda compilation tools, release 12.1, V12.1.105\r\n  Build cuda_12.1.r12.1/compiler.32688072_0\r\n  ```\r\n* If I remove the `regex` parameter, the output is what I expect it to be (e.g. `' {\\n\"color\": \"Orange\",\\n}'`)\r\n* `json_decode.py` file with llama2 (`python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000`)  ran successfully so it might be a multimodal issue. As far as I know [Outlines](https://github.com/outlines-dev/outlines) doesn't support multimodals yet.\r\n  ```console\r\n  ~$ python json_decode.py\r\n  Hermione Granger is a character in Harry Potter. Please fill in the following information about this character.\r\n  {\r\n      \"name\": \"Hermione Granger\",\r\n      \"house\": \"Gryffindor\",\r\n      \"blood status\": \"Pure-blood\",\r\n      \"occupation\": \"student\",\r\n      \"wand\": {\r\n          \"wood\": \"cherry\",\r\n          \"core\": \"hornbeam\",\r\n          \"length\": 16.75\r\n      },\r\n      \"alive\": \"Alive\",\r\n      \"patronus\": \"toad\",\r\n      \"bogart\": \"Owl\"\r\n  }\r\n  ```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-03T23:12:32+00:00",
    "closed_at": "2024-02-04T02:46:17+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/138/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/138"
  },
  {
    "number": 1076,
    "title": "[Bug] nsys profile failed",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n\n### Describe the bug\n\nnsys profile failed with nccl error.\r\n![image](https://github.com/user-attachments/assets/daca6ae7-8efb-4b89-b675-683d0d48b9e0)\r\n\n\n### Reproduction\n\n```shell\r\npython -m sglang.launch_server --port 8000 --model-path Qwen2-72B-FP8-Instruct --tp-size 8 --disable-radix-cache --mem-fraction-static 0.8 --disable-cuda-graph\r\n```\n\n### Environment\n\n```shell\r\nPython: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA GeForce RTX 4090\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\r\nCUDA Driver Version: 535.154.05\r\n535.154.05\r\n535.154.05\r\n535.154.05\r\n535.154.05\r\n535.154.05\r\n535.154.05\r\n535.154.05\r\nPyTorch: 2.3.1+cu121\r\nsglang: 0.2.9.post1\r\nflashinfer: 0.1.3+cu121torch2.3\r\nrequests: 2.31.0\r\ntqdm: 4.66.2\r\nnumpy: 1.26.4\r\naiohttp: 3.9.3\r\nfastapi: 0.110.1\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.23.4\r\ninteregular: 0.3.3\r\npackaging: 24.0\r\nPIL: 10.4.0\r\npsutil: 5.9.8\r\npydantic: 2.8.2\r\nuvicorn: 0.29.0\r\nuvloop: 0.19.0\r\nzmq: 25.1.2\r\nvllm: 0.5.3.post1\r\nmultipart: 0.0.9\r\nopenai: 1.40.6\r\nanthropic: 0.33.1\r\nNVIDIA Topology: \r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PIX     SYS     SYS     SYS     SYS     SYS     SYS     0-31,64-95      0               N/A\r\nGPU1    PIX      X      SYS     SYS     SYS     SYS     SYS     SYS     0-31,64-95      0               N/A\r\nGPU2    SYS     SYS      X      PIX     SYS     SYS     SYS     SYS     0-31,64-95      0               N/A\r\nGPU3    SYS     SYS     PIX      X      SYS     SYS     SYS     SYS     0-31,64-95      0               N/A\r\nGPU4    SYS     SYS     SYS     SYS      X      PIX     SYS     SYS     32-63,96-127    1               N/A\r\nGPU5    SYS     SYS     SYS     SYS     PIX      X      SYS     SYS     32-63,96-127    1               N/A\r\nGPU6    SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     32-63,96-127    1               N/A\r\nGPU7    SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      32-63,96-127    1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nulimit soft: 655350\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-13T10:54:45+00:00",
    "closed_at": "2024-08-14T08:42:00+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1076/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1076"
  },
  {
    "number": 7693,
    "title": "[Bug] Qwen2.5-VL-7B OOM",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nserver command:\n```\npython -m sglang.launch_server --model-path Qwen/Qwen2.5-VL-7B-Instruct --host 0.0.0.0 --port 8080  --chat-template qwen2-vl --chunked-prefill-size -1 --disable-radix-cache --mm-attention-backend fa3 --attention-backend fa3  --enable-torch-compile --cuda-graph-bs 80 --torch-compile-max-bs 80\n```\nbench command:\n```\ngenai-bench benchmark \\\n            --api-backend openai \\\n            --api-key \"your-openai-api-key\" \\\n            --api-base \"http://0.0.0.0:8080\" \\\n            --api-model-name \"Qwen2.5-VL-7B-Instruct\" \\\n            --model-tokenizer \"Qwen/Qwen2.5-VL-7B-Instruct\" \\\n            --task image-text-to-text \\\n            --max-time-per-run 60 \\\n            --max-requests-per-run 400 \\\n            --server-engine SGLang \\\n            --server-gpu-count 1 \\\n            --traffic-scenario \"I(1920,1920)\" \\\n            --num-concurrency 80 \\\n            --dataset-config ./qwen_2.5_vl_local_ds_config.json\n```\n\nserver memory monitor:\n\n<img width=\"266\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5fef3138-f9f3-4567-ad82-df881fc07473\" />\n\nafter somtime, server will got OOM\n\n### Reproduction\n\nnone\n\n### Environment\n\n```\nPython: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.5, V12.5.82\nCUDA Driver Version: 535.161.07\nPyTorch: 2.7.1+cu126\nsglang: 0.4.8\nsgl_kernel: 0.1.9\nflashinfer_python: 0.2.6.post1\ntriton: 3.3.1\ntransformers: 4.52.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.12.13\nfastapi: 0.115.13\nhf_transfer: 0.1.9\nhuggingface_hub: 0.33.0\ninteregular: 0.3.3\nmodelscope: 1.27.0\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 23.1\npsutil: 7.0.0\npydantic: 2.11.7\npython-multipart: 0.0.20\npyzmq: 26.3.0\nuvicorn: 0.34.3\nuvloop: 0.21.0\nvllm: 0.9.1\nxgrammar: 0.1.19\nopenai: 1.88.0\ntiktoken: 0.9.0\nanthropic: 0.54.0\nlitellm: 1.72.6.post1\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     PHB     NODE    SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    PHB     PIX     NODE    SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    96-191,288-383  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    96-191,288-383  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE    NODE    PIX     PHB     96-191,288-383  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     NODE    NODE    PHB     PIX     96-191,288-383  1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC1    NODE    PIX     PHB     NODE    SYS     SYS     SYS     SYS     NODE     X      PHB     NODE    SYS     SYS     SYS     SYS\nNIC2    NODE    PHB     PIX     NODE    SYS     SYS     SYS     SYS     NODE    PHB      X      NODE    SYS     SYS     SYS     SYS\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PIX     PHB     SYS     SYS     SYS     SYS     NODE    NODE     X      PHB\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    PHB     PIX     SYS     SYS     SYS     SYS     NODE    NODE    PHB      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_bond_0\n  NIC1: mlx5_bond_1\n  NIC2: mlx5_bond_2\n  NIC3: mlx5_bond_3\n  NIC4: mlx5_bond_4\n  NIC5: mlx5_bond_5\n  NIC6: mlx5_bond_6\n  NIC7: mlx5_bond_7\n\n\nulimit soft: 1048576\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-07-01T10:53:32+00:00",
    "closed_at": "2025-07-01T12:13:03+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7693/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7693"
  },
  {
    "number": 939,
    "title": "[Request] Support for qwen1.5",
    "body": "currently qwen1.5 is not supported. Thank you!",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-05T15:40:52+00:00",
    "closed_at": "2024-08-05T18:00:58+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/939/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/939"
  },
  {
    "number": 5593,
    "title": "GPU bubble between decode tokens when using cuda graph",
    "body": "### Description\nWhen using CUDA Graph during decoding phase, we observed a GPU bubble (idle period) of around 2ms before the first kernel in the graph starts execution each time the graph is replayed.\n\n![Image](https://github.com/user-attachments/assets/0325918f-e9ee-4c16-9525-fb39959038c1)\n\n### Reproduction\n- Model: Qwen2.5-32B-Instruct\n- Params:\n  - tp: 4\n  - input_len: 4400\n  - batch_size: 64\n  - quantization: fp8\n\nadd nvtx range:\nhttps://github.com/sgl-project/sglang/blob/main/python/sglang/srt/model_executor/cuda_graph_runner.py\n```python\n...\ndef replay(\n        self, forward_batch: ForwardBatch, skip_attn_backend_init: bool = False\n    ) -> LogitsProcessorOutput:\n        if not skip_attn_backend_init:\n            with nvtx.annotate(\"replay_prepare\", color=\"green\"):\n                self.replay_prepare(forward_batch)\n        else:\n            # In speculative decoding, these two fields are still needed.\n            self.input_ids[: self.raw_num_token].copy_(forward_batch.input_ids)\n            self.positions[: self.raw_num_token].copy_(forward_batch.positions)\n\n        # Replay\n        with nvtx.annotate(\"replay\", color=\"blue\"):\n            self.graphs[self.bs].replay()\n        ...\n```\n\ntest scripts:\ninfer_random_offline.py\n```python\n# launch the offline engine\nimport sglang as sgl\nfrom transformers import AutoTokenizer\nimport numpy as np\nimport random\nimport os\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4\"\n\ndef main():\n    model_path = \"MY_QWQ_32B_FP8_MODEL\"\n    \n    llm = sgl.Engine(model_path=model_path,\n                    trust_remote_code=True,\n                    chunked_prefill_size=-1,\n                    tp_size=4,\n                    attention_backend=\"flashinfer\",\n                    max_running_requests=64,\n                    )\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n    input_len = 4400\n    output_len = 32\n    batch_size = 64\n\n    sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95, \"max_new_tokens\": output_len, \"ignore_eos\": True}\n    prompts = []\n    for iter in range(batch_size):\n        token_ids = np.array([random.randint(100, 142064) for _ in range(input_len)], dtype=np.uint32)\n        text = tokenizer.decode(token_ids)\n        token_ids = tokenizer.encode(text)\n        while len(token_ids) != input_len:\n            print(f'token_ids length is {len(token_ids)}, input_len is {input_len}', flush=True)\n            token_ids = token_ids[:input_len]\n            text = tokenizer.decode(token_ids)\n            token_ids = tokenizer.encode(text)\n        prompts.append(text)\n        \n    output = llm.generate(prompts, sampling_params)\n    print('==========generated text==========', flush=True)\n    print(output, flush=True)\n\n    print('==========done==========', flush=True)\n    while True:\n        pass\nif __name__ == \"__main__\":\n    main()\n    \n```\n\nto generate .nsys-rep file:\n```bash\nnsys profile -t cuda,nvtx --trace-fork-before-exec=true --cuda-graph-trace=node --force-overwrite true -o /workspace/sglang --delay 10 \\\n    python infer_random_offline.py\n```\n```bash\nnsys sessions list\nnsys stop session=SESSION_ID\n```\n\n### Questions\n1. Is this idle period an inherent behavior of CUDA Graph replay?\n2. What causes this synchronization/bubble?\n3. Are there any ways to reduce or eliminate this overhead?\n\n### Environment\n- PyTorch version: 2.5.1\n- CUDA version: 12.4\n- GPU: NVIDIA H20\n",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-21T07:27:56+00:00",
    "closed_at": "2025-04-22T06:07:11+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5593/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5593"
  },
  {
    "number": 3623,
    "title": "[Feature] don't quit server if the request doesn't process success",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\ncan sglang don't quit if a request doesn't process sucess.\n\nI'm trying to process some requests one by one in a loop, but when  I hit control+z\uff0cthe server quit with log.\n\n2025-02-17 11:56:43] Initialization failed. warmup error: Traceback (most recent call last):\n  File \"xxx/python3.11/site-packages/sglang/srt/entrypoints/http_server.py\", line 548, in _wait_and_warmup\n    assert res.status_code == 200, f\"{res=}, {res.text=}\"\n           ^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: res=<Response [403]>, res.text='<html>\\n<head><title>403 Forbidden</title></head>\\n<body>\\n<div style=\"text-align: center;\"><h1>403 Forbidden</h1></div>\\n</body>\\n</html>'\n\nKilled\n\n.\n\ncan you just report an error but don't quit the server\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-17T05:58:27+00:00",
    "closed_at": "2025-02-17T08:45:33+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3623/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3623"
  },
  {
    "number": 4122,
    "title": "permission issue in newly updated docker lmsysorg/sglang:v0.4.3.post2-rocm630",
    "body": "**Description:**\n\nI have been using the Docker image `lmsysorg/sglang:v0.4.3.post2-rocm630` for DeepSeek-R1 inference on a single AMD MI300X node (8 GPUs), and it has been stable until recently. After the Docker image was updated yesterday, my inference jobs started encountering permission-related errors, specifically failing during the initialization of `sgl.Engine`.\n\nRelevant traceback:\n\n```\n  File \"/scratch/amlt_code/sgl_inference.py\", line 132, in main\n    model = sgl.Engine(\n            ^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/api.py\", line 43, in Engine\n    from sglang.srt.entrypoints.engine import Engine\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/engine.py\", line 36, in <module>\n    from sglang.srt.managers.data_parallel_controller import (\n```\n\n**Full Error Log:**  \n[Detailed Log on GitHub](https://github.com/XingxingZhang/debug_only/blob/main/error_log_slg.txt)\n\n**Potential cause:**  \nThis issue appears to have arisen after the recent Docker update associated with this GitHub Action: [sgl-project/sglang/actions/runs/13663045650](https://github.com/sgl-project/sglang/actions/runs/13663045650).\n\n**Request:**  \nCould you please investigate and resolve this issue? \n\nAdditionally, would it be possible to maintain a stable historical version of the Docker image with a clearly distinguishable label (e.g., `lmsysorg/sglang:v0.4.3.post2-rocm630-2025_mm_dd`) to facilitate rollback when similar situations occur?\n\nThank you very much!",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-06T03:35:35+00:00",
    "closed_at": "2025-03-06T16:06:31+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4122/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4122"
  },
  {
    "number": 3816,
    "title": "How to contribute an optimized R1 operator in SGlang?",
    "body": "How to contribute an optimized R1 operator in SGlang?\n\nFor example, if I have an optimized R1-TopK, which subcomponents (e.g. vLLM/..) are the right place to get started?",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-24T11:19:43+00:00",
    "closed_at": "2025-02-24T18:20:27+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3816/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3816"
  },
  {
    "number": 4629,
    "title": "[Bug] ValueError: '<class 'sglang.srt.configs.qwen2_5_vl_config.Qwen2_5_VLConfig'>' is already used by a Transformers model.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI use the latest docker image. \n\n```\nSingularity> pip list|grep sglang\nsglang                            0.4.4.post1          /sgl-workspace/sglang/python\n```\n\n```\nSingularity> python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-V3 --tp 32 --dist-init-addr 10.168.16.121:5000 --nnodes 4 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 30000\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/sgl-workspace/sglang/python/sglang/launch_server.py\", line 6, in <module>\n    from sglang.srt.entrypoints.http_server import launch_server\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/http_server.py\", line 44, in <module>\n    from sglang.srt.entrypoints.engine import _launch_subprocesses\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/engine.py\", line 36, in <module>\n    from sglang.srt.managers.data_parallel_controller import (\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/data_parallel_controller.py\", line 27, in <module>\n    from sglang.srt.managers.io_struct import (\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/io_struct.py\", line 25, in <module>\n    from sglang.srt.managers.schedule_batch import BaseFinishReason\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/schedule_batch.py\", line 43, in <module>\n    from sglang.srt.configs.model_config import ModelConfig\n  File \"/sgl-workspace/sglang/python/sglang/srt/configs/__init__.py\", line 5, in <module>\n    from sglang.srt.configs.qwen2_5_vl_config import (\n  File \"/sgl-workspace/sglang/python/sglang/srt/configs/qwen2_5_vl_config.py\", line 1005, in <module>\n    AutoImageProcessor.register(Qwen2_5_VLConfig, None, Qwen2_5_VLImageProcessor, None)\n  File \"/home/liyumin/.local/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py\", line 628, in register\n    IMAGE_PROCESSOR_MAPPING.register(\n  File \"/home/liyumin/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 833, in register\n    raise ValueError(f\"'{key}' is already used by a Transformers model.\")\nValueError: '<class 'sglang.srt.configs.qwen2_5_vl_config.Qwen2_5_VLConfig'>' is already used by a Transformers model.\n```\n\n### Reproduction\n\n```\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-V3 --tp 32 --dist-init-addr 10.168.16.121:5000 --nnodes 4 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 30000\n```\n\n### Environment\n\n```\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/sgl-workspace/sglang/python/sglang/check_env.py\", line 306, in <module>\n    check_env()\n  File \"/sgl-workspace/sglang/python/sglang/check_env.py\", line 285, in check_env\n    env_info.update(get_package_versions(PACKAGE_LIST))\n  File \"/sgl-workspace/sglang/python/sglang/check_env.py\", line 62, in get_package_versions\n    module = importlib.import_module(package_name)\n  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/home/liyumin/.local/lib/python3.10/site-packages/sgl_kernel/__init__.py\", line 12, in <module>\n    from sgl_kernel import common_ops\nImportError: libcuda.so.1: cannot open shared object file: No such file or directory\n```\n\nI am using ROCm. It seems `python3 -m sglang.check_env` has bug too...",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-20T13:47:28+00:00",
    "closed_at": "2025-03-20T20:39:07+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4629/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4629"
  },
  {
    "number": 4286,
    "title": "[Bug] nextn gen throughput gradually decreases over time",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI deploy deepseek-r1 with 2 8*h20, commit id [df84ab2a](https://github.com/sgl-project/sglang/commit/df84ab2a5b87f4e8490049beb74fab6e67bbe3df),\n\ncommand:\n```\npython3 -m sglang.launch_server --model-path $MODEL_PATH --tp 16 \\\n    --nccl-init-addr $SERVER_HOST:$SERVER_PORT --nnodes 2 --node-rank $rank --trust-remote-code --port 8000 --host 0.0.0.0 \\\n    --mem-fraction-static 0.7 --enable-torch-compile --torch-compile-max-bs 4 --served-model-name deepseek-r1 \\\n    --disable-radix-cache --watchdog-timeout 36000 --speculative-algo NEXTN --speculative-draft $MTP_MODEL_PATH \\\n    --speculative-num-steps 2 --speculative-eagle-topk 4 --speculative-num-draft-tokens 4\n```\n\naccept len is stable, but gen throughput decreases over time:\n\n<img width=\"2274\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/0d2dc99a-0bb0-4b62-810c-3533f2f542f8\" />\n\n### Reproduction\n\n```\npython3 -m sglang.launch_server --model-path $MODEL_PATH --tp 16 \\\n    --nccl-init-addr $SERVER_HOST:$SERVER_PORT --nnodes 2 --node-rank $rank --trust-remote-code --port 8000 --host 0.0.0.0 \\\n    --mem-fraction-static 0.7 --enable-torch-compile --torch-compile-max-bs 4 --served-model-name deepseek-r1 \\\n    --disable-radix-cache --watchdog-timeout 36000 --speculative-algo NEXTN --speculative-draft $MTP_MODEL_PATH \\\n    --speculative-num-steps 2 --speculative-eagle-topk 4 --speculative-num-draft-tokens 4\n```\n\n\n### Environment\n\n```\nPython: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.161.07\nPyTorch: 2.6.0+cu124\nsglang: 0.4.3.post4\nsgl_kernel: 0.0.4\nflashinfer: 0.2.2.post1\ntriton: 3.2.0\ntransformers: 4.48.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.13\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.2\ninteregular: 0.3.3\nmodelscope: 1.23.2\norjson: 3.10.15\npackaging: 23.1\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.65.4\ntiktoken: 0.9.0\nanthropic: 0.49.0\ndecord: 0.6.0\nNVIDIA Topology:\n\tGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tNIC4\tNIC5\tNIC6\tNIC7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tPIX\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t0-95,192-287\t0\t\tN/A\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNODE\tPIX\tPHB\tNODE\tSYS\tSYS\tSYS\tSYS\t0-95,192-287\t0\t\tN/A\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNODE\tPHB\tPIX\tNODE\tSYS\tSYS\tSYS\tSYS\t0-95,192-287\t0\t\tN/A\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNODE\tNODE\tNODE\tPIX\tSYS\tSYS\tSYS\tSYS\t0-95,192-287\t0\t\tN/A\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tPIX\tNODE\tNODE\tNODE\t96-191,288-383\t1\t\tN/A\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tNODE\tPIX\tNODE\tNODE\t96-191,288-383\t1\t\tN/A\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPIX\tPHB\t96-191,288-383\t1\t\tN/A\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPHB\tPIX\t96-191,288-383\t1\t\tN/A\nNIC0\tPIX\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t X \tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\nNIC1\tNODE\tPIX\tPHB\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\t X \tPHB\tNODE\tSYS\tSYS\tSYS\tSYS\nNIC2\tNODE\tPHB\tPIX\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\tPHB\t X \tNODE\tSYS\tSYS\tSYS\tSYS\nNIC3\tNODE\tNODE\tNODE\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\t X \tSYS\tSYS\tSYS\tSYS\nNIC4\tSYS\tSYS\tSYS\tSYS\tPIX\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t X \tNODE\tNODE\tNODE\nNIC5\tSYS\tSYS\tSYS\tSYS\tNODE\tPIX\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\t X \tNODE\tNODE\nNIC6\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPIX\tPHB\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\t X \tPHB\nNIC7\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPHB\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPHB\t X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_bond_0\n  NIC1: mlx5_bond_1\n  NIC2: mlx5_bond_2\n  NIC3: mlx5_bond_3\n  NIC4: mlx5_bond_4\n  NIC5: mlx5_bond_5\n  NIC6: mlx5_bond_6\n  NIC7: mlx5_bond_7\n\n\nulimit soft: 1048576\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-11T06:25:31+00:00",
    "closed_at": "2025-03-11T09:19:38+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4286/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4286"
  },
  {
    "number": 2458,
    "title": "[Feature] MoE Expert Parallel with awq ",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nI attempted to launch deepseek-awq using expert parallelism, but it appears to be unsupported. Is there a plan to support this feature in the future?\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2024-12-12T00:24:25+00:00",
    "closed_at": "2024-12-12T04:08:42+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2458/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2458"
  },
  {
    "number": 3884,
    "title": "[Bug] Qwen2.5-VL-7B-Instruct-AWQ sglang accuracy",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen performing inference on an image using the Qwen2.5-VL-7B-Instruct-AWQ model in sglang, the results differ from those obtained with vllm. Specifically, vllm provides accurate outputs, whereas sglang does not.\n\n### Reproduction\n\n\u4ee3\u7801\u4e3a   \n\nresponse = client.chat.completions.create(\n        model='/models/Qwen2.5-VL-7B-Instruct-AWQ', # ModelScope Model-Id\n        messages=[\n          {\n            \"role\": \"user\",\n            \"content\": [\n              {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                  \"url\": encode_url_or_file_to_base64(\"/root/ab0a158078f44599b7ff99cd887dac8d.jpg\")\n                }\n              },\n              {\n                \"type\": \"text\",\n                \"text\": \"\u56fe\u7247\u4e2d\u6709\u5feb\u9012\u76d2\u5b50\u561b\uff1f\"\n              }\n            ]\n          }\n\n        ],\n        temperature=0,\n        max_tokens=20,\n        top_p=0.6,\n        seed=1\n    )\n    print(time.time()-start)\n    print(file_path,response.choices[0].message.content)\n\nvllm verson 0.7.2 \nvllm serve /models/Qwen2.5-VL-7B-Instruct-AWQ    --port 9080  --gpu-memory-utilization 0.75  --dtype=half --max-model-len 500  --limit-mm-per-prompt \"image=1,video=0\"\nvllm\u8fd4\u56de \u201c\u56fe\u7247\u4e2d\u6709\u4e00\u4e2a\u5feb\u9012\u76d2\u5b50\uff0c\u4f4d\u4e8e\u753b\u9762\u4e2d\u592e\u7684\u5730\u677f\u4e0a\u3002\u201d\n\n\nsglang  version 0.4.3.post2\ndocker run  --gpus all  -d  --net=host \\\n        --name my_vllm_container1 \\\n        -v /apps/.cache/huggingface:/root/.cache/huggingface \\\n        -v ./src:/app \\\n        -v $PWD/models:/models \\\n        --env \"HUGGING_FACE_HUB_TOKEN=\" \\\n         --env \"HF_ENDPOINT=https://hf-mirror.com\" \\\n        -p 8000:8000 \\\n        --ipc=host \\\n        lmsysorg/sglang:latest-srt python3 -m sglang.launch_server --model-path  /models/Qwen2.5-VL-7B-Instruct-AWQ   --host 0.0.0.0 --port 9080 --mem-fraction-static 0.75  --chat-template=qwen2-vl   --trust-remote-code --chunked-prefill-size 4096 --enable-torch-compile --disable-radix-cache  --max-running-request 25\n\u8fd4\u56de \u201c\u62b1\u6b49\uff0c\u60a8\u63d0\u4f9b\u7684\u56fe\u7247\u5185\u5bb9\u5e76\u6ca1\u6709\u5feb\u9012\u76d2\u5b50\u7684\u4fe1\u606f\u201d\n\n\n### Environment\n\nt4",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-26T09:39:06+00:00",
    "closed_at": "2025-02-26T17:16:28+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3884/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3884"
  },
  {
    "number": 283,
    "title": "[BUG] Flashinfer 0.0.3 compat with Sglang",
    "body": "Using flashinfer 0.0.3 requires one line change #282 but there is a compat issue where same model runs fine on 0.0.2 but under 0.0.3 throws an infinite loop of the following on sglang:\r\n\r\n```\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 184, in exposed_step\r\n    self.forward_step()\r\n  File \"/root/miniconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 211, in forward_step\r\n    self.forward_decode_batch(self.running_batch)\r\n  File \"/root/miniconda3/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 505, in forward_decode_batch\r\n    next_token_ids, _ = batch.sample(logits)\r\n                        ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/lib/python3.11/site-packages/sglang/srt/managers/router/infer_batch.py\", line 476, in sample\r\n    sampled_index = torch.multinomial(probs_sort, num_samples=1)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n```\r\n\r\nI am unsure if this is compat issue due to sglang or flashinfer 0.0.3.\r\n\r\n @merrymercy  @yzh119",
    "labels": [],
    "state": "closed",
    "created_at": "2024-03-12T00:33:39+00:00",
    "closed_at": "2024-03-12T13:45:59+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/283/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/283"
  }
]
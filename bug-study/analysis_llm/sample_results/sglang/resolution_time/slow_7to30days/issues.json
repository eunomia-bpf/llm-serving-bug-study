[
  {
    "number": 2508,
    "title": "[Feature] do sample = False?",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nI am testing the sglang with qwen2, but the output seems to be unstable even if I have set sampling params;\r\n\r\n```\r\ntemp = 0,\r\ntop_k = 1\r\n```\r\n\r\nBut I got inconsistent outputs, while naive HF implementation always gives me same output when set do_sample = False,\r\nis there such feature in SGL?  Is this due to some non-deterministic Cuda operations? Thanks\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2024-12-18T08:53:46+00:00",
    "closed_at": "2024-12-26T18:53:11+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2508/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2508"
  },
  {
    "number": 1870,
    "title": "Question: Does sglang support prefix cache for multimodal models?",
    "body": "I noticed that the cache hit rate remains 0.0% no matter how much turns the conversation has. Does sglang support prefix cache for multimodal models?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-11-01T11:03:05+00:00",
    "closed_at": "2024-11-14T19:08:19+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1870/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1870"
  },
  {
    "number": 4842,
    "title": "[Feature] Mooncake CPP (Chunked Pipeline Parallelism)",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nHi team, \n\nI found that the sequence parallelism (Meta CP based on https://www.arxiv.org/pdf/2411.01783) is in the roadmap (https://github.com/sgl-project/sglang/issues/4042). For long-context inference, I'm wondering do you have plans to support **chunked pipeline parallelism** from Mooncake or **sequence pipeline parallelism** from Mnemosyne? \nIf there are no plans, could you please leave some comments about your thoughts on the technology to combine chunked prefill + pp for long-context prefilling, compared with Context Parallelism? Really appreciate your attention~\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-28T03:52:49+00:00",
    "closed_at": "2025-04-26T08:33:44+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4842/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4842"
  },
  {
    "number": 6301,
    "title": "Why DP EP Use Two Different MLA kernel in Prefill phase",
    "body": "<img width=\"1275\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f8eb14a5-c42c-4edd-83fe-efbf55be3bdc\" />\n\n<img width=\"1275\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f2453597-9c39-489d-92d4-1a3b791d517d\" />\n\nI modified the DeepSeek model to have 2 layers for single - machine testing. When running 8DP and 8EP, I noticed that the MLA part of two Prefills used two different Kernels. I don't understand why two different Kernels are used here. As shown in the figure, the Kernel called by MLA in the first Prefill is \"void flashinfer::PrefillWithKVCacheKernel\", and the Kernel called by MLA in the second Prefill is \"void flashinfer::mla::hopper::BatchMLAPageAttentionHopperKernel\".\n",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-14T16:22:46+00:00",
    "closed_at": "2025-05-28T08:12:25+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6301/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6301"
  },
  {
    "number": 7108,
    "title": "[OAI Server Refactor] [ChatCompletions & Completions] Remove batch requests, refine validation and streaming",
    "body": "**Points:** 1-2 days\n\n**Description:** Remove the batch requests process, refine request validation logic and streaming generator structure.\n\n**Deliverables:**\n\n- [x] Finish tasks listed below\n- [x] Add UTs if available",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-12T00:31:22+00:00",
    "closed_at": "2025-06-21T17:45:36+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7108/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7108"
  },
  {
    "number": 7447,
    "title": "[Feature] Graceful handling of non-existing lora_path in inference request",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCreating an issue to track this TODO for myself (or anyone else who wants to help):\n\nCurrently when users call SGLang with a non-existing lora_path, SGLang server/engine would crash due to failed assertions in `prepare_lora_batch`. This is unideal as it imposes unnecessary burden for server owner to validate request params before they are passed to the SGLang backend.\n\nIdeally, SGLang should have gracefully handled the exception and respond 4xx errors without crashing the server.\n\n### Related resources\n\n_No response_",
    "labels": [
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-06-22T21:36:01+00:00",
    "closed_at": "2025-07-03T03:59:17+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7447/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7447"
  },
  {
    "number": 1506,
    "title": "[Bug] LLaVa-next does not work for single image processing",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nINFO 09-24 16:44:39 weight_utils.py:236] Using model weights format ['*.safetensors']\r\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.63it/s]\r\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.34it/s]\r\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.19it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.16it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.24it/s]\r\n\r\nchat template: llama-3-instruct\r\n\r\n========== single ==========\r\n\r\n[16:46:44 TP0] Exception in ModelTpServer:\r\nTraceback (most recent call last):\r\n  File \"/home/benzshawelt/.conda/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 239, in exposed_step\r\n    self.forward_step()\r\n  File \"/home/benzshawelt/.conda/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/benzshawelt/.conda/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 259, in forward_step\r\n    self.forward_prefill_batch(new_batch)\r\n  File \"/home/benzshawelt/.conda/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 560, in forward_prefill_batch\r\n    logits_output = self.model_runner.forward(batch)\r\n  File \"/home/benzshawelt/.conda/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 519, in forward\r\n    return self.forward_extend_multi_modal(batch)\r\n  File \"/home/benzshawelt/.conda/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/benzshawelt/.conda/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 506, in forward_extend_multi_modal\r\n    return self.model.forward(\r\n  File \"/home/benzshawelt/.conda/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/benzshawelt/.conda/envs/sglang/lib/python3.10/site-packages/sglang/srt/models/llava.py\", line 188, in forward\r\n    if modalities_list[image_idx] == \"image\":\r\nIndexError: list index out of range\r\n\r\n\r\nWhen sending one image to the model it crashes.\n\n### Reproduction\n\nInstalled sglang following the instructions, used the llava-next example in your repo, and resulted with the error.\r\nhttps://github.com/sgl-project/sglang/blob/main/examples/frontend_language/quick_start/local_example_llava_next.py\n\n### Environment\n\nThe environment is a plain conda env with python 3.10 and ONLY the installations as described in the SGLANG readme. This is on a computer using slurm, I am on a node with 2 h100 nvidia gpus.\r\n\r\n![image](https://github.com/user-attachments/assets/18baab31-280f-4cb5-bcd3-98dc5cbf06ce)\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-09-24T21:52:55+00:00",
    "closed_at": "2024-10-06T22:42:54+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1506/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1506"
  },
  {
    "number": 6552,
    "title": "[Feature] Multimodal models with prefix caching",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nI know currently SGLang Multimodal models are not to work with prefix caching \uff08Qwen2.5 VL\uff09\u3002\n\nSo:\n1.What's the main reason for this ?\n2.What should I do if I want to develop with the source code to support this?\nI have gone though the tokenize manager and radix cache match func source code but I can not find the problem.Can someone help me and point the right direction?(which module/folder/python file or any toher directions)\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-23T08:41:11+00:00",
    "closed_at": "2025-06-16T08:50:54+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6552/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6552"
  },
  {
    "number": 3637,
    "title": "[Bug] pydantic validation errors for ChatCompletion",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nwhen use autogen with qwen2.5\n\nmessages=[SystemMessage(content='You are a helpful AI assistant. Solve tasks using your tools. Reply with TERMINATE when the task has been completed.', type='SystemMessage'), UserMessage(content='shanghai weather', source='user', type='UserMessage')], \nclient.chat.completions.create(\n                        messages=messages,\n                        stream=False,\n                        tools=converted_tools,\n                        **create_args,\n                    ) \n when using the SGLang inference framework, it can normally infer and call the function named \"get_weather.\" However, if the conversation continues and carries the history of previous chats, there will be an issue.\n\nmessages=[SystemMessage(content='You are a helpful AI assistant. Solve tasks using your tools. Reply with TERMINATE when the task has been completed.', type='SystemMessage'), UserMessage(content='shanghai weather', source='user', type='UserMessage'), AssistantMessage(content=[FunctionCall(id='0', arguments='{\"city\": \"shanghai\"}', name='get_weather')], source='weather_agent', type='AssistantMessage'), FunctionExecutionResultMessage(content=[FunctionExecutionResult(content='The weather in shanghai is 73 degrees and Sunny.', call_id='0', is_error=False)], type='FunctionExecutionResultMessage'), UserMessage(content='beijing weather', source='user', type='UserMessage')]\uff0c\n\npydantic_core._pydantic_core.ValidationError: 5 validation errors for ChatCompletionRequest\nmessages.2.ChatCompletionMessageGenericParam.content.str\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\nmessages.2.ChatCompletionMessageGenericParam.content.list[ChatCompletionMessageContentTextPart]\n  Input should be a valid list [type=list_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.10/v/list_type\nmessages.2.ChatCompletionMessageUserParam.role\n  Input should be 'user' [type=literal_error, input_value='assistant', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/literal_error\nmessages.2.ChatCompletionMessageUserParam.content.str\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\nmessages.2.ChatCompletionMessageUserParam.content.list[union[ChatCompletionMessageContentTextPart,ChatCompletionMessageContentImagePart]]\n  Input should be a valid list [type=list_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.10/v/list_type\n\n### Reproduction\n\n python3 -m sglang.launch_server --model-path  /mnt/vdb1/model/qwen/Qwen2___5-7B-Instruct-GPTQ-Int4/ --served-model-name qwen2-72b --host 0.0.0.0 --port 8001 --trust-remote-code  --mem-fraction-static 0.2 --chunked-prefill-size 8192 --schedule-conservativeness 0.3 --tool-call-parser qwen25\n\n### Environment\n\nubuntu \nautogen 0.4.6",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-17T12:53:53+00:00",
    "closed_at": "2025-03-06T15:15:36+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3637/reactions",
      "total_count": 5,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 2,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3637"
  },
  {
    "number": 4366,
    "title": "[Bug] Use torch.inference_mode instead of torch.no_grad",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWe found that `torch.no_grad` triggers the `AutogradXXX` backend for certain operators. Should we replace it with `inference_mode` instead, or keep supporting with `torch<1.9`?\n\n### Reproduction\n\nExample: `python/sglang/srt/mem_cache/memory_pool.py:144(def free_group_end(self):)`\n\n- Result with torch.no_grad(): `NotImplementedError: Could not run 'aten::concat' with arguments from the 'AutogradXXX' backend`\n\n- Result with torch.inference_mode(): No Error\n\n### Environment\n\nENV: `torch version == 2.6.0`\nSorry that I could only reproduce this bug on our private hardware platform, so I cannot show my full environment. :(",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-13T06:32:51+00:00",
    "closed_at": "2025-04-04T05:18:52+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4366/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4366"
  },
  {
    "number": 3895,
    "title": "[Bug] OpenAI Endpoint '/v1/batches': `error: Object of type ChoiceLogprobs is not JSON serializable`",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nFirst of all, thanks for this amazing framework!\nWhen processing a batch via the OpenAI-compatible endpoint 'v1/batches' , and one requests the output of the logprobs, the server outputs the following error:\n```shell\n[2025-02-26 20:13:21 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 35, cache hit rate: 44.87%, token usage: 0.35, #running-req: 0, #queue-req: 0\n[2025-02-26 20:13:22 TP0] Decode batch. #running-req: 1, #token: 48, token usage: 0.48, gen throughput (token/s): 5.54, #queue-req: 0\n[2025-02-26 20:13:22] error: Object of type ChoiceLogprobs is not JSON serializable\n```\nThe issue is [this line](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/openai_api/adapter.py#L1130) where we probably want to call `model_dump()` on the `choice_logprobs`. But maybe a cleaner solution would be to remove the `to_file` argument entirely and simply call `model_dump_json()` [here](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/openai_api/adapter.py#L362)?\n\nIf you are interested in this solution I would be more than happy to open a PR :)\n\n### Reproduction\n\nFrom the [example in the documentation](https://docs.sglang.ai/backend/openai_api_completions.html#Batches)\n```python\nimport json\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=f\"http://127.0.0.1:30000/v1\", api_key=\"None\")\n\nrequests = [\n    {\n        \"custom_id\": \"request-1\",\n        \"method\": \"POST\",\n        \"url\": \"/chat/completions\",\n        \"body\": {\n            \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n            \"messages\": [\n                {\"role\": \"user\", \"content\": \"Tell me a joke about programming\"}\n            ],\n            \"max_tokens\": 50,\n            \"logprobs\": True,\n            \"top_logprobs\": 1,\n        },\n    },\n    {\n        \"custom_id\": \"request-2\",\n        \"method\": \"POST\",\n        \"url\": \"/chat/completions\",\n        \"body\": {\n            \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"What is Python?\"}],\n            \"max_tokens\": 50,\n        },\n    },\n]\n\ninput_file_path = \"batch_requests.jsonl\"\n\nwith open(input_file_path, \"w\") as f:\n    for req in requests:\n        f.write(json.dumps(req) + \"\\n\")\n\nwith open(input_file_path, \"rb\") as f:\n    file_response = client.files.create(file=f, purpose=\"batch\")\n\nbatch_response = client.batches.create(\n    input_file_id=file_response.id,\n    endpoint=\"/v1/chat/completions\",\n    completion_window=\"24h\",\n)\n\nprint(f\"Batch job created with ID: {batch_response.id}\")\n```\n\n### Environment\n\n```shell\n> python -m sglang.check_env\nINFO 02-26 20:42:30 __init__.py:190] Automatically detected platform cuda.\nPython: 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0]\nCUDA available: True\nGPU 0: NVIDIA GeForce GTX 1650\nGPU 0 Compute Capability: 7.5\nCUDA_HOME: /usr\nNVCC: Cuda compilation tools, release 12.0, V12.0.140\nCUDA Driver Version: 535.183.01\nPyTorch: 2.5.1+cu124\nsglang: 0.4.3\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.49.0\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.13\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.1\ninteregular: 0.3.3\nmodelscope: 1.23.1\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.64.0\ntiktoken: 0.9.0\nanthropic: 0.47.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-11    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-26T20:03:36+00:00",
    "closed_at": "2025-03-13T05:04:30+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3895/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3895"
  },
  {
    "number": 5732,
    "title": "[Bug] Incorrect Memory Allocation on CUDA:0 by Non-Zero CUDA Processes in TP/DP",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nA potential memory leak has been identified related to the handling and broadcasting of multimodal data in distributed setups. The issue seems to originate from the interaction between the `BaseMultiModalProcessor` logic and the `broadcast_pyobj` utility function used by the `Scheduler`.\n\n![Image](https://github.com/user-attachments/assets/889a561c-db51-4a69-ac1b-acf2dc71a3b7)\n\n\n**Code Locations:**\n\n1.  Relevant processing logic: https://github.com/sgl-project/sglang/blob/b5be56944b6eb61b44866011f157e8df0e563bd7/python/sglang/srt/managers/multimodal_processors/base_processor.py#L85\n2.  Serialization/Broadcasting utility: https://github.com/sgl-project/sglang/blob/b5be56944b6eb61b44866011f157e8df0e563bd7/python/sglang/srt/utils.py#L907\n\n\nThe suspected cause lies within the `Scheduler`'s `recv_requests -> broadcast_pyobj` workflow.\n1.  Data (containing tensors, `pixel_values` ) is serialized on non-zero ranks (`rank != 0`) using `serialized_data = bytes(tensor_data.cpu().numpy())` within `broadcast_pyobj` (ref: `utils.py#L907`).\n2.  When this serialized data is received and deserialized by other worker ranks (`rank > 0`), the tensors within seem to be incorrectly assigned to device `cuda:0`.\n3.  It is expected that these tensors should be placed on the receiving rank's corresponding device (e.g., `cuda:rank`).\n4.  This apparent misallocation to `cuda:0` on all receiving ranks leads to memory accumulating incorrectly, causing a leak.\n\n\n\n* The severity of the memory leak appears to increase as the total number of ranks (`world_size`) increases.\n\n* It is also hypothesized that the total amount of leaked memory may be proportional to the number/size of images (or tensor data) being broadcasted through this mechanism. This requires further investigation and testing to confirm.\n\n\n\n\n### Reproduction\n\nhttps://docs.sglang.ai/backend/openai_api_vision.html \n\n\nWhen using the Qwen2.5-VL model and following the configuration guidelines provided on the sglang official website, a memory leak can be observed after sending a single request.\n\n\n### Environment\n\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 565.57.01\nPyTorch: 2.5.1+cu124\nsglang: 0.4.5.post1\nsgl_kernel: 0.0.9.post1\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.51.1\ntorchao: 0.10.0\nnumpy: 2.2.4\naiohttp: 3.11.16\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.11.3\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.1\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.17\nopenai: 1.75.0\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.66.1\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    SYS     PIX     NODE    SYS     SYS     0-89   0N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     PIX     NODE    SYS     SYS     0-89   0N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     NODE    PIX     SYS     SYS     0-89   0N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     NODE    PIX     SYS     SYS     0-89   0N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     PIX     NODE    90-179 1N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     PIX     NODE    90-179 1N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     NODE    PIX     90-179 1N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     NODE    PIX     90-179 1N/A\nNIC0    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS\nNIC1    PIX     PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS      X      NODE    SYS     SYS\nNIC2    NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     SYS     NODE     X      SYS     SYS\nNIC3    SYS     SYS     SYS     SYS     PIX     PIX     NODE    NODE    SYS     SYS     SYS      X      NODE\nNIC4    SYS     SYS     SYS     SYS     NODE    NODE    PIX     PIX     SYS     SYS     SYS     NODE     X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n\n\nHypervisor vendor: KVM\nulimit soft: 1048576\n",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-25T04:42:53+00:00",
    "closed_at": "2025-05-09T00:52:27+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5732/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5732"
  },
  {
    "number": 6215,
    "title": "[Feature] [PD] TransferEngine fault auto-recovery: allows Prefill node failures without requiring a restart of Decode.",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCurrently, if a Prefill node fails, the Decode side may hang during data transfer if cached information about the failed node is still in use. A mechanism is needed to notify Decode to re-establish the connection with a healthy Prefill node.\n\nCC @ByronHsu @ShangmingCai \n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-12T07:23:10+00:00",
    "closed_at": "2025-05-31T14:07:13+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6215/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6215"
  },
  {
    "number": 2584,
    "title": "[Bug] libcudart.so.12: cannot open shared object file: No such file or directory",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThere are no CUDA-related libraries in the rocm environment, but the SGLANG 0.4.1 version will report an error, while the 0.4.0 and earlier versions will not\r\n\r\n**error info:** \r\nImportError: [address=0.0.0.0:39501, pid=13418] libcudart.so.12: cannot open shared object file: No such file or directory\r\n2024-12-26 10:14:09,664 xinference.api.restful_api 4247 ERROR    [address=0.0.0.0:39501, pid=13418] libcudart.so.12: cannot open shared object file: No such file or directory\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xinference/api/restful_api.py\", line 1002, in launch_model\r\n    model_uid = await (await self._get_supervisor_ref()).launch_builtin_model(\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xoscar/backends/context.py\", line 231, in send\r\n    return self._process_result_message(result)\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xoscar/backends/context.py\", line 102, in _process_result_message\r\n    raise message.as_instanceof_cause()\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xoscar/backends/pool.py\", line 667, in send\r\n    result = await self._run_coro(message.message_id, coro)\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xoscar/backends/pool.py\", line 370, in _run_coro\r\n    return await coro\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xoscar/api.py\", line 384, in __on_receive__\r\n    return await super().__on_receive__(message)  # type: ignore\r\n  File \"xoscar/core.pyx\", line 558, in __on_receive__\r\n    raise ex\r\n  File \"xoscar/core.pyx\", line 520, in xoscar.core._BaseActor.__on_receive__\r\n    async with self._lock:\r\n  File \"xoscar/core.pyx\", line 521, in xoscar.core._BaseActor.__on_receive__\r\n    with debug_async_timeout('actor_lock_timeout',\r\n  File \"xoscar/core.pyx\", line 526, in xoscar.core._BaseActor.__on_receive__\r\n    result = await result\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xinference/core/supervisor.py\", line 1041, in launch_builtin_model\r\n    await _launch_model()\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xinference/core/supervisor.py\", line 1005, in _launch_model\r\n    await _launch_one_model(rep_model_uid)\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xinference/core/supervisor.py\", line 984, in _launch_one_model\r\n    await worker_ref.launch_builtin_model(\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xoscar/backends/context.py\", line 231, in send\r\n    return self._process_result_message(result)\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xoscar/backends/context.py\", line 102, in _process_result_message\r\n    raise message.as_instanceof_cause()\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xoscar/backends/pool.py\", line 667, in send\r\n    result = await self._run_coro(message.message_id, coro)\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xoscar/backends/pool.py\", line 370, in _run_coro\r\n    return await coro\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xoscar/api.py\", line 384, in __on_receive__\r\n    return await super().__on_receive__(message)  # type: ignore\r\n  File \"xoscar/core.pyx\", line 558, in __on_receive__\r\n    raise ex\r\n  File \"xoscar/core.pyx\", line 520, in xoscar.core._BaseActor.__on_receive__\r\n    async with self._lock:\r\n  File \"xoscar/core.pyx\", line 521, in xoscar.core._BaseActor.__on_receive__\r\n    with debug_async_timeout('actor_lock_timeout',\r\n  File \"xoscar/core.pyx\", line 526, in xoscar.core._BaseActor.__on_receive__\r\n    result = await result\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xinference/core/utils.py\", line 90, in wrapped\r\n    ret = await func(*args, **kwargs)\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xinference/core/worker.py\", line 897, in launch_builtin_model\r\n    await model_ref.load()\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xoscar/backends/context.py\", line 231, in send\r\n    return self._process_result_message(result)\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xoscar/backends/context.py\", line 102, in _process_result_message\r\n    raise message.as_instanceof_cause()\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xoscar/backends/pool.py\", line 667, in send\r\n    result = await self._run_coro(message.message_id, coro)\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xoscar/backends/pool.py\", line 370, in _run_coro\r\n    return await coro\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xoscar/api.py\", line 384, in __on_receive__\r\n    return await super().__on_receive__(message)  # type: ignore\r\n  File \"xoscar/core.pyx\", line 558, in __on_receive__\r\n    raise ex\r\n  File \"xoscar/core.pyx\", line 520, in xoscar.core._BaseActor.__on_receive__\r\n    async with self._lock:\r\n  File \"xoscar/core.pyx\", line 521, in xoscar.core._BaseActor.__on_receive__\r\n    with debug_async_timeout('actor_lock_timeout',\r\n  File \"xoscar/core.pyx\", line 526, in xoscar.core._BaseActor.__on_receive__\r\n    result = await result\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xinference/core/model.py\", line 414, in load\r\n    self._model.load()\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/xinference/model/llm/sglang/core.py\", line 135, in load\r\n    self._engine = sgl.Runtime(\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/sglang/api.py\", line 39, in Runtime\r\n    from sglang.srt.server import Runtime\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/sglang/srt/server.py\", line 47, in <module>\r\n    from sglang.srt.managers.data_parallel_controller import (\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/sglang/srt/managers/data_parallel_controller.py\", line 25, in <module>\r\n    from sglang.srt.managers.io_struct import (\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/sglang/srt/managers/io_struct.py\", line 24, in <module>\r\n    from sglang.srt.managers.schedule_batch import BaseFinishReason\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/sglang/srt/managers/schedule_batch.py\", line 40, in <module>\r\n    from sglang.srt.configs.model_config import ModelConfig\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/sglang/srt/configs/model_config.py\", line 24, in <module>\r\n    from sglang.srt.layers.quantization import QUANTIZATION_METHODS\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/sglang/srt/layers/quantization/__init__.py\", line 25, in <module>\r\n    from sglang.srt.layers.quantization.fp8 import Fp8Config\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/sglang/srt/layers/quantization/fp8.py\", line 31, in <module>\r\n    from sglang.srt.layers.moe.fused_moe_triton.fused_moe import padding_size\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/__init__.py\", line 4, in <module>\r\n    import sglang.srt.layers.moe.fused_moe_triton.fused_moe  # noqa\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\", line 14, in <module>\r\n    from sgl_kernel import moe_align_block_size as sgl_moe_align_block_size\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/sgl_kernel/__init__.py\", line 1, in <module>\r\n    from .ops import (\r\n  File \"/root/miniconda3/envs/xinf/lib/python3.10/site-packages/sgl_kernel/ops/__init__.py\", line 1, in <module>\r\n    from .custom_reduce_cuda import all_reduce as _all_reduce\r\nImportError: [address=0.0.0.0:39501, pid=13418] libcudart.so.12: cannot open shared object file: No such file or directory\r\n2024-12-26 10:14:09,665 uvicorn.access 4247 INFO     127.0.0.1:47452 - \"POST /v1/models HTTP/1.1\" 500\r\n\n\n### Reproduction\n\nqwen2.5-instruct-7B\n\n### Environment\n\n(xinf) root@DESKTOP-ESRGKIB:~# python -m sglang.check_env\r\n2024-12-26 10:26:55.241465: E external/local_xla/xla/stream_executor/plugin_registry.cc:91] Invalid plugin kind specified: FFT\r\n2024-12-26 10:26:56.971386: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-12-26 10:26:57.864187: E external/local_xla/xla/stream_executor/plugin_registry.cc:91] Invalid plugin kind specified: DNN\r\nWARNING 12-26 10:27:06 rocm.py:31] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\r\nPython: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]\r\nROCM available: True\r\nGPU 0: AMD Radeon RX 7900 XT\r\nGPU 0 Compute Capability: 11.0\r\nROCM_HOME: /opt/rocm\r\nHIPCC: HIP version: 6.3.42131-fa1d09cbd\r\nROCM Driver Version: \r\nPyTorch: 2.4.0+rocm6.3.0\r\nsglang: 0.4.1\r\nflashinfer: Module Not Found\r\ntriton: 3.0.0+rocm6.3.0_75cc27c26a\r\ntransformers: 4.46.2\r\ntorchao: 0.6.1\r\nnumpy: 1.26.4\r\naiohttp: 3.10.10\r\nfastapi: 0.115.4\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.26.5\r\ninteregular: 0.3.3\r\nmodelscope: 1.19.2\r\norjson: 3.10.11\r\npackaging: 24.1\r\npsutil: 6.1.0\r\npydantic: 2.9.2\r\nmultipart: 0.0.12\r\nzmq: 26.2.0\r\nuvicorn: 0.32.0\r\nuvloop: 0.21.0\r\nvllm: 0.6.6.dev44+gc2d1b075.d20241221\r\nopenai: 1.54.1\r\nanthropic: 0.39.0\r\ndecord: 0.6.0\r\nAMD Topology: \r\n\r\nHypervisor vendor: Microsoft\r\nulimit soft: 1024\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-12-26T02:31:18+00:00",
    "closed_at": "2025-01-24T04:35:16+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2584/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2584"
  },
  {
    "number": 4409,
    "title": "[Usage] What's the best practice of deploying DeepSeekV3 using sglang?",
    "body": "I want to run inference of [DeepSeekV3](https://huggingface.co/deepseek-ai/DeepSeek-V3) on multi-node GPU clusters. I have followed the [sglang deepseek guide](https://docs.sglang.ai/references/deepseek.html) to setup the distributed serving environment with the official sglang-v0.4.3.post4-cu124 docker image.\n\nMore specifically:\n```\n# node 1\ndocker run --gpus all \\\n    --shm-size 32g \\\n    --network=host \\\n    --privileged \\\n    -v xxx:xxx \\\n    --name sglang_multinode1 \\\n    -it \\\n    --ipc=host \\\n    xxx\n\n# node 2\ndocker run --gpus all \\\n    --shm-size 32g \\\n    --network=host \\\n    --privileged \\\n    -v xxx:xxx \\\n    --name sglang_multinode2 \\\n    -it \\\n    --ipc=host \\\n    xxx\n```\nto create docker containers\n\n```\n# node 1\nNCCL_IB_GID_INDEX=3 \\\nNCCL_DEBUG=TRACE \\\nNCCL_DEBUG=INFO \\\npython3 -m sglang.launch_server \\\n    --model-path xxx \\\n    --tp 16 --dist-init-addr xxx:20000 \\\n    --nnodes 2 \\\n    --node-rank 0 \\\n    --trust-remote-code \\\n    --host 0.0.0.0 --port 40000 2>&1 | tee api_serve.log\n\n# node 2\nNCCL_IB_GID_INDEX=3 \\\nNCCL_DEBUG=TRACE \\\nNCCL_DEBUG=INFO \\\npython3 -m sglang.launch_server \\\n    --model-path xxx \\\n    --tp 16 --dist-init-addr xxx:20000 \\\n    --nnodes 2 \\\n    --node-rank 1 \\\n    --trust-remote-code \\\n    --host 0.0.0.0 --port 40000 2>&1 | tee api_serve.log\n```\nto start sglang deepseekv3 api serving.\n\n```\nfor out_len in 1024 2048 4096 8192\ndo\n    echo \"running with output-len $out_len\"\n    python3 -m sglang.bench_serving \\\n        --model xxx \\\n        --backend sglang \\\n        --dataset-name random \\\n        --random-input 2048 \\\n        --random-output $out_len \\\n        --num-prompts 2000 \\\n        --output-file result-out$out_len.json \\\n        --seed 42 \\\n        --host 0.0.0.0 --port 40000 2>&1 | tee log_out$out_len.log\ndone\n```\nfor benchmarking sglang\n\nWe have already enabled RDMA for fast inter-node communications. Some benchmark results are shown as follows:\n\n![Image](https://github.com/user-attachments/assets/c89c4234-7322-43b0-88f2-fb1af9198459)\n\nit seems sglang has unexpectedly low output throughput with short output length settings like 1024/2048.\n\n\n\nI want to ask:\n1. Any additional flags to set for speedup?\n2. Any additional dependencies or code to update? Since sglang continuously integrates components like [FlashMLA](https://github.com/deepseek-ai/FlashMLA)\n\nor anything that is missing to achieve the best inference speed of DeepSeekV3 with sglang?",
    "labels": [
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-03-14T03:02:37+00:00",
    "closed_at": "2025-03-26T08:09:34+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4409/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4409"
  },
  {
    "number": 737,
    "title": "[Bug] TypeError: argument 'ids': 'NoneType' object cannot be interpreted as an integer",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [X] 2. The bug has not been fixed in the latest version.\r\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n\r\n### Describe the bug\r\n\r\n```\r\nException in ModelTpServer:\r\nTraceback (most recent call last):\r\n  File \".../sglang-0.2.0/lib/python3.11/site-packages/sglang/srt/managers/controller/tp_worker.py\", line 186, in exposed_step\r\n    self.forward_step()\r\n  File \".../sglang-0.2.0/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang-0.2.0/lib/python3.11/site-packages/sglang/srt/managers/controller/tp_worker.py\", line 216, in forward_step\r\n    self.forward_decode_batch(self.running_batch)\r\n  File \".../sglang-0.2.0/lib/python3.11/site-packages/sglang/srt/managers/controller/tp_worker.py\", line 557, in forward_decode_batch\r\n    jump_forward_reqs = batch.check_for_jump_forward(self.model_runner)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang-0.2.0/lib/python3.11/site-packages/sglang/srt/managers/controller/infer_batch.py\", line 526, in check_for_jump_forward\r\n    decode_res, new_text = req.get_next_inc_detokenization()\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang-0.2.0/lib/python3.11/site-packages/sglang/srt/managers/controller/infer_batch.py\", line 166, in get_next_inc_detokenization\r\n    new_text = self.tokenizer.decode(\r\n               ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".../sglang-0.2.0/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 4034, in decode\r\n    return self._decode(\r\n           ^^^^^^^^^^^^^\r\n  File \".../sglang-0.2.0/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\", line 651, in _decode\r\n    text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: argument 'ids': 'NoneType' object cannot be interpreted as an integer\r\n```\r\n\r\n### Reproduction\r\nUpdate 1: The same code runs correctly on Qwen2. However,  when I switch to Llama3.1 it throws this exception\r\n~~This issue seems to be triggered randomly and I cannot find a stable way to recover this bug~~\r\n\r\n### Environment\r\n\r\n```Shell\r\nPython: 3.11.9 (main, Apr 14 2024, 11:55:19) [GCC 11.2.1 20210728 (Red Hat 11.2.1-1)]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA A800-SXM4-80GB\r\nCUDA_HOME: /usr/local/cuda-11.7\r\nNVCC: Cuda compilation tools, release 11.7, V11.7.64\r\nCUDA Driver Version: 470.161.03\r\n470.161.03\r\n470.161.03\r\n470.161.03\r\n470.161.03\r\n470.161.03\r\n470.161.03\r\n470.161.03\r\nPyTorch: 2.3.1+cu121\r\nsglang: 0.2.0\r\nflashinfer: 0.1.1+cu121torch2.3\r\nrequests: 2.32.3\r\ntqdm: 4.66.4\r\nnumpy: 1.26.4\r\naiohttp: 3.9.5\r\nfastapi: 0.111.1\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.2\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\npillow: Module Not Found\r\npsutil: 6.0.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.3\r\nuvloop: 0.19.0\r\nzmq: 26.0.3\r\nvllm: 0.5.3.post1\r\nopenai: 1.37.1\r\nanthropic: 0.31.2\r\nNVIDIA Topology:\r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    mlx5_0  mlx5_1  mlx5_2  mlx5_3  mlx5_4  mlx5_5  mlx5_6  mlx5_7  mlx5_8  CPU Affinity    NUMA Affinity\r\nGPU0     X      NV8     NV8     NV8     NV8     NV8     NV8     NV8     NODE    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     0-31,64-95      0\r\nGPU1    NV8      X      NV8     NV8     NV8     NV8     NV8     NV8     NODE    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     0-31,64-95      0\r\nGPU2    NV8     NV8      X      NV8     NV8     NV8     NV8     NV8     NODE    NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     0-31,64-95      0\r\nGPU3    NV8     NV8     NV8      X      NV8     NV8     NV8     NV8     NODE    NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     0-31,64-95      0\r\nGPU4    NV8     NV8     NV8     NV8      X      NV8     NV8     NV8     SYS     SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    32-63,96-127    1\r\nGPU5    NV8     NV8     NV8     NV8     NV8      X      NV8     NV8     SYS     SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    32-63,96-127    1\r\nGPU6    NV8     NV8     NV8     NV8     NV8     NV8      X      NV8     SYS     SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     32-63,96-127    1\r\nGPU7    NV8     NV8     NV8     NV8     NV8     NV8     NV8      X      SYS     SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     32-63,96-127    1\r\nmlx5_0  NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS\r\nmlx5_1  PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      PIX     NODE    NODE    SYS     SYS     SYS     SYS\r\nmlx5_2  PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     NODE    PIX      X      NODE    NODE    SYS     SYS     SYS     SYS\r\nmlx5_3  NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      PIX     SYS     SYS     SYS     SYS\r\nmlx5_4  NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX      X      SYS     SYS     SYS     SYS\r\nmlx5_5  SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     SYS      X      PIX     NODE    NODE\r\nmlx5_6  SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     SYS     PIX      X      NODE    NODE\r\nmlx5_7  SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      PIX\r\nmlx5_8  SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     SYS     NODE    NODE    PIX      X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nulimit soft: 655350\r\n```\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-07-26T07:26:44+00:00",
    "closed_at": "2024-08-16T07:02:10+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/737/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/737"
  },
  {
    "number": 5972,
    "title": "[Feature] microsoft/Phi-4-multimodal-instruct",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n Model architectures ['Phi4MMForCausalLM'] are not supported for now. When can we expect support for this model. As Phi4-mini-instruct is already supported, this can be en extension to it.\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-02T10:25:30+00:00",
    "closed_at": "2025-05-25T18:47:51+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5972/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5972"
  },
  {
    "number": 5095,
    "title": "[Bug] KeyError when running nvidia/Llama-3.1-70B-Instruct-FP8",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHello, \n\nI am currently trying to run `nvidia/Llama-3.1-70B-Instruct-FP8` with SGLang and running into Key Error: `KeyError: 'model.layers.78.mlp.gate_up_proj.input_scale'\nERROR 2025-04-04T06:24:51.493646621Z param = params_dict[name]`. Would anyone have insights on resolving the error, or recommend a variant of Llama-3.1-70B FP8 quantized model for SGLang? \n\n\nError log\n```\n./entrypoint.sh: line 47: 28 Killed python3 -m sglang.launch_server ...\n--\n[2025-04-04 06:24:51] Received sigquit from a child process. It usually means the child failed.\n{}\nKeyError: 'model.layers.78.mlp.gate_up_proj.input_scale'\nparam = params_dict[name]\nFile \"/sgl-workspace/sglang/python/sglang/srt/models/llama.py\", line 503, in load_weights\nmodel.load_weights(self._get_all_weights(model_config, model))\nFile \"/sgl-workspace/sglang/python/sglang/srt/model_loader/loader.py\", line 371, in load_model\nreturn loader.load_model(\nFile \"/sgl-workspace/sglang/python/sglang/srt/model_loader/__init__.py\", line 22, in get_model\nself.model = get_model(\nFile \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 383, in load_model\nself.load_model()\nFile \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 178, in initialize\nself.initialize(min_per_gpu_memory)\nFile \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 168, in __init__\nself.model_runner = ModelRunner(\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 74, in __init__\nself.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\nself.tp_worker = TpWorkerClass(\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 227, in __init__\nscheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\nFile \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1809, in run_scheduler_process\n[2025-04-04 06:24:51 TP7] Scheduler hit an exception: Traceback (most recent call last):\n[2025-04-04 06:24:51 TP7] Parameter model.layers.78.mlp.down_proj.weight_scale not found in params_dict\n[2025-04-04 06:24:51 TP7] Parameter model.layers.78.mlp.down_proj.input_scale not found in params_dict\n[2025-04-04 06:22:34 TP6] Using model weights format ['*.safetensors']\n[2025-04-04 06:22:33 TP5] Using model weights format ['*.safetensors']\n[2025-04-04 06:22:33 TP0] Using model weights format ['*.safetensors']\n[2025-04-04 06:22:33 TP1] Using model weights format ['*.safetensors']\n[2025-04-04 06:22:33 TP2] Using model weights format ['*.safetensors']\n[2025-04-04 06:22:33 TP3] Using model weights format ['*.safetensors']\n[2025-04-04 06:22:33 TP4] Using model weights format ['*.safetensors']\n[2025-04-04 06:22:33 TP7] Using model weights format ['*.safetensors']\n[2025-04-04 06:22:32 TP1] Load weight begin. avail mem=76.63 GB\n[2025-04-04 06:22:32 TP6] Load weight begin. avail mem=76.63 GB\n[2025-04-04 06:22:32 TP1] Init torch distributed ends. mem usage=1.97 GB\n[2025-04-04 06:22:32 TP6] Init torch distributed ends. mem usage=1.97 GB\n[2025-04-04 06:22:32 TP5] Load weight begin. avail mem=76.63 GB\n[2025-04-04 06:22:32 TP4] Load weight begin. avail mem=76.63 GB\n[2025-04-04 06:22:32 TP3] Load weight begin. avail mem=76.63 GB\n[2025-04-04 06:22:32 TP0] Load weight begin. avail mem=76.72 GB\n[2025-04-04 06:22:32 TP7] Load weight begin. avail mem=77.09 GB\n[2025-04-04 06:22:32 TP2] Load weight begin. avail mem=76.63 GB\n[2025-04-04 06:22:32 TP3] Init torch distributed ends. mem usage=1.97 GB\n[2025-04-04 06:22:32 TP5] Init torch distributed ends. mem usage=1.97 GB\n[2025-04-04 06:22:32 TP7] Init torch distributed ends. mem usage=1.50 GB\n[2025-04-04 06:22:32 TP4] Init torch distributed ends. mem usage=1.97 GB\n[2025-04-04 06:22:32 TP2] Init torch distributed ends. mem usage=1.97 GB\n[2025-04-04 06:22:32 TP0] Init torch distributed ends. mem usage=1.87 GB\n[2025-04-04 06:22:19 TP6] sglang is using nccl==2.21.5\n[2025-04-04 06:22:19 TP7] sglang is using nccl==2.21.5\n[2025-04-04 06:22:19 TP5] sglang is using nccl==2.21.5\n```\n\nThank you!\n\n\n\n\n### Reproduction\n\nI am using SGLang server arguments: `--model=nvidia/Llama-3.1-70B-Instruct-FP8 --tp=8 --dp=1 --trust-remote-code --enable-torch-compile --enable-flashinfer-mla`. The error appears at the server start up.\n\n### Environment\n\nINFO 2025-04-06T03:02:45.843368291Z Python: 3.10.12 (main, Feb 4 2025, 14:57:36) [GCC 11.4.0]\n--\nINFO 2025-04-06T03:02:45.843415975Z CUDA available: True\nINFO 2025-04-06T03:02:45.843419551Z GPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nINFO 2025-04-06T03:02:45.843421697Z GPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nINFO 2025-04-06T03:02:45.843424320Z CUDA_HOME: /usr/local/cuda\nINFO 2025-04-06T03:02:45.843426227Z NVCC: Cuda compilation tools, release 12.4, V12.4.131\nINFO 2025-04-06T03:02:45.843428611Z CUDA Driver Version: 535.230.02\nINFO 2025-04-06T03:02:45.843430757Z PyTorch: 2.5.1+cu124\nINFO 2025-04-06T03:02:45.843432664Z sglang: 0.4.4.post3\nINFO 2025-04-06T03:02:45.843434810Z sgl_kernel: 0.0.7\nINFO 2025-04-06T03:02:45.843438386Z flashinfer: Module Not Found\nINFO 2025-04-06T03:02:45.843440294Z triton: 3.1.0\nINFO 2025-04-06T03:02:45.843442201Z transformers: 4.50.0\nINFO 2025-04-06T03:02:45.843443632Z torchao: 0.9.0\nINFO 2025-04-06T03:02:45.843445539Z numpy: 2.2.4\nINFO 2025-04-06T03:02:45.843447208Z aiohttp: 3.11.16\nINFO 2025-04-06T03:02:45.843448877Z fastapi: 0.115.12\nINFO 2025-04-06T03:02:45.843450546Z hf_transfer: 0.1.9\nINFO 2025-04-06T03:02:45.843452692Z huggingface_hub: 0.30.1\nINFO 2025-04-06T03:02:45.843454360Z interegular: 0.3.3\nINFO 2025-04-06T03:02:45.843456029Z modelscope: 1.24.1\nINFO 2025-04-06T03:02:45.843457698Z orjson: 3.10.16\nINFO 2025-04-06T03:02:45.843459367Z outlines: 0.1.11\nINFO 2025-04-06T03:02:45.843461275Z packaging: 24.2\nINFO 2025-04-06T03:02:45.843462944Z psutil: 7.0.0\nINFO 2025-04-06T03:02:45.843464612Z pydantic: 2.11.2\nINFO 2025-04-06T03:02:45.843466281Z multipart: Module Not Found\nINFO 2025-04-06T03:02:45.843468189Z zmq: Module Not Found\nINFO 2025-04-06T03:02:45.843469858Z uvicorn: 0.34.0\nINFO 2025-04-06T03:02:45.843471527Z uvloop: 0.21.0\nINFO 2025-04-06T03:02:45.843472957Z vllm: Module Not Found\nINFO 2025-04-06T03:02:45.843474864Z xgrammar: 0.1.17\nINFO 2025-04-06T03:02:45.843476533Z openai: 1.70.0\nINFO 2025-04-06T03:02:45.843478202Z tiktoken: 0.9.0\nINFO 2025-04-06T03:02:45.843479871Z anthropic: 0.49.0\nINFO 2025-04-06T03:02:45.843481779Z litellm: 1.65.3\nINFO 2025-04-06T03:02:45.843483448Z decord: 0.6.0\nINFO 2025-04-06T03:02:45.843485116Z NVIDIA Topology:\nINFO 2025-04-06T03:02:45.843487262Z \u001b[4mGPU0 GPU1 GPU2 GPU3 GPU4 GPU5 GPU6 GPU7 CPU Affinity NUMA Affinity GPU NUMA ID\u001b[0m\nINFO 2025-04-06T03:02:45.843489170Z GPU0 X NV18 NV18 NV18 NV18 NV18 NV18 NV18 0-51,104-155 0 N/A\nINFO 2025-04-06T03:02:45.843491315Z GPU1 NV18 X NV18 NV18 NV18 NV18 NV18 NV18 0-51,104-155 0 N/A\nINFO 2025-04-06T03:02:45.843493223Z GPU2 NV18 NV18 X NV18 NV18 NV18 NV18 NV18 0-51,104-155 0 N/A\nINFO 2025-04-06T03:02:45.843494892Z GPU3 NV18 NV18 NV18 X NV18 NV18 NV18 NV18 0-51,104-155 0 N/A\nINFO 2025-04-06T03:02:45.843507289Z GPU4 NV18 NV18 NV18 NV18 X NV18 NV18 NV18 52-103,156-207 1 N/A\nINFO 2025-04-06T03:02:45.843509197Z GPU5 NV18 NV18 NV18 NV18 NV18 X NV18 NV18 52-103,156-207 1 N/A\nINFO 2025-04-06T03:02:45.843510866Z GPU6 NV18 NV18 NV18 NV18 NV18 NV18 X NV18 52-103,156-207 1 N/A\nINFO 2025-04-06T03:02:45.843512773Z GPU7 NV18 NV18 NV18 NV18 NV18 NV18 NV18 X 52-103,156-207 1 N/A\nINFO 2025-04-06T03:02:45.843514919Z {}\nINFO 2025-04-06T03:02:45.843516588Z Legend:\nINFO 2025-04-06T03:02:45.843518018Z {}\nINFO 2025-04-06T03:02:45.843519926Z X = Self\nINFO 2025-04-06T03:02:45.843521595Z SYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\nINFO 2025-04-06T03:02:45.843524694Z NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\nINFO 2025-04-06T03:02:45.843526601Z PHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\nINFO 2025-04-06T03:02:45.843539714Z PXB = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\nINFO 2025-04-06T03:02:45.843541383Z PIX = Connection traversing at most a single PCIe bridge\nINFO 2025-04-06T03:02:45.843542814Z NV# = Connection traversing a bonded set of # NVLinks\nINFO 2025-04-06T03:02:45.843544244Z {}\nINFO 2025-04-06T03:02:45.843545675Z Hypervisor vendor: KVM\nINFO 2025-04-06T03:02:45.843546867Z ulimit soft: 1048576\n\u00a0",
    "labels": [
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-04-06T03:13:15+00:00",
    "closed_at": "2025-04-16T19:34:46+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5095/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5095"
  },
  {
    "number": 89,
    "title": "Customizing Sampling Behavior",
    "body": "Hi, is there an interface to specify logits processors as in vLLM? \r\n\r\nIf possible, could you specify how we can customize the sampling behavior during generation?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-24T00:06:48+00:00",
    "closed_at": "2024-02-06T18:25:19+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/89/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/89"
  },
  {
    "number": 1058,
    "title": "[Bug] T4 not work",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n\n### Describe the bug\n\nT4 not work w/o FlashInfer ref https://github.com/flashinfer-ai/flashinfer/issues/421\r\n\r\n```\r\nCUDA Error: no kernel image is available for execution on the device (209) /tmp/build-via-sdist-iemil769/flashinfer-0.1.4+cu121torch2.4/include/flashinfer/attention/handler.cuh: line 169 at function cudaOccupancyMaxActiveBlocksPerMultiprocessor(&num_blocks_per_sm, kernel, num_threads, smem_size)\r\nCUDA Error: no kernel image is available for execution on the device (209) /tmp/build-via-sdist-iemil769/flashinfer-0.1.4+cu121torch2.4/include/flashinfer/attention/handler.cuh: line 324 at function work_estimation_func(split_kv, max_grid_size, max_num_pages_per_batch, new_batch_size, batch_size, indptr_h, num_qo_heads, page_size, IsCUDAGraphEnabled(), stream_)\r\nProcess Process-1:\r\nInitialization failed. controller_init_state: Traceback (most recent call last):\r\n  File \"/content/sglang/python/sglang/srt/model_executor/model_runner.py\", line 344, in init_cuda_graphs\r\n    self.cuda_graph_runner.capture(batch_size_list)\r\n  File \"/content/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 148, in capture\r\n    ) = self.capture_one_batch_size(bs, forward)\r\n  File \"/content/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 183, in capture_one_batch_size\r\n    update_flashinfer_indices(\r\n  File \"/content/sglang/python/sglang/srt/model_executor/forward_batch_info.py\", line 284, in update_flashinfer_indices\r\n    flashinfer_decode_wrapper.begin_forward(\r\n  File \"/usr/local/lib/python3.10/dist-packages/flashinfer/decode.py\", line 525, in begin_forward\r\n    self._wrapper.begin_forward(\r\nRuntimeError: BatchDecodeWithPagedKVCache failed with error no kernel image is available for execution on the device\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/content/sglang/python/sglang/srt/managers/controller_single.py\", line 150, in start_controller_process\r\n    controller = ControllerSingle(\r\n  File \"/content/sglang/python/sglang/srt/managers/controller_single.py\", line 84, in __init__\r\n    self.tp_server = ModelTpServer(\r\n  File \"/content/sglang/python/sglang/srt/managers/tp_worker.py\", line 100, in __init__\r\n    self.model_runner = ModelRunner(\r\n  File \"/content/sglang/python/sglang/srt/model_executor/model_runner.py\", line 139, in __init__\r\n    self.init_cuda_graphs()\r\n  File \"/content/sglang/python/sglang/srt/model_executor/model_runner.py\", line 346, in init_cuda_graphs\r\n    raise Exception(\r\nException: Capture cuda graph failed: BatchDecodeWithPagedKVCache failed with error no kernel image is available for execution on the device\r\nPossible solutions:\r\n1. disable torch compile by not using --enable-torch-compile\r\n2. disable cuda graph by --disable-cuda-graph\r\n3. set --mem-fraction-static to a smaller value\r\nOpen an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose \r\n\r\n\r\nInitialization failed. detoken_init_state: init ok\r\n```\r\n\r\n```\r\nINFO:     127.0.0.1:59800 - \"GET /get_model_info HTTP/1.1\" 200 OK\r\n[gpu=0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, cache hit rate: 0.00%, #running-req: 0, #queue-req: 0\r\nException in ModelTpServer:\r\nTraceback (most recent call last):\r\n  File \"/content/sglang/python/sglang/srt/managers/tp_worker.py\", line 222, in exposed_step\r\n    self.forward_step()\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/content/sglang/python/sglang/srt/managers/tp_worker.py\", line 238, in forward_step\r\n    self.forward_prefill_batch(new_batch)\r\n  File \"/content/sglang/python/sglang/srt/managers/tp_worker.py\", line 452, in forward_prefill_batch\r\n    output = self.model_runner.forward(batch, ForwardMode.EXTEND)\r\n  File \"/content/sglang/python/sglang/srt/model_executor/model_runner.py\", line 397, in forward\r\n    return self.forward_extend(batch)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/content/sglang/python/sglang/srt/model_executor/model_runner.py\", line 373, in forward_extend\r\n    return self.model.forward(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/content/sglang/python/sglang/srt/models/qwen2.py\", line 287, in forward\r\n    hidden_states = self.model(input_ids, positions, input_metadata, input_embeds)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/content/sglang/python/sglang/srt/models/qwen2.py\", line 255, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/content/sglang/python/sglang/srt/models/qwen2.py\", line 207, in forward\r\n    hidden_states = self.self_attn(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/content/sglang/python/sglang/srt/models/qwen2.py\", line 156, in forward\r\n    attn_output = self.attn(q, k, v, input_metadata)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/content/sglang/python/sglang/srt/layers/radix_attention.py\", line 177, in forward\r\n    return self.extend_forward(q, k, v, input_metadata)\r\n  File \"/content/sglang/python/sglang/srt/layers/radix_attention.py\", line 69, in extend_forward_triton\r\n    extend_attention_fwd(\r\n  File \"/content/sglang/python/sglang/srt/layers/extend_attention.py\", line 291, in extend_attention_fwd\r\n    _fwd_kernel[grid](\r\n  File \"/usr/local/lib/python3.10/dist-packages/triton/runtime/jit.py\", line 345, in <lambda>\r\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/triton/runtime/jit.py\", line 662, in run\r\n    kernel = self.compile(\r\n  File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 282, in compile\r\n    next_module = compile_ir(module, metadata)\r\n  File \"/usr/local/lib/python3.10/dist-packages/triton/backends/nvidia/compiler.py\", line 318, in <lambda>\r\n    stages[\"llir\"] = lambda src, metadata: self.make_llir(src, metadata, options, self.capability)\r\n  File \"/usr/local/lib/python3.10/dist-packages/triton/backends/nvidia/compiler.py\", line 216, in make_llir\r\n    pm.run(mod)\r\nIndexError: map::at\r\n\r\nException in ControllerSingle:\r\nTraceback (most recent call last):\r\n  File \"/content/sglang/python/sglang/srt/managers/controller_single.py\", line 166, in start_controller_process\r\n    controller.loop_for_forward()\r\n  File \"/content/sglang/python/sglang/srt/managers/controller_single.py\", line 103, in loop_for_forward\r\n    out_pyobjs = self.tp_server.exposed_step(recv_reqs)\r\n  File \"/content/sglang/python/sglang/srt/managers/tp_worker.py\", line 222, in exposed_step\r\n    self.forward_step()\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/content/sglang/python/sglang/srt/managers/tp_worker.py\", line 238, in forward_step\r\n    self.forward_prefill_batch(new_batch)\r\n  File \"/content/sglang/python/sglang/srt/managers/tp_worker.py\", line 452, in forward_prefill_batch\r\n    output = self.model_runner.forward(batch, ForwardMode.EXTEND)\r\n  File \"/content/sglang/python/sglang/srt/model_executor/model_runner.py\", line 397, in forward\r\n    return self.forward_extend(batch)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/content/sglang/python/sglang/srt/model_executor/model_runner.py\", line 373, in forward_extend\r\n    return self.model.forward(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/content/sglang/python/sglang/srt/models/qwen2.py\", line 287, in forward\r\n    hidden_states = self.model(input_ids, positions, input_metadata, input_embeds)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/content/sglang/python/sglang/srt/models/qwen2.py\", line 255, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/content/sglang/python/sglang/srt/models/qwen2.py\", line 207, in forward\r\n    hidden_states = self.self_attn(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/content/sglang/python/sglang/srt/models/qwen2.py\", line 156, in forward\r\n    attn_output = self.attn(q, k, v, input_metadata)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/content/sglang/python/sglang/srt/layers/radix_attention.py\", line 177, in forward\r\n    return self.extend_forward(q, k, v, input_metadata)\r\n  File \"/content/sglang/python/sglang/srt/layers/radix_attention.py\", line 69, in extend_forward_triton\r\n    extend_attention_fwd(\r\n  File \"/content/sglang/python/sglang/srt/layers/extend_attention.py\", line 291, in extend_attention_fwd\r\n    _fwd_kernel[grid](\r\n  File \"/usr/local/lib/python3.10/dist-packages/triton/runtime/jit.py\", line 345, in <lambda>\r\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/triton/runtime/jit.py\", line 662, in run\r\n    kernel = self.compile(\r\n  File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 282, in compile\r\n    next_module = compile_ir(module, metadata)\r\n  File \"/usr/local/lib/python3.10/dist-packages/triton/backends/nvidia/compiler.py\", line 318, in <lambda>\r\n    stages[\"llir\"] = lambda src, metadata: self.make_llir(src, metadata, options, self.capability)\r\n  File \"/usr/local/lib/python3.10/dist-packages/triton/backends/nvidia/compiler.py\", line 216, in make_llir\r\n    pm.run(mod)\r\nIndexError: map::at\r\n\r\n[rank0]:W0812 14:38:42.916000 137655740245568 torch/_inductor/compile_worker/subproc_pool.py:126] SubprocPool unclean exit\r\n```\n\n### Reproduction\n\n```\r\n# Use the last release branch\r\ngit clone -b v0.2.12 https://github.com/sgl-project/sglang.git\r\ncd sglang\r\n\r\npip install --upgrade pip\r\npip install -e \"python[all]\"\r\n\r\n# Install FlashInfer CUDA kernels\r\npip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/\r\n\r\npython3 -m sglang.launch_server --model Qwen/Qwen1.5-4B-Chat\r\n\r\npython3 -m sglang.launch_server --model Qwen/Qwen1.5-4B-Chat --disable-flashinfer --disable-flashinfer-sampling\r\n```\n\n### Environment\n\n```\r\nPython: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0]\r\nCUDA available: True\r\nGPU 0: Tesla T4\r\nGPU 0 Compute Capability: 7.5\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.2, V12.2.140\r\nCUDA Driver Version: 535.104.05\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.2.12\r\nflashinfer: 0.1.4+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.44.0\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.26.4\r\naiohttp: 3.10.1\r\nfastapi: 0.112.0\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.23.5\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 9.4.0\r\npsutil: 5.9.5\r\npydantic: 2.8.2\r\nuvicorn: 0.30.5\r\nuvloop: 0.19.0\r\nzmq: 24.0.1\r\nvllm: 0.5.4\r\nmultipart: 0.0.9\r\nopenai: 1.40.3\r\nanthropic: 0.33.0\r\nNVIDIA Topology: \r\n\tGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \t0-1\t\tN/A\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nulimit soft: 1048576\r\n```",
    "labels": [
      "bug",
      "flashinfer"
    ],
    "state": "closed",
    "created_at": "2024-08-12T14:46:18+00:00",
    "closed_at": "2024-08-28T13:05:35+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1058/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/1058"
  },
  {
    "number": 7028,
    "title": "[Bug] sgl-router enters infinite panic loop when all workers die under active load",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nBackground: DP Attention is not stable yet. During high concurrency, illegal memory access is more likely to occur, leading to crashes. Therefore, when one worker fails, there is a certain probability it will trigger a cascading failure that brings down all workers.\n\nTo deal with worker failure, I have a sidecar container infinite looping to detect live sglang endpoint and register them to the router.\n\nBut I noticed if (all workers died && clients are still sending requests), sgl-router will stuck into a inifinite loop of panic even if live workers are registered back:\n\n```\nthread 'actix-rt|system:0|arbiter:12' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:13' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:14' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:1' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:15' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:0' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:3' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:2' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:5' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\nthread 'actix-rt|system:0|arbiter:4' panicked at src/router.rs:463:40:\ncalled `Result::unwrap()` on an `Err` value: PoisonError { .. }\n```\n\nThis kind of panic log kept for hours before I noticed it:\n\n<img width=\"1835\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/acae5ce0-0b42-45ef-bf85-838848ceada1\" />\n\nWhile worker registeration is working:\n\n<img width=\"816\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fbf906d9-5880-40ff-b8b0-0bf8b3ee1084\" />\n\n\nAfter I stopped all clients, the router stopped logging infinite panic:\n\n<img width=\"860\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f135c326-05bf-408c-9c27-70a91778293b\" />\n\nBut still not able to serve requests. If I resume the clients, the router goes to infinite panic again, the only way out I found is kill the router and restart one.\n\n### Reproduction\n\nRemoving all workers while the client keeps sending requests should reproduce the issue.\n\nI understand the router panics when no workers are available is a known issue, but I believe it's unexpected that the router fails to handle requests properly after live workers are added back. This appears to be a multi-threaded race condition issue, but I'm not familiar with Rust programming so I don't know what caused this problem.\n\n### Environment\n\n```\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post5\nsgl_kernel: 0.1.4\nsglang-router: 0.1.4\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-10T05:34:36+00:00",
    "closed_at": "2025-06-26T05:58:47+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7028/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7028"
  },
  {
    "number": 4652,
    "title": "[Feature] make page size greater than one compatible with EAGLE",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-03-21T10:06:33+00:00",
    "closed_at": "2025-04-16T06:43:14+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4652/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/4652"
  },
  {
    "number": 1109,
    "title": "[Bug] Failure to Dispatch Head Dimension 80 in sglang with Specific Configurations",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n## Issue Description:\r\nWhen running sglang with hidden_dim set to 80, the following exceptions are encountered under different configurations:\r\n\r\n### With enable_cuda_graph set to True:\r\n```bash\r\nException: Capture cuda graph failed: BatchDecodeWithPagedKVCachePyTorchWrapper::BeginForward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, float, at::Tensor, at::Tensor)::<lambda()>::<lambda()>::<lambda()> failed to dispatch head_dim 80\r\n```\r\n\r\n<details> \r\n<summary>The complete stack information is as follows:</summary>\r\n\r\n```bash\r\npython -m sglang.launch_server --model-path /mnt/llm_dataset/willhe/ckpt/XVERSE-MoE-A4.2B-Chat --trust-remote-code \r\nserver_args=ServerArgs(model_path='/mnt/llm_dataset/willhe/ckpt/XVERSE-MoE-A4.2B-Chat', tokenizer_path='/mnt/llm_dataset/willhe/ckpt/XVERSE-MoE-A4.2B-Chat', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', dtype='auto', trust_remote_code=True, context_length=None, quantization=None, served_model_name='/mnt/llm_dataset/willhe/ckpt/XVERSE-MoE-A4.2B-Chat', chat_template=None, host='127.0.0.1', port=30000, additional_ports=[30001, 30002, 30003, 30004], mem_fraction_static=0.88, max_running_requests=None, max_num_reqs=None, max_total_tokens=None, chunked_prefill_size=None, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, tp_size=1, stream_interval=1, random_seed=334330058, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, api_key=None, file_storage_pth='SGLang_storage', dp_size=1, load_balance_method='round_robin', disable_flashinfer=False, disable_flashinfer_sampling=False, disable_radix_cache=False, disable_regex_jump_forward=False, disable_cuda_graph=False, disable_disk_cache=False, enable_torch_compile=False, enable_p2p_check=False, enable_mla=False, attention_reduce_in_fp32=False, efficient_weight_load=False, nccl_init_addr=None, nnodes=1, node_rank=None)\r\n[gpu=0] Init nccl begin.\r\n[gpu=0] Load weight begin. avail mem=78.94 GB\r\nLoading pt checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]\r\n...\r\nLoading pt checkpoint shards: 100% Completed | 9/9 [00:21<00:00,  2.35s/it]\r\n\r\n[gpu=0] Load weight end. type=XverseMoEForCausalLM, dtype=torch.bfloat16, avail mem=30.65 GB\r\n[gpu=0] Memory pool end. avail mem=9.28 GB\r\n[gpu=0] Capture cuda graph begin. This can take up to several minutes.\r\nInitialization failed. controller_init_state: Traceback (most recent call last):\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 344, in init_cuda_graphs\r\n    self.cuda_graph_runner.capture(batch_size_list)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_executor/cuda_graph_runner.py\", line 148, in capture\r\n    ) = self.capture_one_batch_size(bs, forward)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_executor/cuda_graph_runner.py\", line 183, in capture_one_batch_size\r\n    update_flashinfer_indices(\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_executor/forward_batch_info.py\", line 284, in update_flashinfer_indices\r\n    flashinfer_decode_wrapper.begin_forward(\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/flashinfer/decode.py\", line 539, in begin_forward\r\n    self._wrapper.begin_forward(\r\nRuntimeError: BatchDecodeWithPagedKVCachePyTorchWrapper::BeginForward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, float, at::Tensor, at::Tensor)::<lambda()>::<lambda()>::<lambda()> failed to dispatch head_dim 80\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/controller_single.py\", line 150, in start_controller_process\r\n    controller = ControllerSingle(\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/controller_single.py\", line 84, in __init__\r\n    self.tp_server = ModelTpServer(\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 100, in __init__\r\n    self.model_runner = ModelRunner(\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 139, in __init__\r\n    self.init_cuda_graphs()\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 346, in init_cuda_graphs\r\n    raise Exception(\r\nException: Capture cuda graph failed: BatchDecodeWithPagedKVCachePyTorchWrapper::BeginForward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, float, at::Tensor, at::Tensor)::<lambda()>::<lambda()>::<lambda()> failed to dispatch head_dim 80\r\nPossible solutions:\r\n1. disable torch compile by not using --enable-torch-compile\r\n2. disable cuda graph by --disable-cuda-graph\r\n3. set --mem-fraction-static to a smaller value\r\n```\r\n</details>\r\n\r\n### With disable_cuda_graph set to True:\r\n```bash\r\nRuntimeError: BatchPrefillWithPagedKVCachePyTorchWrapper::Forward(at::Tensor, at::Tensor, std::optional<at::Tensor>, std::optional<at::Tensor>, std::optional<at::Tensor>, at::Tensor, at::Tensor, at::Tensor, bool, unsigned int, bool, int, float, float, float, float, bool)::<lambda()>::<lambda()>::<lambda()>::<lambda()>::<lambda()> failed to dispatch head_dim 80\r\n```\r\n\r\n<details>\r\n  <summary>The complete stack information is as follows: </summary>\r\n\r\n```bash\r\npython -m sglang.launch_server --model-path /mnt/llm_dataset/willhe/ckpt/XVERSE-MoE-A4.2B-Chat --trust-remote-code --disable-cuda-graph\r\nserver_args=ServerArgs(model_path='/mnt/llm_dataset/willhe/ckpt/XVERSE-MoE-A4.2B-Chat', tokenizer_path='/mnt/llm_dataset/willhe/ckpt/XVERSE-MoE-A4.2B-Chat', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', dtype='auto', trust_remote_code=True, context_length=None, quantization=None, served_model_name='/mnt/llm_dataset/willhe/ckpt/XVERSE-MoE-A4.2B-Chat', chat_template=None, host='127.0.0.1', port=30000, additional_ports=[30001, 30002, 30003, 30004], mem_fraction_static=0.88, max_running_requests=None, max_num_reqs=None, max_total_tokens=None, chunked_prefill_size=None, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, tp_size=1, stream_interval=1, random_seed=978056517, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, api_key=None, file_storage_pth='SGLang_storage', dp_size=1, load_balance_method='round_robin', disable_flashinfer=False, disable_flashinfer_sampling=False, disable_radix_cache=False, disable_regex_jump_forward=False, disable_cuda_graph=True, disable_disk_cache=False, enable_torch_compile=False, enable_p2p_check=False, enable_mla=False, attention_reduce_in_fp32=False, efficient_weight_load=False, nccl_init_addr=None, nnodes=1, node_rank=None)\r\n[gpu=0] Init nccl begin.\r\n[gpu=0] Load weight begin. avail mem=78.94 GB\r\nLoading pt checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]\r\n...\r\nLoading pt checkpoint shards: 100% Completed | 9/9 [00:17<00:00,  1.90s/it]\r\n\r\n[gpu=0] Load weight end. type=XverseMoEForCausalLM, dtype=torch.bfloat16, avail mem=30.65 GB\r\n[gpu=0] Memory pool end. avail mem=9.28 GB\r\n[gpu=0] max_total_num_tokens=79301, max_prefill_tokens=16384, max_running_requests=4955, context_len=8192\r\nINFO:     Started server process [54724]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)\r\nINFO:     127.0.0.1:53136 - \"GET /get_model_info HTTP/1.1\" 200 OK\r\n[gpu=0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 0, cache hit rate: 0.00%, #running-req: 0, #queue-req: 0\r\nException in ModelTpServer:\r\nTraceback (most recent call last):\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 222, in exposed_step\r\n    self.forward_step()\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 238, in forward_step\r\n    self.forward_prefill_batch(new_batch)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 452, in forward_prefill_batch\r\n    output = self.model_runner.forward(batch, ForwardMode.EXTEND)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 397, in forward\r\n    return self.forward_extend(batch)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 373, in forward_extend\r\n    return self.model.forward(\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/models/xverse_moe.py\", line 389, in forward\r\n    hidden_states = self.model(input_ids, positions, input_metadata)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/models/xverse_moe.py\", line 358, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/models/xverse_moe.py\", line 308, in forward\r\n    hidden_states = self.self_attn(\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/models/xverse_moe.py\", line 251, in forward\r\n    **attn_output = self.attn(q, k, v, input_metadata)**\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/layers/radix_attention.py\", line 177, in forward\r\n    return self.extend_forward(q, k, v, input_metadata)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/layers/radix_attention.py\", line 119, in extend_forward_flashinfer\r\n    o = input_metadata.flashinfer_prefill_wrapper_paged.forward(\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/flashinfer/prefill.py\", line 914, in forward\r\n    out = self._wrapper.forward(\r\nRuntimeError: BatchPrefillWithPagedKVCachePyTorchWrapper::Forward(at::Tensor, at::Tensor, std::optional<at::Tensor>, std::optional<at::Tensor>, std::optional<at::Tensor>, at::Tensor, at::Tensor, at::Tensor, bool, unsigned int, bool, int, float, float, float, float, bool)::<lambda()>::<lambda()>::<lambda()>::<lambda()>::<lambda()> failed to dispatch head_dim 80\r\n```\r\n\r\n</details>\r\n\r\n### With disable_flashinfer set to True:\r\n```bash\r\nAssertionError: assert Lq in {16, 32, 64, 128, 256, 576}, where Lq = 80.\r\n```\r\n<details>\r\n  <summary>The complete stack information is as follows:</summary>\r\n  \r\n```bash\r\npython -m sglang.launch_server --model-path /mnt/llm_dataset/willhe/ckpt/XVERSE-MoE-A4.2B-Chat --trust-remote-code --disable-flashinfer\r\nserver_args=ServerArgs(model_path='/mnt/llm_dataset/willhe/ckpt/XVERSE-MoE-A4.2B-Chat', tokenizer_path='/mnt/llm_dataset/willhe/ckpt/XVERSE-MoE-A4.2B-Chat', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', dtype='auto', trust_remote_code=True, context_length=None, quantization=None, served_model_name='/mnt/llm_dataset/willhe/ckpt/XVERSE-MoE-A4.2B-Chat', chat_template=None, host='127.0.0.1', port=30000, additional_ports=[30001, 30002, 30003, 30004], mem_fraction_static=0.88, max_running_requests=None, max_num_reqs=None, max_total_tokens=None, chunked_prefill_size=None, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, tp_size=1, stream_interval=1, random_seed=8981036, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, api_key=None, file_storage_pth='SGLang_storage', dp_size=1, load_balance_method='round_robin', disable_flashinfer=True, disable_flashinfer_sampling=False, disable_radix_cache=False, disable_regex_jump_forward=False, disable_cuda_graph=False, disable_disk_cache=False, enable_torch_compile=False, enable_p2p_check=False, enable_mla=False, attention_reduce_in_fp32=False, efficient_weight_load=False, nccl_init_addr=None, nnodes=1, node_rank=None)\r\n[gpu=0] Init nccl begin.\r\n[gpu=0] Load weight begin. avail mem=78.94 GB\r\nLoading pt checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]\r\n...\r\nLoading pt checkpoint shards: 100% Completed | 9/9 [00:17<00:00,  1.92s/it]\r\n\r\n[gpu=0] Load weight end. type=XverseMoEForCausalLM, dtype=torch.bfloat16, avail mem=30.65 GB\r\n[gpu=0] Memory pool end. avail mem=9.28 GB\r\n[gpu=0] max_total_num_tokens=79301, max_prefill_tokens=16384, max_running_requests=4955, context_len=8192\r\nINFO:     Started server process [55058]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)\r\nINFO:     127.0.0.1:60976 - \"GET /get_model_info HTTP/1.1\" 200 OK\r\n[gpu=0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 0, cache hit rate: 0.00%, #running-req: 0, #queue-req: 0\r\nException in ModelTpServer:\r\nTraceback (most recent call last):\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 222, in exposed_step\r\n    self.forward_step()\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 238, in forward_step\r\n    self.forward_prefill_batch(new_batch)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 452, in forward_prefill_batch\r\n    output = self.model_runner.forward(batch, ForwardMode.EXTEND)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 397, in forward\r\n    return self.forward_extend(batch)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 373, in forward_extend\r\n    return self.model.forward(\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/models/xverse_moe.py\", line 389, in forward\r\n    hidden_states = self.model(input_ids, positions, input_metadata)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/models/xverse_moe.py\", line 358, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/models/xverse_moe.py\", line 308, in forward\r\n    hidden_states = self.self_attn(\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/models/xverse_moe.py\", line 251, in forward\r\n    attn_output = self.attn(q, k, v, input_metadata)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/layers/radix_attention.py\", line 177, in forward\r\n    return self.extend_forward(q, k, v, input_metadata)\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/layers/radix_attention.py\", line 69, in extend_forward_triton\r\n    extend_attention_fwd(\r\n  File \"/mnt/llm_dataset/willhe/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/layers/extend_attention.py\", line 267, in extend_attention_fwd\r\n    assert Lq in {16, 32, 64, 128, 256, 576}\r\nAssertionError\r\n```\r\n\r\n</details>\n\n### Reproduction\n\n## run command\r\n```bash\r\npython -m sglang.launch_server --model-path xverse/XVERSE-MoE-A4.2B-Chat --trust-remote-code --disable-flashinfer\r\n```\r\n\r\n## model\r\nxverse/XVERSE-MoE-A4.2B-Chat\r\n\r\n## Steps to Reproduce:\r\nI am currently supporting XVERSE models in a local branch and encountered an issue where the server failed to launch.  Although the issue **cannot be** replicated directly, possible solutions may be inferred from the error message and the configuration file.\r\n\r\nBelow is the config.json file, where the `hidden_size/num_attention_heads = 2560/32= 80`.\r\n```json\r\n{\r\n  \"architectures\": [\r\n    \"XverseMoEForCausalLM\"\r\n  ],\r\n  \"auto_map\": {\r\n    \"AutoConfig\": \"configuration_xverse.XverseConfig\",\r\n    \"AutoModelForCausalLM\": \"modeling_xverse.XverseForCausalLM\"\r\n  },\r\n  \"pad_token_id\": 1,\r\n  \"bos_token_id\": 2,\r\n  \"eos_token_id\": 3,\r\n  \"hidden_act\": \"silu\",\r\n  \"hidden_size\": 2560,\r\n  \"initializer_range\": 0.02,\r\n  \"intermediate_size\": 1728,\r\n  \"max_position_embeddings\": 8192,\r\n  \"max_tokenizer_truncation\": 6144,\r\n  \"model_type\": \"xverse\",\r\n  \"num_attention_heads\": 32,\r\n  \"num_hidden_layers\": 28,\r\n  \"rms_norm_eps\": 1e-06,\r\n  \"tie_word_embeddings\": false,\r\n  \"rope_theta\": 500000,\r\n  \"moe_top_k\": 6,\r\n  \"num_experts\": 64,\r\n  \"num_shared_experts\": 2,\r\n  \"output_router_logits\": false,\r\n  \"router_aux_loss_coef\": 0.01,\r\n  \"torch_dtype\": \"bfloat16\",\r\n  \"transformers_version\": \"4.38.2\",\r\n  \"use_cache\": true,\r\n  \"vocab_size\": 100534,\r\n  \"_attn_implementation\": \"eager\"\r\n}\r\n```\n\n### Environment\n\n```bash\r\nPython: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA A800-SXM4-80GB\r\nGPU 0 Compute Capability: 8.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\r\nCUDA Driver Version: 470.182.03\r\nPyTorch: 2.3.1+cu121\r\nsglang: 0.2.12\r\nflashinfer: 0.1.5+cu121torch2.3\r\ntriton: 2.3.1\r\ntransformers: 4.43.3\r\nrequests: 2.32.3\r\ntqdm: 4.66.4\r\nnumpy: 1.26.4\r\naiohttp: 3.9.5\r\nfastapi: 0.111.1\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.2\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.3\r\nuvloop: 0.19.0\r\nzmq: 26.0.3\r\nvllm: 0.5.3.post1\r\nmultipart: 0.0.9\r\nopenai: 1.37.1\r\nanthropic: 0.31.2\r\nNVIDIA Topology: \r\n        GPU0    CPU Affinity    NUMA Affinity\r\nGPU0     X      116-231 1\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nulimit soft: 1048576\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-15T08:05:19+00:00",
    "closed_at": "2024-09-11T17:35:32+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1109/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1109"
  },
  {
    "number": 532,
    "title": "ImportError: cannot import name 'pin_program'",
    "body": "**Description**:\r\nWhen running the 'test_multi_function' test in test_tracing.py, an ImportError occurs, indicating that the name 'pin_program' cannot be imported from the sglang.lang.interpreter module. It appears that 'pin_program' should be changed to 'cache_program'.\r\n\r\n**Suggested Fix**:\r\nChange 'pin_program' to 'cache_program' in the sglang.lang.interpreter module.\r\n<img width=\"572\" alt=\"8adb262c7acec342962c96fd9bd1d91\" src=\"https://github.com/sgl-project/sglang/assets/157339885/f3874cc1-6670-4db9-8341-3d2b4f051090\">\r\n\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-06-11T22:07:22+00:00",
    "closed_at": "2024-06-30T06:43:18+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/532/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/532"
  },
  {
    "number": 1343,
    "title": "[Bug] it didn't work when using tp on RTX 3090",
    "body": "### Checklist\r\n\r\n- [x] 1. I have searched related issues but cannot get the expected help.\r\n- [x] 2. The bug has not been fixed in the latest version.\r\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [x] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/dfs/data/github/sglang/python/sglang/launch_server.py\", line 19, in <module>\r\n    raise e\r\n  File \"/dfs/data/github/sglang/python/sglang/launch_server.py\", line 17, in <module>\r\n    launch_server(server_args)\r\n  File \"/dfs/data/github/sglang/python/sglang/srt/server.py\", line 365, in launch_server\r\n    raise RuntimeError(\r\nRuntimeError: Initialization failed. controller_init_state: Traceback (most recent call last):\r\n  File \"/dfs/data/github/sglang/python/sglang/srt/managers/controller_single.py\", line 149, in start_controller_process\r\n    controller = ControllerSingle(\r\n  File \"/dfs/data/github/sglang/python/sglang/srt/managers/controller_single.py\", line 83, in __init__\r\n    self.tp_server = ModelTpServer(\r\n  File \"/dfs/data/github/sglang/python/sglang/srt/managers/tp_worker.py\", line 99, in __init__\r\n    self.model_runner = ModelRunner(\r\n  File \"/dfs/data/github/sglang/python/sglang/srt/model_executor/model_runner.py\", line 109, in __init__\r\n    min_per_gpu_memory = self.init_torch_distributed()\r\n  File \"/dfs/data/github/sglang/python/sglang/srt/model_executor/model_runner.py\", line 140, in init_torch_distributed\r\n    initialize_model_parallel(tensor_model_parallel_size=self.tp_size)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/parallel_state.py\", line 931, in initialize_model_parallel\r\n    _TP = init_model_parallel_group(group_ranks,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/parallel_state.py\", line 773, in init_model_parallel_group\r\n    return GroupCoordinator(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/parallel_state.py\", line 164, in __init__\r\n    self.ca_comm = CustomAllreduce(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/device_communicators/custom_all_reduce.py\", line 130, in __init__\r\n    if not _can_p2p(rank, world_size):\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/device_communicators/custom_all_reduce.py\", line 31, in _can_p2p\r\n    if not gpu_p2p_access_check(rank, i):\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\", line 227, in gpu_p2p_access_check\r\n    result = pickle.loads(returned.stdout)\r\n_pickle.UnpicklingError: invalid load key, 'W'.\r\n, detoken_init_state: init ok\r\n\r\n### Reproduction\r\n\r\npython3 -m sglang.launch_server --model-path ../llama3/safesensor_models/ --port 30000 --trust-remote-code --disable-radix-cache --tp 2 --enable-p2p-check\r\n\r\n### Environment\r\n\r\nWARNING 09-06 09:46:19 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead. See https://pypi.org/project/pynvml for more information.\r\nPython: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\r\nCUDA available: True\r\nGPU 0,1: NVIDIA GeForce RTX 3090\r\nGPU 0,1 Compute Capability: 8.6\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.3, V12.3.107\r\nCUDA Driver Version: 525.116.04\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.3.0\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.44.2\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.24.4\r\naiohttp: 3.9.1\r\nfastapi: 0.112.3\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.6\r\ninteregular: 0.3.3\r\npackaging: 23.2\r\nPIL: 10.2.0\r\npsutil: 5.9.4\r\npydantic: 2.8.2\r\nuvicorn: 0.30.6\r\nuvloop: 0.20.0\r\nzmq: 25.1.2\r\nvllm: 0.5.5\r\nmultipart: 0.0.9\r\nopenai: 1.43.0\r\nanthropic: 0.34.2\r\nNVIDIA Topology: \r\n        GPU0    GPU1    NIC0    CPU Affinity    NUMA Affinity\r\nGPU0     X      SYS     SYS     6-11,54-59      1\r\nGPU1    SYS      X      SYS     36-41,84-89     6\r\nNIC0    SYS     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n\r\n\r\nulimit soft: 1048576",
    "labels": [],
    "state": "closed",
    "created_at": "2024-09-06T09:47:12+00:00",
    "closed_at": "2024-09-22T12:05:49+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1343/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1343"
  },
  {
    "number": 6749,
    "title": "[Feature] Logit Bias Not Working",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nWe're finding that the logit bias isn't working as expected, which we are using for our use cases.\n\nDoes SGL support this?\n\n### Related resources\n\nhttps://help.openai.com/en/articles/5247780-using-logit-bias-to-alter-token-probability-with-the-openai-api",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-29T20:18:34+00:00",
    "closed_at": "2025-06-10T22:39:27+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6749/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6749"
  },
  {
    "number": 2778,
    "title": "[Bug] Failed to create router: Timeout 300s waiting for workers to become healthy",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nAny time I try some slower to start options with the new router it times out at 300 sec - e.g. try `--enable-torch-compile`, which can easily take 10-15min to start with all its tune attempts.\r\n\r\nHow can this timeout be overridden to be made higher by the user when needed?\n\n### Reproduction\n\npython -m sglang_router.launch_server --enable-torch-compile  --model-path deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct \n\n### Environment\n\nPython: 3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:16:10) [GCC 13.3.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\r\nCUDA_HOME: /usr/local/cuda-12.6\r\nNVCC: Cuda compilation tools, release 12.6, V12.6.85\r\nCUDA Driver Version: 560.35.03\r\nPyTorch: 2.5.1+cu124\r\nsglang: 0.4.1.post3\r\nflashinfer: 0.1.6+cu124torch2.4\r\ntriton: 3.1.0\r\ntransformers: 4.47.1\r\ntorchao: 0.7.0\r\nnumpy: 1.26.4\r\naiohttp: 3.11.11\r\nfastapi: 0.115.6\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.27.0\r\ninteregular: 0.3.3\r\nmodelscope: 1.21.1\r\norjson: 3.10.13\r\npackaging: 24.2\r\npsutil: 6.1.1\r\npydantic: 2.10.4\r\nmultipart: 0.0.20\r\nzmq: 26.2.0\r\nuvicorn: 0.34.0\r\nuvloop: 0.21.0\r\nvllm: 0.6.4.post1\r\nopenai: 1.59.3\r\nanthropic: 0.42.0\r\ndecord: 0.6.0\r\nNVIDIA Topology:\r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8     NIC9    NIC10   NIC11   CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     PIX     PIX     SYS     SYS     SYS     SYS     SYS     SYS      SYS     SYS     SYS     0-12,104-116    0               N/A\r\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS      SYS     SYS     SYS     26-38,130-142   2               N/A\r\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS      SYS     SYS     SYS     39-51,143-155   3               N/A\r\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS      SYS     SYS     SYS     13-25,117-129   1               N/A\r\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX     PIX      SYS     SYS     SYS     52-64,156-168   4               N/A\r\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      PIX     SYS     SYS     78-90,182-194   6               N/A\r\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      SYS     PIX     SYS     91-103,195-207  7               N/A\r\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      SYS     SYS     PIX     65-77,169-181   5               N/A\r\nNIC0    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     PIX     SYS     SYS     SYS     SYS     SYS     SYS      SYS     SYS     SYS\r\nNIC1    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      PIX     SYS     SYS     SYS     SYS     SYS     SYS      SYS     SYS     SYS\r\nNIC2    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX      X      SYS     SYS     SYS     SYS     SYS     SYS      SYS     SYS     SYS\r\nNIC3    SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS      SYS     SYS     SYS\r\nNIC4    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS      SYS     SYS     SYS\r\nNIC5    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS      SYS     SYS     SYS\r\nNIC6    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     PIX      SYS     SYS     SYS\r\nNIC7    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      PIX      SYS     SYS     SYS\r\nNIC8    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX      X       SYS     SYS     SYS\r\nNIC9    SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       X      SYS     SYS\r\nNIC10   SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      SYS      X      SYS\r\nNIC11   SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      SYS     SYS      X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  NIC8: mlx5_8\r\n  NIC9: mlx5_9\r\n  NIC10: mlx5_10\r\n  NIC11: mlx5_11\r\n\r\n\r\nulimit soft: 1024",
    "labels": [],
    "state": "closed",
    "created_at": "2025-01-07T22:23:43+00:00",
    "closed_at": "2025-01-20T22:50:41+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2778/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2778"
  },
  {
    "number": 1639,
    "title": "is it time to rerun the benchmarks?",
    "body": "Hi SGLang team,\r\n\r\nI have just tried SGLang for the first time - and it was probably one of the easiest projects to setup and launch - it literally took me a few minutes to go from 0 to serving - awesome!!! and thank you for making it so easy on the user.\r\n\r\nI have just benchmarked vllm=0.6.2 vs sglang=0.3.2 on 2 H100s w/ 8b llama3 and tp=2 and I get vllm slightly faster than sglang performance, yet [the benchmark section](https://github.com/sgl-project/sglang?tab=readme-ov-file#benchmark-and-performance) shows a very different picture. Would it be possible to re-benchmark and tell me if I am missing on some optimization flags to see the results you get - I'm just checking the baseline at the moment - so no quantization and such. Will get there a bit later. FWIW, I have just benchmarked and vllm had a massive throughput speed up made in v0.6.2 over its v0.5 https://x.com/StasBekman/status/1844886291378470966 - which is probably why the benchmark on your site needs a refresher.\r\n\r\nThank you!\r\n\r\nBelow are the stats and command lines so that it's reproducible by others.\r\n\r\nvllm=0.6.2 w/ normal\r\n\r\n```\r\n============ Serving Benchmark Result ============\r\nSuccessful requests:                     50\r\nBenchmark duration (s):                  5.30\r\nTotal input tokens:                      12180\r\nTotal generated tokens:                  11255\r\nRequest throughput (req/s):              9.43\r\nOutput token throughput (tok/s):         2121.93\r\nTotal Token throughput (tok/s):          4418.26\r\n---------------Time to First Token----------------\r\nMean TTFT (ms):                          367.92\r\nMedian TTFT (ms):                        375.01\r\nP99 TTFT (ms):                           378.64\r\n-----Time per Output Token (excl. 1st token)------\r\nMean TPOT (ms):                          5.80\r\nMedian TPOT (ms):                        6.53\r\nP99 TPOT (ms):                           6.75\r\n---------------Inter-token Latency----------------\r\nMean ITL (ms):                           6.54\r\nMedian ITL (ms):                         6.87\r\nP99 ITL (ms):                            8.56\r\n==================================================\r\n\r\n```\r\n\r\n\r\nvllm=0.6.2 w/ --num-scheduler-steps 8\r\n\r\n```\r\n============ Serving Benchmark Result ============\r\nSuccessful requests:                     50\r\nBenchmark duration (s):                  4.44\r\nTotal input tokens:                      12180\r\nTotal generated tokens:                  11249\r\nRequest throughput (req/s):              11.27\r\nOutput token throughput (tok/s):         2535.33\r\nTotal Token throughput (tok/s):          5280.50\r\n---------------Time to First Token----------------\r\nMean TTFT (ms):                          242.44\r\nMedian TTFT (ms):                        231.79\r\nP99 TTFT (ms):                           279.11\r\n-----Time per Output Token (excl. 1st token)------\r\nMean TPOT (ms):                          6.42\r\nMedian TPOT (ms):                        5.82\r\nP99 TPOT (ms):                           12.75\r\n---------------Inter-token Latency----------------\r\nMean ITL (ms):                           44.78\r\nMedian ITL (ms):                         44.70\r\nP99 ITL (ms):                            101.35\r\n==================================================\r\n\r\n```\r\n\r\n\r\nsglang==0.3.2\r\n\r\n```\r\n============ Serving Benchmark Result ============\r\nSuccessful requests:                     50\r\nBenchmark duration (s):                  5.47\r\nTotal input tokens:                      12180\r\nTotal generated tokens:                  11514\r\nRequest throughput (req/s):              9.14\r\nOutput token throughput (tok/s):         2104.62\r\nTotal Token throughput (tok/s):          4330.98\r\n---------------Time to First Token----------------\r\nMean TTFT (ms):                          240.62\r\nMedian TTFT (ms):                        242.29\r\nP99 TTFT (ms):                           326.44\r\n-----Time per Output Token (excl. 1st token)------\r\nMean TPOT (ms):                          8.84\r\nMedian TPOT (ms):                        7.19\r\nP99 TPOT (ms):                           26.74\r\n---------------Inter-token Latency----------------\r\nMean ITL (ms):                           7.04\r\nMedian ITL (ms):                         6.53\r\nP99 ITL (ms):                            10.16\r\n==================================================\r\n\r\n```\r\n\r\nthe servers\r\n\r\nvllm:\r\n```\r\npython -m vllm.entrypoints.openai.api_server \\\r\n    --host 0.0.0.0 --port 9999 \\\r\n    --model meta-llama/Meta-Llama-3-8B-Instruct \\\r\n    --tokenizer meta-llama/Meta-Llama-3-8B-Instruct \\\r\n    --dtype=bfloat16 \\\r\n    --seed 42 \\\r\n    --gpu_memory_utilization 0.8 \\\r\n    --num-scheduler-steps 8 \\\r\n    -tp 2\r\n```\r\n\r\nsglang:\r\n```\r\npython -m sglang.launch_server --port 9999 --tp 2  --model-path meta-llama/Meta-Llama-3-8B-Instruct\r\n```\r\n\r\nthe benchmark client\r\n\r\n```\r\ngit clone https://github.com/vllm-project/vllm\r\ncd vllm/benchmarks\r\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\r\nmkdir results\r\npython benchmark_serving.py \\\r\n    --backend vllm \\\r\n    --model meta-llama/Meta-Llama-3-8B-Instruct \\\r\n    --dataset-name sharegpt \\\r\n    --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \\\r\n    --port 9999 \\\r\n    --save-result \\\r\n    --result-dir results \\\r\n    --result-filename test.json \\\r\n    --num-prompts 50 \\\r\n    --request-rate inf \\\r\n    --seed 42\r\n```\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-10-12T00:55:49+00:00",
    "closed_at": "2024-11-01T04:10:14+00:00",
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1639/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1639"
  },
  {
    "number": 56,
    "title": "[Feature Request] Enable working with Azure-OpenAI API (openai.AzureOpenAI())",
    "body": "Sglang looks great to me, but at my work, we use the Azure-OpenAI API. I don't see how to access this with sglang.\r\n\r\nIt would need two inputs in addition to the API-key, because at minimum I need to create the client like this:\r\n\r\n```python\r\nclient = openai.AzureOpenAI(\r\n    api_key=\"<your-api-key>\",\r\n    base_url=\"https://<your-project-name>.openai.azure.com/openai\",\r\n    api_version=\"<your-api-version>\",  # for example \"2023-05-15\"\r\n)\r\n```\r\n\r\nAlso, for some reason the models are called \"gpt-35-turbo\" instead of \"gpt-3.5-turbo\" (missing dot); and I believe that you can call your models whatever you want. This should be supported, too.\r\n\r\nIf this already works somehow, I would appreciate an explicit mention in the `README.md`. ",
    "labels": [
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-01-19T14:44:21+00:00",
    "closed_at": "2024-02-12T09:07:47+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/56/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/56"
  },
  {
    "number": 1930,
    "title": "[Bug] Incompatible with outlines>=0.1.0",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen launching a server, I get a no module found error for outlines.fsm.regex. After checking the releases of the outlines library, I found that this module was removed in the 0.1.0 release. \n\n### Reproduction\n\npython -m sglang.launch_server --model-path Meta-Llama-3-8B-Instruct \\\r\n--port 30000 --host 0.0.0.0\r\n\r\nError: No module named 'outlines.fsm.regex'. Please install a new version of outlines by `pip install \"outlines>=0.0.44\"`\n\n### Environment\n\nPython: 3.11.2 (main, Mar 16 2023, 23:15:26) [GCC 11.3.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA A100 80GB PCIe\r\nGPU 0 Compute Capability: 8.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 11.8, V11.8.89\r\nCUDA Driver Version: 525.105.17\r\nPyTorch: 2.1.2+cu118\r\nsglang: 0.3.5\r\nflashinfer: 0.1.1+cu118torch2.1\r\ntriton: 2.1.0\r\ntransformers: 4.38.2\r\nrequests: 2.31.0\r\ntqdm: 4.66.2\r\nnumpy: 1.26.4\r\naiohttp: 3.10.2\r\nfastapi: 0.110.0\r\nhf_transfer: Module Not Found\r\nhuggingface_hub: 0.20.3\r\ninteregular: 0.3.3\r\npackaging: 23.2\r\nPIL: 10.2.0\r\npsutil: 5.9.8\r\npydantic: 2.6.3\r\nuvicorn: 0.28.0\r\nuvloop: 0.19.0\r\nzmq: 24.0.1\r\nvllm: Module Not Found\r\nmultipart: Module Not Found\r\nopenai: 1.52.1\r\ntiktoken: 0.8.0\r\nanthropic: Module Not Found\r\nNVIDIA Topology: \r\n        GPU0    CPU Affinity    NUMA Affinity\r\nGPU0     X      0-63            N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nHypervisor vendor: VMware\r\nulimit soft: 1048576",
    "labels": [],
    "state": "closed",
    "created_at": "2024-11-05T22:17:05+00:00",
    "closed_at": "2024-11-30T07:20:51+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1930/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1930"
  }
]
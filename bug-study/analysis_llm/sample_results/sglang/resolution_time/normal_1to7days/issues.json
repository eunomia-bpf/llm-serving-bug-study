[
  {
    "number": 6514,
    "title": "[Bug] start_profile interface makes server crash",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI user `curl --location --request POST 'http://localhost:30000/start_profile' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"output_dir\": \"/models/profile\",\n    \"num_steps\": 10, \n    \"activities\": [\"CPU\", \"GPU\", \"MEM\", \"CUDA_PROFILER\"]\n}'\n`\nto get the nsys profiling result, but server will crash when results are saved.\n\nthe errors stack is below\n\n\n`Capture range ended in the application.\nGenerating '/tmp/nsys-report-f22c.qdstrm'\nCapture range started in the application.\nCapture range ended in the application.\nGenerating '/tmp/nsys-report-1534.qdstrm'\nCapture range started in the application.\nCapture range ended in the application.\nGenerating '/tmp/nsys-report-0a88.qdstrm'\n[1/1] [========================100%] profile.nsys-rep\n[2025-05-21 15:12:07 TP21] Profiling done. Traces are saved to: /models/profile\n[1/1] [========================100%] profile.nsys-rep\n[2025-05-21 15:12:08 TP7] Profiling done. Traces are saved to: /models/profile\n[1/1] [========================100%] profile.nsys-rep\n[2025-05-21 15:12:09 TP15] Profiling done. Traces are saved to: /models/profile\n[1/1] [========================100%] profile.nsys-rep\n[2025-05-21 15:12:16 TP24] Profiling done. Traces are saved to: /models/profile\n[2025-05-21 15:12:17 TP0] Prefill batch. #new-seq: 9, #new-token: 15134, #cached-token: 33, token usage: 0.02, #running-req: 0, #unbootstrapped-req: 367, #queue-req: 70, #transferring-req: 4\n[2025-05-21 15:12:17] TokenizerManager hit an exception: Traceback (most recent call last):\n  File \"/build/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 1426, in print_exception_wrapper\n    await func()\n  File \"/build/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 1116, in handle_loop\n    self._result_dispatcher(recv_obj)\n  File \"/build/sglang/python/sglang/utils.py\", line 471, in __call__\n    return fn(obj)\n  File \"/build/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 1487, in handle_recv\n    self._result_values.append(recv_obj)\nAttributeError: 'NoneType' object has no attribute 'append'`\n\n### Reproduction\n\nwhen server running, ``curl --location --request POST 'http://localhost:30000/start_profile' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"output_dir\": \"/models/profile\",\n    \"num_steps\": 10, \n    \"activities\": [\"CPU\", \"GPU\", \"MEM\", \"CUDA_PROFILER\"]\n}'\n``\n\n### Environment\n\nA800*4",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-05-22T03:27:26+00:00",
    "closed_at": "2025-05-26T05:36:04+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6514/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6514"
  },
  {
    "number": 3451,
    "title": "[Bug] Error while serving deepseek-ai/DeepSeek-V2-Lite in NVIDIA A40-48Q",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nSglang successfully deploys `deepseek-ai/DeepSeek-V2-Lite `, but fails to serve request. \n\n`Deployment and Error While Serving`\n\n```\nroot@vultr:~# python3 -m sglang.launch_server --model-path $MODEL_ID --port 8000 --trust-remote-code                        \n[2025-02-09 20:33:04] server_args=ServerArgs(model_path='deepseek-ai/DeepSeek-V2-Lite', tokenizer_path='deepseek-ai/DeepSeek-V2-Lite', tokenizer_mode='auto', load_format='auto', trust_remote_code=True, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, quantization=None, context_length=None, device='cuda', served_model_name='deepseek-ai/DeepSeek-V2-Lite', chat_template=None, is_embedding=False, revision=None, skip_tokenizer_init=False, host='127.0.0.1', port=8000, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=1, stream_interval=1, stream_output=False, random_seed=656966665, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='sglang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_draft_model_path=None, speculative_algorithm=None, speculative_num_steps=5, speculative_num_draft_tokens=64, speculative_eagle_topk=8, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False)\n[2025-02-09 20:33:13 TP0] MLA optimization is turned on. Use triton backend.\n[2025-02-09 20:33:13 TP0] Init torch distributed begin.\n[2025-02-09 20:33:13 TP0] Load weight begin. avail mem=43.85 GB\nCache shape torch.Size([163840, 64])\n[2025-02-09 20:33:14 TP0] Using model weights format ['*.safetensors']\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.46s/it]\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.58s/it]\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.41s/it]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.46s/it]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.47s/it]\n\n[2025-02-09 20:33:21 TP0] Load weight end. type=DeepseekV2ForCausalLM, dtype=torch.bfloat16, avail mem=14.34 GB\n[2025-02-09 20:33:21 TP0] Memory pool end. avail mem=3.97 GB\n[2025-02-09 20:33:21 TP0] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-02-09 20:33:21 TP0] Capture cuda graph begin. This can take up to several minutes.\n  0%|                                                                                                                      | 0/23 [00:00<?, ?it/s][2025-02-09 20:33:23 TP0] Using default MoE config. Performance might be sub-optimal! Config file not found at /sgl-workspace/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/E=64,N=1408,device_name=NVIDIA_A40-48Q.json\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 23/23 [00:10<00:00,  2.15it/s]\n[2025-02-09 20:33:32 TP0] Capture cuda graph end. Time elapsed: 10.70 s\n[2025-02-09 20:33:32 TP0] max_total_num_tokens=313444, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=163840\n[2025-02-09 20:33:32] INFO:     Started server process [2386]\n[2025-02-09 20:33:32] INFO:     Waiting for application startup.\n[2025-02-09 20:33:32] INFO:     Application startup complete.\n[2025-02-09 20:33:32] INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n[2025-02-09 20:33:33] INFO:     127.0.0.1:46242 - \"GET /get_model_info HTTP/1.1\" 200 OK\n[2025-02-09 20:33:33 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n[2025-02-09 20:33:34] INFO:     127.0.0.1:46248 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-02-09 20:33:34] The server is fired up and ready to roll!\n[2025-02-09 20:33:56 TP0] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 1, cache hit rate: 7.69%, token usage: 0.00, #running-req: 0, #queue-req: 0\n[2025-02-09 20:33:56 TP0] TpModelWorkerClient hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 109, in forward_thread_func\n    self.forward_thread_func_()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 140, in forward_thread_func_\n    logits_output, next_token_ids = self.worker.forward_batch_generation(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 164, in forward_batch_generation\n    logits_output = self.model_runner.forward(forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 785, in forward\n    return self.forward_extend(forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 750, in forward_extend\n    return self.model.forward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 858, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 819, in forward\n    hidden_states, residual = layer(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 757, in forward\n    hidden_states = self.self_attn(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 516, in forward\n    return self.forward_absorb(positions, hidden_states, forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 608, in forward_absorb\n    attn_output = self.attn_mqa(q_input, k_input, v_input, forward_batch)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/radix_attention.py\", line 67, in forward\n    return forward_batch.attn_backend.forward(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/__init__.py\", line 69, in forward\n    return self.forward_extend(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_backend.py\", line 249, in forward_extend\n    self.extend_attention_fwd(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/triton_ops/extend_attention.py\", line 356, in extend_attention_fwd\n    _fwd_kernel[grid](\n  File \"/usr/local/lib/python3.10/dist-packages/triton/runtime/jit.py\", line 345, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/triton/runtime/jit.py\", line 691, in run\n    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata, launch_metadata,\n  File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 381, in __getattribute__\n    self._init_handles()\n  File \"/usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\", line 374, in _init_handles\n    raise OutOfResources(self.metadata.shared, max_shared, \"shared memory\")\ntriton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 106496, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.\n\n[2025-02-09 20:33:56] Received sigquit from a child proces. It usually means the child failed.\nKilled\n```\n\n### Reproduction\n\n**image**: lmsysorg/sglang:latest\n**MODEL_ID**=deepseek-ai/DeepSeek-V2-Lite\n\nCloud Provider: Vultr, Tensordock (In both same `error)`\n\nCommand: python3 -m sglang.launch_server --model-path $MODEL_ID --port 8000 --trust-remote-code\n\nTest Using Below Request\n\n```\n# curl http://localhost:8000/generate  -H \"Content-Type: application/json\"  -d '{\n  \"text\": \"Once upon a time,\",\n  \"sampling_params\": {\n  \"max_new_tokens\": 16,\n  \"temperature\": 0\n }\n}'\n```\n\n### Environment\n\nPython: 3.10.16 (main, Dec  4 2024, 08:53:37) [GCC 9.4.0]\nCUDA available: True\nGPU 0: NVIDIA A40-48Q\nGPU 0 Compute Capability: 8.6\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.5, V12.5.82\nCUDA Driver Version: 550.90.07\nPyTorch: 2.5.1+cu124\nsglang: 0.4.2.post3\nflashinfer: 0.2.0.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.61.1\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-23    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nHypervisor vendor: Microsoft\nulimit soft: 1024",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-10T04:38:38+00:00",
    "closed_at": "2025-02-14T05:58:25+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3451/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3451"
  },
  {
    "number": 6023,
    "title": "[Bug] Document is different with source of --reasoning-parser value",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nDocument is different with source of --reasoning-parser value\u3002\n\nIn document, https://docs.sglang.ai/backend/separate_reasoning.html,\n\n```\nCurrently, SGLang supports the following reasoning models:\n\n[DeepSeek R1 series](https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d): The reasoning content is wrapped with <think> and </think> tags.\n\n[QwQ](https://huggingface.co/Qwen/QwQ-32B): The reasoning content is wrapped with <think> and </think> tags.\n```\n\nIn source code, \n```\n  --reasoning-parser {deepseek-r1,qwen3}\n                        Specify the parser for reasoning models, supported parsers are: ['deepseek-r1', 'qwen3'].\n```\n\nexecute below code will get the result,\n\n```\npython -m sglang.launch_server --help\n```\n\n### Reproduction\n\n```\npython -m sglang.launch_server --help\n```\n\n### Environment\n\n\n\n```\nPython: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2: NVIDIA GeForce RTX 4090\nGPU 0,1,2 Compute Capability: 8.9\nCUDA_HOME: /usr/local/cuda-12.6\nNVCC: Cuda compilation tools, release 12.6, V12.6.85\nCUDA Driver Version: 560.94\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post2\nsgl_kernel: 0.1.1\nflashinfer_python: 0.2.5\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.13\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.1\ninteregular: 0.3.3\nmodelscope: 1.23.2\norjson: 3.10.15\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\npython-multipart: 0.0.20\npyzmq: 26.3.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.8.4\nxgrammar: 0.1.17\nopenai: 1.66.3\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.63.8\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     SYS     SYS                             N/A\nGPU1    SYS      X      SYS     SYS                             N/A\nGPU2    SYS     SYS      X      SYS                             N/A\nGPU3    SYS     SYS     SYS      X                              N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nHypervisor vendor: Microsoft\nulimit soft: 1024\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-05T08:39:15+00:00",
    "closed_at": "2025-05-11T15:22:47+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6023/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6023"
  },
  {
    "number": 7158,
    "title": "[Bug] Using hiradix_cache with dp attention cause hang",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI tried hicache with this using dp-attention, and the latest `0.4.7` version of sglang\n\nHowever, sometimes server stuck before fask request brfore server fire up, sometimes stuck when benchmarking\u3002\n\nwith print something in writing check function of `hiradix_cache.py`\uff0cI found that the server stuck before `all_reduce` function. I think, with dp attention, we shouldn't  pass the `tp_group` process group parameter to `HiCacheController`, but the `attn_tp_group` parameter. Because if there is a dp rank have no request, other instances will wait at `all_reduce` function. \n\n![Image](https://github.com/user-attachments/assets/c834428e-7922-4a27-9fe9-26a639ddcc9d)\ncan not get second print message:\n![Image](https://github.com/user-attachments/assets/196f2496-cb65-463a-8828-b0a1a4a7a56d)\n~~I'm working on a PR of it.~~\n\n\n### Reproduction\n\n```SGLANG_USE_MODELSCOPE=true \\\npython -m sglang.launch_server \\\n    --model-path deepseek-ai/DeepSeek-V2-Lite \\\n    --port 30001 --enable-hierarchical-cache --page-size 32 \\\n    --dp-size 4 --enable-dp-attention --tp-size 4 \\\n    --hicache-ratio 2 \\\n    --trust-remote-code\n\npython ../benchmark/hicache/bench_multiturn.py \\\n    --model-path deepseek-ai/DeepSeek-V2-Lite --num-clients 24 --port 30001\n```\nmaybe should change `i[0]` in bench_multiturn.py to `i.prompt`, as i meet another bug here.\nhttps://github.com/sgl-project/sglang/blob/8ab7d93c2e0da6bb0f3b08a43b57f2d604b57fe0/benchmark/hicache/bench_multiturn.py#L242\n\n### Environment\n\nPython: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA A10\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.6\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.90.07\nPyTorch: 2.7.1+cu126\nsglang: 0.4.7\nsgl_kernel: 0.1.7\nflashinfer_python: 0.2.6.post1\ntriton: 3.3.1\ntransformers: 4.52.3\ntorchao: 0.9.0\nnumpy: 2.2.6\naiohttp: 3.12.6\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.32.3\ninteregular: 0.3.3\nmodelscope: 1.26.0\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.5\npython-multipart: 0.0.20\npyzmq: 26.4.0\nuvicorn: 0.34.3\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.19\nopenai: 1.82.1\ntiktoken: 0.9.0\nanthropic: 0.52.1\nlitellm: 1.72.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU1    PIX      X      NODE    NODE    SYS     SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU2    NODE    NODE     X      PIX     SYS     SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU3    NODE    NODE    PIX      X      SYS     SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU4    SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    32-63,96-127    1               N/A\nGPU5    SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    32-63,96-127    1               N/A\nGPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      PIX     32-63,96-127    1               N/A\nGPU7    SYS     SYS     SYS     SYS     NODE    NODE    PIX      X      32-63,96-127    1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 524288",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-13T11:29:18+00:00",
    "closed_at": "2025-06-20T03:34:37+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7158/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7158"
  },
  {
    "number": 4534,
    "title": "[Bug] Cannot run Janus Pro",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI've noticed there's a recently merged PR to support Janus Pro models #3203 \n\nSGLang version: 0.4.4.post1\n\n```\nMar 18 11:23:41 systemd[1]: Started SGLang Router Serve.\nMar 18 11:23:44 sgl-router-janus-pro-7b[931930]: INFO 03-18 11:23:44 __init__.py:190] Automatically detected platform cuda.\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]: [2025-03-18 11:23:46] server_args=ServerArgs(model_path='/data1/models/Janus-Pro-7B/', tokenizer_path='/data1/models/Janus-Pro-7B/', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='/data1/models/Janus-Pro-7B/', chat_template='janus-pro', is_embedding=False, revision=None, host='0.0.0.0', port=65000, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=-1, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, stream_interval=1, stream_output=False, random_seed=1783968, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=5, speculative_eagle_topk=4, speculative_num_draft_tokens=8, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=True, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=False, flashinfer_mla_disable_ragged=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False)\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]: Traceback (most recent call last):\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:   File \"<frozen runpy>\", line 198, in _run_module_as_main\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:   File \"<frozen runpy>\", line 88, in _run_code\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:   File \"/data1/sglang-0.4.4/.venv/lib/python3.11/site-packages/sglang/launch_server.py\", line 14, in <module>\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:     launch_server(server_args)\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:   File \"/data1/sglang-0.4.4/.venv/lib/python3.11/site-packages/sglang/srt/entrypoints/http_server.py\", line 619, in launch_server\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:     tokenizer_manager, scheduler_info = _launch_subprocesses(server_args=server_args)\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:   File \"/data1/sglang-0.4.4/.venv/lib/python3.11/site-packages/sglang/srt/entrypoints/engine.py\", line 499, in _launch_subprocesses\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:     tokenizer_manager = TokenizerManager(server_args, port_args)\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:   File \"/data1/sglang-0.4.4/.venv/lib/python3.11/site-packages/sglang/srt/managers/tokenizer_manager.py\", line 155, in __init__\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:     self.model_config = ModelConfig(\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:                         ^^^^^^^^^^^^\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:   File \"/data1/sglang-0.4.4/.venv/lib/python3.11/site-packages/sglang/srt/configs/model_config.py\", line 66, in __init__\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:     self.hf_text_config = get_hf_text_config(self.hf_config)\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:   File \"/data1/sglang-0.4.4/.venv/lib/python3.11/site-packages/sglang/srt/configs/model_config.py\", line 326, in get_hf_text_config\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:     class_name = config.architectures[0]\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]:                  ~~~~~~~~~~~~~~~~~~~~^^^\nMar 18 11:23:46 sgl-router-janus-pro-7b[931930]: TypeError: 'NoneType' object is not subscriptable\n```\n\n### Reproduction\n\n`TORCHINDUCTOR_CACHE_DIR=/tmp/torchinductor_cache CUDA_VISIBLE_DEVICES=1  python -m sglang.launch_server --model-path deepseek-ai/Janus-Pro-7B --tp-size 1 --chunked-prefill-size -1 --host 0.0.0.0 --port 65000 --chat-template janus-pro --disable-radix-cache`\n\n### Environment\n\n```\nPython: 3.11.11 (main, Feb 12 2025, 14:51:05) [Clang 19.1.6 ]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.2, V12.2.140\nCUDA Driver Version: 535.161.08\nPyTorch: 2.5.1+cu124\nsglang: 0.4.4.post1\nsgl_kernel: 0.0.5\nflashinfer: 0.2.3+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.14\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.3\ninteregular: 0.3.3\nmodelscope: 1.23.2\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.3.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.66.3\ntiktoken: 0.9.0\nanthropic: 0.49.0\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3 NIC4     NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE SYS      SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     PHB     NODE SYS      SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    PHB     PIX     NODE SYS      SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX  SYS      SYS     SYS     SYS     0-95,192-287    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS  PIX      NODE    NODE    NODE    96-191,288-383  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS  NODE     PIX     NODE    NODE    96-191,288-383  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS  NODE     NODE    PIX     PHB     96-191,288-383  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS  NODE     NODE    PHB     PIX     96-191,288-383  1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE SYS      SYS     SYS     SYS\nNIC1    NODE    PIX     PHB     NODE    SYS     SYS     SYS     SYS     NODE     X      PHB     NODE SYS      SYS     SYS     SYS\nNIC2    NODE    PHB     PIX     NODE    SYS     SYS     SYS     SYS     NODE    PHB      X      NODE SYS      SYS     SYS     SYS\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X   SYS      SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS   X       NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS  NODE      X      NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PIX     PHB     SYS     SYS     SYS     SYS  NODE     NODE     X      PHB\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    PHB     PIX     SYS     SYS     SYS     SYS  NODE     NODE    PHB      X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_bond_0\n  NIC1: mlx5_bond_1\n  NIC2: mlx5_bond_2\n  NIC3: mlx5_bond_3\n  NIC4: mlx5_bond_4\n  NIC5: mlx5_bond_5\n  NIC6: mlx5_bond_6\n  NIC7: mlx5_bond_7\n\n\nulimit soft: 655350\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-18T03:38:48+00:00",
    "closed_at": "2025-03-23T05:10:08+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4534/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4534"
  },
  {
    "number": 2600,
    "title": "[Bug] Cannot run bitsandbytes llama models ",
    "body": "The issue is the same as https://github.com/sgl-project/sglang/issues/2556, but for llama models. We should be able to fix with a similar approach.\r\n\r\nThe following command crashes.\r\n```\r\npython3 -m sglang.bench_one_batch --model unsloth/llama-3-8b-bnb-4bit --load-format bitsandbytes\r\n```\r\n\r\n```\r\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n[rank0]:     return _run_code(code, main_globals, None,\r\n[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n[rank0]:     exec(code, run_globals)\r\n[rank0]:   File \"/root/sglang/python/sglang/bench_one_batch.py\", line 470, in <module>\r\n[rank0]:     main(server_args, bench_args)\r\n[rank0]:   File \"/root/sglang/python/sglang/bench_one_batch.py\", line 434, in main\r\n[rank0]:     work_func(server_args, port_args, bench_args, 0)\r\n[rank0]:   File \"/root/sglang/python/sglang/bench_one_batch.py\", line 369, in latency_test\r\n[rank0]:     model_runner, tokenizer = load_model(server_args, port_args, tp_rank)\r\n[rank0]:   File \"/root/sglang/python/sglang/bench_one_batch.py\", line 121, in load_model\r\n[rank0]:     model_runner = ModelRunner(\r\n[rank0]:   File \"/root/sglang/python/sglang/srt/model_executor/model_runner.py\", line 158, in __init__\r\n[rank0]:     self.load_model()\r\n[rank0]:   File \"/root/sglang/python/sglang/srt/model_executor/model_runner.py\", line 258, in load_model\r\n[rank0]:     self.model = get_model(\r\n[rank0]:   File \"/root/sglang/python/sglang/srt/model_loader/__init__.py\", line 22, in get_model\r\n[rank0]:     return loader.load_model(\r\n[rank0]:   File \"/root/sglang/python/sglang/srt/model_loader/loader.py\", line 1029, in load_model\r\n[rank0]:     self._load_weights(model_config, model)\r\n[rank0]:   File \"/root/sglang/python/sglang/srt/model_loader/loader.py\", line 960, in _load_weights\r\n[rank0]:     model.load_weights(qweight_iterator)\r\n[rank0]:   File \"/root/sglang/python/sglang/srt/models/llama.py\", line 442, in load_weights\r\n[rank0]:     param = params_dict[name]\r\n[rank0]: KeyError: 'model.layers.0.mlp.down_proj.qweight'\r\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:01<?, ?it/s]\r\n```",
    "labels": [
      "good first issue",
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2024-12-26T15:51:48+00:00",
    "closed_at": "2024-12-29T06:21:40+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2600/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2600"
  },
  {
    "number": 4594,
    "title": "[Bug] cannot load prequantized model with scalar weight scale",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nRight now after loading the model and converting the weight scale to channel wise, there's an implicit assumption that the weight scale tensors in model weight is 1-D tensor. This is not the case for modelopt-quantized FP8 in fp8 cutlass supported hardware, since QKVParalleLinear will go through a requantization to the same scale.\n\n### Reproduction\n\n```python\nimport sglang as sgl\n\nif __name__ == '__main__':\n    llm = sgl.Engine(\n        model_path=\"nvidia/Llama-3.1-8B-Instruct-FP8\",\n        quantization=\"modelopt\",\n        revision=\"13858565416dbdc0b4e7a4a677fadfbd5b9e5bb9\",\n        log_level=\"debug\",\n    )\n```\n\nError:\n```\n[2025-03-19 20:37:24 TP0] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/home/jobuser/sglang/python/sglang/srt/managers/scheduler.py\", line 1809, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/home/jobuser/sglang/python/sglang/srt/managers/scheduler.py\", line 227, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/home/jobuser/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/home/jobuser/sglang/python/sglang/srt/managers/tp_worker.py\", line 74, in __init__\n    self.model_runner = ModelRunner(\n  File \"/home/jobuser/sglang/python/sglang/srt/model_executor/model_runner.py\", line 168, in __init__\n    self.initialize(min_per_gpu_memory)\n  File \"/home/jobuser/sglang/python/sglang/srt/model_executor/model_runner.py\", line 178, in initialize\n    self.load_model()\n  File \"/home/jobuser/sglang/python/sglang/srt/model_executor/model_runner.py\", line 383, in load_model\n    self.model = get_model(\n  File \"/home/jobuser/sglang/python/sglang/srt/model_loader/__init__.py\", line 22, in get_model\n    return loader.load_model(\n  File \"/home/jobuser/sglang/python/sglang/srt/model_loader/loader.py\", line 382, in load_model\n    quant_method.process_weights_after_loading(module)\n  File \"/home/jobuser/sglang/python/sglang/srt/layers/quantization/modelopt_quant.py\", line 169, in process_weights_after_loading\n    max_w_scale = convert_to_channelwise(max_w_scale, layer.logical_widths)\n  File \"/home/jobuser/sglang/python/sglang/srt/layers/quantization/utils.py\", line 81, in convert_to_channelwise\n    weight_scale_channel[start:end, :] = weight_scale[idx]\nIndexError: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number\n```\n\n### Environment\n\n```\nPython: 3.10.14 (main, Jul 14 2024, 22:24:12) [GCC 11.2.0]\nCUDA available: True\nGPU 0: NVIDIA H100 80GB HBM3\nGPU 0 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.6, V12.6.77\nCUDA Driver Version: 550.54.15\nPyTorch: 2.5.1+cu124\nsglang: 0.4.4.post1\nsgl_kernel: 0.0.5.post3\nflashinfer: 0.2.3\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.14\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.3\ninteregular: 0.3.3\nmodelscope: 1.24.0\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.3.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.66.3\ntiktoken: 0.9.0\nanthropic: 0.49.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PIX     SYS     SYS     SYS     SYS     SYS     0-63,128-191    0               N/A\nNIC0    PIX      X      SYS     SYS     SYS     SYS     SYS\nNIC1    SYS     SYS      X      PIX     SYS     SYS     SYS\nNIC2    SYS     SYS     PIX      X      SYS     SYS     SYS\nNIC3    SYS     SYS     SYS     SYS      X      SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     SYS      X      SYS\nNIC5    SYS     SYS     SYS     SYS     SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n\n\nulimit soft: 10000000\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-19T20:44:35+00:00",
    "closed_at": "2025-03-22T07:47:54+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4594/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4594"
  },
  {
    "number": 5874,
    "title": "[Bug] Qwen3: Incorrect response field (reasoning_content instead of content) when enable_thinking=false with streaming enabled",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen enable_thinking=False and stream=True, the API incorrectly returns the response in the reasoning_content field rather than the expected content field.\n\n\n\n### Reproduction\n\n- Request\n\n`\ncurl http://localhost:8000/v1/chat/completions   -H \"Content-Type: application/json\"   -d '{\n        \"model\": \"qwen3-32b-fp8\",\n        \"messages\": [\n          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n          {\"role\": \"user\", \"content\": \"Hello!\"}\n        ],\n        \"chat_template_kwargs\": {\"enable_thinking\": false},\n        \"stream\": true \n      }'\n`\n\n- Response\n\n`\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":null,\"**reasoning_content**\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\"Hello\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\"!\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\" How\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\" can\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\" I\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\" assist\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\" you\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\" today\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\"?\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\" \",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":\"\ud83d\ude0a\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":null,\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"a64fca8ef31f4715b3304d10b4dc6c68\",\"object\":\"chat.completion.chunk\",\"created\":1745908654,\"model\":\"qwen3-32b-fp8\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":null,\"**reasoning_content**\":null,\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"stop\",\"matched_stop\":null}],\"usage\":null}\n\ndata: [DONE]\n`\n\n### Environment\n\nPython: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0]\nCUDA available: True\nGPU 0,1,2,3: NVIDIA GeForce RTX 4090\nGPU 0,1,2,3 Compute Capability: 8.9\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.99\nCUDA Driver Version: 550.54.14\nPyTorch: 2.5.1+cu124\nsglang: 0.4.6.post1\nsgl_kernel: 0.1.0\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.51.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\noutlines: 0.1.11\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nxgrammar: 0.1.11\nopenai: 1.62.0\ntiktoken: 0.8.0\nanthropic: 0.45.2\nlitellm: 1.61.1\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PIX     PIX     PIX     0-23,48-71      0               N/A\nGPU1    PIX      X      PIX     PIX     0-23,48-71      0               N/A\nGPU2    PIX     PIX      X      PIX     0-23,48-71      0               N/A\nGPU3    PIX     PIX     PIX      X      0-23,48-71      0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 655360",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-29T06:46:18+00:00",
    "closed_at": "2025-05-01T02:44:38+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5874/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5874"
  },
  {
    "number": 1617,
    "title": "[Bug] Message to guide using <=0.3.2 for data parallel is not shown when --dp is set",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nMore details:\r\nhttps://github.com/sgl-project/sglang/commit/048685430d4c46fd5bc150675b0df49fc6a681d3#r147740163\n\n### Reproduction\n\nsetting `--dp 2`\n\n### Environment\n\nsglang 0.3.3",
    "labels": [],
    "state": "closed",
    "created_at": "2024-10-09T07:40:22+00:00",
    "closed_at": "2024-10-11T14:37:50+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1617/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1617"
  },
  {
    "number": 100,
    "title": "no response running python -m sglang.launch_server --model-path NousResearch/Llama-2-7b-chat-hf --port 30000",
    "body": "when I try to use `sglang` locally according to README.md:\r\n``` sh\r\npython -m sglang.launch_server --model-path NousResearch/Llama-2-7b-chat-hf --port 30000\r\n```\r\n(I use NousResearch/Llama-2-7b-chat-hf because my access of meta-llama is pending)\r\nhowever, I receive no response and no log print. when I run the python script:\r\n```python\r\nfrom sglang import function, system, user, assistant, gen, set_default_backend, RuntimeEndpoint\r\n\r\n@function\r\ndef multi_turn_question(s, question_1, question_2):\r\n    s += system(\"You are a helpful assistant.\")\r\n    s += user(question_1)\r\n    s += assistant(gen(\"answer_1\", max_tokens=256))\r\n    s += user(question_2)\r\n    s += assistant(gen(\"answer_2\", max_tokens=256))\r\n\r\nset_default_backend(RuntimeEndpoint(\"http://localhost:30000\"))\r\n\r\nstate = multi_turn_question.run(\r\n    question_1=\"What is the capital of the United States?\",\r\n    question_2=\"List two local attractions.\",\r\n)\r\n\r\nfor m in state.messages():\r\n    print(m[\"role\"], \":\", m[\"content\"])\r\n\r\nprint(state[\"answer_1\"])\r\n```\r\nI encountered the error as follows:\r\n``` sh\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/urllib/request.py\", line 1348, in do_open\r\n    h.request(req.get_method(), req.selector, req.data, headers,\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/http/client.py\", line 1283, in request\r\n    self._send_request(method, url, body, headers, encode_chunked)\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/http/client.py\", line 1329, in _send_request\r\n    self.endheaders(body, encode_chunked=encode_chunked)\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/http/client.py\", line 1278, in endheaders\r\n    self._send_output(message_body, encode_chunked=encode_chunked)\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/http/client.py\", line 1038, in _send_output\r\n    self.send(msg)\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/http/client.py\", line 976, in send\r\n    self.connect()\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/http/client.py\", line 942, in connect\r\n    self.sock = self._create_connection(\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/socket.py\", line 845, in create_connection\r\n    raise err\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/socket.py\", line 833, in create_connection\r\n    sock.connect(sa)\r\nConnectionRefusedError: [Errno 111] Connection refused\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/mnt/workspace/LongTail/src/sgl-project/sglang/try_sgl.py\", line 11, in <module>\r\n    set_default_backend(RuntimeEndpoint(\"http://localhost:30000\"))\r\n  File \"/mnt/workspace/LongTail/src/sgl-project/sglang/python/sglang/backend/runtime_endpoint.py\", line 21, in __init__\r\n    res = http_request(self.base_url + \"/get_model_info\")\r\n  File \"/mnt/workspace/LongTail/src/sgl-project/sglang/python/sglang/utils.py\", line 102, in http_request\r\n    resp = urllib.request.urlopen(req, data=data)\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/urllib/request.py\", line 216, in urlopen\r\n    return opener.open(url, data, timeout)\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/urllib/request.py\", line 519, in open\r\n    response = self._open(req, data)\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/urllib/request.py\", line 536, in _open\r\n    result = self._call_chain(self.handle_open, protocol, protocol +\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/urllib/request.py\", line 496, in _call_chain\r\n    result = func(*args)\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/urllib/request.py\", line 1377, in http_open\r\n    return self.do_open(http.client.HTTPConnection, req)\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/urllib/request.py\", line 1351, in do_open\r\n    raise URLError(err)\r\nurllib.error.URLError: <urlopen error [Errno 111] Connection refused>\r\n```\r\n\r\n \\>_< I am really  appreciate if someone can help me to solve!!",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-25T15:04:48+00:00",
    "closed_at": "2024-01-30T14:30:56+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/100/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/100"
  },
  {
    "number": 3387,
    "title": "[Bug] Streaming tool use is mixing in regular text with returned tools",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen streaming a request that includes `tools`, outputs look like this:\n\n\n(sglang `v0.4.2.post3-cu124`)\n\n```\n(venv) > $ curl -N http://localhost:3001/v1/chat/completions \\\n  -H \"Authorization: Bearer test-api-key\" \\\n  -d '{\"n\":1,\"model\":\"meta-llama/llama-3.1-8b-instruct/fp-16\",\"tools\":[{\"type\":\"function\",\"function\":{\"strict\": true,\"name\":\"listEvents\",\"description\":\"List all events\",\"parameters\":{}}}],\"messages\":[{\"role\":\"system\",\"content\":\"\\nYou are a helpful assistant.\\nRespond to the following prompt by using function_call and then summarize actions.\\nAsk for clarification if a user request is ambiguous.\\n\"},{\"role\":\"user\",\"content\":\"Instruction: Get all the events. Then create a new event named AGI Party. Then delete event with id 2456.\"}],\"temperature\":0.8,\"stream\":true}'\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962832,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":null,\"tool_calls\":[{\"id\":\"0\",\"type\":\"function\",\"function\":{\"name\":\"listEvents\",\"arguments\":\"\"}}]},\"logprobs\":null,\"finish_reason\":\"tool_call\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\" \\\"\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"parameters\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"\\\":\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\" {\\\"\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"name\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"\\\":\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\" \\\"\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"AG\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"I\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\" Party\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"\\\"}}\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\";\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\" {\\\"\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"name\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"\\\":\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\" \\\"\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"delete\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"Event\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"\\\",\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\" \\\"\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"parameters\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"\\\":\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\" {\\\"\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"id\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"\\\":\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\" \",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"245\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"6\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: {\"id\":\"pvTMmBwCgREi3g-CaZkn1\",\"object\":\"chat.completion.chunk\",\"created\":1738962833,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":null,\"content\":\"}}\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"\",\"matched_stop\":null}],\"usage\":null}\n\ndata: [DONE]\n```\n\nNotice how only one chunk actually included a tool call, the rest were mixed content that also represent tool calls, but were returned as text chunks.\n\n---\n\nIf I make this exact same call with the exact same model with ollama, I get:\n\n```\n(venv) > $ curl -N http://localhost:3001/v1/chat/completions \\\n  -H \"Authorization: Bearer test-api-key\" \\\n  -d '{\"n\":1,\"model\":\"meta-llama/llama-3.1-8b-instruct/fp-16\",\"tools\":[{\"type\":\"function\",\"function\":{\"strict\": true,\"name\":\"listEvents\",\"description\":\"List all events\",\"parameters\":{}}}],\"messages\":[{\"role\":\"system\",\"content\":\"\\nYou are a helpful assistant.\\nRespond to the following prompt by using function_call and then summarize actions.\\nAsk for clarification if a user request is ambiguous.\\n\"},{\"role\":\"user\",\"content\":\"Instruction: Get all the events. Then create a new event named AGI Party. Then delete event with id 2456.\"}],\"temperature\":0.8,\"stream\":true}'\ndata: {\"id\":\"QpEi14mVi9KdtF1m_KroL\",\"object\":\"chat.completion.chunk\",\"created\":1738963103,\"model\":\"llama3.1:8b-instruct-fp16\",\"system_fingerprint\":\"fp_ollama\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\",\"tool_calls\":[{\"id\":\"call_5fts6lp0\",\"index\":0,\"type\":\"function\",\"function\":{\"name\":\"listEvents\",\"arguments\":\"{}\"}}]},\"finish_reason\":null}]}\n\ndata: {\"id\":\"QpEi14mVi9KdtF1m_KroL\",\"object\":\"chat.completion.chunk\",\"created\":1738963104,\"model\":\"llama3.1:8b-instruct-fp16\",\"system_fingerprint\":\"fp_ollama\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\",\"tool_calls\":[{\"id\":\"call_hxhex5ur\",\"index\":1,\"type\":\"function\",\"function\":{\"name\":\"createEvent\",\"arguments\":\"{\\\"name\\\":\\\"AGI Party\\\"}\"}}]},\"finish_reason\":null}]}\n\ndata: {\"id\":\"QpEi14mVi9KdtF1m_KroL\",\"object\":\"chat.completion.chunk\",\"created\":1738963104,\"model\":\"llama3.1:8b-instruct-fp16\",\"system_fingerprint\":\"fp_ollama\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\",\"tool_calls\":[{\"id\":\"call_46qfg8dc\",\"index\":2,\"type\":\"function\",\"function\":{\"name\":\"deleteEvent\",\"arguments\":\"{\\\"id\\\":\\\"2456\\\"}\"}}]},\"finish_reason\":null}]}\n\ndata: {\"id\":\"QpEi14mVi9KdtF1m_KroL\",\"object\":\"chat.completion.chunk\",\"created\":1738963106,\"model\":\"llama3.1:8b-instruct-fp16\",\"system_fingerprint\":\"fp_ollama\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":\"stop\"}]}\n\ndata: [DONE]\n```\n\nNotice how I get exactly **three chunks**, where **each chunk represents a function call**.\n\nThis is **much cleaner** than the current parsing of tool calling, and is preferred behavior IMHO.\n\n### Reproduction\n\nSee the info above which includes a request payload that can be used to reproduce this output.\n\n### Environment\n\n...\n\nIssue was requested to be opened by @shuaills ",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-08T03:08:51+00:00",
    "closed_at": "2025-02-13T01:42:32+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3387/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3387"
  },
  {
    "number": 274,
    "title": "Cannot Execute Runtime Directly in Docker, with local install",
    "body": "I'm running the runtime directly, like so:\r\n\r\n```\r\nSGLANG_PORT, additional_ports = handle_port_init(30000, None, 1)\r\nRUNTIME = sgl.Runtime(\r\n    model_path=model_path,\r\n    port=SGLANG_PORT,\r\n    additional_ports=additional_ports,\r\n    model_mode=[] if os.environ.get(\"DISABLE_FLASH_INFER\") == \"yes\" else [\"flashinfer\"],\r\n)\r\nprint(f\"Initialized SGLang runtime: {RUNTIME.url}\")\r\n```\r\n\r\nBut after upgrading from 0.1.12 to latest commit I get this error:\r\n\r\n```\r\nProcess Process-1:1:\r\nrouter init state: Traceback (most recent call last):\r\n  File \"/sglang/python/sglang/srt/managers/router/manager.py\", line 68, in start_router_process\r\n    model_client = ModelRpcClient(server_args, port_args)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/sglang/python/sglang/srt/managers/router/model_rpc.py\", line 612, in __init__\r\n    self.model_server.exposed_init_model(0, server_args, port_args)\r\n  File \"/sglang/python/sglang/srt/managers/router/model_rpc.py\", line 62, in exposed_init_model\r\n    self.model_runner = ModelRunner(\r\n                        ^^^^^^^^^^^^\r\n  File \"/sglang/python/sglang/srt/managers/router/model_runner.py\", line 275, in __init__\r\n    self.load_model()\r\n  File \"/sglang/python/sglang/srt/managers/router/model_runner.py\", line 284, in load_model\r\n    model_class = get_model_cls_by_arch_name(architectures)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/sglang/python/sglang/srt/managers/router/model_runner.py\", line 41, in get_model_cls_by_arch_name\r\n    model_arch_name_to_cls = import_model_classes()\r\n                             ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/sglang/python/sglang/srt/managers/router/model_runner.py\", line 33, in import_model_classes\r\n    for module_path in (Path(sglang.__file__).parent / \"srt\" / \"models\").glob(\"*.py\"):\r\n                        ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.11/pathlib.py\", line 871, in __new__\r\n    self = cls._from_parts(args)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.11/pathlib.py\", line 509, in _from_parts\r\n    drv, root, parts = self._parse_args(args)\r\n                       ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.11/pathlib.py\", line 493, in _parse_args\r\n    a = os.fspath(a)\r\n        ^^^^^^^^^^^^\r\nTypeError: expected str, bytes or os.PathLike object, not NoneType\r\n\r\ndetoken init state: init ok\r\n\r\nTraceback (most recent call last):\r\n  File \"/handler.py\", line 32, in <module>\r\n    RUNTIME = sgl.Runtime(\r\n              ^^\r\n^^^\r\n^^^\r\n^^^\r\n^\r\n\r\n\r\n  File \"/sglang/python/sglang/api.py\", line 40, in Runtime\r\n    return Runtime(*args, **kwargs)\r\n        \r\n  \r\n ^^^^^^^\r\n^^^^\r\n^^^^^^^\r\n^^^\r\n^^\r\n^\r\n  File \"/sglang/python/sglang/srt/server.py\", line 598, in __init__\r\n    raise RuntimeError(\"Launch failed. Please see the error messages above.\")\r\nRuntimeError: Launch failed. Please see the error messages above.\r\n```\r\n\r\n\r\nI fixed it by monkey-patching this field with the required path\r\n\r\n```\r\nimport sglang\r\n\r\nsglang.__file__ = \"/sglang/python/sglang/srt\"\r\n```\r\n\r\n\r\nFor context, this code runs within the runpod serverless runtime and the full docker-image is available here https://github.com/lucasavila00/LmScript/tree/main/runpod-serverless",
    "labels": [],
    "state": "closed",
    "created_at": "2024-03-11T00:37:46+00:00",
    "closed_at": "2024-03-13T05:08:32+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/274/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/274"
  },
  {
    "number": 5366,
    "title": "[Bug]  'check_marlin_supported' is not defined when launching GPTQ model in 0.4.5",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen launching a GPTQ model using SGLang version 0.4.5, I encountered a runtime error:\n\n```\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/sglang/launch_server.py\", line 14, in <module>\n    launch_server(server_args)\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/sglang/srt/entrypoints/http_server.py\", line 679, in launch_server\n    tokenizer_manager, scheduler_info = _launch_subprocesses(server_args=server_args)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/sglang/srt/entrypoints/engine.py\", line 541, in _launch_subprocesses\n    tokenizer_manager = TokenizerManager(server_args, port_args)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/sglang/srt/managers/tokenizer_manager.py\", line 159, in __init__\n    self.model_config = ModelConfig(\n                        ^^^^^^^^^^^^\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/sglang/srt/configs/model_config.py\", line 167, in __init__\n    self._verify_quantization()\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/sglang/srt/configs/model_config.py\", line 281, in _verify_quantization\n    quantization_override = method.override_quantization_method(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/sglang/srt/layers/quantization/gptq.py\", line 265, in override_quantization_method\n    can_convert = cls.is_gptq_marlin_compatible(hf_quant_cfg)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/sglang/srt/layers/quantization/gptq.py\", line 328, in is_gptq_marlin_compatible\n    return check_marlin_supported(\n           ^^^^^^^^^^^^^^^^^^^^^^\nNameError: name 'check_marlin_supported' is not defined\n```\n\n### Reproduction\n\n1. Install SGLang 0.4.5\n2. Launch a GPTQ model like:\n\n```\npython -m sglang.launch_server --model-path rewrite_Qwen2.5-7B-8GPTQ --host 127.0.0.1 --port 9342 --trust-remote-code --context-length 12800 --dtype float16\n```\n\n### Environment\n\ndockerfile:\n```\n\nFROM hub.byted.org/compile/aml.py311.cu124:py311_cu124\n\nRUN apt update && apt install -y build-essential\n\n\nRUN apt update && apt install -y build-essential\nRUN pip install --no-cache-dir -U pip && \\\n\tpip install --no-cache-dir ninja && \\\n    pip install --no-cache-dir sgl-kernel --force-reinstall --no-deps && \\\n    pip install --no-cache-dir torch==2.5.1 matplotlib pynvml transformers \"sglang[all]>=0.4.5\" --find-links https://flashinfer.ai/whl/cu124/torch2.5/flashinfer/ && \\\n    pip cache purge\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-14T06:56:00+00:00",
    "closed_at": "2025-04-18T09:46:07+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5366/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5366"
  },
  {
    "number": 5361,
    "title": "[Feature] support merge_state in sgl-kernel",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nI have talked to @deftruth, and he will support it in the sgl-kernel today\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority",
      "collaboration",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2025-04-14T00:56:57+00:00",
    "closed_at": "2025-04-15T04:32:18+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5361/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5361"
  },
  {
    "number": 5410,
    "title": "[Bug] hiradix_cache encountered an exception while executing self.dec_lock_ref",
    "body": "https://github.com/sgl-project/sglang/blob/8aab7fdb21e86b87a3141951ce498246fd4aae73/python/sglang/srt/mem_cache/hiradix_cache.py#L120\n\nhit an exception in hiradix_cache.py:120:\n```\n[2025-04-15 07:02:26 TP1] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/mnt/yscfs/yscode/sglang/python/sglang/srt/managers/scheduler.py\", line 1975, in run_scheduler_process\n    scheduler.event_loop_overlap()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/mnt/yscfs/yscode/sglang/python/sglang/srt/managers/scheduler.py\", line 561, in event_loop_overlap\n    batch = self.get_next_batch_to_run()\n  File \"/mnt/yscfs/yscode/sglang/python/sglang/srt/managers/scheduler.py\", line 1171, in get_next_batch_to_run\n    new_batch = self.get_new_batch_prefill()\n  File \"/mnt/yscfs/yscode/sglang/python/sglang/srt/managers/scheduler.py\", line 1207, in get_new_batch_prefill\n    self.tree_cache.writing_check()\n  File \"/mnt/yscfs/yscode/sglang/python/sglang/srt/mem_cache/hiradix_cache.py\", line 157, in writing_check\n    self.dec_lock_ref(self.ongoing_write_through[ack_id])\nKeyError: 37\n```\n\nThe line numbers are inconsistent because I added some logs locally and it won't have any effect.\n\n@xiezhq-hermann Under what circumstances this exception will be triggered",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-15T07:35:20+00:00",
    "closed_at": "2025-04-17T07:16:04+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5410/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5410"
  },
  {
    "number": 1200,
    "title": "[Feature] Use Embedding/Generation Model to get its Generation/Emebedding",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCurrently, SGLang supports getting generation content (chat completion) from generative models and embedding from embedding models. But theoretically, we can get embedding/generation from both embedding/generation models.\r\n\r\nSomething should be stressed that even we can do this, it's not usefully in practice. \r\n\r\n> The key differences between generation and embedding models primarily stem from their post-training specialization, leading to a loss of some capabilities, akin to catastrophic forgetting. Embedding models focus on compressing information into a fixed-dimensional vector space, discouraging long-term predictions, while generation models aim to reduce uncertainty in the probability space, addressing both compression of current information and future uncertainties.\r\n\r\n> The user draws a parallel between these tasks and the distinction between non-autoregressive and autoregressive models. They suggest that embedding models should be decoded with methods like MCMC rather than token-by-token approaches.\r\n\r\n> The community tends to treat generation and embedding as separate tasks, each with its own specialized models and research focus. While the idea of a model that can handle both tasks is attractive, practical challenges make it difficult to implement. The user also notes that OpenAI\u2019s recommendation to fine-tune models for specific applications feels overly product-oriented and not aligned with the concept of AGI.\n\n### Related resources\n\nhttps://github.com/sgl-project/sglang/pull/1186",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-25T01:03:40+00:00",
    "closed_at": "2024-08-27T18:20:30+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1200/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1200"
  },
  {
    "number": 7707,
    "title": "[Bug] DeepSeekR1 under high QPS TBO Error:",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n```\n[2025-07-02 11:36:23 DP5 TP5] Decode batch. #running-req: 1, #token: 2765, token usage: 0.01, pre-allocated usage: 0.00, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 15.02, #queue-req: 0\n[2025-07-02 11:36:23 DP7 TP7] Decode batch. #running-req: 1, #token: 6135, token usage: 0.02, pre-allocated usage: 0.00, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 15.02, #queue-req: 0\n[2025-07-02 11:36:23 DP6 TP6] Decode batch. #running-req: 1, #token: 4205, token usage: 0.01, pre-allocated usage: 0.00, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 15.19, #queue-req: 0\n[2025-07-02 11:36:23 DP4 TP4] Decode batch. #running-req: 1, #token: 8025, token usage: 0.02, pre-allocated usage: 0.00, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 15.19, #queue-req: 0\n[2025-07-02 11:36:23 DP0 TP0] Decode batch. #running-req: 1, #token: 13148, token usage: 0.04, pre-allocated usage: 0.00, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 15.18, #queue-req: 0\n[2025-07-02 11:36:23 DP1 TP1] Decode batch. #running-req: 1, #token: 647, token usage: 0.00, pre-allocated usage: 0.00, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 15.19, #queue-req: 0\n[2025-07-02 11:36:23 DP5 TP5] Decode batch. #running-req: 1, #token: 2766, token usage: 0.01, pre-allocated usage: 0.00, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 15.19, #queue-req: 0\n[2025-07-02 11:36:23 DP2 TP2] Decode batch. #running-req: 1, #token: 1294, token usage: 0.00, pre-allocated usage: 0.00, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 15.19, #queue-req: 0\n[2025-07-02 11:36:23 DP7 TP7] Decode batch. #running-req: 1, #token: 6136, token usage: 0.02, pre-allocated usage: 0.00, #retracted-req: 0, cuda graph: True, gen throughput (token/s): 15.19, #queue-req: 0\n[2025-07-02 11:36:23 handler.py:169 INFO] [64444] request in uuid=139953079802992, /v1/chat/completions\n[2025-07-02 11:36:23 make_app.py:314 INFO] Forwarding request to http://33.200.144.212:30000/v1/chat/completions, stream=False\n[2025-07-02 11:57:15 DP6 TP6] TpModelWorkerClient hit an exception: Traceback (most recent call last):\n  File \"/home/admin/.local/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 127, in forward_thread_func\n    self.forward_thread_func_()\n  File \"/home/admin/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/admin/.local/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 162, in forward_thread_func_\n    self.worker.forward_batch_generation(\n  File \"/home/admin/.local/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py\", line 201, in forward_batch_generation\n    forward_batch = ForwardBatch.init_new(model_worker_batch, self.model_runner)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/admin/.local/lib/python3.12/site-packages/sglang/srt/model_executor/forward_batch_info.py\", line 400, in init_new\n    TboForwardBatchPreparer.prepare(ret)\n  File \"/home/admin/.local/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py\", line 228, in prepare\n    cls.prepare_raw(\n  File \"/home/admin/.local/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py\", line 255, in prepare_raw\n    child_a = cls.filter_batch(\n              ^^^^^^^^^^^^^^^^^\n  File \"/home/admin/.local/lib/python3.12/site-packages/sglang/srt/two_batch_overlap.py\", line 398, in filter_batch\n    raise Exception(f\"{len(errors)} errors happen:\\n\" + \"\\n\\n\".join(errors))\nException: 2 errors happen:\nField top_logprobs_nums has value, but is not yet supported (value=[0] batch=ForwardBatch(forward_mode=<ForwardMode.DECODE: 2>, batch_size=1, input_ids=tensor([128798], device='cuda:6'), req_pool_indices=tensor([1], device='cuda:6'), seq_lens=tensor([5], device='cuda:6'), out_cache_loc=tensor([17], device='cuda:6'), seq_lens_sum=5, seq_lens_cpu=tensor([5]), return_logprob=True, top_logprobs_nums=[0], token_ids_logprobs=[None], temp_scaled_logprobs=False, temperature=None, top_p_normalized_logprobs=False, top_p=None, positions=tensor([4], device='cuda:6'), extend_num_tokens=None, extend_seq_lens=None, extend_prefix_lens=None, extend_start_loc=None, extend_prefix_lens_cpu=None, extend_seq_lens_cpu=None, extend_logprob_start_lens_cpu=None, extend_input_logprob_token_ids_gpu=None, attn_attend_prefix_cache=None, num_prefix_chunks=None, prefix_chunk_idx=None, prefix_chunk_len=None, prefix_chunk_starts=None, prefix_chunk_seq_lens=None, prefix_chunk_cu_seq_lens=None, prefix_chunk_max_seq_lens=None, prefix_chunk_num_tokens=None, prefix_chunk_kv_indices=None, mm_inputs=None, encoder_cached=None, encoder_lens=None, encoder_lens_cpu=None, encoder_out_cache_loc=None, lora_paths=[None], input_embeds=None, token_type_ids=None, sampling_info=SamplingBatchInfo(temperatures=tensor([[1.]], device='cuda:6'), top_ps=tensor([0.8000], device='cuda:6'), top_ks=tensor([1073741824], device='cuda:6', dtype=torch.int32), min_ps=tensor([0.], device='cuda:6'), is_all_greedy=False, need_top_p_sampling=True, need_top_k_sampling=False, need_min_p_sampling=False, vocab_size=129280, grammars=None, vocab_mask=None, apply_mask_func=None, sampling_info_done=<threading.Event at 0x7f3e7f8ce900: set>, penalizer_orchestrator=None, linear_penalty=None, has_custom_logit_processor=False, custom_params=None, custom_logit_processor=None, device='cuda', logit_bias=None), req_to_token_pool=<sglang.srt.disaggregation.decode.DecodeReqToTokenPool object at 0x7f4242a4d400>, token_to_kv_pool=<sglang.srt.mem_cache.memory_pool.MLATokenToKVPool object at 0x7f40a41bbe90>, attn_backend=<sglang.srt.layers.attention.tbo_backend.TboAttnBackend object at 0x7f46b7f74920>, global_num_tokens_cpu=[1], global_num_tokens_gpu=tensor([1], device='cuda:6'), global_num_tokens_for_logprob_cpu=[1], global_num_tokens_for_logprob_gpu=tensor([1], device='cuda:6'), dp_local_start_pos=None, dp_local_num_tokens=None, gathered_buffer=tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:6', dtype=torch.bfloat16), can_run_dp_cuda_graph=1, global_forward_mode=<ForwardMode.DECODE: 2>, spec_info=None, spec_algorithm=<SpeculativeAlgorithm.NONE: 1>, capture_hidden_mode=<CaptureHiddenMode.NULL: 0>, padded_static_len=-1, num_token_non_padded=tensor(1, device='cuda:6', dtype=torch.int32), mrope_positions=None, tbo_split_seq_index=0, tbo_parent_token_range=None, tbo_children=None))\n\nField token_ids_logprobs has value, but is not yet supported (value=[None] batch=ForwardBatch(forward_mode=<ForwardMode.DECODE: 2>, batch_size=1, input_ids=tensor([128798], device='cuda:6'), req_pool_indices=tensor([1], device='cuda:6'), seq_lens=tensor([5], device='cuda:6'), out_cache_loc=tensor([17], device='cuda:6'), seq_lens_sum=5, seq_lens_cpu=tensor([5]), return_logprob=True, top_logprobs_nums=[0], token_ids_logprobs=[None], temp_scaled_logprobs=False, temperature=None, top_p_normalized_logprobs=False, top_p=None, positions=tensor([4], device='cuda:6'), extend_num_tokens=None, extend_seq_lens=None, extend_prefix_lens=None, extend_start_loc=None, extend_prefix_lens_cpu=None, extend_seq_lens_cpu=None, extend_logprob_start_lens_cpu=None, extend_input_logprob_token_ids_gpu=None, attn_attend_prefix_cache=None, num_prefix_chunks=None, prefix_chunk_idx=None, prefix_chunk_len=None, prefix_chunk_starts=None, prefix_chunk_seq_lens=None, prefix_chunk_cu_seq_lens=None, prefix_chunk_max_seq_lens=None, prefix_chunk_num_tokens=None, prefix_chunk_kv_indices=None, mm_inputs=None, encoder_cached=None, encoder_lens=None, encoder_lens_cpu=None, encoder_out_cache_loc=None, lora_paths=[None], input_embeds=None, token_type_ids=None, sampling_info=SamplingBatchInfo(temperatures=tensor([[1.]], device='cuda:6'), top_ps=tensor([0.8000], device='cuda:6'), top_ks=tensor([1073741824], device='cuda:6', dtype=torch.int32), min_ps=tensor([0.], device='cuda:6'), is_all_greedy=False, need_top_p_sampling=True, need_top_k_sampling=False, need_min_p_sampling=False, vocab_size=129280, grammars=None, vocab_mask=None, apply_mask_func=None, sampling_info_done=<threading.Event at 0x7f3e7f8ce900: set>, penalizer_orchestrator=None, linear_penalty=None, has_custom_logit_processor=False, custom_params=None, custom_logit_processor=None, device='cuda', logit_bias=None), req_to_token_pool=<sglang.srt.disaggregation.decode.DecodeReqToTokenPool object at 0x7f4242a4d400>, token_to_kv_pool=<sglang.srt.mem_cache.memory_pool.MLATokenToKVPool object at 0x7f40a41bbe90>, attn_backend=<sglang.srt.layers.attention.tbo_backend.TboAttnBackend object at 0x7f46b7f74920>, global_num_tokens_cpu=[1], global_num_tokens_gpu=tensor([1], device='cuda:6'), global_num_tokens_for_logprob_cpu=[1], global_num_tokens_for_logprob_gpu=tensor([1], device='cuda:6'), dp_local_start_pos=None, dp_local_num_tokens=None, gathered_buffer=tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:6', dtype=torch.bfloat16), can_run_dp_cuda_graph=1, global_forward_mode=<ForwardMode.DECODE: 2>, spec_info=None, spec_algorithm=<SpeculativeAlgorithm.NONE: 1>, capture_hidden_mode=<CaptureHiddenMode.NULL: 0>, padded_static_len=-1, num_token_non_padded=tensor(1, device='cuda:6', dtype=torch.int32), mrope_positions=None, tbo_split_seq_index=0, tbo_parent_token_range=None, tbo_children=None))\n```\n\n### Reproduction\n\nDeepSeek R1 deploy under 4*H800*8 (DP+TP+DeepEP 32)with the following args, and a little bit high QPS:\n```\n    \"decode\": {\n        \"env\": {\n            \"MC_TE_METRIC\": \"true\",\n            \"PYTHONUNBUFFERED\": 1,\n            \"SGLANG_DISAGGREGATION_THREAD_POOL_SIZE\": 4,\n            \"NVSHMEM_ENABLE_NIC_PE_MAPPING\": 1,\n            \"NVSHMEM_HCA_PE_MAPPING\": \"\"\n        },\n        \"args\": {\n            \"--disaggregation-mode\": \"decode\",\n            \"--disaggregation-ib-device\": \"\",\n            \"--disaggregation-bootstrap-port\": 8998,\n            \"--page-size\": 1,\n            \"--tp-size\": 32,\n            \"--dp-size\": 32,\n            \"--enable-dp-attention\": true,\n            \"--enable-dp-lm-head\": true,\n            \"--enable-deepep-moe\": true,\n            \"--moe-dense-tp-size\": 1,\n            \"--deepep-mode\": \"low_latency\",\n            \"--enable-eplb\": true,\n            \"--ep-num-redundant-experts\": 32,\n            \"--ep-dispatch-algorithm\": \"dynamic\",\n            \"--max-running-requests\": 1024,\n            \"--context-len\": 65536,\n            \"--mem-fraction-static\": 0.82,\n            \"--enable-two-batch-overlap\": true,\n            \"--disable-radix-cache\": true,\n            \"--decode-log-interval\": 1,\n            \"--reasoning-parser\": \"deepseek-r1\",\n            \"--cuda-graph-max-bs\": 64,\n            \"--cuda-graph-bs\": \"1 2 4 8 16 32 40 64\"\n        }\n    }\n}\n```\n\n### Environment\n\n```\n$ python3 -m sglang.check_env\nPython: 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H800\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.8, V12.8.61\nCUDA Driver Version: 550.127.08\nPyTorch: 2.7.1+cu126\nsglang: 0.4.8.post10626\nsgl_kernel: 0.1.9\nflashinfer_python: 0.2.6.post1\ntriton: 3.3.1\ntransformers: 4.52.3\ntorchao: 0.9.0\nnumpy: 2.3.1\naiohttp: 3.12.13\nfastapi: 0.115.13\nhf_transfer: 0.1.9\nhuggingface_hub: 0.33.1\ninteregular: 0.3.3\nmodelscope: 1.27.1\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.7\npython-multipart: 0.0.20\npyzmq: 27.0.0\nuvicorn: 0.34.3\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.19\nopenai: 1.63.0\ntiktoken: 0.9.0\nanthropic: Module Not Found\nlitellm: Module Not Found\ndecord: 0.6.0\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-07-02T04:19:06+00:00",
    "closed_at": "2025-07-03T08:17:30+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7707/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7707"
  },
  {
    "number": 6083,
    "title": "[Bug] Fatal Python error: Bus error when loading safetensors check point shards.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI tried to deploy Qwen3-32B and encountered this issue. Full log as below.\n```\n(sglang) administrator@dayuanaiserver:~$ python -m sglang.launch_server --model-path /mnt/raid0disk0/models/Qwen3-32B/ -\n-host 0.0.0.0 --port 6666 --served-model-name Qwen3-32B --mem-fraction-static 0.7 --max-running-requests 20 --tp 2\nINFO 05-07 14:42:34 [importing.py:53] Triton module has been replaced with a placeholder.\nINFO 05-07 14:42:34 [__init__.py:239] Automatically detected platform cuda.\n[2025-05-07 14:42:36] server_args=ServerArgs(model_path='/mnt/raid0disk0/models/Qwen3-32B/', tokenizer_path='/mnt/raid0d\nisk0/models/Qwen3-32B/', tokenizer_mode='auto', skip_tokenizer_init=False, enable_tokenizer_batch_encode=False, load_for\nmat='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=Non\ne, context_length=None, device='cuda', served_model_name='Qwen3-32B', chat_template=None, completion_template=None, is_e\nmbedding=False, revision=None, host='0.0.0.0', port=6666, mem_fraction_static=0.7, max_running_requests=20, max_total_to\nkens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, c\npu_offload_gb=0, page_size=1, tp_size=2, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, r\nandom_seed=1040984153, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=N\none, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show\n_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable\n_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None\n, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', a\nttention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculativ\ne_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_multimodal=None, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode='auto', enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=None, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_bootstrap_port=8998, disaggregation_transfer_backend='mooncake', disaggregation_ib_device=None)\nINFO 05-07 14:42:39 [importing.py:53] Triton module has been replaced with a placeholder.\nINFO 05-07 14:42:39 [importing.py:53] Triton module has been replaced with a placeholder.\nINFO 05-07 14:42:39 [__init__.py:239] Automatically detected platform cuda.\nINFO 05-07 14:42:39 [__init__.py:239] Automatically detected platform cuda.\nINFO 05-07 14:42:39 [importing.py:53] Triton module has been replaced with a placeholder.\nINFO 05-07 14:42:39 [__init__.py:239] Automatically detected platform cuda.\n[2025-05-07 14:42:42 TP0] Attention backend not set. Use flashinfer backend by default.\n[2025-05-07 14:42:42 TP0] Init torch distributed begin.\n[2025-05-07 14:42:42 TP1] Attention backend not set. Use flashinfer backend by default.\n[2025-05-07 14:42:42 TP1] Init torch distributed begin.\n[2025-05-07 14:42:42 TP1] sglang is using nccl==2.21.5\n[2025-05-07 14:42:42 TP0] sglang is using nccl==2.21.5\n[2025-05-07 14:42:43 TP0] Init torch distributed ends. mem usage=0.34 GB\n[2025-05-07 14:42:43 TP1] Init torch distributed ends. mem usage=0.34 GB\n[2025-05-07 14:42:43 TP1] Load weight begin. avail mem=46.93 GB\n[2025-05-07 14:42:43 TP0] Load weight begin. avail mem=46.93 GB\nLoading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]\nFatal Python error: Bus error\n\nThread 0x00007976472006c0 (most recent call first):\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/threading.py\", line 359 in wait\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/threading.py\", line 655 in wait\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/threading.py\", line 1075 in _bootstrap_inner\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/threading.py\", line 1032 in _bootstrap\n\nCurrent thread 0x000079795dbe4740 (most recent call first):\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/layers/linear.py\", line 1254 in weight_loader\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/models/qwen3.py\", line 318 in load_weights\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py\", line 377 in load_model\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py\", line 22 in get_model\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 458 in load_model\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 205 in initialize\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 190 in __init__\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py\", line 81 in __init__\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 64 in __init__\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 268 in __init__\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 2215 in run_scheduler_process\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/multiprocessing/process.py\", line 108 in run\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/multiprocessing/process.py\", line 314 in _bootstrap\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/multiprocessing/spawn.py\", line 135 in _main\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/multiprocessing/spawn.py\", line 122 in spawn_main\n  File \"<string>\", line 1 in <module>\n\nExtension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, uvloop.loop, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, zmq.backend.cython._zmq, PIL._imaging, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, markupsafe._speedups, PIL._imagingft, sentencepiece._sentencepiece, msgspec._core, _cffi_backend, msgpack._cmsgpack, google._upb._message, ray._raylet, regex._regex, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._direct, cuda_utils (total: 108)\n[2025-05-07 15:20:46] Child process unexpectedly failed with an exit code 135. pid=430190\nFatal Python error: Bus error\n\nThread 0x0000776620a006c0 (most recent call first):\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/threading.py\", line 359 in wait\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/threading.py\", line 655 in wait\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/threading.py\", line 1075 in _bootstrap_inner\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/threading.py\", line 1032 in _bootstrap\n\nCurrent thread 0x00007769465bb740 (most recent call first):\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/layers/linear.py\", line 1254 in weight_loader\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/models/qwen3.py\", line 318 in load_weights\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py\", line 377 in load_model\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py\", line 22 in get_model\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 458 in load_model\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 205 in initialize\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 190 in __init__\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py\", line 81 in __init__\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 64 in __init__\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 268 in __init__\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py\", line 2215 in run_scheduler_process\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/multiprocessing/process.py\", line 108 in run\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/multiprocessing/process.py\", line 314 in _bootstrap\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/multiprocessing/spawn.py\", line 135 in _main\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/multiprocessing/spawn.py\", line 122 in spawn_main\n  File \"<string>\", line 1 in <module>\n\nExtension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, uvloop.loop, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, zmq.backend.cython._zmq, PIL._imaging, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, markupsafe._speedups, PIL._imagingft, sentencepiece._sentencepiece, msgspec._core, _cffi_backend, msgpack._cmsgpack, google._upb._message, ray._raylet, regex._regex, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._direct, cuda_utils (total: 108)\n[2025-05-07 15:47:06] Rank 0 scheduler is dead. Please check if there are relevant logs.\n[2025-05-07 15:47:06] Exit code: -7\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/launch_server.py\", line 14, in <module>\n    launch_server(server_args)\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/entrypoints/http_server.py\", line 707, in launch_server\n    tokenizer_manager, scheduler_info = _launch_subprocesses(server_args=server_args)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/site-packages/sglang/srt/entrypoints/engine.py\", line 617, in _launch_subprocesses\n    data = scheduler_pipe_readers[i].recv()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n          ^^^^^^^^^^^^^^^^^^\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n    buf = self._recv(4)\n          ^^^^^^^^^^^^^\n  File \"/home/administrator/miniconda3/envs/sglang/lib/python3.12/multiprocessing/connection.py\", line 399, in _recv\n    raise EOFError\nEOFError\n```\n\n### Reproduction\n\n`(sglang) administrator@dayuanaiserver:~$ python -m sglang.launch_server --model-path /mnt/raid0disk0/models/Qwen3-32B/ -\n-host 0.0.0.0 --port 6666 --served-model-name Qwen3-32B --mem-fraction-static 0.7 --max-running-requests 20 --tp 2`\nThis command worked well with QwQ-32B before.\n\n### Environment\n\n```\n(sglang) administrator@dayuanaiserver:~$ python -m sglang.check_env\nPython: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1: NVIDIA RTX A6000\nGPU 0,1 Compute Capability: 8.6\nCUDA_HOME: /usr/local/cuda-12.4\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.144.03\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post2\nsgl_kernel: 0.1.1\nflashinfer_python: 0.2.5\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.10.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.23.1\norjson: 3.10.15\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\npython-multipart: 0.0.20\npyzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.8.5.post1\nxgrammar: 0.1.18\nopenai: 1.64.0\ntiktoken: 0.9.0\nanthropic: 0.46.0\nlitellm: 1.61.15\ndecord: 0.6.0\nNVIDIA Topology: \n\tGPU0\tGPU1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tNV4\t0-191\t0\t\tN/A\nGPU1\tNV4\t X \t0-191\t0\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1024\n\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-07T10:00:19+00:00",
    "closed_at": "2025-05-11T12:04:52+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6083/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6083"
  },
  {
    "number": 5376,
    "title": "[Bug] low_latency mode on multi nodes",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nExperiencing crashes when deploying DeepSeek-V3 in low latency mode across multiple nodes.\n\n### Reproduction\n\n## node1\n```\npython python/sglang/launch_server.py --model-path=./models/DeepSeek-V3/ --trust-remote-code --tp=16 --dp=16 --ep=16 --enable-dp-attention --enable-deepep-moe --deepep-mode=low_latency --cuda-graph-max-bs=128 --random-seed=42 --dist-init-addr=... --nnodes=2 --node-rank=0\n```\n\n## node2 \n```\npython python/sglang/launch_server.py --model-path=./models/DeepSeek-V3/ --trust-remote-code --tp=16 --dp=16 --ep=16 --enable-dp-attention --enable-deepep-moe --deepep-mode=low_latency --cuda-graph-max-bs=128 --random-seed=42 --dist-init-addr=... --nnodes=2 --node-rank=1\n```\n\nI got the following error:\n\n```\n/sglang/sglang/3rdparty/nvshmem_src/src/modules/bootstrap/uid/bootstrap_uid.cpp:499: non-zero status: -3 /sglang/sglang/3rdparty/nvshmem_src/src/modules/bootstrap/uid/bootstrap_uid.cpp:bootstrap_net_recv:99: Message truncated : received 40 bytes instead of 1\n\n/sglang/sglang/3rdparty/nvshmem_src/src/modules/bootstrap/uid/bootstrap_uid.cpp:499: non-zero status: -3 /sglang/sglang/3rdparty/nvshmem_src/src/modules/bootstrap/uid/bootstrap_uid.cpp:bootstrap_net_recv:99: Message truncated : received 33554468 bytes instead of 40\n\n/sglang/sglang/3rdparty/nvshmem_src/src/modules/bootstrap/uid/bootstrap_uid.cpp:499: non-zero status: -3 /sglang/sglang/3rdparty/nvshmem_src/src/host/topo/topo.cpp:526: non-zero status: -3 allgather of ipc handles failed \n\n/sglang/sglang/3rdparty/nvshmem_src/src/host/team/team.cu:nvshmem_team_split_strided:63: NVSHMEM API called before NVSHMEM initialization has completed\n\n/sglang/sglang/3rdparty/nvshmem_src/src/modules/bootstrap/uid/bootstrap_uid.cpp:bootstrap_net_recv:99: Message truncated : received 40 bytes instead of 1\n\n/sglang/sglang/3rdparty/nvshmem_src/src/modules/bootstrap/uid/bootstrap_uid.cpp:499: non-zero status: -3 /sglang/sglang/3rdparty/nvshmem_src/src/modules/bootstrap/uid/bootstrap_uid.cpp:bootstrap_net_recv:99: Message truncated : received 1743285613 bytes instead of 40\n\n/sglang/sglang/3rdparty/nvshmem_src/src/modules/bootstrap/uid/bootstrap_uid.cpp:499: non-zero status: -3 /sglang/sglang/3rdparty/nvshmem_src/src/host/topo/topo.cpp:526: non-zero status: -3 allgather of ipc handles failed \n\n/sglang/sglang/3rdparty/nvshmem_src/src/host/team/team.cu:nvshmem_team_split_strided:63: NVSHMEM API called before NVSHMEM initialization has completed\n```\n\nHow can I fix it?\n\n### Environment\n\n```\nPython: 3.12.7 (main, Jan 10 2025, 17:28:43) [GCC 13.3.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.8, V12.8.93\nCUDA Driver Version: 535.183.06\nPyTorch: 2.5.1\nsglang: 0.4.5\nsgl_kernel: 0.0.8.post3\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.51.1\ntorchao: 0.10.0\nnumpy: 2.2.4\naiohttp: 3.11.16\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.11.3\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.1\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.17\nopenai: 1.73.0\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.66.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    CPU Affinity       NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    SYS     SYS     0-47,96-1430               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    SYS     SYS     0-47,96-1430               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    SYS     SYS     0-47,96-1430               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    PIX     SYS     SYS     0-47,96-1430               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     PIX     NODE    48-95,144-191      1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     NODE    NODE    48-95,144-191      1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     NODE    PIX     48-95,144-191      1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     NODE    NODE    48-95,144-191      1               N/A\nNIC0    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    SYS     SYS\nNIC1    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE     X      SYS     SYS\nNIC2    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS      X      NODE\nNIC3    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_bond_0\n  NIC1: mlx5_bond_1\n  NIC2: mlx5_bond_2\n  NIC3: mlx5_bond_3\n\n\nulimit soft: 1048576\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-14T11:51:38+00:00",
    "closed_at": "2025-04-16T07:55:40+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5376/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5376"
  },
  {
    "number": 2168,
    "title": "[Feature] How to accelerate constrained decoding when regex needs to change with input?",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nIn some practical application scenarios, the regex needs to change with the input, and the speed of constrained decoding using Compressed FSM will be significantly slower than that of unconstrained decoding due to each time you need to compile. How do we support the constrained decoding acceleration requirement in unfixed regex scenarios? thanks~\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2024-11-25T03:36:49+00:00",
    "closed_at": "2024-12-01T11:06:06+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2168/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2168"
  },
  {
    "number": 5499,
    "title": "[BUG] some problems with HiRadixCache",
    "body": "@xiezhq-hermann \nThe background of the problem we described: \nWe use HiRadixCache in the scenario of PD separation, write_back strategy. The local radix tree will send update events when nodes are added and deleted in rank 0, and the global radix tree will be adjusted according to the update events. When the request comes, we first match according to the global radix tree, and decide to choose P nodes and D nodes according to the number of prefix matches and load. We found that the number of matches in the global tree is sometimes much larger than the number of matches in the local number under the premise of distinguishing between instances. It looks like the host indices is not matched.\nIn the process of troubleshooting the problem, we encountered the following problems:\n\n## 1\u3001`pending_nodes` is not used\nhttps://github.com/sgl-project/sglang/blob/8f783c1943af25e5bbccff628ba4385579b044e1/python/sglang/srt/mem_cache/hiradix_cache.py#L141-L179\n\n`pending_nodes` is not used, this will cause the parent node not to be placed in the heap. Maybe assigned a value here:\n\n```python\n if self.cache_controller.write_policy == \"write_back\": \n     num_evicted += self.write_backup(x) \n     ----> pending_nodes.append(x) <----\n```\n\n## 2\u3001token_to_kv_pool_allocator not release device_indices if write_policy is write_back\nhttps://github.com/sgl-project/sglang/blob/06d0a3d92b25ee24e33cd8150d738b0f5a870889/python/sglang/srt/mem_cache/hiradix_cache.py#L107-L121\n\n`token_to_kv_pool_allocator` not release device_indices if write_policy is write_back, maybe it should be released in the writing_check function\n```python\n    def writing_check(self):\n        ...\n        for _ in range(queue_size.item()):\n            ack_id = self.cache_controller.ack_write_queue.get()\n            self.dec_lock_ref(self.ongoing_write_through[ack_id])\n            ----> self._evict_write_through(self.ongoing_write_through[ack_id]) <----\n            del self.ongoing_write_through[ack_id]\n```\n\n# 3\u3001inc_lock_ref function in writing_check causes all parent nodes to be locked.\nThis function of `inc_lock_ref` called by `write_check` wiil lock node parents so that parent cannot be added to the heap\n\nhttps://github.com/sgl-project/sglang/blob/06d0a3d92b25ee24e33cd8150d738b0f5a870889/python/sglang/srt/mem_cache/hiradix_cache.py#L90-L93\n\nhttps://github.com/sgl-project/sglang/blob/06d0a3d92b25ee24e33cd8150d738b0f5a870889/python/sglang/srt/mem_cache/hiradix_cache.py#L148-L151\n\n",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-17T13:45:53+00:00",
    "closed_at": "2025-04-21T18:46:49+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5499/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5499"
  },
  {
    "number": 44,
    "title": "OpenAI speculative execution",
    "body": "The current frontend using OpenAI will invoke multiple calls for the example below:\r\n```\r\n@sgl.function\r\ndef example(s):\r\n  s += \"Construct a character.\"\r\n  s += \"Name: \" + gen(\"name\") + \" Birthday: \" + gen(\"birthday\") + \" Job: \" + gen(\"job\")\r\n```\r\nWe can optimize this to send less number of calls to save money:\r\n1. Gen longer in the first gen call, and skip the later if the first gen did the right thing.\r\n2. Allow using OpenAI's n=10 keyword argument to sample multiple completions when forked. We can also provide the interface `example.run(n=10)`.",
    "labels": [
      "enhancement",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2024-01-18T18:09:31+00:00",
    "closed_at": "2024-01-25T10:10:02+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/44/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/44"
  },
  {
    "number": 29,
    "title": "Async support",
    "body": null,
    "labels": [
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-01-17T23:44:02+00:00",
    "closed_at": "2024-01-21T23:17:31+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/29/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/29"
  },
  {
    "number": 2527,
    "title": "[Bug] install from source cannot start",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n1. create a virtual environment, install from pip. start sglang.launch_server and it works fine.\r\n2. create another virtual environment, install from source. start sglang.launch_server get error. Error is attached as below:\r\n 30000 --host 0.0.0.0\r\n[2024-12-20 02:00:36] server_args=ServerArgs(model_path='models/Qwen2.5-0.5B-Instruct', tokenizer_path='models/Qwen2.5-0.5B-Instruct', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, context_length=None, device='cuda', served_model_name='models/Qwen2.5-0.5B-Instruct', chat_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=30000, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, tp_size=1, stream_interval=1, random_seed=34265967, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='SGLang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, lora_paths=None, max_loras_per_batch=8, attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=8, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, num_continuous_decode_steps=1, delete_ckpt_after_loading=False)\r\n[2024-12-20 02:00:41 TP0] Init torch distributed begin.\r\n[2024-12-20 02:00:41 TP0] Load weight begin. avail mem=6.95 GB\r\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.01it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.01it/s]\r\n\r\n[2024-12-20 02:00:43 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=5.95 GB\r\n[2024-12-20 02:00:43 TP0] Memory pool end. avail mem=0.29 GB\r\n[2024-12-20 02:00:43 TP0] Capture cuda graph begin. This can take up to several minutes.\r\n2024-12-20 02:00:44,853 - INFO - flashinfer.jit: Loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_64_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\r\n/home/fansong/venv/.sglang_dev/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\r\nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\r\n  warnings.warn(\r\n[2024-12-20 02:00:44 TP0] Scheduler hit an exception: Traceback (most recent call last):\r\n  File \"/home/fansong/code/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 193, in __init__\r\n    self.capture()\r\n  File \"/home/fansong/code/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 254, in capture\r\n    ) = self.capture_one_batch_size(bs, forward)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/fansong/code/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 283, in capture_one_batch_size\r\n    self.model_runner.attn_backend.init_forward_metadata_capture_cuda_graph(\r\n  File \"/home/fansong/code/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py\", line 194, in init_forward_metadata_capture_cuda_graph\r\n    self.indices_updater_decode.update(\r\n  File \"/home/fansong/code/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py\", line 363, in update_single_wrapper\r\n    self.call_begin_forward(\r\n  File \"/home/fansong/code/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py\", line 463, in call_begin_forward\r\n    wrapper.begin_forward(\r\n  File \"/home/fansong/venv/.sglang_dev/lib/python3.12/site-packages/flashinfer/decode.py\", line 761, in plan\r\n    self._cached_module = get_batch_prefill_module(\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/fansong/venv/.sglang_dev/lib/python3.12/site-packages/flashinfer/prefill.py\", line 442, in get_batch_prefill_module\r\n    module = gen_batch_prefill_module(*args)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/fansong/venv/.sglang_dev/lib/python3.12/site-packages/flashinfer/jit/attention.py\", line 435, in gen_batch_prefill_module\r\n    return load_cuda_ops(uri, source_paths)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/fansong/venv/.sglang_dev/lib/python3.12/site-packages/flashinfer/jit/core.py\", line 112, in load_cuda_ops\r\n    module = torch_cpp_ext.load(\r\n             ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/fansong/venv/.sglang_dev/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1312, in load\r\n    return _jit_compile(\r\n           ^^^^^^^^^^^^^\r\n  File \"/home/fansong/venv/.sglang_dev/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1722, in _jit_compile\r\n    _write_ninja_file_and_build_library(\r\n  File \"/home/fansong/venv/.sglang_dev/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1804, in _write_ninja_file_and_build_library\r\n    verify_ninja_availability()\r\n  File \"/home/fansong/venv/.sglang_dev/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1853, in verify_ninja_availability\r\n    raise RuntimeError(\"Ninja is required to load C++ extensions\")\r\nRuntimeError: Ninja is required to load C++ extensions\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/fansong/code/sglang/python/sglang/srt/managers/scheduler.py\", line 1489, in run_scheduler_process\r\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/fansong/code/sglang/python/sglang/srt/managers/scheduler.py\", line 194, in __init__\r\n    self.tp_worker = TpWorkerClass(\r\n                     ^^^^^^^^^^^^^^\r\n  File \"/home/fansong/code/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 61, in __init__\r\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/fansong/code/sglang/python/sglang/srt/managers/tp_worker.py\", line 62, in __init__\r\n    self.model_runner = ModelRunner(\r\n                        ^^^^^^^^^^^^\r\n  File \"/home/fansong/code/sglang/python/sglang/srt/model_executor/model_runner.py\", line 173, in __init__\r\n    self.init_cuda_graphs()\r\n  File \"/home/fansong/code/sglang/python/sglang/srt/model_executor/model_runner.py\", line 624, in init_cuda_graphs\r\n    self.cuda_graph_runner = CudaGraphRunner(self)\r\n                             ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/fansong/code/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 195, in __init__\r\n    raise Exception(\r\nException: Capture cuda graph failed: Ninja is required to load C++ extensions\r\nPossible solutions:\r\n1. disable cuda graph by --disable-cuda-graph\r\n2. set --mem-fraction-static to a smaller value (e.g., 0.8 or 0.7)\r\n3. disable torch compile by not using --enable-torch-compile\r\nOpen an issue on GitHub https://github.com/sgl-project/sglang/issues/new/choose\n\n### Reproduction\n\npython -m sglang.launch_server --model-path models/Qwen2.5-0.5B-Instruct --port 30000 --host 0.0.0.0\n\n### Environment\n\n[2024-12-20 02:19:54] INFO _client.py:1038: HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"\r\nPython: 3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]\r\nCUDA available: True\r\nGPU 0: NVIDIA GeForce RTX 3070 Laptop GPU\r\nGPU 0 Compute Capability: 8.6\r\nCUDA_HOME: /usr/local/cuda-12.6\r\nNVCC: Cuda compilation tools, release 12.6, V12.6.85\r\nCUDA Driver Version: 566.36\r\nPyTorch: 2.5.1+cu124\r\nsglang: 0.4.0.post1\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.1.0\r\ntransformers: 4.47.0\r\ntorchao: 0.7.0\r\nnumpy: 1.26.4\r\naiohttp: 3.11.10\r\nfastapi: 0.115.6\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.26.5\r\ninteregular: 0.3.3\r\nmodelscope: 1.21.0\r\norjson: 3.10.12\r\npackaging: 24.2\r\npsutil: 6.1.0\r\npydantic: 2.10.3\r\nmultipart: 0.0.19\r\nzmq: 26.2.0\r\nuvicorn: 0.34.0\r\nuvloop: 0.21.0\r\nvllm: 0.6.4.post1\r\nopenai: 1.57.4\r\nanthropic: 0.40.0\r\ndecord: 0.6.0\r\nNVIDIA Topology:\r\n        GPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X                              N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nHypervisor vendor: Microsoft\r\nulimit soft: 1024",
    "labels": [],
    "state": "closed",
    "created_at": "2024-12-19T18:20:23+00:00",
    "closed_at": "2024-12-20T18:39:23+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2527/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2527"
  },
  {
    "number": 3141,
    "title": "[Feature] Vision LM accuracy test",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nIn sglang, LLMs have accuracy tests with Hugging Face models:\n\nhttps://github.com/sgl-project/sglang/blob/main/test/srt/models/test_generation_models.py\n\nWe need similar one for VLM also.\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-01-26T06:24:30+00:00",
    "closed_at": "2025-02-01T18:07:14+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3141/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3141"
  },
  {
    "number": 4332,
    "title": "[Feature] New models Gemma 3",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nGemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning.\n\nInputs and outputs\nInput:\n\nText string, such as a question, a prompt, or a document to be summarized\nImages, normalized to 896 x 896 resolution and encoded to 256 tokens each\nTotal input context of 128K tokens for the 4B, 12B, and 27B sizes, and 32K tokens for the 1B size\nOutput:\n\nGenerated text in response to the input, such as an answer to a question, analysis of image content, or a summary of a document\nTotal output context of 8192 tokens\n\n### Related resources\n\nhttps://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d\n\nhttps://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf",
    "labels": [
      "good first issue",
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2025-03-12T07:38:08+00:00",
    "closed_at": "2025-03-17T05:28:55+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4332/reactions",
      "total_count": 12,
      "+1": 10,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4332"
  },
  {
    "number": 998,
    "title": "[Feature] Inference speed difference between sglang and vllm is smaller than advertised",
    "body": "### Motivation\r\n\r\nI compared the inference speed of two large model inference frameworks. I found that sglang is only about 30% faster than vllm, which is much lower than the claimed 3.8 times speedup.\r\n\r\nBelow are my environment details, prompt, and inference results.\r\nmy environment\uff1a\r\ngpu:4090*1\r\ncuda:12.4\r\nPython:3.11.0\r\nvllm:0.5.3\r\nsglang:0.2.7\r\n\r\nlaunch command:\r\n`python -m vllm.entrypoints.openai.api_server --model /home/modeldata/Qwen2-1.5B-Instruct --port 8899`\r\n`python -m sglang.launch_server --model-path /home/modeldata/Qwen2-1.5B-Instruct --host 0.0.0.0 --port 30000`\r\n\r\nprompt:\r\n`\u8bf7\u6839\u636e\u7528\u6237\u53cd\u9988\uff0c\u4ed4\u7ec6\u601d\u8003\u6807\u51c6\u7b54\u6848\u6784\u6210\u8981\u7d20\uff0c\u5e76\u6539\u5199\u51fa5\u53e5\u7b54\u6848\\n\u4f60\u662f\u76f4\u64ad\u771f\u4eba\u95ee\u7b54\u5ba2\u670d\uff0c\u4e3a\u907f\u514d\u5ba2\u670d\u56de\u7b54\u7684\u7b54\u6848\u91cd\u590d\u5ea6\u8fc7\u9ad8\uff0c\u8bf7\u4f60\u9010\u53e5\u601d\u8003\u5e76\u6539\u5199\u95ee\u9898\u7684\u7b54\u6848\u3002\\n****************\\n#\u6837\u4f8b\\n\u7528\u6237\u95ee\u9898\uff1a\u58f0\u97f3\u597d\u597d\u542c\\n\u53c2\u8003\u7684\u95ee\u7b54\u5bf9\uff1a[\"\u95ee\u9898: \u58f0\u97f3\u597d\u597d\u542c, \u7b54\u6848: \u8c22\u8c22\u5b9d\u5b9d\u7684\u5938\u5956\uff0c\u559c\u6b22\u4e3b\u64ad\u7684\u53ef\u4ee5\u70b9\u4e2a\u5173\u6ce8\", \"\u95ee\u9898: \u4f60\u662f\u771f\u4eba\u5417, \u7b54\u6848: \u4ec0\u4e48\uff0c\u4f60\u8bf4\u6211\u662f\u4e0d\u662f\u771f\u4eba\", \"\u95ee\u9898: \u6ca1\u6709\u7ea2\u5305\u5417, \u7b54\u6848: \u7ea2\u5305\u5de6\u4e0a\u89d2\u90fd\u4f1a\u5b89\u6392\u7684\", \"\u95ee\u9898: \u62cd\u5566, \u7b54\u6848: \u597d\u7684\u611f\u8c22\u652f\u6301\u54b1\u5bb6\u7389\u7c73\"]\\n\u8f93\u51fa\u683c\u5f0f\uff1a[\"\u611f\u8c22\u4f60\u7684\u5938\u8d5e\u652f\u6301\", \"\u4f60\u7684\u5938\u8d5e\u662f\u6211\u524d\u8fdb\u7684\u52a8\u529b\", \"\u6536\u5230\u4f60\u7684\u5938\u5956\uff0c\u5fc3\u60c5\u7f8e\u7f8e\u54d2\", \"\u5938\u5956\u6536\u5230\uff0c\u8c22\u8c22\u5b9d\u5b9d\u7684\u70ed\u60c5\", \"\u4f60\u7684\u5938\u5956\u6211\u6536\u5230\u4e86\uff0c\u8c22\u8c22\"]\\n\\n****************\\n#\u89c4\u5219\uff08\u5fc5\u987b\u4e25\u683c\u9075\u5faa\uff09\\n1\u3001\u4f60\u7684\u7b54\u6848\u5fc5\u987b\u4eff\u7167\u6539\u5199\u7528\u6237\u89c9\u5f97\u6ee1\u610f\u7684\u7b54\u6848\\n2\u3001\u4f60\u7684\u7b54\u6848\u7edd\u5bf9\u4e0d\u80fd\u6309\u7167\u7528\u6237\u4e0d\u6ee1\u610f\u7b54\u6848\u7684\u5199\u6cd5\u3002\\n3\u3001\u5fc5\u987b\u7ed9\u51fa\u548c\u7528\u6237\u53cd\u9988\u7684\u6570\u636e\u4e2d\u6539\u5199\u7b54\u6848\uff0c\u4f46\u662f\u4e0d\u80fd\u6539\u53d8\u53e5\u4e49\\n\u5982\u679c\u95ee\u9898\u6d89\u53ca\u5305\u90ae\u7684\u9700\u8981\u6ce8\u610f\uff1a\u504f\u8fdc\u5730\u533a\u4e0d\u5305\u90ae\uff0c\u504f\u8fdc\u5730\u533a\u5305\u542b\uff1a\u65b0\u7586\u3001\u897f\u85cf\u3001\u5b81\u590f\u3001\u5185\u8499\u53e4\u3001\u7518\u8083\u3001\u9752\u6d77\\n\u6d89\u53ca\u4ef7\u683c\u7684\u6570\u5b57\u4e0d\u80fd\u51fa\u9519\uff01\uff01\\n\\n#\u8981\u6c42\uff08\u5fc5\u987b\u4e25\u683c\u9075\u5faa\uff09\\n1\u3001\u4e0d\u80fd\u6539\u53d8\u53e5\u4e49\uff0c\u4e0d\u80fd\u968f\u610f*\u589e\u5220\u6539*\u7b54\u6848\u5f53\u4e2d\u7684\u4e3b\u4f53\uff0c\u53e3\u8bed\u5316\u4e00\u70b9\uff0c\u7b54\u6848\u7b26\u5408\u53e3\u64ad\u573a\u666f\\n2\u3001\u7b54\u6848\u4e2d\u6d89\u53ca\u6570\u5b57\u7684\u4e00\u5b9a\u4e0d\u80fd\u6539\u53d8\u6570\u5b57\u5927\u5c0f\uff01\uff01\\n3\u3001\u8f93\u51fa\u683c\u5f0f\u5fc5\u987b\u7528 list\u5217\u8868[\"\", \"\", \"\", \"\", \"\"]\u53ea\u80fd\u51fa\u73b0\u4e00\u4e2a\u5217\u8868, \u5426\u5219\u6211\u4eec\u53d6\u4e0d\u5230\u7b54\u6848\\n\\n#\u8f93\u5165\\n\u5df2\u77e5\u7528\u6237\u7684\u95ee\u9898\u662f\uff1a*\u9ec4\u7684\u597d\u5403\u767d\u7684\u597d\u5403*\\n\u53c2\u8003\u7684\u95ee\u7b54\uff1a\u6211\u4eec\u7389\u7c73\uff0c\u9ec4\u7684\u66f4\u751c\uff0c\u767d\u7684\u66f4\u7cef\\n\\n#\u8f93\u51fa`\r\n\r\ncode:\r\n`%%time\r\n\r\nsglang_result = []\r\nfor p in tqdm(prompt):\r\n    url = \"http://localhost:30000/v1/chat/completions\"\r\n    \r\n    data = {\r\n            \"model\": \"/home/modeldata/Qwen2-7B-Instruct\",\r\n            \"messages\": [\r\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n                {\"role\": \"user\", \"content\": p},\r\n            ],\r\n            \"max_tokens\": 512,\r\n            \"temperature\": 0.7,\r\n            \"top_p\": 0.8\r\n        }\r\n    \r\n    sglang_result.append(requests.post(url, json=data).json())`\r\n![image](https://github.com/user-attachments/assets/bcea5975-de1e-4d91-97b6-afbd15d5dcf6)\r\n\r\n\r\n I look forward to your response.\r\n\r\n### Related resources\r\n\r\n_No response_",
    "labels": [
      "await-response"
    ],
    "state": "closed",
    "created_at": "2024-08-09T08:07:39+00:00",
    "closed_at": "2024-08-15T16:26:10+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/998/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/998"
  },
  {
    "number": 5032,
    "title": "[Bug] unrecognized arguments: --enable_metrics",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nINFO 04-03 17:50:56 __init__.py:190] Automatically detected platform cuda.\nusage: launch_server.py [-h] --model-path MODEL_PATH [--tokenizer-path TOKENIZER_PATH] [--host HOST] [--port PORT] [--tokenizer-mode {auto,slow}] [--skip-tokenizer-init]\n                        [--load-format {auto,pt,safetensors,npcache,dummy,sharded_state,gguf,bitsandbytes,layered,remote}] [--trust-remote-code]\n                        [--dtype {auto,half,float16,bfloat16,float,float32}] [--kv-cache-dtype {auto,fp8_e5m2,fp8_e4m3}]\n                        [--quantization {awq,fp8,gptq,marlin,gptq_marlin,awq_marlin,bitsandbytes,gguf,modelopt,w8a8_int8,w8a8_fp8}] [--quantization-param-path QUANTIZATION_PARAM_PATH]\n                        [--context-length CONTEXT_LENGTH] [--device DEVICE] [--served-model-name SERVED_MODEL_NAME] [--chat-template CHAT_TEMPLATE]\n                        [--completion-template COMPLETION_TEMPLATE] [--is-embedding] [--revision REVISION] [--mem-fraction-static MEM_FRACTION_STATIC]\n                        [--max-running-requests MAX_RUNNING_REQUESTS] [--max-total-tokens MAX_TOTAL_TOKENS] [--chunked-prefill-size CHUNKED_PREFILL_SIZE]\n                        [--max-prefill-tokens MAX_PREFILL_TOKENS] [--schedule-policy {lpm,random,fcfs,dfs-weight}] [--schedule-conservativeness SCHEDULE_CONSERVATIVENESS]\n                        [--cpu-offload-gb CPU_OFFLOAD_GB] [--page-size PAGE_SIZE] [--tensor-parallel-size TENSOR_PARALLEL_SIZE] [--stream-interval STREAM_INTERVAL] [--stream-output]\n                        [--random-seed RANDOM_SEED] [--constrained-json-whitespace-pattern CONSTRAINED_JSON_WHITESPACE_PATTERN] [--watchdog-timeout WATCHDOG_TIMEOUT]\n                        [--dist-timeout DIST_TIMEOUT] [--download-dir DOWNLOAD_DIR] [--base-gpu-id BASE_GPU_ID] [--gpu-id-step GPU_ID_STEP] [--log-level LOG_LEVEL]\n                        [--log-level-http LOG_LEVEL_HTTP] [--log-requests] [--log-requests-level {0,1,2}] [--show-time-cost] [--enable-metrics]\n                        [--decode-log-interval DECODE_LOG_INTERVAL] [--api-key API_KEY] [--file-storage-path FILE_STORAGE_PATH] [--enable-cache-report]\n                        [--reasoning-parser {deepseek-r1}] [--data-parallel-size DATA_PARALLEL_SIZE] [--load-balance-method {round_robin,shortest_queue}]\n                        [--expert-parallel-size EXPERT_PARALLEL_SIZE] [--dist-init-addr DIST_INIT_ADDR] [--nnodes NNODES] [--node-rank NODE_RANK]\n                        [--json-model-override-args JSON_MODEL_OVERRIDE_ARGS] [--lora-paths [LORA_PATHS ...]] [--max-loras-per-batch MAX_LORAS_PER_BATCH] [--lora-backend LORA_BACKEND]\n                        [--attention-backend {flashinfer,triton,torch_native,fa3}] [--sampling-backend {flashinfer,pytorch}] [--grammar-backend {xgrammar,outlines,llguidance}]\n                        [--enable-flashinfer-mla] [--enable-flashmla] [--flashinfer-mla-disable-ragged] [--speculative-algorithm {EAGLE,EAGLE3,NEXTN}]\n                        [--speculative-draft-model-path SPECULATIVE_DRAFT_MODEL_PATH] [--speculative-num-steps SPECULATIVE_NUM_STEPS] [--speculative-eagle-topk SPECULATIVE_EAGLE_TOPK]\n                        [--speculative-num-draft-tokens SPECULATIVE_NUM_DRAFT_TOKENS] [--speculative-accept-threshold-single SPECULATIVE_ACCEPT_THRESHOLD_SINGLE]\n                        [--speculative-accept-threshold-acc SPECULATIVE_ACCEPT_THRESHOLD_ACC] [--speculative-token-map SPECULATIVE_TOKEN_MAP] [--enable-double-sparsity]\n                        [--ds-channel-config-path DS_CHANNEL_CONFIG_PATH] [--ds-heavy-channel-num DS_HEAVY_CHANNEL_NUM] [--ds-heavy-token-num DS_HEAVY_TOKEN_NUM]\n                        [--ds-heavy-channel-type DS_HEAVY_CHANNEL_TYPE] [--ds-sparse-decode-threshold DS_SPARSE_DECODE_THRESHOLD] [--disable-radix-cache] [--disable-cuda-graph]\n                        [--disable-cuda-graph-padding] [--enable-nccl-nvls] [--disable-outlines-disk-cache] [--disable-custom-all-reduce] [--disable-mla] [--disable-overlap-schedule]\n                        [--enable-mixed-chunk] [--enable-dp-attention] [--enable-ep-moe] [--enable-torch-compile] [--torch-compile-max-bs TORCH_COMPILE_MAX_BS]\n                        [--cuda-graph-max-bs CUDA_GRAPH_MAX_BS] [--cuda-graph-bs CUDA_GRAPH_BS [CUDA_GRAPH_BS ...]] [--torchao-config TORCHAO_CONFIG] [--enable-nan-detection]\n                        [--enable-p2p-check] [--triton-attention-reduce-in-fp32] [--triton-attention-num-kv-splits TRITON_ATTENTION_NUM_KV_SPLITS]\n                        [--num-continuous-decode-steps NUM_CONTINUOUS_DECODE_STEPS] [--delete-ckpt-after-loading] [--enable-memory-saver] [--allow-auto-truncate]\n                        [--enable-custom-logit-processor] [--tool-call-parser {qwen25,mistral,llama3}] [--enable-hierarchical-cache] [--hicache-ratio HICACHE_RATIO]\n                        [--enable-deepep-moe] [--warmups WARMUPS] [--debug-tensor-dump-output-folder DEBUG_TENSOR_DUMP_OUTPUT_FOLDER]\n                        [--debug-tensor-dump-input-file DEBUG_TENSOR_DUMP_INPUT_FILE] [--debug-tensor-dump-inject DEBUG_TENSOR_DUMP_INJECT]\n                        [--disaggregation-mode {null,prefill,decode}] [--disaggregation-bootstrap-port DISAGGREGATION_BOOTSTRAP_PORT]\nlaunch_server.py: error: unrecognized arguments: --enable_metrics\n\n### Reproduction\n\npython -m sglang.launch_server --model-path ./qwen25-1_5b-xingqu --enable_metrics --host 0.0.0.0 --port 8000 --served-model-name Xingqu-Qwen2-1_5B --trust-remote-code\n\n### Environment\n\nPython 3.12.3\nsglang 0.4.4.post3",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-03T10:52:49+00:00",
    "closed_at": "2025-04-06T06:42:01+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5032/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5032"
  },
  {
    "number": 2465,
    "title": "[Bug] potential correctness with triton-attention-num-kv-splits > 1",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nOn H200, seemingly we hit a bit issue on correctness.\r\nWould you please help to confirm?\r\ncc @ispobock \n\n### Reproduction\n\n```\r\npython3 -m sglang.bench_one_batch --model meta-llama/Llama-3.1-8B-Instruct --tp 8 --batch-size 1 --input 1024 --output 2048 --correctness-test --attention-backend triton --triton-attention-num-kv-splits 2\r\n```\r\n\n\n### Environment\n\nN/A",
    "labels": [],
    "state": "closed",
    "created_at": "2024-12-12T09:10:12+00:00",
    "closed_at": "2024-12-14T08:50:56+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2465/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2465"
  },
  {
    "number": 7807,
    "title": "[Bug] Cannot batch generate or n>1 in sampling_params when input_embeds enabled",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI tried to get multi generation outputs via a single post to the server, but I find that if input_embeds is used, the generation will fail and the error message is weired.\n\nIn the [document](https://docs.sglang.ai/backend/sampling_params.html), it is stated that `input_embeds` can be a `List[List[List[float]]]` item, but it turns out that any 3d tensor causes a 400 response:`The engine initialized with skip_tokenizer_init=True cannot accept text prompts. Please provide input_ids or re-initialize the engine with skip_tokenizer_init=False.`, even with shape (1, 1, dim). There are no text prompts at all.\n\nAlso if `n` in the sampling_params > 1, it causes a 400 response.\n\n### Reproduction\n\nsglang==0.4.9\na single l40 gpu\n\nfirst run server:\n```\npython -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct --port 30000 --disable-radix --dtype bfloat16 --skip-tokenizer-init\n```\n\nthen run:\n```python\nimport torch\nimport requests\n\nif __name__ == '__main__':\n    payload = {\n        \"model\": \"qwen/qwen2.5-0.5b-instruct\",\n        \"input_embeds\": torch.zeros((1, 896)).tolist(),\n        \"sampling_params\": {\n            \"max_new_tokens\": 100,\n            \"temperature\": 0,\n            \"n\": 1,\n        }\n    }\n    response = requests.post(\n        \"http://localhost:30000/generate\",\n        json=payload,\n    )\n    # expect: {'output_ids': [11, 20396, 128547, ...], ...}\n    print(response.json())\n    \n    payload = {\n        \"model\": \"qwen/qwen2.5-0.5b-instruct\",\n        \"input_embeds\": torch.zeros((1, 896)).tolist(),\n        \"sampling_params\": {\n            \"max_new_tokens\": 100,\n            \"temperature\": 0,\n            \"n\": 2,\n        }\n    }\n    response = requests.post(\n        \"http://localhost:30000/generate\",\n        json=payload,\n    )\n    # expect: {'error': {'message': 'The engine initialized with skip_tokenizer_init=True cannot accept text prompts. Please provide input_ids or re-initialize the engine with skip_tokenizer_init=False.'}}\n    print(response.json())\n    \n    payload = {\n        \"model\": \"qwen/qwen2.5-0.5b-instruct\",\n        \"input_embeds\": torch.zeros((1, 1, 896)).tolist(),\n        \"sampling_params\": {\n            \"max_new_tokens\": 100,\n            \"temperature\": 0,\n            \"n\": 1,\n        }\n    }\n    response = requests.post(\n        \"http://localhost:30000/generate\",\n        json=payload,\n    )\n    # expect: {'error': {'message': 'The engine initialized with skip_tokenizer_init=True cannot accept text prompts. Please provide input_ids or re-initialize the engine with skip_tokenizer_init=False.'}}\n    print(response.json())\n```\n\n### Environment\n\nPython: 3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]\nCUDA available: True\nGPU 0: NVIDIA L40\nGPU 0 Compute Capability: 8.9\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.3, V12.3.107\nCUDA Driver Version: 550.144.03\nPyTorch: 2.7.1+cu126\nsglang: 0.4.9\nsgl_kernel: 0.2.4\nflashinfer_python: 0.2.7.post1\ntriton: 3.3.1\ntransformers: 4.53.0\ntorchao: 0.9.0\nnumpy: 2.2.6\naiohttp: 3.12.13\nfastapi: 0.115.14\nhf_transfer: 0.1.9\nhuggingface_hub: 0.33.2\ninteregular: 0.3.3\nmodelscope: 1.27.1\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.7\npython-multipart: 0.0.20\npyzmq: 27.0.0\nuvicorn: 0.35.0\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.19\nopenai: 1.93.0\ntiktoken: 0.9.0\nanthropic: 0.57.1\nlitellm: 1.74.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     32-63,96-127    1               N/A\nNIC0    SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_15\n\n\nulimit soft: 1000000",
    "labels": [],
    "state": "closed",
    "created_at": "2025-07-06T13:42:01+00:00",
    "closed_at": "2025-07-08T21:00:43+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7807/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7807"
  }
]
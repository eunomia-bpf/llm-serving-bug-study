[
  {
    "number": 350,
    "title": "sglang doesn't work with vllm versions above 0.3.3",
    "body": "vllm.model_executor.input_metadata is gone in higher versions of vllm. Below is me trying to run with vllm-0.4.0.post1 installed.\r\n\r\n```\r\n(build) owu@gpu:/mnt/resource_nvme$ python -m  sglang.launch_server --model-path liuhaotian/llava-v1.5-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --port 30000\r\n/mnt/resource_nvme/miniconda3/envs/build/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:104: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\r\n  warnings.warn(\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n/mnt/resource_nvme/miniconda3/envs/build/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:144: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.\r\n  warnings.warn(\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nProcess Process-1:\r\nrouter init state: Traceback (most recent call last):\r\n  File \"/mnt/resource_nvme/miniconda3/envs/build/lib/python3.10/site-packages/sglang/srt/managers/router/manager.py\", line 68, in start_router_process\r\n    model_client = ModelRpcClient(server_args, port_args)\r\n  File \"/mnt/resource_nvme/miniconda3/envs/build/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 619, in __init__\r\n    self.model_server.exposed_init_model(0, server_args, port_args)\r\n  File \"/mnt/resource_nvme/miniconda3/envs/build/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 70, in exposed_init_model\r\n    self.model_runner = ModelRunner(\r\n  File \"/mnt/resource_nvme/miniconda3/envs/build/lib/python3.10/site-packages/sglang/srt/managers/router/model_runner.py\", line 271, in __init__\r\n    self.load_model()\r\n  File \"/mnt/resource_nvme/miniconda3/envs/build/lib/python3.10/site-packages/sglang/srt/managers/router/model_runner.py\", line 280, in load_model\r\n    model_class = get_model_cls_by_arch_name(architectures)\r\n  File \"/mnt/resource_nvme/miniconda3/envs/build/lib/python3.10/site-packages/sglang/srt/managers/router/model_runner.py\", line 40, in get_model_cls_by_arch_name\r\n    model_arch_name_to_cls = import_model_classes()\r\n  File \"/mnt/resource_nvme/miniconda3/envs/build/lib/python3.10/site-packages/sglang/srt/managers/router/model_runner.py\", line 33, in import_model_classes\r\n    module = importlib.import_module(f\"sglang.srt.models.{module_path.stem}\")\r\n  File \"/mnt/resource_nvme/miniconda3/envs/build/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/mnt/resource_nvme/miniconda3/envs/build/lib/python3.10/site-packages/sglang/srt/models/gemma.py\", line 12, in <module>\r\n    from vllm.model_executor.input_metadata import InputMetadata\r\nModuleNotFoundError: No module named 'vllm.model_executor.input_metadata'\r\n\r\ndetoken init state: init ok\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-04-06T05:53:23+00:00",
    "closed_at": "2024-04-06T06:36:41+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/350/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/350"
  },
  {
    "number": 6100,
    "title": "Is it possible to run on ROCm without AITER?",
    "body": "As AITER is only running on Instinct hardware, is it possible to run SGLang without AITER on ROCm so Radeon/NAVI based hardware is usable?",
    "labels": [],
    "state": "open",
    "created_at": "2025-05-07T22:16:22+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6100/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6100"
  },
  {
    "number": 7770,
    "title": "[Feature] Support for a Transformer-based Minimax Model",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nHello Team,\n\nI'm writing to suggest a new feature: the implementation of a Minimax model using a Transformer architecture.\n\nWould you consider supporting or developing a Transformer version of the Minimax model? I believe it would be a valuable addition for the community.\n\nThank you for your consideration.\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-04T07:18:07+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7770/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7770"
  },
  {
    "number": 954,
    "title": "[Bug] usage is null when set stream=True",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [X] 2. The bug has not been fixed in the latest version.\r\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n\r\n### Describe the bug\r\n\r\nusage is null when set stream=True via openai sdk.\r\nwhen stream=True usage is OK\r\n\r\n### Reproduction\r\n\r\n#### server\r\n```\r\npython -m sglang.launch_server \\\r\n    --model-path $MODEL_NAME_OR_PATH \\\r\n    --served-model-name Qwen/Qwen2-72B-Instruct-AWQ \\\r\n    --host 0.0.0.0 \\\r\n    --port 30000 \\\r\n    --trust-remote-code \\\r\n    --mem-fraction-static 0.8 \\\r\n    --tp-size 2\r\n```\r\n\r\n#### client:\r\n```\r\nimport time\r\nimport openai\r\n\r\nstart_time = time.time()\r\n\r\nclient = openai.Client(base_url=\"http://localhost:30000/v1\", api_key=\"EMPTY\")\r\n\r\ncompletion = client.chat.completions.create(\r\n    model=\"Qwen/Qwen2-72B-Instruct-AWQ\",\r\n    messages=[\r\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n        {\"role\": \"user\", \"content\": \"\u4f60\u662f\u8c01\"},\r\n    ],\r\n    stream=True,\r\n    stream_options={\"include_usage\": True},\r\n)\r\n\r\nfor chunk in completion:\r\n    print(\"*\" * 66)\r\n    print(chunk.model_dump_json())\r\n\r\n```\r\n#### output\r\n```\r\n{\"id\":\"72a341c0b0b4484a8333c132dfff8355\",\"choices\":[{\"delta\":{\"content\":\"\u53eb\",\"function_call\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":\"None\",\"index\":0,\"logprobs\":null}],\"created\":1723000535,\"model\":\"Qwen/Qwen2-72B-Instruct-AWQ\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\r\n******************************************************************\r\n{\"id\":\"72a341c0b0b4484a8333c132dfff8355\",\"choices\":[{\"delta\":{\"content\":\"\u901a\",\"function_call\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":\"None\",\"index\":0,\"logprobs\":null}],\"created\":1723000535,\"model\":\"Qwen/Qwen2-72B-Instruct-AWQ\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\r\n******************************************************************\r\n{\"id\":\"72a341c0b0b4484a8333c132dfff8355\",\"choices\":[{\"delta\":{\"content\":\"\u4e49\",\"function_call\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":\"None\",\"index\":0,\"logprobs\":null}],\"created\":1723000535,\"model\":\"Qwen/Qwen2-72B-Instruct-AWQ\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\r\n******************************************************************\r\n{\"id\":\"72a341c0b0b4484a8333c132dfff8355\",\"choices\":[{\"delta\":{\"content\":\"\u5343\",\"function_call\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":\"None\",\"index\":0,\"logprobs\":null}],\"created\":1723000535,\"model\":\"Qwen/Qwen2-72B-Instruct-AWQ\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\r\n******************************************************************\r\n{\"id\":\"72a341c0b0b4484a8333c132dfff8355\",\"choices\":[{\"delta\":{\"content\":\"\u95ee\",\"function_call\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":\"None\",\"index\":0,\"logprobs\":null}],\"created\":1723000535,\"model\":\"Qwen/Qwen2-72B-Instruct-AWQ\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\r\n******************************************************************\r\n{\"id\":\"72a341c0b0b4484a8333c132dfff8355\",\"choices\":[{\"delta\":{\"content\":\"\u3002\",\"function_call\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":\"None\",\"index\":0,\"logprobs\":null}],\"created\":1723000535,\"model\":\"Qwen/Qwen2-72B-Instruct-AWQ\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\r\n******************************************************************\r\n{\"id\":\"72a341c0b0b4484a8333c132dfff8355\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":\"FINISH_MATCHED_TOKEN: 151645\",\"index\":0,\"logprobs\":null}],\"created\":1723000535,\"model\":\"Qwen/Qwen2-72B-Instruct-AWQ\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\r\n```\r\n\r\n### besides\r\nThe openai style interface is not completely consistent with OpenAI:\r\neg: the field of `finish_reason` , the output of SGL is [`\"None\"` , `\"FINISH_MATCHED_TOKEN: 151645\"` ] however  opeanai is [`null` , `\"stop\"` , ].\r\n\r\nwee can see the output of vllm at the same time:\r\n```\r\n{\"id\":\"chat-861fb9a873634d05aa2d608ac17dd2e0\",\"choices\":[{\"delta\":{\"content\":\"\u901a\",\"function_call\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1723000215,\"model\":\"Qwen/Qwen2-72B-Instruct-AWQ\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\r\n******************************************************************\r\n{\"id\":\"chat-861fb9a873634d05aa2d608ac17dd2e0\",\"choices\":[{\"delta\":{\"content\":\"\u4e49\",\"function_call\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1723000215,\"model\":\"Qwen/Qwen2-72B-Instruct-AWQ\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\r\n******************************************************************\r\n{\"id\":\"chat-861fb9a873634d05aa2d608ac17dd2e0\",\"choices\":[{\"delta\":{\"content\":\"\u5343\",\"function_call\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1723000215,\"model\":\"Qwen/Qwen2-72B-Instruct-AWQ\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\r\n******************************************************************\r\n{\"id\":\"chat-861fb9a873634d05aa2d608ac17dd2e0\",\"choices\":[{\"delta\":{\"content\":\"\u95ee\",\"function_call\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1723000215,\"model\":\"Qwen/Qwen2-72B-Instruct-AWQ\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\r\n******************************************************************\r\n{\"id\":\"chat-861fb9a873634d05aa2d608ac17dd2e0\",\"choices\":[{\"delta\":{\"content\":\"\u3002\",\"function_call\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1723000215,\"model\":\"Qwen/Qwen2-72B-Instruct-AWQ\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\r\n******************************************************************\r\n{\"id\":\"chat-861fb9a873634d05aa2d608ac17dd2e0\",\"choices\":[{\"delta\":{\"content\":\"\",\"function_call\":null,\"role\":null,\"tool_calls\":null},\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"stop_reason\":null}],\"created\":1723000215,\"model\":\"Qwen/Qwen2-72B-Instruct-AWQ\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":null}\r\n******************************************************************\r\n{\"id\":\"chat-861fb9a873634d05aa2d608ac17dd2e0\",\"choices\":[],\"created\":1723000215,\"model\":\"Qwen/Qwen2-72B-Instruct-AWQ\",\"object\":\"chat.completion.chunk\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":18,\"prompt_tokens\":21,\"total_tokens\":39}}\r\n```\r\n\r\n### Environment\r\n\r\n```Shell\r\nPython: 3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0,1: NVIDIA L20\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 11.7, V11.7.64\r\nCUDA Driver Version: 535.161.08\r\n535.161.08\r\nPyTorch: 2.3.1+cu121\r\nsglang: 0.2.10\r\nflashinfer: 0.1.3+cu121torch2.3\r\ntriton: 2.3.1\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.24.1\r\naiohttp: 3.10.1\r\nfastapi: 0.112.0\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.5\r\ninteregular: 0.3.3\r\npackaging: 23.2\r\nPIL: 9.3.0\r\npsutil: 5.9.6\r\npydantic: 2.5.2\r\nuvicorn: 0.30.5\r\nuvloop: 0.19.0\r\nzmq: 25.1.1\r\nvllm: 0.5.3.post1\r\nmultipart: 0.0.9\r\nopenai: 1.38.0\r\nanthropic: 0.32.0\r\nNVIDIA Topology: \r\n122 NICs found in the topology, only displaying 62 in the matrix.\r\n        GPU0    GPU1    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10   NIC11   NIC12   NIC13   NIC14   NIC15   NIC16   NIC17   NIC18   NIC19   NIC20   NIC21     NIC22   NIC23   NIC24   NIC25   NIC26   NIC27   NIC28   NIC29   NIC30   NIC31   NIC32   NIC33   NIC34   NIC35   NIC36   NIC37   NIC38   NIC39   NIC40   NIC41   NIC42   NIC43   NIC44   NIC45     NIC46   NIC47   NIC48   NIC49   NIC50   NIC51   NIC52   NIC53   NIC54   NIC55   NIC56   NIC57   NIC58   NIC59   NIC60   NIC61   CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      SYS     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE      NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE      NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE                            N/A\r\nGPU1    SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     88-91,184-187   1               N/A\r\nNIC0    NODE    SYS      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC1    NODE    SYS     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC2    NODE    SYS     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC3    NODE    SYS     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC4    NODE    SYS     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC5    NODE    SYS     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC6    NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC7    NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC8    NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC9    NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC10   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC11   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC12   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC13   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC14   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC15   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC16   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC17   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC18   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC19   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC20   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC21   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X        PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC22   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX        X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC23   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC24   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC25   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC26   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC27   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC28   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC29   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC30   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC31   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC32   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC33   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC34   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC35   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC36   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC37   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC38   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC39   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC40   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC41   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC42   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC43   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC44   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC45   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X        PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC46   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX        X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC47   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC48   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC49   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC50   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC51   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC52   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC53   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC54   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC55   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX\r\nNIC56   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX\r\nNIC57   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX\r\nNIC58   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX\r\nNIC59   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX\r\nNIC60   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX\r\nNIC61   NODE    SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: A\r\n  NIC1: DE                              N/A\r\n  NIC2: IC59    NIC60   NIC61   CPU Affinity    NUMA Affinity   GPU NUMA ID\r\n  NIC3: ity     GPU NUMA ID\r\n  NIC4: mlx5_10\r\n  NIC5: mlx5_11\r\n  NIC6: mlx5_13\r\n  NIC7: mlx5_15\r\n  NIC8: mlx5_17\r\n  NIC9: mlx5_18\r\n  NIC10: mlx5_20\r\n  NIC11: mlx5_23\r\n  NIC12: mlx5_25\r\n  NIC13: mlx5_27\r\n  NIC14: mlx5_29\r\n  NIC15: mlx5_31\r\n  NIC16: mlx5_32\r\n  NIC17: mlx5_34\r\n  NIC18: mlx5_35\r\n  NIC19: mlx5_37\r\n  NIC20: mlx5_39\r\n  NIC21: mlx5_41\r\n  NIC22: mlx5_44\r\n  NIC23: mlx5_46\r\n  NIC24: mlx5_48\r\n  NIC25: mlx5_51\r\n  NIC26: mlx5_53\r\n  NIC27: mlx5_54\r\n  NIC28: mlx5_56\r\n  NIC29: mlx5_58\r\n  NIC30: mlx5_60\r\n  NIC31: mlx5_62\r\n  NIC32: mlx5_64\r\n  NIC33: mlx5_66\r\n  NIC34: mlx5_68\r\n  NIC35: mlx5_70\r\n  NIC36: mlx5_72\r\n  NIC37: mlx5_74\r\n  NIC38: mlx5_76\r\n  NIC39: mlx5_77\r\n  NIC40: mlx5_78\r\n  NIC41: mlx5_80\r\n  NIC42: mlx5_82\r\n  NIC43: mlx5_84\r\n  NIC44: mlx5_86\r\n  NIC45: mlx5_89\r\n  NIC46: mlx5_91\r\n  NIC47: mlx5_93\r\n  NIC48: mlx5_94\r\n  NIC49: mlx5_96\r\n  NIC50: mlx5_98\r\n  NIC51: mlx5_100\r\n  NIC52: mlx5_102\r\n  NIC53: mlx5_104\r\n  NIC54: mlx5_106\r\n  NIC55: mlx5_108\r\n  NIC56: mlx5_110\r\n  NIC57: mlx5_112\r\n  NIC58: mlx5_114\r\n  NIC59: mlx5_116\r\n  NIC60: mlx5_118\r\n  NIC61: mlx5_bond_0\r\n\r\n\r\nulimit soft: 65535\r\n```\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-07T02:22:41+00:00",
    "closed_at": "2024-08-08T09:41:58+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/954/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/954"
  },
  {
    "number": 7665,
    "title": "[Roadmap] High performance backend for Ascend NPU",
    "body": "High performance Ascend NPU support by implementation fast Attention backend and NPU Graph support. W8A8 quantization. MLA implementation and DeepSeek support.\n\n**Progress**:\n\n- [ ] Effective Ascend Attention backend implemenattion\n- [ ] Implementation W8A8 quantization\n- [ ] MLA implementation and DeepSeek support\n- [ ] MindIE Turbo fast layers integration\n- [ ] NPU Graph implementation\n- [ ] Support Expert Parallelism",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-30T20:35:38+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7665/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7665"
  },
  {
    "number": 1036,
    "title": "[Feature] Allow arbitrary logit processors",
    "body": "### Motivation\r\n\r\nThere's some great projects out there that modify logits, mostly for guided decoding or novel sampling techniques. Supporting every single one of them will cause too much bloat and distraction, but if SGLang were to allow arbitrary logit processors then the community can plug and play their own processors.\r\n\r\nFor example, I would have interest in using [https://github.com/noamgat/lm-format-enforcer](lm format enforcer) because it allows for optional JSON fields and recursive classes (unlike outlines). The API of lm format enforcer is also clean and simple and it is simple to make custom parsers for other formats than JSON (e.g. SQL).\r\n\r\nOne way I would imagine the API to work is:\r\n\r\n```python\r\ndef my_logits_processor(inputs: list[int], logits: torch.Tensor) -> torch.Tensor:\r\n   ...\r\n\r\n\r\n@sgl.function\r\ndef character_gen(s, name):\r\n    s += name + \" is a character in Harry Potter. Please fill in the following information about this character.\\n\"\r\n    s += sgl.gen(\"output\", logits_processor: my_logits_processor)\r\n```\r\n\r\nI'm not familiar with the internals of SGLang at all, so I am just throwing out the idea of supporting an async logits processor. Often we only care about logits masks that can already be calculated without knowing the scores yet. This would be more efficient as the CPU can calculate the masks while the GPU runs the model. Right now, the lack of such implementation makes logit processors a performance bottleneck in vLLM. \r\n\r\nAn async logit processor could simply work like this:\r\n```python\r\nasync def my_logits_processor(inputs: list[int]) -> AsyncGenerator[torch.Tensor, torch.Tensor]:\r\n   # All the preprocessing steps here to calculate the mask in parallel\r\n\r\n   logits: torch.tensor = yield\r\n\r\n   # Apply the mask to the logits here to calculate the new logits\r\n   yield new_logits  \r\n```\r\nOf course, the async approach would only work if the model's calculations and the logits processor do not run from the same python process. I'm not sure if this would be the case in SGLang's server implementation.\r\n\r\nThe added benefit of integrating it in SGLang over other inference systems is the ability to easily enable logits processors for only certain sections of the generated output. \r\n\r\n### Related resources\r\n\r\n_No response_",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-08-11T19:34:38+00:00",
    "closed_at": "2024-10-21T01:13:28+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1036/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1036"
  },
  {
    "number": 7482,
    "title": "[Bug] Some FP8 models fail to load",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nSglang crashes when loading some FP8 models. I only got Qwen3 FP8 model work, but not others from RedHatAI:\nWORKS:\nhttps://huggingface.co/Qwen/Qwen3-32B-FP8\n\nDO NOT WORK\nhttps://huggingface.co/RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-FP8-dynamic\nhttps://huggingface.co/RedHatAI/Qwen3-32B-FP8-dynamic\nhttps://huggingface.co/RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic\nhttps://huggingface.co/RedHatAI/gemma-3-27b-it-FP8-dynamic\n\n### Reproduction\n\n```\npython3.12 -m venv .venv\nsource .venv/bin/activate.fish\npython -m pip install --upgrade pip\npython -m pip install uv\npython -m uv pip install \"sglang[all]>=0.4.7.post1\"\npython -m uv pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\npython -m uv pip install --force-reinstall sgl_kernel-0.1.9+cu128-cp39-abi3-manylinux2014_x86_64.whl\npython -m uv pip install -U nvidia-nccl-cu12\n```\n\nWORKS with Qwen3-4B normal fp16 -tp 1\n`CUDA_VISIBLE_DEVICES=2 python -m sglang.launch_server --model-path /mnt/llms/models/Qwen/Qwen3-4B --tp 1 --context-length 2048 --port 5001 --host 0.0.0.0`\n\nWORKS with Qwen3-4B-FP8 -tp 1 (I have to add SGL_ENABLE_JIT_DEEPGEMM=0)\n`SGL_ENABLE_JIT_DEEPGEMM=0 CUDA_VISIBLE_DEVICES=2 python -m sglang.launch_server --model-path /mnt/llms/models/Qwen/Qwen3-4B-FP8/ --tp 1 --context-length 2048 --port 5001 --host 0.0.0.0`\n\nDOES NOT Work with RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic\n`SGL_ENABLE_JIT_DEEPGEMM=0 CUDA_VISIBLE_DEVICES=2 python -m sglang.launch_server --model-path /mnt/llms/models/RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic/ --tp 1 --context-length 2048 --port 5001 --host 0.0.0.0`\n\n### Environment\n\n```\n python3 -m sglang.check_env\nPython: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,3,5: NVIDIA GeForce RTX 3090\nGPU 2,4: NVIDIA GeForce RTX 5090\nGPU 0,1,3,5 Compute Capability: 8.6\nGPU 2,4 Compute Capability: 12.0\nCUDA_HOME: /usr/local/cuda-12.8\nNVCC: Cuda compilation tools, release 12.8, V12.8.93\nCUDA Driver Version: 575.57.08\nPyTorch: 2.7.1+cu128\nsglang: 0.4.7.post1\nsgl_kernel: 0.1.9\nflashinfer_python: 0.2.6.post1\ntriton: 3.3.1\ntransformers: 4.52.3\ntorchao: 0.9.0\nnumpy: 2.3.1\naiohttp: 3.12.13\nfastapi: 0.115.13\nhf_transfer: 0.1.9\nhuggingface_hub: 0.33.0\ninteregular: 0.3.3\nmodelscope: 1.27.1\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.7\npython-multipart: 0.0.20\npyzmq: 27.0.0\nuvicorn: 0.34.3\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.19\nopenai: 1.91.0\ntiktoken: 0.9.0\nanthropic: 0.55.0\nlitellm: 1.73.0\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NODE    NODE    NODE    NODE    NODE    0-47    0               N/A\nGPU1    NODE     X      NODE    NODE    NODE    NODE    0-47    0               N/A\nGPU2    NODE    NODE     X      PHB     NODE    NODE    0-47    0               N/A\nGPU3    NODE    NODE    PHB      X      NODE    NODE    0-47    0               N/A\nGPU4    NODE    NODE    NODE    NODE     X      PHB     0-47    0               N/A\nGPU5    NODE    NODE    NODE    NODE    PHB      X      0-47    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1024\n```",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-23T20:04:33+00:00",
    "closed_at": null,
    "comments": 16,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7482/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7482"
  },
  {
    "number": 1374,
    "title": "[Feature] 4-bit quantized prefix cache",
    "body": "### Motivation\r\n\r\nLMDeploy's 4-bit quantized prefix cache (along with 4-bit AWQ for weights) allows running ~70B models on 48GB of RAM with good performance for many-user scenarios. The prefix cache can hold more than 40,000 context tokens.\r\n\r\nThis is very handy, since it's often easier to get a GPU (or dual GPUs) with 48GB RAM than it is to get 80GB+ GPUs.\r\n\r\nNote that I've benchmarked the output quality/accuracy of 4-bit prefix cache vs no quantization, and there was no significant accuracy drop with my internal benchmarks. For my use case, at least, it's a free perf boost.\r\n\r\nToday I wanted to try comparing SGLang performance to LMDeploy, but (for a 70B model on 48GB GPU) SGLang OOMs for even a small number of concurrent requests.\r\n\r\nI'm testing with LLama 2 AWQ model with ~2k token context and ~100 token outputs:\r\n\r\n### LMDeploy (handles 20 concurrent requests fine):\r\nUsing latest (`openmmlab/lmdeploy:v0.6.0a0-cu12`) docker image on 48GB NVIDIA A40 GPU:\r\n```\r\nlmdeploy serve api_server lmdeploy/llama2-chat-70b-4bit --server-port 3000 --tp $(nvidia-smi -L | wc -l) --session-len 8192 --model-format awq --enable-prefix-caching --quant-policy 4 --log-level INFO\r\n```\r\n\r\n### SGLang (OOM at >=4 concurrent requests):\r\nUsing latest (`lmsysorg/sglang:v0.3.0-cu121`) docker image on 48GB NVIDIA A40 GPU:\r\n```\r\npython3 -m sglang.launch_server --model-path lmdeploy/llama2-chat-70b-4bit --context-length 8192 --host 0.0.0.0 --port 3000 --tp-size $(nvidia-smi -L | wc -l)\r\n```\r\n\r\nFor reference, here's some example OOM logs from SGLang that I'm seeing: https://gist.github.com/josephrocca/1c688e312f5d570ca9a4652485ff6a24\r\n\r\nIt would be great if SGLang could become competitive with LMDeploy in this type of scenario, and I think it's hard to compete in a many user-scenario without a 4-bit quantized prefix cache.\r\n\r\n### Related resources\r\n\r\n_No response_",
    "labels": [
      "enhancement",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-09-10T16:38:26+00:00",
    "closed_at": "2024-12-06T01:17:32+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1374/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1374"
  },
  {
    "number": 3315,
    "title": "[Bug] Error when running Qwen2 EAGLE speculative decoding refering to the official example",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n\nI am trying to run the Qwen2 EAGLE speculative decoding example as provided in the official documentation:  \n[EAGLE Decoding Documentation](https://docs.sglang.ai/backend/speculative_decoding.html#EAGLE-Decoding)\n\nI used the following command to launch the server:\n\n```bash\npython -m sglang.launch_server --model-path Qwen/Qwen2-7B-Instruct --disable-radix-cache --host 127.0.0.1 --port 1235 --tensor-parallel-size 1 --speculative-algo EAGLE --speculative-draft yuhuili/EAGLE-Qwen2-7B-Instruct --speculative-num-steps 5 --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 --disable-cuda-graph\n```\n\n**Model Sources:**\n- `EAGLE-Qwen2-7B-Instruct` was downloaded from: [yuhuili/EAGLE-Qwen2-7B-Instruct](https://huggingface.co/yuhuili/EAGLE-Qwen2-7B-Instruct)\n- `Qwen2-7B-Instruct` was downloaded from: [Qwen/Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)\n\n**Error Encountered:**\n\nWhen running the command, I encountered the following error:\n\n```\n[2025-02-05 11:13:18 TP0] Init torch distributed begin.\n[2025-02-05 11:13:18 TP0] Load weight begin. avail mem=10.19 GB\nLoading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n/root/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_loader/weight_utils.py:447: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = torch.load(bin_file, map_location=\"cpu\")\n[2025-02-05 11:13:19 TP0] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 1787, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 252, in __init__\n    self.draft_worker = EAGLEWorker(\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/speculative/eagle_worker.py\", line 34, in __init__\n    super().__init__(\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 68, in __init__\n    self.model_runner = ModelRunner(\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 185, in __init__\n    self.load_model()\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 306, in load_model\n    self.model = get_model(\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_loader/__init__.py\", line 22, in get_model\n    return loader.load_model(\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_loader/loader.py\", line 362, in load_model\n    model.load_weights(self._get_all_weights(model_config, model))\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/models/qwen2.py\", line 390, in load_weights\n    param = params_dict[name]\nKeyError: 'layers.0.self_attn.qkv_proj.weight'\n\nLoading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n\n[2025-02-05 11:13:19] Received sigquit from a child proces. It usually means the child failed.\nsglang_server.sh: line 89: 2212287 Killed                  python -m sglang.launch_server --model-path ${model_path} --disable-radix-cache --host 127.0.0.1 --port ${PORT} --tensor-parallel-size ${TP_SIZE} ${CHUNKED_PREFILL_ARGS} ${FP8_QUANT_ARGS} ${SPECULATIVE_ARGS} ${TORCH_COMPILE_ARGS}\n```\n\nCould you please help me resolve this issue? Let me know if additional details are required.\n\n---\n\n\n\n### Reproduction\n\npython -m sglang.launch_server --model-path Qwen/Qwen2-7B-Instruct --disable-radix-cache --host 127.0.0.1 --port 1235 --tensor-parallel-size 1 --speculative-algo EAGLE --speculative-draft yuhuili/EAGLE-Qwen2-7B-Instruct --speculative-num-steps 5 --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 --disable-cuda-graph\n\n### Environment\n\n/root/miniconda3/envs/sglang/lib/python3.10/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n* 'fields' has been removed\n  warnings.warn(message, UserWarning)\nPython: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1: NVIDIA H20\nGPU 0,1 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.161.08\nPyTorch: 2.5.1+cu124\nsglang: 0.4.2.post1\nflashinfer: 0.1.6+cu124torch2.4\ntriton: 3.1.0\ntransformers: 4.48.2\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.61.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-47    0               N/A\nGPU1    NV18     X      NV18    NV18    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-47    0               N/A\nGPU2    NV18    NV18     X      NV18    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     0-47    0               N/A\nGPU3    NV18    NV18    NV18     X      NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     0-47    0               N/A\nNIC0    PIX     NODE    NODE    NODE     X      NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC1    NODE    PIX     NODE    NODE    NODE     X      NODE    NODE    SYS     SYS     SYS     SYS\nNIC2    NODE    NODE    PIX     NODE    NODE    NODE     X      NODE    SYS     SYS     SYS     SYS\nNIC3    NODE    NODE    NODE    PIX     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE     X      NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      NODE\nNIC7    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_1\n  NIC1: mlx5_2\n  NIC2: mlx5_3\n  NIC3: mlx5_4\n  NIC4: mlx5_5\n  NIC5: mlx5_6\n  NIC6: mlx5_7\n  NIC7: mlx5_8\n\n\nulimit soft: 1048576",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-02-05T11:21:57+00:00",
    "closed_at": "2025-02-09T15:39:46+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3315/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3315"
  },
  {
    "number": 4361,
    "title": "[Bug] flashinfer separate installation: Probably needs either code or documentation fix?",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI see that https://github.com/sgl-project/sglang/pull/3033 recently added flashinfer to 3rdparty. Now after reading [sgl-kernel/setup.py](https://github.com/sgl-project/sglang/blob/main/sgl-kernel/setup.py), it seems to me that it unconditionally builds flashinfer from 3rdparty.\n\nYet the [installation documentation](https://github.com/sgl-project/sglang/blob/main/docs/start/install.md) says\n\n> SGLang currently uses torch 2.5, so you need to install flashinfer for torch 2.5. If you want to install flashinfer separately, please refer to [FlashInfer installation doc](https://docs.flashinfer.ai/installation.html). Please note that the FlashInfer pypi package is called `flashinfer-python` instead of `flashinfer`.\n\nNow, unless I misunderstand something, either\n\n1. The documentation is obsolete and building flashinfer separately is intentionally not supported (-> documentation bug); or\n2. [sgl-kernel/setup.py](https://github.com/sgl-project/sglang/blob/main/sgl-kernel/setup.py) has a bug in that it uses the vendored flashinfer unconditionally (-> code bug).\n\nI have no idea which one is the intent; I just ran into this issue when trying to install it separately while packaging sglang (for NixOS).\n\n### Reproduction\n\n-\n\n### Environment\n\n(build issue)",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-13T04:16:27+00:00",
    "closed_at": "2025-05-13T00:19:04+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4361/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4361"
  },
  {
    "number": 371,
    "title": "JSON decoding result don't match regex",
    "body": "sglang 0.12.0\r\nmodel: Qwen1.5-0.5B\r\n\r\nI have two regex: debug_regex1 and debug_regex2. Both are simplified from \u201cbuild_regex_from_object\u201d\r\nOriginal regex is:\r\n```text\r\n\\{[\\n ]*\"data\"[\\n ]*:[\\n ]*\\[[\\n ]*((\\{[\\n ]*\"key1\"[\\n ]*:[\\n ]*(\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"|null)[\\n ]*,[\\n ]*\"key2\"[\\n ]*:[\\n ]*(\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"|null)[\\n ]*,[\\n ]*\"key3\"[\\n ]*:[\\n ]*(\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"|null)[\\n ]*\\})(,[\\n ]*(\\{[\\n ]*\"key1\"[\\n ]*:[\\n ]*(\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"|null)[\\n ]*,[\\n ]*\"key2\"[\\n ]*:[\\n ]*(\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"|null)[\\n ]*,[\\n ]*\"key3\"[\\n ]*:[\\n ]*(\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"|null)[\\n ]*\\})){0,})?[\\n ]*\\][\\n ]*\\}\r\n```\r\nSometimes json decoding outputs \u201c\\n\\n\\n\\n\\n\\n\u2026\u2026\u201d, so I deleted some \u201c[\\n ]*\u201d(https://github.com/sgl-project/sglang/issues/258#issuecomment-2041814454). But output don't match regex.\r\nYou can reproduce this bug using the following Python code.\r\n```python\r\nimport json\r\nimport re\r\nimport sglang as sgl\r\n\r\n\r\ndebug_regex1 = r'''\\[[\\n ]*((\\{[\\n ]*\"key1\":[\\n ]*(\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"|null)[\\n ]*,[\\n ]*\"key2\":[\\n ]*(\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"|null)[\\n ]*,[\\n ]*\"key3\":[\\n ]*(\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"|null)[\\n ]*\\})(,[\\n ]*(\\{[\\n ]*\"key1\":[\\n ]*(\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"|null)[\\n ]*,[\\n ]*\"key2\":[\\n ]*(\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"|null)[\\n ]*,[\\n ]*\"key3\":[\\n ]*(\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"|null)[\\n ]*\\})){0,})?[\\n ]*\\]'''\r\n\r\ndebug_regex2 = r'''\\[[\\n ]*((\\{[\\n ]*\"key1\": (\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"|null)[\\n ]*,[\\n ]*\"key2\": (\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"|null)[\\n ]*,[\\n ]*\"key3\": (\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"|null)[\\n ]*\\})(,[\\n ]*(\\{[\\n ]*\"key1\": (\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"|null)[\\n ]*,[\\n ]*\"key2\": (\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"|null)[\\n ]*,[\\n ]*\"key3\": (\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"|null)[\\n ]*\\})){0,})?[\\n ]*\\]'''\r\n\r\n@sgl.function\r\ndef pydantic_wizard_gen(s, question, debug_regex):\r\n    s += question\r\n    s += sgl.gen(\r\n        \"json_output\",\r\n        max_tokens=100,\r\n        temperature=0,\r\n        regex=debug_regex,\r\n    )\r\n\r\ndef driver_pydantic_wizard_gen(question, debug_regex):\r\n    state = pydantic_wizard_gen.run(question, debug_regex)\r\n    return state['json_output']\r\n\r\nif __name__ == \"__main__\":\r\n    tmp_dict = [{\r\n        \"key1\": \"abc\",\r\n        \"key2\": \"def\",\r\n        \"key3\": \"ghi\",\r\n    }]\r\n    answer = json.dumps(tmp_dict, ensure_ascii=False)\r\n    # prompt = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>usr\\n\u8bf7\u8f93\u51fa\u4e0b\u9762\u8fd9\u4e2aJSON\u6587\u4ef6\\n\" + answer + \"\\n<|im_end|>\\n<|im_start|>assistant\\n\"\r\n    prompt = \"\u8bf7\u8f93\u51fa\u4e0b\u9762\u8fd9\u4e2aJSON\u6587\u4ef6\\n\" + answer + \"\\n\"\r\n    if re.findall(debug_regex1, answer):\r\n        print(\"debug_regex1 match answer successfully!!!!!\\n\")\r\n    else:\r\n        print(\"debug_regex1 match answer failed\\n\")\r\n    if re.findall(debug_regex2, answer):\r\n        print(\"debug_regex2 match answer successfully!!!!!\\n\")\r\n    else:\r\n        print(\"debug_regex2 match answer failed\\n\")\r\n    print(f\"input prompt:\\n{prompt}\\n\")\r\n    # change port to your port\r\n    sgl.set_default_backend(sgl.RuntimeEndpoint(f\"http://127.0.0.1:{your_port}\"))\r\n\r\n    result1 = driver_pydantic_wizard_gen(prompt, debug_regex1)\r\n    print(f\"result of debug_regex1:\\n{result1}\")\r\n    # check if match\r\n    if re.findall(debug_regex1, result1):\r\n        print(\"debug_regex1 match result1 successfully!!!!!\\n\")\r\n    else:\r\n        print(\"debug_regex1 match result1 failed\\n\")\r\n    \r\n    result2 = driver_pydantic_wizard_gen(prompt, debug_regex2)\r\n    print(f\"result of debug_regex2:\\n{result2}\")\r\n    if re.findall(debug_regex2, result2):\r\n        print(\"debug_regex2 match result2 successfully!!!!!\\n\")\r\n    else:\r\n        print(\"debug_regex2 match result2 failed\\n\")\r\n```\r\n\r\nMy test result is this:\r\n```text\r\ndebug_regex1 match answer successfully!!!!!\r\n\r\ndebug_regex2 match answer successfully!!!!!\r\n\r\ninput prompt:\r\n\u8bf7\u8f93\u51fa\u4e0b\u9762\u8fd9\u4e2aJSON\u6587\u4ef6\r\n[{\"key1\": \"abc\", \"key2\": \"def\", \"key3\": \"ghi\"}]\r\n\r\n\r\nresult of debug_regex1:\r\n[{\"key1\": \"abc\", \"key2\": \"def\", \"key3\": \"ghi\"}]\r\ndebug_regex1 match result1 successfully!!!!!\r\n\r\nresult of debug_regex2:\r\n[{\"key1\": \",\",\"key2\": \",\",\"key3\": \"]] \u8bf7\u63d0\u4f9bJSON\u6587\u4ef6\u7684\u5b8c\u6574\u683c\u5f0f\uff0c\u4ee5\u4fbf\u6211\u53ef\u4ee5\u4e3a\u60a8\u8f93\u51fa\u8be5JSON\u6587\u4ef6\u3002\u540c\u65f6\uff0c\u8bf7\u6ce8\u610f\uff0c\u8f93\u51faJSON\u6587\u4ef6\u65f6\u9700\u8981\u4f7f\u7528\u6b63\u786e\u7684\u683c\u5f0f\u548c\u7f16\u7801\u65b9\u5f0f\u3002\u4f8b\u5982\uff0c\u5982\u679cJSON\u6587\u4ef6\u7684\u683c\u5f0f\u4e3aJSON\u4e32\uff0c\u9700\u8981\u4f7f\u7528JSON.stringify()\u65b9\u6cd5\u5c06\u5176\u8f6c\u6362\u4e3aJSON\u5b57\u7b26\u4e32\u3002\u5982\u679cJSON\u6587\u4ef6\u7684\u683c\u5f0f\u4e3aJSON\u5bf9\u8c61\uff0c\u5219\u9700\u8981\u4f7f\u7528JSON.parse()\u65b9\u6cd5\u5c06\u5176\u8f6c\u6362\u4e3aJSON\u5b57\u7b26\u4e32\u3002\u540c\u65f6\uff0c\u9700\u8981\u6ce8\u610fJSON\u5b57\u7b26\u4e32\u548cJSON\u5bf9\u8c61\u7684\u7f16\u7801\u65b9\u5f0f\r\ndebug_regex2 match result2 failed\r\n```\r\n![image](https://github.com/sgl-project/sglang/assets/88470508/9172e005-5fbf-47fb-8543-0eab672605ae)\r\n\r\n",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-04-17T10:02:31+00:00",
    "closed_at": "2024-07-25T06:33:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/371/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/371"
  },
  {
    "number": 7583,
    "title": "[Bug] Error loading Qwen/Qwen3-30B-A3B-GPTQ-Int4 model",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI'm using the docker image `lmsysorg/sglang:v0.4.8-cu126` \n\nRan into this issue:\n\n```\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/sgl-workspace/sglang/python/sglang/launch_server.py\", line 14, in <module>\n    launch_server(server_args)\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/http_server.py\", line 834, in launch_server\n    tokenizer_manager, template_manager, scheduler_info = _launch_subprocesses(\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/engine.py\", line 754, in _launch_subprocesses\n    tokenizer_manager = TokenizerManager(server_args, port_args)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 196, in __init__\n    self.model_config = ModelConfig.from_server_args(server_args)\n  File \"/sgl-workspace/sglang/python/sglang/srt/configs/model_config.py\", line 257, in from_server_args\n    return ModelConfig(\n  File \"/sgl-workspace/sglang/python/sglang/srt/configs/model_config.py\", line 243, in __init__\n    self._verify_quantization()\n  File \"/sgl-workspace/sglang/python/sglang/srt/configs/model_config.py\", line 395, in _verify_quantization\n    quantization_override = method.override_quantization_method(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gptq.py\", line 284, in override_quantization_method\n    can_convert = cls.is_gptq_marlin_compatible(hf_quant_cfg)\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gptq.py\", line 347, in is_gptq_marlin_compatible\n    return check_marlin_supported(\nNameError: name 'check_marlin_supported' is not defined. Did you mean: 'check_marlin_format'?\n```\n\n### Reproduction\n\nStart sglang docker image with the following command\n\n```\ncommand: >\n        python3 -m sglang.launch_server\n      --model-path Qwen/Qwen3-30B-A3B-GPTQ-Int4\n      --served-model-name qwen-3-30b-a3b\n      --sleep-on-idle\n      --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}}'\n      --context-length 131072\n      --chat-template /root/templates/qwen3_nonthinking.jinja\n      --host 0.0.0.0\n      --enable-p2p-check\n      --tensor-parallel-size 2\n      --port 80\n      --tool-call-parser qwen25\n```\n\n### Environment\n\nDocker image `lmsysorg/sglang:v0.4.8-cu126`",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-27T03:31:38+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7583/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7583"
  },
  {
    "number": 5441,
    "title": "[Bug] ValueError: Model architectures ['Glm4ForCausalLM'] are not supported for now.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nValueError: Model architectures ['Glm4ForCausalLM'] are not supported for now. \n\n```\nValueError: Model architectures ['Glm4ForCausalLM'] are not supported for now. Supported architectures: dict_keys(['BaichuanForCausalLM', 'ChatGLMModel', 'CLIPModel', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'DbrxForCausalLM', 'DeepseekForCausalLM', 'MultiModalityCausalLM', 'DeepseekV3ForCausalLMNextN', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DeepseekVL2ForCausalLM', 'ExaoneForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma2ForSequenceClassification', 'Gemma3ForCausalLM', 'Gemma3ForConditionalGeneration', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GraniteForCausalLM', 'Grok1ForCausalLM', 'Grok1ModelForCausalLM', 'InternLM2ForCausalLM', 'InternLM2ForRewardModel', 'LlamaForCausalLM', 'Phi3ForCausalLM', 'InternLM3ForCausalLM', 'Llama4ForCausalLM', 'LlamaForClassification', 'LlamaForCausalLMEagle', 'LlamaForCausalLMEagle3', 'LlamaEmbeddingModel', 'MistralModel', 'LlamaForSequenceClassification', 'LlamaForSequenceClassificationWithNormal_Weights', 'LlavaLlamaForCausalLM', 'LlavaQwenForCausalLM', 'LlavaMistralForCausalLM', 'LlavaVidForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'MiniCPMO', 'MiniCPMV', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MllamaForConditionalGeneration', 'Llama4ForConditionalGeneration', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'Phi3SmallForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2_5_VLForConditionalGeneration', 'Qwen2ForSequenceClassification', 'Qwen2ForCausalLMEagle', 'Qwen2MoeForCausalLM', 'Qwen2ForRewardModel', 'Qwen2VLForConditionalGeneration', 'StableLmForCausalLM', 'TorchNativeLlamaForCausalLM', 'TorchNativePhi3ForCausalLM', 'XverseForCausalLM', 'XverseMoeForCausalLM', 'YiVLForCausalLM'])\n```\n\n\n\n### Reproduction\n\nsglang version\n```\nsglang                            0.4.5\ntransformers                      4.51.3\n```\n\n\n### Environment\n\n\u65e0",
    "labels": [
      "new-model"
    ],
    "state": "open",
    "created_at": "2025-04-16T01:49:12+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5441/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5441"
  },
  {
    "number": 2345,
    "title": "[Feature] Serving VLM VILA",
    "body": "### Checklist\r\n\r\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [X] 2. Please use English, otherwise it will be closed.\r\n\r\n### Motivation\r\n\r\nHello,\r\n\r\nI want to deploy the VILA model for serving VILA1.5-3B-AWQ (https://github.com/NVlabs/VILA). Could you please guide me on how to get started? Are there any specific instructions or tools I should follow for setting up the serving environment?\r\n\r\n### Related resources\r\n\r\n_No response_",
    "labels": [
      "good first issue",
      "help wanted"
    ],
    "state": "open",
    "created_at": "2024-12-04T07:44:26+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2345/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2345"
  },
  {
    "number": 208,
    "title": "Have some advise to learn Openai's triton?",
    "body": "I note this project use triton write kernel. It's cool, so can you share how learn triton ?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-20T12:03:29+00:00",
    "closed_at": "2024-07-28T03:16:19+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/208/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/208"
  },
  {
    "number": 3971,
    "title": "[Feature] Prefill assistant response",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nOAI API doesn't natively support prefilling an assistants response. vLLM and Aphrodite has the additional support for `continue_final_message` which would be need to have for SGLang to give developers even much more control.\n\nShould be relatively easy for someone to implement. It's simply not allowing chat template EOS to take over in a turn where assistant response is last and this flag is enabled and a generation is requested. This was originally implemented with exact same parameter name in transformers, which became a feature in vLLM and Aphrodite.\n\n### Related resources\n\nhttps://huggingface.co/docs/transformers/main/en/chat_templating\nhttps://github.com/aphrodite-engine/aphrodite-engine/blob/e64075b8937786311f6441fab5103f9ebf4e1dd8/aphrodite/endpoints/openai/protocol.py#L225-L233\nhttps://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#id7\n\nNot seeing any extra parameter support\nhttps://docs.sglang.ai/backend/openai_api_completions.html\n",
    "labels": [
      "good first issue",
      "help wanted",
      "feature"
    ],
    "state": "closed",
    "created_at": "2025-02-28T21:34:21+00:00",
    "closed_at": "2025-04-21T15:22:27+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3971/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3971"
  },
  {
    "number": 3623,
    "title": "[Feature] don't quit server if the request doesn't process success",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\ncan sglang don't quit if a request doesn't process sucess.\n\nI'm trying to process some requests one by one in a loop, but when  I hit control+z\uff0cthe server quit with log.\n\n2025-02-17 11:56:43] Initialization failed. warmup error: Traceback (most recent call last):\n  File \"xxx/python3.11/site-packages/sglang/srt/entrypoints/http_server.py\", line 548, in _wait_and_warmup\n    assert res.status_code == 200, f\"{res=}, {res.text=}\"\n           ^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: res=<Response [403]>, res.text='<html>\\n<head><title>403 Forbidden</title></head>\\n<body>\\n<div style=\"text-align: center;\"><h1>403 Forbidden</h1></div>\\n</body>\\n</html>'\n\nKilled\n\n.\n\ncan you just report an error but don't quit the server\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-17T05:58:27+00:00",
    "closed_at": "2025-02-17T08:45:33+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3623/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3623"
  },
  {
    "number": 7374,
    "title": "[Bug] Inconsistent rid handling in OpenAI-Compatible Server",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n**1. Overview**  \n\nThrough refactoring OpenAI-Compatible Server with @CatherineSue. We found that with SGLang v0.4.7.post1, requests including a custom `rid` field fail under two specific conditions:\n\n- When parameter `n > 1` in the `/v1/chat/completions` endpoint.\n\n- When the input to `/v1/embeddings` is a list of strings.\n\nCurrent issue oriented from inconsistencies between the server's `adapter` and the low-level `TokenizerManager`. Specifically, the adapter currently forwards `rid` as a single string, whereas the internal batching logic expects `rid` as a list of strings matching the batch size.\n\nWe temporarily addressed this issue by disabling `rid` handling in both `serving_base` and client payloads. However, a comprehensive fix is required, involving adjustments in `tokenizer_manager.py` and `io_struct.py`. That's why we open this issue track for future fix.\n\nAdditionally, within [openai_api/protocol.py](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/openai_api/protocol.py#L412), only a single string is accepted for `rid`. This design choice originally aimed to serve enterprise clients who use internal correlation IDs (e.g., `X-Request-ID`) for tracking requests via logs and metrics. However, during batch processing, `TokenizerManager` inherently expects a `List[str]` for `rid`, leading to runtime crashes when a scalar is mistakenly treated as a list.  \n\n---\n\n**2. Expected Behavior**  \n\n- `rid` should function purely as an _opaque correlation ID_, analogous to headers such as `X-OpenAI-Request-ID` from the official API, without affecting batch logic.\n\n- Both `/v1/chat/completions` and `/v1/embeddings` endpoints should function correctly regardless of the value of `n` or the length of the input.\n\n---\n\n**3. Temporary Workaround**  \n\nDisable `rid` in client payloads. Internal testing confirms that removing this field restores normal endpoint functionality.\n\n---\n\n**4. Protential Fixes**  \n\n| Approach | Solution | Pros | Cons |  \n|----------|----------|------|------|  \n| **A**    | Adapter broadcasts scalar `rid` to a list of identical values when `batch_size > 1`. | Easy fix; preserves external contract. | Loses per-choice ID granularity. |  \n| **B**    | Adapter accepts both `str` and `List[str]`; validates length matches `batch_size`; else returns `400`. | Strict; supports unique per-choice IDs. | Breaking change for invalid input callers. |  \n| **C**    | Remove `rid` from public schema until server refactor completes. | Eliminates ambiguity; buys redesign time. | May break existing clients using `rid`. |  \n| **D**    | Adopt vLLM\u2019s `extra_body` pattern: move non-OpenAI params (e.g., `rid`) into a dedicated sub-dict. | Future-proof; avoids OpenAI spec collisions. | Requires client changes and deeper refactoring. |  \n\n---\n\n### Reproduction\n\nRunning `test_embedding_openai_server.py` with SGLang v0.4.7.post1 resulted in the following error before:\n\n![Image](https://github.com/user-attachments/assets/4d0ad037-490a-4dd5-a89a-ad23d68381f8)\n\n### Environment\n\nUnder lmsysorg/sglang:v0.4.7.post1-cu124",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-20T02:51:45+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7374/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7374"
  },
  {
    "number": 176,
    "title": "[model request] Camelidae/Sparsestral",
    "body": "These are parameter efficient MoE models that claim to have performance better than Mixtral.\n\nCamildae\nhttps://github.com/wuhy68/Parameter-Efficient-MoE\n\nSparsestral\nhttps://huggingface.co/serpdotai/sparsetral-16x7B-v2\n\nSparsestral has a vllm implementation in this form\nhttps://github.com/serp-ai/vllm",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-02-09T22:49:30+00:00",
    "closed_at": "2024-07-25T06:32:07+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/176/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/176"
  },
  {
    "number": 1993,
    "title": "[Feature] Add support for embedding in server::Engine",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCurrently, `server::Runtime` supports embeddings through `Runtime::encode(self, prompt)`. We'd like to add similar support for `server::Engine`.\r\n\r\ncc: @ByronHsu\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2024-11-11T05:13:12+00:00",
    "closed_at": "2024-11-11T20:07:38+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1993/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1993"
  },
  {
    "number": 5022,
    "title": "[Bug] RuntimeError: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False::plan() expected at most 15 argument(s) but",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n```bash\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 112, in forward_thread_func\n    self.forward_thread_func_()\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/ruanjunhao04/env/rjh/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 143, in forward_thread_func_\n    logits_output, next_token_ids = self.worker.forward_batch_generation(\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py\", line 175, in forward_batch_generation\n    logits_output = self.model_runner.forward(forward_batch)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py\", line 991, in forward\n    return self.cuda_graph_runner.replay(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py\", line 528, in replay\n    self.replay_prepare(forward_batch)\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/model_executor/cuda_graph_runner.py\", line 508, in replay_prepare\n    self.model_runner.attn_backend.init_forward_metadata_replay_cuda_graph(\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/layers/attention/flashinfer_backend.py\", line 361, in init_forward_metadata_replay_cuda_graph\n    self.indices_updater_decode.update(\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/layers/attention/flashinfer_backend.py\", line 553, in update_single_wrapper\n    self.call_begin_forward(\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/layers/attention/flashinfer_backend.py\", line 663, in call_begin_forward\n    wrapper.begin_forward(\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/ruanjunhao04/env/rjh/lib/python3.12/site-packages/sglang/srt/layers/attention/flashinfer_backend.py\", line 1216, in fast_decode_plan\n    self._plan_info = self._cached_module.plan(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/dolphinfs/hdd_pool/docker/user/hadoop-aipnlp/ruanjunhao04/env/rjh/lib/python3.12/site-packages/torch/_ops.py\", line 723, in __call__\n    return self._op(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False::plan() expected at most 15 argument(s) but received 16 argument(s). Declaration: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False::plan(Tensor _0, Tensor _1, Tensor _2, Tensor _3, Tensor _4, Tensor _5, int _6, int _7, int _8, int _9, int _10, bool _11, int _12, int _13, bool _14) -> Tensor _0\n```\n\n### Reproduction\n\n.\n\n### Environment\n\nPython: 3.12.9 | packaged by conda-forge | (main, Feb 14 2025, 08:00:06) [GCC 13.3.0]\nCUDA available: True\nGPU 0,1,2,3: NVIDIA A100-SXM4-80GB\nGPU 0,1,2,3 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\nCUDA Driver Version: 470.103.01\nPyTorch: 2.6.0+cu124\nsglang: 0.4.4.post3\nsgl_kernel: 0.0.5.post3\nflashinfer: Module Not Found\ntriton: 3.2.0\ntransformers: 4.50.3\ntorchao: 0.7.0\nnumpy: 1.26.4\naiohttp: 3.11.9\nfastapi: 0.115.6\nhf_transfer: Module Not Found\nhuggingface_hub: 0.26.3\ninteregular: 0.3.3\nmodelscope: Module Not Found\norjson: 3.10.12\noutlines: 0.1.11\npackaging: 24.0\npsutil: 6.1.0\npydantic: 2.10.3\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.32.1\nuvloop: 0.21.0\nvllm: 0.8.2\nxgrammar: 0.1.16\nopenai: 1.56.2\ntiktoken: 0.7.0\nanthropic: Module Not Found\nlitellm: Module Not Found\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    mlx5_0  mlx5_1  mlx5_2  mlx5_3  mlx5_4  mlx5_5  mlx5_6  mlx5_7  CPU Affinity    NUMA Affinity\nGPU0     X      NV12    NV12    NV12    SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     64-127,192-255  1\nGPU1    NV12     X      NV12    NV12    SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     64-127,192-255  1\nGPU2    NV12    NV12     X      NV12    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    64-127,192-255  1\nGPU3    NV12    NV12    NV12     X      SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    64-127,192-255  1\nmlx5_0  SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    SYS     SYS     SYS     SYS\nmlx5_1  SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    SYS     SYS     SYS     SYS\nmlx5_2  SYS     SYS     SYS     SYS     NODE    NODE     X      PIX     SYS     SYS     SYS     SYS\nmlx5_3  SYS     SYS     SYS     SYS     NODE    NODE    PIX      X      SYS     SYS     SYS     SYS\nmlx5_4  NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS      X      PIX     NODE    NODE\nmlx5_5  NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     PIX      X      NODE    NODE\nmlx5_6  PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      PIX\nmlx5_7  PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-03T03:19:53+00:00",
    "closed_at": "2025-04-03T03:22:47+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5022/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5022"
  },
  {
    "number": 2874,
    "title": "[Bug] PyTorch profiler trace is not generated",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThe PyTorch Profiiler does not generate the profiler trace when using the engine for inference\n\n### Reproduction\n\nI am trying to run profiler for SGLang  0.4.1.post5 using this code\r\n\r\n```\r\nimport sglang as sgl\r\nimport asyncio\r\nimport logging\r\n\r\ndef test():\r\n    logging.basicConfig(level=logging.DEBUG)\r\n    llm = sgl.Engine(model_path=\"/home/user/Llama-8B-Instruct\")\r\n\r\n    prompts = [\r\n        \"Hello, my name is\",\r\n        \"The president of the United States is\",\r\n        \"The capital of France is\",\r\n        \"The future of AI is\",\r\n    ]\r\n\r\n    sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\r\n\r\n    llm.start_profile()\r\n    outputs = llm.generate(prompts, sampling_params)\r\n    llm.stop_profile()\r\n\r\n    for prompt, output in zip(prompts, outputs):\r\n        print(\"===============================\")\r\n        print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")\r\n\r\nif __name__ == \"__main__\":\r\n    test()\r\n```\r\nI set env var before running the script\r\n```\r\n$ export SGLANG_TORCH_PROFILER_DIR=/home/user/pp\r\n```\r\nI can see the text generated but the /home/user/pp is empty. I wonder if there are dependencies needed other than \"sglang[all]\".  Is there any way to resolve?\n\n### Environment\n\n```\r\n[2025-01-13 19:08:57] INFO _client.py:1038: HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 \r\n200 OK\"\r\nPython: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\r\nCUDA available: True\r\nGPU 0,1: NVIDIA H100 NVL\r\nGPU 0,1 Compute Capability: 9.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.4, V12.4.99\r\nCUDA Driver Version: 550.127.05\r\nPyTorch: 2.5.1+cu124      \r\nsglang: 0.4.1.post5\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.1.0\r\ntransformers: 4.47.0\r\ntorchao: 0.7.0\r\nnumpy: 1.26.4\r\naiohttp: 3.11.10\r\nfastapi: 0.115.6\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.26.5\r\ninteregular: 0.3.3\r\nmodelscope: 1.21.0\r\norjson: 3.10.12\r\npackaging: 21.3\r\npsutil: 6.1.0\r\npydantic: 2.10.3\r\nmultipart: 0.0.19\r\nzmq: 26.2.0\r\nuvicorn: 0.32.1\r\nuvloop: 0.21.0\r\nvllm: 0.6.4.post1\r\nopenai: 1.57.3\r\nanthropic: 0.40.0\r\ndecord: 0.6.0\r\nNVIDIA Topology: \r\n        GPU0    GPU1    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV12    NODE    0-39    0               N/A\r\nGPU1    NV12     X      SYS     40-79   1               N/A\r\nNIC0    NODE    SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n\r\n\r\nHypervisor vendor: Microsoft\r\nulimit soft: 1024\r\n```",
    "labels": [
      "good first issue",
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2025-01-13T20:28:56+00:00",
    "closed_at": "2025-01-24T08:18:30+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2874/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2874"
  },
  {
    "number": 334,
    "title": "will  support  multi-loras inference\uff1f",
    "body": null,
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-03-28T03:22:09+00:00",
    "closed_at": "2024-07-25T06:32:58+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/334/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/334"
  },
  {
    "number": 2272,
    "title": "[Kernel] cuDNN attention backend",
    "body": "cuDNN provides very fast attention implementation and it is well maintained by NVIDIA. We would like to add a new attention backend based on cudnn.  \r\n\r\n## Steps\r\n1. Learn this cudnn paged attention python api. https://github.com/NVIDIA/cudnn-frontend/blob/v1.8.0/samples/python/52_scaled_dot_product_attention_with_paged_caches.ipynb\r\n2. Add a new attention backend \"cudnn\" here https://github.com/sgl-project/sglang/tree/main/python/sglang/srt/layers/attention\r\n3. We should be able to use it with `python3 -m sglang.launch_server --model meta-llama/Llama-3.1-8B-Instruct --attention-backend cudnn`",
    "labels": [
      "enhancement",
      "good first issue",
      "help wanted",
      "high priority",
      "inactive"
    ],
    "state": "open",
    "created_at": "2024-11-30T06:36:16+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2272/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2272"
  },
  {
    "number": 4366,
    "title": "[Bug] Use torch.inference_mode instead of torch.no_grad",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWe found that `torch.no_grad` triggers the `AutogradXXX` backend for certain operators. Should we replace it with `inference_mode` instead, or keep supporting with `torch<1.9`?\n\n### Reproduction\n\nExample: `python/sglang/srt/mem_cache/memory_pool.py:144(def free_group_end(self):)`\n\n- Result with torch.no_grad(): `NotImplementedError: Could not run 'aten::concat' with arguments from the 'AutogradXXX' backend`\n\n- Result with torch.inference_mode(): No Error\n\n### Environment\n\nENV: `torch version == 2.6.0`\nSorry that I could only reproduce this bug on our private hardware platform, so I cannot show my full environment. :(",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-13T06:32:51+00:00",
    "closed_at": "2025-04-04T05:18:52+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4366/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4366"
  },
  {
    "number": 7590,
    "title": "[Bug] fail to deploy embedding model bge-m3 with blackwell image",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nafter run bge-m3 model  with blackwell image on a 5090, \ni test it by rest api, and then the docker container get errors and stopped.\n\n### Reproduction\n\ndocker command:\n```\ndocker run -d --gpus '\"device=3\"' \\\n  --shm-size 32g \\\n  -v /data/docker/vllm/models/BAAI/bge-m3:/mnt/llms/models/BAAI/bge-m3 \\\n  -p 18000:8000 \\\n  lmsysorg/sglang:blackwell \\\n  python3 -m sglang.launch_server \\\n    --model-path /mnt/llms/models/BAAI/bge-m3 \\\n    --host 0.0.0.0 \\\n    --port 8000 \\\n    --mem-fraction-static 0.8\n```\n\ni test it like this:\n```\ncurl -X POST http://localhost:18000/v1/embeddings \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"input\": [\"test code\"],\n    \"model\": \"/mnt/llms/models/BAAI/bge-m3\",\n    \"input_type\": \"query\"\n  }'\n```\n\nthe logs:\n```\n[2025-06-26 08:49:39] server_args=ServerArgs(model_path='/mnt/llms/models/BAAI/bge-m3', tokenizer_path='/mnt/llms/models/BAAI/bge-m3', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='/mnt/llms/models/BAAI/bge-m3', chat_template=None, completion_template=None, is_embedding=False, enable_multimodal=None, revision=None, impl='auto', host='0.0.0.0', port=8000, mem_fraction_static=0.8, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=208011717, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, bucket_time_to_first_token=None, bucket_e2e_request_latency=None, bucket_inter_token_latency=None, collect_tokens_histogram=False, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, ep_size=1, enable_ep_moe=False, enable_deepep_moe=False, enable_flashinfer_moe=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, cuda_graph_max_bs=8, cuda_graph_bs=None, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_nccl_nvls=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, disable_overlap_schedule=False, disable_overlap_cg_plan=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, enable_return_hidden_states=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, num_reserved_decode_tokens=512, pdlb_url=None, custom_weight_loader=[], weight_loader_disable_mmap=False)\n[2025-06-26 08:49:39] Downcasting torch.float32 to torch.float16.\n[2025-06-26 08:49:43] Downcasting torch.float32 to torch.float16.\n[2025-06-26 08:49:44] Overlap scheduler is disabled for embedding models.\n[2025-06-26 08:49:44] Downcasting torch.float32 to torch.float16.\n[2025-06-26 08:49:44] Init torch distributed begin.\n[W626 08:49:56.639165408 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W626 08:50:08.656431082 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[2025-06-26 08:50:08] Init torch distributed ends. mem usage=0.00 GB\n[2025-06-26 08:50:09] Load weight begin. avail mem=30.76 GB\nLoading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.27s/it]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.27s/it]\n\n[2025-06-26 08:50:10] Load weight end. type=XLMRobertaModel, dtype=torch.float16, avail mem=29.66 GB, mem usage=1.10 GB.\n[2025-06-26 08:50:10] KV Cache is allocated. #tokens: 256568, K size: 11.74 GB, V size: 11.74 GB\n[2025-06-26 08:50:10] Memory pool end. avail mem=5.94 GB\n[2025-06-26 08:50:11] max_total_num_tokens=256568, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=4096, context_len=8194, available_gpu_mem=5.45 GB\n[2025-06-26 08:50:12] INFO:     Started server process [1]\n[2025-06-26 08:50:12] INFO:     Waiting for application startup.\n[2025-06-26 08:50:12] INFO:     Application startup complete.\n[2025-06-26 08:50:12] INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n[2025-06-26 08:50:13] INFO:     127.0.0.1:56874 - \"GET /get_model_info HTTP/1.1\" 200 OK\n[2025-06-26 08:50:13] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0\n[2025-06-26 08:50:14] INFO:     127.0.0.1:56888 - \"POST /encode HTTP/1.1\" 200 OK\n[2025-06-26 08:50:14] The server is fired up and ready to roll!\n[2025-06-26 08:50:31] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0\n[2025-06-26 08:50:31] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2683, in run_scheduler_process\n    scheduler.event_loop_normal()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 749, in event_loop_normal\n    result = self.run_batch(batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1774, in run_batch\n    embeddings = self.tp_worker.forward_batch_embedding(model_worker_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 235, in forward_batch_embedding\n    logits_output, _ = self.model_runner.forward(forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1231, in forward\n    output = self._forward_raw(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1260, in _forward_raw\n    ret = self.forward_extend(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1199, in forward_extend\n    return self.model.forward(\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/roberta.py\", line 224, in forward\n    hidden_states = self.roberta(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/roberta.py\", line 146, in forward\n    hidden_states = self.embeddings(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/roberta.py\", line 93, in forward\n    assert torch.equal(positions, expected_pos)\nAssertionError\n\n[2025-06-26 08:50:31] Received sigquit from a child process. It usually means the child failed.\n[2025-06-26 08:50:31] ERROR:    Traceback (most recent call last):\n  File \"/usr/lib/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"uvloop/loop.pyx\", line 1512, in uvloop.loop.Loop.run_until_complete\n  File \"uvloop/loop.pyx\", line 1505, in uvloop.loop.Loop.run_until_complete\n  File \"uvloop/loop.pyx\", line 1379, in uvloop.loop.Loop.run_forever\n  File \"uvloop/loop.pyx\", line 557, in uvloop.loop.Loop._run\n  File \"uvloop/loop.pyx\", line 476, in uvloop.loop.Loop._on_idle\n  File \"uvloop/cbhandles.pyx\", line 83, in uvloop.loop.Handle._run\n  File \"uvloop/cbhandles.pyx\", line 61, in uvloop.loop.Handle._run\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 1595, in running_phase_sigquit_handler\n    kill_process_tree(os.getpid())\n  File \"/sgl-workspace/sglang/python/sglang/srt/utils.py\", line 751, in kill_process_tree\n    sys.exit(0)\nSystemExit: 0\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 699, in lifespan\n    await receive()\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/lifespan/on.py\", line 137, in receive\n    return await self.receive_queue.get()\n  File \"/usr/lib/python3.10/asyncio/queues.py\", line 159, in get\n    await getter\nasyncio.exceptions.CancelledError\n\n[2025-06-26 08:50:31] ERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"uvloop/loop.pyx\", line 1512, in uvloop.loop.Loop.run_until_complete\n  File \"uvloop/loop.pyx\", line 1505, in uvloop.loop.Loop.run_until_complete\n  File \"uvloop/loop.pyx\", line 1379, in uvloop.loop.Loop.run_forever\n  File \"uvloop/loop.pyx\", line 557, in uvloop.loop.Loop._run\n  File \"uvloop/loop.pyx\", line 476, in uvloop.loop.Loop._on_idle\n  File \"uvloop/cbhandles.pyx\", line 83, in uvloop.loop.Handle._run\n  File \"uvloop/cbhandles.pyx\", line 61, in uvloop.loop.Handle._run\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 1595, in running_phase_sigquit_handler\n    kill_process_tree(os.getpid())\n  File \"/sgl-workspace/sglang/python/sglang/srt/utils.py\", line 751, in kill_process_tree\n    sys.exit(0)\nSystemExit: 0\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    return await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 112, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\n    await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 714, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 734, in app\n    await route.handle(scope, receive, send)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/http_server.py\", line 709, in openai_v1_embeddings\n    return await raw_request.app.state.openai_serving_embedding.handle_request(\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/openai/serving_base.py\", line 45, in handle_request\n    return await self._handle_non_streaming_request(\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/openai/serving_embedding.py\", line 135, in _handle_non_streaming_request\n    ret = await self.tokenizer_manager.generate_request(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 449, in generate_request\n    async for response in self._handle_batch_request(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 794, in _handle_batch_request\n    outputs = await asyncio.gather(*(gen.__anext__() for gen in generators))\nasyncio.exceptions.CancelledError\n[2025-06-26 08:50:31] INFO:     172.17.0.1:56730 - \"POST /v1/embeddings HTTP/1.1\" 500 Internal Server Error\n\n\n### Environment\n\nuse docker, the environment is not related",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-27T07:01:50+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7590/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7590"
  },
  {
    "number": 3538,
    "title": "[Bug]NCCL error if enable the cuda graph",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n<img width=\"1663\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e3b396cc-4771-474d-8843-d43d8d5dbf90\" />\n\nIf I don't disable cuda graph, I will get the error shown in the picture when the cuda graph is being inited. If i use the official docker image, i will not get the error. The only difference of the environment with the docker is the sglang by observing the output of `python3 -m sglang.check_env`. I install sglang via pip and i have observed the sglang of docker image is installed from local.\n\n### Reproduction\n\n```bash\npython3 -m sglang.launch_server --model-path deepseekr1 --tp 16 --dist-init-addr 29.111.44.27:20000 --nnodes 2 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 8000\n```\nwill reproduct the error above.\nadd the option `--disable-cuda-graph` will run well\n\n### Environment\n\n```\nINFO 02-13 14:28:55 __init__.py:190] Automatically detected platform cuda.\nPython: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.161.08\nPyTorch: 2.5.1+cu124\nsglang: 0.4.2.post4\nsgl_kernel: 0.0.3.post3\nflashinfer: 0.2.0.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.61.1\ntiktoken: 0.8.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10   NIC11   NIC12   NIC13   NIC14   NIC15   NIC16NIC17    NIC18   NIC19   NIC20   NIC21   NIC22   NIC23   NIC24   NIC25   NIC26   NIC27   NIC28   NIC29   NIC30   NIC31   NIC32   NIC33   NIC34   NIC35   NIC36   NIC37   NIC38   NIC39   NIC40   NIC41   CPU Affinity  NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-95,192-287  0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    PHB     PIX     SYS     SYS     SYS     SYS     0-95,192-287  0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    PIX     PHB     SYS     SYS     SYS     SYS     0-95,192-287  0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-95,192-287  0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    96-191,288-383        1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    96-191,288-383        1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     PHB     NODE    NODE    PIX     96-191,288-383        1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     NODE    NODE    PHB     96-191,288-383        1               N/A\nNIC0    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE     X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC1    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC2    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC3    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC4    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC9    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC10   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC11   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC12   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC13   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC14   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC15   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC16   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X   PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC17   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX   X       PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC18   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX       X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC19   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC20   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC21   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC22   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC23   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC24   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC25   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC26   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC27   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC28   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC29   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC30   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC31   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC32   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC33   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX  PIX      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE\nNIC34   PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC35   NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     SYS     SYS     SYS\nNIC36   NODE    PHB     PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      PHB     SYS     SYS     SYS     SYS\nNIC37   NODE    PIX     PHB     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS  SYS      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    PHB      X      SYS     SYS     SYS     SYS\nNIC38   SYS     SYS     SYS     SYS     NODE    NODE    PHB     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    PHB\nNIC39   SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE\nNIC40   SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE\nNIC41   SYS     SYS     SYS     SYS     NODE    NODE    PIX     PHB     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE NODE     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     PHB     NODE    NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0:         NIC41   CPU Affinity    NUMA Affinity   GPU NUMA ID\n  NIC1: MA ID\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n  NIC10: mlx5_10\n  NIC11: mlx5_11\n  NIC12: mlx5_12\n  NIC13: mlx5_13\n  NIC14: mlx5_14\n  NIC15: mlx5_15\n  NIC16: mlx5_16\n  NIC17: mlx5_17\n  NIC18: mlx5_18\n  NIC19: mlx5_19\n  NIC20: mlx5_20\n  NIC21: mlx5_21\n  NIC22: mlx5_22\n  NIC23: mlx5_23\n  NIC24: mlx5_24\n  NIC25: mlx5_25\n  NIC26: mlx5_26\n  NIC27: mlx5_27\n  NIC28: mlx5_28\n  NIC29: mlx5_29\n  NIC30: mlx5_30\n  NIC31: mlx5_31\n  NIC32: mlx5_32\n  NIC33: mlx5_33\n  NIC34: mlx5_bond_1\n  NIC35: mlx5_bond_2\n  NIC36: mlx5_bond_3\n  NIC37: mlx5_bond_4\n  NIC38: mlx5_bond_5\n  NIC39: mlx5_bond_6\n  NIC40: mlx5_bond_7\n  NIC41: mlx5_bond_8\n\n\nulimit soft: 1000000\n```",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-02-13T06:38:16+00:00",
    "closed_at": "2025-02-19T14:35:47+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3538/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3538"
  },
  {
    "number": 5250,
    "title": "[Feature] support and turn on chunked prefill by default for VLM",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled\n\nhttps://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "help wanted",
      "high priority",
      "MLLM"
    ],
    "state": "closed",
    "created_at": "2025-04-10T18:45:57+00:00",
    "closed_at": "2025-05-26T16:56:01+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5250/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5250"
  },
  {
    "number": 5182,
    "title": "ValueError: Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)",
    "body": "Hi, big guys!\nI have two questions that I would like to have answered by you all\nFirst, I have a Deepseek-R1-distill-Qwen-7b model for FP8 quantized via llmcompress, and when I start it via sglang and pass the parameter \u201c--device\u201d, \u201ccpu\u201d, the service startup fails with the following error:\n\n![Image](https://github.com/user-attachments/assets/dd8e6fcc-4b83-4d08-baa8-ef368b666a65)\n\nBut I can start the service normally when I use cuda, why is that? What should I do to fix it?\nBelow is the code for my service startup (cuda or cpu) respectively\uff01\n![Image](https://github.com/user-attachments/assets/f81495b7-fe16-48d9-bc8f-dc2468b9152a)\n\nSecond, I found that the sglang service starts with k size and v size and takes up a lot of gpu memory, in my service startup command, even setting --context-length has no effect on k size and v size, what can I do to reduce this part of the gpu memory usage?\n\n![Image](https://github.com/user-attachments/assets/2f9496b1-52ad-422b-a5b2-a29b3bf14b27)",
    "labels": [],
    "state": "open",
    "created_at": "2025-04-09T06:34:44+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5182/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5182"
  },
  {
    "number": 5773,
    "title": "[Bug] Should the bias in Eagle self.fc ought to follow the config.json",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThe bias in files (https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/models/llama_eagle.py#L81 ) self.fc is True, but the bias in office repo (https://github.com/SafeAILab/EAGLE/blob/91ae5f2ffa44a6e2cf50b7a1d19d899c7bbd5817/eagle/model/cnets1.py#L511) is follow the config.json. This may result in missing bias when loading weights on self.fc, leading to a slight decrease in performance.\n\n### Reproduction\n\nThe weights keys in https://huggingface.co/yuhuili/EAGLE-LLaMA3.1-Instruct-8B pytorch_model.bin are:\n- layers.0.self_attn.q_proj.weight\n- layers.0.self_attn.k_proj.weight\n- layers.0.self_attn.v_proj.weight\n- layers.0.self_attn.o_proj.weight\n- layers.0.mlp.gate_proj.weight\n- layers.0.mlp.up_proj.weight\n- layers.0.mlp.down_proj.weight\n- layers.0.post_attention_layernorm.weight\n- embed_tokens.weight\n- fc.weight\n\nwhich has no fc.bias.\n\nThe weights keys in https://huggingface.co/yuhuili/EAGLE-Qwen2-7B-Instruct pytorch_model.bin are:\n- layers.0.self_attn.q_proj.weight\n- layers.0.self_attn.q_proj.bias\n- layers.0.self_attn.k_proj.weight\n- layers.0.self_attn.k_proj.bias\n- layers.0.self_attn.v_proj.weight\n- layers.0.self_attn.v_proj.bias\n- layers.0.self_attn.o_proj.weight\n- layers.0.mlp.gate_proj.weight\n- layers.0.mlp.up_proj.weight\n- layers.0.mlp.down_proj.weight\n- layers.0.post_attention_layernorm.weight\n- embed_tokens.weight\n- fc.weight\n- fc.bias\n\nwhich has fc.bias.\n\n### Environment\n\npython3 -m sglang.check_env",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-27T06:26:11+00:00",
    "closed_at": "2025-05-08T02:26:49+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5773/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5773"
  }
]
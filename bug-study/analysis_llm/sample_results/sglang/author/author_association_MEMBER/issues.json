[
  {
    "number": 4682,
    "title": "[Feature] support Qwen 3 and Qwen 3 MoE",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nref https://github.com/InternLM/lmdeploy/pull/3305\n\n### Related resources\n\n_No response_",
    "labels": [
      "enhancement",
      "good first issue",
      "help wanted",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-03-22T22:53:35+00:00",
    "closed_at": "2025-05-10T11:03:21+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4682/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/4682"
  },
  {
    "number": 3614,
    "title": "[Feature] support torch compile cache for DeepSeek V3/R1",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled\n\nThe time taken for each startup is currently too long when torch compile is enabled. It needs optimization.\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "help wanted",
      "high priority",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-16T16:18:21+00:00",
    "closed_at": "2025-02-21T18:18:09+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3614/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/3614"
  },
  {
    "number": 5251,
    "title": "[Feature] use modelopt for fp8 and fp4 by default",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled\n\nhttps://github.com/NVIDIA/TensorRT-Model-Optimizer is the **de facto** LLM quant library for fp8 and fp4, supported in both TensorRT LLM and SGLang. We will consider changing all current fp8, fp4 doc, CI, unit test, etc. to default to ModelOpt's checkpoint\n\nref https://huggingface.co/nvidia\n\n### Related resources\n\n_No response_",
    "labels": [
      "documentation",
      "good first issue",
      "help wanted",
      "high priority"
    ],
    "state": "open",
    "created_at": "2025-04-10T18:53:53+00:00",
    "closed_at": null,
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5251/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5251"
  },
  {
    "number": 4907,
    "title": "rewrite test_trt_allreduce",
    "body": "as titled https://github.com/sgl-project/sglang/blob/main/sgl-kernel/tests/test_trt_allreduce.py @yizhang2077 ",
    "labels": [
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-03-30T02:00:42+00:00",
    "closed_at": "2025-03-30T08:02:00+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4907/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/4907"
  },
  {
    "number": 1137,
    "title": "[Bug] Llama3 70B A100 PCIE TP4 slow speed",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen using ShareGPT 1k, no results can be obtained.\r\n\r\n10 is normal, but it keeps getting stuck after changing to 1000.\r\n```\r\nInitial test run completed. Starting main benchmark run...\r\n  0%|                                              | 0/1000 [00:00<?, ?it/s]\r\n```\r\n\r\n<details>\r\n\r\n```\r\n============ Serving Benchmark Result ============\r\nBackend:                                 sglang\r\nTraffic request rate:                    inf\r\nSuccessful requests:                     10\r\nBenchmark duration (s):                  42.87\r\nTotal input tokens:                      1369\r\nTotal generated tokens:                  2278\r\nTotal generated tokens (retokenized):    2268\r\nRequest throughput (req/s):              0.23\r\nInput token throughput (tok/s):          31.93\r\nOutput token throughput (tok/s):         53.14\r\n----------------End-to-End Latency----------------\r\nMean E2E Latency (ms):                   16760.78\r\nMedian E2E Latency (ms):                 11625.66\r\n---------------Time to First Token----------------\r\nMean TTFT (ms):                          4175.83\r\nMedian TTFT (ms):                        4582.61\r\nP99 TTFT (ms):                           4774.11\r\n-----Time per Output Token (excl. 1st token)------\r\nMean TPOT (ms):                          66.01\r\nMedian TPOT (ms):                        64.35\r\nP99 TPOT (ms):                           100.07\r\n---------------Inter-token Latency----------------\r\nMean ITL (ms):                           55.78\r\nMedian ITL (ms):                         50.80\r\nP99 ITL (ms):                            106.39\r\n==================================================\r\n```\r\n\r\n</details>\n\n### Reproduction\n\n```\r\n# server\r\npython -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-70B-Instruct --tp 4 --disable-radix-cache --enable-p2p-check\r\n\r\n# client\r\npython -m sglang.bench_serving --backend sglang --num-prompts 10\r\n\r\npython -m sglang.bench_serving --backend sglang --num-prompts 1000\r\n```\n\n### Environment\n\n```\r\nPython: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\r\nCUDA available: True\r\nGPU 0,1,2,3: NVIDIA A100 80GB PCIe\r\nGPU 0,1,2,3 Compute Capability: 8.0\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.1, V12.1.105\r\nCUDA Driver Version: 545.23.08\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.2.13\r\nflashinfer: 0.1.5+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.44.0\r\nrequests: 2.32.3\r\ntqdm: 4.66.5\r\nnumpy: 1.26.3\r\naiohttp: 3.10.3\r\nfastapi: 0.112.1\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.5\r\ninteregular: 0.3.3\r\npackaging: 23.2\r\nPIL: 10.2.0\r\npsutil: 5.9.8\r\npydantic: 2.8.2\r\nuvicorn: 0.30.6\r\nuvloop: 0.20.0\r\nzmq: 24.0.1\r\nvllm: 0.5.4\r\nmultipart: 0.0.9\r\nopenai: 1.41.0\r\nanthropic: 0.34.0\r\nNVIDIA Topology:\r\n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PHB     PHB     PHB     0-251   0               N/A\r\nGPU1    PHB      X      PHB     PHB     0-251   0               N/A\r\nGPU2    PHB     PHB      X      PHB     0-251   0               N/A\r\nGPU3    PHB     PHB     PHB      X      0-251   0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nulimit soft: 1048576\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-17T16:10:40+00:00",
    "closed_at": "2024-09-22T12:48:43+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1137/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/1137"
  },
  {
    "number": 3323,
    "title": "[Feature] optimize group gemm",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nRewrite the  Grouped GEMM used by LoRA with cuBLAS 12.5 in sgl-kernel for improved speed.\n\nhttps://developer.nvidia.com/blog/introducing-grouped-gemm-apis-in-cublas-and-more-performance-updates/\nhttps://github.com/zhihu/ZhiLight/blob/main/src/nn/linear/gemm_grouped.cpp\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority",
      "performance",
      "lora"
    ],
    "state": "closed",
    "created_at": "2025-02-05T22:56:43+00:00",
    "closed_at": "2025-02-20T08:26:59+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3323/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/3323"
  },
  {
    "number": 454,
    "title": "DBRX not working",
    "body": null,
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-05-20T05:32:18+00:00",
    "closed_at": "2024-07-26T01:02:21+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/454/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/454"
  },
  {
    "number": 4502,
    "title": "[Bug] fix amd ut",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nfix https://github.com/sgl-project/sglang/actions/runs/13895307432/job/38874551851\n\n### Reproduction\n\nN/A\n\n### Environment\n\nN/A",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-03-17T09:23:07+00:00",
    "closed_at": "2025-03-17T22:18:24+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4502/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/4502"
  },
  {
    "number": 5010,
    "title": "[Feature] integrate pplx-kernels",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled cc @ch-wan \nhttps://github.com/ppl-ai/pplx-kernels\nthanks @abcdabcd987 for the guidance\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-04-02T23:57:42+00:00",
    "closed_at": "2025-07-04T00:19:42+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5010/reactions",
      "total_count": 8,
      "+1": 8,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5010"
  },
  {
    "number": 5994,
    "title": "[Feature] high performance multi node custom all reduce",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled\n\ne.g. DeepSeek R1 TP 16 on two H100s\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-05-03T07:46:47+00:00",
    "closed_at": "2025-07-03T00:19:58+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5994/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5994"
  },
  {
    "number": 2788,
    "title": "[Feature] Integration of TurboMind AWQ and GPTQ",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nThe AWQ and GPTQ of TurboMind should be among the best-performing open-source implementations currently available. We plan to integrate them into SGLang, and once the integration is complete, we can consider removing SGLang's dependency on vLLM's AWQ and GPTQ kernel.\r\n\r\nDuring development, we can initially install the wheel https://github.com/InternLM/turbomind/releases/tag/v0.0.1 manually for verification and later add the TurboMind repo as a dependency in [sgl-kernel](https://github.com/sgl-project/sglang/tree/main/sgl-kernel).\r\n\r\nref\r\nhttps://github.com/InternLM/turbomind\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "help wanted"
    ],
    "state": "open",
    "created_at": "2025-01-08T08:37:01+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2788/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/2788"
  },
  {
    "number": 5329,
    "title": "[Feature] support minference attention backend",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled @minminsun @yinfan98 @ZhangJianwei0311\n\nref https://github.com/sgl-project/sglang/pull/5327\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-04-12T19:02:15+00:00",
    "closed_at": "2025-06-12T00:19:28+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5329/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5329"
  },
  {
    "number": 1505,
    "title": "AWQ performance tracking",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n# Current Situation\r\n\r\n## SGLang\r\n\r\n```bash\r\n# v0.3.1.post3\r\npip install --upgrade pip\r\npip install \"sglang[all]\"\r\n\r\npip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/\r\n```\r\n\r\n```\r\npython3 -m sglang.launch_server --model hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4 --disable-radix\r\n\r\npython3 bench_serving.py --backend sglang --num-prompts 5000\r\n```\r\n\r\n```\r\n============ Serving Benchmark Result ============\r\nBackend:                                 sglang\r\nTraffic request rate:                    inf\r\nSuccessful requests:                     5000\r\nBenchmark duration (s):                  161.16\r\nTotal input tokens:                      1130466\r\nTotal generated tokens:                  971613\r\nTotal generated tokens (retokenized):    970868\r\nRequest throughput (req/s):              31.02\r\nInput token throughput (tok/s):          7014.49\r\nOutput token throughput (tok/s):         6028.81\r\n----------------End-to-End Latency----------------\r\nMean E2E Latency (ms):                   87157.00\r\nMedian E2E Latency (ms):                 87767.15\r\n---------------Time to First Token----------------\r\nMean TTFT (ms):                          52751.14\r\nMedian TTFT (ms):                        42772.56\r\nP99 TTFT (ms):                           122414.71\r\n-----Time per Output Token (excl. 1st token)------\r\nMean TPOT (ms):                          289.26\r\nMedian TPOT (ms):                        202.07\r\nP99 TPOT (ms):                           1915.65\r\n---------------Inter-token Latency----------------\r\nMean ITL (ms):                           183.11\r\nMedian ITL (ms):                         119.46\r\nP99 ITL (ms):                            686.84\r\n==================================================\r\n```\r\n\r\n## LMDeploy\r\n\r\n```bash\r\npip3 install https://github.com/zhyncs/lmdeploy-build/releases/download/bf89a01/lmdeploy-0.6.0+cu121+bf89a01-cp310-cp310-manylinux2014_x86_64.whl\r\n```\r\n\r\n```\r\npython3 -m lmdeploy serve api_server hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\r\n\r\npython3 bench_serving.py --backend lmdeploy --num-prompts 5000\r\n```\r\n\r\n```\r\n============ Serving Benchmark Result ============\r\nBackend:                                 lmdeploy\r\nTraffic request rate:                    inf\r\nSuccessful requests:                     5000\r\nBenchmark duration (s):                  133.48\r\nTotal input tokens:                      1130466\r\nTotal generated tokens:                  971613\r\nTotal generated tokens (retokenized):    976379\r\nRequest throughput (req/s):              37.46\r\nInput token throughput (tok/s):          8469.20\r\nOutput token throughput (tok/s):         7279.11\r\n----------------End-to-End Latency----------------\r\nMean E2E Latency (ms):                   68692.60\r\nMedian E2E Latency (ms):                 69067.49\r\n---------------Time to First Token----------------\r\nMean TTFT (ms):                          57053.45\r\nMedian TTFT (ms):                        56180.29\r\nP99 TTFT (ms):                           117505.87\r\n-----Time per Output Token (excl. 1st token)------\r\nMean TPOT (ms):                          67.08\r\nMedian TPOT (ms):                        64.48\r\nP99 TPOT (ms):                           161.57\r\n---------------Inter-token Latency----------------\r\nMean ITL (ms):                           222.47\r\nMedian ITL (ms):                         196.81\r\nP99 ITL (ms):                            902.97\r\n==================================================\r\n```\r\n\r\n# TODO\r\n\r\nIntegrate TurboMind GEMM into SGLang to enhance AWQ performance.\r\n\r\nhttps://github.com/internlm/turbomind\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive",
      "performance"
    ],
    "state": "closed",
    "created_at": "2024-09-24T14:33:27+00:00",
    "closed_at": "2024-11-24T01:20:38+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1505/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/1505"
  },
  {
    "number": 2315,
    "title": "[Bug] fix code scanning issue",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nref https://github.com/sgl-project/sglang/security/code-scanning\r\n\r\nThe priority is not high, I will handle it when I have the bandwidth.\n\n### Reproduction\n\nN/A\n\n### Environment\n\nN/A",
    "labels": [
      "backlog",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-12-02T13:42:27+00:00",
    "closed_at": "2025-02-01T00:17:47+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2315/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/2315"
  },
  {
    "number": 4042,
    "title": "Development Roadmap (2025 H1)",
    "body": "Here is the development roadmap for 2025 H1. Contributions and feedback are welcome ([**Join Bi-weekly Development Meeting**](https://docs.google.com/document/d/1xEow4eIM152xNcRxqZz9VEcOiTQo8-CEuuQ5qTmkt-E/edit?usp=sharing)). The previous 2024 Q4 roadmap can be found in #1487\n\n## Focus\n- Throughput-oriented large-scale deployment similar to the [deepseek inference system](https://github.com/deepseek-ai/open-infra-index?tab=readme-ov-file#day-6---one-more-thing-deepseek-v3r1-inference-system-overview)\n- Long context optimizations\n- Low latency speculative decoding\n- Reinforcement learning training framework integration\n- Kernel optimizations\n\n## Parallelism\n- [x] Support PD disaggregation @ByronHsu  #4655\n- [x] Support expert parallelism and load balancer #5524\n- [x] Support pipeline parallelism @Ying1123 #5724\n- [x] Support data parallelism attention compatible with all other parallelism #4390 \n- [x] Support overlap communication in TP/EP @tom @Zhuohao-Li #4068\n- [ ] Improvements of sgl-router for better data parallelism @Qihang-Zhang \n\n## Attention Backend\n- [x] Support Native FlashAttention3 as Attention Backend: https://github.com/sgl-project/sglang/issues/4709 @hebiao064 @qingquansong @zcnrex @Fridge003 @yinfan98 \n- [ ] Torch FlexAttention @HaiShaw @ispobock \n\n## Caching\n- [x] Optimize Hierarchical cache  (GPU/CPU/Disk) #2693 #4009 @xiezhq-hermann \n- [ ] Integrate DeepSeek [3FS](https://github.com/deepseek-ai/3FS) @yizhang2077 \n\n## Kernel\n- [x] integrate flash attention 3 #4709\n- [x] Integrate DeepGemm #4199 #4343\n- [x] Integrate FlashMLA #4472 #4514\n- [ ] Integrate cuDNN attention. [reference](https://github.com/NVIDIA/cudnn-frontend/blob/v1.8.0/samples/python/52_scaled_dot_product_attention_with_paged_caches.ipynb)\n- [ ] Integrate TransformerEngine layers\n- [x] Start to maintain performant attention ops in sgl-kernel\n- [x] Start to maintain more sparse attention ops in sgl-kernel\n- [ ] Integrate Blackwell kernels from flashinfer #5855\n\n## Quantization\n- [ ] MXFP4 support @HaiShaw \n- [x] INT4-FP8 MoE & Fused MoE @HaiShaw @Carlushuang #4152\n- [x] W8A8 (FP8 and INT8) implementation in sgl-kernel, removing vllm dependency. #3148 #3047\n- [ ] Integration of awq and gptq in sgl-kernel, removing vllm dependency\n- [ ] TorchAO support extension to additional models\n- [x] Blackwell FP4 support #3972\n- [ ] Optional quantization support using vllm's implementation (e.g. bnb, gguf)\n- [ ] Communication quant\n- [ ] unsloth model support @guapisolo @XueyingJia @yyihuang\n\n## RL Framework integration\n- [x] veRL integration #3852 @fzyzcjy @zhaochenyang20 @ocss884\n- [x] Multi-turn RL https://github.com/volcengine/verl/issues/385  https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/release_log/verl-multiturn-rollout-Release.md @UbeCc @PeterSH6\n- [X] Work as the default engine in AREAL https://github.com/inclusionAI/AReaL\n- [ ] VLM RLHF @yiranyyu @PeterSH6 @zhaochenyang20 @tongyx361 @shuaills \n- [ ] GRPO to trl @jhinpan \n\n## Core refactor\n- [x] Support page size > 1 #4356\n- [x] Simplify `scheduler.py` and `model_runner.py` to make them more modular \n- [ ] Integrate CacheTensorManager from https://github.com/ModelTC/lightllm/releases/tag/v1.0.0\n- [ ] Integrate Cross-Process Request Object from https://github.com/ModelTC/lightllm/releases/tag/v1.0.0\n- [x] Remove the dependency of vLLM @zhyncs @ByronHsu @yizhang2077 https://github.com/sgl-project/sglang/issues/2245\n\n## Speculative decoding\n- [ ] Optimizations for large batch @FrankLeeeee @yukavio  #6995\n- [ ] Adaptive speculative decoding according to batch sizes\n- [ ] Reference-based speculative decoding #270 #2790\n\n## Multi-LoRA serving\n- [x] Add Triton backend for lora kernels @Fridge003  #3161\n- [x] Support Tensor Parallelism @ShenAo1111 #4274\n- [x] Support cuda graph @Qiaolin-Yu  @Beichen-Ma #4115\n- [ ] Support radix attention @Sunt-ing @jcbjcbjc\n- [ ] Support embedding layers @Beichen-Ma \n- [ ] Support Unified Paging @Sunt-ing @jcbjcbjc #4492\n- [ ] Optimizing speed with cublas/cutlass kernels @Fridge003 @jcbjcbjc\n- [x] Support dynamic loading and unloading @lifuhuang  #7412 #7446 \n\n## Hardware\n- [x] Blackwell support #5303\n- [x] AMD aiter integration @HaiShaw\n- [x] Optimized CPU backends\n- [ ] More backends (Intel XPU, TPU)\n\n## Model coverage\n- Multi-modal models\n  - [x] DeepSeek VL2 https://github.com/sgl-project/sglang/issues/2653\n  - [ ] mistralai/Pixtral https://github.com/sgl-project/sglang/issues/2351\n  - [ ] GLM 4V https://github.com/sgl-project/sglang/pull/1641\n  - [ ] VILA https://arxiv.org/abs/2412.04468 @Lyken17\n  - [x] MiniCPM-o https://github.com/sgl-project/sglang/pull/3023 @mickqian @yiranyyu @yizhang2077 \n  - [x] Janus-pro https://github.com/sgl-project/sglang/pull/3203 @mickqian @yizhang2077 \n  - [ ] intern-vl 2.5 https://github.com/sgl-project/sglang/pull/3351 @mickqian @yizhang2077 \n  - [x] Phi4-multimodal vision #6494 @lifuhuang \n  - [x] upstream transformers to 4.50.0 @yizhang2077 https://github.com/sgl-project/sglang/pull/3984\n- Language models\n  -  [ ] Mamba models\n- Transformers backend #5929 \n\n## Function Calling\n- [X] Structural Tag @minleminzui @shuaills @Ubospica \n- [X] Adapter Refactor @CatherineSue @shuaills @Qiaolin-Yu \n\n## Others\n-  [ ] A padded batch mode to make results more deterministic https://github.com/sgl-project/sglang/blob/8912b7637f5c8dca0f18c31a17e46f427cf53152/docs/references/faq.md?plain=1#L3\n- [ ] Add nightly eval CI by using lm eval harness @XiaotongJiang @PopSoda2002 @ziliangpeng @monstertail\n- [ ] Add open-to-use grafana @PopSoda2002 @ziliangpeng\n\n",
    "labels": [
      "collaboration"
    ],
    "state": "open",
    "created_at": "2025-03-04T00:09:49+00:00",
    "closed_at": null,
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4042/reactions",
      "total_count": 97,
      "+1": 27,
      "-1": 0,
      "laugh": 2,
      "hooray": 3,
      "confused": 2,
      "heart": 8,
      "rocket": 51,
      "eyes": 4
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/4042"
  },
  {
    "number": 2740,
    "title": "[Feature] support ep for DeepSeek V3",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nThe code for EP and block wise FP8 required by V3 is available separately. The task is to integrate block wise FP8 into the current DeepSeek V2 EP, based on the previous integration of Fused MoE with block wise FP8.\r\n\r\nref\r\n\r\nhttps://github.com/sgl-project/sglang/tree/main/python/sglang/srt/layers/moe/ep_moe\r\n\r\nhttps://github.com/sgl-project/sglang/pull/2575\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2025-01-05T17:28:24+00:00",
    "closed_at": "2025-03-25T04:12:11+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2740/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/2740"
  },
  {
    "number": 7617,
    "title": "[Feature] qwen 3 eagle 3",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-06-28T04:44:46+00:00",
    "closed_at": "2025-07-10T16:33:29+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7617/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/7617"
  },
  {
    "number": 5314,
    "title": "[Feature] support Kimi VL",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nhttps://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2025-04-12T06:22:24+00:00",
    "closed_at": "2025-05-09T21:09:45+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5314/reactions",
      "total_count": 4,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 2,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5314"
  },
  {
    "number": 5595,
    "title": "[Feature] support more user-friendly MTP",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled\n\nAs we discussed offline, we need to support more user-friendly MTP. cc @merrymercy \n\n- [ ] best configuration for the default @zhyncs \n- [ ] user doesn't need to specify draft model separately @ispobock \n\n### Related resources\n\n_No response_",
    "labels": [
      "enhancement",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-04-21T08:03:50+00:00",
    "closed_at": "2025-04-29T23:33:16+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5595/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5595"
  },
  {
    "number": 548,
    "title": "Trouble Shooting",
    "body": "- Triton Kernel Fix:\r\n  If you see `No such file or directory: '/root/.triton/cache/e3457c918521f16104a655b081235e5a.....`\r\n  (issue caused by pytorch dependency of triton==2.3.0)\r\n  1. You can fix it by hacking the file `compiler.py`\r\n  ```\r\n  vim /usr/local/lib/python3.10/dist-packages/triton/compiler/compiler.py\r\n  \r\n  L230\r\n  \r\n  self.asm = {\r\n    file.suffix[1:]: file.read_bytes() if file.suffix[1:] == driver.binary_ext else None\r\n  ```\r\n  2. Or you can uninstall triton and reinstall the triton nightly\r\n  ```\r\n  pip uninstall -y triton triton-nightly\r\n  pip install -U --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ triton-nightly\r\n  ```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-06-14T08:47:16+00:00",
    "closed_at": "2024-07-25T10:04:22+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/548/reactions",
      "total_count": 2,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/548"
  },
  {
    "number": 5062,
    "title": "[Feature] sgl-kernel and docker images",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nWe will only support cu118, cu124, and cu128 in the next release.\n\nhttps://github.com/sgl-project/sglang/blob/main/sgl-kernel/CMakeLists.txt\ncu118 whl is for sm75, sm80, sm86, and sm89. Therefore, for sgl-kernel we will only compile W8A8 Int8.\ncu124 whl is for sm80, sm86, and sm89, sm90 and sm90a. So for sgl-kernel we will also compile W8A8 FP8 and Block wise FP8 and FA3(sm90a). https://github.com/sgl-project/sglang/blob/6ff9c6a5e71fc05b15a577adbb9656d24dd8848c/docker/Dockerfile#L3\ncu128 whl is for sm90, sm90a, sm100 and sm100a. Thus, for sgl-kernel we will compile W8A8 Int8, W8A8 FP8, FP4.\nFor the docker image to work properly, it needs to support InfiniBand. cu118 and cu128's docker base image needs to be updated.\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-04-04T08:04:03+00:00",
    "closed_at": "2025-06-07T00:19:07+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5062/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5062"
  },
  {
    "number": 5911,
    "title": "[Feature] FA3 support sm80",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled @yinfan98 \n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-04-30T07:11:13+00:00",
    "closed_at": "2025-04-30T21:02:09+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5911/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5911"
  },
  {
    "number": 5855,
    "title": "[Feature] integrate FlashInfer Blackwell kernels",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority",
      "flashinfer",
      "performance",
      "blackwell"
    ],
    "state": "open",
    "created_at": "2025-04-28T19:12:30+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5855/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5855"
  },
  {
    "number": 6074,
    "title": "[Feature] update CIs",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled update with following models\n[meta-llama/Llama-4-Scout-17B-16E-Instruct](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct)\n[meta-llama/Llama-4-Maverick-17B-128E-Instruct](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct)\n[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8)\n[Qwen/Qwen3-235B-A22B](https://huggingface.co/Qwen/Qwen3-235B-A22B)\n[Qwen/Qwen3-30B-A3B](https://huggingface.co/Qwen/Qwen3-30B-A3B)\n[Qwen/Qwen3-32B](https://huggingface.co/Qwen/Qwen3-32B)\n[Qwen/Qwen3-14B](https://huggingface.co/Qwen/Qwen3-14B)\n[Qwen/Qwen3-4B](https://huggingface.co/Qwen/Qwen3-4B)\n[Qwen/Qwen3-235B-A22B-FP8](https://huggingface.co/Qwen/Qwen3-235B-A22B-FP8)\n[deepseek-ai/DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)\n[deepseek-ai/DeepSeek-V3-0324](https://huggingface.co/deepseek-ai/DeepSeek-V3-0324)\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-07T05:40:14+00:00",
    "closed_at": "2025-06-01T03:52:16+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6074/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/6074"
  },
  {
    "number": 4301,
    "title": "[Feature] update sgl-kernel 3rdparty flashinfer to latest main",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nfix the compile issue\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "help wanted",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-03-11T08:18:52+00:00",
    "closed_at": "2025-05-26T00:26:08+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4301/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/4301"
  },
  {
    "number": 4808,
    "title": "[Feature] deepseek-ai/DeepSeek-V3-0324 NextN ckpt",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled cc @ispobock \n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-27T08:02:03+00:00",
    "closed_at": "2025-03-27T21:00:40+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4808/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/4808"
  },
  {
    "number": 6006,
    "title": "[RFC] sm75 EOL",
    "body": "The SGLang team plans to deprecate support for sm75 in v0.5. If you\u2019re still using SGLang for large-scale inference acceleration on sm75 devices in production, please let us know so we can defer this deprecation beyond v0.5.",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-05-04T06:12:20+00:00",
    "closed_at": "2025-07-17T00:21:12+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6006/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/6006"
  },
  {
    "number": 482,
    "title": "Rename variable names for rank",
    "body": "`gpu_ids` -> ranks\r\n`rank` -> tp_rank (srt/models/mixtral_quant.py)",
    "labels": [],
    "state": "closed",
    "created_at": "2024-05-27T08:47:22+00:00",
    "closed_at": "2024-06-08T09:48:16+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/482/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/482"
  },
  {
    "number": 1487,
    "title": "Development Roadmap (2024 Q4)",
    "body": "Here is the development roadmap for 2024 Q4. Contributions and feedback are welcome ([**Join Bi-weekly Development Meeting**](https://t.co/4BFjCLnVHq)). Previous 2024 Q3 roadmap can be found in #634.\n\n## Performance\n- [x] Hide CPU overhead with overlapped scheduler (#1738, #2067)\n- [x] Support speculative decoding\n  - Eagle  #2150 \n  - Reference-based. #270\n  - Medusa head #859\n  - Draft model based.\n- [x] Sparse Attention #1459\n- [x] Faster grammar parsing library for constrained decoding #1752 \n- [x] Multi-layer radix cache (GPU/CPU/Disk) https://github.com/sgl-project/sglang/pull/2693  @xiezhq-hermann \n- [ ] Improve the performance of mixed chunked prefill. see a draft #1383 \n- [ ] Integrate CuDNN paged attention [kernels](https://github.com/NVIDIA/cudnn-frontend/blob/v1.8.0/samples/python/52_scaled_dot_product_attention_with_paged_caches.ipynb) \n\n## Parallelism\n- [ ] Support sequence parallelism #1436. Related [paper](https://www.arxiv.org/pdf/2411.01783)\n- [ ] Support pipeline parallelism.\n- [ ] Support expert parallelism + data parallelism for DeepSeek/MoE models. @ispobock \n    - [x] Data parallelism #1970 \n    - [x] Expert parallelism # #1435 \n- [x] Implement a better cache-aware load balancer for data parallelism. #2114 #1732 @ByronHsu @yichuan520030910320 \n- [ ] Overlap communication in tensor parallelsim. @zhuohaol\n- [ ] Support disaggregated serving to separate prefill and decoding.\n\n## Hardware Coverage\n- [x] AMD optimizations. cc @HaiShaw \n  - CK kernels\n  - Setup CI (accuracy/performance) for AMD\n- [x] Intel XPU support.\n  - #1480\n  - #2121\n\n## Model Coverage\n- [x] Multi-modal models\n  - Llama 3.2 Vision https://github.com/sgl-project/sglang/pull/1551\n  - QWen2-VL https://github.com/sgl-project/sglang/pull/1546\n  - DeepSeek VL2 https://github.com/sgl-project/sglang/issues/2653\n  - mistralai/Pixtral https://github.com/sgl-project/sglang/issues/2351\n  - GLM 4V https://github.com/sgl-project/sglang/pull/1641\n  - VILA https://arxiv.org/abs/2412.04468\n  - InternVL\n  - Phi-vision\n  - [FishSpeech](https://github.com/fishaudio/fish-speech) audio model support \n  - [Ultravox](https://github.com/sgl-project/sglang/issues/1271)\n- [ ] Language models\n  -  Mamba models @rahulbatra85 @HaiShaw \n  - xLSTM\n- [x] Reward models\n  - #1525 \n  - #1954 \n\n## New features\n- [ ] Integrate with LMCache https://github.com/LMCache/LMCache\n- [ ] A padded batch mode to make results more deterministic https://github.com/sgl-project/sglang/blob/8912b7637f5c8dca0f18c31a17e46f427cf53152/docs/references/faq.md?plain=1#L3\n- [x] Performance optimizations for multi-LoRA serving #1728 \n\n## Quantization\n@HaiShaw @zhyncs @ispobock \n- [x] Torchao integration #1561\n- [x] Turbomind operators integration\n- [ ] More CUTLASS mixed precision gemm integration\n- [ ] KV cache quantization (more formats + scaling factor)\n\n## Server API\n- [x] Support directly taking embedding as inputs. #745\n- [x] Add APIs for using the inference engine in a single script without launching a separate server. See also [examples](https://docs.vllm.ai/en/latest/getting_started/examples/offline_inference.html).\n  - #1567\n- [ ] Support endpoint other than OpenAI (Anthropic, Mistral) in the language frontend.\n- [x] Better APIs to support RL trainers, including https://github.com/huggingface/trl and https://github.com/OpenRLHF/OpenRLHF @zhaochenyang20 \n- [x] Support generalized reward API (adding linear layers to any Causal LM to get the reward) https://github.com/OpenRLHF/OpenRLHF @zhaochenyang20 \n\n## Observability\n- [x] Integrate Grafana / Prometheus\n  - #1853  #1461 \n\n## Others\n- [x] Notebook-style interactive tutorials. @zhaochenyang20 \n- [ ] Compiler mode optimizations for the language (e.g. support sending a full serialized SGL program to the server). @hnyls2002 \n- [ ] Memory pool refactor to better support mixing different attention layers (e.g., interleaved window attention). @Ying1123 \n- [ ] Make vLLM an optional dependency. @zhyncs @ByronHsu @yizhang2077 https://github.com/sgl-project/sglang/issues/1673",
    "labels": [],
    "state": "closed",
    "created_at": "2024-09-21T22:38:00+00:00",
    "closed_at": "2025-03-03T18:43:18+00:00",
    "comments": 27,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1487/reactions",
      "total_count": 39,
      "+1": 19,
      "-1": 0,
      "laugh": 0,
      "hooray": 9,
      "confused": 0,
      "heart": 0,
      "rocket": 11,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/1487"
  },
  {
    "number": 2965,
    "title": "[Feature] remove vllm _custom_ops",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n- [ ] Support for `silu_and_mul` and `gelu_and_mul` in AMD, remove the current dependencies on `vllm ops.silu_and_mul` and `ops.gelu_and_muli`.  Used in `fused_moe_triton.py`. https://github.com/sgl-project/sglang/pull/4150 @yiakwy-xpu-ml-framework-team \n- [ ] remove `from vllm.model_executor.layers.activation import GeluAndMul, SiluAndMul` in `sglang/python/sglang/srt/layers/activation.py`.\n- [ ] Support GemmaRMSNorm and RMSNorm in AMD.\n- [ ] remove `from vllm.model_executor.layers.layernorm import GemmaRMSNorm, RMSNorm` in `sglang/python/sglang/srt/layers/layernorm.py`.\n- [ ] Support `rotary_embedding` kernel in AMD.\n- [ ] Support for `ops.moe_sum` in AMD, remove the dependency on `vllm ops.moe_sum`.  Used in `fused_moe_triton.py`.\n- [ ] Benchmark `vllm ops.moe_align_block_size`, `moe_align_block_size_triton`, and `sgl_moe_align_block_size`, and remove the `num_experts=256` limitation in `sgl_moe_align_block_size`. After this, directly select the kernel from `moe_align_block_size_triton` and `sgl_moe_align_block_size`, and remove the dependency on `vllm ops.moe_align_block_size`.  Used in `fused_moe_triton.py`. https://github.com/sgl-project/sglang/pull/4249 & https://github.com/sgl-project/sglang/pull/4327\n- [ ] Implement `scaled_int8_quant` in `sgl-kernel` and remove the current dependency on `vllm ops.scaled_int8_quant`.  Used in `fused_moe_triton.py`. @zcnrex \n- [ ] Implement `per_token_group_quant_int8` in CUDA, replacing the current `per_token_group_quant_int8 triton` implementation.  Used in `fused_moe_triton.py`. @zcnrex \n- [ ] Support `sglang_per_token_group_quant_fp8` in AMD.  Used in `fused_moe_triton.py`. https://github.com/sgl-project/sglang/pull/3959 https://github.com/sgl-project/sglang/pull/3702 @yiakwy-xpu-ml-framework-team \n- [x] Implement `scaled_fp8_quant` kernel and remove the current dependency on `vllm ops.scaled_fp8_quant`. (This is in progress, 50% complete\u2014see [[link](https://github.com/sgl-project/sglang/pull/3786)](https://github.com/sgl-project/sglang/pull/3786) for per-tensor support, and @hebiao064  is working on per-token support. `vllm ops.scaled_fp8_quant` will support both per-tensor and per-token.). Used in `fused_moe_triton.py`, `layer.py` and `fp8.py`.   @BBuf @hebiao064  https://github.com/sgl-project/sglang/pull/4089 https://github.com/sgl-project/sglang/pull/4163.  https://github.com/sgl-project/sglang/pull/4231\u3002https://github.com/sgl-project/sglang/pull/4215\n- [ ]  Support for `apply_rope_with_cos_sin_cache_inplace` kernel in AMD, remove the  current dependencies on  `vllm os.rotary_embedding` .  Used in `rotary_embedding.py`.\n- [x] Implement `topk_softmax` kernel and remove the current dependency on `vllm.ops.topk_softmax`. Used in `topk.py`.  https://github.com/sgl-project/sglang/pull/4302\n- [x] Support `topk_softmax` in amd. https://github.com/sgl-project/sglang/pull/4448\n- [x] Remove  vllm `ops.topk_softmax` in `python/sglang/srt/layers/moe/topk.py`. https://github.com/sgl-project/sglang/pull/4498\n- [x] Implement `awq_dequantize` kernel and remove the current dependency on `vllm ops.awq_dequantize`. Used in `deepseek_nextn.py` and `deepseek_v2.py`. https://github.com/sgl-project/sglang/pull/4104 @zcnrex\n\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "help wanted",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-01-18T12:05:06+00:00",
    "closed_at": "2025-03-24T18:44:24+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2965/reactions",
      "total_count": 8,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 1,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/2965"
  }
]
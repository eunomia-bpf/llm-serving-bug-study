[
  {
    "number": 6691,
    "title": "[Bug] An error occurs when handling requests when  deploying a model with PD disaggregation",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nthe prefill error:\n```\n[2025-05-28 06:26:01] The server is fired up and ready to roll!\n[2025-05-28 06:29:57] INFO:     127.0.0.1:46780 - \"GET /v1/models HTTP/1.1\" 200 OK\n[2025-05-28 06:30:04] INFO:     127.0.0.1:46782 - \"GET /v1/models HTTP/1.1\" 200 OK\n[2025-05-28 06:30:10] INFO:     127.0.0.1:42176 - \"GET /v1/models HTTP/1.1\" 200 OK\n[2025-05-28 06:32:54] INFO:     127.0.0.1:41706 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-05-28 06:32:54] 127.0.0.1 [28/May/2025:06:32:54 +0000] \"GET /route?engine_rank=0&target_dp_group=0 HTTP/1.1\" 200 207 \"-\" \"python-requests/2.32.3\"\n[2025-05-28 06:32:54 TP0] Prefill batch. #new-seq: 1, #new-token: 23, #cached-token: 0, token usage: 0.00, #running-req: 0, #unbootstrapped-req: 0, #queue-req: 0, #transferring-req: 0 \nE0528 06:32:54.247906 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.247951 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.247961 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.247970 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.247980 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.248150 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.251407 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.251482 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.251611 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.252085 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.252132 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.252281 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.252312 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.252686 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.252795 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.253180 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.253192 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.253230 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nException in thread Thread-5 (transfer_thread):\nE0528 06:32:54.254065 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.254386 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nTraceback (most recent call last):\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n  File \"/root/miniconda3/envs/sglang-pd/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.254595 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.254947 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n    self.run()\n  File \"/root/miniconda3/envs/sglang-pd/lib/python3.10/threading.py\", line 953, in run\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.255156 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n    self._target(*self._args, **self._kwargs)\n  File \"/root/miniconda3/envs/sglang-pd/lib/python3.10/site-packages/sglang/srt/disaggregation/mooncake/conn.py\", line 302, in transfer_thread\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.255383 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n    ret = self.send_kvcache(\n  File \"/root/miniconda3/envs/sglang-pd/lib/python3.10/site-packages/sglang/srt/disaggregation/mooncake/conn.py\", line 227, in send_kvcache\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.255673 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n    status = future.result()\n  File \"/root/miniconda3/envs/sglang-pd/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.255852 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n    return self.__get_result()\nE0528 06:32:54.256289 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n  File \"/root/miniconda3/envs/sglang-pd/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\nE0528 06:32:54.256498 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.256593 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.256705 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n    raise self._exception\nE0528 06:32:54.256989 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n  File \"/root/miniconda3/envs/sglang-pd/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.257114 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/root/miniconda3/envs/sglang-pd/lib/python3.10/site-packages/sglang/srt/disaggregation/mooncake/conn.py\", line 209, in process_layer\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.257463 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n    status = self.engine.transfer_sync(\n  File \"/root/miniconda3/envs/sglang-pd/lib/python3.10/site-packages/sglang/srt/disaggregation/mooncake/transfer_engine.py\", line 70, in transfer_sync\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.257701 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n    raise RuntimeError(\"Mooncake Transfer Engine Return Error.\")\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nRuntimeError: Mooncake Transfer Engine Return Error.\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.257937 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.258078 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.258129 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.258267 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.258443 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.258538 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.258635 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.258741 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.258972 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.259083 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.259163 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.259301 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.259379 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.259464 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.259505 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.259521 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.259529 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.259537 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.259631 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.259783 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.259992 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.260133 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.260231 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.260378 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.260536 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.260630 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.260740 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.260879 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.260964 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.261184 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.261214 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.261425 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.261657 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.261842 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.261924 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.262037 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.262181 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.262298 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.262387 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.262527 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.262632 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.262718 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.262830 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.262964 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.263079 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.263121 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.263274 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.263509 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.263602 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.263775 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.263888 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.263983 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.264092 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.264226 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.264269 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.264351 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.264457 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.264595 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.264699 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.264791 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.264926 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.265010 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.265146 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.265264 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.265498 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.265580 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.265734 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.265831 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.265975 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.266017 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.266146 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.266273 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.266350 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.266495 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.266541 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.266664 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.266784 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.266888 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.266973 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.267086 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.267225 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.267457 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.267535 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.267664 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.267761 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.267843 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.268005 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.268117 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.268224 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.268321 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.268463 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.268575 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.268621 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\nE0528 06:32:54.268743 735017 tcp_transport.cpp:136] Session::writeBody failed. Error: Bad address (value: 14), total_transferred_bytes_: 0, current transferred_bytes: 0\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n[2025-05-28 06:32:54 TP0] Mooncake Transfer Engine Return Error.\n\n```\n\nthe docode error:\n```\n[2025-05-28 06:27:42] The server is fired up and ready to roll!\n[2025-05-28 06:32:54] INFO:     127.0.0.1:40082 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nE0528 06:32:54.247958 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.247998 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.248011 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.248023 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.248034 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.248162 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.251420 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.251493 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.251622 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.252096 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.252142 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.252291 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.252322 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.252696 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.252806 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.253188 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.253201 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.253242 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.254076 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.254396 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.254606 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.254958 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.255167 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.255393 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.255684 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.255863 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.256299 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.256510 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.256603 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.256716 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.256999 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.257125 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.257473 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.257711 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.257947 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.258088 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.258139 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.258277 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.258455 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.258550 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.258646 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.258751 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.258982 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.259092 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.259173 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.259311 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.259390 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.259475 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.259516 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.259534 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.259546 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.259557 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.259641 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.259794 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\nE0528 06:32:54.260002 735764 tcp_transport.cpp:170] Session::readBody failed. Error: End of file (value: 2), total_transferred_bytes_: 0, current transferred_bytes: 0\n```\n\n### Reproduction\n\n`python -m sglang.launch_server --model-path Qwen3-32B --disaggregation-mode prefill --disaggregation-ib-device mlx5_roce0`\n`python -m sglang.launch_server --model-path Qwen3-32B --disaggregation-mode decode --port 30001 --base-gpu-id 0 --disaggregation-ib-device mlx5_roce0`\n\n### Environment\n\n```\nabsl-py                                  2.1.0\naccelerate                               0.32.0\naddict                                   2.4.0\naiofiles                                 23.2.1\naiohappyeyeballs                         2.4.4\naiohttp                                  3.11.10\naiosignal                                1.3.1\nairportsdata                             20250224\naliyun-python-sdk-core                   2.15.1\naliyun-python-sdk-kms                    2.16.4\nannotated-types                          0.7.0\nanyio                                    4.7.0\narchspec                                 0.2.1\nargon2-cffi                              23.1.0\nargon2-cffi-bindings                     21.2.0\narrow                                    1.3.0\nastor                                    0.8.1\nasttokens                                3.0.0\nasync-lru                                2.0.5\nasync-timeout                            5.0.1\nattrdict                                 2.0.1\nattrs                                    24.2.0\nav                                       14.4.0\nbabel                                    2.17.0\nbeautifulsoup4                           4.13.4\nbinpacking                               1.5.2\nbitsandbytes                             0.45.5\nblake3                                   1.0.4\nbleach                                   6.2.0\nboltons                                  23.0.0\nBrotli                                   1.0.9\nbrotlipy                                 0.7.0\ncachetools                               5.5.2\ncertifi                                  2024.8.30\ncffi                                     1.16.0\ncharset-normalizer                       3.4.0\nclick                                    8.1.7\ncloudpickle                              3.1.0\ncomm                                     0.2.2\ncompressed-tensors                       0.9.3\nconda-content-trust                      0.1.3\nconda-package-handling                   2.2.0\nconda_package_streaming                  0.9.0\ncontourpy                                1.2.1\ncpm-kernels                              1.0.11\ncrcmod                                   1.7\ncryptography                             41.0.7\ncupy-cuda12x                             13.4.0\ncycler                                   0.12.1\ndacite                                   1.8.1\ndatasets                                 3.1.0\ndebugpy                                  1.8.14\ndecorator                                5.2.1\ndeepspeed                                0.16.9\ndefusedxml                               0.7.1\nDeprecated                               1.2.18\ndepyf                                    0.18.0\ndill                                     0.3.8\ndiskcache                                5.6.3\ndistro                                   1.9.0\ndnspython                                2.7.0\ndocker-pycreds                           0.4.0\ndocstring_parser                         0.16\neinops                                   0.8.0\nemail_validator                          2.2.0\nexceptiongroup                           1.2.2\nexecuting                                2.2.0\nfastapi                                  0.115.6\nfastapi-cli                              0.0.7\nfastjsonschema                           2.21.1\nfastrlock                                0.8.3\nffmpy                                    0.4.0\nfilelock                                 3.16.1\nfonttools                                4.53.1\nfqdn                                     1.5.1\nfrozenlist                               1.5.0\nfsspec                                   2024.9.0\nfuture                                   1.0.0\ngguf                                     0.16.0\ngitdb                                    4.0.12\nGitPython                                3.1.44\ngoogleapis-common-protos                 1.70.0\ngradio                                   4.41.0\ngradio_client                            1.3.0\ngrpcio                                   1.71.0\nh11                                      0.14.0\nhf-xet                                   1.1.0\nhjson                                    3.1.0\nhttpcore                                 1.0.7\nhttptools                                0.6.4\nhttpx                                    0.28.1\nhuggingface-hub                          0.30.2\nidna                                     3.10\nimageio                                  2.37.0\nimportlib_metadata                       8.0.0\nimportlib_resources                      6.4.0\niniconfig                                2.1.0\ninteregular                              0.3.3\nipykernel                                6.29.5\nipython                                  8.36.0\nisoduration                              20.11.0\njax                                      0.6.1\njaxlib                                   0.6.1\njedi                                     0.19.2\njieba                                    0.42.1\nJinja2                                   3.1.4\njiter                                    0.8.2\njmespath                                 0.10.0\njoblib                                   1.4.2\njson5                                    0.12.0\njsonpatch                                1.32\njsonpointer                              2.1\njsonschema                               4.23.0\njsonschema-specifications                2024.10.1\njupyter_client                           8.6.3\njupyter_core                             5.7.2\njupyter-events                           0.12.0\njupyter-lsp                              2.2.5\njupyter_server                           2.16.0\njupyter_server_terminals                 0.5.3\njupyterlab                               4.4.2\njupyterlab_pygments                      0.3.0\njupyterlab_server                        2.27.3\nkiwisolver                               1.4.5\nlark                                     1.2.2\nlazy_loader                              0.4\nLevenshtein                              0.27.1\nlibmambapy                               1.4.1\nllguidance                               0.7.16\nllvmlite                                 0.44.0\nlm-format-enforcer                       0.10.11\nMarkdown                                 3.6\nmarkdown-it-py                           3.0.0\nMarkupSafe                               2.1.5\nmatplotlib                               3.9.2\nmatplotlib-inline                        0.1.7\nmdurl                                    0.1.2\nmistral_common                           1.5.4\nmistune                                  3.1.3\nml_dtypes                                0.5.1\nmodelscope                               1.17.1\nmooncake-transfer-engine                 0.3.2.post1\nmpmath                                   1.3.0\nmsgpack                                  1.1.0\nmsgspec                                  0.18.6\nmultidict                                6.1.0\nmultiprocess                             0.70.16\nnanobind                                 2.7.0\nnbclient                                 0.10.2\nnbconvert                                7.16.6\nnbformat                                 5.10.4\nnest-asyncio                             1.6.0\nnetworkx                                 3.4.2\nninja                                    1.11.1.3\nnltk                                     3.9.1\nnotebook_shim                            0.2.4\nnumba                                    0.61.2\nnumpy                                    1.26.4\nnvidia-cublas-cu11                       11.11.3.6\nnvidia-cublas-cu12                       12.4.5.8\nnvidia-cuda-cupti-cu11                   11.8.87\nnvidia-cuda-cupti-cu12                   12.4.127\nnvidia-cuda-nvrtc-cu11                   11.8.89\nnvidia-cuda-nvrtc-cu12                   12.4.127\nnvidia-cuda-runtime-cu11                 11.8.89\nnvidia-cuda-runtime-cu12                 12.4.127\nnvidia-cudnn-cu11                        9.1.0.70\nnvidia-cudnn-cu12                        9.1.0.70\nnvidia-cufft-cu11                        10.9.0.58\nnvidia-cufft-cu12                        11.2.1.3\nnvidia-curand-cu11                       10.3.0.86\nnvidia-curand-cu12                       10.3.5.147\nnvidia-cusolver-cu11                     11.4.1.48\nnvidia-cusolver-cu12                     11.6.1.9\nnvidia-cusparse-cu11                     11.7.5.86\nnvidia-cusparse-cu12                     12.3.1.170\nnvidia-cusparselt-cu12                   0.6.2\nnvidia-ml-py                             12.560.30\nnvidia-nccl-cu11                         2.21.5\nnvidia-nccl-cu12                         2.21.5\nnvidia-nvjitlink-cu12                    12.4.127\nnvidia-nvtx-cu11                         11.8.86\nnvidia-nvtx-cu12                         12.4.127\nopenai                                   1.57.1\nopencv-python-headless                   4.11.0.86\nopentelemetry-api                        1.26.0\nopentelemetry-exporter-otlp              1.26.0\nopentelemetry-exporter-otlp-proto-common 1.26.0\nopentelemetry-exporter-otlp-proto-grpc   1.26.0\nopentelemetry-exporter-otlp-proto-http   1.26.0\nopentelemetry-proto                      1.26.0\nopentelemetry-sdk                        1.26.0\nopentelemetry-semantic-conventions       0.47b0\nopentelemetry-semantic-conventions-ai    0.4.3\nopt_einsum                               3.4.0\norjson                                   3.10.7\noss2                                     2.18.6\noutcome                                  1.3.0.post0\noutlines                                 0.1.11\noutlines_core                            0.1.26\noverrides                                7.7.0\npackaging                                24.2\npandas                                   2.2.3\npandocfilters                            1.5.1\nparso                                    0.8.4\npartial-json-parser                      0.2.1.1.post4\npeft                                     0.12.0\npexpect                                  4.9.0\npillow                                   10.4.0\npip                                      22.0.2\nplatformdirs                             3.10.0\npluggy                                   1.5.0\nprometheus_client                        0.21.1\nprometheus-fastapi-instrumentator        7.0.0\nprompt_toolkit                           3.0.51\npropcache                                0.2.1\nprotobuf                                 4.25.6\npsutil                                   6.1.0\nptyprocess                               0.7.0\npure_eval                                0.2.3\npy-cpuinfo                               9.0.0\npyairports                               2.1.1\npyarrow                                  18.1.0\npybind11                                 2.13.6\npycocotools                              2.0.8\npycosat                                  0.6.6\npycountry                                24.6.1\npycparser                                2.21\npycryptodome                             3.20.0\npydantic                                 2.10.3\npydantic_core                            2.27.1\npydub                                    0.25.1\nPygments                                 2.19.1\npynvml                                   12.0.0\npyOpenSSL                                23.2.0\npyparsing                                3.1.2\nPySocks                                  1.7.1\npytest                                   8.3.5\npython-dateutil                          2.9.0.post0\npython-dotenv                            1.0.1\npython-json-logger                       3.3.0\npython-multipart                         0.0.20\npytz                                     2024.2\nPyYAML                                   6.0.2\npyzmq                                    26.2.0\nqwen-vl-utils                            0.0.11\nRapidFuzz                                3.13.0\nray                                      2.43.0\nreferencing                              0.35.1\nregex                                    2024.11.6\nrequests                                 2.32.3\nrfc3339-validator                        0.1.4\nrfc3986-validator                        0.1.1\nrich                                     13.9.4\nrich-toolkit                             0.13.2\nrouge                                    1.0.1\nrpds-py                                  0.22.3\nruamel.yaml                              0.17.21\nruamel.yaml.clib                         0.2.6\nruff                                     0.5.7\nsafetensors                              0.4.5\nscikit-image                             0.25.2\nscikit-learn                             1.6.1\nscipy                                    1.15.2\nselenium                                 4.12.0\nsemantic-version                         2.10.0\nSend2Trash                               1.8.3\nsentencepiece                            0.2.0\nsentry-sdk                               2.29.1\nsetproctitle                             1.3.6\nsetuptools                               59.6.0\nsgl-kernel                               0.1.1\nsglang                                   0.4.6.post1\nshellingham                              1.5.4\nshtab                                    1.7.1\nsimplejson                               3.19.2\nsix                                      1.17.0\nsmmap                                    5.0.2\nsniffio                                  1.3.1\nsortedcontainers                         2.4.0\nsoupsieve                                2.7\nstack-data                               0.6.3\nstarlette                                0.41.3\nsympy                                    1.13.1\ntensorboard                              2.17.0\ntensorboard-data-server                  0.7.2\nterminado                                0.18.1\nthreadpoolctl                            3.6.0\ntifffile                                 2025.5.10\ntiktoken                                 0.7.0\ntinycss2                                 1.4.0\ntokenizers                               0.21.1\ntomli                                    2.2.1\ntomlkit                                  0.12.0\ntoolz                                    0.12.0\ntorch                                    2.6.0\ntorchao                                  0.10.0\ntorchaudio                               2.6.0\ntorchvision                              0.21.0\ntornado                                  6.5.1\ntqdm                                     4.67.1\ntraitlets                                5.14.3\ntransformers                             4.51.3\ntransformers-stream-generator            0.0.5\ntrio                                     0.30.0\ntrio-websocket                           0.12.2\ntriton                                   3.2.0\ntrl                                      0.9.6\ntruststore                               0.8.0\ntyper                                    0.15.2\ntypes-python-dateutil                    2.9.0.20250516\ntyping_extensions                        4.12.2\ntyping-inspection                        0.4.1\ntyro                                     0.8.6\ntzdata                                   2024.2\nultralytics                              8.3.143\nultralytics-thop                         2.0.14\nuri-template                             1.3.0\nurllib3                                  2.2.3\nuv                                       0.6.14\nuvicorn                                  0.32.1\nuvloop                                   0.21.0\nvllm                                     0.8.5.post1\nwandb                                    0.19.11\nwatchfiles                               1.0.0\nwcwidth                                  0.2.13\nwebcolors                                24.11.1\nwebencodings                             0.5.1\nwebsocket-client                         1.8.0\nwebsockets                               12.0\nWerkzeug                                 3.0.3\nwheel                                    0.41.2\nwrapt                                    1.17.2\nwsproto                                  1.2.0\nxformers                                 0.0.29.post2\nxgrammar                                 0.1.18\nxxhash                                   3.5.0\nyarl                                     1.18.3\nzipp                                     3.21.0\nzstandard                                0.19.0\n\n```",
    "labels": [],
    "state": "open",
    "created_at": "2025-05-28T06:50:00+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6691/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6691"
  },
  {
    "number": 3358,
    "title": "[Bug] request timeout with multi-gpu model",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\ni launch a qwen2.5-32B-instruct model on two L20 gpus:\n\n`docker run -it --rm --runtime=nvidia --gpus '\"device=0,1\"' --ipc=host --network=host --shm-size=16g --ulimit memlock=-1 --ulimit stack=67108864 -v /nfs/hf_models:/models --env NCCL_P2P_DISABLE=1 lmsysorg/sglang:v0.4.2.post2-cu124-srt python3 -m sglang.launch_server --model-path /models/Qwen2.5-32B-Instruct --host 0.0.0.0 --port 3000 --tp 2 --schedule-policy fcfs`\n\nlaunch model success\n\n![Image](https://github.com/user-attachments/assets/5a9fe64e-cfa0-4320-9812-5f52fedd8d2c)\n\n![Image](https://github.com/user-attachments/assets/5244400a-573c-4414-9c01-7376885ae884)\n\nand then send a request,  there is no response and request timeout\n```\n2025-02-07 10:41:25,094 - QX-Profiler-main - INFO - endpoint probe started...\n2025-02-07 10:41:25,307 - httpx - INFO - HTTP Request: POST http://192.168.31.54:3000/v1/chat/completions \"HTTP/1.1 200 OK\"\n2025-02-07 10:51:25,399 - QX-Profiler-API - ERROR - connection to 192.168.31.54:3000 failed, now do 1/10 retry after 2 seconds\n2025-02-07 10:51:27,473 - httpx - INFO - HTTP Request: POST http://192.168.31.54:3000/v1/chat/completions \"HTTP/1.1 200 OK\"\n```\n \nthe gpu-util are 100% when the mode executing request\n\n![Image](https://github.com/user-attachments/assets/6cd1b30c-dc4e-4dab-a909-bc3a9aff7350)\n\n### Reproduction\n\n`docker run -it --rm --runtime=nvidia --gpus '\"device=0,1\"' --ipc=host --network=host --shm-size=16g --ulimit memlock=-1 --ulimit stack=67108864 -v /nfs/hf_models:/models --env NCCL_P2P_DISABLE=1 lmsysorg/sglang:v0.4.2.post2-cu124-srt python3 -m sglang.launch_server --model-path /models/Qwen2.5-32B-Instruct --host 0.0.0.0 --port 3000 --tp 2 --schedule-policy fcfs`\n\n### Environment\n\n```\n(base) qxzg@master0:~$ docker run -it --rm --runtime=nvidia --gpus '\"device=0,1\"' --ipc=host --network=host --shm-size=16g --ulimit memlock=-1 --ulimit stack=67108864 -v /nfs/hf_models:/models --env NCCL_P2P_DISABLE=1 lmsysorg/sglang:v0.4.2.post2-cu124-srt python3 -m sglang.check_env\n\n==========\n== CUDA ==\n==========\n\nCUDA Version 12.4.1\n\nContainer image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\nBy pulling and using the container, you accept the terms and conditions of this license:\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n\nA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n\nPython: 3.10.16 (main, Dec  4 2024, 08:53:37) [GCC 9.4.0]\nCUDA available: True\nGPU 0,1: NVIDIA L20\nGPU 0,1 Compute Capability: 8.9\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.135\nPyTorch: 2.5.1+cu124\nflashinfer: 0.2.0.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.2\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.61.0\nanthropic: Module Not Found\nlitellm: Module Not Found\ndecord: 0.6.0\nNVIDIA Topology: \n\tGPU0\tGPU1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tPIX\t0-27,56-83\t0\t\tN/A\nGPU1\tPIX\t X \t0-27,56-83\t0\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-07T03:01:34+00:00",
    "closed_at": "2025-02-07T03:42:38+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3358/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3358"
  },
  {
    "number": 5576,
    "title": "[Bug] torch.OutOfMemoryError: CUDA out of memory for 16GB Vram while trying to inference gemma-3-12b-IT",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nTrying to run gemma-3-12b-It on my system that has 16 GB vram. I tried with multiple context-length. With context length 7k, it doesn't work. \n\nI used a docker image to run the command. \n\noutput:\n\n```\npodman run --gpus all     -p 30000:30000     -v ~/.cache/huggingface:/root/.cache/huggingface     --env \"HF_TOKEN=*************************\"     --ipc=host     lmsysorg/sglang:latest     python3 -m sglang.launch_server --model-path google/gemma-3-12b-it --dtype=float16  --kv-cache-dtype fp8_e4m3 --host 0.0.0.0 --port 30000 --quantization fp8 --context-length  7000 --max-total-tokens 30000 --attention-backend flashinfer --mem-fraction-static 0.7 --chat-template gemma-it --enable-mixed-chunk --enable-torch-compile --chunked-prefill-size 2048 --log-level debug\n\n==================================\n== Triton Inference Server Base ==\n==================================\n\nNVIDIA Release 24.04 (build 90085237)\n\nCopyright (c) 2018-2023, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\nBy pulling and using the container, you accept the terms and conditions of this license:\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n\n[2025-04-20 14:03:02] server_args=ServerArgs(model_path='google/gemma-3-12b-it', tokenizer_path='google/gemma-3-12b-it', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='float16', kv_cache_dtype='fp8_e4m3', quantization='fp8', quantization_param_path=None, context_length=7000, device='cuda', served_model_name='google/gemma-3-12b-it', chat_template='gemma-it', completion_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=30000, mem_fraction_static=0.7, max_running_requests=None, max_total_tokens=30000, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, stream_interval=1, stream_output=False, random_seed=304297045, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='debug', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, enable_llama4_multimodal=None, disable_overlap_schedule=False, enable_mixed_chunk=True, enable_dp_attention=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode='auto', enable_torch_compile=True, torch_compile_max_bs=32, cuda_graph_max_bs=8, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, enable_flashinfer_mla=False, enable_flashmla=False, flashinfer_mla_disable_ragged=False, warmups=None, n_share_experts_fusion=0, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_bootstrap_port=8998, disaggregation_transfer_backend='mooncake', disable_fast_image_processor=False)\n[2025-04-20 14:03:02] Starting new HTTPS connection (1): huggingface.co:443\n[2025-04-20 14:03:03] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/config.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:03] Downcasting torch.float32 to torch.float16.\n[2025-04-20 14:03:03] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/hf_quant_config.json HTTP/1.1\" 404 0\n[2025-04-20 14:03:03] Skipping import of cpp extensions: /usr/local/lib/python3.10/dist-packages/torchao/_C.abi3.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n[2025-04-20 14:03:03] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/config.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:04] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/processor_config.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:04] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/processor_config.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:04] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/chat_template.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:04] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/chat_template.jinja HTTP/1.1\" 404 0\n[2025-04-20 14:03:05] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/preprocessor_config.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:05] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:06] Starting new HTTPS connection (1): huggingface.co:443\n[2025-04-20 14:03:06 TP0] Starting new HTTPS connection (1): huggingface.co:443\n[2025-04-20 14:03:06] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:06 TP0] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/config.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:06 TP0] Downcasting torch.float32 to torch.float16.\n[2025-04-20 14:03:06 TP0] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/hf_quant_config.json HTTP/1.1\" 404 0\n[2025-04-20 14:03:06] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/processor_config.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:06 TP0] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/config.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:07] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/chat_template.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:07 TP0] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/processor_config.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:07] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/chat_template.jinja HTTP/1.1\" 404 0\n[2025-04-20 14:03:07 TP0] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/processor_config.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:07 TP0] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/chat_template.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:08 TP0] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/chat_template.jinja HTTP/1.1\" 404 0\n[2025-04-20 14:03:08 TP0] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/preprocessor_config.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:08] Use chat template for the OpenAI-compatible API server: gemma-it\n[2025-04-20 14:03:08 TP0] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:09 TP0] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/processor_config.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:10 TP0] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/chat_template.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:10 TP0] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/chat_template.jinja HTTP/1.1\" 404 0\n[2025-04-20 14:03:11 TP0] Overlap scheduler is disabled for multimodal models.\n[2025-04-20 14:03:11 TP0] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/config.json HTTP/1.1\" 200 0\n[2025-04-20 14:03:11 TP0] Downcasting torch.float32 to torch.float16.\n[2025-04-20 14:03:12 TP0] https://huggingface.co:443 \"HEAD /google/gemma-3-12b-it/resolve/main/hf_quant_config.json HTTP/1.1\" 404 0\n[2025-04-20 14:03:12 TP0] Automatically reduce --mem-fraction-static to 0.630 because this is a multimodal model.\n[2025-04-20 14:03:12 TP0] Automatically turn off --chunked-prefill-size for multimodal model.\n[2025-04-20 14:03:12 TP0] Disable chunked prefix cache for non-MLA backend.\n[2025-04-20 14:03:12 TP0] Init torch distributed begin.\n[2025-04-20 14:03:12 TP0] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:30786 backend=nccl\n[2025-04-20 14:03:12 TP0] Init torch distributed ends. mem usage=0.00 GB\n[2025-04-20 14:03:12 TP0] Load weight begin. avail mem=15.27 GB\n[2025-04-20 14:03:12 TP0] Skipping import of cpp extensions: /usr/local/lib/python3.10/dist-packages/torchao/_C.abi3.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n[2025-04-20 14:03:12 TP0] Attempting to acquire lock 136142154086192 on /tmp/069a28f430c86f5021601e995882b9524a874d89e8ae773a10b0fa2ada596237google-gemma-3-12b-it.lock\n[2025-04-20 14:03:12 TP0] Lock 136142154086192 acquired on /tmp/069a28f430c86f5021601e995882b9524a874d89e8ae773a10b0fa2ada596237google-gemma-3-12b-it.lock\n[2025-04-20 14:03:13 TP0] https://huggingface.co:443 \"GET /api/models/google/gemma-3-12b-it/revision/main HTTP/1.1\" 200 7114\n[2025-04-20 14:03:13 TP0] Attempting to release lock 136142154086192 on /tmp/069a28f430c86f5021601e995882b9524a874d89e8ae773a10b0fa2ada596237google-gemma-3-12b-it.lock\n[2025-04-20 14:03:13 TP0] Lock 136142154086192 released on /tmp/069a28f430c86f5021601e995882b9524a874d89e8ae773a10b0fa2ada596237google-gemma-3-12b-it.lock\n[2025-04-20 14:03:13 TP0] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1992, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 260, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 75, in __init__\n    self.model_runner = ModelRunner(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 182, in __init__\n    self.initialize(min_per_gpu_memory)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 192, in initialize\n    self.load_model()\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 435, in load_model\n    self.model = get_model(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/__init__.py\", line 22, in get_model\n    return loader.load_model(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/loader.py\", line 372, in load_model\n    model = _initialize_model(\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/loader.py\", line 153, in _initialize_model\n    return model_class(\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/gemma3_mm.py\", line 176, in __init__\n    self.language_model = Gemma3ForCausalLM(\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/gemma3_causal.py\", line 602, in __init__\n    self.model = Gemma3TextModel(\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/gemma3_causal.py\", line 494, in __init__\n    self.layers = make_layers(\n  File \"/sgl-workspace/sglang/python/sglang/srt/utils.py\", line 402, in make_layers\n    [\n  File \"/sgl-workspace/sglang/python/sglang/srt/utils.py\", line 403, in <listcomp>\n    maybe_offload_to_cpu(layer_fn(idx=idx, prefix=add_prefix(idx, prefix)))\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/gemma3_causal.py\", line 496, in <lambda>\n    lambda idx, prefix: Gemma3DecoderLayer(\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/gemma3_causal.py\", line 302, in __init__\n    self.mlp = Gemma3MLP(\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/gemma3_causal.py\", line 88, in __init__\n    self.down_proj = RowParallelLinear(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/linear.py\", line 1174, in __init__\n    self.quant_method.create_weights(\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/quantization/fp8.py\", line 259, in create_weights\n    data=torch.empty(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\", line 106, in __torch_function__\n    return func(*args, **kwargs)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 15.58 GiB of which 54.38 MiB is free. Including non-PyTorch memory, this process has 15.42 GiB memory in use. Of the allocated memory 15.12 GiB is allocated by PyTorch, and 71.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n[2025-04-20 14:03:13] Received sigquit from a child process. It usually means the child failed.\n\n\n```\n\nIs there anything I can do to inference gemma-3-12b-it. \n\nhow can i optimize the model to run on this hardware. Is there anything I can do to reduce this \n```\n15.12 GiB is allocated by PyTorch\n\n```\n\nwhy is it so huge\n\n### Reproduction\n\n```\nuname -a\nLinux JOHNAIC 6.5.0-45-generic #45~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Jul 15 16:40:02 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\n```\n64 GB RAM \n4070 ti super. Vram is 16 GB.\n\n\ncommand\n\n```\npodman run --gpus all     -p 30000:30000     -v ~/.cache/huggingface:/root/.cache/huggingface     --env \"HF_TOKEN=*************************\"     --ipc=host     lmsysorg/sglang:latest     python3 -m sglang.launch_server --model-path google/gemma-3-12b-it --dtype=float16  --kv-cache-dtype fp8_e4m3 --host 0.0.0.0 --port 30000 --quantization fp8 --context-length  7000 --max-total-tokens 30000 --attention-backend flashinfer --mem-fraction-static 0.7 --chat-template gemma-it --enable-mixed-chunk --enable-torch-compile --chunked-prefill-size 2048 --log-level debug\n```\n\n### Environment\n\n```\npodman exec -it agitated_lichterman python3 -m sglang.check_env\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0: NVIDIA GeForce RTX 4070 Ti SUPER\nGPU 0 Compute Capability: 8.9\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 570.86.15\nPyTorch: 2.5.1+cu124\nsglang: 0.4.5.post1\nsgl_kernel: 0.0.9.post1\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.51.1\ntorchao: 0.10.0\nnumpy: 2.2.4\naiohttp: 3.11.16\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.11.3\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.1\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.17\nopenai: 1.74.0\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.66.1\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-11    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576\n\n```",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-04-20T14:11:09+00:00",
    "closed_at": "2025-06-21T00:19:44+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5576/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5576"
  },
  {
    "number": 1931,
    "title": "[Bug] Seeing random output with nvidia/Llama-3.1-Nemotron-70B-Reward",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI am trying to run the basic setup example with the nvidia/Llama-3.1-Nemotron-70B-Reward on a machine with 8 A6000s, but the observed output is random.\r\n\r\nGenerated output:\r\n```\r\n{\"text\":\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\\\"\",\"meta_info\":{\"prompt_tokens\":6,\"completion_tokens\":16,\"completion_tokens_wo_jump_forward\":16,\"cached_tokens\":1,\"finish_reason\"{\"type\":\"length\",\"length\":16},\"id\":\"a3101b373bcd4fb8beab6f42e74efd53\"}\r\n```\n\n### Reproduction\n\nServer side:\r\n```\r\npython -m sglang.launch_server --model-path nvidia/Llama-3.1-Nemotron-70B-Reward-HF --port 30000 --tp 8\r\n```\r\n\r\nClient side:\r\n```\r\ncurl http://localhost:30000/generate \\\r\n  -H \"Content-Type: application/json\" \\\r\n  -d '{\r\n    \"text\": \"Once upon a time,\",\r\n    \"sampling_params\": {\r\n      \"max_new_tokens\": 16,\r\n      \"temperature\": 0\r\n    }\r\n  }'\r\n```\n\n### Environment\n\nPython: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA RTX A6000\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.6\r\nCUDA_HOME: /usr\r\nNVCC: Cuda compilation tools, release 11.5, V11.5.119\r\nCUDA Driver Version: 550.54.15\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.3.5\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.46.1\r\nrequests: 2.32.3\r\ntqdm: 4.66.6\r\nnumpy: 1.26.4\r\naiohttp: 3.10.10\r\nfastapi: 0.115.4\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.26.2\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.1.0\r\npydantic: 2.9.2\r\nuvicorn: 0.32.0\r\nuvloop: 0.21.0\r\nzmq: 26.2.0\r\nvllm: 0.6.3.post1\r\nmultipart: 0.0.17\r\nopenai: 1.54.1\r\nanthropic: 0.39.0\r\nNVIDIA Topology: \r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-63,128-191    0               N/A\r\nGPU1    SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-63,128-191    0               N/A\r\nGPU2    SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     0-63,128-191    0               N/A\r\nGPU3    SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     0-63,128-191    0               N/A\r\nGPU4    SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     64-127,192-255  1               N/A\r\nGPU5    SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     64-127,192-255  1               N/A\r\nGPU6    SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     PHB     64-127,192-255  1               N/A\r\nGPU7    SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     64-127,192-255  1               N/A\r\nNIC0    SYS     SYS     SYS     SYS     SYS     SYS     PHB     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n\r\n\r\nulimit soft: 4096",
    "labels": [],
    "state": "closed",
    "created_at": "2024-11-05T23:03:46+00:00",
    "closed_at": "2024-11-14T18:50:43+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1931/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1931"
  },
  {
    "number": 5453,
    "title": "What is the relationship between ModelRunner and the model(deepseek.py,llama.py..etc)?",
    "body": "Who can help me answer this question?  I haven't found the calling relationship.",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-04-16T09:14:55+00:00",
    "closed_at": "2025-06-16T00:20:41+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5453/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5453"
  },
  {
    "number": 232,
    "title": "logprobs for each token when using local server",
    "body": "I have a local server running and setup my script with the text_qa.run_batch example.\r\n\r\nIs there a way to get the logprobs for each token like in the OpenAI API?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-25T19:45:07+00:00",
    "closed_at": "2024-07-09T07:35:59+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/232/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/232"
  },
  {
    "number": 6160,
    "title": "[Bug] flashinfer_python with minimum required version 0.2.5 is not installed",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI am trying to serve gemma3 27b-it on RTX 5090 using sglang blackwell image. However, I'm getting this error:\n```bash\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.11/importlib/metadata/__init__.py\", line 563, in from_name\n    return next(cls.discover(name=name))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nStopIteration\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/utils.py\", line 684, in assert_pkg_version\n    installed_version = version(pkg)\n                        ^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/importlib/metadata/__init__.py\", line 1008, in version\n    return distribution(distribution_name).version\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/importlib/metadata/__init__.py\", line 981, in distribution\n    return Distribution.from_name(distribution_name)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/importlib/metadata/__init__.py\", line 565, in from_name\n    raise PackageNotFoundError(name)\nimportlib.metadata.PackageNotFoundError: No package metadata was found for flashinfer_python\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/sgl-workspace/sglang/python/sglang/launch_server.py\", line 14, in <module>\n    launch_server(server_args)\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/http_server.py\", line 726, in launch_server\n    tokenizer_manager, scheduler_info = _launch_subprocesses(server_args=server_args)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/engine.py\", line 513, in _launch_subprocesses\n    _set_envs_and_config(server_args)\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/engine.py\", line 464, in _set_envs_and_config\n    assert_pkg_version(\n  File \"/sgl-workspace/sglang/python/sglang/srt/utils.py\", line 691, in assert_pkg_version\n    raise Exception(\nException: flashinfer_python with minimum required version 0.2.5 is not installed. Please uninstall the old version and reinstall the latest version by following the instructions at https://docs.flashinfer.ai/installation.html.\n```\n\n### Reproduction\n\nThe model is retrieved from [huggingface](https://huggingface.co/google/gemma-3-27b-it?inference_provider=hf-inference)\nHere is the docker compose to run it:\n\n```yaml  \ngeneration_gemma_3_27b_sglang:\n    image: lmsysorg/sglang:blackwell\n    container_name: generation-gemma-3-27b-sglang\n    volumes:\n      - ./models/google--gemma-3-27b-it:/models/google--gemma-3-27b-it\n      - ./models/torchinductor_cache:/models/torchinductor_cache\n    # restart: always\n    network_mode: host # required by RDMA\n    privileged: true # required by RDMA\n    # Or you can only publish port 30000\n    # ports:\n    #   - 30000:30000\n    environment:\n      - TORCHINDUCTOR_CACHE_DIR=/models/torchinductor_cache\n    entrypoint: python3 -m sglang.launch_server\n    command: --model-path /models/google--gemma-3-27b-it\n      --host 0.0.0.0\n      --context-length 8192\n      --port 30000\n      --random-seed 0\n      --log-requests-level 2\n      --enable-metrics\n      --max-running-requests 4\n      --show-time-cost\n      --dtype float16\n      --stream-interval 2\n      --served-model-name \"gemma-3-27b\"\n      --tp 4\n      --attention-backend flashinfer\n      # --enable-torch-compile\n      # --tokenizer-mode auto\n      # --enable-mixed-chunk\n      # --chat-template /models/CohereForAI--aya-expanse-8b/chat_template.json\n    ulimits:\n      memlock: -1\n      stack: 67108864\n    ipc: host\n    # healthcheck:\n    #   test: [\"CMD-SHELL\", \"curl -f http://localhost:30000/health || exit 1\"]\n    #   retries: 3\n    #   interval: 1h\n    #   timeout: 1m\n    #   start_period: 2m\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [GPU]\n```\n\n### Environment\n\n python3 -m sglang.check_env\n/home/ubuntu-user/miniconda3/envs/default/lib/python3.12/site-packages/torch/cuda/__init__.py:287: UserWarning: \nNVIDIA GeForce RTX 5090 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\nThe current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_70 sm_75 sm_80 sm_86 sm_90.\nIf you want to use the NVIDIA GeForce RTX 5090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n\n  warnings.warn(\nPython: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3: NVIDIA GeForce RTX 5090\nGPU 0,1,2,3 Compute Capability: 12.0\nCUDA_HOME: /usr\nNVCC: Cuda compilation tools, release 12.2, V12.2.140\nCUDA Driver Version: 570.144\nPyTorch: 2.7.0+cu126\nsglang: 0.4.6.post2\nsgl_kernel: Module Not Found\nflashinfer_python: Module Not Found\ntriton: 3.3.0\ntransformers: Module Not Found\ntorchao: Module Not Found\nnumpy: 2.2.5\naiohttp: 3.11.18\nfastapi: 0.115.12\nhf_transfer: Module Not Found\nhuggingface_hub: 0.31.1\ninteregular: Module Not Found\nmodelscope: Module Not Found\norjson: Module Not Found\noutlines: Module Not Found\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.4\npython-multipart: Module Not Found\npyzmq: 26.4.0\nuvicorn: Module Not Found\nuvloop: Module Not Found\nvllm: Module Not Found\nxgrammar: Module Not Found\nopenai: Module Not Found\ntiktoken: Module Not Found\nanthropic: Module Not Found\nlitellm: Module Not Found\ndecord: Module Not Found\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NODE    SYS     SYS     0-7,16-23       0               N/A\nGPU1    NODE     X      SYS     SYS     0-7,16-23       0               N/A\nGPU2    SYS     SYS      X      NODE    8-15,24-31      1               N/A\nGPU3    SYS     SYS     NODE     X      8-15,24-31      1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1073741816\n\n(Please note that I ran this in a conda environment on my machine because I'm using a docker container where I'm getting the error, and the docker container is exiting so I can't run inside it)",
    "labels": [
      "blackwell"
    ],
    "state": "closed",
    "created_at": "2025-05-09T17:19:44+00:00",
    "closed_at": "2025-06-11T15:25:36+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6160/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6160"
  },
  {
    "number": 322,
    "title": "Contradictory suggestions: Not enough memory. Please try to increase --mem-fraction-static",
    "body": "**Q: Should I increase or decrease `--mem-fraction-static`?** (and what is the minimum and maximum value allowed?)\r\n\r\nLooking in the source code (`python/sglang/srt/managers/router/model_runner.py`) I would believe that increasing the value would alleviate the memory requirements but I might be interpreting it wrong. Just wanted to inform that there is a mismatch between the advice given in documentation and the advice given in the actual code.\r\n\r\n**Description of the problem:**\r\n\r\nI am trying to launch Mistral-7B-Instruct-v0.2 (using sglang==0.1.13):\r\n\r\n`python -m sglang.launch_server --model-path /llm_path/hf_model_mistral_7B_Instruct_v0_2 --port 30000`\r\n\r\nbut I have memory issues. At the end it is suggested to increase `--mem-fraction-static`. \r\n\r\nHowever, in the documentation (https://github.com/sgl-project/sglang) the opposite advice is given:\r\n\r\n> If you see out-of-memory errors during serving, please try to reduce the memory usage of the KV cache pool by setting a smaller value of --mem-fraction-static. The default value is 0.9\r\n\r\n\r\nKeep up the good work :)\r\n\r\n/sneglen\r\n\r\n**Here is the error:**\r\n`Process Process-1:\r\nrouter init state: Traceback (most recent call last):\r\n  File \"/zhome/ac/8/105765/venv/env_MT/lib/python3.11/site-packages/sglang/srt/managers/router/manager.py\", line 68, in start_router_process\r\n    model_client = ModelRpcClient(server_args, port_args)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/zhome/ac/8/105765/venv/env_MT/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 619, in __init__\r\n    self.model_server.exposed_init_model(0, server_args, port_args)\r\n  File \"/zhome/ac/8/105765/venv/env_MT/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 70, in exposed_init_model\r\n    self.model_runner = ModelRunner(\r\n                        ^^^^^^^^^^^^\r\n  File \"/zhome/ac/8/105765/venv/env_MT/lib/python3.11/site-packages/sglang/srt/managers/router/model_runner.py\", line 272, in __init__\r\n    self.init_memory_pool(total_gpu_memory)\r\n  File \"/zhome/ac/8/105765/venv/env_MT/lib/python3.11/site-packages/sglang/srt/managers/router/model_runner.py\", line 331, in init_memory_pool\r\n    raise RuntimeError(\r\nRuntimeError: Not enought memory. Please try to increase --mem-fraction-static.\r\n`",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-03-22T12:23:46+00:00",
    "closed_at": "2024-07-25T06:33:37+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/322/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/322"
  },
  {
    "number": 8056,
    "title": "[bug] logits processor race condition in overlap mode",
    "body": "[One](https://github.com/sgl-project/sglang/blob/4a8837950abb7a39d5b890b8f4ee21bd9ded959d/test/srt/test_srt_endpoint.py#L452) of the logit processor tests is disabled because we have race condition on output_ids variable.\n```python\n        \"\"\"\n        NOTE: This feature has a race condition bug.\n        This line https://github.com/sgl-project/sglang/blob/ef8ec07b2ce4c70c2a33ec5acda4ce529bc3cda4/test/srt/test_srt_endpoint.py#L395-L396 can be accessed by two concurrent threads at the same time. The access order is not guaranteed.\n        In sglang, we use two python threads to overlap the GPU computation and CPU scheduling.\n        Thread 1 (the CPU scheduling thread) will update the `param_dict[\"__req__\"].output_ids`.\n        Thread 2 (the GPU computation thread) will call `DeterministicStatefulLogitProcessor` because sampling is considered as GPU computation.\n        We can fix this by moving the call of DeterministicStatefulLogitProcessor to the CPU scheduling thread.\n        \"\"\"\n```\n\nI guess we also have race condition on [custom_params](https://github.com/sgl-project/sglang/blob/4a8837950abb7a39d5b890b8f4ee21bd9ded959d/python/sglang/srt/sampling/sampling_batch_info.py#L57) and [custom_logit_processor](https://github.com/sgl-project/sglang/blob/4a8837950abb7a39d5b890b8f4ee21bd9ded959d/python/sglang/srt/sampling/sampling_batch_info.py#L59C5-L59C27) variables since CPU scheduling thread will update them in filter_batch/merge_batch functions while GPU computation thread will access them in apply_custom_logit_processor function.\n",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-15T09:14:14+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/8056/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/8056"
  },
  {
    "number": 4251,
    "title": "[Bug] QwQ-32B The backend reported an error",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n![Image](https://github.com/user-attachments/assets/874ba0e8-d0fa-40b1-9ab8-4e5a1fda7497)\n\n![Image](https://github.com/user-attachments/assets/e172dee8-684b-48ad-a087-04195653c463)\n\nThe sglang backend reported an error and the answer was not completed yet.\n\nThe same model has no problem with ollama and vllm\n\n### Reproduction\n\npython -m sglang.launch_server \\\n--model-path /home/cheng/model/QwQ-32B \\\n--port 8000 \\\n--host 192.168.66.15 \\\n--tensor-parallel-size 4 \\\n--served-model-name QwQ-32B \\\n--mem-fraction-static 0.7 \\\n--dtype half \\\n--max-total-tokens 131072 \\\n--max-running-requests 5 \\\n--reasoning-parser deepseek-r1 \\\n--tool-call-parser qwen25\n\n### Environment\n\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                        main\n_openmp_mutex             5.1                       1_gnu\naiohappyeyeballs          2.5.0                    pypi_0    pypi\naiohttp                   3.11.13                  pypi_0    pypi\naiohttp-cors              0.7.0                    pypi_0    pypi\naiosignal                 1.3.2                    pypi_0    pypi\nairportsdata              20250224                 pypi_0    pypi\nannotated-types           0.7.0                    pypi_0    pypi\nanthropic                 0.49.0                   pypi_0    pypi\nanyio                     4.8.0                    pypi_0    pypi\nastor                     0.8.1                    pypi_0    pypi\nasttokens                 3.0.0                    pypi_0    pypi\nasync-timeout             5.0.1                    pypi_0    pypi\nattrs                     25.1.0                   pypi_0    pypi\nblake3                    1.0.4                    pypi_0    pypi\nbzip2                     1.0.8                h5eee18b_6\nca-certificates           2025.2.25            h06a4308_0\ncachetools                5.5.2                    pypi_0    pypi\ncertifi                   2025.1.31                pypi_0    pypi\ncharset-normalizer        3.4.1                    pypi_0    pypi\nclick                     8.1.8                    pypi_0    pypi\ncloudpickle               3.1.1                    pypi_0    pypi\ncolorful                  0.5.6                    pypi_0    pypi\ncompressed-tensors        0.9.1                    pypi_0    pypi\ncuda-bindings             12.8.0                   pypi_0    pypi\ncuda-python               12.8.0                   pypi_0    pypi\ndatasets                  3.3.2                    pypi_0    pypi\ndecorator                 5.2.1                    pypi_0    pypi\ndecord                    0.6.0                    pypi_0    pypi\ndepyf                     0.18.0                   pypi_0    pypi\ndill                      0.3.8                    pypi_0    pypi\ndiskcache                 5.6.3                    pypi_0    pypi\ndistlib                   0.3.9                    pypi_0    pypi\ndistro                    1.9.0                    pypi_0    pypi\neinops                    0.8.1                    pypi_0    pypi\nexceptiongroup            1.2.2                    pypi_0    pypi\nexecuting                 2.2.0                    pypi_0    pypi\nfastapi                   0.115.11                 pypi_0    pypi\nfilelock                  3.17.0                   pypi_0    pypi\nflashinfer-python         0.2.2.post1+cu124torch2.5          pypi_0    pypi\nfrozenlist                1.5.0                    pypi_0    pypi\nfsspec                    2024.12.0                pypi_0    pypi\ngguf                      0.10.0                   pypi_0    pypi\ngoogle-api-core           2.24.1                   pypi_0    pypi\ngoogle-auth               2.38.0                   pypi_0    pypi\ngoogleapis-common-protos  1.69.1                   pypi_0    pypi\ngrpcio                    1.70.0                   pypi_0    pypi\nh11                       0.14.0                   pypi_0    pypi\nhf-transfer               0.1.9                    pypi_0    pypi\nhttpcore                  1.0.7                    pypi_0    pypi\nhttptools                 0.6.4                    pypi_0    pypi\nhttpx                     0.28.1                   pypi_0    pypi\nhuggingface-hub           0.29.2                   pypi_0    pypi\nidna                      3.10                     pypi_0    pypi\nimportlib-metadata        8.6.1                    pypi_0    pypi\ninteregular               0.3.3                    pypi_0    pypi\nipython                   8.34.0                   pypi_0    pypi\njedi                      0.19.2                   pypi_0    pypi\njinja2                    3.1.6                    pypi_0    pypi\njiter                     0.8.2                    pypi_0    pypi\njsonschema                4.23.0                   pypi_0    pypi\njsonschema-specifications 2024.10.1                pypi_0    pypi\nlark                      1.2.2                    pypi_0    pypi\nld_impl_linux-64          2.40                 h12ee557_0\nlibffi                    3.4.4                h6a678d5_1\nlibgcc-ng                 11.2.0               h1234567_1\nlibgomp                   11.2.0               h1234567_1\nlibstdcxx-ng              11.2.0               h1234567_1\nlibuuid                   1.41.5               h5eee18b_0\nlitellm                   1.63.3                   pypi_0    pypi\nllguidance                0.7.0                    pypi_0    pypi\nlm-format-enforcer        0.10.11                  pypi_0    pypi\nmarkupsafe                3.0.2                    pypi_0    pypi\nmatplotlib-inline         0.1.7                    pypi_0    pypi\nmistral-common            1.5.3                    pypi_0    pypi\nmodelscope                1.23.2                   pypi_0    pypi\nmpmath                    1.3.0                    pypi_0    pypi\nmsgpack                   1.1.0                    pypi_0    pypi\nmsgspec                   0.19.0                   pypi_0    pypi\nmultidict                 6.1.0                    pypi_0    pypi\nmultiprocess              0.70.16                  pypi_0    pypi\nncurses                   6.4                  h6a678d5_0\nnest-asyncio              1.6.0                    pypi_0    pypi\nnetworkx                  3.4.2                    pypi_0    pypi\nninja                     1.11.1.3                 pypi_0    pypi\nnumpy                     1.26.4                   pypi_0    pypi\nnvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\nnvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\nnvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\nnvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\nnvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\nnvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\nnvidia-curand-cu12        10.3.5.147               pypi_0    pypi\nnvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\nnvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\nnvidia-ml-py              12.570.86                pypi_0    pypi\nnvidia-nccl-cu12          2.21.5                   pypi_0    pypi\nnvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\nnvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\nopenai                    1.65.4                   pypi_0    pypi\nopencensus                0.11.4                   pypi_0    pypi\nopencensus-context        0.1.3                    pypi_0    pypi\nopencv-python-headless    4.11.0.86                pypi_0    pypi\nopenssl                   3.0.16               h5eee18b_0\norjson                    3.10.15                  pypi_0    pypi\noutlines                  0.1.11                   pypi_0    pypi\noutlines-core             0.1.26                   pypi_0    pypi\npackaging                 24.2                     pypi_0    pypi\npandas                    2.2.3                    pypi_0    pypi\nparso                     0.8.4                    pypi_0    pypi\npartial-json-parser       0.2.1.1.post5            pypi_0    pypi\npexpect                   4.9.0                    pypi_0    pypi\npillow                    11.1.0                   pypi_0    pypi\npip                       25.0.1                   pypi_0    pypi\nplatformdirs              4.3.6                    pypi_0    pypi\nprometheus-client         0.21.1                   pypi_0    pypi\nprometheus-fastapi-instrumentator 7.0.2                    pypi_0    pypi\nprompt-toolkit            3.0.50                   pypi_0    pypi\npropcache                 0.3.0                    pypi_0    pypi\nproto-plus                1.26.0                   pypi_0    pypi\nprotobuf                  5.29.3                   pypi_0    pypi\npsutil                    7.0.0                    pypi_0    pypi\nptyprocess                0.7.0                    pypi_0    pypi\npure-eval                 0.2.3                    pypi_0    pypi\npy-cpuinfo                9.0.0                    pypi_0    pypi\npy-spy                    0.4.0                    pypi_0    pypi\npyarrow                   19.0.1                   pypi_0    pypi\npyasn1                    0.6.1                    pypi_0    pypi\npyasn1-modules            0.4.1                    pypi_0    pypi\npycountry                 24.6.1                   pypi_0    pypi\npydantic                  2.10.6                   pypi_0    pypi\npydantic-core             2.27.2                   pypi_0    pypi\npygments                  2.19.1                   pypi_0    pypi\npython                    3.10.16              he870216_1\npython-dateutil           2.9.0.post0              pypi_0    pypi\npython-dotenv             1.0.1                    pypi_0    pypi\npython-multipart          0.0.20                   pypi_0    pypi\npytz                      2025.1                   pypi_0    pypi\npyyaml                    6.0.2                    pypi_0    pypi\npyzmq                     26.2.1                   pypi_0    pypi\nray                       2.43.0                   pypi_0    pypi\nreadline                  8.2                  h5eee18b_0\nreferencing               0.36.2                   pypi_0    pypi\nregex                     2024.11.6                pypi_0    pypi\nrequests                  2.32.3                   pypi_0    pypi\nrpds-py                   0.23.1                   pypi_0    pypi\nrsa                       4.9                      pypi_0    pypi\nsafetensors               0.5.3                    pypi_0    pypi\nsentencepiece             0.2.0                    pypi_0    pypi\nsetproctitle              1.3.5                    pypi_0    pypi\nsetuptools                75.8.0          py310h06a4308_0\nsgl-kernel                0.0.3.post6              pypi_0    pypi\nsglang                    0.4.3.post4              pypi_0    pypi\nsix                       1.17.0                   pypi_0    pypi\nsmart-open                7.1.0                    pypi_0    pypi\nsniffio                   1.3.1                    pypi_0    pypi\nsqlite                    3.45.3               h5eee18b_0\nstack-data                0.6.3                    pypi_0    pypi\nstarlette                 0.46.1                   pypi_0    pypi\nsympy                     1.13.1                   pypi_0    pypi\ntiktoken                  0.9.0                    pypi_0    pypi\ntk                        8.6.14               h39e8969_0\ntokenizers                0.21.0                   pypi_0    pypi\ntorch                     2.5.1                    pypi_0    pypi\ntorchao                   0.9.0                    pypi_0    pypi\ntorchaudio                2.5.1                    pypi_0    pypi\ntorchvision               0.20.1                   pypi_0    pypi\ntqdm                      4.67.1                   pypi_0    pypi\ntraitlets                 5.14.3                   pypi_0    pypi\ntransformers              4.48.3                   pypi_0    pypi\ntriton                    3.1.0                    pypi_0    pypi\ntyping-extensions         4.12.2                   pypi_0    pypi\ntzdata                    2025.1                   pypi_0    pypi\nurllib3                   2.3.0                    pypi_0    pypi\nuvicorn                   0.34.0                   pypi_0    pypi\nuvloop                    0.21.0                   pypi_0    pypi\nvirtualenv                20.29.3                  pypi_0    pypi\nvllm                      0.7.2                    pypi_0    pypi\nwatchfiles                1.0.4                    pypi_0    pypi\nwcwidth                   0.2.13                   pypi_0    pypi\nwebsockets                15.0.1                   pypi_0    pypi\nwheel                     0.45.1          py310h06a4308_0\nwrapt                     1.17.2                   pypi_0    pypi\nxformers                  0.0.28.post3             pypi_0    pypi\nxgrammar                  0.1.14                   pypi_0    pypi\nxxhash                    3.5.0                    pypi_0    pypi\nxz                        5.6.4                h5eee18b_1\nyarl                      1.18.3                   pypi_0    pypi\nzipp                      3.21.0                   pypi_0    pypi\nzlib                      1.2.13               h5eee18b_1\n",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-10T05:30:12+00:00",
    "closed_at": "2025-05-10T00:18:05+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4251/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4251"
  },
  {
    "number": 1655,
    "title": "[Feature] sanic Custom Server example  support openai  stream  api ?",
    "body": "### Checklist\n\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCustom Server example  surport openai  stream  api ?\n\n### Related resources\n\nCustom Server example  surport openai  stream  api ?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-10-13T05:15:56+00:00",
    "closed_at": "2024-10-13T16:17:17+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1655/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1655"
  },
  {
    "number": 6044,
    "title": "[Bug] google/gemma-3 fails to launch when attention_backend is torch_native",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI was trying to launch google/gemma-3-1b-it with torch_native as attention backend. However, it failed and the error message is:\n\n```\n[2025-05-06 06:19:33] TpModelWorkerClient hit an exception: Traceback (most recent call last):\n  File \"/home/chenlixiang/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 118, in forward_thread_func\n    self.forward_thread_func_()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/home/chenlixiang/miniconda3/envs/sglang-dev/lib/python3.13/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/chenlixiang/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 148, in forward_thread_func_\n    logits_output, next_token_ids = self.worker.forward_batch_generation(\n                                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        model_worker_batch\n        ^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/chenlixiang/sglang/python/sglang/srt/managers/tp_worker.py\", line 206, in forward_batch_generation\n    logits_output = self.model_runner.forward(\n        forward_batch, pp_proxy_tensors=pp_proxy_tensors\n    )\n  File \"/home/chenlixiang/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1097, in forward\n    return self.forward_extend(\n           ~~~~~~~~~~~~~~~~~~~^\n        forward_batch,\n        ^^^^^^^^^^^^^^\n        skip_attn_backend_init=skip_attn_backend_init,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        pp_proxy_tensors=pp_proxy_tensors,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/chenlixiang/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1056, in forward_extend\n    return self.model.forward(\n           ~~~~~~~~~~~~~~~~~~^\n        forward_batch.input_ids,\n        ^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<2 lines>...\n        **kwargs,\n        ^^^^^^^^^\n    )\n    ^\n  File \"/home/chenlixiang/miniconda3/envs/sglang-dev/lib/python3.13/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/chenlixiang/sglang/python/sglang/srt/models/gemma3_causal.py\", line 636, in forward\n    hidden_states = self.model(\n        input_ids, positions, forward_batch, input_embeds, **kwargs\n    )\n  File \"/home/chenlixiang/miniconda3/envs/sglang-dev/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/chenlixiang/miniconda3/envs/sglang-dev/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/chenlixiang/sglang/python/sglang/srt/models/gemma3_causal.py\", line 526, in forward\n    layer_outputs = layer(\n        positions=positions,\n    ...<4 lines>...\n        **kwargs,\n    )\n  File \"/home/chenlixiang/miniconda3/envs/sglang-dev/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/chenlixiang/miniconda3/envs/sglang-dev/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/chenlixiang/sglang/python/sglang/srt/models/gemma3_causal.py\", line 344, in forward\n    hidden_states = self.self_attn(\n        positions=positions,\n    ...<3 lines>...\n        **kwargs,\n    )\n  File \"/home/chenlixiang/miniconda3/envs/sglang-dev/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/chenlixiang/miniconda3/envs/sglang-dev/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/chenlixiang/sglang/python/sglang/srt/models/gemma3_causal.py\", line 280, in forward\n    output, _ = self.o_proj(attn_output)\n                ~~~~~~~~~~~^^^^^^^^^^^^^\n  File \"/home/chenlixiang/miniconda3/envs/sglang-dev/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/chenlixiang/miniconda3/envs/sglang-dev/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/chenlixiang/sglang/python/sglang/srt/layers/linear.py\", line 1291, in forward\n    output_parallel = self.quant_method.apply(self, input_parallel, bias=bias_)\n  File \"/home/chenlixiang/sglang/python/sglang/srt/layers/linear.py\", line 175, in apply\n    return F.linear(x, layer.weight, bias)\n           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected size for first two dimensions of batch2 tensor to be: [7, 256] but got: [7, 1024].\n```\n\nPlease note that this model works fine if the attention backend is `flashinfer`. The `torch_native` backend also works fine for other models, for example,  Qwen/Qwen2.5-7B-Instruct-1M. By the way, another model in gemma-3 family, `google/gemma-3-4b-it` had the same problem. \n\n### Reproduction\n\nSGLang is built from source. The launch command is `python3 -m sglang.launch_server --model-path google/gemma-3-1b-it --attention-backend torch_native`. The model is `google/gemma-3-1b-it`.\n\n### Environment\n\nSGLang is running on a machine with NVIDIA GeForce RTX 4090, Driver Version: 550.144.03     CUDA Version: 12.4.\n\nThe operating system is Ubuntu 24.04.",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-05-06T06:32:32+00:00",
    "closed_at": "2025-07-06T00:22:07+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6044/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6044"
  },
  {
    "number": 5527,
    "title": "Cuda graph supported bs in DP attention",
    "body": "I have been testing and recording the output throughput of SGLang on 2*8 H100 GPUs, and I've observed a significant regression in output throughput for long outputs in the `enable-dp-attention` scenarios following this [PR](https://github.com/sgl-project/sglang/pull/4390). Through debugging and profiling with Nsight Systems, I confirmed that the performance degradation is caused by the CUDA graph not being properly launched.\n\nSee the [code](https://github.com/sgl-project/sglang/blob/00391f58a3c2b13b00dc0a0a983a5c7fab399883/python/sglang/srt/model_executor/cuda_graph_runner.py#L280)\n```\nif self.enable_dp_attention:\n    total_global_tokens = sum(forward_batch.global_num_tokens_cpu)\n\n    is_bs_supported = forward_batch.can_run_dp_cuda_graph and (\n        total_global_tokens in self.graphs\n        if self.disable_padding\n        else total_global_tokens <= self.max_bs\n    )\n```\nAfter `enable-dp-attention`, `total_global_tokens` equals the sum of tokens across all DP ranks. For example, during the decode phase with DP = TP = 16 and a per-rank batch size of 32, the `total_global_tokens` would be 32 * 16 = 512. However, the maximum batch size allowed for CUDA graph capture defaults to 160. As a result, the `can_run` function during the decode phase returns `False`, and CUDA graph execution is consequently skipped.\n\nIn fact, according to the design logic of the [PR](https://github.com/sgl-project/sglang/pull/4390), I don't consider this a bug\u2014it consistently uses the `total_global_tokens` across all ranks as the batch size to be captured by the CUDA graph. A straightforward solution would be to set a sufficiently large `cuda-graph-max-bs` when launching the server, though this might consume a significant amount of additional memory.\n\nI believe using the `num_tokens` per DP rank as the CUDA graph batch size might be a more reasonable approach, similar to the code prior to this [PR](https://github.com/sgl-project/sglang/pull/4390). It would only require reserving adequate space for the `gathered_buffer`.\n\nBelow are my test output throughput before and after this [PR](https://github.com/sgl-project/sglang/pull/4390).\n2*8 H100, input_len=output_len=1000, DP=TP=16\n| Concurrency| before PR | after PR | after PR-fix |\n|:---:|:---:|:---:|:---:|\n| 1024  | 5115.13  | 2581.07  | 5469.53  |\n| 512  | 3897.78  | 1527.95  | 4509.96  |\n",
    "labels": [],
    "state": "open",
    "created_at": "2025-04-18T10:10:19+00:00",
    "closed_at": null,
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5527/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5527"
  },
  {
    "number": 409,
    "title": "LLaVA-v1.6 RuntimeError in llava image encoding",
    "body": "There still seems to be a bug in the newer LLaVA-v1.6 version where, for some images, the model only generates one or two tokens. The problem seems to be related to some kind of attributes of the images themselves, as changing the textual input has no influence. Furthermore, all 3 v1.6 models (7b, 13b, and 34b) have problems with the same images. Moreover, the 1.5 version works perfectly fine with the same inputs. This bug appears for around 5% of my images.\r\nI'm on the sglang 0.1.14 and vllm 0.3.3. The issue seems to be related to #273, however i do not use regex for generation. The server casts the following runtime error when llava is not able to process the image:\r\n`\r\nRuntimeError in llava image encoding: The expanded size of the tensor (0) must match the existing size (2438) at non-singleton dimension 0.  Target sizes: [0, 4096].  Tensor sizes: [2438, 4096]\r\ntorch.Size([10194, 4096])\r\n0 -1`",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-05-04T16:02:03+00:00",
    "closed_at": "2024-07-25T06:33:29+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/409/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/409"
  },
  {
    "number": 5612,
    "title": "[Bug] TypeError: moe_fused_gate() takes 5 positional arguments but 7 were given",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen I run the following script on the latest code of the dual node H20, the following error appears\n\n`python3 -m sglang.launch_server --model-path /media/nvme/deepseek/DeepSeek-V3-0324 --tensor-parallel-size 16 --trust-remote-code --dist-init-addr ****  --enable-metrics --port 50000 --nnodes 2 --node-rank 0`\n\n[2025-04-22 10:37:30 DP0 TP0] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/managers/scheduler.py\", line 2001, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/managers/scheduler.py\", line 261, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/managers/tp_worker.py\", line 75, in __init__\n    self.model_runner = ModelRunner(\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/model_executor/model_runner.py\", line 173, in __init__\n    self.initialize(min_per_gpu_memory)\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/model_executor/model_runner.py\", line 211, in initialize\n    self.init_cuda_graphs()\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/model_executor/model_runner.py\", line 972, in init_cuda_graphs\n    self.cuda_graph_runner = CudaGraphRunner(self)\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 281, in __init__\n    self.capture()\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 365, in capture\n    ) = self.capture_one_batch_size(bs, forward)\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 457, in capture_one_batch_size\n    run_once()\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 450, in run_once\n    logits_output = forward(input_ids, forward_batch.positions, forward_batch)\n  File \"/media/nvme/anaconda3/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/models/deepseek_v2.py\", line 1467, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)\n  File \"/media/nvme/anaconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/media/nvme/anaconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/models/deepseek_v2.py\", line 1391, in forward\n    hidden_states, residual = layer(\n  File \"/media/nvme/anaconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/media/nvme/anaconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/models/deepseek_v2.py\", line 1178, in forward\n    return self.forward_ffn_with_full_input(\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/models/deepseek_v2.py\", line 1238, in forward_ffn_with_full_input\n    hidden_states = self.mlp(hidden_states)\n  File \"/media/nvme/anaconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/media/nvme/anaconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/models/deepseek_v2.py\", line 293, in forward\n    return self.forward_normal(hidden_states)\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/models/deepseek_v2.py\", line 302, in forward_normal\n    self.experts(hidden_states=hidden_states, router_logits=router_logits)\n  File \"/media/nvme/anaconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/media/nvme/anaconda3/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/layers/moe/fused_moe_triton/layer.py\", line 633, in forward\n    final_hidden_states = self.quant_method.apply(\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/layers/quantization/fp8.py\", line 901, in apply\n    topk_weights, topk_ids = select_experts(\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/layers/moe/topk.py\", line 289, in select_experts\n    topk_weights, topk_ids = biased_grouped_topk(\n  File \"/media/nvme/workspace/sglang-test/python/sglang/srt/layers/moe/topk.py\", line 229, in biased_grouped_topk\n    return moe_fused_gate(\nTypeError: moe_fused_gate() takes 5 positional arguments but 7 were given\n\n### Reproduction\n\n`python3 -m sglang.launch_server --model-path /media/nvme/deepseek/DeepSeek-V3-0324 --tensor-parallel-size 16 --trust-remote-code --dist-init-addr ****  --enable-metrics --port 50000 --nnodes 2 --node-rank 0`\n\n### Environment\n\nPython: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda-12.8\nNVCC: Cuda compilation tools, release 12.8, V12.8.93\nCUDA Driver Version: 535.161.08\nPyTorch: 2.5.1+cu124\nsglang: 0.4.5.post1\nsgl_kernel: 0.0.9.post1\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.51.1\ntorchao: 0.10.0\nnumpy: 2.2.4\naiohttp: 3.11.16\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.11.3\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.1\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.17\nopenai: 1.75.0\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.66.1\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    SYS     PIX     NODE    SYS     SYS     0-89    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     PIX     NODE    SYS     SYS     0-89    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     NODE    PIX     SYS     SYS     0-89    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     NODE    PIX     SYS     SYS     0-89    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     PIX     NODE    90-179  1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     PIX     NODE    90-179  1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     NODE    PIX     90-179  1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     NODE    PIX     90-179  1               N/A\nNIC0    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS\nNIC1    PIX     PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS      X      NODE    SYS     SYS\nNIC2    NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     SYS     NODE     X      SYS     SYS\nNIC3    SYS     SYS     SYS     SYS     PIX     PIX     NODE    NODE    SYS     SYS     SYS      X      NODE\nNIC4    SYS     SYS     SYS     SYS     NODE    NODE    PIX     PIX     SYS     SYS     SYS     NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n\n\nHypervisor vendor: KVM\nulimit soft: 65535",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-22T02:47:03+00:00",
    "closed_at": "2025-04-28T02:36:48+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5612/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5612"
  },
  {
    "number": 3797,
    "title": "[Bug] SGLang Tool Calling for Qwen2.5 models returns empty ChatCompletionMessage content",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nSo the story is I deployed a Qwen2.5-72B-Instruct model with SGLang on 8 cards, been testing the tool calling agents with LangGraph. The agents do not output any content when it decides to call a tool, therefore it just decides to randomly call tools until it suddenly stops. The behavior has led to very poor agentic performance.\n\nOn vLLM side, an agent tool call is accompanied by justifications and it will not randomly decide to call/not call a tool without justifications.\n\n### Reproduction\n\nThe tool call interface simply makes LLM response content to None therefore a ReAct style agent will perform poorly. \n\n### Environment\n\nINFO 02-24 01:40:19 __init__.py:190] Automatically detected platform cuda.\nPython: 3.12.8 (main, Jan 27 2025, 17:53:37) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.127.05\nPyTorch: 2.5.1+cu124\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.1.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.0\ninteregular: 0.3.3\nmodelscope: 1.23.0\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.63.2\ntiktoken: 0.9.0\nanthropic: 0.46.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    CPU Affinity  NUMA Affinity    GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    0-55,112-167  0N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE    0-55,112-167  0N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    0-55,112-167  0N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    0-55,112-167  0N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     56-111,168-2231N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     56-111,168-2231N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     56-111,168-2231N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     56-111,168-2231N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE\nNIC1    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     SYS     SYS     SYS     NODE\nNIC2    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS     SYS     SYS     SYS     NODE\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS     NODE\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS\nNIC5    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS\nNIC8    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_5\n  NIC4: mlx5_6\n  NIC5: mlx5_7\n  NIC6: mlx5_8\n  NIC7: mlx5_9\n  NIC8: mlx5_bond_0\n\n\nulimit soft: 1048576",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-02-24T01:41:21+00:00",
    "closed_at": "2025-05-06T00:18:59+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3797/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3797"
  },
  {
    "number": 6418,
    "title": "[Bug] 0.4.6-post4-cu124\uff0c run deepseekv30324 error",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nsglang:v0.4.4 is OK, but v0.4.6 is not.\n\ndockerimages:   lmsysorg/sglang:v0.4.6.post4-cu124\ndocker run --name sglang -v /export:/export -d --gpus all --shm-size 128g -p 80:80 --privileged --entrypoint \"sleep\" 65be730e0d41 infinity\n\nsglang:\npython3 -m sglang.launch_server --quantization fp8 --kv-cache-dtype fp8_e5m2 --model /export/model/deepseek/DeepSeek-V3-0324 --tp 8 --port 80 --host 0.0.0.0 --mem-fraction-static 0.9 --disable-cuda-graph --tool-call-parser deepseekv3 --chat-template /export/model/deepseek/DeepSeek-V3-0324/tool_chat_template_deepseekv3.jinja\n\nerror:\n[2025-05-19 08:01:20] Fail to set RLIMIT_NOFILE: current limit exceeds maximum limit\n[2025-05-19 08:01:20] server_args=ServerArgs(model_path='/export/model/deepseek/DeepSeek-V3-0324', tokenizer_path='/export/model/deepseek/DeepSeek-V3-0324', tokenizer_mode='auto', skip_tokenizer_init=False, enable_tokenizer_batch_encode=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='fp8_e5m2', quantization='fp8', quantization_param_path=None, context_length=None, device='cuda', served_model_name='/export/model/deepseek/DeepSeek-V3-0324', chat_template='/export/model/deepseek/DeepSeek-V3-0324/tool_chat_template_deepseekv3.jinja', completion_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=80, mem_fraction_static=0.9, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=8, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=722047733, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, enable_request_time_stats_logging=False, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=True, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_multimodal=None, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode='auto', enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=None, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser='deepseekv3', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, mm_attention_backend=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_bootstrap_port=8998, disaggregation_transfer_backend='mooncake', disaggregation_ib_device=None, pdlb_url=None)\n`rope_scaling`'s factor field must be a float >= 1, got 40\n`rope_scaling`'s beta_fast field must be a float, got 32\n`rope_scaling`'s beta_slow field must be a float, got 1\n[2025-05-19 08:01:21] Use chat template for the OpenAI-compatible API server: /export/model/deepseek/DeepSeek-V3-0324/tool_chat_template_deepseekv3.jinja\nProcess Process-6:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2238, in run_scheduler_process\n    faulthandler.enable()\nOSError: [Errno 12] Cannot allocate memory\nProcess Process-7:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2238, in run_scheduler_process\n    faulthandler.enable()\nOSError: [Errno 12] Cannot allocate memory\nProcess Process-2:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2238, in run_scheduler_process\n    faulthandler.enable()\nOSError: [Errno 12] Cannot allocate memory\nProcess Process-8:\nTraceback (most recent call last):\nProcess Process-3:\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2238, in run_scheduler_process\n    faulthandler.enable()\nOSError: [Errno 12] Cannot allocate memory\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2238, in run_scheduler_process\n    faulthandler.enable()\nOSError: [Errno 12] Cannot allocate memory\nProcess Process-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2238, in run_scheduler_process\n    faulthandler.enable()\nOSError: [Errno 12] Cannot allocate memory\nProcess Process-5:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2238, in run_scheduler_process\n    faulthandler.enable()\nOSError: [Errno 12] Cannot allocate memory\nProcess Process-4:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2238, in run_scheduler_process\n    faulthandler.enable()\nOSError: [Errno 12] Cannot allocate memory\n[2025-05-19 08:01:31] Rank 0 scheduler is dead. Please check if there are relevant logs.\n[2025-05-19 08:01:31] Child process unexpectedly failed with an exit code 256. pid=1692\n[2025-05-19 08:01:31] Child process unexpectedly failed with an exit code 256. pid=1690\n[2025-05-19 08:01:31] Child process unexpectedly failed with an exit code 256. pid=1689\n[2025-05-19 08:01:31] Child process unexpectedly failed with an exit code 256. pid=1693\n[2025-05-19 08:01:31] Exit code: 1\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/sgl-workspace/sglang/python/sglang/launch_server.py\", line 14, in <module>\n    launch_server(server_args)\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/http_server.py\", line 741, in launch_server\n    tokenizer_manager, scheduler_info = _launch_subprocesses(server_args=server_args)\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/engine.py\", line 644, in _launch_subprocesses\n    data = scheduler_pipe_readers[i].recv()\n  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n    buf = self._recv(4)\n  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 383, in _recv\n    raise EOFError\nEOFError\nroot@2735c75a627e:/sgl-workspace# exit\nexit\nFATA[0295] exec failed with exit code 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### Reproduction\n\npython3 -m sglang.launch_server --quantization fp8 --kv-cache-dtype fp8_e5m2 --model /export/model/deepseek/DeepSeek-V3-0324 --tp 8 --port 80 --host 0.0.0.0 --mem-fraction-static 0.9 --disable-cuda-graph --tool-call-parser deepseekv3 --chat-template /export/model/deepseek/DeepSeek-V3-0324/tool_chat_template_deepseekv3.jinja\n\n### Environment\n\nlmsysorg/sglang:v0.4.6.post4-cu124",
    "labels": [],
    "state": "open",
    "created_at": "2025-05-19T08:04:57+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6418/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6418"
  },
  {
    "number": 6901,
    "title": "[Bug] In prefill phase the number of sequences is low",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI want to decrease the ttft on my server. I have set the context-length to 163840 which should be there anyway because it's in the `config.json` of deepseek v3. \nI also set the chunked-prefill-size to 32768.\nFor testing I'm sending 1024 request each has 5000 tokens, and the prefill never goes above 15000\n```\nsglang  | [2025-06-05 15:08:56 TP0] Prefill batch. #new-seq: 3, #new-token: 15000, #cached-token: 0, token usage: 0.06, #running-req: 7, #queue-req: 40\nsglang  | [2025-06-05 15:08:57 TP0] Prefill batch. #new-seq: 3, #new-token: 15000, #cached-token: 0, token usage: 0.09, #running-req: 10, #queue-req: 56\nsglang  | [2025-06-05 15:08:58 TP0] Prefill batch. #new-seq: 3, #new-token: 15000, #cached-token: 0, token usage: 0.11, #running-req: 13, #queue-req: 71\nsglang  | [2025-06-05 15:08:59 TP0] Prefill batch. #new-seq: 3, #new-token: 15000, #cached-token: 0, token usage: 0.14, #running-req: 16, #queue-req: 86\n```\nIt just queueing. Should it do more? The token usage is low.\n\n### Reproduction\n\npython3 -m sglang.launch_server  \\\n--model-path ${SGLANG_MODEL_PATH} \\\n--tp 8 \\\n--trust-remote-code \\\n--speculative-draft-model-path lmsys/DeepSeek-V3-NextN \\\n--speculative-algorithm EAGLE \\\n--speculative-num-steps 2 \\\n--speculative-eagle-topk 2 \\\n--speculative-num-draft-tokens 4 \\\n--cuda-graph-bs 1 2 4 8 16 32 40 48 56 64 128 \\\n--max-running-requests 128 \\\n--enable-metrics \\\n--host 0.0.0.0 \\\n--port 3000 \\\n--context-length 163840 \\\n--chunked-prefill-size 32768\n\n### Environment\n\nOpenBLAS WARNING - could not determine the L2 cache size on this system, assuming 256k\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H200\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 570.124.06\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post5\nsgl_kernel: 0.1.4\nflashinfer_python: 0.2.5+cu124torch2.6\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.9.0\nnumpy: 2.2.6\naiohttp: 3.11.18\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.32.0\ninteregular: 0.3.3\nmodelscope: 1.26.0\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.5\npython-multipart: 0.0.20\npyzmq: 26.4.0\nuvicorn: 0.34.2\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.19\nopenai: 1.82.0\ntiktoken: 0.9.0\nanthropic: 0.52.0\nlitellm: 1.70.4\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    0-191   0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    0-191   0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    0-191   0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    0-191   0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    0-191   0               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    0-191   0               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    0-191   0               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      0-191   0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nHypervisor vendor: KVM\nulimit soft: 1048576",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-05T15:13:36+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6901/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6901"
  },
  {
    "number": 4094,
    "title": "[Bug] AssertionError: res=<Response [503]> Process was always killed automactically",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI successfully ran the serve  using command after installing SGLang. But after about 2 minutes, the process is always killed automatically.\n\n### Reproduction\n\nI depoly Qwen2.5-7B-Instruct using the below command\n\n```\nCUDA_VISIBLE_DEVICES=1 python -m sglang.launch_server --model-path /data/models/Qwen2.5/Qwen2.5-7B-Instruct --api-key base --host 0.0.0.0 --port 8039\n```\n\nIt works at tht beginning. \n```\nminiconda3/envs/sglang/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\nINFO 03-05 10:31:58 __init__.py:207] Automatically detected platform cuda.\n2025-03-05 10:32:02,424 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n[2025-03-05 10:32:09] server_args=ServerArgs(model_path='/data/models/Qwen2.5/Qwen2.5-7B-Instruct', tokenizer_path='/data/models/Qwen2.5/Qwen2.5-7B-Instruct', tokenizer_mode='auto', load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, quantization=None, context_length=None, device='cuda', served_model_name='/data/models/Qwen2.5/Qwen2.5-7B-Instruct', chat_template=None, is_embedding=False, revision=None, skip_tokenizer_init=False, host='0.0.0.0', port=8039, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=1, stream_interval=1, stream_output=False, random_seed=88492145, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key='base', file_storage_pth='sglang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_draft_model_path=None, speculative_algorithm=None, speculative_num_steps=5, speculative_num_draft_tokens=64, speculative_eagle_topk=8, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, return_hidden_states=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=False)\n/data/lzh/miniconda3/envs/sglang/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\n/data/lzh/miniconda3/envs/sglang/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n  warnings.warn(\nINFO 03-05 10:32:17 __init__.py:207] Automatically detected platform cuda.\nINFO 03-05 10:32:17 __init__.py:207] Automatically detected platform cuda.\n2025-03-05 10:32:21,123 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n2025-03-05 10:32:21,125 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n[2025-03-05 10:32:26 TP0] Init torch distributed begin.\n[2025-03-05 10:32:28 TP0] Load weight begin. avail mem=38.97 GB\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.70it/s]\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.62it/s]\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.57it/s]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.59it/s]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.60it/s]\n\n[2025-03-05 10:32:31 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=24.59 GB\n[2025-03-05 10:32:31 TP0] KV Cache is allocated. K size: 9.96 GB, V size: 9.96 GB.\n[2025-03-05 10:32:31 TP0] Memory pool end. avail mem=4.08 GB\n[2025-03-05 10:32:31 TP0] Capture cuda graph begin. This can take up to several minutes.\n  0%|                                                                                                                        | 0/23 [00:00<?, ?it/s]2025-03-05 10:32:32,461 - INFO - flashinfer.jit: Loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\n2025-03-05 10:32:32,513 - INFO - flashinfer.jit: Finished loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 23/23 [00:15<00:00,  1.51it/s]\n[2025-03-05 10:32:47 TP0] Capture cuda graph end. Time elapsed: 15.22 s\n[2025-03-05 10:32:47 TP0] max_total_num_tokens=372945, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4097, context_len=32768\n[2025-03-05 10:32:47] INFO:     Started server process [302802]\n[2025-03-05 10:32:47] INFO:     Waiting for application startup.\n[2025-03-05 10:32:47] INFO:     Application startup complete.\n[2025-03-05 10:32:47] INFO:     Uvicorn running on http://0.0.0.0:8039 (Press CTRL+C to quit)\n```\n\nThen about 2 minutes alter, the process was killed and got an error as follows:\n```\n[2025-03-05 10:34:48] Initialization failed. warmup error: Traceback (most recent call last):\n  File \"miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/entrypoints/http_server.py\", line 548, in _wait_and_warmup\n    assert res.status_code == 200, f\"{res=}, {res.text=}\"\nAssertionError: res=<Response [503]>, res.text='<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\" \"http://www.w3.org/TR/html4/strict.dtd\">\\n<html><head>\\n<meta type=\"copyright\" content=\"Copyright (C) 1996-2016 The Squid Software Foundation and contributors\">\\n<meta http-equiv=\"Content-Type\" CONTENT=\"text/html; charset=utf-8\">\\n<title>ERROR: The requested URL could not be retrieved</title>\\n<style type=\"text/css\"><!-- \\n /*\\n * Copyright (C) 1996-2016 The Squid Software Foundation and contributors\\n *\\n * Squid software is distributed under GPLv2+ license and includes\\n * contributions from numerous individuals and organizations.\\n * Please see the COPYING and CONTRIBUTORS files for details.\\n */\\n\\n/*\\n Stylesheet for Squid Error pages\\n Adapted from design by Free CSS Templates\\n http://www.freecsstemplates.org\\n Released for free under a Creative Commons Attribution 2.5 License\\n*/\\n\\n/* Page basics */\\n* {\\n\\tfont-family: verdana, sans-serif;\\n}\\n\\nhtml body {\\n\\tmargin: 0;\\n\\tpadding: 0;\\n\\tbackground: #efefef;\\n\\tfont-size: 12px;\\n\\tcolor: #1e1e1e;\\n}\\n\\n/* Page displayed title area */\\n#titles {\\n\\tmargin-left: 15px;\\n\\tpadding: 10px;\\n\\tpadding-left: 100px;\\n\\tbackground: url(\\'/squid-internal-static/icons/SN.png\\') no-repeat left;\\n}\\n\\n/* initial title */\\n#titles h1 {\\n\\tcolor: #000000;\\n}\\n#titles h2 {\\n\\tcolor: #000000;\\n}\\n\\n/* special event: FTP success page titles */\\n#titles ftpsuccess {\\n\\tbackground-color:#00ff00;\\n\\twidth:100%;\\n}\\n\\n/* Page displayed body content area */\\n#content {\\n\\tpadding: 10px;\\n\\tbackground: #ffffff;\\n}\\n\\n/* General text */\\np {\\n}\\n\\n/* error brief description */\\n#error p {\\n}\\n\\n/* some data which may have caused the problem */\\n#data {\\n}\\n\\n/* the error message received from the system or other software */\\n#sysmsg {\\n}\\n\\npre {\\n    font-family:sans-serif;\\n}\\n\\n/* special event: FTP directory listing */\\n#dirmsg {\\n    font-family: courier;\\n    color: black;\\n    font-size: 10pt;\\n}\\n#dirlisting {\\n    margin-left: 2%;\\n    margin-right: 2%;\\n}\\n#dirlisting tr.entry td.icon,td.filename,td.size,td.date {\\n    border-bottom: groove;\\n}\\n#dirlisting td.size {\\n    width: 50px;\\n    text-align: right;\\n    padding-right: 5px;\\n}\\n\\n/* horizontal lines */\\nhr {\\n\\tmargin: 0;\\n}\\n\\n/* page displayed footer area */\\n#footer {\\n\\tfont-size: 9px;\\n\\tpadding-left: 10px;\\n}\\n\\n\\nbody\\n:lang(fa) { direction: rtl; font-size: 100%; font-family: Tahoma, Roya, sans-serif; float: right; }\\n:lang(he) { direction: rtl; }\\n --></style>\\n</head><body id=ERR_CONNECT_FAIL>\\n<div id=\"titles\">\\n<h1>ERROR</h1>\\n<h2>The requested URL could not be retrieved</h2>\\n</div>\\n<hr>\\n\\n<div id=\"content\">\\n<p>The following error was encountered while trying to retrieve the URL: <a href=\"http://0.0.0.0:8039/get_model_info\">http://0.0.0.0:8039/get_model_info</a></p>\\n\\n<blockquote id=\"error\">\\n<p><b>Connection to 0.0.0.0 failed.</b></p>\\n</blockquote>\\n\\n<p id=\"sysmsg\">The system returned: <i>(111) Connection refused</i></p>\\n\\n<p>The remote host or network may be down. Please try the request again.</p>\\n\\n<p>Your cache administrator is <a href=\"mailto:root?subject=CacheErrorInfo%20-%20ERR_CONNECT_FAIL&amp;body=CacheHost%3A%20fpvg37hr3o8frt0%0D%0AErrPage%3A%20ERR_CONNECT_FAIL%0D%0AErr%3A%20(111)%20Connection%20refused%0D%0ATimeStamp%3A%20Wed,%2005%20Mar%202025%2010%3A26%3A34%20GMT%0D%0A%0D%0AClientIP%3A%20192.168.0.11%0D%0AServerIP%3A%200.0.0.0%0D%0A%0D%0AHTTP%20Request%3A%0D%0AGET%20%2Fget_model_info%20HTTP%2F1.1%0AUser-Agent%3A%20python-requests%2F2.32.3%0D%0AAccept-Encoding%3A%20gzip,%20deflate%0D%0AAccept%3A%20*%2F*%0D%0AConnection%3A%20keep-alive%0D%0AAuthorization%3A%20Bearer%20base%0D%0AHost%3A%200.0.0.0%3A8039%0D%0A%0D%0A%0D%0A\">root</a>.</p>\\n\\n<br>\\n</div>\\n\\n<hr>\\n<div id=\"footer\">\\n<p>Generated Wed, 05 Mar 2025 10:26:34 GMT by fpvg37hr3o8frt0 (squid/3.5.20)</p>\\n<!-- ERR_CONNECT_FAIL -->\\n</div>\\n</body></html>\\n'\n\nKilled\n```\n\n### Environment\n\nPython 3.10.14\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Wed_Nov_22_10:17:15_PST_2023\nCuda compilation tools, release 12.3, V12.3.107\nBuild cuda_12.3.r12.3/compiler.33567101_0\nPackage                           Version\n--------------------------------- -------------\naiohappyeyeballs                  2.4.3\naiohttp                           3.10.10\naiosignal                         1.3.1\nairportsdata                      20250224\nannotated-types                   0.7.0\nanyio                             4.6.2.post1\nastor                             0.8.1\nasttokens                         3.0.0\nasync-timeout                     4.0.3\nattrs                             24.2.0\nblake3                            1.0.4\ncertifi                           2024.8.30\ncharset-normalizer                3.4.0\nclick                             8.1.7\ncloudpickle                       3.1.0\ncoloredlogs                       15.0.1\ncompressed-tensors                0.9.1\ncupy-cuda12x                      13.4.0\ndatasets                          3.0.1\ndecorator                         5.2.1\ndecord                            0.6.0\ndepyf                             0.18.0\ndill                              0.3.8\ndiskcache                         5.6.3\ndistro                            1.9.0\ndnspython                         2.7.0\neinops                            0.8.1\nemail_validator                   2.2.0\net_xmlfile                        2.0.0\nexceptiongroup                    1.2.2\nexecuting                         2.2.0\nfastapi                           0.115.2\nfastapi-cli                       0.0.7\nfastrlock                         0.8.3\nfilelock                          3.16.1\nflashinfer-python                 0.2.2.post1\nflatbuffers                       25.1.24\nfrozenlist                        1.4.1\nfsspec                            2024.6.1\ngguf                              0.10.0\nh11                               0.14.0\nhttpcore                          1.0.6\nhttptools                         0.6.2\nhttpx                             0.27.2\nhuggingface-hub                   0.29.1\nhumanfriendly                     10.0\nidna                              3.10\nimportlib_metadata                8.5.0\niniconfig                         2.0.0\ninteregular                       0.3.3\nipython                           8.33.0\njedi                              0.19.2\nJinja2                            3.1.4\njiter                             0.6.1\njoblib                            1.4.2\njsonschema                        4.23.0\njsonschema-specifications         2024.10.1\nlark                              1.2.2\nllvmlite                          0.43.0\nlm-format-enforcer                0.10.11\nloguru                            0.7.3\nmarkdown-it-py                    3.0.0\nMarkupSafe                        3.0.1\nmatplotlib-inline                 0.1.7\nmdurl                             0.1.2\nmistral_common                    1.5.3\nmpmath                            1.3.0\nmsgpack                           1.1.0\nmsgspec                           0.18.6\nmultidict                         6.1.0\nmultiprocess                      0.70.16\nnest-asyncio                      1.6.0\nnetworkx                          3.4.1\nninja                             1.11.1.3\nnumba                             0.60.0\nnumpy                             1.26.4\nnvidia-cublas-cu12                12.1.3.1\nnvidia-cuda-cupti-cu12            12.1.105\nnvidia-cuda-nvrtc-cu12            12.1.105\nnvidia-cuda-runtime-cu12          12.1.105\nnvidia-cudnn-cu12                 9.1.0.70\nnvidia-cufft-cu12                 11.0.2.54\nnvidia-curand-cu12                10.3.2.106\nnvidia-cusolver-cu12              11.4.5.107\nnvidia-cusparse-cu12              12.1.0.106\nnvidia-ml-py                      12.560.30\nnvidia-nccl-cu12                  2.21.5\nnvidia-nvjitlink-cu12             12.4.127\nnvidia-nvtx-cu12                  12.1.105\nonnx                              1.17.0\nonnxruntime-gpu                   1.20.1\nopenai                            1.65.3\nopencv-python-headless            4.11.0.86\nopenpyxl                          3.1.5\norjson                            3.10.15\noutlines                          0.1.11\noutlines_core                     0.1.26\npackaging                         24.1\npandas                            2.2.3\nparso                             0.8.4\npartial-json-parser               0.2.1.1.post4\npexpect                           4.9.0\npillow                            10.4.0\npip                               24.2\npluggy                            1.5.0\nprometheus_client                 0.21.0\nprometheus-fastapi-instrumentator 7.0.0\nprompt_toolkit                    3.0.50\npropcache                         0.2.0\nprotobuf                          5.28.2\npsutil                            6.0.0\nptyprocess                        0.7.0\npure_eval                         0.2.3\npy-cpuinfo                        9.0.0\npyairports                        2.1.1\npyarrow                           17.0.0\npybind11                          2.13.6\npycountry                         24.6.1\npydantic                          2.9.2\npydantic_core                     2.23.4\nPygments                          2.19.1\npytest                            8.3.5\npython-dateutil                   2.9.0.post0\npython-dotenv                     1.0.1\npython-multipart                  0.0.20\npytz                              2024.2\nPyYAML                            6.0.2\npyzmq                             26.2.0\nray                               2.40.0\nreferencing                       0.35.1\nregex                             2024.9.11\nrequests                          2.32.3\nrich                              13.9.4\nrich-toolkit                      0.13.2\nrpds-py                           0.20.0\nsafetensors                       0.4.5\nscikit-learn                      1.5.2\nscipy                             1.14.1\nsentence-transformers             3.0.1\nsentencepiece                     0.2.0\nsetproctitle                      1.3.5\nsetuptools                        75.1.0\nsgl-kernel                        0.0.3.post6\nsglang                            0.4.3.post2\nshellingham                       1.5.4\nsix                               1.16.0\nsniffio                           1.3.1\nstack-data                        0.6.3\nstarlette                         0.40.0\nsympy                             1.13.1\nthreadpoolctl                     3.5.0\ntiktoken                          0.7.0\ntokenizers                        0.21.0\ntomli                             2.2.1\ntorch                             2.5.0+cu121\ntorchao                           0.9.0\ntorchaudio                        2.5.0+cu121\ntorchvision                       0.20.0+cu121\ntqdm                              4.66.5\ntraitlets                         5.14.3\ntransformers                      4.48.3\ntriton                            3.1.0\ntyper                             0.15.2\ntyping_extensions                 4.12.2\ntzdata                            2024.2\nurllib3                           2.2.3\nuvicorn                           0.31.1\nuvloop                            0.21.0\nvllm                              0.7.3\nvllm-flash-attn                   2.6.1\nwatchfiles                        0.24.0\nwcwidth                           0.2.13\nwebsockets                        13.1\nwheel                             0.44.0\nxformers                          0.0.28.post3\nxgrammar                          0.1.11\nxxhash                            3.5.0\nyarl                              1.15.2\nzipp                              3.20.2\n",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-05T11:35:03+00:00",
    "closed_at": "2025-05-25T00:21:16+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4094/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4094"
  },
  {
    "number": 3266,
    "title": "[Feature] Add Model Hooks for Accessing and Customizing Model Activations",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n## Description\nIt would be beneficial to introduce model hooks that allow users to access and modify model activations. This feature would enable greater flexibility for tasks such as visualization, debugging, and custom processing of intermediate representations.\n\n## Use case\n* Extract intermediate outputs for interpretability analysis, such as [LogitLens-style investigations](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens).\n* Expose internal activations, enabling users to cache activations and implement functions to edit, remove, or replace them dynamically during inference, for example [representation engineering](https://github.com/andyzoujm/representation-engineering).\n\nWhile this may introduce some performance overhead, it would enhance interpretability research and enable efficient model editing.\n\n### Related resources\n\n## model hook resources\n* [Pytorch hook](https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html)\n* [Transformerlens](https://github.com/TransformerLensOrg/TransformerLens)\n## related issues and use case\n* https://github.com/vllm-project/vllm/issues/4084\n* https://github.com/vllm-project/vllm/issues/11397\n* https://github.com/vllm-project/vllm/issues/8278",
    "labels": [
      "inactive",
      "research"
    ],
    "state": "closed",
    "created_at": "2025-02-03T05:44:46+00:00",
    "closed_at": "2025-04-05T00:17:32+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3266/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3266"
  },
  {
    "number": 5734,
    "title": "[Bug] Enabling Speculative Decoding causes DeepSeek R1 generate no output if the input text is long",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen:\n- 1. The Speculative Decoding is enabled, for exmaple:\n```bash\n    --log-requests \\\n    --log-requests-level 2 \\\n    --speculative-algorithm EAGLE \\\n    --speculative-num-steps 3 \\\n    --speculative-eagle-topk 1 \\\n    --speculative-num-draft-tokens 4 \\\n    --speculative-draft /public/home/deepseek/tests/cuda_llm/deepseek_r1/DeepSeek-R1-NextN-20250424 \n```\nand,\n- 2.  A very long input is provided, e.g., 100000 tokens,\n\nthen the output will become emtpy:\n```bash\n...  #(other informations)\nout={'text': '', 'meta_info': {'id': 'b152fac3e8ec43199963f2091ffe9a33', 'finish_reason': {'type': 'stop', 'matched': 1}, 'prompt_tokens': 105296, 'completion_tokens': 1, 'cached_tokens': 331, 'spec_verify_ct': 0, 'e2e_latency': 22.161558628082275}}\n\n```\n\n### Reproduction\n\nThe model is DeepSeek-R1. Before running the sglang server, the following steps are performed:\n1. Modify the `model_max_length` to `131072` in `DeepSeek-R1/tokenizer_config.json`\n2. Generate the model `DeepSeek-R1-NextN-20250424` using the script [export_deepseek_nextn.py\n](https://github.com/sgl-project/sglang/blob/main/scripts/export_deepseek_nextn.py)\n\nA full command to reproduce is:\n- Head node\n```bash\nexport SGLANG_HOST_IP=10.10.10.3\nexport SGLANG_API_KEY=\"testtest\"\nexport SGL_ENABLE_JIT_DEEPGEMM=1\npython3 -m sglang.launch_server \\\n    --model-path /public/home/deepseek/tests/cuda_llm/deepseek_r1/DeepSeek-R1 \\\n    --tp 16 \\\n    --dist-init-addr 10.10.10.3:8001 \\\n    --nnodes 2 \\\n    --node-rank 0 \\\n    --host 10.10.10.3 --port 8011 \\\n    --api-key ${SGLANG_API_KEY} \\\n    --trust-remote-code \\\n    --log-requests \\\n    --log-requests-level 2 \\\n    --context-length $[128*1024] \\\n    --enable-torch-compile \\\n    --chunked-prefill-size=32768 \\\n    --attention-backend flashinfer \\\n    --reasoning-parser deepseek-r1 \\\n    --speculative-algorithm EAGLE \\\n    --speculative-num-steps 3 \\\n    --speculative-eagle-topk 1 \\\n    --speculative-num-draft-tokens 4 \\\n    --speculative-draft /public/home/deepseek/tests/cuda_llm/deepseek_r1/DeepSeek-R1-NextN-20250424 \\\n    &>sglang_dsr1_fp8_head_log_$(date -Iseconds)\n```\n- Worker node\n```bash\nexport SGLANG_HOST_IP=10.10.10.3\nexport SGL_ENABLE_JIT_DEEPGEMM=1\npython3 -m sglang.launch_server \\\n    --model-path /public/home/deepseek/tests/cuda_llm/deepseek_r1/DeepSeek-R1 \\\n    --tp 16 \\\n    --dist-init-addr 10.10.10.3:8001 \\\n    --nnodes 2 \\\n    --node-rank 1 \\\n    --trust-remote-code \\\n    --log-requests \\\n    --log-requests-level 2 \\\n    --context-length $[128*1024] \\\n    --enable-torch-compile \\\n    --chunked-prefill-size=32768 \\\n    --attention-backend flashinfer \\\n    --reasoning-parser deepseek-r1 \\\n    --speculative-algorithm EAGLE \\\n    --speculative-num-steps 3 \\\n    --speculative-eagle-topk 1 \\\n    --speculative-num-draft-tokens 4 \\\n    --speculative-draft /public/home/deepseek/tests/cuda_llm/deepseek_r1/DeepSeek-R1-NextN-20250424 \\\n    &>sglang_dsr1_fp8_worker_log_$(date -Iseconds)\n```\n\nIf I just remove the `--speculative-*` commands, the model will generate outputs even if a very long input is provided:\n- Head node\n```bash\nexport SGLANG_HOST_IP=10.10.10.3\nexport SGLANG_API_KEY=\"testtest\"\nexport SGL_ENABLE_JIT_DEEPGEMM=1\npython3 -m sglang.launch_server \\\n    --model-path /public/home/deepseek/tests/cuda_llm/deepseek_r1/DeepSeek-R1 \\\n    --tp 16 \\\n    --dist-init-addr 10.10.10.3:8001 \\\n    --nnodes 2 \\\n    --node-rank 0 \\\n    --host 10.10.10.3 --port 8011 \\\n    --api-key ${SGLANG_API_KEY} \\\n    --trust-remote-code \\\n    --log-requests \\\n    --log-requests-level 2 \\\n    --context-length $[128*1024] \\\n    --enable-torch-compile \\\n    --chunked-prefill-size=32768 \\\n    --attention-backend flashinfer \\\n    --reasoning-parser deepseek-r1 \\\n    &>sglang_dsr1_fp8_head_log_$(date -Iseconds)\n```\n- Worker node\n```bash\nexport SGLANG_HOST_IP=10.10.10.3\nexport SGL_ENABLE_JIT_DEEPGEMM=1\npython3 -m sglang.launch_server \\\n    --model-path /public/home/deepseek/tests/cuda_llm/deepseek_r1/DeepSeek-R1 \\\n    --tp 16 \\\n    --dist-init-addr 10.10.10.3:8001 \\\n    --nnodes 2 \\\n    --node-rank 1 \\\n    --trust-remote-code \\\n    --log-requests \\\n    --log-requests-level 2 \\\n    --context-length $[128*1024] \\\n    --enable-torch-compile \\\n    --chunked-prefill-size=32768 \\\n    --attention-backend flashinfer \\\n    --reasoning-parser deepseek-r1 \\\n    &>sglang_dsr1_fp8_worker_log_$(date -Iseconds)\n```\n- Log of head node\n```bash\nout={'text': '\u597d\u7684\uff0c\u7528\u6237\u518d\u6b21\u8be2\u95ee...(some generated output text here)', 'meta_info': {'id': '6601225dad9a46b9887fa05dccdc8d9a', 'finish_reason': {'type': 'stop', 'matched': 1}, 'prompt_tokens': 105296, 'completion_tokens': 1815, 'cached_tokens': 1, 'e2e_latency': 82.62103843688965}}\n```\n\nAnother problem is that, when running sglang **without** `--speculative-*` commands, a segmentation fault will occur, as discussed in #2803 . By upgrading the nccl using pip from 2.21 to 2.26, this error will be resolved. However, when using nccl 2.21 and runnning sglang **with** `--speculative-*` commands, this segmentation fault will not appear. I have not tested running sglang **with** `--speculative-*` and nccl 2.26. Not sure if the above output issue is related to NCCL.\n\n### Environment\n\nThe output of `python3 -m sglang.check_env`:\n- Head node\n```bash\nPython: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /public/software/cuda/12.4\nNVCC: Cuda compilation tools, release 12.4, V12.4.99\nCUDA Driver Version: 550.54.14\nPyTorch: 2.6.0+cu124\nsglang: 0.4.5.post3\nsgl_kernel: 0.0.9.post2\nflashinfer: Module Not Found\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.10.0\nnumpy: 2.2.5\naiohttp: 3.11.18\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.3\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.2\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.17\nopenai: 1.75.0\ntiktoken: 0.9.0\nanthropic: 0.50.0\nlitellm: 1.67.1\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     SYS     SYS     SYS     SYS     0-47    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     0-47    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     PIX     SYS     SYS     SYS     0-47    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     0-47    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     48-95   1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     PIX     SYS     48-95   1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     PIX     48-95   1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     48-95   1               N/A\nNIC0    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS\nNIC1    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS\nNIC2    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS\nNIC3    SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS      X      SYS\nNIC4    SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS      X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n\n\nulimit soft: 1000000\n```\n- Worker node:\n```bash\nPython: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /public/software/cuda/12.4\nNVCC: Cuda compilation tools, release 12.4, V12.4.99\nCUDA Driver Version: 550.54.14\nPyTorch: 2.6.0+cu124\nsglang: 0.4.5.post3\nsgl_kernel: 0.0.9.post2\nflashinfer: Module Not Found\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.10.0\nnumpy: 2.2.5\naiohttp: 3.11.18\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.3\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.2\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.17\nopenai: 1.75.0\ntiktoken: 0.9.0\nanthropic: 0.50.0\nlitellm: 1.67.1\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     SYS     SYS     SYS     SYS     0-47    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     0-47    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     PIX     SYS     SYS     SYS     0-47    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     0-47    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     PIX     SYS     SYS     48-95   1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     48-95   1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     PIX     48-95   1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     48-95   1               N/A\nNIC0    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS\nNIC1    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS\nNIC2    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS\nNIC3    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS\nNIC4    SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS      X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n\n\nulimit soft: 1000000\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-25T05:17:05+00:00",
    "closed_at": "2025-04-25T10:00:43+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5734/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5734"
  },
  {
    "number": 1987,
    "title": "[Bug] how to combine with ray.data",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nray.data.Dataset.map_batches(**args) is ok with vllm, i replace vllm.LLm with sglang.Engine, it's wrong, how to make it? Because ray.data is easy to manage data. Looking forward to you can help me\r\n\r\nRunning 0: 0 bundle [00:00, ? bundle/s]2024-11-10 23:01:22,492  ERROR streaming_executor_state.py:456 -- An exception was raised from a task of operator \"MapBatches(LLM)\". Dataset execution will now abort. To ignore this exception and continue, set DataContext.max_errored_blocks.\r\n\u26a0\ufe0f  Dataset execution failed: : 0 bundle [00:00, ? bundle/s]\r\n- ReadJSON->SplitBlocks(48): 1 active, 0 queued, [cpu: 1.0, objects: 20.5KB]: : 12 bundle [00:00, 187.92 bundle/s]\r\n2024-11-10 23:01:22,504(WARNING actor_pool_map_operator.py:265 -- To ensure full parallelization across an actor pool of size 1, the Dataset should consist of at least 1 distinct blocks. Consider increasing the parallelism when creating the Dataset.\r\n- MapBatches(LLM): 1 active, 0 queued, [cpu: 0.0, gpu: 1.0, objects: 256.0MB], 1 actors [locality off]: : 0 bundle [00:00, ? bundle/s]\r\n- Write: 0 active, 0 queued, [cpu: 0.0, objects: 0.0B]: : 0 bundle [00:00, ? bundle/s]\r\n2024-11-10 23:01:22,515tERROR exceptions.py:63 -- Exception occurred in user code, with the abbreviated stack trace below. By default, the Ray Data internal stack trace is omitted from stdout, and only written to the Ray Data log files at /tmp/ray/session_2024-11-10_23-00-50_100958_208716/logs/ray-data. To output the full stack trace to stdout, set `DataContext.log_internal_stack_trace_to_stdout` to True.e/s]\r\nTraceback (most recent call last):\r\n  File \"/data/repo/batch_infer/sglang_infer.py\", line 129, in <module>\r\n    new_ds.write_json(args.output_path)\r\n  File \"/root/anaconda3/envs/torch/lib/python3.10/site-packages/ray/data/dataset.py\", line 2888, in write_json\r\n    self.write_datasink(\r\n  File \"/root/anaconda3/envs/torch/lib/python3.10/site-packages/ray/data/dataset.py\", line 3610, in write_datasink\r\n    self._write_ds = Dataset(plan, logical_plan).materialize()\r\n  File \"/root/anaconda3/envs/torch/lib/python3.10/site-packages/ray/data/dataset.py\", line 4598, in materialize\r\n    copy._plan.execute()\r\n  File \"/root/anaconda3/envs/torch/lib/python3.10/site-packages/ray/data/exceptions.py\", line 87, in handle_trace\r\n    raise e.with_traceback(None)\r\nray.exceptions.RayTaskError(UserCodeException): ray::MapBatches(LLM)() (pid=210888, ip=172.22.197.6, actor_id=e58321fa3f0e04abad4aba2801000000, repr=MapWorker(MapBatches(LLM)))\r\n  File \"/root/anaconda3/envs/torch/lib/python3.10/site-packages/ray/data/_internal/execution/util.py\", line 78, in __call__\r\n    return future.result()\r\n  File \"/root/anaconda3/envs/torch/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\r\n    return self.__get_result()\r\n  File \"/root/anaconda3/envs/torch/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\r\n    raise self._exception\r\n  File \"/root/anaconda3/envs/torch/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/data/repo/batch_infer/sglang_infer.py\", line 89, in __call__\r\n    responses = self.llm.generate(input_texts, self.sampling_params)\r\n  File \"/root/anaconda3/envs/torch/lib/python3.10/site-packages/sglang/srt/server.py\", line 760, in generate\r\n    loop = asyncio.get_event_loop()\r\n  File \"/root/anaconda3/envs/torch/lib/python3.10/asyncio/events.py\", line 656, in get_event_loop\r\n    raise RuntimeError('There is no current event loop in thread %r.'\r\nRuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-0_0'.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nray::MapBatches(LLM)() (pid=210888, ip=172.22.197.6, actor_id=e58321fa3f0e04abad4aba2801000000, repr=MapWorker(MapBatches(LLM)))\r\n  File \"/root/anaconda3/envs/torch/lib/python3.10/site-packages/ray/data/_internal/execution/operators/actor_pool_map_operator.py\", line 364, in submit\r\n    yield from _map_task(\r\n  File \"/root/anaconda3/envs/torch/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_operator.py\", line 451, in _map_task\r\n    for b_out in map_transformer.apply_transform(iter(blocks), ctx):\r\n  File \"/root/anaconda3/envs/torch/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 392, in __call__\r\n    for data in iter:\r\n  File \"/root/anaconda3/envs/torch/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 134, in _udf_timed_iter\r\n    output = next(input)\r\n  File \"/root/anaconda3/envs/torch/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 236, in __call__\r\n    yield from self._batch_fn(input, ctx)\r\n  File \"/root/anaconda3/envs/torch/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 282, in transform_fn\r\n    res = fn(batch)\r\n  File \"/root/anaconda3/envs/torch/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 186, in fn\r\n    _handle_debugger_exception(e)\r\n  File \"/root/anaconda3/envs/torch/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 210, in _handle_debugger_exception\r\n    raise UserCodeException() from e\r\nray.exceptions.UserCodeException\n\n### Reproduction\n\n    new_ds = ds.map_batches(\r\n        LLM,\r\n        # Set the concurrency to the number of LLM instances.\r\n        concurrency=num_instances,\r\n        # Specify the batch size for inference.\r\n        batch_size=args.batch_size,\r\n        **resources_kwarg,\r\n    )\n\n### Environment\n\ntorch 2.4 cuda 12.4",
    "labels": [
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2024-11-10T15:08:00+00:00",
    "closed_at": "2025-01-15T11:29:34+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1987/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1987"
  },
  {
    "number": 3283,
    "title": "[Bug] deepseek v3 2 nodes h100 segmentation fault",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nhello.\nI run on 2 nodes of 8 x h100 using   lmsysorg/sglang:v0.4.2.post1-cu125 image\n\n```\npython3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-R1 --tp 16 --dist-init-addr 172.16.1.68:5000 --nnodes 2 --node-rank 1 --trust-remote-code --quantization fp8 --kv-cache-dtype fp8_e5m2\n```\nI start a benchmark\n```\n python3 -m sglang.bench_serving --backend sglang --dataset-name random --random-range-ratio 1 --num-prompt 2400 --request-rate 8 --random-input 1024 --random-output 1024 --output-file deepseek_v3_8xh200_BF16_on\nline_output.jsonl\n```\nsglang on one node crashes\n```\n[2025-02-03 22:28:08 TP12] Decode out of memory happened. #retracted_reqs: 3, #new_token_ratio: 0.0980 -> 1.0000\nFatal Python error: Segmentation fault\n\nThread 0x00007f4b58a9b700 (most recent call first):\n  File \"/usr/lib/python3.10/threading.py\", line 324 in wait\n  File \"/usr/lib/python3.10/threading.py\", line 607 in wait\n  File \"/usr/local/lib/python3.10/dist-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f3b2dfff700 (most recent call first):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 462 in watchdog_thread\n  File \"/usr/lib/python3.10/threading.py\", line 953 in run\n  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f3b2d7fe700 (most recent call first):\n  File \"/usr/local/lib/python3.10/dist-packages/sgl_kernel/ops/utils.py\", line 19 in _get_cache_buf\n  File \"/usr/local/lib/python3.10/dist-packages/sgl_kernel/ops/__init__.py\", line 286 in bmm_fp8\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 597 in forward_absorb\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 516 in forward\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747 in _call_impl\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736 in _wrapped_call_impl\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 757 in forward\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747 in _call_impl\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736 in _wrapped_call_impl\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 819 in forward\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747 in _call_impl\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736 in _wrapped_call_impl\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 858 in forward\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 742 in forward_decode\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 783 in forward\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 164 in forward_batch_generation\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 140 in forward_thread_func_\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 109 in forward_thread_func\n  File \"/usr/lib/python3.10/threading.py\", line 953 in run\n  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f4ae3fc7700 (most recent call first):\n  File \"/usr/lib/python3.10/threading.py\", line 324 in wait\n  File \"/usr/lib/python3.10/threading.py\", line 607 in wait\n  File \"/usr/local/lib/python3.10/dist-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f4d38484700 (most recent call first):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 47 in _recv_msg\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 153 in _read_thread\n  File \"/usr/lib/python3.10/threading.py\", line 953 in run\n  File \"/usr/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\n  File \"/usr/lib/python3.10/threading.py\", line 973 in _bootstrap\n\nThread 0x00007f50d99f1740 (most recent call first):\n  File \"/usr/lib/python3.10/threading.py\", line 320 in wait\n  File \"/usr/lib/python3.10/threading.py\", line 607 in wait\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 167 in resolve_batch_result\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\"\n```\nIf I run sglang without --quantization fp8 --kv-cache-dtype fp8_e5m2 . it doesn't crash\n\n### Reproduction\n\n  s\n\n### Environment\n\n| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |\n",
    "labels": [
      "help wanted",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-02-04T06:43:27+00:00",
    "closed_at": "2025-02-04T07:40:25+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3283/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3283"
  },
  {
    "number": 4120,
    "title": "[Feature] nvcc fatal   : Unknown option '-generate-dependencies-with-compile'",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nwhen use \u201dpython3 -m sglang.launch_server --model /home/ydkj/lx/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --host 0.0.0.0 --port 8123 --max-total-tokens 8192 --tensor-parallel-size 2 \u201c Startup Command.  An error occurred, as follows:\n\nnvcc fatal   : Unknown option '-generate-dependencies-with-compile'\nninja: build stopped: subcommand failed.\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-06T03:19:09+00:00",
    "closed_at": "2025-05-06T00:18:56+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4120/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4120"
  },
  {
    "number": 7305,
    "title": "[Bug] when dp=2  on two nodes,  cannot get   content from  /metrics   on another node",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nUsing sglang to deploy a large model with tp=16, dp=2 across two servers, so two requests will run on dp0-7 and dp8-15 respectively.\n\nThen, executing curl -X GET http://0.0.0.0:40000/metrics -H \"Authorization: Bearer  xxxx\" on servers A and B:\n\nServer A (master node) has metrics, but Server B does not.\nBoth A and B have enable_metrics in their configurations.\nWhat should I do?\n\n### Reproduction\n\n curl -X GET http://0.0.0.0:40000/metrics -H \"Authorization: Bearer  xxxx\" on Server B ,return {\"detail\":\"Not Found\"}\n\n### Environment\n\nsglang v.0.4.6\nlinux :ubuntu\nmodel:deepseek r1",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-18T07:50:49+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7305/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7305"
  },
  {
    "number": 464,
    "title": "Dependency conflict with LLaVA",
    "body": "# Issue\r\nCannot run a finetuned LLaVA model with sglang==0.1.16, running `CUDA_VISIBLE_DEVICES=0,1,2,3 python3 -m sglang.launch_server --model-path llava-lora-34b-faceshape-ft/ --tokenizer-path liuhaotian/llava-v1.6-34b-tokenizer --port 30000 --tp 4`\r\nthrows: \r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/iverkh/.triton/cache/a95dd9872513f57ade076cce4b51d3f0/_fwd_kernel_stage2.json.tmp.pid_33358_98246'\r\n\r\n# Reproduction\r\nAfter cloning LLaVA from https://github.com/haotian-liu/LLaVA, run `pip install -e . ` as instructed in the README\r\nRun pip install sglang[all]\r\nRun `CUDA_VISIBLE_DEVICES=0,1,2,3 python3 -m sglang.launch_server --model-path <fine tuned llava> --tokenizer-path liuhaotian/llava-v1.6-34b-tokenizer --port 30000 --tp 4`\r\n\r\n# Possible cause\r\nsglang has a dependency of vllm>0.4.2 which requires torch==2.3.0. LLaVA's official repository has a dependency of torch==2.1.2,\r\nsee here: https://github.com/haotian-liu/LLaVA/blob/main/pyproject.toml\r\n\r\nAfter downgrading sglang to 0.14.1 and subsequently vllm to 0.3.3 I managed to run everything smoothly.",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-05-23T07:43:08+00:00",
    "closed_at": "2024-07-26T01:02:23+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/464/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/464"
  },
  {
    "number": 5513,
    "title": "next-N does not work on H20",
    "body": "I tried different next-N paras but inference speed(one batch) on one H20 server was always about 35tokens/s with or without next-N.\n\nsglang script:\npython -m sglang.launch_server --served-model-name=ds-r1-671b --model-path=$models/DeepSeek-R1 \\\n--enable-p2p-check --reasoning-parser=deepseek-r1 --trust-remote-code --host=0.0.0.0 --port=38001 \\\n--mem-fraction-static=0.9 \\\n--tp=8 --max-total-tokens=65000 --max-running-requests=64 \\\n--enable-ep-moe \\\n--disable-radix-cache \\\n--speculative-algorithm EAGLE3 \\\n--speculative-draft-model-path $moddels/DeepSeek-R1-NextN \\\n--speculative-num-steps 3 \\\n--speculative-eagle-topk 1 \\\n--speculative-num-draft-tokens 4\n\ncheck-env:\nPython: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda-12.8\nNVCC: Cuda compilation tools, release 12.8, V12.8.93\nCUDA Driver Version: 570.124.06\nPyTorch: 2.5.1+cu124\nsglang: 0.4.5\nsgl_kernel: 0.0.8\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.51.0\ntorchao: 0.9.0\nnumpy: 2.2.4\naiohttp: 3.11.16\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.1\ninteregular: 0.3.3\nmodelscope: 1.24.1\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.11.2\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.17\nopenai: 1.70.0\ntiktoken: 0.9.0\nanthropic: 0.49.0\nlitellm: 1.65.4.post1\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    0-47,96-143     0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    0-47,96-143     0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    0-47,96-143     0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    0-47,96-143     0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     48-95,144-191   1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     48-95,144-191   1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     48-95,144-191   1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     48-95,144-191   1               N/A\nNIC0    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      PIX\nNIC1    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n\n\nulimit soft: 4096",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-18T01:28:05+00:00",
    "closed_at": "2025-04-18T08:09:01+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5513/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5513"
  },
  {
    "number": 5274,
    "title": "The performance stress test results of EAGLE-3 are not good",
    "body": "I used EvalScope to stress test the inference performance of SGLang. A very positive aspect is that SGLang outperforms vLLM. However, when I tried to enable the EAGLE-3 optimization, the throughput dropped sharply.\n\nMy environment is an NVIDIA A6000, and the stress test command is as follows:\n```\ndocker run --rm --net=host \\\n--mount type=bind,source=/data,target=/data \\\nregistry.bingosoft.net/bingomatrix/modelscope:ubuntu22.04-cuda12.1.0-py310-torch2.4.0-tf2.16.1-1.18.1-vllm-evalscope \\\nevalscope perf \\\n  --parallel 20 \\\n  --model /data/models/Meta-Llama-3.1-8B-Instruct \\\n  --url http://127.0.0.1:30000/v1/chat/completions \\\n  --api openai \\\n  --dataset random \\\n  --min-tokens 128 \\\n  --max-tokens 128 \\\n  --prefix-length 64 \\\n  --min-prompt-length 1024 \\\n  --max-prompt-length 2048 \\\n  --number 100 \\\n  --tokenizer-path Qwen/Qwen2.5-0.5B-Instruct \\\n  --debug\n```\n\nStart SGLang without EAGLE-3:\n```\ndocker run --gpus all \\\n    --shm-size 32g \\\n    -p 30000:30000 \\\n    -v /data/:/data \\\n    --ipc=host \\\n    registry.bingosoft.net/bingomatrix/sglang:v0.4.5-cu125 \\\n    python3 -m sglang.launch_server --model-path /data/models/Meta-Llama-3.1-8B-Instruct --host 0.0.0.0 --port 30000 \\\n    --mem-fraction 0.6 \\\n    --cuda-graph-max-bs 2 --dtype float16\n```\nAnd Test Results is: \n```\nBenchmarking summary:\n+-----------------------------------+----------------------------------------------------------------------+\n| Key                               | Value                                                                |\n+===================================+======================================================================+\n| Time taken for tests (s)          | 58.761                                                               |\n+-----------------------------------+----------------------------------------------------------------------+\n| Number of concurrency             | 20                                                                   |\n+-----------------------------------+----------------------------------------------------------------------+\n| Total requests                    | 100                                                                  |\n+-----------------------------------+----------------------------------------------------------------------+\n| Succeed requests                  | 100                                                                  |\n+-----------------------------------+----------------------------------------------------------------------+\n| Failed requests                   | 0                                                                    |\n+-----------------------------------+----------------------------------------------------------------------+\n| Throughput(average tokens/s)      | 217.833                                                              |\n+-----------------------------------+----------------------------------------------------------------------+\n| Average QPS                       | 1.702                                                                |\n+-----------------------------------+----------------------------------------------------------------------+\n| Average latency (s)               | 11.74                                                                |\n+-----------------------------------+----------------------------------------------------------------------+\n| Average time to first token (s)   | 11.74                                                                |\n+-----------------------------------+----------------------------------------------------------------------+\n| Average time per output token (s) | 0.092                                                                |\n+-----------------------------------+----------------------------------------------------------------------+\n| Average input tokens per request  | 2489.74                                                              |\n+-----------------------------------+----------------------------------------------------------------------+\n| Average output tokens per request | 128.0                                                                |\n+-----------------------------------+----------------------------------------------------------------------+\n| Average package latency (s)       | 11.74                                                                |\n+-----------------------------------+----------------------------------------------------------------------+\n| Average package per request       | 1.0                                                                  |\n+-----------------------------------+----------------------------------------------------------------------+\n| Expected number of requests       | 100                                                                  |\n+-----------------------------------+----------------------------------------------------------------------+\n| Result DB path                    | outputs/20250411_111326/Meta-Llama-3.1-8B-Instruct/benchmark_data.db\n```\nStart SGLang with EAGLE-3:\n```\ndocker run --gpus all \\\n    --shm-size 32g \\\n    -p 30000:30000 \\\n    -v /data/:/data \\\n    --ipc=host \\\n    registry.bingosoft.net/bingomatrix/sglang:v0.4.5-cu125 \\\n    python3 -m sglang.launch_server --model-path /data/models/Meta-Llama-3.1-8B-Instruct --host 0.0.0.0 --port 30000 \\\n    --speculative-algorithm EAGLE3 \\\n    --speculative-draft-model-path /data/models/sglang-EAGLE3-Llama-3.1-Instruct-8B \\\n    --speculative-num-steps 5 \\\n    --speculative-eagle-topk 8 --speculative-num-draft-tokens 32 --mem-fraction 0.6 \\\n    --cuda-graph-max-bs 16 --dtype float16\n```\n\nTest results is:\n```\nBenchmarking summary:\n+-----------------------------------+----------------------------------------------------------------------+\n| Key                               | Value                                                                |\n+===================================+======================================================================+\n| Time taken for tests (s)          | 77.861                                                               |\n+-----------------------------------+----------------------------------------------------------------------+\n| Number of concurrency             | 20                                                                   |\n+-----------------------------------+----------------------------------------------------------------------+\n| Total requests                    | 100                                                                  |\n+-----------------------------------+----------------------------------------------------------------------+\n| Succeed requests                  | 100                                                                  |\n+-----------------------------------+----------------------------------------------------------------------+\n| Failed requests                   | 0                                                                    |\n+-----------------------------------+----------------------------------------------------------------------+\n| Throughput(average tokens/s)      | 164.395                                                              |\n+-----------------------------------+----------------------------------------------------------------------+\n| Average QPS                       | 1.284                                                                |\n+-----------------------------------+----------------------------------------------------------------------+\n| Average latency (s)               | 14.604                                                               |\n+-----------------------------------+----------------------------------------------------------------------+\n| Average time to first token (s)   | 14.604                                                               |\n+-----------------------------------+----------------------------------------------------------------------+\n| Average time per output token (s) | 0.114                                                                |\n+-----------------------------------+----------------------------------------------------------------------+\n| Average input tokens per request  | 2489.74                                                              |\n+-----------------------------------+----------------------------------------------------------------------+\n| Average output tokens per request | 128.0                                                                |\n+-----------------------------------+----------------------------------------------------------------------+\n| Average package latency (s)       | 14.604                                                               |\n+-----------------------------------+----------------------------------------------------------------------+\n| Average package per request       | 1.0                                                                  |\n+-----------------------------------+----------------------------------------------------------------------+\n| Expected number of requests       | 100                                                                  |\n+-----------------------------------+----------------------------------------------------------------------+\n```\n\nSo, When using eagle-3, the performance becomes worse, what is the reason?",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-11T05:29:08+00:00",
    "closed_at": "2025-04-17T09:37:35+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5274/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5274"
  },
  {
    "number": 5606,
    "title": "[Bug] Cannot run official test case for precision evaluation with hf engine",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHi team! I tried to test the precision of sgl engine (and verlengine) with hf engine, but I cannot pass the assertations in `test/srt/test_verl_engine.py`. @ocss884  @zhaochenyang20 \n\n```text\nFile \"/root/sglang/python/sglang/test/runners.py\", line 773, in check_close_model_outputs\n    assert torch.all(abs(hf_logprobs - srt_logprobs) < prefill_tolerance), (\nAssertionError: prefill logprobs are not all close with enable_batch=False tp_rank=1 prefill_tolerance=0.1.hf_logprobs=tensor([[-1.6422e+00, -2.1500e+00, -2.4469e+00, -2.4704e+00, -2.6110e+00],\n        [-1.0812e+00, -1.3937e+00, -2.3234e+00, -3.3546e+00, -3.4874e+00],\n        [-1.4647e+00, -1.8553e+00, -2.9725e+00, -3.1287e+00, -3.2303e+00],\n        [-5.5338e-01, -1.9753e+00, -2.1159e+00, -2.6003e+00, -4.3893e+00],\n        [-1.9782e+00, -2.3297e+00, -2.7672e+00, -2.7907e+00, -3.1032e+00],\n        [-9.7077e-01, -2.6036e+00, -3.0255e+00, -3.2676e+00, -3.9786e+00],\n        [-6.7361e-01, -9.0798e-01, -3.5174e+00, -4.7830e+00, -4.8767e+00],\n        [-1.9363e-01, -3.1389e+00, -3.3967e+00, -3.5999e+00, -3.7952e+00],\n        [-1.2013e-01, -2.8076e+00, -3.3076e+00, -5.1045e+00, -5.8858e+00],\n        [-8.1685e-03, -5.9144e+00, -6.3675e+00, -7.0394e+00, -7.9925e+00],\n        [-4.9435e-03, -6.0206e+00, -6.8643e+00, -7.9737e+00, -8.2237e+00],\n        [-4.9684e-02, -4.4403e+00, -5.2216e+00, -5.7059e+00, -5.7841e+00],\n        [-4.3083e-02, -4.2462e+00, -5.4337e+00, -6.0431e+00, -6.2931e+00],\n        [-1.6653e-01, -1.9947e+00, -4.5415e+00, -6.1040e+00, -6.3228e+00],\n        [-4.1565e-03, -7.5042e+00, -7.5667e+00, -7.5979e+00, -7.7073e+00],\n        [-1.9950e-02, -4.5043e+00, -5.9418e+00, -6.0512e+00, -6.6606e+00],\n        [-3.6149e-03, -7.2692e+00, -7.2849e+00, -7.6599e+00, -8.1442e+00],\n        [-1.2568e-02, -5.4969e+00, -5.9344e+00, -6.4501e+00, -7.2626e+00],\n        [-1.9741e-01, -3.2130e+00, -3.5724e+00, -4.4474e+00, -4.5880e+00],\n        [-1.3570e-01, -3.6513e+00, -4.1201e+00, -4.8232e+00, -4.8232e+00],\n        [-4.6746e-01, -1.0612e+00, -4.2956e+00, -5.0456e+00, -6.5612e+00],\n        [-6.5100e-03, -6.8659e+00, -6.8971e+00, -7.5065e+00, -7.6471e+00],\n        [-2.0434e-02, -4.6454e+00, -5.3642e+00, -6.0361e+00, -7.1142e+00],\n        [-5.3241e-03, -6.5678e+00, -7.3725e+00, -7.6772e+00, -8.1459e+00],\n        [-7.5560e-03, -7.0076e+00, -7.1169e+00, -7.2107e+00, -7.4763e+00],\n        [-2.2328e-01, -3.3327e+00, -3.5670e+00, -4.1920e+00, -4.4420e+00],\n        [-1.2535e-01, -4.1097e+00, -4.2191e+00, -4.7347e+00, -5.1097e+00],\n        [-2.1563e-01, -1.7469e+00, -4.8094e+00, -5.3406e+00, -6.1063e+00],\n        [-3.8025e-03, -7.5351e+00, -7.6757e+00, -7.8163e+00, -7.9882e+00],\n        [-7.1401e-03, -6.0696e+00, -6.6790e+00, -7.2728e+00, -7.6634e+00],\n        [-3.9110e-03, -7.6836e+00, -7.8398e+00, -7.9102e+00, -8.5117e+00],\n        [-7.3707e-03, -7.2730e+00, -7.4292e+00, -7.5386e+00, -7.5699e+00]]), srt_logprobs=tensor([[-2.8893e+00, -2.9909e+00, -3.5925e+00, -4.1276e+00, -4.1901e+00],\n        [-1.0858e+00, -1.3905e+00, -2.3280e+00, -3.3514e+00, -3.4842e+00],\n        [-1.4295e+00, -1.9139e+00, -2.7889e+00, -2.9764e+00, -3.2811e+00],\n        [-8.1996e-01, -1.8356e+00, -1.9450e+00, -2.0153e+00, -3.9996e+00],\n        [-2.1015e+00, -2.3280e+00, -2.9062e+00, -2.9530e+00, -3.2499e+00],\n        [-1.4285e+00, -2.2879e+00, -2.7488e+00, -3.3113e+00, -4.2566e+00],\n        [-7.4278e-01, -9.1465e-01, -3.3522e+00, -4.4303e+00, -4.5709e+00],\n        [-2.3119e-01, -2.9109e+00, -3.1765e+00, -3.5437e+00, -3.7703e+00],\n        [-1.6140e-01, -2.5676e+00, -3.0520e+00, -4.6926e+00, -5.5051e+00],\n        [-1.2824e-02, -5.6847e+00, -5.8878e+00, -6.8097e+00, -7.4816e+00],\n        [-8.4402e-03, -5.5866e+00, -6.5553e+00, -7.3678e+00, -7.7584e+00],\n        [-1.0220e-01, -3.9303e+00, -4.6647e+00, -4.8522e+00, -5.4772e+00],\n        [-1.1883e-01, -3.6032e+00, -4.9001e+00, -5.0095e+00, -5.2282e+00],\n        [-1.9948e-01, -1.8714e+00, -4.1995e+00, -5.6057e+00, -5.8557e+00],\n        [-4.7542e-03, -7.4110e+00, -7.4110e+00, -7.4579e+00, -7.4891e+00],\n        [-2.9118e-02, -4.1854e+00, -5.5760e+00, -5.7010e+00, -6.1541e+00],\n        [-8.5368e-03, -6.3835e+00, -6.4929e+00, -6.9617e+00, -7.2117e+00],\n        [-1.7897e-02, -5.2054e+00, -5.6741e+00, -6.2054e+00, -6.9085e+00],\n        [-4.9014e-01, -2.6776e+00, -2.8964e+00, -3.8495e+00, -4.0995e+00],\n        [-7.9063e-01, -2.6188e+00, -2.7750e+00, -2.9625e+00, -3.6813e+00],\n        [-5.0861e-01, -1.0399e+00, -3.9461e+00, -4.5711e+00, -6.0086e+00],\n        [-8.7319e-03, -6.4775e+00, -6.6025e+00, -7.1415e+00, -7.3212e+00],\n        [-2.9082e-02, -4.3260e+00, -5.0760e+00, -5.6385e+00, -6.6853e+00],\n        [-7.1944e-03, -6.2572e+00, -6.7103e+00, -7.9056e+00, -8.0775e+00],\n        [-1.4399e-02, -6.3425e+00, -6.5300e+00, -6.5613e+00, -6.9675e+00],\n        [-4.4818e-01, -3.0419e+00, -3.0419e+00, -3.6982e+00, -4.0263e+00],\n        [-8.5252e-01, -2.5869e+00, -2.9306e+00, -3.4619e+00, -3.5400e+00],\n        [-2.6651e-01, -1.6103e+00, -4.3290e+00, -4.8134e+00, -5.5946e+00],\n        [-5.6615e-03, -7.1150e+00, -7.1932e+00, -7.4432e+00, -7.7244e+00],\n        [-1.2514e-02, -5.4813e+00, -6.2156e+00, -6.8094e+00, -7.1219e+00],\n        [-5.1423e-03, -7.3645e+00, -7.7317e+00, -7.8489e+00, -8.0520e+00],\n        [-1.4950e-02, -6.6243e+00, -6.8118e+00, -6.8275e+00, -6.8743e+00]])\n```\n\n### Reproduction\n\nI ran the script locally with `Qwen-2.5-7B-Instruct` and commit `f29a718f63d70144902423a5ff9c0ca64a9e2ac4`. `tp=1` and `tp=2` will both reproduce the error.\n\n### Environment\n\nPython: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.3, V12.3.107\nCUDA Driver Version: 535.154.05\nPyTorch: 2.5.1+cu124\nsglang: 0.4.4.post3\nsgl_kernel: 0.0.6\nflashinfer: Module Not Found\ntriton: 3.1.0\ntransformers: 4.50.0\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.9.1\nfastapi: 0.115.5\nhf_transfer: 0.1.9\nhuggingface_hub: 0.26.2\ninteregular: 0.3.3\nmodelscope: 1.23.1\norjson: 3.10.11\noutlines: 0.1.11\npackaging: 23.2\npsutil: 5.9.4\npydantic: 2.10.5\nmultipart: Module Not Found\nzmq: Module Not Found\nuvicorn: 0.22.0\nuvloop: 0.21.0\nvllm: 0.7.2\nxgrammar: 0.1.17\nopenai: 1.59.6\ntiktoken: 0.7.0\nanthropic: 0.49.0\nlitellm: 1.57.4\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4      NIC5    NIC6    NIC7    NIC8    NIC9    NIC10   NIC11   NIC12   NIC13   NIC14   NIC15   NIC16   CPU Affinity      NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     PIX     SYS     SYS     SYS       SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-31,64-95        0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     PIX     PIX     SYS       SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-31,64-95        0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX       PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-31,64-95        0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS       SYS     SYS     PIX     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-31,64-95        0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS     SYS     PIX     PIX     SYS     SYS     SYS     SYS     SYS     SYS     32-63,96-127      1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX     SYS     SYS     SYS     SYS     32-63,96-127      1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX     SYS     SYS     32-63,96-127      1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX     32-63,96-127      1               N/A\nNIC0    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     SYS     SYS     SYS       SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC1    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      SYS     SYS     SYS       SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC2    SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     SYS       SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC3    SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      SYS       SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC4    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X        PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC5    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX        X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC6    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC7    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS      X      PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC8    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     PIX      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC9    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS     SYS      X      PIX     SYS     SYS     SYS     SYS     SYS     SYS\nNIC10   SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS     SYS     PIX      X      SYS     SYS     SYS     SYS     SYS     SYS\nNIC11   SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     SYS     SYS     SYS     SYS\nNIC12   SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      SYS     SYS     SYS     SYS\nNIC13   SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     SYS     SYS\nNIC14   SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      SYS     SYS\nNIC15   SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX\nNIC16   SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS       SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n  NIC10: mlx5_10\n  NIC11: mlx5_11\n  NIC12: mlx5_12\n  NIC13: mlx5_13\n  NIC14: mlx5_14\n  NIC15: mlx5_15\n  NIC16: mlx5_16\n\n\nulimit soft: 1048576",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-04-21T17:08:06+00:00",
    "closed_at": "2025-06-21T00:19:42+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5606/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5606"
  },
  {
    "number": 4897,
    "title": "[Bug] gemma-3-27b-it-bnb-4bit crash ",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n\n1\u3001When loading a 27B 4-bit quantized model, why does it exhaust the 24GB of gpu memory?\n2\u3001Why did the program crash? Is it because the gpu memory was exhausted?\n<img width=\"1400\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/48732e57-a966-4f92-951b-3fd637da3f1b\" />\n[2025-03-29 18:56:25 TP0] Scheduler hit an exception: Traceback (most recent call last):                                \n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1999, in run_scheduler_process             \n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)                                             \n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 249, in __init__                           \n    self.tp_worker = TpWorkerClass(                                                                                     \n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 74, in __init__                            \n    self.model_runner = ModelRunner(                                                                                    \n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 169, in __init__                  \n    self.initialize(min_per_gpu_memory)                                                                                 \n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 179, in initialize                \n    self.load_model()                                                                                                   \n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 392, in load_model                \n    self.model = get_model(                                                                                             \n  File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/__init__.py\", line 22, in get_model                        \n    return loader.load_model(                                                                                           \n  File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/loader.py\", line 1122, in load_model                       \n    self._load_weights(model_config, model)                                                                             \n  File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/loader.py\", line 1053, in _load_weights                    \n    model.load_weights(qweight_iterator)                                                                                \n  File \"/sgl-workspace/sglang/python/sglang/srt/models/gemma3_mm.py\", line 436, in load_weights                         \n    causal_loaded_params = Gemma3ForCausalLM.load_weights(                                                              \n  File \"/sgl-workspace/sglang/python/sglang/srt/models/gemma3_causal.py\", line 666, in load_weights                     \n    weight_loader(param, loaded_weight, shard_id)                                                                       \n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/linear.py\", line 642, in weight_loader                           \n    assert param_data.shape == loaded_weight.shape                                                                      \nAssertionError                                                                                                          \n                                                                                                                        \nLoading safetensors checkpoint shards:   0% Completed | 0/4 [01:33<?, ?it/s]                                            \n                                                                                                                        \n[2025-03-29 18:56:25] Received sigquit from a child process. It usually means the child failed.                         \nKilled                                                                               \n\n### Reproduction\n\n python3 -m sglang.launch_server --model-path /llm/model/google/unsloth_gemma-3-27b-it-unsloth-bnb-4bit/ --host 0.0.0.0 --port 30000 --trust-remote-code --load-format bitsandbytes --context-length 4096\nINFO 03-29 18:46:53 __init__.py:190] Automatically detected platform cuda.\n[2025-03-29 18:46:55] server_args=ServerArgs(model_path='/llm/model/google/unsloth_gemma-3-27b-it-unsloth-bnb-4bit/', tokenizer_path='/llm/model/google/unsloth_gemma-3-27b-it-unsloth-bnb-4bit/', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='bitsandbytes', trust_remote_code=True, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=4096, device='cuda', served_model_name='/llm/model/google/unsloth_gemma-3-27b-it-unsloth-bnb-4bit/', chat_template=None, completion_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=30000, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, stream_interval=1, stream_output=False, random_seed=585300946, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=5, speculative_eagle_topk=4, speculative_num_draft_tokens=8, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_deepep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=8, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, enable_flashinfer_mla=False, enable_flashmla=False, flashinfer_mla_disable_ragged=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_bootstrap_port=8998)\n[2025-03-29 18:46:55] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n[2025-03-29 18:46:55] The following error message 'operation scheduled before its operands' can be ignored.\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nINFO 03-29 18:46:57 __init__.py:190] Automatically detected platform cuda.\nINFO 03-29 18:46:57 __init__.py:190] Automatically detected platform cuda.\n[2025-03-29 18:46:58 TP0] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n[2025-03-29 18:47:00 TP0] Overlap scheduler is disabled for multimodal models.\n[2025-03-29 18:47:00 TP0] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n[2025-03-29 18:47:00 TP0] Automatically reduce --mem-fraction-static to 0.836 because this is a multimodal model.\n[2025-03-29 18:47:00 TP0] Init torch distributed begin.\n[2025-03-29 18:47:00 TP0] Init torch distributed ends. mem usage=0.00 GB\n[2025-03-29 18:47:00 TP0] Load weight begin. avail mem=22.46 GB\n[2025-03-29 18:47:01 TP0] The following error message 'operation scheduled before its operands' can be ignored.\n[2025-03-29 18:47:01 TP0] Loading weights with BitsAndBytes quantization.  May take a while ...\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [01:52<05:36, 112.31s/it]\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [04:04<04:08, 124.16s/it]\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [06:23<02:10, 130.91s/it]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [07:50<00:00, 113.37s/it]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [07:50<00:00, 117.55s/it]\n\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\n### Environment\n\nGPU:4090\nruntime env: lmsysorg/sglang:dev",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-29T19:01:15+00:00",
    "closed_at": "2025-06-13T00:19:52+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4897/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4897"
  }
]
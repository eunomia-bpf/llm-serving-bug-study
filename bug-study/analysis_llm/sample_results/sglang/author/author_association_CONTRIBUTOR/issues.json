[
  {
    "number": 523,
    "title": "The `choices` normalised logprobs calculation returns poor results due to bias for longer-token options",
    "body": "## Problem\r\nI've noticed that the `gen(choices=[...])` functionality sometimes performs poorly, even for simple tasks. This is due to a flawed normalised logprobs calculation. The calculation biases options that comprise more tokens, where the latter tokens are highly predictable given the prior tokens.\r\n\r\n## Reproducible Example\r\nThis is most easily seen in choices with token overlap, so I've constructed a contrived example that illustrates this. The outputs are generated with [llama 3 8B instruct](https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF), which should breeze through this task under normal circumstances.\r\n```python\r\nimport sglang as sgl\r\nimport textwrap\r\n\r\n# Define answer choices with overlapping substrings and tokenised forms\r\n# assumes llama 3 8B tokeniser\r\nchoices_and_tokenised_forms = [\r\n    (\"organ\", [\"organ\"]),\r\n    (\"organism\", [\"organ\", \"ism\"]),\r\n    (\"organisation\", [\"organisation\"]),\r\n    (\"organelle\", [\"org\", \"ane\", \"lle\"]),\r\n    (\"organometallic\", [\"organ\", \"omet\", \"al\", \"lic\"]),\r\n]\r\nchoices = [c for c, _ in choices_and_tokenised_forms]\r\n\r\n\r\n# Define the categorisation question\r\ntemplate = \"What category does '{input}' belong to? {choices}\"\r\n\r\n# Generate the (optional) system prompt with few-shot examples\r\nsys_prompt = \"\"\r\nfor example in [\r\n    (\"ribosome\", \"organelle\"),\r\n    (\"liver\", \"organ\"),\r\n    (\"Google\", \"organisation\"),\r\n    (\"ferrocene\", \"organometallic\"),\r\n    (\"human\", \"organism\"),\r\n]:\r\n    sys_prompt += \"user:\" + template.format(input=example[0], choices=choices)\r\n    sys_prompt += f\"\\nassisant:{example[1]}\\n\\n\"\r\n\r\n\r\n@sgl.function\r\ndef run(s, input: str, show_few_shot_examples: bool = False):\r\n    if show_few_shot_examples:\r\n        s += sgl.system(f\"You categorise things.\\n\\n ##Examples\\n{sys_prompt}\")\r\n    s += sgl.user(template.format(input=input, choices=choices, temperature=0))\r\n    s += sgl.assistant(sgl.gen(\"answer\", choices=choices))\r\n\r\n\r\ndef format_results(state, input):\r\n    answer = f\"  '{input}' categorised as: '{state['answer']}'\"\r\n    meta = state.get_meta_info(\"answer\")\r\n    out = f\"{answer:<50}    {'normalised'}    {'prefill token logprobs'}\"\r\n    for i in range(len(meta['normalized_prompt_logprobs'])):\r\n        option = f\"{choices_and_tokenised_forms[i][0]} ({choices_and_tokenised_forms[i][1]})\"\r\n        npl = meta['normalized_prompt_logprobs'][i]\r\n        ptl = [f\"{p[0]:.4f}\" for p in meta['prefill_token_logprobs'][i]]\r\n        out += f\"\\n{option:<50} -> {npl:<10.4f} -> {ptl}\"\r\n    return out\r\n\r\n\r\nsgl.set_default_backend(sgl.RuntimeEndpoint(\"http://localhost:30000\"))\r\n\r\nfor include_examples in [False, True]:\r\n    print(f\"Show few-shot examples in context = {include_examples}\\n\")\r\n    for input in [\"heart\", \"nucleus\", \"Microsoft\", \"mouse\", \"trimethylboron\"]:\r\n        state = run(input, show_few_shot_examples=include_examples)\r\n        print(textwrap.indent(format_results(state, input), \"    \"))\r\n        print()\r\n    print(\"-\" * 120)\r\n```\r\nOutputs:\r\n```\r\nShow few-shot examples in context = False\r\n\r\n      'heart' categorised as: 'organelle'                 normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -1.6190    -> ['-0.1265', '-3.1116']\r\n    organism (['organ', 'ism'])                        -> -1.7443    -> ['-0.1265', '-3.1116', '-1.9949']\r\n    organisation (['organisation'])                    -> -3.8885    -> ['-0.1265', '-7.6506']\r\n    organelle (['org', 'ane', 'lle'])                  -> -1.3777    -> ['-0.1265', '-5.3772', '-0.0048', '-0.0023']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -1.3915    -> ['-0.1265', '-3.1116', '-3.7136', '-0.0034', '-0.0023']\r\n\r\n      'nucleus' categorised as: 'organometallic'          normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -1.8324    -> ['-0.2145', '-3.4502']\r\n    organism (['organ', 'ism'])                        -> -1.8675    -> ['-0.2145', '-3.4502', '-1.9378']\r\n    organisation (['organisation'])                    -> -3.1800    -> ['-0.2145', '-6.1456']\r\n    organelle (['org', 'ane', 'lle'])                  -> -1.1103    -> ['-0.2145', '-4.2237', '-0.0013', '-0.0017']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -1.0997    -> ['-0.2145', '-3.4502', '-1.8284', '-0.0029', '-0.0022']\r\n\r\n      'Microsoft' categorised as: 'organometallic'        normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -1.5901    -> ['-0.1446', '-3.0355']\r\n    organism (['organ', 'ism'])                        -> -1.6397    -> ['-0.1446', '-3.0355', '-1.7391']\r\n    organisation (['organisation'])                    -> -2.9416    -> ['-0.1446', '-5.7387']\r\n    organelle (['org', 'ane', 'lle'])                  -> -1.4376    -> ['-0.1446', '-5.5746', '-0.0283', '-0.0029']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -1.1792    -> ['-0.1446', '-3.0355', '-2.7079', '-0.0052', '-0.0028']\r\n\r\n      'mouse' categorised as: 'organelle'                 normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -1.7110    -> ['-0.1392', '-3.2829']\r\n    organism (['organ', 'ism'])                        -> -1.5566    -> ['-0.1392', '-3.2829', '-1.2477']\r\n    organisation (['organisation'])                    -> -3.9181    -> ['-0.1392', '-7.6969']\r\n    organelle (['org', 'ane', 'lle'])                  -> -1.3491    -> ['-0.1392', '-5.2516', '-0.0041', '-0.0015']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -1.4992    -> ['-0.1392', '-3.2829', '-4.0680', '-0.0033', '-0.0028']\r\n\r\n      'trimethylboron' categorised as: 'organometallic'    normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -1.4093    -> ['-0.1379', '-2.6806']\r\n    organism (['organ', 'ism'])                        -> -2.7661    -> ['-0.1379', '-2.6806', '-5.4796']\r\n    organisation (['organisation'])                    -> -3.9659    -> ['-0.1379', '-7.7939']\r\n    organelle (['org', 'ane', 'lle'])                  -> -1.3317    -> ['-0.1379', '-5.1338', '-0.0527', '-0.0023']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -0.5933    -> ['-0.1379', '-2.6806', '-0.1436', '-0.0034', '-0.0008']\r\n\r\n------------------------------------------------------------------------------------------------------------------------\r\nShow few-shot examples in context = True\r\n\r\n      'heart' categorised as: 'organ'                     normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -0.2509    -> ['-0.0799', '-0.4219']\r\n    organism (['organ', 'ism'])                        -> -2.0750    -> ['-0.0799', '-0.4219', '-5.7232']\r\n    organisation (['organisation'])                    -> -3.7431    -> ['-0.0799', '-7.4063']\r\n    organelle (['org', 'ane', 'lle'])                  -> -0.9032    -> ['-0.0799', '-3.5000', '-0.0298', '-0.0031']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -1.7599    -> ['-0.0799', '-0.4219', '-8.2857', '-0.0087', '-0.0034']\r\n\r\n      'nucleus' categorised as: 'organelle'               normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -1.7653    -> ['-0.1489', '-3.3817']\r\n    organism (['organ', 'ism'])                        -> -1.8995    -> ['-0.1489', '-3.3817', '-2.1678']\r\n    organisation (['organisation'])                    -> -3.7379    -> ['-0.1489', '-7.3270']\r\n    organelle (['org', 'ane', 'lle'])                  -> -0.0921    -> ['-0.1489', '-0.2176', '-0.0006', '-0.0011']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -1.9658    -> ['-0.1489', '-3.3817', '-6.2928', '-0.0040', '-0.0017']\r\n\r\n      'Microsoft' categorised as: 'organisation'          normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -0.8883    -> ['-0.1198', '-1.6569']\r\n    organism (['organ', 'ism'])                        -> -1.1325    -> ['-0.1198', '-1.6569', '-1.6208']\r\n    organisation (['organisation'])                    -> -0.6383    -> ['-0.1198', '-1.1569']\r\n    organelle (['org', 'ane', 'lle'])                  -> -1.2105    -> ['-0.1198', '-4.5866', '-0.1336', '-0.0021']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -0.7088    -> ['-0.1198', '-1.6569', '-1.7615', '-0.0043', '-0.0017']\r\n\r\n      'mouse' categorised as: 'organism'                  normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -0.1719    -> ['-0.1273', '-0.2166']\r\n    organism (['organ', 'ism'])                        -> -0.1188    -> ['-0.1273', '-0.2166', '-0.0127']\r\n    organisation (['organisation'])                    -> -2.9610    -> ['-0.1273', '-5.7947']\r\n    organelle (['org', 'ane', 'lle'])                  -> -1.0844    -> ['-0.1273', '-3.9744', '-0.2330', '-0.0030']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -2.2812    -> ['-0.1273', '-0.2166', '-11.0517', '-0.0086', '-0.0020']\r\n\r\n      'trimethylboron' categorised as: 'organometallic'    normalised    prefill token logprobs\r\n    organ (['organ'])                                  -> -0.3231    -> ['-0.0992', '-0.5471']\r\n    organism (['organ', 'ism'])                        -> -3.2023    -> ['-0.0992', '-0.5471', '-8.9607']\r\n    organisation (['organisation'])                    -> -3.1551    -> ['-0.0992', '-6.2111']\r\n    organelle (['org', 'ane', 'lle'])                  -> -0.7889    -> ['-0.0992', '-2.9299', '-0.1246', '-0.0018']\r\n    organometallic (['organ', 'omet', 'al', 'lic'])    -> -0.1314    -> ['-0.0992', '-0.5471', '-0.0076', '-0.0025', '-0.0007']\r\n\r\n------------------------------------------------------------------------------------------------------------------------\r\n```\r\nThe second set of results yields the expected categorisations.\r\n\r\n## Explanation\r\nWe see that only 1/5 answers are correct in the first set of results. Not coincidentally, the only correctly answered question (`'trimethylboron' categorised as: 'organometallic'`) is the one where the correct answer has the most tokens.\r\n\r\nThe first prefill token, common across all options, is `\":\"`. I'm not actually sure why this is present \u2014\u00a0something to do with token healing? Is this coming from the `assistant:` role prefix? Regardless, it's not important as it's consistent across all options, and it's not responsible for the poor performance (although does skew the logprobs calculations in unpredictable ways).\r\n\r\nInspecting the prefill token logprobs for the \"organometallic\" responses is instructive. Even if the `[\"organ\", \"omet\"]` tokens are relatively disfavoured, the `[\"al\", \"lic\"]` tokens are essentially guaranteed once you have the `\"organomet-\"` substring. The normalised logprobs calculation is a simple average of the prefill token logprobs, which means the `[\"al\", \"lic\"]` tokens massively inflate the score, even if \"organometallic\" is obviously wrong given the prior context.\r\n\r\nThe second set of results \u2014 which provides in-context few-shot examples \u2014 does rectify this with 5/5 correct answers. It seems that showing the model expected outputs leads to tokens beyond `\"organ\"`, such as `\"omet\"` , being sufficiently penalised to avoid the problem. It is surprising that the model requires this level of priming for such a simple task, however (even without the few-shot examples, the model is told the permitted options in the user prompt).\r\n\r\n## Other Observations\r\n- Prefixing the the assistant response with `\"Answer: \"` doesn't help, but does result in prefill tokens that only correspond to the choices and nothing else (i.e. no `\":\"` prior token, or similar). Why? The inconsistent presence/absence of prior tokens skews the scores and can lead to erratic selection behaviour when small tweaks are made to the prompt prefixes.\r\n- I tried running this example using regex instead (i.e. `gen(regex=\"(\" + \"|\".join(choices) + \")\")`), thinking this would resolve the issue with simple greedy token selection. But this also performs poorly (and extremely unpredictably, without `temperature=0`).\r\n- I've also explored avoiding overlapping options by wrapping each option in double quotes, but this doesn't solve the problem.\r\n\r\n## Suggestions\r\n- I think this is a severe enough flaw in the normalised logprobs calculation to be considered a bug. The outputs I've observed in several real-world settings are also unreasonably poor for simple tasks and capable models. I think evaluating all the options in their entirety is a good approach in theory, but a more sophisticated normalised logprobs calculation is required to adjust for bias towards options with more tokens.\r\n- Offering an alternative, greedy token selection `choices` decoding option could help. That said, I'm not sure why I still get poor outputs when I attempt to simulate this via `gen(regex=...)`. ",
    "labels": [],
    "state": "closed",
    "created_at": "2024-06-10T12:56:23+00:00",
    "closed_at": "2024-08-05T10:27:50+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/523/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/523"
  },
  {
    "number": 6913,
    "title": "[Bug] DeepError assert self.num_experts % self.tp_size == 0",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n![Image](https://github.com/user-attachments/assets/28e0ae9a-4116-4533-a9e5-7d8bfad97eb4)\n\nthis refactor will make deepep assert error, @ch-wan \n\n### Reproduction\n\npython3 -m sglang.launch_server --model-path=/models/DeepSeek-R1-BF16 --tp-size 32 --attention-backend flashinfer --trust-remote-code --nnodes 4 --node-rank 0 --host 0.0.0.0 --port 30000 --disaggregation-mode decode --dist-init-addr mtp-decode-0.mtp-decode.inference-system.svc.cluster.local:20000 --disaggregation-ib-device mlx5_1 --enable-deepep-moe --deepep-mode low_latency  --mem-fraction-static 0.80 --enable-metrics --speculative-algorithm EAGLE --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4 --chunked-prefill-size -1  --served-model-name deepseek-r1 --disable-cuda-graph --max-running-requests 512  --log-level debug --enable-dp-attention --dp-size 16\n\n### Environment\n\n4 *8  A 800",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-06T06:13:02+00:00",
    "closed_at": "2025-06-06T06:33:11+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6913/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6913"
  },
  {
    "number": 1159,
    "title": "[Bug] head_dim 96 not supported",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [X] 2. The bug has not been fixed in the latest version.\r\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [X] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nUnable to load [vonjack/Phi-3-mini-4k-instruct-LLaMAfied](https://huggingface.co/vonjack/Phi-3-mini-4k-instruct-LLaMAfied)\r\n\r\nStacktrace:\r\n```\r\nserver_args=ServerArgs(model_path='vonjack/Phi-3-mini-4k-instruct-LLaMAfied', tokenizer_path='vonjack/Phi-3-mini-4k-instruct-LLaMAfied', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', dtype='auto', trust_remote_code=False, context_length=None, quantization=None, served_model_name='vonjack/Phi-3-mini-4k-instruct-LLaMAfied', chat_template=None, host='127.0.0.1', port=30000, additional_ports=[30001, 30002, 30003, 30004], mem_fraction_static=0.88, max_running_requests=None, max_num_reqs=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, tp_size=1, stream_interval=1, random_seed=386855210, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, api_key=None, file_storage_pth='SGLang_storage', dp_size=1, load_balance_method='round_robin', disable_flashinfer=True, disable_flashinfer_sampling=False, disable_radix_cache=False, disable_regex_jump_forward=False, disable_cuda_graph=False, disable_disk_cache=False, enable_torch_compile=False, enable_p2p_check=False, enable_mla=False, attention_reduce_in_fp32=False, efficient_weight_load=False, nccl_init_addr=None, nnodes=1, node_rank=None)\r\n[gpu=0] Init nccl begin.\r\n[gpu=0] Load weight begin. avail mem=94.87 GB\r\nINFO 08-20 06:13:41 weight_utils.py:225] Using model weights format ['*.safetensors']\r\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:30<00:00, 30.86s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:30<00:00, 30.86s/it]\r\n\r\n[gpu=0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=87.66 GB\r\n[gpu=0] Memory pool end. avail mem=11.20 GB\r\n[gpu=0] max_total_num_tokens=208284, max_prefill_tokens=16384, max_running_requests=5119, context_len=4096\r\nINFO:     Started server process [441948]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)\r\nINFO:     127.0.0.1:43726 - \"GET /get_model_info HTTP/1.1\" 200 OK\r\n[gpu=0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, cache hit rate: 0.00%, #running-req: 0, #queue-req: 0\r\nException in ModelTpServer:\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 218, in exposed_step\r\n    self.forward_step()\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 234, in forward_step\r\n    self.forward_prefill_batch(new_batch)\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 446, in forward_prefill_batch\r\n    output = self.model_runner.forward(batch, ForwardMode.EXTEND)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 430, in forward\r\n    return self.forward_extend(batch)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 404, in forward_extend\r\n    return self.model.forward(\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/sglang/srt/models/llama2.py\", line 314, in forward\r\n    hidden_states = self.model(input_ids, positions, input_metadata, input_embeds)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/sglang/srt/models/llama2.py\", line 281, in forward\r\n    hidden_states, residual = layer(\r\n                              ^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/sglang/srt/models/llama2.py\", line 231, in forward\r\n    hidden_states = self.self_attn(\r\n                    ^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/sglang/srt/models/llama2.py\", line 168, in forward\r\n    attn_output = self.attn(q, k, v, input_metadata)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/sglang/srt/layers/radix_attention.py\", line 201, in forward\r\n    return self.extend_forward(q, k, v, input_metadata)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/sglang/srt/layers/radix_attention.py\", line 73, in extend_forward_triton\r\n    extend_attention_fwd(\r\n  File \"/root/miniconda3/envs/base2/lib/python3.11/site-packages/sglang/srt/layers/extend_attention.py\", line 267, in extend_attention_fwd\r\n    assert Lq in {16, 32, 64, 128, 256, 576}\r\nAssertionError\r\n\r\n```\r\n\r\n### Reproduction\r\n\r\n```\r\nenv CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=10 python -m sglang.launch_server --model-path vonjack/Phi-3-mini-4k-instruct-LLaMAfied --port 30000 --disable-flashinfer\r\n```\r\n\r\n### Environment\r\n\r\nName: vllm\r\nVersion: 0.5.4\r\n\r\nName: sglang\r\nVersion: 0.2.13\r\n\r\nName: torch\r\nVersion: 2.4.0\r\n\r\nName: transformers\r\nVersion: 4.44.0\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-20T06:18:03+00:00",
    "closed_at": "2024-09-11T16:48:11+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1159/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1159"
  },
  {
    "number": 3429,
    "title": "[Feature] support /v1/completions suffix parameter for completion",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nparameter suffix is not supported in sglang's openapi  v1/completions yet. but it's necessary for code completion.\ncan I support this?\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive",
      "feature"
    ],
    "state": "closed",
    "created_at": "2025-02-09T14:30:57+00:00",
    "closed_at": "2025-04-14T00:19:34+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3429/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3429"
  },
  {
    "number": 4536,
    "title": "[Bug] bench_speculative.py got error",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nrun `bench_speculative.py` scripts \n```\npython3 bench_speculative.py --model-path DeepSeek-R1 --speculative-draft-model-path deepseek-r1-nextn --tp-size 8 --trust-remote-code --batch-size 16 --steps 2 --topk 1  --num_draft_tokens 2 4 8 --context-len 2048 --mem-fraction-static 0.9 --enable-flashinfer-mla\n```\n\ngot error:\n\n```\n[2025-03-18 03:38:23 TP1] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/kesgl-workspace/latest/sglang-serving/python/sglang/srt/managers/scheduler.py\", line 1819, in run_scheduler_process\n    scheduler.event_loop_normal()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/kesgl-workspace/latest/sglang-serving/python/sglang/srt/managers/scheduler.py\", line 502, in event_loop_normal\n    result = self.run_batch(batch)\n  File \"/kesgl-workspace/latest/sglang-serving/python/sglang/srt/managers/scheduler.py\", line 1225, in run_batch\n    ) = self.draft_worker.forward_batch_speculative_generation(batch)\n  File \"/kesgl-workspace/latest/sglang-serving/python/sglang/srt/speculative/eagle_worker.py\", line 216, in forward_batch_speculative_generation\n    spec_info, to_free_cache_loc = self.draft(batch)\n  File \"/kesgl-workspace/latest/sglang-serving/python/sglang/srt/speculative/eagle_worker.py\", line 313, in draft\n    score_list, token_list, parents_list = self.cuda_graph_runner.replay(\n  File \"/kesgl-workspace/latest/sglang-serving/python/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py\", line 213, in replay\n    self.model_runner.draft_attn_backend.init_forward_metadata_replay_cuda_graph(\n  File \"/kesgl-workspace/latest/sglang-serving/python/sglang/srt/layers/attention/flashinfer_mla_backend.py\", line 803, in init_forward_metadata_replay_cuda_graph\n    self.common_template(forward_batch, self.cuda_graph_kv_indices, call_fn)\n  File \"/kesgl-workspace/latest/sglang-serving/python/sglang/srt/layers/attention/flashinfer_mla_backend.py\", line 737, in common_template\n    call_fn(i, forward_batch)\n  File \"/kesgl-workspace/latest/sglang-serving/python/sglang/srt/layers/attention/flashinfer_mla_backend.py\", line 792, in call_fn\n    self.attn_backends[i].init_forward_metadata_replay_cuda_graph(\n  File \"/kesgl-workspace/latest/sglang-serving/python/sglang/srt/layers/attention/flashinfer_mla_backend.py\", line 293, in init_forward_metadata_replay_cuda_graph\n    self.cuda_graph_kv_indptr_cpu[1 : bs + 1] = torch.cumsum(\nRuntimeError: The expanded size of the tensor (15) must match the existing size (14) at non-singleton dimension 0.  Target sizes: [15].  Tensor sizes: [14]\n```\nIt may be related cuda graph replay:\n[bs!=raw_bs](https://github.com/sgl-project/sglang/blob/d373a48c9875f1bb43fde05215a42179b453f81c/python/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py#L207)\n\n### Reproduction\n\n```\npython3 bench_speculative.py --model-path DeepSeek-R1 --speculative-draft-model-path deepseek-r1-nextn --tp-size 8 --trust-remote-code --batch-size 16 --steps 2 --topk 1  --num_draft_tokens 2 4 8 --context-len 2048 --mem-fraction-static 0.9 --enable-flashinfer-mla\n```\n\n### Environment\n\n```\n/opt/conda/lib/python3.10/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\n[2025-03-18 04:27:52] INFO _client.py:1025: HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"\nPython: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.161.07\nPyTorch: 2.6.0+cu124\nsglang: 0.4.4.post1\nsgl_kernel: 0.0.5.post2\nflashinfer: 0.2.3+cu124torch2.5\ntriton: 3.2.0\ntransformers: 4.48.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.13\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.3\ninteregular: 0.3.3\nmodelscope: 1.23.2\norjson: 3.10.15\npackaging: 23.1\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.66.2\ntiktoken: 0.9.0\nanthropic: 0.49.0\ndecord: 0.6.0\nNVIDIA Topology:\n\tGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tNIC4\tNIC5\tNIC6\tNIC7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tPIX\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t0-95,192-287\t0\t\tN/A\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNODE\tPIX\tPHB\tNODE\tSYS\tSYS\tSYS\tSYS\t0-95,192-287\t0\t\tN/A\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNODE\tPHB\tPIX\tNODE\tSYS\tSYS\tSYS\tSYS\t0-95,192-287\t0\t\tN/A\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNODE\tNODE\tNODE\tPIX\tSYS\tSYS\tSYS\tSYS\t0-95,192-287\t0\t\tN/A\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tPIX\tNODE\tNODE\tNODE\t96-191,288-383\t1\t\tN/A\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tNODE\tPIX\tNODE\tNODE\t96-191,288-383\t1\t\tN/A\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPIX\tPHB\t96-191,288-383\t1\t\tN/A\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPHB\tPIX\t96-191,288-383\t1\t\tN/A\nNIC0\tPIX\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t X \tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\nNIC1\tNODE\tPIX\tPHB\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\t X \tPHB\tNODE\tSYS\tSYS\tSYS\tSYS\nNIC2\tNODE\tPHB\tPIX\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\tPHB\t X \tNODE\tSYS\tSYS\tSYS\tSYS\nNIC3\tNODE\tNODE\tNODE\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tNODE\t X \tSYS\tSYS\tSYS\tSYS\nNIC4\tSYS\tSYS\tSYS\tSYS\tPIX\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t X \tNODE\tNODE\tNODE\nNIC5\tSYS\tSYS\tSYS\tSYS\tNODE\tPIX\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\t X \tNODE\tNODE\nNIC6\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPIX\tPHB\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\t X \tPHB\nNIC7\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPHB\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPHB\t X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_bond_0\n  NIC1: mlx5_bond_1\n  NIC2: mlx5_bond_2\n  NIC3: mlx5_bond_3\n  NIC4: mlx5_bond_4\n  NIC5: mlx5_bond_5\n  NIC6: mlx5_bond_6\n  NIC7: mlx5_bond_7\n\n\nulimit soft: 1048576\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-18T04:30:41+00:00",
    "closed_at": "2025-03-19T01:43:09+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4536/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4536"
  },
  {
    "number": 3739,
    "title": "[Feature] Add SageMaker Support",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nHi.\n\nSageMaker Endpoints will listen for invocations at `/invocations` and for health checks at `/health` ([ref](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-container-response)). On modifying the server to listen on these endpoints, sglang can be used for SageMaker endpoints for hosting models. Is it possible to add support for this?\n\nThanks\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-21T03:13:58+00:00",
    "closed_at": "2025-02-21T22:49:50+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3739/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3739"
  },
  {
    "number": 4123,
    "title": "[Bug] Inaccurate or Inconsistent Output in Qwen2.5-VL Multi-Image Testing with sglang",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen testing multi-image input with `sglang` using the Qwen2.5-VL model, the output descriptions of the image contents are inaccurate or inconsistent. Compared to the output from directly calling the same model with `transformers`, the results from `sglang` show significant differences in describing the commonalities between multiple images.\n\n### Reproduction\n\nargs:\n```\n--chat-template qwen2-vl --tp-size 2 --mem-fraction-static 0.75 --disable-radix-cache --chunked-prefill-size -1\n```\nrequest:\n```\n{\n    \"model\": \"default\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"\u63cf\u8ff0\u4e00\u4e0b\u4e24\u5f20\u56fe\u7247\u76f8\u540c\u70b9\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"/home/panlyu/sglang/kitchen.jpg\"\n                    }\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"/home/panlyu/sglang/street.jpg\"\n                    }\n                }\n            ]\n        }\n    ],\n    \"stream\": true\n}\n```\n\noutput:\n1: \n```\n\"\u8fd9\u4e24\u5f20\u56fe\u7247\u7684\u4e3b\u8981\u76f8\u540c\u70b9\u5982\u4e0b\uff1a\\n\\n1. **\u65f6\u95f4\u6233**\uff1a\u4e24\u5f20\u56fe\u7247\u7684\u65f6\u95f4\u6233\u663e\u793a\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u90fd\u662f2022\u5e7410\u670810\u65e5\u65e9\u4e0a10\u70b923\u5206\u5de6\u53f3\u3002\\n2. **\u5185\u5bb9\u63cf\u8ff0**\uff1a\u4e24\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e24\u4e2a\u4eba\u5728\u4e00\u4e2a\u53a8\u623f\u73af\u5883\u4e2d\uff0c\u5176\u4e2d\u4e00\u4e2a\u4eba\u7a7f\u7740\u53a8\u5e08\u670d\u548c\u56f4\u88d9\uff0c\u53e6\u4e00\u4f4d\u7a7f\u7740\u9ed1\u8272\u5916\u5957\u3002\u4e24\u4eba\u90fd\u7ad9\u5728\u4e00\u53f0\u5927\u578b\u53a8\u623f\u70f9\u996a\u8bbe\u5907\u65c1\u8fb9\uff0c\u770b\u8d77\u6765\u50cf\u662f\u4e00\u4e2a\u70f9\u996a\u793a\u8303\u6216\u6559\u5b66\u573a\u666f\u3002\\n3. **\u573a\u666f**\uff1a\u4e24\u5f20\u56fe\u7247\u7684\u573a\u666f\u975e\u5e38\u76f8\u4f3c\uff0c\u80cc\u666f\u548c\u8bbe\u5907\u5e03\u5c40\u51e0\u4e4e\u4e00\u81f4\uff0c\u8868\u660e\u8fd9\u4e24\u5f20\u56fe\u7247\u662f\u540c\u4e00\u4e8b\u4ef6\u6216\u6d3b\u52a8\u7684\u4e00\u90e8\u5206\u3002\\n\\n\u4ee5\u4e0b\u662f\u4e24\u5f20\u56fe\u7247\u7684\u5177\u4f53\u63cf\u8ff0\uff1a\\n\\n**\u7b2c\u4e00\u5f20\u56fe\u7247\u63cf\u8ff0**\uff1a\\n- \u65f6\u95f4\u6233\u663e\u793a2022\u5e7410\u670810\u65e510:23:46\u3002\\n- \u573a\u666f\u662f\u5728\u4e00\u4e2a\u53a8\u623f\u73af\u5883\u4e2d\uff0c\u4e00\u4e2a\u4eba\u8eab\u7a7f\u53a8\u5e08\u670d\u548c\u56f4\u88d9\uff0c\u7ad9\u5728\u4e00\u53f0\u5927\u578b\u70f9\u996a\u8bbe\u5907\u65c1\u8fb9\uff0c\u624b\u6301\u4e00\u4e2a\u624b\u673a\u53ef\u80fd\u5728\u64cd\u4f5c\u6216\u67e5\u770b\u67d0\u4e9b\u4fe1\u606f\u3002\\n- \u540e\u65b9\u6709\u4e00\u5f20\u684c\u5b50\u548c\u4e00\u4e9b\u53a8\u623f\u7528\u54c1\u3002\\n\\n**\u7b2c\u4e8c\u5f20\u56fe\u7247\u63cf\u8ff0**\uff1a\\n- \u65f6\u95f4\u6233\u663e\u793a2022\u5e7410\u670810\u65e510:19:04\u3002\\n- \u573a\u666f\u4e5f\u662f\u4e00\u6837\u7684\uff0c\u4e24\u4e2a\u4eba\u7ad9\u5728\u540c\u4e00\u4e2a\u53a8\u623f\u73af\u5883\u4e2d\uff0c\u80cc\u666f\u8bbe\u5907\u548c\u5e03\u5c40\u548c\u7b2c\u4e00\u5f20\u56fe\u7247\u76f8\u540c\u3002\\n- \u4e00\u4e2a\u4eba\u8eab\u7a7f\u53a8\u5e08\u670d\u548c\u56f4\u88d9\uff0c\u53e6\u4e00\u4e2a\u7a7f\u7740\u9ed1\u8272\u5916\u5957\uff0c\u90fd\u5728\u76f8\u540c\u7684\u5927\u578b\u70f9\u996a\u8bbe\u5907\u65c1\u8fb9\u3002\\n\\n\u4e24\u5f20\u56fe\u7247\u63cf\u7ed8\u4e86\u540c\u4e00\u4e2a\u53a8\u623f\u573a\u666f\uff0c\u663e\u793a\u4e24\u4e2a\u4eba\u5728\u8fdb\u884c\u67d0\u79cd\u70f9\u996a\u6d3b\u52a8\u6216\u6559\u5b66\uff0c\u53ef\u80fd\u662f\u53a8\u5e08\u6f14\u793a\u6216\u6559\u5b66\uff0c\u4e5f\u6709\u53ef\u80fd\u5728\u62cd\u6444\u6559\u5b66\u89c6\u9891\u6216\u76f4\u64ad\u3002\"\n```\n2: \n```\n\"\u8fd9\u4e24\u5f20\u56fe\u7247\u7684\u76f8\u540c\u70b9\u5982\u4e0b\uff1a\\n\\n1. **\u65f6\u95f4\u6233\u4e00\u81f4**\uff1a\u4e24\u5f20\u56fe\u7247\u7684\u65f6\u95f4\u6233\u90fd\u662f**2022-06-29 20:19:04**\uff0c\u663e\u793a\u8fd9\u4e24\u5f20\u56fe\u7247\u662f\u5728\u540c\u4e00\u65f6\u95f4\u62cd\u6444\u7684\u3002\\n\\n2. **\u573a\u666f\u76f8\u4f3c**\uff1a\u4e24\u5f20\u56fe\u7247\u7684\u573a\u666f\u770b\u8d77\u6765\u76f8\u4f3c\uff0c\u80cc\u666f\u548c\u73af\u5883\u90fd\u975e\u5e38\u76f8\u4f3c\uff0c\u90fd\u663e\u793a\u4e86\u4e00\u4e2a\u5546\u4e1a\u8857\u6216\u8857\u533a\u7684\u60c5\u51b5\uff0c\u6709\u8def\u706f\u3001\u5efa\u7b51\u7269\u548c\u4e00\u4e9b\u884c\u4eba\u3002\\n\\n3. **\u884c\u4eba\u53ca\u8f66\u8f86\u4fe1\u606f\u76f8\u540c**\uff1a\u4e24\u5f20\u56fe\u7247\u4e2d\u7684\u884c\u4eba\u548c\u8f66\u8f86\u7684\u4f4d\u7f6e\u3001\u59ff\u6001\u90fd\u975e\u5e38\u76f8\u4f3c\uff0c\u5c24\u5176\u662f\u56fe\u7247\u4e2d\u7684\u884c\u4eba\u7a7f\u7740\u84dd\u8272\u548c\u9ed1\u8272\u8863\u670d\uff0c\u4ed6\u4eec\u5728\u6b65\u884c\u6216\u7ad9\u7acb\u7684\u4f4d\u7f6e\u51e0\u4e4e\u6ca1\u6709\u53d8\u5316\u3002\\n\\n4. **\u76f8\u673a\u4f4d\u7f6e\u76f8\u4f3c**\uff1a\u4e24\u5f20\u56fe\u7247\u7684\u62cd\u6444\u89d2\u5ea6\u548c\u76f8\u673a\u4f4d\u7f6e\u975e\u5e38\u63a5\u8fd1\uff0c\u8868\u660e\u53ef\u80fd\u662f\u540c\u4e00\u4e2a\u6444\u50cf\u5934\u7684\u4e0d\u540c\u5e27\u56fe\u50cf\u3002\\n\\n5. **\u753b\u9762\u5185\u5bb9\u76f8\u540c**\uff1a\u4e24\u5f20\u56fe\u7247\u7684\u524d\u666f\u548c\u80cc\u666f\u5185\u5bb9\u51e0\u4e4e\u4e00\u81f4\uff0c\u5305\u62ec\u5efa\u7b51\u7269\u3001\u5e97\u94fa\u62db\u724c\u3001\u8def\u706f\u3001\u884c\u4eba\u548c\u8f66\u8f86\u7b49\u5143\u7d20\u3002\\n\\n\u901a\u8fc7\u4ee5\u4e0a\u5bf9\u6bd4\u53ef\u4ee5\u770b\u51fa\uff0c\u8fd9\u4e24\u5f20\u56fe\u7247\u5f88\u53ef\u80fd\u6765\u6e90\u4e8e\u540c\u4e00\u4e2a\u89c6\u9891\u7247\u6bb5\u7684\u4e0d\u540c\u5e27\u56fe\u50cf\u3002\"\n```\n\nUsing transformers:\n```\nfrom transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLProcessor\nfrom qwen_vl_utils import process_vision_info\n\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    \"/home/panlyu/lmodels/Qwen2.5-VL-7B-Instruct\", device_map=\"cuda:0\"\n)\n\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = Qwen2_5_VLProcessor.from_pretrained(\n    \"/home/panlyu/lmodels/Qwen2.5-VL-7B-Instruct\",\n    min_pixels=min_pixels,\n    max_pixels=max_pixels,\n)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"/home/panlyu/lmodels/Qwen2.5-VL-7B-Instruct/street.jpg\",\n            },\n            {\n                \"type\": \"image\",\n                \"image\": \"/home/panlyu/lmodels/Qwen2.5-VL-7B-Instruct/kitchen.jpg\",\n            },\n            {\"type\": \"text\", \"text\": \"\u63cf\u8ff0\u4e00\u4e0b\u4e24\u5f20\u56fe\u7247\u76f8\u540c\u70b9\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(model.device)\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=512)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\noutput:\n```\n['\u8fd9\u4e24\u5f20\u56fe\u7247\u7684\u76f8\u540c\u70b9\u5305\u62ec\uff1a\\n\\n1. **\u65f6\u95f4\u6233**\uff1a\u4e24\u5f20\u56fe\u7247\u90fd\u663e\u793a\u4e86\u65e5\u671f\u548c\u65f6\u95f4\uff0c\u7b2c\u4e00\u5f20\u56fe\u7247\u7684\u65f6\u95f4\u662f2022\u5e746\u670829\u65e520:19:04\uff0c\u7b2c\u4e8c\u5f20\u56fe\u7247\u7684\u65f6\u95f4\u662f2019\u5e7411\u670827\u65e514:16:46\u3002\\n   \\n2. **\u4eba\u7269\u5b58\u5728**\uff1a\u4e24\u5f20\u56fe\u7247\u4e2d\u90fd\u6709\u4eba\u5728\u573a\u3002\u7b2c\u4e00\u5f20\u56fe\u7247\u4e2d\u6709\u4e24\u4e2a\u4eba\u5728\u4eba\u884c\u9053\u4e0a\u884c\u8d70\uff0c\u7b2c\u4e8c\u5f20\u56fe\u7247\u4e2d\u6709\u4e09\u4e2a\u4eba\u7ad9\u5728\u53a8\u623f\u533a\u57df\u3002\\n\\n3. **\u573a\u666f\u73af\u5883**\uff1a\u4e24\u5f20\u56fe\u7247\u90fd\u5c55\u793a\u4e86\u516c\u5171\u573a\u6240\u6216\u5de5\u4f5c\u573a\u6240\u7684\u573a\u666f\u3002\u7b2c\u4e00\u5f20\u56fe\u7247\u662f\u4e00\u4e2a\u57ce\u5e02\u8857\u9053\u7684\u4ea4\u53c9\u53e3\uff0c\u7b2c\u4e8c\u5f20\u56fe\u7247\u5219\u662f\u5728\u4e00\u4e2a\u53a8\u623f\u73af\u5883\u4e2d\u3002\\n\\n4. **\u6587\u5b57\u4fe1\u606f**\uff1a\u4e24\u5f20\u56fe\u7247\u4e2d\u90fd\u6709\u4e2d\u6587\u6587\u5b57\uff0c\u7b2c\u4e00\u5f20\u56fe\u7247\u53f3\u4e0b\u89d2\u6709\u201c\u514b\u62c9\u739b\u4f9d Y\u535a\u8fbe\u5e02\u573a\u5165\u53e31\u201d\u7684\u5b57\u6837\uff0c\u7b2c\u4e8c\u5f20\u56fe\u7247\u5de6\u4e0a\u89d2\u6709\u65e5\u671f\u548c\u65f6\u95f4\u4fe1\u606f\u3002\\n\\n5. **\u80cc\u666f\u5143\u7d20**\uff1a\u4e24\u5f20\u56fe\u7247\u7684\u80cc\u666f\u90fd\u5305\u542b\u4e00\u4e9b\u5546\u4e1a\u5143\u7d20\uff0c\u6bd4\u5982\u7b2c\u4e00\u5f20\u56fe\u7247\u4e2d\u7684\u5546\u5e97\u62db\u724c\u548c\u7b2c\u4e8c\u5f20\u56fe\u7247\u4e2d\u7684\u53a8\u623f\u8bbe\u5907\u3002\\n\\n\u8fd9\u4e9b\u5171\u540c\u70b9\u8868\u660e\u4e24\u5f20\u56fe\u7247\u53ef\u80fd\u6765\u81ea\u4e0d\u540c\u7684\u76d1\u63a7\u6444\u50cf\u5934\uff0c\u4f46\u90fd\u8bb0\u5f55\u4e86\u516c\u5171\u7a7a\u95f4\u5185\u7684\u6d3b\u52a8\u573a\u666f\u3002']\n```\n\n![Image](https://github.com/user-attachments/assets/796b63c3-ceba-4aca-831b-53f5429ac078)\n![Image](https://github.com/user-attachments/assets/05bf5ff3-5d54-42d4-8870-cc8e31157931)\n\n\n### Environment\n\nPython: 3.12.9 (main, Feb 12 2025, 14:50:50) [Clang 19.1.6 ]\nCUDA available: True\nGPU 0,1,2,3: NVIDIA RTX 6000 Ada Generation\nGPU 0,1,2,3 Compute Capability: 8.9\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 11.8, V11.8.89\nCUDA Driver Version: 525.105.17\nPyTorch: 2.5.1+cu124\nsglang: 0.4.3.post2\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.2.post1+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.22.3\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.62.0\ntiktoken: 0.8.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity\nGPU0     X      NODE    SYS     SYS     0-27,56-83      0\nGPU1    NODE     X      SYS     SYS     0-27,56-83      0\nGPU2    SYS     SYS      X      NODE    28-55,84-111    1\nGPU3    SYS     SYS     NODE     X      28-55,84-111    1\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1048576",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-06T03:59:30+00:00",
    "closed_at": "2025-03-06T16:06:04+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4123/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4123"
  },
  {
    "number": 5537,
    "title": "[Feature] Support Deepseek-vl2-tiny model, in which mla is disabled",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nAccording to https://github.com/sgl-project/sglang/issues/2653, the Deepseek-vl2 models are supported, but not all models in the series are supported as I used. Deepseek-vl2's models series is composed of three variants: DeepSeek-VL2-Tiny, DeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated parameters respectively. The tiny models has different model structure with small and normal model, which MLA (Multi-Head Latent Attention) is disabled. And if using DeepseekV2ForCasualLLM as a language model, the qk_nope_head_dim and qk_rope_head_dim  added as qk_head_dim  will cause a ZeroDivisionError in the later sampling var calculation for **-0.5 operation (https://github.com/sgl-project/sglang/blob/bfa392245159147a2b7dbd67178c825e5035c329/python/sglang/srt/models/deepseek_v2.py#L427). There is a stardard solution in vllm, which replaces DeepseekV2ForCasualLLM with DeepseekForCasualLLM.\n\nBesides, the chat template in deepseek-vl2 is also not aligned with vllm. A `<image>` will be added in the end of prompt automatically during the chat conversation generation process. If user pass `<image>`  in their prompt to denote the image, then there will be a dismatch between real image counts and prompt `<image>` counts.\n\nI have already validated the deepseek-vl2-tiny model locally, ensuring that the output results are consistent with those from vllm. Additionally, I\u2019ve proved some performance improvements, with speeds potentially being 5% to 20% faster depending on the number of decoding steps (thanks to the excellent sglang backend).  **I\u2019m wondering if I can contribute to this feature.** with just a little more work of reorganizing code elegantly and adding some tests. I\u2019m really looking forward to receiving feedback from the community. Thanks!\n\n### Related resources\n\nDifference between tiny and normal size models.\n[deepseek-vl2] https://huggingface.co/deepseek-ai/deepseek-vl2/blob/main/config.json\n[deepseek-vl2-tiny] https://huggingface.co/deepseek-ai/deepseek-vl2-tiny/blob/main/config.json\n\nSpecial token maps:\nhttps://huggingface.co/deepseek-ai/deepseek-vl2-small/blob/main/special_tokens_map.json\n\nDeepseek-vl2 chat examples (with \\<image\\> input):\nhttps://github.com/deepseek-ai/DeepSeek-VL2?tab=readme-ov-file#simple-inference-example-with-one-image\n\nSome code suggestions  and examples from vllm:\nhttps://github.com/vllm-project/vllm/blob/686623c5e7a0ee0c7679c052ced565dd83055709/vllm/model_executor/models/deepseek_vl2.py#L355\n",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-18T14:07:50+00:00",
    "closed_at": "2025-04-30T08:45:22+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5537/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5537"
  },
  {
    "number": 283,
    "title": "[BUG] Flashinfer 0.0.3 compat with Sglang",
    "body": "Using flashinfer 0.0.3 requires one line change #282 but there is a compat issue where same model runs fine on 0.0.2 but under 0.0.3 throws an infinite loop of the following on sglang:\r\n\r\n```\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 184, in exposed_step\r\n    self.forward_step()\r\n  File \"/root/miniconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 211, in forward_step\r\n    self.forward_decode_batch(self.running_batch)\r\n  File \"/root/miniconda3/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 505, in forward_decode_batch\r\n    next_token_ids, _ = batch.sample(logits)\r\n                        ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/miniconda3/lib/python3.11/site-packages/sglang/srt/managers/router/infer_batch.py\", line 476, in sample\r\n    sampled_index = torch.multinomial(probs_sort, num_samples=1)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n```\r\n\r\nI am unsure if this is compat issue due to sglang or flashinfer 0.0.3.\r\n\r\n @merrymercy  @yzh119",
    "labels": [],
    "state": "closed",
    "created_at": "2024-03-12T00:33:39+00:00",
    "closed_at": "2024-03-12T13:45:59+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/283/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/283"
  },
  {
    "number": 7048,
    "title": "[Bug] Missing tool call id if tool call index >0 in streaming tool call output.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nIn streaming function call result, the tool call id will be \"None\" if index > 0.\nhere is the streaming return with 2 tool calls, we can see \"index=1 id=None\"\n```\n[ChoiceDeltaToolCall(index=0, id='call_UQzAnUrYQreNoTmmHGnNyA', function=ChoiceDeltaToolCallFunction(arguments='', name='get_current_temperature'), type='function')]\n[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"', name=None), type='function')]\n[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='location', name=None), type='function')]\n[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":', name=None), type='function')]\n[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' \"', name=None), type='function')]\n[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='Beijing', name=None), type='function')]\n[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=',', name=None), type='function')]\n[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' China', name=None), type='function')]\n[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\"}', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name='get_temperature_date'), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='location', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments=' \"', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='Beijing', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments=',', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments=' China', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='\",', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments=' \"', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='date', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments=' \"', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='202', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='4', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='-', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='06', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='-', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='12', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='\"}', name=None), type='function')]\n```\n\n### Reproduction\n\nTest script:\n```\nfrom openai import OpenAI\n\nopenai_api_base = \"\"\nopenai_api_key = \"\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base + \"/v1\",\n)\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_temperature\",\n            \"description\": \"Get current temperature at a location.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": 'The location to get the temperature for, in the format \"City, State, Country\".',\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                        \"description\": 'The unit to return the temperature in. Defaults to \"celsius\".',\n                    },\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_temperature_date\",\n            \"description\": \"Get temperature at a location and date.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": 'The location to get the temperature for, in the format \"City, State, Country\".',\n                    },\n                    \"date\": {\n                        \"type\": \"string\",\n                        \"description\": 'The date to get the temperature for, in the format \"Year-Month-Day\".',\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                        \"description\": 'The unit to return the temperature in. Defaults to \"celsius\".',\n                    },\n                },\n                \"required\": [\"location\", \"date\"],\n            },\n        },\n    },\n]\n\n\ntool_calls_stream = client.chat.completions.create(\n    model=client.models.list().data[0].id,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"\u73b0\u5728\u7684\u65e5\u671f\u662f: 2024-09-30\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"\u5317\u4eac\u4eca\u5929\u7684\u5929\u6c14\u5982\u4f55\uff1f\u660e\u5929\u5462\uff1f\",\n        },\n    ],\n    tools=tools,\n    tool_choice=\"auto\",\n    stream=True,\n    max_completion_tokens=8192,\n)\n\nfor chunk in tool_calls_stream:\n    if chunk.choices[0].delta.tool_calls:\n        print(chunk.choices[0].delta.tool_calls)\n````\n\nWill get:\n```\n[ChoiceDeltaToolCall(index=0, id='call_UQzAnUrYQreNoTmmHGnNyA', function=ChoiceDeltaToolCallFunction(arguments='', name='get_current_temperature'), type='function')]\n[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"', name=None), type='function')]\n[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='location', name=None), type='function')]\n[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":', name=None), type='function')]\n[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' \"', name=None), type='function')]\n[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='Beijing', name=None), type='function')]\n[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=',', name=None), type='function')]\n[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' China', name=None), type='function')]\n[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\"}', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name='get_temperature_date'), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='location', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments=' \"', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='Beijing', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments=',', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments=' China', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='\",', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments=' \"', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='date', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments=' \"', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='202', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='4', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='-', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='06', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='-', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='12', name=None), type='function')]\n[ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='\"}', name=None), type='function')]\n```\n\n\n### Environment\n\nLatest main branch of SGLang repo.",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-10T10:03:45+00:00",
    "closed_at": "2025-06-11T02:27:30+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7048/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7048"
  },
  {
    "number": 6189,
    "title": "[Feature] Improve transmission performance",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nWhen transmitting tensors via zmq from tokenizer manager (where processors are called) to scheduler, recv_pyobj leads to severe performance decrease. Maybe it is because of the weak performance of pickle\n\nThe picture shows the comparison of returning all medias in one mm_item and in respective mm_items in the processor. As the number of medias increases, this problem becomes prominent.\n\n<img width=\"2166\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/6f59be35-5d2c-4476-9b4b-25ca429cafb4\" />\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-05-11T05:05:08+00:00",
    "closed_at": "2025-07-11T00:20:22+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6189/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6189"
  },
  {
    "number": 4208,
    "title": "[Feature] sglang-router should perform extra status check on workers upon startup in addition to port reachability",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nWhen Co-launch Router and Runtimes (with cuda-graph and torch-compile), worker start-up can take longer than 300s, result in health check timeout. \n\nReproduce: start the router and server with `python3 -m sglang_router.launch_server  --enable-torch-compile`, I run with a Mistral 8*7B here. Then get error:\n\n```\nSingleProcess AUTOTUNE benchmarking takes 1.3427 seconds and 8.8429 seconds precompiling\nAUTOTUNE mm(4x4096, 4096x8)\n  mm 0.0134 ms 100.0%\n  triton_mm_99 0.0134 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, .......\nSingleProcess AUTOTUNE benchmarking takes 1.3672 seconds and 8.3651 seconds precompiling\n[Router (Rust)] 2025-03-08 02:33:09 - INFO - Worker http://0.0.0.0:31000 health check is pending with error: error sending request for url (http://0.0.0.0:31000/health)\n[Router (Rust)] 2025-03-08 02:33:09 - INFO - Worker http://0.0.0.0:31778 health check is pending with error: error sending request for url (http://0.0.0.0:31778/health)\n[Router (Rust)] 2025-03-08 02:33:09 - INFO - Unhealthy workers:\n[Router (Rust)] 2025-03-08 02:33:09 - INFO -   http://0.0.0.0:31000 - Error: error sending request for url (http://0.0.0.0:31000/health)\n[Router (Rust)] 2025-03-08 02:33:09 - INFO -   http://0.0.0.0:31778 - Error: error sending request for url (http://0.0.0.0:31778/health)\n[Router (Rust)] 2025-03-08 02:33:19 - INFO - Worker http://0.0.0.0:31000 health check is pending with error: error sending request for url (http://0.0.0.0:31000/health)\n[Router (Rust)] 2025-03-08 02:33:19 - INFO - Worker http://0.0.0.0:31778 health check is pending with error: error sending request for url (http://0.0.0.0:31778/health)\n[Router (Rust)] 2025-03-08 02:33:19 - INFO - Unhealthy workers:\n[Router (Rust)] 2025-03-08 02:33:19 - INFO -   http://0.0.0.0:31000 - Error: error sending request for url (http://0.0.0.0:31000/health)\n[Router (Rust)] 2025-03-08 02:33:19 - INFO -   http://0.0.0.0:31778 - Error: error sending request for url (http://0.0.0.0:31778/health)\nERROR - Timeout 300s waiting for workers [\"http://0.0.0.0:31000\", \"http://0.0.0.0:31778\"] to become healthy. Please set --router-worker-startup-timeout-secs (sglang_router.launch_server) or --worker-startup-timeout-secs (sglang_worker.router) to a larger value\n```\n\nHowever, as long as I avoid using sglang-router and simply start the server once in the normal manner to allow it to complete the compilation, then switch to router and this error will no longer occur.\n\nMy sglang version is:\n```\nsglang==0.4.3.post4\nsglang-router==0.1.4\n```\n\nAlthough there is a option `--router-worker-startup-timeout` to setup the waiting time, but strictly speaking, the worker is not inactive at this point but rather does not have an open port. I am uncertain about the original design intent of this option. But I noticed setting `--router-worker-startup-timeout` to a very big value may result in the router itself becoming unresponsive (e.g. No response to Ctrl+C) or sometimes worker is already dead, but router doesn't know it (I encountered a situation where a typo in the model path caused the worker to terminate almost immediately. However, since there are a log of output on the screen, I didn't notice it and waited for a long time before discovering the issue).\n\n I recommend that when the router and worker start up, in addition to checking the accessibility of the API port, the worker should also output its current status (whether it is compiling or capturing the CUDA graph) to avoid setting up a timeout threshold (it's also hard to know how long would it take when facing a new model).\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-08T12:09:22+00:00",
    "closed_at": "2025-05-10T00:18:08+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4208/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4208"
  },
  {
    "number": 4780,
    "title": "[Bug] Master node didn't detect worker node not functional, requests hang until timeout",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nContext: \n\nThe issue was identified after six days of stable operation.\n\nBehavior:\n\nFrom master node (aka. node rank 0) everything looks normal, API server still responding requests to `/health` API endpoint, but `/health_generate` will get 503 because of timeout: \n\n<img width=\"1730\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/53078369-9fc2-4329-af9e-72709619cc72\" />\n\nManually sending request to `/get_server_info` or `v1/chat/` also get no response until timeout.\n\nI also noticed there is a \"last_heartbeat time: 08:07:56\" in the `/health_generate` log. After reviewing the code, I found that the so-called \"heartbeat\" refers to the time of the last response received from the detokenizer. I am unsure if this variable name is misleading.\n\nThe `/metrics` API endpoint is always returning the statistic of the last moment:\n\n<img width=\"1903\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b71b70a1-206a-4f7f-bacb-aaff74890189\" />\n\nThe processes seem to exist but `<defunct>`:\n\n```\nroot@sgl-deepseek-0-1:/sgl-workspace# ps aux\nUSER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot           1  0.0  0.0 50313164 757756 ?     Ssl  Mar19   8:23 python3 -m sglang.launch_server --host 0.0.0.0 --port 8888 --model-path /tmp/scratch-space/DeepSeek-R1 --tp\nroot         100  0.0  0.0      0     0 ?        Z    Mar19   0:00 [python3] <defunct>\nroot         101  153  0.0      0     0 ?        Z    Mar19 13793:02 [sglang::schedul] <defunct>\nroot         102  152  0.0      0     0 ?        Z    Mar19 13744:18 [sglang::schedul] <defunct>\nroot         103  153  0.0      0     0 ?        Z    Mar19 13797:47 [sglang::schedul] <defunct>\nroot         104  151  0.0      0     0 ?        Z    Mar19 13646:06 [sglang::schedul] <defunct>\nroot         105  152  0.0      0     0 ?        Z    Mar19 13719:02 [sglang::schedul] <defunct>\nroot         106  151  0.0      0     0 ?        Z    Mar19 13667:59 [sglang::schedul] <defunct>\nroot         107  150  0.0      0     0 ?        Z    Mar19 13580:48 [sglang::schedul] <defunct>\nroot         108  150  0.0      0     0 ?        Z    Mar19 13542:28 [sglang::schedul] <defunct>\nroot      264037  0.0  0.0      0     0 ?        Z    Mar25   0:00 [dash] <defunct>\nroot      264038  0.0  0.0      0     0 ?        Z    Mar25   0:00 [py-spy] <defunct>\n```\n\nPod log of dead worker (too long to be put into the issue):\n\nhttps://gist.github.com/junliu-mde/9cef18417bf8f8c0fbb351a5f6584089\n\n\n### Reproduction\n\n2 H100*8 Node, run with command: `python3 -m sglang.launch_server --model-path /path/to/DeepSeek-R1 --tp 16 --dist-init-addr X.X.X.X:5000 --nnodes 2 --node-rank 1 --trust-remote-code --enable-torch-compile --show-time-cost --max-running-requests 256 --reasoning-parser deepseek-r1 --speculative-algo NEXTN --speculative-draft /tmp/scratch-space/DeepSeek-R1-NextN --speculative-eagle-topk 1 --speculative-num-steps 3 --speculative-num-draft-tokens 4 --enable-metrics`\n\nUnfortunately, I am unable to reproduce this issue. Thanks Kebe in the comment reminded me the metrics issue is already known. \n\n### Environment\n\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.90.07\nPyTorch: 2.5.1+cu124\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.2.post1+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.13\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.2\ninteregular: 0.3.3\nmodelscope: 1.23.2\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.65.4\ntiktoken: 0.9.0\nanthropic: 0.49.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    CPU Affinity    NUMA AffinityGPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    0-47    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    0-47    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    0-47    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    0-47    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS     48-95   1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS     48-95   1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS     48-95   1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS     48-95   1               N/A\nNIC0    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE\nNIC1    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     SYS     SYS     SYS     NODE\nNIC2    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS     SYS     SYS     SYS     NODE\nNIC3    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS     NODE\nNIC4    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS\nNIC5    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS\nNIC8    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n\n\nHypervisor vendor: Microsoft\nulimit soft: 1048576",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-26T04:24:15+00:00",
    "closed_at": "2025-04-23T12:50:53+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4780/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4780"
  },
  {
    "number": 943,
    "title": "[Bug] Poor generation quality with Qwen2-57B-A14B-Instruct model",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n\n### Describe the bug\n\nWe noticed poor generation quality with the [Qwen2-57B-A14B-Instruct](https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct) model (compared with vLLM). Here are some examples:\r\n\r\n- prompt: `<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n\u8bf7\u4ecb\u7ecd\u4e00\u4e0b\u676d\u5dde<|im_end|>\\n<|im_start|>assistant\\n<|im_start|>assistant\\n`\r\n- sampling params:\r\n  - temperature: 0.7\r\n  - top_k: 20\r\n  - top_p: 0.8\r\n  - repetition_penalty: 1.05\r\n- vLLM outputs: `\u5bf9\u4e0d\u8d77\uff0c\u6211\u4e4b\u524d\u8bef\u89e3\u4e86\u4f60\u7684\u95ee\u9898\u3002\u4f5c\u4e3a\u4e00\u4e2aAI\u8bed\u8a00\u6a21\u578b\uff0c\u6211\u6ca1\u6709\u4e2a\u4eba\u7ecf\u9a8c\u6216\u4e3b\u89c2\u610f\u89c1\u3002\u4ee5\u4e0b\u662f\u5173\u4e8e\u676d\u5dde\u5e02\u7684\u4e00\u4e9b\u57fa\u672c\u4fe1\u606f\uff1a\\n\\n\u676d\u5dde\u5e02\u662f\u4e2d\u56fd\u6d59\u6c5f\u7701\u7684\u7701\u4f1a\u57ce\u5e02\uff0c\u4f4d\u4e8e\u4e2d\u56fd\u4e1c\u5357\u90e8\uff0c\u662f\u957f\u6c5f\u4e09\u89d2\u6d32\u5730\u533a\u7684\u91cd\u8981\u57ce\u5e02\u4e4b\u4e00\u3002\u676d \u5dde\u4ee5\u5176\u7f8e\u4e3d\u7684\u897f\u6e56\u800c\u95fb\u540d\u4e8e\u4e16\uff0c\u4e5f\u662f\u4e2d\u56fd\u5386\u53f2\u6587\u5316\u540d\u57ce\u548c\u91cd\u8981\u7684\u7ecf\u6d4e\u3001\u6587\u5316\u4e2d\u5fc3\u3002\\n\\n\u676d\u5dde\u5e02\u62e5\u6709\u4e30\u5bcc\u7684\u65c5\u6e38\u8d44\u6e90\uff0c\u9664\u4e86\u897f\u6e56\u5916\uff0c\u8fd8\u6709\u7075\u9690\u5bfa\u3001\u5343\u5c9b\u6e56\u3001\u897f\u6eaa\u6e7f\u5730\u7b49\u8457\u540d\u666f\u70b9\u3002\u6b64\u5916\uff0c\u676d\u5dde\u5e02\u8fd8\u662f\u4e2d\u56fd\u7535\u5b50\u5546\u52a1\u548c\u4e92\u8054\u7f51\u4ea7\u4e1a\u7684\u91cd\u8981\u4e2d\u5fc3\uff0c\u963f\u91cc\u5df4\u5df4\u603b\u90e8\u5c31\u4f4d\u4e8e\u676d\u5dde\u5e02\u3002\\n\\n\u5982\u679c\u4f60\u9700\u8981\u66f4\u8be6\u7ec6\u7684\u4fe1\u606f\uff0c\u8bf7\u544a\u8bc9\u6211\u3002\\n\\n\u8bf7\u4ecb\u7ecd\u4e00\u4e0b\u676d\u5dde\u7684\u6c14\u5019\u548c\u5730\u7406\u7279\u5f81\\n\\n\u676d\u5dde\u5730\u5904\u4e2d\u56fd\u4e1c\u5357\u6cbf\u6d77\u5730\u533a\uff0c\u5c5e\u4e8e\u4e9a\u70ed\u5e26\u5b63\u98ce\u6c14\u5019\uff0c\u56db\u5b63\u5206\u660e\uff0c\u96e8\u91cf\u5145\u6c9b\uff0c\u6c14\u5019\u6e29\u548c\u6e7f\u6da6\u3002\u676d\u5dde\u590f\u5b63\u708e\u70ed\u6f6e\u6e7f\uff0c\u51ac\u5b63\u5bd2\u51b7\u5e72\u71e5\uff0c\u6625\u79cb\u5b63\u8282\u6c14\u6e29\u9002\u4e2d\uff0c\u9002\u5408\u65c5\u6e38\u548c\u6237\u5916\u6d3b\u52a8\u3002\\n\\n\u676d\u5dde\u7684\u5730\u5f62\u4ee5\u4e18\u9675\u548c\u5e73\u539f\u4e3a\u4e3b\uff0c\u5730\u52bf\u8f83\u4e3a\u5e73\u5766\uff0c\u6d77\u62d4\u9ad8\u5ea6\u8f83\u4f4e\u3002\u676d\u5dde\u5883\u5185\u6709\u4f17\u591a\u6cb3\u6d41\u6e56\u6cca\uff0c\u5176\u4e2d\u6700\u4e3a\u8457\u540d\u7684\u662f\u897f\u6e56\uff0c\u5b83\u662f\u4e2d\u56fd\u6700\u8457\u540d\u7684\u6e56\u6cca\u4e4b\u4e00\uff0c\u4e5f\u662f\u676d\u5dde\u5e02\u7684\u6807\u5fd7\u6027\u666f\u70b9\u3002\u6b64\u5916\uff0c\u676d\u5dde\u8fd8\u6709\u94b1\u5858\u6c5f\u3001\u5bcc\u6625\u6c5f\u7b49\u6cb3\u6d41\uff0c\u4ee5\u53ca\u5343\u5c9b\u6e56\u3001\u897f\u6eaa\u6e7f\u5730\u7b49\u81ea\u7136\u4fdd\u62a4\u533a\u3002\\n\\n\u676d\u5dde\u662f\u4e00\u4e2a\u73b0\u4ee3\u5316\u7684\u57ce\u5e02\uff0c\u5e02\u533a\u5185\u9ad8\u697c\u6797\u7acb\uff0c\u4ea4\u901a\u4fbf\u5229\uff0c\u751f\u6d3b\u8bbe\u65bd\u5b8c\u5584\u3002\u540c\u65f6\uff0c\u676d\u5dde\u4e5f\u662f\u4e00\u4e2a\u5386\u53f2\u60a0\u4e45\u7684\u6587\u5316\u540d\u57ce\uff0c\u4fdd\u7559\u4e86\u8bb8\u591a\u53e4\u5efa\u7b51\u548c\u6587\u5316\u9057\u4ea7\uff0c\u5982\u7075\u9690\u5bfa\u3001\u516d\u548c\u5854\u3001\u5357\u5b8b\u5fa1\u8857\u7b49\u3002<|endoftext|>`\r\n- sglang outputs: `\u676d\u5dde\uff0c\u7b80\u79f0\u201c\u676d\u201d\uff0c\u53e4\u79f0\u4e34\u5b89\u3001\u94b1\u5858\uff0c\u662f\u6d59\u6c5f\u7701\u7701\u4f1a\u3001\u526f\u7701\u7ea7\u5e02\u3001\u676d\u5dde\u90fd\u5e02\u5708\u6838\u5fc3\u57ce\u5e02\uff0c\u56fd\u52a1\u9662\u6279\u590d\u786e\u5b9a\u7684\u6d59\u6c5f\u7701\u7701\u4f1a\u548c\u5168\u7701\u7ecf\u6d4e\u3001\u6587\u5316\u3001\u79d1\u6559\u4e2d\u5fc3\u3001\u957f\u6c5f\u4e09\u89d2\u6d32\u4e2d\u5fc3\u57ce\u5e02\u4e4b\u4e00\u3002\u622a\u81f32019\u5e74\uff0c\u5168\u5e02\u4e0b\u8f9610\u4e2a\u533a\u30012\u4e2a\u53bf\u3001\u4ee3\u7ba1\r\n1\u4e2a\u53bf\u7ea7\u5e02\uff0c\u603b\u9762\u79ef16853.57\u5e73\u65b9\u5343\u7c73\uff0c\u5efa\u6210\u533a\u9762\u79ef648.46\u5e73\u65b9\u5343\u7c73\uff0c\u5e38\u4f4f\u4eba\u53e31036\u4e07\u4eba\uff0c\u57ce\u9547\u4eba\u53e3813.26\u4e07\u4eba\uff0c\u57ce\u9547\u5316\u738778.5%\u3002\u676d\u5dde\u5730\u5904\u4e2d\u56fd\u534e\u4e1c\u5730\u533a\u3001\u94b1\u5858\u6c5f\u4e0b\u6e38\u3001\u4e1c\u5357\u6cbf\u6d77\u3001\u6d59\u6c5f\u5317\u90e8\u3001\u4eac\u676d\u5927\u8fd0\u6cb3\u5357\u7aef\uff0c\u662f\u73af\u676d\u5dde\u6e7e\u5927\u6e7e\u533a\u6838\r\n\u5fc3\u57ce\u5e02\u3001\u6caa\u5609\u676dG60\u79d1\u521b\u8d70\u5eca\u4e2d\u5fc3\u57ce\u5e02\u3001\u56fd\u9645\u91cd\u8981\u7684\u7535\u5b50\u5546\u52a1\u4e2d\u5fc3\u3002\u676d\u5dde\u4eba\u6587\u53e4\u8ff9\u4f17\u591a\uff0c\u897f\u6e56\u53ca\u5176\u5468\u8fb9\u6709\u5927\u91cf\u7684\u81ea\u7136\u53ca\u4eba\u6587\u666f\u89c2\u9057\u8ff9\uff0c\u5177\u4ee3\u8868\u6027\u7684\u6709\u897f\u6e56\u6587\u5316\u3001\u826f\u6e1a\u6587\u5316\u3001\u4e1d\u7ef8\u6587\u5316\u3001\u8336\u6587\u5316\uff0c\u4ee5\u53ca\u6d41\u4f20\u4e0b\u6765\u7684\u8bb8\u591a\u6545\u4e8b\u4f20\u8bf4\u3002\u676d\u5dde\u81ea\r\n\u79e6\u671d\u8bbe\u53bf\u6cbb\u4ee5\u6765\u5df2\u67092200\u591a\u5e74\u7684\u5386\u53f2\uff0c\u66fe\u662f\u5434\u8d8a\u56fd\u548c\u5357\u5b8b\u7684\u90fd\u57ce\u3002\u56e0\u98ce\u666f\u79c0\u4e3d\uff0c\u7d20\u6709\u201c\u4eba\u95f4\u5929\u5802\u201d\u7684\u7f8e\u8a89\u3002\u676d\u5dde\u5f97\u76ca\u4e8e\u4eac\u676d\u8fd0\u6cb3\u548c\u901a\u5546\u53e3\u5cb8\u7684\u4fbf\u5229\uff0c\u4ee5\u53ca\u81ea\u8eab\u53d1\u8fbe\u7684\u4e1d\u7ef8\u548c\u7cae\u98df\u4ea7\u4e1a\uff0c\u5386\u53f2\u4e0a\u66fe\u662f\u91cd\u8981\u7684\u5546\u4e1a\u96c6\u6563\u4e2d\u5fc3\u3002\u540e\u6765\u4f9d\u6258\u6caa\u676d\u94c1\r\n\u8def\u7b49\u94c1\u8def\u7ebf\u8def\u7684\u901a\u8f66\u4ee5\u53ca\u4e0a\u6d77\u5728\u8fdb\u51fa\u53e3\u8d38\u6613\u65b9\u9762\u7684\u5e26\u52a8\uff0c\u8f7b\u5de5\u4e1a\u53d1\u5c55\u8fc5\u901f\u3002\u65b0\u4e16\u7eaa\u4ee5\u6765\uff0c\u968f\u7740\u963f\u91cc\u5df4\u5df4\u7b49\u9ad8\u79d1\u6280\u4f01\u4e1a\u7684\u5e26\u52a8\uff0c\u4e92\u8054\u7f51\u7ecf\u6d4e\u6210\u4e3a\u676d\u5dde\u65b0\u7684\u7ecf\u6d4e\u589e\u957f\u70b9\u30022018\u5e74\u4e16\u754c\u77ed\u6c60\u6e38\u6cf3\u9526\u6807\u8d5b\u30012022\u5e74\u676d\u5dde\u4e9a\u8fd0\u4f1a\u30012022\u5e74\u676d\u5dde\u4e9a\u8fd0\r\n\u4f1a\u5ef6\u671f\u81f32023\u5e74\u4e3e\u884c\u30022017\u5e74\u4e2d\u56fd\u767e\u5f3a\u57ce\u5e02\u6392\u884c\u699c\u6392\u7b2c7\u4f4d\u30022019\u5e746\u670814\u65e5\uff0c\u676d\u5dde\u5165\u9009\u201c2019\u4e2d\u56fd\u6700\u4f73\u65c5\u6e38\u76ee\u7684\u5730\u57ce\u5e02\u201d\u30022019\u5e748\u6708\uff0c\u4e2d\u56fd\u6d77\u5173\u603b\u7f72\u4e3b\u529e\u7684\u300a\u4e2d\u56fd\u6d77\u5173\u300b\u6742\u5fd7\u516c\u5e03\u4e862018\u5e74\u201c\u4e2d\u56fd\u5916\u8d38\u767e\u5f3a\u57ce\u5e02\u201d\u6392\u540d\uff0c\u676d\u5dde\u6392\u540d\u7b2c8\u30022019\u4e2d\u56fd\u767e\u5f3a\u57ce\u5e02\u6392\u884c\u699c\u6392\u7b2c5\u4f4d\u30022019\u5e7412\u6708\uff0c\u676d\u5dde\u5165\u9009\u201c2019\u4e2d\u56fd\u6700\u5177\u5e78\u798f\u611f\u57ce\u5e02\u201d\uff08\u7b2c\u4e00\u540d\uff09\u30022020\u5e741\u6708\uff0c\u5165\u9009\u201c\u4e2d\u56fd\u57ce\u5e02\u79d1\u6280\u521b\u65b0\u53d1\u5c55\u6307\u65702019\u201d\u7b2c5\u4f4d\u30022020\u5e743\u6708\uff0c\u676d\u5dde\u88ab\u82f1\u56fd\u6742\u5fd7\u300a\u7ecf\u6d4e\u5b66\u4eba\u300b\u8bc4\u4e3a\u5168\u7403\u6700\u5b9c\u5c45\u57ce\u5e02\u6392\u540d\u7b2c\u516b\u4f4d\r\n\u30022020\u5e744\u6708\uff0c\u5165\u9009\u201c2020\u4e2d\u56fd\u907f\u6691\u540d\u57ce\u699c\u201d\u3002\\n\u676d\u5dde\u5386\u53f2\u6cbf\u9769 \u676d\u5dde\u57ce\u7684\u5386\u53f2\u53ef\u4ee5\u4e0a\u6eaf\u5230\u516c\u5143\u524d3000\u5e74\u5de6\u53f3\u7684\u826f\u6e1a\u6587\u5316\u65f6\u671f\u3002\u826f\u6e1a\u6587\u5316\u65f6\u671f\uff0c\u4eba\u4eec\u5df2\u7ecf\u5f00\u59cb\u751f\u4ea7\u7a3b\u7c73\uff0c\u79cd\u690d\u852c\u83dc\uff0c\u9972\u517b\u5bb6\u755c\uff0c\u8fd8\u5236\u4f5c\u9676\u5668\u3001\u7389\u5668\u548c\u77f3\u5668\u7b49\u3002\u826f\u6e1a\u6587\u5316\u65f6\u671f\r\n\u7684\u6587\u5316\u9057\u5740\uff0c\u81f3\u4eca\u5728\u676d\u5dde\u5883\u5185\u5df2\u53d1\u73b050\u591a\u5904\uff0c\u5176\u4e2d\u826f\u6e1a\u9057\u5740\u3001\u4f59\u676d\u533a\u7684\u8305\u5c71\u9057\u5740\u3001\u5434\u5c71\u9057\u5740\u3001\u94b1\u5c71\u6f3e\u9057\u5740\u7b49\u90fd\u662f\u826f\u6e1a\u6587\u5316\u65f6\u671f\u7684\u5178\u578b\u9057\u5740\u3002\u6625\u79cb\u65f6\u671f\uff0c\u5434\u8d8a\u4e24\u56fd\u4e89\u9738\uff0c\u676d\u5dde\u5148\u5c5e\u5434\uff0c\u8d8a\u706d\u5434\u540e\uff0c\u676d\u5dde\u53c8\u5c5e\u8d8a\u3002\u6218\u56fd\u65f6\u671f\uff0c\u695a\u706d\u8d8a\u540e\uff0c\u676d\r\n\u5dde\u53c8\u5f52\u5165\u695a\u56fd\u7684\u7248\u56fe\u3002\u79e6\u59cb\u7687\u7edf\u4e00\u516d\u56fd\u540e\uff0c\u4eca\u676d\u5dde\u4e00\u5e26\u5c5e\u4f1a\u7a3d\u90e1\uff0c\u90e1\u6cbb\u5728\u5434\u53bf\uff08\u4eca\u82cf\u5dde\uff09\u3002\u6c49\u671d\u65f6\uff0c\u676d\u5dde\u4ecd\u5c5e\u4f1a\u7a3d\u90e1\uff0c\u4e1c\u6c49\u987a\u5e1d\u65f6\u5206\u4f1a\u7a3d\u90e1\u4e3a\u5434\u90e1\u548c\u4f1a\u7a3d\u90e1\uff0c\u676d\u5dde\u5c5e\u4f1a\u7a3d\u90e1\u5bcc\u6625\u53bf\u3002\u4e09\u56fd\u65f6\u5c5e\u5434\u56fd\uff0c\u5b59\u6743\u628a\u4ed6\u7684\u54e5\u54e5\u5b59\u7b56\u4e4b\u5b50\u5b59\u7ecd\u5c01\u4e3a\r\n\u5bcc\u6625\u4faf\uff0c\u628a\u4ed6\u7684\u5973\u513f\u8bb8\u914d\u7ed9\u5b59\u7ecd\u3002\u5434\u9ec4\u9f99\u5e74\u95f4\uff08233\u2014234\u5e74\uff09\uff0c\u5b59\u6743\u5728\u5bcc\u6625\u53bf\u8bbe\u7acb\u90fd\u5c09\u5e9c\uff0c\u90fd\u5c09\u7684\u5b98\u804c\u76f8\u5f53\u4e8e\u90e1\u5b88\uff0c\u90fd\u5c09\u5e9c\u7684\u8bbe\u7acb\uff0c\u8bf4\u660e\u5f53\u65f6\u5bcc\u6625\u4e00\u5e26\u5df2\u6210\u4e3a\u5434\u56fd\u7684\u4e00\u4e2a\u519b\u4e8b\u91cd\u9547\u3002\u4e1c\u664b\u54b8\u548c\u5143\u5e74\uff08326\u5e74\uff09\uff0c\u676d\u5dde\u8bbe\u7acb\u4e3a\u94b1\u5510\u53bf\uff0c\u96b6\u4e8e\r\n\u626c\u5dde\u5434\u90e1\u3002\u5357\u671d\u9648\u6c38\u5b9a\u4e8c\u5e74\uff08558\u5e74\uff09\uff0c\u9648\u6587\u5e1d\u5e9f\u94b1\u5510\u53bf\u8bbe\u7acb\u94b1\u5858\u90e1\uff0c\u90e1\u6cbb\u4ecd\u5728\u94b1\u5510\u53bf\uff0c\u8fd9\u662f\u676d\u5dde\u8bbe\u90e1\u7684\u5f00\u59cb\u3002\u968b\u5f00\u7687\u4e5d\u5e74\uff08589\u5e74\uff09\uff0c\u968b\u6587\u5e1d\u5e9f\u94b1\u5858\u90e1\uff0c\u4ee5\u539f\u94b1\u5510\u53bf\u7684\u5730\u57df\u8bbe\u7acb\u4f59\u676d\u53bf\uff0c\u53bf\u6cbb\u4ecd\u5728\u94b1\u5510\uff0c\u8fd9\u662f\u676d\u5dde\u6539\u540d\u7684\u5f00\u59cb\u3002\u5510\u6b66\u5fb7\r\n\u56db\u5e74\uff08621\u5e74\uff09\uff0c\u5510\u9ad8\u7956\u674e\u6e0a\u53c8\u5728\u4f59\u676d\u53bf\u8bbe\u7acb\u94b1\u5858\u90e1\uff0c\u90e1\u6cbb\u4ecd\u5728\u4f59\u676d\u53bf\u3002\u5510\u5929\u5b9d\u5143\u5e74\uff08742\u5e74\uff09\uff0c\u5510\u7384\u5b97\u674e\u9686\u57fa\u6539\u94b1\u5858\u90e1\u4e3a\u676d\u5dde\uff0c\u8fd9\u662f\u201c\u676d\u5dde\u201d\u5f97\u540d\u7684\u5f00\u59cb\u3002\u676d\u5dde\u6539\u540d\u540e\uff0c\u4ecd\u4ee5\u4f59\u676d\u53bf\u4e3a\u676d\u5dde\u7684\u6cbb\u6240\u3002\u676d\u5dde\u7684\u5efa\u5236\u5728\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u7ecf\u5386\u4e86\r\n\u5434\u8d8a\u56fd\u548c\u5357\u5b8b\u4e24\u4e2a\u671d\u4ee3\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u5434\u8d8a\u56fd\u504f\u5b89\u4e1c\u5357\uff0c\u5efa\u90fd\u676d\u5dde\u3002\u5434\u8d8a\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u57ce\u5e02\u89c4\u6a21\u8fdb\u4e00\u6b65\u6269\u5927\uff0c\u4eba\u53e3\u4e5f\u8fdb\u4e00\u6b65\u589e\u52a0\u3002\u5434\u8d8a\u56fd\u7684\u5f00\u56fd\u4e4b\u541b\u94b1\u9560\uff0c\u5bf9\u676d\u5dde\u7684\u7ecf\u6d4e\u548c\u6587\u5316\u53d1\u5c55\u4e5f\u4f5c\u51fa\u4e86\u8d21\u732e\u3002\u4ed6\u91cd\u89c6\u6c34\u5229\u5efa\u8bbe\uff0c\u5174\u4fee\u4e86\u6d59\u6c5f\r\n\u3001\u94b1\u5858\u6c5f\u3001\u897f\u6e56\u7b49\u8bb8\u591a\u6c34\u5229\u5de5\u7a0b\u3002\u4ed6\u91cd\u89c6\u519c\u4e1a\u751f\u4ea7\uff0c\u7ec4\u7ec7\u4eba\u529b\u5728\u94b1\u5858\u6c5f\u6cbf\u5cb8\u4fee\u7b51\u634d\u6d77\u77f3\u5858\uff0c\u6709\u6548\u5730\u9632\u6b62\u4e86\u6d77\u6f6e\u7684\u5165\u4fb5\u3002\u4ed6\u91cd\u89c6\u53d1\u5c55\u5546\u4e1a\uff0c\u9f13\u52b1\u5546\u4e1a\u8d38\u6613\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u5546\u4e1a\u548c\u624b\u5de5\u4e1a\u5341\u5206\u53d1\u8fbe\uff0c\u5e02\u573a\u7e41\u8363\uff0c\u4eba\u53e3\u4f17\u591a\uff0c\u6210\u4e3a\u5168\r\n\u56fd\u7ecf\u6d4e\u3001\u6587\u5316\u4e2d\u5fc3\u4e4b\u4e00\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u4e1d\u7ec7\u4e1a\u3001\u5236\u74f7\u4e1a\u3001\u9020\u7eb8\u4e1a\u3001\u5370\u5237\u4e1a\u7b49\u624b\u5de5\u4e1a\u90fd\u5f88\u53d1\u8fbe\uff0c\u5546\u4e1a\u8d38\u6613\u4e5f\u5f88\u5174\u76db\u3002\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\u4e00\u5e26\uff0c\u90a3\u91cc\u662f\u676d\u5dde\u7684\u5546\u4e1a\u4e2d\u5fc3\u3002\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\r\n\u3001\u74f7\u5668\u3001\u8336\u53f6\u3001\u7eb8\u5f20\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\u4e00\u5e26\u4e3a\u4e2d\u5fc3\u3002\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\u3001\u74f7\u5668\u3001\u8336\u53f6\u3001\u7eb8\u5f20\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\r\n\u4e00\u5e26\u4e3a\u4e2d\u5fc3\u3002\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\u3001\u74f7\u5668\u3001\u8336\u53f6\u3001\u7eb8\u5f20\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\u4e00\u5e26\u4e3a\u4e2d\u5fc3\u3002\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\u3001\u74f7\u5668\u3001\u8336\u53f6\u3001\u7eb8\u5f20\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\r\n\uff0c\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\u4e00\u5e26\u4e3a\u4e2d\u5fc3\u3002\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\u3001\u74f7\u5668\u3001\u8336\u53f6\u3001\u7eb8\u5f20\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\u4e00\u5e26\u4e3a\u4e2d\u5fc3\u3002\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\u3001\u74f7\u5668\r\n\u3001\u8336\u53f6\u3001\u7eb8\u5f20\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\u4e00\u5e26\u4e3a\u4e2d\u5fc3\u3002\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\u3001\u74f7\u5668\u3001\u8336\u53f6\u3001\u7eb8\u5f20\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\u4e00\u5e26\u4e3a\r\n\u4e2d\u5fc3\u3002\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\u3001\u74f7\u5668\u3001\u8336\u53f6\u3001\u7eb8\u5f20\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\u4e00\u5e26\u4e3a\u4e2d\u5fc3\u3002\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\u3001\u74f7\u5668\u3001\u8336\u53f6\u3001\u7eb8\u5f20\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\r\n\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\u4e00\u5e26\u4e3a\u4e2d\u5fc3\u3002\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\u3001\u74f7\u5668\u3001\u8336\u53f6\u3001\u7eb8\u5f20\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\u4e00\u5e26\u4e3a\u4e2d\u5fc3\u3002\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\u3001\u74f7\u5668\u3001\u8336\u53f6\r\n\u3001\u7eb8\u5f20\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\u4e00\u5e26\u4e3a\u4e2d\u5fc3\u3002\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\u3001\u74f7\u5668\u3001\u8336\u53f6\u3001\u7eb8\u5f20\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\u4e00\u5e26\u4e3a\u4e2d\u5fc3\u3002\r\n\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\u3001\u74f7\u5668\u3001\u8336\u53f6\u3001\u7eb8\u5f20\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\u4e00\u5e26\u4e3a\u4e2d\u5fc3\u3002\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\u3001\u74f7\u5668\u3001\u8336\u53f6\u3001\u7eb8\u5f20\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u5546\u4e1a\r\n\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\u4e00\u5e26\u4e3a\u4e2d\u5fc3\u3002\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\u3001\u74f7\u5668\u3001\u8336\u53f6\u3001\u7eb8\u5f20\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\u4e00\u5e26\u4e3a\u4e2d\u5fc3\u3002\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\u3001\u74f7\u5668\u3001\u8336\u53f6\u3001\u7eb8\u5f20\r\n\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\u4e00\u5e26\u4e3a\u4e2d\u5fc3\u3002\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\u3001\u74f7\u5668\u3001\u8336\u53f6\u3001\u7eb8\u5f20\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\u4e00\u5e26\u4e3a\u4e2d\u5fc3\u3002\u676d\u5dde\u7684\r\n\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\u3001\u74f7\u5668\u3001\u8336\u53f6\u3001\u7eb8\u5f20\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001\u6e05\u6cb3\u574a\u4e00\u5e26\u4e3a\u4e2d\u5fc3\u3002\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\u4e3b\u8981\u4ee5\u4e1d\u7ef8\u3001\u74f7\u5668\u3001\u8336\u53f6\u3001\u7eb8\u5f20\u3001\u4e66\u7c4d\u7b49\u4e3a\u4e3b\u3002\u4e94\u4ee3\u5341\u56fd\u65f6\u671f\uff0c\u676d\u5dde\u7684\u5546\u4e1a\u8d38\u6613\uff0c\r\n\u4e3b\u8981\u4ee5\u6cb3\u574a\u8857\u3001`\r\n\n\n### Reproduction\n\nLaunch sglang/vllm server and run inference.\n\n### Environment\n\n```Shell\nPython: 3.11.2 (main, Mar 13 2024, 17:30:23) [GCC 10.2.1 20200825 (Alibaba 10.2.1-3.8 2.32)]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA H800\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.2, V12.2.91\r\nCUDA Driver Version: 535.54.03\r\n\r\nPyTorch: 2.3.0+cu121\r\nsglang: 0.2.5\r\nflashinfer: 0.1.2+cu121torch2.3\r\nrequests: 2.32.3\r\ntqdm: 4.66.4\r\nnumpy: 1.26.4\r\naiohttp: 3.9.5\r\nfastapi: 0.111.1\r\nhf_transfer: Module Not Found\r\nhuggingface_hub: 0.24.2\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\npillow: Module Not Found\r\npsutil: 6.0.0\r\npydantic: 2.7.4\r\nuvicorn: 0.30.3\r\nuvloop: 0.19.0\r\nzmq: 26.0.3\r\nvllm: 0.5.3.post1\r\nopenai: 1.38.0\r\nanthropic: 0.32.0\r\nlitellm: Module Not Found\r\n```\n```\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-06T02:23:13+00:00",
    "closed_at": "2024-08-06T07:07:49+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/943/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/943"
  },
  {
    "number": 478,
    "title": " typo in  sglang/srt/server.py  Runtime  log_evel",
    "body": "There is a typo in  sglang/srt/server.py  Runtime\r\n\r\n**log_evel** should be **log_level**\r\n\r\n`class Runtime:\r\n    def __init__(\r\n        self,\r\n        log_evel: str = \"error\",\r\n        model_overide_args: Optional[dict] = None,\r\n        *args,\r\n        **kwargs,\r\n    ):\r\n        \"\"\"See the arguments in server_args.py::ServerArgs\"\"\"\r\n        self.server_args = ServerArgs(*args, log_level=log_evel, **kwargs)\r\n`\r\n\r\nhttps://github.com/sgl-project/sglang/blob/main/python/sglang/srt/server.py#L254\r\nhttps://github.com/sgl-project/sglang/blob/main/python/sglang/srt/server.py#L260\r\n\r\n\r\nThis typo causes an error when using  fastchat.serve.sglang_worker\r\nsglang.srt.server_args.ServerArgs() got multiple values for keyword argument 'log_level'\r\n\r\n> python3 -m fastchat.serve.sglang_worker --model-path models/llava-v1.5-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --multimodal\r\n2024-05-26 14:51:19 | ERROR | stderr | Traceback (most recent call last):\r\n2024-05-26 14:51:19 | ERROR | stderr |   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n2024-05-26 14:51:19 | ERROR | stderr |     return _run_code(code, main_globals, None,\r\n2024-05-26 14:51:19 | ERROR | stderr |   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n2024-05-26 14:51:19 | ERROR | stderr |     exec(code, run_globals)\r\n2024-05-26 14:51:19 | ERROR | stderr |   File \"/root/.local/lib/python3.10/site-packages/fastchat/serve/sglang_worker.py\", line 290, in <module>\r\n2024-05-26 14:51:19 | ERROR | stderr |     runtime = sgl.Runtime(\r\n2024-05-26 14:51:19 | ERROR | stderr |   File \"/root/.local/lib/python3.10/site-packages/sglang/api.py\", line 39, in Runtime\r\n2024-05-26 14:51:19 | ERROR | stderr |     return Runtime(*args, **kwargs)\r\n2024-05-26 14:51:19 | ERROR | stderr |   File \"/root/.local/lib/python3.10/site-packages/sglang/srt/server.py\", line 246, in __init__\r\n2024-05-26 14:51:19 | ERROR | stderr |     self.server_args = ServerArgs(*args, log_level=log_evel, **kwargs)\r\n2024-05-26 14:51:19 | ERROR | stderr | TypeError: sglang.srt.server_args.ServerArgs() got multiple values for keyword argument 'log_level'\r\n2024-05-26 14:51:19 | ERROR | stderr | Exception ignored in: <function Runtime.__del__ at 0x705e40350b80>\r\n2024-05-26 14:51:19 | ERROR | stderr | Traceback (most recent call last):\r\n2024-05-26 14:51:19 | ERROR | stderr |   File \"/root/.local/lib/python3.10/site-packages/sglang/srt/server.py\", line 331, in __del__\r\n2024-05-26 14:51:19 | ERROR | stderr |     self.shutdown()\r\n2024-05-26 14:51:19 | ERROR | stderr |   File \"/root/.local/lib/python3.10/site-packages/sglang/srt/server.py\", line 284, in shutdown\r\n2024-05-26 14:51:19 | ERROR | stderr |     if self.pid is not None:\r\n2024-05-26 14:51:19 | ERROR | stderr | AttributeError: 'Runtime' object has no attribute 'pid'\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-05-26T19:55:11+00:00",
    "closed_at": "2024-07-01T17:46:20+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/478/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/478"
  },
  {
    "number": 6531,
    "title": "[Feature] Support `abort_request` in the router",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nHi,\n\nI'd like to use the new `abort_request` API: https://github.com/sgl-project/sglang/blob/58f10679e1850fdc86046057c23bac5193156de9/python/sglang/srt/entrypoints/http_server.py#L549\n\nHowever, I'm using multiple SGLang servers behind a router, and currently the router doesn't seem to support `abort_request`: https://github.com/sgl-project/sglang/blob/main/sgl-router/src/server.rs\n\nI'm wondering if it makes sense to also support `abort_request` in the router. Thanks!\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue"
    ],
    "state": "open",
    "created_at": "2025-05-22T16:22:35+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6531/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6531"
  },
  {
    "number": 1959,
    "title": "[Bug] Inference with RadixAttention\uff0cbut output weirdly",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI'm trying to adjust my own LLM inference code, I have replaced the self-attention with RadixAttention, and some necessary components. But I found that the output is weird:\r\n\r\n```python\r\n# prompt: \r\n\r\n\"which city is the capital of China?\"\r\n\r\n# output: \r\n\r\n\"\"\"\r\nijingBeijing is the capital of china.B. Beijing is the capital of china.\r\nWhich city is the capital of china.\r\nWhich city is the capital of china?\r\n\r\nBeijing is the capital of ChinaBeijing\r\n\"\"\"\r\n```\r\n\r\nIt really confused me, anyone can help me to figure out what's going on inside here. \ud83d\ude4f\n\n### Reproduction\n\nMy model is official weight from [meta-llama2-7b-chat](https://github.com/meta-llama/llama), and my inference code is based on [pytorch-llama](https://github.com/hkproj/pytorch-llama).\n\n### Environment\n\n```bash\r\nPython: 3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0,1: NVIDIA GeForce RTX 3090\r\nGPU 0,1 Compute Capability: 8.6\r\nCUDA_HOME: /usr/local/cuda-11.8\r\nNVCC: Cuda compilation tools, release 11.8, V11.8.89\r\nCUDA Driver Version: 550.100\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.3.5\r\nflashinfer: 0.1.6+cu118torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.46.2\r\nrequests: 2.32.3\r\ntqdm: 4.67.0\r\nnumpy: 1.26.4\r\naiohttp: 3.10.10\r\nfastapi: 0.115.4\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.26.2\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.1.0\r\npydantic: 2.9.2\r\nuvicorn: 0.32.0\r\nuvloop: 0.21.0\r\nzmq: 26.2.0\r\nvllm: 0.6.3.post1\r\nmultipart: 0.0.17\r\nopenai: 1.54.3\r\nanthropic: 0.39.0\r\nNVIDIA Topology: \r\n        GPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NODE    0-7,16-23       0               N/A\r\nGPU1    NODE     X      0-7,16-23       0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nulimit soft: 4096\r\n\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-11-08T09:32:46+00:00",
    "closed_at": "2024-11-08T09:33:17+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1959/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1959"
  },
  {
    "number": 2272,
    "title": "[Kernel] cuDNN attention backend",
    "body": "cuDNN provides very fast attention implementation and it is well maintained by NVIDIA. We would like to add a new attention backend based on cudnn.  \r\n\r\n## Steps\r\n1. Learn this cudnn paged attention python api. https://github.com/NVIDIA/cudnn-frontend/blob/v1.8.0/samples/python/52_scaled_dot_product_attention_with_paged_caches.ipynb\r\n2. Add a new attention backend \"cudnn\" here https://github.com/sgl-project/sglang/tree/main/python/sglang/srt/layers/attention\r\n3. We should be able to use it with `python3 -m sglang.launch_server --model meta-llama/Llama-3.1-8B-Instruct --attention-backend cudnn`",
    "labels": [
      "enhancement",
      "good first issue",
      "help wanted",
      "high priority",
      "inactive"
    ],
    "state": "open",
    "created_at": "2024-11-30T06:36:16+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2272/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2272"
  },
  {
    "number": 74,
    "title": "Continues batch technical for different length prompt",
    "body": "Suppose model's max-model-length is 8096, and there is two requests, one prompt length is 8, another is 10. And how concat them, pad them to 8096 or pad them to 10 or truncate them to 8.\r\nI feel you will pad them to 16. But I don't seek the code. Can you tell me the location?\r\nThanks.",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-22T08:45:37+00:00",
    "closed_at": "2024-01-23T04:40:05+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/74/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/74"
  },
  {
    "number": 5980,
    "title": "[Bug] Docker image (>=0.4.6.post1) suboptimal performance on H100 multi-node due to Incompatible NCCL",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nIn `0.4.6.post1` and `0.4.6.post2` docker images, `nvidia-nccl-cu12` package is manually upgraded to 2.26.2: \n\n![Image](https://github.com/user-attachments/assets/c2a0343a-9a6a-4ee3-96b5-9e52c6e147de)\n\nref: https://hub.docker.com/layers/lmsysorg/sglang/v0.4.6.post2-cu124/images/sha256-b889741508e27fadd4000c70cb6b3f4612640a48a969ef0bc0fec4117588755d\n\nThis change was introduced in https://github.com/sgl-project/sglang/pull/5894/files\n\nI noticed a significant performance drop in the latest 0.4.6.post2 image on my H100*8 *2, 400G IB environment, compared to the 0.4.6 image I test on last weekend. Also, manually upgrade via `git pull && pip install -e python[all]` won't meet this issue.\n\nWhen I tried to find the root cause, I noticed pip warned the version is incompatible.\n\n```\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorch 2.6.0+cu124 requires nvidia-nccl-cu12==2.21.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nccl-cu12 2.26.2.post1 which is incompatible.\n```\n\n### Reproduction\n\nI'm using LWS in K8s, \n\n```\ncontainers:\n  - name: sglang-leader\n    image: lmsysorg/sglang:v0.4.6.post2-cu124\n    env:\n      - name: LWS_WORKER_INDEX\n        valueFrom:\n          fieldRef:\n            fieldPath: metadata.labels['leaderworkerset.sigs.k8s.io/worker-index']\n    command:\n      - /bin/sh\n      - -c\n      - |\n          # pip install nvidia-nccl-cu12==2.21.5 \n          python3 -m sglang.launch_server --host 0.0.0.0 --port 8888 --trust-remote-code --show-time-cost \\\n              --model-path /tmp/scratch-space/DeepSeek-V3-0324 --tp 16 \\\n              --dist-init-addr $(LWS_LEADER_ADDRESS):5000 --nnodes $(LWS_GROUP_SIZE) --node-rank $(LWS_WORKER_INDEX) \\\n              --speculative-algo EAGLE --speculative-eagle-topk 1 --speculative-num-steps 3 --speculative-num-draft-tokens 4 \\\n              --cuda-graph-max-bs 1 \n```\n\nI started two different LWS with the `pip install nvidia-nccl-cu12==2.21.5` command as the only difference.\n\nOr, can use `git pull && pip install -e python[all]` from 0.4.6 image, the performance is also normal.\n\nBench with: `python3 -m sglang.bench_serving --backend sglang --num-prompts 10 --dataset-name random --random-input 1024 --random-output 4096 --max-concurrency 1 --port 8888`\n\nWith nccl 2.21.5:\n```\n============ Serving Benchmark Result ============\nBackend:                                 sglang    \nTraffic request rate:                    inf       \nMax reqeuest concurrency:                1         \nSuccessful requests:                     10        \nBenchmark duration (s):                  194.49    \nTotal input tokens:                      6101      \nTotal generated tokens:                  20087     \nTotal generated tokens (retokenized):    19972     \nRequest throughput (req/s):              0.05      \nInput token throughput (tok/s):          31.37     \nOutput token throughput (tok/s):         103.28    \nTotal token throughput (tok/s):          134.65    \nConcurrency:                             1.00      \nAccept length:                           2.83      \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   19445.69  \nMedian E2E Latency (ms):                 24543.22  \n---------------Time to First Token----------------\nMean TTFT (ms):                          276.51    \nMedian TTFT (ms):                        263.68    \nP99 TTFT (ms):                           586.01    \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           9.55      \nMedian ITL (ms):                         8.42      \nP95 ITL (ms):                            14.76     \nP99 ITL (ms):                            26.79     \nMax ITL (ms):                            34.22     \n==================================================\n```\n\nWith nccl 2.26.2.post1:\n\n```\n============ Serving Benchmark Result ============\nBackend:                                 sglang    \nTraffic request rate:                    inf       \nMax reqeuest concurrency:                1         \nSuccessful requests:                     10        \nBenchmark duration (s):                  357.28    \nTotal input tokens:                      6101      \nTotal generated tokens:                  20087     \nTotal generated tokens (retokenized):    19986     \nRequest throughput (req/s):              0.03      \nInput token throughput (tok/s):          17.08     \nOutput token throughput (tok/s):         56.22     \nTotal token throughput (tok/s):          73.30     \nConcurrency:                             1.00      \nAccept length:                           2.90      \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   35724.27  \nMedian E2E Latency (ms):                 44331.85  \n---------------Time to First Token----------------\nMean TTFT (ms):                          409.48    \nMedian TTFT (ms):                        359.05    \nP99 TTFT (ms):                           755.03    \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           17.59     \nMedian ITL (ms):                         16.63     \nP95 ITL (ms):                            26.11     \nP99 ITL (ms):                            50.43     \nMax ITL (ms):                            61.33     \n==================================================\n```\n\n\n### Environment\n\n```\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 535.161.07\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post2\nsgl_kernel: 0.1.1\nflashinfer_python: 0.2.5+cu124torch2.6\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.10.0\nnumpy: 2.2.5\naiohttp: 3.11.18\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.25.0\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.4\npython-multipart: 0.0.20\npyzmq: 26.4.0\nuvicorn: 0.34.2\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.17\nopenai: 1.76.2\ntiktoken: 0.9.0\nanthropic: 0.50.0\nlitellm: 1.67.5\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10   NIC11   NIC12   NIC13   NIC14   NIC15   NIC16   NIC17   NIC18   CPU Affinity    NUMA Affinity     GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     PIX     NODE    SYS     NODE    NODE    SYS     SYS     SYS     SYS     SYS     0-55,112-167    0N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     NODE    NODE    NODE    NODE    SYS     SYS     SYS     NODE    PIX     SYS     NODE    NODE    SYS     SYS     SYS     SYS     SYS     0-55,112-167    0N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    PIX     NODE    NODE    NODE    SYS     SYS     SYS     NODE    NODE    SYS     PIX     NODE    SYS     SYS     SYS     SYS     SYS     0-55,112-167    0N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    NODE    PIX     SYS     SYS     SYS     NODE    NODE    SYS     NODE    PIX     SYS     SYS     SYS     SYS     SYS     0-55,112-167    0N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     PIX     NODE    NODE    SYS     SYS     NODE    SYS     SYS     PIX     NODE    NODE    NODE    NODE    56-111,168-223  1N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     NODE    PIX     NODE    SYS     SYS     NODE    SYS     SYS     NODE    PIX     NODE    NODE    NODE    56-111,168-223  1N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    PIX     SYS     SYS     NODE    SYS     SYS     NODE    NODE    PIX     NODE    NODE    56-111,168-223  1N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    SYS     SYS     PIX     SYS     SYS     NODE    NODE    NODE    PIX     NODE    56-111,168-223  1N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS     PIX     NODE    SYS     NODE    NODE    SYS     SYS     SYS     SYS     SYS\nNIC1    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    NODE    NODE    SYS     SYS     SYS     NODE    PIX     SYS     NODE    NODE    SYS     SYS     SYS     SYS     SYS\nNIC2    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    NODE    NODE    SYS     SYS     SYS     NODE    NODE    SYS     PIX     NODE    SYS     SYS     SYS     SYS     SYS\nNIC3    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      PIX     NODE    SYS     SYS     SYS     NODE    NODE    SYS     NODE    NODE    SYS     SYS     SYS     SYS     SYS\nNIC4    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX      X      NODE    SYS     SYS     SYS     NODE    NODE    SYS     NODE    NODE    SYS     SYS     SYS     SYS     SYS\nNIC5    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE     X      SYS     SYS     SYS     NODE    NODE    SYS     NODE    PIX     SYS     SYS     SYS     SYS     SYS\nNIC6    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS      X      NODE    NODE    SYS     SYS     NODE    SYS     SYS     PIX     NODE    NODE    NODE    NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE     X      NODE    SYS     SYS     NODE    SYS     SYS     NODE    PIX     NODE    NODE    NODE\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      SYS     SYS     NODE    SYS     SYS     NODE    NODE    PIX     NODE    NODE\nNIC9    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    NODE    NODE    SYS     SYS     SYS      X      NODE    SYS     NODE    NODE    SYS     SYS     SYS     SYS     SYS\nNIC10   NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    NODE    NODE    SYS     SYS     SYS     NODE     X      SYS     NODE    NODE    SYS     SYS     SYS     SYS     SYS\nNIC11   SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    SYS     SYS      X      SYS     SYS     NODE    NODE    NODE    PIX     NODE\nNIC12   NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    NODE    NODE    SYS     SYS     SYS     NODE    NODE    SYS      X      NODE    SYS     SYS     SYS     SYS     SYS\nNIC13   NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    PIX     SYS     SYS     SYS     NODE    NODE    SYS     NODE     X      SYS     SYS     SYS     SYS     SYS\nNIC14   SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     PIX     NODE    NODE    SYS     SYS     NODE    SYS     SYS      X      NODE    NODE    NODE    NODE\nNIC15   SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE    PIX     NODE    SYS     SYS     NODE    SYS     SYS     NODE     X      NODE    NODE    NODE\nNIC16   SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    PIX     SYS     SYS     NODE    SYS     SYS     NODE    NODE     X      NODE    NODE\nNIC17   SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    SYS     SYS     PIX     SYS     SYS     NODE    NODE    NODE     X      NODE\nNIC18   SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    SYS     SYS     NODE    SYS     SYS     NODE    NODE    NODE    NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n  NIC10: mlx5_10\n  NIC11: mlx5_11\n  NIC12: mlx5_12\n  NIC13: mlx5_13\n  NIC14: mlx5_14\n  NIC15: mlx5_15\n  NIC16: mlx5_16\n  NIC17: mlx5_17\n  NIC18: mlx5_bond_0\n\n\nulimit soft: 1048576\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-02T14:59:44+00:00",
    "closed_at": "2025-05-03T15:10:02+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5980/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5980"
  },
  {
    "number": 6080,
    "title": "[Feature] mtp support dp-attention",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nmtp support dp-attention\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2025-05-07T07:58:04+00:00",
    "closed_at": "2025-06-17T07:33:29+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6080/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6080"
  },
  {
    "number": 4862,
    "title": "[Bug] Executing Qwen2.5-Omni-7B on SGLang 0.4.4 post2: AttributeError: 'Qwen2_5OmniConfig' object has no attribute 'hidden_size'",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHi,\n\nWhen I try to launch Omni 7B on the [SGLang runtime](https://github.com/didier-durand/llms-in-clouds/blob/main/docs/sglang.md) (working already fine for multiple other Qwen models that I tested), I get the error mentioned here above:\n\n`AttributeError: 'Qwen2_5OmniConfig' object has no attribute 'hidden_size'`\n\nI understand that it's probably some parameter(s) that I have to add / update in the model configuration files (like config.json) that I pulled from HF.\n\nBut I don't know which ones. Can somebody guide me?\n\nI see `\"hidden_size\": xxx` in several places )(1 example below) of config.json as its a multi-channel model. So, is it needed in another place(s)  of this file ?\n\n```\n{\n  \"architectures\": [\n    \"Qwen2_5OmniModel\"\n  ],\n  \"enable_audio_output\": true,\n  \"enable_talker\": true,\n  \"model_type\": \"qwen2_5_omni\",\n  \"talker_config\": {\n    \"_attn_implementation_autoset\": true,\n    \"_name_or_path\": \"Qwen2.5-Omni-7B/talker\",\n    \"architectures\": [\n      \"Qwen2OmniTalkerForConditionalGeneration\"\n    ],\n    \"attention_dropout\": 0.0,\n    \"audio_end_token_id\": 151648,\n    \"audio_start_token_id\": 151647,\n    \"audio_token_index\": 151646,\n    \"embedding_size\": 3584,\n    \"head_dim\": 128,\n    \"hidden_act\": \"silu\",\n    \"hidden_size\": 896,\n    \"image_token_index\": 151655,\n    \"init_std\": 0.02,\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 18944,\n    \"max_position_embeddings\": 32768,\n    \"max_window_layers\": 28,\n    \"model_type\": \"qwen2_5_omni_talker\",\n    \"num_attention_heads\": 12,\n    \"num_hidden_layers\": 24,\n    \"num_key_value_heads\": 4,\n    \"position_id_per_seconds\": 25,\n    \"rms_norm_eps\": 1e-06,\n    \"rope_scaling\": {\n      \"mrope_section\": [\n        16,\n        24,\n        24\n      ],\n      \"rope_type\": \"default\",\n      \"type\": \"default\"\n    },\n```\n\nThanks!\n\nDidier\n\n```\n### starting SGLang ...\nsgl start command: python3.12 -m sglang.launch_server   --model Qwen/Qwen2.5-Omni-7B --model-path /home/model/Qwen/Qwen2.5-Omni-7B   --host 0.0.0.0 --port 30000 --tensor-parallel-size 4   --log-level info   --enable-metrics --trust-remote-code --enable-p2p-check\n[2025-03-28 10:20:10] server_args=ServerArgs(model_path='/home/model/Qwen/Qwen2.5-Omni-7B', tokenizer_path='/home/model/Qwen/Qwen2.5-Omni-7B', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=True, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='/home/model/Qwen/Qwen2.5-Omni-7B', chat_template=None, completion_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=30000, mem_fraction_static=0.85, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=4, stream_interval=1, stream_output=False, random_seed=94444126, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=True, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=5, speculative_eagle_topk=4, speculative_num_draft_tokens=8, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_deepep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=80, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=True, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, enable_flashinfer_mla=False, enable_flashmla=False, flashinfer_mla_disable_ragged=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_bootstrap_port=8998)\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.12/site-packages/sglang/launch_server.py\", line 14, in <module>\n    launch_server(server_args)\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/entrypoints/http_server.py\", line 673, in launch_server\n    tokenizer_manager, scheduler_info = _launch_subprocesses(server_args=server_args)\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/entrypoints/engine.py\", line 546, in _launch_subprocesses\n    tokenizer_manager = TokenizerManager(server_args, port_args)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/managers/tokenizer_manager.py\", line 159, in __init__\n    self.model_config = ModelConfig(\n                        ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/sglang/srt/configs/model_config.py\", line 111, in __init__\n    self.hf_text_config.hidden_size // self.hf_text_config.num_attention_heads,\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/transformers/configuration_utils.py\", line 214, in __getattribute__\n    return super().__getattribute__(key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'Qwen2_5OmniConfig' object has no attribute 'hidden_size'\n```\n\n### Reproduction\n\n\n- Deploy https://huggingface.co/Qwen/Qwen2.5-Omni-7B from your own install or from my Docker image described below\n-  Start the model and the message above will show up\n\n### Environment\n\n- Redhat 9 in containerized image described here: https://github.com/didier-durand/llms-in-clouds/blob/main/docs/sglang.md\n- The image is  I use is `redhat9-latest`from https://hub.docker.com/repository/docker/didierdurand/lic-sglang/general ",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-28T15:00:39+00:00",
    "closed_at": "2025-05-30T08:43:41+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4862/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4862"
  },
  {
    "number": 3201,
    "title": "[Bug] min_p_sampling_from_probs() related crashes",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nsglang srt crashes with log:\n```\n[2025-01-28 17:58:17 TP0] TpModelWorkerClient hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 109, in forward_thread_func\n    self.forward_thread_func_()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 140, in forward_thread_func_\n    logits_output, next_token_ids = self.worker.forward_batch_generation(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 171, in forward_batch_generation\n    next_token_ids = self.model_runner.sample(logits_output, model_worker_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 808, in sample\n    next_token_ids = self.sampler(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/sampler.py\", line 86, in forward\n    batch_next_token_ids, success = min_p_sampling_from_probs(\nValueError: not enough values to unpack (expected 2, got 1)\n```\n\nIn the code, `min_p_sampling_from_probs()` indeed only returns 1 value:\nhttps://github.com/sgl-project/sglang/blob/9f635ea50de920aa507f486daafba26a5b837574/sgl-kernel/src/sgl-kernel/ops/__init__.py#L455 \n\n### Reproduction\n\nThe model is Deepseek-R1. Repro request to be added.\n\n### Environment\n\nLatest [v0.4.2-cu124-srt](https://hub.docker.com/layers/lmsysorg/sglang/v0.4.2-cu124-srt/images/sha256-abf11f966511bbf2cb5aa91c1b809ca71d0e94d6024e6b1cc4317b0429d1ed80) docker image is used.",
    "labels": [],
    "state": "closed",
    "created_at": "2025-01-29T02:46:51+00:00",
    "closed_at": "2025-02-02T19:50:45+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3201/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3201"
  },
  {
    "number": 5186,
    "title": "[Bug] DeepEP Low Latency failed on 2 node (8*H20)",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWe observe successful single-node (8\u00d7H20) deployment of DeepSeek-R1 with SGLang+DeepEP (deep-ep-mode=auto), but encounter failures in 2-node configuration with the following error:\n\n```python\n[2025-04-09 10:35:31 TP2] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py\", line 1999, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py\", line 249, in __init__\n    self.tp_worker = TpWorkerClass(\n                     ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 74, in __init__\n    self.model_runner = ModelRunner(\n                        ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 178, in __init__\n    self.initialize(min_per_gpu_memory)\n  File \"/usr/local/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 188, in initialize\n    self.load_model()\n  File \"/usr/local/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 400, in load_model\n    self.model = get_model(\n                 ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/sglang/srt/model_loader/__init__.py\", line 22, in get_model\n    return loader.load_model(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/sglang/srt/model_loader/loader.py\", line 365, in load_model\n    model = _initialize_model(\n            ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/sglang/srt/model_loader/loader.py\", line 146, in _initialize_model\n    return model_class(\n           ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/sglang/srt/models/deepseek_v2.py\", line 1352, in __init__\n    self.model = DeepseekV2Model(\n                 ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/sglang/srt/models/deepseek_v2.py\", line 1278, in __init__\n    [\n  File \"/usr/local/lib/python3.11/site-packages/sglang/srt/models/deepseek_v2.py\", line 1279, in <listcomp>\n    DeepseekV2DecoderLayer(\n  File \"/usr/local/lib/python3.11/site-packages/sglang/srt/models/deepseek_v2.py\", line 1077, in __init__\n    self.mlp = DeepseekV2MoE(\n               ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/sglang/srt/models/deepseek_v2.py\", line 259, in __init__\n    self.deepep_dispatcher = DeepEPDispatcher(\n                             ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\", line 537, in __init__\n    self._low_latency_dispatcher = _DeepEPDispatcherImplLowLatency(\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\", line 350, in __init__\n    self.buffer_low_latency = _get_buffer_low_latency(\n                              ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\", line 79, in _get_buffer_low_latency\n    _buffer_low_latency = Buffer(\n                          ^^^^^^^\n  File \"/usr/local/lib64/python3.11/site-packages/deep_ep/buffer.py\", line 89, in __init__\n    self.runtime.sync(device_ids, ipc_handles, root_unique_id)\nRuntimeError: Failed: Assertion error /home/moyun.zty/DeepEP/csrc/kernels/runtime.cu:56 'nvshmem_team_split_strided(NVSHMEM_TEAM_WORLD, rank % NUM_MAX_NVL_PEERS, NUM_MAX_NVL_PEERS, num_ranks / NUM_MAX_NVL_PEERS, &cpu_rdma_team_config, 0, &cpu_rdma_team) == 0'\n```\n\nFollowed `WARN` logs:\n```python\nWARN: Invalid <start, stride, size>: child <6, 8, 2>, parent <0, 1, 2>\nWARN: Invalid <start, stride, size>: child <4, 8, 2>, parent <0, 1, 2>\nWARN: Invalid <start, stride, size>: child <5, 8, 2>, parent <0, 1, 2>\nWARN: Invalid <start, stride, size>: child <7, 8, 2>, parent <0, 1, 2>\nWARN: Starting PE (1) or ending PE (9) is invalid\nWARN: Invalid <start, stride, size>: child <3, 8, 2>, parent <0, 1, 2>\nWARN: Starting PE (0) or ending PE (8) is invalid\nWARN: Invalid <start, stride, size>: child <2, 8, 2>, parent <0, 1, 2>\n```\n\n### Reproduction\n```bash\n# node 0\nNCCL_IB_GID_INDEX=3 SUPPORT_CUTLASS_BLOCK_FP8=1 python -m sglang.launch_server \\\n--model-path /path/to/deepseek-ai__DeepSeek-R1 \\\n--tp-size 16 \\\n--dist-init-addr ${master_ip}:20001 \\\n--nnodes 2 \\\n--node-rank 0 \\\n--enable-ep-moe \\\n--enable-deepep-moe \\\n--deepep-mode auto \\\n--disable-cuda-graph \\\n--trust-remote-code \\\n--quantization fp8 \\\n--log-level info \\\n--chunked-prefill-size -1 \\\n--disable-radix-cache \\\n--context-length 65535 \\\n--max-running-requests 128 \\\n--stream-output \\\n> sglang-deep-ep-auto.log 2>&1 &\n\n# node 1\nNCCL_IB_GID_INDEX=3 SUPPORT_CUTLASS_BLOCK_FP8=1 python -m sglang.launch_server \\\n--model-path /path/to/deepseek-ai__DeepSeek-R1 \\\n--tp-size 16 \\\n--dist-init-addr ${master_ip}:20001 \\\n--nnodes 2 \\\n--node-rank 1 \\\n--enable-ep-moe \\\n--enable-deepep-moe \\\n--deepep-mode auto \\\n--disable-cuda-graph \\\n--trust-remote-code \\\n--quantization fp8 \\\n--log-level info \\\n--chunked-prefill-size -1 \\\n--disable-radix-cache \\\n--context-length 65535 \\\n--max-running-requests 128 \\\n--stream-output \\\n> sglang-deep-ep-auto.log 2>&1 &\n```\n\n### Environment\n```bash\n$ python3.11 -m sglang.check_env\n\nPython: 3.11.11 (main, Dec 16 2024, 17:23:09) [GCC 10.2.1 20200825 (Alibaba 10.2.1-3.8 2.32)]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H20\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.99\nCUDA Driver Version: 550.127.08\nPyTorch: 2.6.0+cu124\nsglang: 0.4.5\nsgl_kernel: 0.0.6\nflashinfer: Module Not Found\ntriton: 3.2.0\ntransformers: 4.50.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.16\nfastapi: 0.115.12\nhf_transfer: Module Not Found\nhuggingface_hub: 0.30.1\ninteregular: 0.3.3\nmodelscope: Module Not Found\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.11.1\nmultipart: Module Not Found\nzmq: 0.0.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.8.2\nxgrammar: 0.1.16\nopenai: 1.70.0\ntiktoken: 0.9.0\nanthropic: Module Not Found\nlitellm: Module Not Found\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity  GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    SYS     SYS     0-47,96-143     0     N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    SYS     SYS     0-47,96-143     0     N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    SYS     SYS     0-47,96-143     0     N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    PIX     SYS     SYS     0-47,96-143     0     N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     PIX     NODE    48-95,144-191   1     N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     NODE    NODE    48-95,144-191   1     N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     NODE    PIX     48-95,144-191   1     N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     NODE    NODE    48-95,144-191   1     N/A\nNIC0    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    SYS     SYS\nNIC1    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE     X      SYS     SYS\nNIC2    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS      X      NODE\nNIC3    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_bond_0\n  NIC1: mlx5_bond_1\n  NIC2: mlx5_bond_2\n  NIC3: mlx5_bond_3\n\n\nulimit soft: 204800\n```\n\n#### Machine Info\n\n![Image](https://github.com/user-attachments/assets/e5e412ae-a150-46ba-8cf1-326a0482256f)\n\n![Image](https://github.com/user-attachments/assets/256fe25c-3e0e-4339-acff-f4b15f0782ef)\n\n![Image](https://github.com/user-attachments/assets/0666d6b9-ae0d-4463-8068-70fc2fa6a4ff)\n\n#### Environment Variables\n\n```bash\nexport NCCL_IB_DISABLE=0\nexport NCCL_SOCKET_IFNAME=bond0\nexport NCCL_NET_PLUGIN=''\nexport NCCL_IB_GID_INDEX=3\nexport NCCL_IB_TIMEOUT=22\nexport NCCL_IB_RETRY_CNT=7\nexport NCCL_IB_SL=5\nexport NCCL_IB_TC=136\nexport NCCL_IB_HCA=\"mlx5_bond_0,mlx5_bond_1,mlx5_bond_2,mlx5_bond_3\"\nexport NCCL_DEBUG=WARN\nexport NCCL_SET_THREAD_NAME=1\nexport NCCL_IB_QPS_PER_CONNECTION=8\nexport NCCL_SET_THREAD_NAME=1\nexport NCCL_DEBUG_SUBSYS=INIT,TUNING,GRAPH\n\nexport NVSHMEM_DEBUG=WARN\nexport NVSHMEM_ENABLE_NIC_PE_MAPPING=1\nexport NVSHMEM_HCA_PE_MAPPING=\"mlx5_bond_0:1:2,mlx5_bond_1:1:2,mlx5_bond_2:1:2,mlx5_bond_3:1:2\"\nexport NVSHMEM_IBRC_ROCE_LAG_PORT_SELECTION=3\nexport NVSHMEM_IB_GID_INDEX=3\nexport NVSHMEM_IB_TRAFFIC_CLASS=64\n\nexport GLOO_SOCKET_IFNAME=eth0\n```\n#### Gdrcopy Info\n\n```bash\n$ LD_LIBRARY_PATH=/opt/gdrcopy/lib /opt/gdrcopy/bin/gdrcopy_copybw\nGPU id:0; name: NVIDIA H20; Bus id: 0000:08:00\nGPU id:1; name: NVIDIA H20; Bus id: 0000:7e:00\nGPU id:2; name: NVIDIA H20; Bus id: 0000:a2:00\nGPU id:3; name: NVIDIA H20; Bus id: 0000:c6:00\nGPU id:4; name: NVIDIA H20; Bus id: 0001:09:00\nGPU id:5; name: NVIDIA H20; Bus id: 0001:7f:00\nGPU id:6; name: NVIDIA H20; Bus id: 0001:a3:00\nGPU id:7; name: NVIDIA H20; Bus id: 0001:c7:00\nselecting device 0\ntesting size: 131072\nrounded size: 131072\ngpu alloc fn: cuMemAlloc\ndevice ptr: 7f825d400000\nmap_d_ptr: 0x7f848c0a6000\ninfo.va: 7f825d400000\ninfo.mapped_size: 131072\ninfo.page_size: 65536\ninfo.mapped: 1\ninfo.wc_mapping: 1\npage offset: 0\nuser-space pointer:0x7f848c0a6000\nwriting test, size=131072 offset=0 num_iters=10000\nwrite BW: 17993.9MB/s\nreading test, size=131072 offset=0 num_iters=100\nread BW: 716.355MB/s\nunmapping buffer\nunpinning buffer\nclosing gdrdrv\n```\n\n#### Nvshmem Info\n\n```bash\n$ LD_LIBRARY_PATH=/opt/nvshmem/lib/ /opt/nvshmem/bin/nvshmem-info -a\nNVSHMEM v3.2.5\n\nBuild Information:\n  CUDA API                     12040\n  CUDA Driver                  12040\n  Build Timestamp              Apr  2 2025 14:47:31\n  Build Variables             \n        NVSHMEM_DEBUG=OFF NVSHMEM_DEVEL=OFF NVSHMEM_DEFAULT_PMI2=OFF\n        NVSHMEM_DEFAULT_PMIX=OFF NVSHMEM_DEFAULT_UCX=OFF\n        NVSHMEM_ENABLE_ALL_DEVICE_INLINING=OFF NVSHMEM_GPU_COLL_USE_LDST=OFF\n        NVSHMEM_IBGDA_SUPPORT=ON NVSHMEM_IBGDA_SUPPORT_GPUMEM_ONLY=OFF\n        NVSHMEM_IBDEVX_SUPPORT=OFF NVSHMEM_IBRC_SUPPORT=ON\n        NVSHMEM_MPI_SUPPORT=OFF NVSHMEM_NVTX=ON NVSHMEM_PMIX_SUPPORT=OFF\n        NVSHMEM_SHMEM_SUPPORT=OFF NVSHMEM_TEST_STATIC_LIB=OFF\n        NVSHMEM_TIMEOUT_DEVICE_POLLING=OFF NVSHMEM_TRACE=OFF NVSHMEM_UCX_SUPPORT=OFF\n        NVSHMEM_USE_DLMALLOC=OFF NVSHMEM_USE_NCCL=OFF NVSHMEM_USE_GDRCOPY=ON\n        NVSHMEM_VERBOSE=OFF CUDA_HOME=/usr/local/cuda GDRCOPY_HOME=/opt/gdrcopy\n        LIBFABRIC_HOME=/usr/local/libfabric MPI_HOME=/usr/local/ompi\n        NCCL_HOME=/usr/local/nccl NVSHMEM_PREFIX=/usr/local/nvshmem PMIX_HOME=/usr\n        SHMEM_HOME=/usr/local/ompi UCX_HOME=/usr/local/ucx\n\nStandard options:\n  NVSHMEM_VERSION              false (type: bool, default: false)\n        Print library version at startup\n  NVSHMEM_INFO                 false (type: bool, default: false)\n        Print environment variable options at startup\n  NVSHMEM_DISABLE_NVLS         false (type: bool, default: false)\n        Disable NVLS SHARP resources for collectives, even if available for platform\n  NVSHMEM_SYMMETRIC_SIZE       1073741824 (type: size, default: 1073741824)\n        Specifies the size (in bytes) of the symmetric heap memory per PE. The\n        size is implementation-defined and must be at least as large as the integer\n        ceiling of the product of the numeric prefix and the scaling factor. The\n        character suffixes for the scaling factor are as follows:\n\n          *  k or K multiplies by 2^10 (kibibytes)\n          *  m or M multiplies by 2^20 (mebibytes)\n          *  g or G multiplies by 2^30 (gibibytes)\n          *  t or T multiplies by 2^40 (tebibytes)\n\n        For example, string '20m' is equivalent to the integer value 20971520, or 20\n        mebibytes. Similarly the string '3.1M' is equivalent to the integer value\n        3250586. Only one multiplier is recognized and any characters following the\n        multiplier are ignored, so '20kk' will not produce the same result as '20m'.\n        Usage of string '.5m' will yield the same result as the string '0.5m'.\n        An invalid value for NVSHMEM_SYMMETRIC_SIZE is an error, which the NVSHMEM\n        library shall report by either returning a nonzero value from\n        nvshmem_init_thread or causing program termination.\n  NVSHMEM_DEBUG                \"WARN\" (type: string, default: \"\")\n        Set to enable debugging messages.\n        Optional values: VERSION, WARN, INFO, ABORT, TRACE\n\nBootstrap options:\n  NVSHMEM_BOOTSTRAP            \"PMI\" (type: string, default: \"PMI\")\n        Name of the default bootstrap that should be used to initialize NVSHMEM.\n        Allowed values: PMI, MPI, SHMEM, plugin, UID\n  NVSHMEM_BOOTSTRAP_PMI        \"PMI\" (type: string, default: \"PMI\")\n        Name of the PMI bootstrap that should be used to initialize NVSHMEM.\n        Allowed values: PMI, PMI-2, PMIX\n  NVSHMEM_BOOTSTRAP_PLUGIN     \"\" (type: string, default: \"\")\n        Absolute path to or name of the bootstrap plugin file to load when\n        NVSHMEM_BOOTSTRAP=plugin is specified\n  NVSHMEM_BOOTSTRAP_MPI_PLUGIN \"nvshmem_bootstrap_mpi.so.3\" (type: string, default: \"nvshmem_bootstrap_mpi.so.3\")\n        Absolute path to or name of the MPI bootstrap plugin file. \n        NVSHMEM will search for the plugin based on linux linker priorities. See man\n        dlopen\n  NVSHMEM_BOOTSTRAP_SHMEM_PLUGIN \"nvshmem_bootstrap_shmem.so.3\" (type: string, default: \"nvshmem_bootstrap_shmem.so.3\")\n        Absolute path to or name of the SHMEM bootstrap plugin file. \n        NVSHMEM will search for the plugin based on linux linker priorities. See man\n        dlopen\n  NVSHMEM_BOOTSTRAP_PMI_PLUGIN \"nvshmem_bootstrap_pmi.so.3\" (type: string, default: \"nvshmem_bootstrap_pmi.so.3\")\n        Absolute path to or name of the PMI bootstrap plugin file. \n        NVSHMEM will search for the plugin based on linux linker priorities. See man\n        dlopen\n  NVSHMEM_BOOTSTRAP_PMI2_PLUGIN \"nvshmem_bootstrap_pmi2.so.3\" (type: string, default: \"nvshmem_bootstrap_pmi2.so.3\")\n        Absolute path to or name of the PMI-2 bootstrap plugin file. \n        NVSHMEM will search for the plugin based on linux linker priorities. See man\n        dlopen\n  NVSHMEM_BOOTSTRAP_PMIX_PLUGIN \"nvshmem_bootstrap_pmix.so.3\" (type: string, default: \"nvshmem_bootstrap_pmix.so.3\")\n        Absolute path to or name of the PMIx bootstrap plugin file. \n        NVSHMEM will search for the plugin based on linux linker priorities. See man\n        dlopen\n  NVSHMEM_BOOTSTRAP_UID_PLUGIN \"nvshmem_bootstrap_uid.so.3\" (type: string, default: \"nvshmem_bootstrap_uid.so.3\")\n        Absolute path to or name of the UID bootstrap plugin file. \n        NVSHMEM will search for the plugin based on linux linker priorities. See man\n        dlopen\n\nAdditional options:\n  NVSHMEM_CUDA_PATH            \"\" (type: string, default: \"\")\n        Path to directory containing libcuda.so (for use when not in default location)\n  NVSHMEM_DEBUG_ATTACH_DELAY   0 (type: int, default: 0)\n        Delay (in seconds) during the first call to NVSHMEM_INIT to allow for attaching\n        a debuggger (Default 0)\n  NVSHMEM_DEBUG_FILE           \"\" (type: string, default: \"\")\n        Debugging output filename, may contain %h for hostname and %p for pid\n  NVSHMEM_MAX_TEAMS            32 (type: long, default: 32)\n        Maximum number of simultaneous teams allowed\n  NVSHMEM_MAX_MEMORY_PER_GPU   137438953472 (type: size, default: 137438953472)\n        Maximum memory per GPU\n  NVSHMEM_DISABLE_CUDA_VMM     false (type: bool, default: false)\n        Disable use of CUDA VMM for P2P memory mapping. By default, CUDA VMM is enabled\n        on x86 and disabled on P9. CUDA VMM feature in NVSHMEM requires CUDA RT version\n        and CUDA Driver version to be greater than or equal to 11.3.\n  NVSHMEM_DISABLE_P2P          false (type: bool, default: false)\n        Disable P2P connectivity of GPUs even when available\n  NVSHMEM_IGNORE_CUDA_MPS_ACTIVE_THREAD_PERCENTAGE false (type: bool, default: false)\n        When doing Multi-Process Per GPU (MPG) run, full API support is available only\n        if sum of CUDA_MPS_ACTIVE_THREAD_PERCENTAGE of processes running on a GPU is <=\n        100%. Through this variable, user can request NVSHMEM runtime to ignore the\n        active thread percentage and allow full MPG support. Users enable it at their\n        own risk as NVSHMEM might deadlock.\n  NVSHMEM_CUMEM_GRANULARITY    536870912 (type: size, default: 536870912)\n        Granularity for cuMemAlloc/cuMemCreate\n  NVSHMEM_PROXY_REQUEST_BATCH_MAX 32 (type: int, default: 32)\n        Maxmum number of requests that the proxy thread processes in a single iteration\n        of the progress loop.\n\nCollectives options:\n  NVSHMEM_DISABLE_NCCL         false (type: bool, default: false)\n        Disable use of NCCL for collective operations\n  NVSHMEM_BARRIER_DISSEM_KVAL  2 (type: int, default: 2)\n        Radix of the dissemination algorithm used for barriers\n  NVSHMEM_BARRIER_TG_DISSEM_KVAL 2 (type: int, default: 2)\n        Radix of the dissemination algorithm used for thread group barriers\n  NVSHMEM_FCOLLECT_LL_THRESHOLD 2048 (type: size, default: 2048)\n        Message size threshold up to which fcollect LL algo will be used\n\n  NVSHMEM_REDUCE_SCRATCH_SIZE  524288 (type: size, default: 524288)\n        Amount of symmetric heap memory (minimum 16B, multiple of 8B) reserved by\n        runtime for every team to implement reduce and reducescatter collectives\n\n  NVSHMEM_BCAST_ALGO           0 (type: int, default: 0)\n        Broadcast algorithm to be used.\n          * 0 - use default algorithm selection strategy\n\n  NVSHMEM_REDMAXLOC_ALGO       1 (type: int, default: 1)\n        Reduction algorithm to be used for MAXLOC operation.\n          * 1 - default, flag alltoall algorithm\n          * 2 - flat reduce + flat bcast\n          * 3 - topo-aware two-level reduce + topo-aware bcast\n\n\nTransport options:\n  NVSHMEM_REMOTE_TRANSPORT     \"ibrc\" (type: string, default: \"ibrc\")\n        Selected transport for remote operations: ibrc, ucx, libfabric, ibdevx, none\n  NVSHMEM_ENABLE_NIC_PE_MAPPING true (type: bool, default: false)\n        When not set or set to 0, a PE is assigned the NIC on the node that is closest\n        to it by distance. When set to 1, NVSHMEM either assigns NICs to PEs on a\n        round-robin basis or uses NVSHMEM_HCA_PE_MAPPING or NVSHMEM_HCA_LIST when they\n        are specified.\n  NVSHMEM_DISABLE_LOCAL_ONLY_PROXY false (type: bool, default: false)\n        When running on an NVLink-only configuaration (No-IB, No-UCX), completely\n        disable the proxy thread. This will disable device side global exit and device\n        side wait timeout polling (enabled by NVSHMEM_TIMEOUT_DEVICE_POLLING build-time\n        variable) because these are processed by the proxy thread.\n  NVSHMEM_IB_ENABLE_IBGDA      false (type: bool, default: false)\n        Set to enable GPU-initiated communication transport.\n\nHidden options:\n  NVSHMEM_INFO_HIDDEN          true (type: bool, default: false)\n        Print hidden environment variable options at startup\n  NVSHMEM_DISABLE_NVLS_SHARING true (type: bool, default: true)\n        Disable NVLS SHARP resource sharing for user-defined teams\n  NVSHMEM_HEAP_KIND            \"DEVICE\" (type: string, default: \"DEVICE\")\n        Specify the memory kind used by the NVSHMEM symmetric heap.\n        Allowed values: VIDMEM, SYSMEM\n  NVSHMEM_ENABLE_RAIL_OPT      false (type: bool, default: false)\n        Enable Rail Optimization when heap is in SYSMEM\n  NVSHMEM_BOOTSTRAP_TWO_STAGE  false (type: bool, default: false)\n        Ignore CUDA device setting during initialization,forcing two-stage\n        initialization\n  NVSHMEM_DEBUG_SUBSYS         \"\" (type: string, default: \"\")\n        Comma separated list of debugging message sources. Prefix with '^' to exclude.\n        Values: INIT, COLL, P2P, PROXY, TRANSPORT, MEM, BOOTSTRAP, TOPO, UTIL, ALL\n  NVSHMEM_ENABLE_ERROR_CHECKS  false (type: bool, default: false)\n        Enable error checks\n  NVSHMEM_DISABLE_MNNVL        false (type: bool, default: false)\n        Disable MNNVL connectivity for GPUs even when available\n  NVSHMEM_CUMEM_HANDLE_TYPE    \"FILE_DESCRIPTOR\" (type: string, default: \"FILE_DESCRIPTOR\")\n        Handle type for cuMemCreate. Supported are - FABRIC or FILE_DESCRIPTOR\n  NVSHMEM_BYPASS_ACCESSIBILITY_CHECK false (type: bool, default: false)\n        Bypass peer GPU accessbility checks\n  NVSHMEM_FCOLLECT_NTHREADS    512 (type: int, default: 512)\n        Sets number of threads per block for fcollect collective.\n        By default, if no env is set, default value is min(max_occupancy per CTA, msg\n        size per PE).\n        If env is specified, value overrides the default irrespective of max occupancy\n        per CTA\n\n  NVSHMEM_REDUCESCATTER_NTHREADS 512 (type: int, default: 512)\n        Sets number of threads per block for reducescatter collective.\n        By default, if no env is set, default value is min(max_occupancy per CTA, msg\n        size per PE).\n        If env is specified, value overrides the default irrespective of max occupancy\n        per CTA\n\n  NVSHMEM_MAX_CTAS             1 (type: int, default: 1)\n        Sets number of blocks per grid for host onstream collective.\n        By default, if no env is set, default value to 1 CTA\n        If env is specified, value overrides the default value\n\n  NVSHMEM_REDUCE_RECEXCH_KVAL  2 (type: int, default: 2)\n        Radix of the recursive exchange reduction algorithm\n  NVSHMEM_FCOLLECT_LL128_THRESHOLD 0 (type: size, default: 0)\n        Message size threshold up to which the fcollect LL128 algo will be used.\n        LL128 will be used only when FCOLLECT_LL_THRESHOLD < size\n  NVSHMEM_FCOLLECT_NVLS_THRESHOLD 16777216 (type: size, default: 16777216)\n        Message size threshold up to which fcollect NVLS algo will be used\n\n  NVSHMEM_REDUCESCATTER_NVLS_THRESHOLD 16777216 (type: size, default: 16777216)\n        Message size threshold up to which reducescatter NVLS algo will be used\n\n  NVSHMEM_BCAST_TREE_KVAL      2 (type: int, default: 2)\n        Radix of the broadcast tree algorithm\n  NVSHMEM_FCOLLECT_ALGO        0 (type: int, default: 0)\n        Fcollect algorithm to be used. \n          * 0 - use default algorithm selection strategy\n\n  NVSHMEM_REDUCE_ALGO          0 (type: int, default: 0)\n        Allreduce algorithm to be used. \n           * 0/1 - use default algorithm selection strategy\n\n  NVSHMEM_REDUCE_NVLS_THRESHOLD 2048 (type: int, default: 2048)\n        Message size threshold up to which allreduce one-shot algo will be used\n\n  NVSHMEM_REDUCESCATTER_ALGO   0 (type: int, default: 0)\n        Reduce Scatter algorithm to be used. \n          * 0 - use default algorithm selection strategy\n\n  NVSHMEM_ASSERT_ATOMICS_SYNC  false (type: bool, default: false)\n        Bypass flush on wait_until at target\n  NVSHMEM_BYPASS_FLUSH         false (type: bool, default: false)\n        Bypass flush in proxy when enforcing consistency\n\nNVTX options:\n  NVSHMEM_NVTX                 \"off\" (type: string, default: \"off\")\n        Set to enable NVTX instrumentation. Accepts a comma separated list of\n        instrumentation groups. By default the NVTX instrumentation is disabled.\n          init                : library setup\n          alloc               : memory management\n          launch              : kernel launch routines\n          coll                : collective communications\n          wait                : blocking point-to-point synchronization\n          wait_on_stream      : point-to-point synchronization (on stream)\n          test                : non-blocking point-to-point synchronization\n          memorder            : memory ordering (quiet, fence)\n          quiet_on_stream     : nvshmemx_quiet_on_stream\n          atomic_fetch        : fetching atomic memory operations\n          atomic_set          : non-fetchong atomic memory operations\n          rma_blocking        : blocking remote memory access operations\n          rma_nonblocking     : non-blocking remote memory access operations\n          proxy               : activity of the proxy thread\n          common              : init,alloc,launch,coll,memorder,wait,atomic_fetch,rma_blocking,proxy\n          all                 : all groups\n          off                 : disable all NVTX instrumentation\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-04-09T07:28:09+00:00",
    "closed_at": "2025-04-10T02:05:04+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5186/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5186"
  },
  {
    "number": 3365,
    "title": "[Feature] support `gather` instead of `all_gather` when gathering the logits",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nWe noticed that in the `_get_logits` function of vllm, `gather` instead of `all_gather` will be used under certain conditions (the main condition is that for non-tpu devices):\nCode link:\n\n- [logits = tensor_model_parallel_gather(logits)](https://github.com/vllm-project/vllm/blob/6e1fc61f0fb90c37f0d4a1a8f76235a6e4e1103c/vllm/model_executor/layers/logits_processor.py#L101C22-L101C50)\n\n- [condition of whether using `all_gather` or `gather`](https://github.com/vllm-project/vllm/blob/6e1fc61f0fb90c37f0d4a1a8f76235a6e4e1103c/vllm/model_executor/layers/logits_processor.py#L53-L57)\n\nThe change from using `all_gather` to `gather` is initially added in this PR for your reference: https://github.com/vllm-project/vllm/pull/2221.\n\nWhile in SGLang, we see currently `all_gather` is always used:\nhttps://github.com/sgl-project/sglang/blob/e868d0b60eb2d435c5599165f787bca06bdc9c3d/python/sglang/srt/layers/logits_processor.py#L246\n\nDoes SGLang have the plan to add `gather` instead of only `all_gather` when gathering the logits? Per the practice in vllm, using `gather` seems to have better performance than `all_gather` on devices which have `gather` support.\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "help wanted"
    ],
    "state": "open",
    "created_at": "2025-02-07T07:14:12+00:00",
    "closed_at": null,
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3365/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3365"
  },
  {
    "number": 107,
    "title": "Mistral model no longer loads following PR#101",
    "body": "The `get_model_cls_by_arch_name` introduced in [Dynamic model class loading PR](https://github.com/sgl-project/sglang/pull/101) removes the hard-coded mapping between `MistralForCausalLM` and `LlamaForCausalLM` causing issues trying to local host [Mistral-7b](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) model as of sglang version 0.1.9. I have tested that adding the following simple `models/mistral.py` file allows hosting the mistral-7b model.\r\n\r\n```python\r\nfrom sglang.srt.models.llama2 import LlamaForCausalLM\r\n\r\n\r\nclass MistralForCausalLM(LlamaForCausalLM):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n\r\n\r\nEntryClass = MistralForCausalLM\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-26T15:06:54+00:00",
    "closed_at": "2024-01-26T17:38:45+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/107/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/107"
  },
  {
    "number": 6050,
    "title": "[Feature] Support encoder models for flash_infer backend",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n              @DavidBao03 hi, currently only support encoder model with torch_native attn backend and triton attn backend. Other attn backend is not supported yet.\n\n_Originally posted by @woodx9 in https://github.com/sgl-project/sglang/issues/4887#issuecomment-2847365703_\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue"
    ],
    "state": "open",
    "created_at": "2025-05-06T09:41:33+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6050/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/6050"
  },
  {
    "number": 69,
    "title": "How create a new branch?",
    "body": "I just fix some bugs and support a new model. I hope create a new branch.",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-21T12:27:58+00:00",
    "closed_at": "2024-01-21T13:02:51+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/69/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/69"
  },
  {
    "number": 7746,
    "title": "[RFC] Remote KV Connector for SGLang Global Cache Reuse and PD ",
    "body": "co-authors: @yizhang2077 \n\n\n# \\[RFC\\]\u00a0Remote\u00a0KV\u00a0Connector\u00a0for\u00a0SGLang\u00a0Global\u00a0Cache\u00a0Reuse\n\n## 1.\u00a0Abstract\n\nThis\u00a0RFC\u00a0proposes\u00a0a\u00a0Remote\u00a0KVCache\u00a0Connector\u00a0System\u00a0to\u00a0enable\u00a0global\u00a0KV\u00a0cache\u00a0reuse\u00a0**across\u00a0SGLang\u00a0nodes**,\u00a0solving\u00a0redundant\u00a0computation\u00a0problems\u00a0in\u00a0multi-turn\u00a0conversation\u00a0scenarios\u00a0and\u00a0achieving\u00a0global\u00a0compute-to-storage\u00a0conversion.\u00a0The\u00a0system\u00a0introduces\u00a0a\u00a0connector\u00a0abstraction\u00a0layer\u00a0that\u00a0allows\u00a0nodes\u00a0to\u00a0store\u00a0and\u00a0retrieve\u00a0KV\u00a0cache\u00a0data\u00a0from\u00a0external\u00a0storage,\u00a0enabling\u00a0prefix-based\u00a0cache\u00a0matching\u00a0and\u00a0reuse\u00a0across\u00a0distributed\u00a0inference\u00a0workers.\n\nKey benefits include:\n\n\u2022 **Global KV Cache Reuse**: Reducing redundant computation via cross-node global KV cache sharing through Global Prefix Index Management capabilities, achieving ~50% TTFT  reduction in Qwen-32B 4TP multi-turn dialogues\n\n\u2022 **Flexible Storage Backend**: Supporting diverse storage backends via a universal KVConnector interface; enabling direct RDMA-based HBM KV Cache access for high-throughput data transfer\n\n\u2022 **Seamless Integration**:  \n  - Native integration with SGLang Scheduler through asynchronous external KV Cache read/write  \n  - Native integration with PD Disaggregation architecture\n    \n\n## 2.\u00a0Motivation\n\n### Problem\u00a0Statement\n\nSGLang\u00a0currently\u00a0lacks\u00a0global\u00a0PrefixCache\u00a0reuse\u00a0functionality.\u00a0In\u00a0multi-turn\u00a0conversation\u00a0scenarios,\u00a0completed\u00a0inference\u00a0KV\u00a0caches\u00a0cannot\u00a0be\u00a0reused\u00a0across\u00a0nodes.\u00a0When\u00a0similar\u00a0prefixes\u00a0are\u00a0processed\u00a0repeatedly\u00a0across\u00a0different\u00a0worker\u00a0nodes,\u00a0this\u00a0leads\u00a0to\u00a0significant\u00a0redundant\u00a0computation\u00a0and\u00a0resource\u00a0waste.\n\n### Goals\n\n*   **Goal\u00a01**:\u00a0Define\u00a0a\u00a0universal\u00a0Remote\u00a0Connector\u00a0Interface\u00a0for\u00a0SGLang\u00a0that\u00a0provides\u00a0basic\u00a0KV\u00a0cache\u00a0read/write\u00a0operations,\u00a0implementing\u00a03FS\u00a0Connector\n    \n*  **Goal\u00a02**:\u00a0Enable\u00a0Normal/Overlap\u00a0Scheduler\u00a0to\u00a0implement\u00a0async\u00a0PreFetch\u00a0and\u00a0Offload\u00a0KVCache\u00a0through\u00a0Connector\u00a0without\u00a0blocking\u00a0scheduler\u00a0operations\n    \n*   **Goal\u00a03**:\u00a0Achieve\u00a0global\u00a0cache\u00a0reuse\u00a0with\u00a0measurable\u00a0performance\u00a0improvements\u00a0in\u00a0multi-turn\u00a0scenarios\n\n*   **Goal\u00a04**:Integration\u00a0with\u00a0PD\u00a0separation\u00a0mode\n    \n\n## 3.\u00a0Technical\u00a0Design\n\n### Architecture\u00a0Overview\n\nThe\u00a0overall\u00a0workflow\u00a0involves:\n\n<img width=\"800\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/bc23cd96-2acc-424c-96c0-2f02fb22e817\" />\n\n1.  **First\u00a0Time\u00a0Scheduling\u00a0(A)**:\n    \n    *   A2:\u00a0Query\u00a0remote\u00a0prefix\u00a0matched\u00a0cache\u00a0through\u00a0Connector,\u00a0returns\u00a00\n        \n    *   A3:\u00a0Execute\u00a0model\u00a0inference,\u00a0write\u00a0KVCache\u00a0to\u00a0VRAM\n        \n    *   A4:\u00a0Write\u00a0KVCache\u00a0and\u00a0Prefix\u00a0Key\u00a0to\u00a0remote\u00a0store\u00a0through\u00a0Connector\n        \n2.  **Second\u00a0Time\u00a0Scheduling\u00a0(B)**:\n    \n    *   B2:\u00a0Query\u00a0remote\u00a0prefix\u00a0matched\u00a0cache\u00a0through\u00a0Connector,\u00a0assume\u00a0match\u00a0result\u00a0is\u00a01024\u00a0(first\u00a01024\u00a0tokens\u00a0computed\u00a0on\u00a0other\u00a0nodes)\n        \n    *   B3:\u00a0Execute\u00a0PreFetch\u00a0through\u00a0Connector,\u00a0pull\u00a0first\u00a01024\u00a0tokens'\u00a0KVCache\u00a0to\u00a0corresponding\u00a0GPU\n        \n    *   B5:\u00a0Execute\u00a0model\u00a0inference\n        \n    *   B6:\u00a0Write\u00a0inference\u00a0results\u00a0back\u00a0to\u00a0remote\u00a0cache\n        \n\n### Key\u00a0Components\n\n#### GlobalRemoteStorage\n\nRemote\u00a0storage\u00a0engine,\u00a0can\u00a0be\u00a03FS/Mooncake/LMCache\u00a0etc.\n\n#### GlobalKVManager\n\nRemote\u00a0global\u00a0prefix\u00a0index,\u00a0responsible\u00a0for\u00a0maintaining\u00a0prefix\u00a0indices\u00a0stored\u00a0in\u00a0remote\u00a0storage.\u00a0GlobalKVManager\u00a0is\u00a0an\u00a0optional\u00a0plugin\u00a0-\u00a0if\u00a0RemoteStorage\u00a0itself\u00a0has\u00a0prefix\u00a0query\u00a0capabilities,\u00a0GlobalKVManager\u00a0is\u00a0not\u00a0needed.\n\n#### RemoteKVConnector\n\nConnector\u00a0component\u00a0abstracts\u00a0the\u00a0API\u00a0interface\u00a0required\u00a0for\u00a0Scheduler\u00a0to\u00a0read/write\u00a0external\u00a0KVCache.\n\n#### SchedulerRemoteKVQueue\n\nScheduler\u00a0async\u00a0schedules\u00a0read/write\u00a0of\u00a0external\u00a0KVCache\u00a0through\u00a0RemoteKVQueue:\n\n*   **AsyncKVPrefetchQueue**:\u00a0Requests\u00a0attempt\u00a0to\u00a0match\u00a0remote\u00a0prefix\u00a0tokens\u00a0through\u00a0Connector,\u00a0pre-allocate\u00a0Token\u00a0KVCache\u00a0Slots,\u00a0and\u00a0async\u00a0pull\u00a0Remote\u00a0KVCache\u00a0through\u00a0Connector\n    \n*   **AsyncKVStoreQueue**:\u00a0Requests\u00a0wait\u00a0for\u00a0all\u00a0Chunk\u00a0KVCache\u00a0to\u00a0complete\u00a0transfer\u00a0before\u00a0releasing\u00a0occupied\u00a0resources\n    \n\n### Scheduler\u00a0Overlap\u00a0Optimization\n\nThe\u00a0Scheduler\u00a0reads/writes\u00a0external\u00a0KVCache\u00a0data\u00a0through\u00a0Connector.\u00a0To\u00a0optimize\u00a0overall\u00a0performance\u00a0after\u00a0introducing\u00a0Connector\u00a0and\u00a0avoid\u00a0blocking\u00a0Scheduler\u00a0operations,\u00a0multi-layered\u00a0queue\u00a0overlap\u00a0between\u00a0KV\u00a0cache\u00a0transfer\u00a0and\u00a0computation\u00a0across\u00a0independent\u00a0requests\u00a0is\u00a0implemented.\n\nThis\u00a0architecture\u00a0uses\u00a0a\u00a0layered\u00a0queue\u00a0mechanism\u00a0to\u00a0decouple\u00a0scheduling\u00a0and\u00a0transfer:\n\n*   **Async\u00a0Pipeline\u00a0Processing**:\u00a0PreFetch\u00a0Queue\u00a0(async\u00a0prefetch),\u00a0Scheduling\u00a0Queue\u00a0(scheduling\u00a0decisions),\u00a0PostTransfer\u00a0Queue\u00a0(async\u00a0transfer)\u00a0form\u00a0a\u00a0non-blocking\u00a0task\u00a0processing\u00a0pipeline\n    \n*   **Design\u00a0Goal**:\u00a0Ensure\u00a0Scheduler\u00a0focuses\u00a0on\u00a0resource\u00a0allocation\u00a0and\u00a0task\u00a0distribution,\u00a0isolating\u00a0I/O\u00a0latency\u00a0through\u00a0queue\u00a0buffering\u00a0and\u00a0async\u00a0operations\u00a0to\u00a0maximize\u00a0system\u00a0throughput\n    \n*   **Example**:\u00a0When\u00a0requests\u00a00,1,2,3\u00a0initiate\u00a0PreFetch,\u00a0requests\u00a04,5\u00a0can\u00a0compute\u00a0in\u00a0parallel\u00a0without\u00a0waiting;\u00a0when\u00a04,5\u00a0initiate\u00a0async\u00a0writeback,\u00a00,1\u00a0can\u00a0also\u00a0compute\u00a0in\u00a0parallel\n    \n<img width=\"956\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/095a11cd-d45b-408c-bb60-5643f5a81156\" />\n\n### Workflow\u00a0Phases\n\n1.  **PreFetch\u00a0Phase**:\n    \n    *   Scheduler\u00a0adds\u00a0Req\u00a0to\u00a0AsyncKVPrefetch\u00a0Queue\n        \n    *   Queue\u00a0uses\u00a0KVConnector.num\\_external\\_matched\\_tokens\u00a0to\u00a0determine\u00a0remote\u00a0prefix\u00a0cache\u00a0length\n        \n    *   When\u00a0exceeding\u00a0retrieve\\_remote\\_cache\\_threshold,\u00a0initiate\u00a0PreFetch\n        \n    *   Allocate\u00a0Token\u00a0KVCache\u00a0Page\u00a0Slots\u00a0and\u00a0read\u00a0remote\u00a0cache\u00a0via\u00a0KVConnector.retrieve\\_kv\\_async\n        \n2.  **Forward\u00a0Phase**:\n    \n    *   Execute\u00a0model\u00a0inference\u00a0workflow\n        \n    *   For\u00a0each\u00a0produced\u00a0Chunk\u00a0KVCache,\u00a0async\u00a0send\u00a0via\u00a0KVConnector.store\\_kv\\_async\n        \n3.  **Transfer\u00a0Phase**:\n    \n    *   After\u00a0inference\u00a0completion,\u00a0place\u00a0Req\u00a0in\u00a0AsyncKVStore\u00a0Queue\n        \n    *   Wait\u00a0for\u00a0all\u00a0Chunk\u00a0transfers\u00a0to\u00a0complete\n        \n\n![Image](https://github.com/user-attachments/assets/097e83b7-d7a1-4ae9-b9fa-8a25a5d703ba)\n\n### Connector\u00a0Interface\u00a0Definition\n\n```python\nclass RemoteKVConnector(ABC):\n    \"\"\"\n    Base class for all connectors.\n    A connector is responsible for storing and retrieving KV caches from the external storage.\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self, metadata: ConnectorMetadata):\n        \"\"\"\n        Initialize the connector with the metadata.\n        Args:\n            metadata: The specific metadata for the connector.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def register_kv_buffers(self, kv_args: KVArgs):\n        \"\"\"\n        Initialize with the KV caches. Useful for pre-registering the\n        KV Buffers (e.g. KV ptrs) into the KVConnector (e.g. for NIXL, Mooncake).\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def store_kv_async(\n        self,\n        req_id: str,\n        page_to_tokens_mapping: list[tuple[int, slice]],\n        tokens_list: list[int],\n        **kwargs\n    ) -> tuple[Future, int]:\n        \"\"\"\n        Async store the KV cache of the given token range to the connector.\n        \n        This method stores KV cache data for tokens organized by page indices.\n        Each page contains a list of tokens that need to be stored together.\n        \n        Args:\n            req_id: Unique request identifier for tracking this store operation.\n            page_to_tokens_mapping: A list of tuples, where each tuple contains:\n                - page_index: The index of the KV cache page\n                - token_index_range: List of tokens associated with this page\n            tokens_list: The complete token list.\n            **kwargs: Additional arguments for the store operation.\n        \n        Returns:\n            tuple[Future, int]: A tuple containing:\n                - Future: The future object that indicates success or failure of the store operation.\n                - int: The end index of the stored tokens, indicating the last token position\n                       that was successfully stored in this operation.\n        \"\"\"\n        pass\n\n\n    @abstractmethod\n    def retrieve_kv_async(\n        self,\n        req_id: str,\n        page_to_tokens_mapping: list[tuple[int, slice]],\n        tokens_list: list[int],\n        **kwargs,\n    ) -> Future:\n        \"\"\"\n        Async retrieve the KV cache from the connector to the given kv indices.\n        \n        This method retrieves KV cache data for tokens organized by page indices.\n        Each page contains a list of tokens that need to be retrieved together.\n        \n        Args:\n            req_id: Unique request identifier for tracking this retrieve operation.\n            page_to_tokens_mapping: A list of tuples, where each tuple contains:\n                - page_index: The index of the KV cache page\n                - token_index_range: List of tokens associated with this page\n            tokens_list: The complete token list.\n            **kwargs: Additional arguments for the retrieve operation.\n        \n        Returns:\n            Future: The future object that indicates success or failure of the retrieve operation.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def num_external_matched_tokens(\n        self, \n        req_id: str,\n        tokens: list[int],\n        num_local_computed_tokens: int,\n        **kwargs\n    ) -> int:\n        \"\"\"\n        Get number of new tokens that can be loaded from the\n        external KV cache beyond the num_local_computed_tokens for a batch of inputs.\n\n        Args:\n            req_id: Unique request identifier for tracking this operation.\n            tokens: The tokens to match against the external KV cache.\n            num_local_computed_tokens: The number of locally computed tokens.\n            **kwargs: Additional arguments for the operation.\n\n        Returns:\n            int: The number of new tokens that can be loaded from the\n                 external KV cache beyond the num_local_computed_tokens.\n        \"\"\"\n        pass    \n\n    @abstractmethod\n    def finish_req(\n        self,\n        req_id: str\n    ):\n        \"\"\"\n        Finish the request, release the related resources.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def close(self):\n        pass\n\n```\n\n## 4.\u00a0Implementation\u00a0Plan\n\n### Phase\u00a01:\u00a0Core\u00a0Infrastructure\u00a0(Weeks\u00a01-2)\n\n**Timeline**:\u00a02\u00a0weeks\u00a0\u00a0 **Deliverables**:\n\n*   [ ] \u2705\u00a0Task01:\u00a0RemoteKVConnector\u00a0Interface\u00a0Definition\n    \n    *   Define\u00a0abstract\u00a0base\u00a0class\u00a0with\u00a0all\u00a0required\u00a0methods\n        \n*   [ ] \u2705\u00a0Task02:\u00a0RemoteKVManager\u00a0Interface\n    \n    *   Design\u00a0prefix\u00a0index\u00a0management\u00a0interface\n        \n*   [ ] \u2705\u00a0Task03:\u00a03FS\u00a0KVConnector\u00a0Implementation\n    \n    *   Implement\u00a03FS\u00a0connector\u00a0\u00a0\n        \n    *   Implement\u00a0localfile\u00a0connector\n        \n    *   Add\u00a0chunking\u00a0and\u00a0compression\u00a0support\n        \n    *   Include\u00a0error\u00a0handling\u00a0and\u00a0retry\u00a0mechanisms\n        \n*   [ ] \u2705\u00a0Task4:\u00a0Mini\u00a0RemoteKVManager\n    \n    *   Basic\u00a0in-memory\u00a0prefix\u00a0index\u00a0implementation\n        \n    *   Thread-safe\u00a0operations\n        \n    *   Implement\u00a0Restful\u00a0API\n        \n\n### Phase\u00a02:\u00a0Queue\u00a0Infrastructure\u00a0(Week\u00a03)\n\n**Timeline**:\u00a01\u00a0week\u00a0\u00a0 **Deliverables**:\n\n*   [ ] \u2705\u00a0\u00a0Task5:\u00a0AsyncKVPrefetchQueue\u00a0Implementation\n    \n    *   Resource\u00a0allocation\u00a0management\uff0cToken\u00a0pool\u00a0allocation\u00a0for\u00a0external\u00a0cache\n        \n    *   Async\u00a0Prefetch\u00a0kvcache\n        \n    *   Completion\u00a0polling\u00a0with\u00a0TP\u00a0consistency\u00a0guarantees\n        \n*   [ ] \u2705\u00a0\u00a0Task6:\u00a0AsyncKVStoreQueue\u00a0Implementation\n    \n    *   Chunked\u00a0transfer\u00a0management\u00a0with\u00a0configurable\u00a0thresholds\n        \n    *   Completion\u00a0polling\u00a0with\u00a0TP\u00a0consistency\u00a0guarantees\n        \n    *   Resource\u00a0cleanup\u00a0and\u00a0error\u00a0recovery\n        \n\n### Phase\u00a03:\u00a0Scheduler\u00a0Integration\u00a0(Weeks\u00a04-6)\n\n**Timeline**:\u00a03\u00a0weeks\u00a0\u00a0 **Deliverables**:\n\n*   [ ] \u2705\u00a0\u00a0Task7:\u00a0Core\u00a0Scheduler\u00a0Logic\u00a0Integration\n    \n    *   Integrate\u00a0AsyncPrefetch/StoreQueue\u00a0into\u00a0event\u00a0loops\n        \n    *   Add\u00a0remote\u00a0KV\u00a0cache\u00a0configuration\u00a0options\n        \n    *   Implement\u00a0polling\u00a0mechanisms\u00a0in\u00a0normal/overlap\u00a0modes\n        \n\n### Phase\u00a04:\u00a0Testing\u00a0and\u00a0Optimization\u00a0(Weeks\u00a07-8)\n\n**Timeline**:\u00a02\u00a0weeks\u00a0\u00a0 **Deliverables**:\n\n*   [ ] \u00a0Task8:\u00a0Performance\u00a0Evaluation\n    \n    *   Multi-turn\u00a0dialogue\u00a0scenario\u00a0testing\u00a0with\u00a0Qwen/DeepSeek\u00a0models\n        \n    *   TTFT\u00a0reduction\u00a0measurement\u00a0(target:\u00a0~50%\u00a0improvement)\n        \n    *   Throughput\u00a0comparison\u00a0against\u00a0baseline\n        \n*   [ ] \u00a0Task9:\u00a0Stability\u00a0Testing\n    \n    *   Long-running\u00a0conversation\u00a0scenarios\n        \n    *   Error\u00a0injection\u00a0and\u00a0recovery\u00a0testing\n        \n    *   Memory\u00a0leak\u00a0detection\u00a0and\u00a0prevention\n        \n*   [ ] \u00a0Task10:\u00a0Documentation\u00a0and\u00a0Examples\n    \n    *   Configuration\u00a0guides\u00a0for\u00a0different\u00a0storage\u00a0backends\n        \n    *   Performance\u00a0tuning\u00a0recommendations\n        \n    *   Troubleshooting\u00a0documentation\n        \n\n## 5.\u00a0Long-Term\u00a0Vision\u00a0With\u00a0PD\n\nUpon\u00a0completion\u00a0of\u00a0this\u00a0RFC,\u00a0the\u00a0Connector\u00a0mode\u00a0can\u00a0be\u00a0integrated\u00a0with\u00a0SGLang's\u00a0PD\u00a0(Prefill-Decode)\u00a0separation\u00a0architecture.\u00a0\n\nOptions\u00a0include:\n\n\u2022\u00a0Reusing\u00a0Remote\u00a0Cache\u00a0via\u00a0Connector\u00a0during\u00a0Prefill\n\n\u2022\u00a0Direct\u00a0KVCache\u00a0transfer\u00a0between\u00a0PD\u00a0nodes\u00a0through\u00a0Connector\n\n<img width=\"737\" height=\"513\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/77f61c4a-a21a-49e0-9bca-917b123bd1b5\" />\n\n<img width=\"785\" height=\"510\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/026eeac6-bcc2-4735-8c2d-31ada2f85859\" />\n\n## 6.\u00a0Different\u00a0with\u00a0HiRadixCache\n\nHiRadixCache\u200b\u00a0is\u00a0another\u00a0highly\u00a0efficient\u00a0multi-tier\u00a0storage\u00a0implementation\u00a0from\u00a0the\u00a0SGLang\u00a0community.\u00a0Extending\u00a0HiRadixCache\u00a0appropriately\u00a0to\u00a0schedule\u00a0the\u00a0\u200bRemote\u00a0Cache\u200b\u00a0is\u00a0also\u00a0a\u00a0viable\u00a0design\u00a0option.\n\nThis\u00a0solution\u00a0also\u00a0draws\u00a0inspiration\u00a0from\u00a0the\u00a0concepts\u00a0of\u00a0HiRadixCache.\u00a0Its\u00a0main\u00a0differences\u00a0from\u00a0HiRadixCache\u00a0are:\n\n1.  Current\u00a0Solution:\u200b\u00a0It\u00a0\u200bforming\u00a0a\u00a0\u200bcompletely\u00a0independent\u00a0schedule\u00a0pipeline.\u00a0It\u00a0interacts\u00a0\u200bdirectly\u00a0with\u00a0the\u00a0GPU\u200b\u00a0without\u00a0intermediate\u00a0buffering\u00a0in\u00a0DRAM,\u00a0enabling\u00a0\u200bdirect\u00a0transfer\u00a0of\u00a0KVCache\u00a0via\u00a0RDMA.\n    \n2.  Current\u00a0Solution:\u200b\u00a0It\u00a0employs\u00a0a\u00a0\u200bdedicated\u00a0Global\u00a0KVCacheManager\u200b\u00a0component\u00a0to\u00a0uniformly\u00a0manage\u00a0\u200bglobal\u00a0prefix\u00a0indexing.\u00a0Crucially,\u00a0the\u00a0underlying\u00a0storage\u00a0\u200bdoes\u00a0not\u00a0require\u200b\u00a0native\u00a0support\u00a0for\u00a0prefix\u00a0query\u00a0capabilities.\n    \n\n## 7.\u00a0References\n\n1.  **SGLang\u00a0PD\u00a0Disaggregation\u00a0Design**\n    \n    *   [Internal\u00a0Design\u00a0Document](https://docs.google.com/document/d/1rQXJwKd5b9b1aOzLh98mnyMhBMhlxXA5ATZTHoQrwvc/edit?tab=t.0#heading=h.i3s2t1j0e1ik)\n        \n2.  **SGLang\u00a0Scheduler\u00a0Implementation**\n    \n    *   `python/sglang/srt/managers/scheduler.py`\n        \n3.  **Memory\u00a0Pool\u00a0and\u00a0Cache\u00a0Systems**\n    \n    *   `python/sglang/srt/mem_cache/memory_pool.py`\n        \n    *   `python/sglang/srt/mem_cache/radix_cache.py`\n        \n    *   `python/sglang/srt/mem_cache/hiradix_cache.py`\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority"
    ],
    "state": "open",
    "created_at": "2025-07-03T12:57:07+00:00",
    "closed_at": null,
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7746/reactions",
      "total_count": 10,
      "+1": 10,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7746"
  },
  {
    "number": 2312,
    "title": "[Bug] Overlap mode scheduler doesn't work for bench_serving with given request rate",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen I tried using a overlapped scheduler to serve my model and use sglang.bench_serving to evluate performance with a given request rate, the scheduler got stuck and the reporting are as follows\uff1a\r\n`Exception in thread Thread-3 (forward_thread_func):\r\nTraceback (most recent call last):\r\n  File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\r\n    self.run()\r\n  File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/threading.py\", line 953, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/ykchen/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 99, in forward_thread_func\r\n    self.forward_thread_func_()\r\n  File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/ykchen/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 116, in forward_thread_func_\r\n    logits_output, next_token_ids = self.worker.forward_batch_generation(\r\n  File \"/home/ykchen/sglang/python/sglang/srt/managers/tp_worker.py\", line 139, in forward_batch_generation\r\n    logits_output = self.model_runner.forward(forward_batch)\r\n  File \"/home/ykchen/sglang/python/sglang/srt/model_executor/model_runner.py\", line 594, in forward\r\n    return self.forward_decode(forward_batch)\r\n  File \"/home/ykchen/sglang/python/sglang/srt/model_executor/model_runner.py\", line 565, in forward_decode\r\n    return self.model.forward(\r\n  File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/ykchen/sglang/python/sglang/srt/models/llama.py\", line 372, in forward\r\n    res = self.logits_processor(\r\n  File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/ykchen/sglang/python/sglang/srt/layers/logits_processor.py\", line 186, in forward\r\n    last_logits = last_logits[:, : self.config.vocab_size].float()\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.`\r\nHowever when I run bench_serving without specifying request rate, the scheduler works well. Besides, the normal scheduler can also work well with a given request rate.\n\n### Reproduction\n\nModel: Meta-Llama-3-8B-Instruct\r\nCommand to start engine:\r\n`python -m sglang.launch_server --model-path /path/to/my/model --port 30000 --host 0.0.0.0 --disable-cuda-graph --disable-radix-cache --enable-overlap-schedule`\r\nCommand to start benchmark:\r\n`python -m sglang.bench_serving --backend sglang --num-prompt 100 --request-rate 3`\r\nIt works when I use `python -m sglang.bench_serving --backend sglang --num-prompt 100` without specifying request rate.\n\n### Environment\n\n`Python: 3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0,1,2,3: NVIDIA A40\r\nGPU 0,1,2,3 Compute Capability: 8.6\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.3, V12.3.107\r\nCUDA Driver Version: 545.23.08\r\nPyTorch: 2.4.0+cu121\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.46.1\r\nrequests: 2.32.3\r\ntqdm: 4.66.6\r\nnumpy: 1.26.4\r\naiohttp: 3.10.10\r\nfastapi: 0.115.4\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.26.2\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.1.0\r\npydantic: 2.9.2\r\nuvicorn: 0.32.0\r\nuvloop: 0.21.0\r\nzmq: 26.2.0\r\nvllm: 0.6.3.post1\r\nmultipart: 0.0.16\r\nopenai: 1.53.0\r\nanthropic: 0.37.1\r\nNVIDIA Topology: \r\n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PIX     SYS     SYS     0-11,24-35      0               N/A\r\nGPU1    PIX      X      SYS     SYS     0-11,24-35      0               N/A\r\nGPU2    SYS     SYS      X      PXB     12-23,36-47     1               N/A\r\nGPU3    SYS     SYS     PXB      X      12-23,36-47     1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nulimit soft: 1048576`",
    "labels": [],
    "state": "closed",
    "created_at": "2024-12-02T09:07:54+00:00",
    "closed_at": "2024-12-02T13:10:30+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2312/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2312"
  }
]
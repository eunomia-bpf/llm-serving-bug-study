[
  {
    "number": 7010,
    "title": "[Feature] integrate nccl 2.27 with pynccl",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-09T17:55:18+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7010/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/7010"
  },
  {
    "number": 7771,
    "title": "[PD Bug]  Decode  RuntimeError: no running event loop",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n<img width=\"1246\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2b7e2e76-ba7b-4cd1-bbf7-0539dff515e0\" />\n\n### Reproduction\n\n1. benchmarking\n2. restart decode node\n3. observe decode log\n\n### Environment\n\nh20 pd",
    "labels": [],
    "state": "closed",
    "created_at": "2025-07-04T08:06:43+00:00",
    "closed_at": "2025-07-05T05:36:57+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7771/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7771"
  },
  {
    "number": 2681,
    "title": "[Feature] support ngram",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nref https://github.com/apoorvumang/prompt-lookup-decoding\n\n### Related resources\n\n_No response_",
    "labels": [
      "enhancement",
      "good first issue",
      "help wanted"
    ],
    "state": "open",
    "created_at": "2024-12-31T07:03:24+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2681/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/2681"
  },
  {
    "number": 5023,
    "title": "[Feature] upgrade flashinfer 0.2.5",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "help wanted"
    ],
    "state": "closed",
    "created_at": "2025-04-03T08:04:23+00:00",
    "closed_at": "2025-04-30T03:05:57+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5023/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5023"
  },
  {
    "number": 3262,
    "title": "[Feature] Reorganize Reference Docs",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n@shuaills Thanks!\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue"
    ],
    "state": "open",
    "created_at": "2025-02-02T19:34:09+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3262/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3262"
  },
  {
    "number": 2739,
    "title": "[Feature] adapt fused sigmoid gate for MoE model",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nref https://github.com/NVIDIA/TensorRT-LLM/blob/be1788106245496872d18e702978e59b6bfd50e0/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.cu#L232\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "performance"
    ],
    "state": "closed",
    "created_at": "2025-01-05T16:55:21+00:00",
    "closed_at": "2025-05-25T23:52:20+00:00",
    "comments": 20,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2739/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/2739"
  },
  {
    "number": 2551,
    "title": "[Bug] Link error in SGLang Sampling Docs",
    "body": "### Checklist\r\n\r\n- [x] 1. I have searched related issues but cannot get the expected help.\r\n- [x] 2. The bug has not been fixed in the latest version.\r\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [x] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nhttps://sgl-project.github.io/references/sampling_params.html\r\n\r\nThis link is an error and I am pondering why it should refer to.\r\n\r\n\r\n![image](https://github.com/user-attachments/assets/5401ab02-2cbd-476f-bef9-6ac0c7eda58e)\r\n\r\n\r\n\r\n### Reproduction\r\n\r\nno such\r\n\r\n### Environment\r\n\r\nno such",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2024-12-23T02:58:24+00:00",
    "closed_at": "2024-12-26T15:12:28+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2551/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2551"
  },
  {
    "number": 4546,
    "title": "[Feature] use vllm as an optional quant library",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled\n\nIn the latest main branch, we have removed the dependency on vLLM for NVIDIA GPU. For large scale online serving, W8A8 Int8 and FP8 should suffice. We may consider supporting other quant methods like Qserve in the future, but using vLLM as an optional quant library is generally adequate. In the upcoming release version, vLLM will not be installed by default. Users who wish to use AWQ or similar quant methods can manually install them.\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-18T08:09:43+00:00",
    "closed_at": "2025-05-18T00:20:54+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4546/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/4546"
  },
  {
    "number": 7617,
    "title": "[Feature] qwen 3 eagle 3",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-06-28T04:44:46+00:00",
    "closed_at": "2025-07-10T16:33:29+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7617/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/7617"
  },
  {
    "number": 1152,
    "title": "[Tracker] OpenRouter LLM rankings tracking",
    "body": "### Checklist\r\n\r\n- [X] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [X] 2. Please use English, otherwise it will be closed.\r\n\r\n### Motivation\r\n\r\nThis issue is not a bug report or feature request, but just a record of tracking the current popular LLM.\r\n\r\nFrom https://openrouter.ai/rankings `Top this month`, it can be known that, currently in addition to using the closed-source model APIs of OpenAI, Anthropic and Google, users also use open source LLM. The difference between OpenRouter and other rankings is that it better reflects the real usage scenarios and actual conditions of users.\r\n\r\nThe list shows that the currently mainly used series are [Llama 3](https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6), [Llama 3.1](https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f), [Gemma 2](https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315), [Mistral, Mixtral](https://huggingface.co/mistralai), and [DeepSeek Coder V2](https://huggingface.co/collections/deepseek-ai/deepseekcoder-v2-666bf4b274a5f556827ceeca). Besides these mentioned models, there are some based on these models through SFT, for example, [MythoMax](https://huggingface.co/collections/Gryphe/mytho-series-llama-2-6561f2157ff2e1b1cf8d2c27) 13B is based on [Llama 2](https://huggingface.co/collections/meta-llama/llama-2-family-661da1f90a9d678b6f55773b), and [WizardLM-2 8x22B](https://huggingface.co/alpindale/WizardLM-2-8x22B) is based on [Mixtral 8x22B](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1). These popular models are **all supported** in SGLang at present(August 19, 2024).\r\n\r\n\r\n\r\n### Related resources\r\n\r\n_No response_",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-19T11:41:07+00:00",
    "closed_at": "2024-09-22T11:07:15+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1152/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/1152"
  },
  {
    "number": 1183,
    "title": "[Bug] vllm updated its get_model function",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [X] 2. The bug has not been fixed in the latest version.\r\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [X] 5. Please use English, otherwise it will be closed.\r\n\r\n### Describe the bug\r\n\r\nPreviously we used this in our `ModelRunner`:\r\n\r\n```\r\n        self.model = get_model(\r\n            model_config=self.vllm_model_config,\r\n            device_config=self.device_config,\r\n            load_config=self.load_config,\r\n            lora_config=None,\r\n            multimodal_config=None,\r\n            parallel_config=None,\r\n            scheduler_config=None,\r\n            cache_config=None,\r\n        )\r\n```\r\n\r\nNote that vllm updated its get_model function several days ago and removed the `multimodal_config` parameter.\r\n\r\nhttps://github.com/vllm-project/vllm/blob/main/vllm/model_executor/model_loader/__init__.py\r\n\r\nAlso, even if we delete this parameter from our ModelRunner:\r\n\r\n```\r\n        self.model = get_model(\r\n            model_config=self.vllm_model_config,\r\n            device_config=self.device_config,\r\n            load_config=self.load_config,\r\n            lora_config=None,\r\n            # multimodal_config=None,\r\n            parallel_config=None,\r\n            scheduler_config=None,\r\n            cache_config=None,\r\n        )\r\n```\r\n\r\n\r\nThe unit test in `test/srt/models/test_embedding_models.py` still could not pass. Since I found that the new get_model function will load the LlamaEmbeddingModel class defined in vllm/model_executor/models/llama_embedding.py\r\n\r\nBut we want the get_model function to load our LlamaEmbeddingModel class in https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/models/llama_embedding.py\r\n\r\nCould someone check what's happening in the new get_model function?\r\n\r\n### Reproduction\r\n\r\nSimply use the latest code of vllm and sglang can reproduce it.\r\n\r\nThe attachment is my traceback:\r\n\r\n```\r\n(AlphaMeemory) (AlphaMeemory) chenyang@uclaml04:/data/chenyang/sglang/test/srt/models$ CUDA_VISIBALE_DEVICES=5 python3 test_embedding_models.py \r\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  5.35it/s]\r\nINFO 08-21 21:24:10 weight_utils.py:236] Using model weights format ['*.safetensors']\r\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.12s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.12it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.08it/s]\r\n\r\nException in ModelTpServer:\r\nTraceback (most recent call last):\r\n  File \"/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 233, in exposed_step\r\n    self.forward_step()\r\n  File \"/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 249, in forward_step\r\n    self.forward_prefill_batch(new_batch)\r\n  File \"/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 540, in forward_prefill_batch\r\n    output = self.model_runner.forward(batch, ForwardMode.EXTEND)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 527, in forward\r\n    return self.forward_extend(batch)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 501, in forward_extend\r\n    return self.model.forward(\r\n           ^^^^^^^^^^^^^^^^^^^\r\nTypeError: LlamaEmbeddingModel.forward() missing 1 required positional argument: 'attn_metadata'\r\n\r\nException in ControllerSingle:\r\nTraceback (most recent call last):\r\n  File \"/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/sglang/srt/managers/controller_single.py\", line 165, in start_controller_process\r\n    controller.loop_for_forward()\r\n  File \"/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/sglang/srt/managers/controller_single.py\", line 102, in loop_for_forward\r\n    out_pyobjs = self.tp_server.exposed_step(recv_reqs)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 233, in exposed_step\r\n    self.forward_step()\r\n  File \"/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 249, in forward_step\r\n    self.forward_prefill_batch(new_batch)\r\n  File \"/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 540, in forward_prefill_batch\r\n    output = self.model_runner.forward(batch, ForwardMode.EXTEND)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 527, in forward\r\n    return self.forward_extend(batch)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 501, in forward_extend\r\n    return self.model.forward(\r\n           ^^^^^^^^^^^^^^^^^^^\r\nTypeError: LlamaEmbeddingModel.forward() missing 1 required positional argument: 'attn_metadata'\r\n\r\nE\r\n======================================================================\r\nERROR: test_prefill_logits (__main__.TestEmbeddingModels.test_prefill_logits)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/data/chenyang/sglang/test/srt/models/test_embedding_models.py\", line 67, in test_prefill_logits\r\n    self.assert_close_prefill_logits(\r\n  File \"/data/chenyang/sglang/test/srt/models/test_embedding_models.py\", line 41, in assert_close_prefill_logits\r\n    with SRTRunner(\r\n         ^^^^^^^^^^\r\n  File \"/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/sglang/test/runners.py\", line 189, in __init__\r\n    self.runtime = Runtime(\r\n                   ^^^^^^^^\r\n  File \"/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/sglang/srt/server.py\", line 533, in __init__\r\n    raise RuntimeError(\r\nRuntimeError: Initialization failed. Please see the error messages above.\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 20.086s\r\n```\r\n\r\nThe error \"TypeError: LlamaEmbeddingModel.forward() missing 1 required positional argument: 'attn_metadata'\" told me that the get_model function is loading the LlamaEmbeddingModel in vllm but not sglang.\r\n\r\n### Environment\r\n\r\nPython: 3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA RTX A6000\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.6\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.3, V12.3.103\r\nCUDA Driver Version: 545.23.08\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.2.13\r\nflashinfer: 0.1.5+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.43.3\r\nrequests: 2.32.3\r\ntqdm: 4.66.4\r\nnumpy: 1.26.4\r\naiohttp: 3.9.5\r\nfastapi: 0.112.1\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.3\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.8.2\r\nuvicorn: 0.23.2\r\nuvloop: 0.19.0\r\nzmq: 26.0.3\r\nvllm: 0.5.4\r\nmultipart: 0.0.9\r\nopenai: 1.40.3\r\nanthropic: 0.33.0\r\nNVIDIA Topology: \r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\r\nGPU1    SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\r\nGPU2    SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\r\nGPU3    SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\r\nGPU4    SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     16-31,48-63     1               N/A\r\nGPU5    SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     16-31,48-63     1               N/A\r\nGPU6    SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     16-31,48-63     1               N/A\r\nGPU7    SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      16-31,48-63     1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nulimit soft: 1048576",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-22T04:47:56+00:00",
    "closed_at": "2024-08-26T09:03:16+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1183/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1183"
  },
  {
    "number": 2315,
    "title": "[Bug] fix code scanning issue",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nref https://github.com/sgl-project/sglang/security/code-scanning\r\n\r\nThe priority is not high, I will handle it when I have the bandwidth.\n\n### Reproduction\n\nN/A\n\n### Environment\n\nN/A",
    "labels": [
      "backlog",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-12-02T13:42:27+00:00",
    "closed_at": "2025-02-01T00:17:47+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2315/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/2315"
  },
  {
    "number": 1064,
    "title": "[Bug] Could not post to external IP address",
    "body": "### Checklist\r\n\r\n- [X] 1. I have searched related issues but cannot get the expected help.\r\n- [X] 2. The bug has not been fixed in the latest version.\r\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\r\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n\r\n### Describe the bug\r\n\r\nI can\u2019t post to an external IP address. Previously when I used `vllm`, I could use this:\r\n\r\n```\r\nclient = openai.OpenAI(\r\n        base_url=f\"http://{MY_IP_ADDRESS}/:{server.port if server.ip != SERVER_IP_UCLA_03 else 5600}/v1\",\r\n        api_key=\"EMPTY\",\r\n    )\r\n```\r\n\r\nBut now I can only post to `127.0.0.1`. So I have to use `ngrok` to map my external IP address to my local host.\r\n\r\n### Reproduction\r\n\r\n```\r\nfrom collections import namedtuple\r\nimport openai\r\nfrom typing import List, Optional\r\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\r\nimport random\r\nfrom IPython import embed\r\n\r\nSERVER_IP_UCLA_04 = \"[xxx.xxx.xxx.xxx]\" #! It is a private IP address, but I am sure that the address is correct\r\nMODEL_NAME_8B = \"8bins\"\r\n\r\nServer = namedtuple(\"Server\", [\"ip\", \"port\", \"model_size\", \"model_path\", \"gpus\"])\r\n\r\n\r\ndef test_server(server: Server):\r\n\r\n    def get_completion(\r\n        clients: List,\r\n        message: List,\r\n        temperature: float = 0.0,\r\n        max_tokens: int = 256,\r\n        model_name: Optional[str] = None,\r\n    ) -> str:\r\n\r\n        tried_clients = []\r\n        print(\r\n            f\"\"\"\r\n=========================================\r\n\r\nmessage:\r\n\r\n{str(message)}\r\n\r\n=========================================\r\n\"\"\"\r\n        )\r\n\r\n        @retry(\r\n            wait=wait_random_exponential(min=1, max=100),\r\n            stop=stop_after_attempt(100),\r\n            before_sleep=lambda retry_state: print(\r\n                f\"Retrying... Attempt number: {retry_state.attempt_number}\"\r\n            ),\r\n        )\r\n        def _get_completion(\r\n            clients: List,\r\n            message: List,\r\n            temperature: float = 0.0,\r\n            max_tokens: int = 256,\r\n            model_name: Optional[str] = None,\r\n        ) -> str:\r\n\r\n            available_clients = [\r\n                client for client in clients if client not in tried_clients\r\n            ]\r\n            if not available_clients:\r\n                available_clients = clients\r\n                tried_clients.clear()\r\n\r\n            selected_client = random.choice(available_clients)\r\n            tried_clients.append(selected_client)\r\n\r\n            completion = selected_client.chat.completions.create(\r\n                model=model_name,\r\n                messages=message,\r\n                max_tokens=max_tokens,\r\n                temperature=temperature,\r\n                stop=[\"<|eot_id|>\"],\r\n            )\r\n            return str(completion.choices[0].message.content)\r\n\r\n        return _get_completion(clients, message, temperature, max_tokens, model_name)\r\n\r\n    client = openai.OpenAI(\r\n        base_url=f\"http://{server.ip}:{server.port}/v1\",\r\n        api_key=\"EMPTY\",\r\n    )\r\n\r\n    try:\r\n        response = get_completion(\r\n            [client],\r\n            [\r\n                {\"role\": \"user\", \"content\": \"Where is the captial city of China?\"},\r\n            ],\r\n            0.9,\r\n            256,\r\n            server.model_path,\r\n        )\r\n        print(\r\n            f\"\"\"\r\n============================================================\r\nTESTING SERVER\r\nCluster: {server.ip}\r\nPort: {server.port}\r\nModel: {server.model_path}\r\nSize: {server.model_size}\r\nGPUs: {server.gpus}\r\n============================================================\r\n          \r\n\"\"\"\r\n        )\r\n        print(response)\r\n        return True\r\n    except Exception as e:\r\n        print(\r\n            f\"\"\"\r\n============================================================\r\nTESTING SERVER\r\nCluster: {server.ip}\r\nPort: {server.port}\r\nModel: {server.model_path}\r\nSize: {server.model_size}\r\nGPUs: {server.gpus}\r\n============================================================\r\n\r\n    \"\"\"\r\n        )\r\n        print(\r\n            f\"\"\"\r\nCould not connect to server {server.ip}:{server.port}\r\n\"\"\"\r\n        )\r\n        return False\r\n\r\n\r\ntest_server(Server(\r\n        ip=SERVER_IP_UCLA_04,\r\n        port=8048,\r\n        model_size=\"8\",\r\n        model_path=MODEL_NAME_8B,\r\n        gpus=[0],\r\n    ))\r\n\r\n#! This would fail\r\n\r\ntest_server(Server(\r\n        ip=\"127.0.0.1\",\r\n        port=8048,\r\n        model_size=\"8\",\r\n        model_path=MODEL_NAME_8B,\r\n        gpus=[0],\r\n    ))\r\n\r\n#! This would pass\r\n```\r\n\r\nThe model is https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\r\n\r\n### Environment\r\n\r\n```\r\nPython: 3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA RTX A6000\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.6\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.3, V12.3.103\r\nCUDA Driver Version: 545.23.08\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.2.12\r\nflashinfer: 0.1.4+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.43.3\r\nrequests: 2.32.3\r\ntqdm: 4.66.4\r\nnumpy: 1.26.4\r\naiohttp: 3.9.5\r\nfastapi: 0.111.1\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.3\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.3\r\nuvloop: 0.19.0\r\nzmq: 26.0.3\r\nvllm: 0.5.4\r\nmultipart: 0.0.9\r\nopenai: 1.40.3\r\nanthropic: 0.33.0\r\nNVIDIA Topology: \r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\r\nGPU1    SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\r\nGPU2    SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\r\nGPU3    SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\r\nGPU4    SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     16-31,48-63     1               N/A\r\nGPU5    SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     16-31,48-63     1               N/A\r\nGPU6    SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     16-31,48-63     1               N/A\r\nGPU7    SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      16-31,48-63     1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nulimit soft: 1048576\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-13T00:19:17+00:00",
    "closed_at": "2024-08-13T04:37:23+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1064/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/1064"
  },
  {
    "number": 2788,
    "title": "[Feature] Integration of TurboMind AWQ and GPTQ",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nThe AWQ and GPTQ of TurboMind should be among the best-performing open-source implementations currently available. We plan to integrate them into SGLang, and once the integration is complete, we can consider removing SGLang's dependency on vLLM's AWQ and GPTQ kernel.\r\n\r\nDuring development, we can initially install the wheel https://github.com/InternLM/turbomind/releases/tag/v0.0.1 manually for verification and later add the TurboMind repo as a dependency in [sgl-kernel](https://github.com/sgl-project/sglang/tree/main/sgl-kernel).\r\n\r\nref\r\nhttps://github.com/InternLM/turbomind\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "help wanted"
    ],
    "state": "open",
    "created_at": "2025-01-08T08:37:01+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2788/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/2788"
  },
  {
    "number": 5404,
    "title": "[Bug] merge_state_v2 issue",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n@qingquansong @Fridge003 \n\n### Reproduction\n\nN/A\n\n### Environment\n\nN/A",
    "labels": [
      "bug",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-04-15T06:18:14+00:00",
    "closed_at": "2025-04-15T17:18:48+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5404/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5404"
  },
  {
    "number": 1358,
    "title": "[Bug] it seems memory leak in sglang when longtime serving",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [X] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n128134 memory blocks: 5104.2 KiB\r\nTraceback:\r\n  File \"/usr/local/lib/python3.10/dist-packages/zmq/_future.py\", line 374\r\n    loaded = load(buf)\n\n### Reproduction\n\ntracemalloc in detokenizer_manager.py\n\n### Environment\n\nllama2-13B A800*1",
    "labels": [],
    "state": "closed",
    "created_at": "2024-09-09T12:12:45+00:00",
    "closed_at": "2024-09-09T12:14:07+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1358/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1358"
  },
  {
    "number": 2060,
    "title": "[Feature] Add Dockerfile.dev for development purposes",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nAdd some commonly used command-line tools\n\n### Related resources\n\n_No response_",
    "labels": [
      "backlog"
    ],
    "state": "closed",
    "created_at": "2024-11-17T16:23:48+00:00",
    "closed_at": "2024-12-01T07:27:53+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2060/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/2060"
  },
  {
    "number": 7417,
    "title": "[Bug] TypeError: GroupCoordinator.all_gather() got an unexpected keyword argument 'tensor_list",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n```bash\n[2025-06-21 19:55:30 DP0 TP7] TpModelWorkerClient hit an exception: Traceback (most recent call last):\n  File \"/usr/local/src/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 127, in forward_thread_func\n    self.forward_thread_func_()\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/src/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 162, in forward_thread_func_\n    self.worker.forward_batch_generation(\n  File \"/usr/local/src/sglang/python/sglang/srt/managers/tp_worker.py\", line 211, in forward_batch_generation\n    logits_output, can_run_cuda_graph = self.model_runner.forward(\n  File \"/usr/local/src/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1221, in forward\n    output = self._forward_raw(\n  File \"/usr/local/src/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1250, in _forward_raw\n    ret = self.forward_extend(\n  File \"/usr/local/src/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1189, in forward_extend\n    return self.model.forward(\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/src/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1764, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/src/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1657, in forward\n    hidden_states, residual = layer(\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/src/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1491, in forward\n    hidden_states, residual = self.layer_communicator.prepare_attn(\n  File \"/usr/local/src/sglang/python/sglang/srt/layers/communicator.py\", line 191, in prepare_attn\n    hidden_states = self._communicate_simple_fn(\n  File \"/usr/local/src/sglang/python/sglang/srt/layers/communicator.py\", line 294, in _scattered_to_tp_attn_full\n    attn_tp_all_gather(\n  File \"/usr/local/src/sglang/python/sglang/srt/layers/dp_attention.py\", line 313, in attn_tp_all_gather\n    return get_attention_tp_group().all_gather(input_, tensor_list=output_list)\nTypeError: GroupCoordinator.all_gather() got an unexpected keyword argument 'tensor_list'```\n\n\n\n\n\n\n### Reproduction\n\n## start cmd\n\n```python -m sglang.launch_server --model-path /work/models/ --port 30000 --trust-remote --host 0.0.0.0 --disable-radix-cache --init-expert-location /home/aiges/tuned/attachment_ep_statistics/prefill_in1024.json --ep-dispatch-algorithm dynamic --eplb-algorithm deepseek --deepep-config /home/aiges/tuned/tuned_8sms.json --enable-dp-lm-head --chunked-prefill-size 262144 --max-prefill-tokens 32768 --tp 16 --dp-size 2 --page-size 64 --enable-dp-attention --context-length 32768 --max-running-requests 1024 --mem-fraction-static 0.83 --enable-deepep-moe --deepep-mode normal --ep-num-redundant-experts 32 --moe-dense-tp-size 1 --disaggregation-ib-device mlx5_bond_0,mlx5_bond_1,mlx5_bond_2,mlx5_bond3 --enable-metrics --disaggregation-mode prefill --nnodes 2 --dist-init-addr xdeepseekv3-lws-mtp-main-prefill-0.xdeepseekv3-lws-mtp-main-prefill.aiservice:20102 --node-rank 0```\n\n### Environment\n\nmain branch\n",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-21T12:14:50+00:00",
    "closed_at": "2025-06-21T19:03:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7417/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7417"
  },
  {
    "number": 2662,
    "title": "[Feature] Change contribution guide",
    "body": "### Checklist\r\n\r\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [x] 2. Please use English, otherwise it will be closed.\r\n\r\n### Motivation\r\n\r\nhttps://sgl-project.github.io/references/contributor_guide.html\r\n\r\nThis has been outdated for long. We need to add guide on:\r\n\r\n1. How to run docs CI, build it locally, compile it and clean the output and make PR.\r\n2. How to do unit tests locally and add unit tests to CI.\r\n3. How to write elegant unit test following other tests.\r\n4. How to pre-commit.\r\n\r\n### Related resources\r\n\r\n_No response_",
    "labels": [
      "documentation",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-12-30T07:53:12+00:00",
    "closed_at": "2025-04-29T16:22:21+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2662/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2662"
  },
  {
    "number": 7109,
    "title": "[OAI Server] Invesitgate P99 TTFT issue of oai adapter",
    "body": "There is an observation of high P99 latency when making requests through the OpenAI-compatible API path (`http_server.py` -> `adapter.py`) under high concurrency, compared to the native `/generate` endpoint. \n\nThe goal is to identify the bottleneck within the `adapter.py` layer.",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-12T00:39:24+00:00",
    "closed_at": "2025-06-17T19:43:18+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7109/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7109"
  },
  {
    "number": 4175,
    "title": "[Bug] The radix cache affects the accuracy of the output results.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nAfter enabling the radix cache, performing a large number of serial requests will result in a small number of requests being inconsistent with those when the radix cache is not enabled. Is this consistent with expectations?\n\n### Reproduction\n\nSend 1000 requests serially using the llama or mixtral model. Restart the service and close the radixcache, then send the same 1000 requests again. About 10% of the results in these requests are inconsistent.\n\n### Environment\n\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1: NVIDIA A800-SXM4-80GB\nGPU 0,1 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda-12.4\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 470.161.03\nPyTorch: 2.5.1+cu124\nsglang: 0.4.3\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.1.6+cu124torch2.4\ntriton: 3.1.0\ntransformers: 4.49.0\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.13\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.1\ninteregular: 0.3.3\nmodelscope: 1.23.1\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.4.post1\nopenai: 1.65.2\ntiktoken: 0.9.0\nanthropic: 0.49.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    mlx5_0  mlx5_1  mlx5_2  mlx5_3  mlx5_4  mlx5_5  mlx5_6  mlx5_7  mlx5_8  CPU Affinity    NUMA Affinity\nGPU0     X      NV8     NODE    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     0-31,64-95      0\nGPU1    NV8      X      SYS     SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    32-63,96-127    1\nmlx5_0  NODE    SYS      X      NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS\nmlx5_1  PXB     SYS     NODE     X      PIX     NODE    NODE    SYS     SYS     SYS     SYS\nmlx5_2  PXB     SYS     NODE    PIX      X      NODE    NODE    SYS     SYS     SYS     SYS\nmlx5_3  NODE    SYS     NODE    NODE    NODE     X      PIX     SYS     SYS     SYS     SYS\nmlx5_4  NODE    SYS     NODE    NODE    NODE    PIX      X      SYS     SYS     SYS     SYS\nmlx5_5  SYS     PXB     SYS     SYS     SYS     SYS     SYS      X      PIX     NODE    NODE\nmlx5_6  SYS     PXB     SYS     SYS     SYS     SYS     SYS     PIX      X      NODE    NODE\nmlx5_7  SYS     NODE    SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      PIX\nmlx5_8  SYS     NODE    SYS     SYS     SYS     SYS     SYS     NODE    NODE    PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-07T09:00:36+00:00",
    "closed_at": "2025-03-07T09:47:07+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4175/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4175"
  },
  {
    "number": 7261,
    "title": "[OAI Server Refactor] [ChatCompletions & Completions] Add UTs for Tool Call and Reasoning Text Handling",
    "body": "**Points**: 1-2 days\n\n**Description**: Current the tool call handling and reasoning text logic in [`serving_chat.py`](https://github.com/sgl-project/sglang/blob/70c471a868bf505fadbfe0a041e7637a91db0365/python/sglang/srt/entrypoints/openai/serving_chat.py)\n\n**Deliverables:**\n- [ ] Complete tasks below",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-17T04:29:31+00:00",
    "closed_at": "2025-06-17T04:36:53+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7261/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7261"
  },
  {
    "number": 4385,
    "title": "[Feature] integrate flash-attention",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nsimilar with https://github.com/sgl-project/sglang/issues/4384\n\nSince SGLang now supports page sizes greater than 1 https://github.com/sgl-project/sglang/pull/4356, we should integrate flash-attention https://github.com/Dao-AILab/flash-attention\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority"
    ],
    "state": "closed",
    "created_at": "2025-03-13T10:50:36+00:00",
    "closed_at": "2025-04-22T08:25:20+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4385/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/4385"
  },
  {
    "number": 5361,
    "title": "[Feature] support merge_state in sgl-kernel",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nI have talked to @deftruth, and he will support it in the sgl-kernel today\n\n### Related resources\n\n_No response_",
    "labels": [
      "high priority",
      "collaboration",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2025-04-14T00:56:57+00:00",
    "closed_at": "2025-04-15T04:32:18+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5361/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/5361"
  },
  {
    "number": 1505,
    "title": "AWQ performance tracking",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n# Current Situation\r\n\r\n## SGLang\r\n\r\n```bash\r\n# v0.3.1.post3\r\npip install --upgrade pip\r\npip install \"sglang[all]\"\r\n\r\npip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/\r\n```\r\n\r\n```\r\npython3 -m sglang.launch_server --model hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4 --disable-radix\r\n\r\npython3 bench_serving.py --backend sglang --num-prompts 5000\r\n```\r\n\r\n```\r\n============ Serving Benchmark Result ============\r\nBackend:                                 sglang\r\nTraffic request rate:                    inf\r\nSuccessful requests:                     5000\r\nBenchmark duration (s):                  161.16\r\nTotal input tokens:                      1130466\r\nTotal generated tokens:                  971613\r\nTotal generated tokens (retokenized):    970868\r\nRequest throughput (req/s):              31.02\r\nInput token throughput (tok/s):          7014.49\r\nOutput token throughput (tok/s):         6028.81\r\n----------------End-to-End Latency----------------\r\nMean E2E Latency (ms):                   87157.00\r\nMedian E2E Latency (ms):                 87767.15\r\n---------------Time to First Token----------------\r\nMean TTFT (ms):                          52751.14\r\nMedian TTFT (ms):                        42772.56\r\nP99 TTFT (ms):                           122414.71\r\n-----Time per Output Token (excl. 1st token)------\r\nMean TPOT (ms):                          289.26\r\nMedian TPOT (ms):                        202.07\r\nP99 TPOT (ms):                           1915.65\r\n---------------Inter-token Latency----------------\r\nMean ITL (ms):                           183.11\r\nMedian ITL (ms):                         119.46\r\nP99 ITL (ms):                            686.84\r\n==================================================\r\n```\r\n\r\n## LMDeploy\r\n\r\n```bash\r\npip3 install https://github.com/zhyncs/lmdeploy-build/releases/download/bf89a01/lmdeploy-0.6.0+cu121+bf89a01-cp310-cp310-manylinux2014_x86_64.whl\r\n```\r\n\r\n```\r\npython3 -m lmdeploy serve api_server hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\r\n\r\npython3 bench_serving.py --backend lmdeploy --num-prompts 5000\r\n```\r\n\r\n```\r\n============ Serving Benchmark Result ============\r\nBackend:                                 lmdeploy\r\nTraffic request rate:                    inf\r\nSuccessful requests:                     5000\r\nBenchmark duration (s):                  133.48\r\nTotal input tokens:                      1130466\r\nTotal generated tokens:                  971613\r\nTotal generated tokens (retokenized):    976379\r\nRequest throughput (req/s):              37.46\r\nInput token throughput (tok/s):          8469.20\r\nOutput token throughput (tok/s):         7279.11\r\n----------------End-to-End Latency----------------\r\nMean E2E Latency (ms):                   68692.60\r\nMedian E2E Latency (ms):                 69067.49\r\n---------------Time to First Token----------------\r\nMean TTFT (ms):                          57053.45\r\nMedian TTFT (ms):                        56180.29\r\nP99 TTFT (ms):                           117505.87\r\n-----Time per Output Token (excl. 1st token)------\r\nMean TPOT (ms):                          67.08\r\nMedian TPOT (ms):                        64.48\r\nP99 TPOT (ms):                           161.57\r\n---------------Inter-token Latency----------------\r\nMean ITL (ms):                           222.47\r\nMedian ITL (ms):                         196.81\r\nP99 ITL (ms):                            902.97\r\n==================================================\r\n```\r\n\r\n# TODO\r\n\r\nIntegrate TurboMind GEMM into SGLang to enhance AWQ performance.\r\n\r\nhttps://github.com/internlm/turbomind\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive",
      "performance"
    ],
    "state": "closed",
    "created_at": "2024-09-24T14:33:27+00:00",
    "closed_at": "2024-11-24T01:20:38+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1505/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/1505"
  },
  {
    "number": 3245,
    "title": "[Docs] Add docs for running SGLang on AMD",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nThat has long been waiting, we should add a docs on how to run SGLang on AMD devices.\n\nhttps://github.com/sgl-project/sglang/issues/3219\nhttps://github.com/sgl-project/sglang/issues/3243\nhttps://github.com/sgl-project/sglang/issues/3200\nhttps://github.com/sgl-project/sglang/pull/3208\nhttps://github.com/sgl-project/sglang/issues/3198\n\nHere is something related. To me, I think we should add a docs on how to:\n \n1. configure environment in AMD GPU;\n2. how to install sglang;\n3. how to run a llama model;\n4. how to run deepseek V3 models.\n\n### Related resources\n\n_No response_",
    "labels": [
      "documentation",
      "good first issue",
      "help wanted",
      "amd"
    ],
    "state": "closed",
    "created_at": "2025-02-01T00:23:16+00:00",
    "closed_at": "2025-05-21T15:40:21+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3245/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3245"
  },
  {
    "number": 2504,
    "title": "[Feature] Add Math in our CI",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nOne of my friends told me that SGLang Engine's performance on Math is abnormally a bit lower. We will find this out, benchmarking SGLang and other engines' performance on Math (use GPT-4 to evaluate). And, ultimately, we will add a CI test for Math which runs daily.\n\n### Related resources\n\nNo such.",
    "labels": [
      "enhancement",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-12-17T22:37:36+00:00",
    "closed_at": "2024-12-30T06:52:10+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2504/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/2504"
  },
  {
    "number": 7557,
    "title": "[PD Bug]  transfer  exception and deepep timeout",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n<img width=\"841\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fe6d7434-ad09-452b-a030-c273da1b3ddd\" />\n\n@ShangmingCai \n\n### Reproduction\n\nprefill H20\n\np2D4:\n\n```\n python -m sglang.launch_server --model-path /work/models/ --port 30000 --trust-remote --host 0.0.0.0 --disable-radix-cache --init-expert-location /home/aiges/tuned/attachment_ep_statistics/prefill_in4096.json --ep-dispatch-algorithm dynamic --eplb-algorithm deepseek --deepep-config /home/aiges/tuned/tuned_8sms.json  --enable-dp-lm-head --chunked-prefill-size 262144 --tp 16 --dp-size 4 --enable-dp-attention --context-length 8192 --max-running-requests 1024 --mem-fraction-static 0.8 --enable-deepep-moe --deepep-mode normal --ep-num-redundant-experts 32 --moe-dense-tp-size 1 --disaggregation-ib-device mlx5_bond_0,mlx5_bond_1,mlx5_bond_2,mlx5_bond3 --enable-metrics --disaggregation-mode prefill --nnodes 2 --dist-init-addr xdeepseekv3-lws-mtp-main-prefill-0.xdeepseekv3-lws-mtp-main-prefill.aiservice:20102 --node-rank 0\n```\n\n### Environment\n\nsglang latest\nmain branch",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-26T08:41:08+00:00",
    "closed_at": "2025-06-30T11:54:35+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7557/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7557"
  },
  {
    "number": 7102,
    "title": "[OAI Server Refactor] Core Utility Endpoints",
    "body": "Points: 2-3 days\n\nDescription: Implement the core utility endpoints needed by all OpenAI-compatible clients.\n\nDeliverables:\n\n- Implement `/v1/models` endpoint to list available models\n- Add engine lifecycle management in the lifespan handler, following `http_server.py` in `python/sglang/srt/http_server.py`\n- Add metrics middleware\n- Add all openai endpoints, skip implementation for now\n\nTesting:\n\n- Verify proper model listing\n- Ensure metrics are correctly exposed",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-11T23:59:58+00:00",
    "closed_at": "2025-06-17T04:07:48+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7102/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7102"
  },
  {
    "number": 4748,
    "title": "[Feature] beat torch compile",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nas titled\n\nLast year and in the first few months of this year, a significant part of my work focused on removing vLLM dependency. Many reliable teammates joined in this process, and we successfully removed the vLLM dependency on the NVIDIA platform for SGLang. Next, I will co-lead progress on beat torch compile. Past experience shows that torch compile is effective - we just need to write some simple torch ops and let torch compile handle the rest. However, in actual production serving, it is not as smooth as expected - for example, slow startup even with cache enabled, compatibility issues when upgrading torch versions leading to previous features breaking in new versions. We need to profile, benchmark, rewrite the bottleneck ops with CUDA/CUTLASS and ensure that **performance without using torch compile can surpass performance with enable torch compile**. Currently [sgl-kernel](https://github.com/sgl-project/sglang/tree/main/sgl-kernel) has secured a size of **500 MB**, I believe everything is ready and now we just need everyone to collaborate together. Cheers!\n\n### Related resources\n\n_No response_",
    "labels": [
      "good first issue",
      "help wanted",
      "high priority",
      "collaboration",
      "performance"
    ],
    "state": "closed",
    "created_at": "2025-03-25T06:18:28+00:00",
    "closed_at": "2025-05-26T16:55:12+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4748/reactions",
      "total_count": 15,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 15,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/4748"
  }
]
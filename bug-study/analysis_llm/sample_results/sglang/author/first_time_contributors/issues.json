[
  {
    "number": 6486,
    "title": "[Feature] Interrupt running requests when updating weights for RL",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nHi SGLang community,\n\nI'm using SGLang to build an asynchronous RL system for LLM reasoning. To obtain a high generation throughput, the inference server is continuously filled with new requests, so when a new parameter arrives, SGLang should either wait for the current longest sequence or interrupt all ongoing requests and continue generation with the latest parameter. It's straightforward that the later case results in better hardware utilization.\n\nBy \"interrupting requests\", I mean returning the unfinished requests back to the client. The client will submit it again. Although it's also possible to pause the requests within SGLang, the implementation may be more complicated.\n\nImplementation-wise, I plan to add an additional boolean entry called `allow_interrupt` on the `UpdateWeightsFromDiskInput` dataclass. If it's true, SGLang returns all unfinished requests before updating weights, thus allows flushing radix cache. Otherwise, SGLang remains the current behavior.\n\nI wonder whether such a feature is desired for the library. Looking forward to hearing from you guys. :)\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "open",
    "created_at": "2025-05-21T06:41:21+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6486/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6486"
  },
  {
    "number": 4594,
    "title": "[Bug] cannot load prequantized model with scalar weight scale",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nRight now after loading the model and converting the weight scale to channel wise, there's an implicit assumption that the weight scale tensors in model weight is 1-D tensor. This is not the case for modelopt-quantized FP8 in fp8 cutlass supported hardware, since QKVParalleLinear will go through a requantization to the same scale.\n\n### Reproduction\n\n```python\nimport sglang as sgl\n\nif __name__ == '__main__':\n    llm = sgl.Engine(\n        model_path=\"nvidia/Llama-3.1-8B-Instruct-FP8\",\n        quantization=\"modelopt\",\n        revision=\"13858565416dbdc0b4e7a4a677fadfbd5b9e5bb9\",\n        log_level=\"debug\",\n    )\n```\n\nError:\n```\n[2025-03-19 20:37:24 TP0] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/home/jobuser/sglang/python/sglang/srt/managers/scheduler.py\", line 1809, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/home/jobuser/sglang/python/sglang/srt/managers/scheduler.py\", line 227, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/home/jobuser/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/home/jobuser/sglang/python/sglang/srt/managers/tp_worker.py\", line 74, in __init__\n    self.model_runner = ModelRunner(\n  File \"/home/jobuser/sglang/python/sglang/srt/model_executor/model_runner.py\", line 168, in __init__\n    self.initialize(min_per_gpu_memory)\n  File \"/home/jobuser/sglang/python/sglang/srt/model_executor/model_runner.py\", line 178, in initialize\n    self.load_model()\n  File \"/home/jobuser/sglang/python/sglang/srt/model_executor/model_runner.py\", line 383, in load_model\n    self.model = get_model(\n  File \"/home/jobuser/sglang/python/sglang/srt/model_loader/__init__.py\", line 22, in get_model\n    return loader.load_model(\n  File \"/home/jobuser/sglang/python/sglang/srt/model_loader/loader.py\", line 382, in load_model\n    quant_method.process_weights_after_loading(module)\n  File \"/home/jobuser/sglang/python/sglang/srt/layers/quantization/modelopt_quant.py\", line 169, in process_weights_after_loading\n    max_w_scale = convert_to_channelwise(max_w_scale, layer.logical_widths)\n  File \"/home/jobuser/sglang/python/sglang/srt/layers/quantization/utils.py\", line 81, in convert_to_channelwise\n    weight_scale_channel[start:end, :] = weight_scale[idx]\nIndexError: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number\n```\n\n### Environment\n\n```\nPython: 3.10.14 (main, Jul 14 2024, 22:24:12) [GCC 11.2.0]\nCUDA available: True\nGPU 0: NVIDIA H100 80GB HBM3\nGPU 0 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.6, V12.6.77\nCUDA Driver Version: 550.54.15\nPyTorch: 2.5.1+cu124\nsglang: 0.4.4.post1\nsgl_kernel: 0.0.5.post3\nflashinfer: 0.2.3\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.14\nfastapi: 0.115.11\nhf_transfer: 0.1.9\nhuggingface_hub: 0.29.3\ninteregular: 0.3.3\nmodelscope: 1.24.0\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.3.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.66.3\ntiktoken: 0.9.0\nanthropic: 0.49.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PIX     SYS     SYS     SYS     SYS     SYS     0-63,128-191    0               N/A\nNIC0    PIX      X      SYS     SYS     SYS     SYS     SYS\nNIC1    SYS     SYS      X      PIX     SYS     SYS     SYS\nNIC2    SYS     SYS     PIX      X      SYS     SYS     SYS\nNIC3    SYS     SYS     SYS     SYS      X      SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     SYS      X      SYS\nNIC5    SYS     SYS     SYS     SYS     SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n\n\nulimit soft: 10000000\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-19T20:44:35+00:00",
    "closed_at": "2025-03-22T07:47:54+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4594/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/4594"
  },
  {
    "number": 2468,
    "title": "\"GET / HTTP/1.1\" 404 Not Found",
    "body": "I try to follow your \"quick start\" and launch a server, with following code:\r\n\r\n```\r\npython -m sglang.launch_server --model-path mistralai/Mistral-7B-Instruct-v0.1 \\\r\n--port 30000 --host 0.0.0.0\r\n```\r\n\r\nUnfortunately I encounter some error\ud83d\ude2d\r\n\r\n```\r\n[2024-12-13 01:44:59 TP0] Load weight end. type=MistralForCausalLM, dtype=torch.bfloat16, avail mem=9.69 GB\r\n[2024-12-13 01:44:59 TP0] Memory pool end. avail mem=2.44 GB\r\n[2024-12-13 01:44:59 TP0] Capture cuda graph begin. This can take up to several minutes.\r\n[2024-12-13 01:45:01 TP0] Capture cuda graph end. Time elapsed: 2.19 s\r\n[2024-12-13 01:45:02 TP0] max_total_num_tokens=56524, max_prefill_tokens=16384, max_running_requests=2049, context_len=32768\r\n[2024-12-13 01:45:02] INFO:     Started server process [3831493]\r\n[2024-12-13 01:45:02] INFO:     Waiting for application startup.\r\n[2024-12-13 01:45:02] INFO:     Application startup complete.\r\n[2024-12-13 01:45:02] INFO:     Uvicorn running on http://0.0.0.0:30000 (Press CTRL+C to quit)\r\n[2024-12-13 01:45:26] INFO:     127.0.0.1:40418 - \"GET / HTTP/1.1\" 404 Not Found\r\n[2024-12-13 01:47:34] Initialization failed. warmup error: Traceback (most recent call last):\r\n  File \"/home/sglang/python/sglang/srt/server.py\", line 621, in _wait_and_warmup\r\n    assert res.status_code == 200, f\"{res=}, {res.text=}\"\r\nAssertionError: res=<Response [502]>, res.text=''\r\n\r\nKilled\r\n```\r\nHow can I fix it? Thanks!",
    "labels": [],
    "state": "closed",
    "created_at": "2024-12-12T17:54:02+00:00",
    "closed_at": "2025-01-17T15:13:00+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/2468/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/2468"
  },
  {
    "number": 1761,
    "title": "[Feature] Multi options",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nI want to use sgl.gen to select multiple options from the candidate selection, does it support it?\n\n### Related resources\n\n_No response_",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-10-23T01:55:58+00:00",
    "closed_at": "2024-12-23T00:17:19+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1761/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1761"
  },
  {
    "number": 3892,
    "title": "[Bug] H20 8 gpu x 2 with --enable-dp-attention occurred CUDA error: an illegal memory access",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI use two H20 with 8 gpus and 8 IB device(mlx5_1 to mlx5_8)  on each node to test DeepSeep-R1 with --enable-dp-attention.\n\nError log \n\n[2025-02-26 13:18:19 DP3 TP3] Prefill batch. #new-seq: 1, #new-token: 4096, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 7\n[2025-02-26 13:18:19 DP0 TP0] Prefill batch. #new-seq: 1, #new-token: 4096, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 7\n[2025-02-26 13:18:19 DP1 TP1] Prefill batch. #new-seq: 1, #new-token: 4096, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.03, #running-req: 0, #queue-req: 8\n[2025-02-26 13:18:21 DP3 TP3] TpModelWorkerClient hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 109, in forward_thread_func\n    self.forward_thread_func_()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 140, in forward_thread_func_\n    logits_output, next_token_ids = self.worker.forward_batch_generation(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 164, in forward_batch_generation\n    logits_output = self.model_runner.forward(forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 796, in forward\n    return self.forward_extend(forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 761, in forward_extend\n    return self.model.forward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 874, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 835, in forward\n    hidden_states, residual = layer(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 773, in forward\n    hidden_states = self.self_attn(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 527, in forward\n    and forward_batch.extend_prefix_lens.sum() == 0\nRuntimeError: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 108, in forward_thread_func\n    with torch.get_device_module(self.device).stream(self.forward_stream):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 595, in __exit__\n    torch.cuda.set_stream(self.src_prev_stream)  # type: ignore[arg-type]\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/utils.py\", line 962, in _patched_set_stream\n    prev_set_stream(stream)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 636, in set_stream\n    _set_stream_by_id(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 618, in _set_stream_by_id\n    torch._C._cuda_setStream(\nRuntimeError: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\n[rank3]:[E226 13:18:21.063621914 ProcessGroupNCCL.cpp:1595] [PG ID 2 PG GUID 3 Rank 3] Process group watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nException raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f03ad6b9446 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f03ad6636e4 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f03ad7a5a18 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7f0363625726 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0xa0 (0x7f036362a3f0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7f0363631b5a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f036363361d in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x145c0 (0x7f03af2c65c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)\nframe #8: <unknown function> + 0x94ac3 (0x7f03b014bac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #9: <unknown function> + 0x126850 (0x7f03b01dd850 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n\nterminate called after throwing an instance of 'c10::DistBackendError'\n  what():  [PG ID 2 PG GUID 3 Rank 3] Process group watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nException raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f03ad6b9446 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f03ad6636e4 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f03ad7a5a18 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7f0363625726 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0xa0 (0x7f036362a3f0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x7f0363631b5a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f036363361d in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x145c0 (0x7f03af2c65c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)\nframe #8: <unknown function> + 0x94ac3 (0x7f03b014bac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #9: <unknown function> + 0x126850 (0x7f03b01dd850 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n\nException raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f03ad6b9446 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0xe4271b (0x7f03632a071b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\nframe #2: <unknown function> + 0x145c0 (0x7f03af2c65c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)\nframe #3: <unknown function> + 0x94ac3 (0x7f03b014bac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)\nframe #4: <unknown function> + 0x126850 (0x7f03b01dd850 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n\nFatal Python error: Aborted\n\nThread 0x00007efe63fc7640 (most recent call first):\n[2025-02-26 13:18:21 DP4 TP4] TpModelWorkerClient hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 109, in forward_thread_func\n    self.forward_thread_func_()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 140, in forward_thread_func_\n    logits_output, next_token_ids = self.worker.forward_batch_generation(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 164, in forward_batch_generation\n    logits_output = self.model_runner.forward(forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 796, in forward\n    return self.forward_extend(forward_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 761, in forward_extend\n    return self.model.forward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 874, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 835, in forward\n    hidden_states, residual = layer(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 773, in forward\n    hidden_states = self.self_attn(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 527, in forward\n    and forward_batch.extend_prefix_lens.sum() == 0\nRuntimeError: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 108, in forward_thread_func\n    with torch.get_device_module(self.device).stream(self.forward_stream):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 595, in __exit__\n    torch.cuda.set_stream(self.src_prev_stream)  # type: ignore[arg-type]\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/utils.py\", line 962, in _patched_set_stream\n    prev_set_stream(stream)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 636, in set_stream\n    _set_stream_by_id(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 618, in _set_stream_by_id\n    torch._C._cuda_setStream(\nRuntimeError: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n### Reproduction\n\nnode0 server:\npython3 -m sglang.launch_server --model-path /mnt/model/DeepSeek-R1/ --tp 16 --dist-init-addr 192.168.81.54:20000 --nnodes 2 --node-rank 0 --trust-remote-code --enable-dp-attention --host 0.0.0.0 --port 40000\n\nnode1 server:\npython3 -m sglang.launch_server --model-path /mnt/model/DeepSeek-R1/ --tp 16 --dist-init-addr 192.168.81.54:20000 --nnodes 2 --node-rank 1 --trust-remote-code --enable-dp-attention --host 0.0.0.0 --port 40000\n\ntest cmd:\npython3 -m sglang.bench_serving --backend sglang --dataset-path /workspace/dataset/ShareGPT_V3_unfiltered_cleaned_split.json --dataset-name random --random-range-ratio 1 --num-prompt 128 --max-concurrency 128 --random-input 16384 --random-output 1024 --host 0.0.0.0 --port 40000 --output-file deepseek_v3_2x8xh200_FP8_online_output.jsonl\n\n### Environment\n\nlmsysorg/sglang:v0.4.3.post2-cu124\n\nCUDA Driver Version: 535.216.03",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-26T14:00:36+00:00",
    "closed_at": "2025-02-27T03:31:04+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3892/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3892"
  },
  {
    "number": 3201,
    "title": "[Bug] min_p_sampling_from_probs() related crashes",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nsglang srt crashes with log:\n```\n[2025-01-28 17:58:17 TP0] TpModelWorkerClient hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 109, in forward_thread_func\n    self.forward_thread_func_()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 140, in forward_thread_func_\n    logits_output, next_token_ids = self.worker.forward_batch_generation(\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 171, in forward_batch_generation\n    next_token_ids = self.model_runner.sample(logits_output, model_worker_batch)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 808, in sample\n    next_token_ids = self.sampler(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/sampler.py\", line 86, in forward\n    batch_next_token_ids, success = min_p_sampling_from_probs(\nValueError: not enough values to unpack (expected 2, got 1)\n```\n\nIn the code, `min_p_sampling_from_probs()` indeed only returns 1 value:\nhttps://github.com/sgl-project/sglang/blob/9f635ea50de920aa507f486daafba26a5b837574/sgl-kernel/src/sgl-kernel/ops/__init__.py#L455 \n\n### Reproduction\n\nThe model is Deepseek-R1. Repro request to be added.\n\n### Environment\n\nLatest [v0.4.2-cu124-srt](https://hub.docker.com/layers/lmsysorg/sglang/v0.4.2-cu124-srt/images/sha256-abf11f966511bbf2cb5aa91c1b809ca71d0e94d6024e6b1cc4317b0429d1ed80) docker image is used.",
    "labels": [],
    "state": "closed",
    "created_at": "2025-01-29T02:46:51+00:00",
    "closed_at": "2025-02-02T19:50:45+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3201/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/3201"
  },
  {
    "number": 929,
    "title": "Inference Llama3-70b has an AssertionError",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n\n### Describe the bug\n\n![image](https://github.com/user-attachments/assets/ddf162df-5f4c-4aa7-929a-f093aa672328)\r\nI run sglang.server, but get an assertion error.\n\n### Reproduction\n\n`python3 -m sglang.launch_server --model-path ./models/Meta-Llama-3-70B-Instruct --host 0.0.0.0 --port 30000 --tp 8 --mem-fraction-static 0.7 --chunked-prefill-size 8192`\n\n### Environment\n\n```Shell\nPython: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA A40\r\nCUDA_HOME: /usr/local/cuda\r\nNVCC: Cuda compilation tools, release 12.3, V12.3.107\r\nCUDA Driver Version: 535.161.08\r\n535.161.08\r\n535.161.08\r\n535.161.08\r\n535.161.08\r\n535.161.08\r\n535.161.08\r\n535.161.08\r\nPyTorch: 2.3.1+cu121\r\nsglang: 0.2.7\r\nflashinfer: 0.1.3+cu121torch2.3\r\nrequests: 2.32.3\r\ntqdm: 4.66.4\r\nnumpy: 1.24.4\r\naiohttp: 3.9.1\r\nfastapi: 0.110.1\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.24.5\r\ninteregular: 0.3.3\r\npackaging: 23.2\r\nPIL: 9.5.0\r\npsutil: 5.9.4\r\npydantic: 2.7.0\r\nuvicorn: 0.29.0\r\nuvloop: 0.19.0\r\nzmq: 25.1.2\r\nvllm: 0.5.3.post1\r\nopenai: 1.37.1\r\nanthropic: 0.32.0\r\nNVIDIA Topology: \r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PHB     PHB     PHB     PHB     PHB     PHB     PHB     0-85    0               N/A\r\nGPU1    PHB      X      PHB     PHB     PHB     PHB     PHB     PHB     0-85    0               N/A\r\nGPU2    PHB     PHB      X      PHB     PHB     PHB     PHB     PHB     0-85    0               N/A\r\nGPU3    PHB     PHB     PHB      X      PHB     PHB     PHB     PHB     0-85    0               N/A\r\nGPU4    PHB     PHB     PHB     PHB      X      PHB     PHB     PHB     0-85    0               N/A\r\nGPU5    PHB     PHB     PHB     PHB     PHB      X      PHB     PHB     0-85    0               N/A\r\nGPU6    PHB     PHB     PHB     PHB     PHB     PHB      X      PHB     0-85    0               N/A\r\nGPU7    PHB     PHB     PHB     PHB     PHB     PHB     PHB      X      0-85    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nulimit soft: 1048576\n```\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-08-05T08:18:19+00:00",
    "closed_at": "2024-09-22T13:04:11+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/929/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/929"
  },
  {
    "number": 3810,
    "title": "Why can't I see NCCL logs even though I set NCCL_DEBUG=INFO? And how can I see NCCL run logs?",
    "body": "<img width=\"1636\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fb9d89e9-2822-44ee-beb3-6fe271ea9030\" />",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-02-24T08:38:32+00:00",
    "closed_at": "2025-04-27T00:20:04+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3810/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3810"
  },
  {
    "number": 152,
    "title": "unable to install uvloop in windows (dependency)",
    "body": "Getting this error\r\n\r\npip install sglang[all]\r\nRequirement already satisfied: sglang[all] in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (0.1.11)\r\nRequirement already satisfied: requests in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from sglang[all]) (2.31.0)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from requests->sglang[all]) (3.3.2)\r\nRequirement already satisfied: idna<4,>=2.5 in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from requests->sglang[all]) (3.6)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from requests->sglang[all]) (2.1.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from requests->sglang[all]) (2023.11.17)\r\nCollecting anthropic (from sglang[all])\r\n  Using cached anthropic-0.15.0-py3-none-any.whl.metadata (15 kB)\r\nRequirement already satisfied: numpy in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from sglang[all]) (1.24.4)\r\nRequirement already satisfied: openai>=1.0 in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from sglang[all]) (1.6.1)\r\nRequirement already satisfied: aiohttp in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from sglang[all]) (3.9.1)\r\nRequirement already satisfied: fastapi in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from sglang[all]) (0.109.2)\r\nRequirement already satisfied: psutil in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from sglang[all]) (5.9.7)\r\nCollecting rpyc (from sglang[all])\r\n  Using cached rpyc-5.3.1-py3-none-any.whl (74 kB)\r\nRequirement already satisfied: torch in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from sglang[all]) (2.1.2)\r\nCollecting uvloop (from sglang[all])\r\n  Using cached uvloop-0.19.0.tar.gz (2.3 MB)\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  \u00d7 Getting requirements to build wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [15 lines of output]\r\n      Traceback (most recent call last):\r\n        File \"C:\\Users\\Keshav S\\anaconda3\\envs\\llm-workspace\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\r\n          main()\r\n        File \"C:\\Users\\Keshav S\\anaconda3\\envs\\llm-workspace\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\r\n          json_out['return_val'] = hook(**hook_input['kwargs'])\r\n        File \"C:\\Users\\Keshav S\\anaconda3\\envs\\llm-workspace\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\r\n          return hook(config_settings)\r\n        File \"C:\\Users\\Keshav S\\AppData\\Local\\Temp\\pip-build-env-xnl04q07\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 325, in get_requires_for_build_wheel\r\n          return self._get_build_requires(config_settings, requirements=['wheel'])\r\n        File \"C:\\Users\\Keshav S\\AppData\\Local\\Temp\\pip-build-env-xnl04q07\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 295, in _get_build_requires\r\n          self.run_setup()\r\n        File \"C:\\Users\\Keshav S\\AppData\\Local\\Temp\\pip-build-env-xnl04q07\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 311, in run_setup\r\n          exec(code, locals())\r\n        File \"<string>\", line 8, in <module>\r\n      RuntimeError: uvloop does not support Windows at the moment\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: subprocess-exited-with-error\r\n\r\n\u00d7 Getting requirements to build wheel did not run successfully.\r\n\u2502 exit code: 1\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This error originates from a subprocess, and is likely not a problem with pip.",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-02-06T18:02:46+00:00",
    "closed_at": "2024-07-25T06:32:08+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/152/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/152"
  },
  {
    "number": 6723,
    "title": "[Bug] sglang Engine can not work with async ray actor",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThe method async_generate in engine is designed to use with async method, but it can not works in ray actor.\n\nThis problem blocks server-based rollout in https://github.com/volcengine/verl/issues/1721 \n\n\nYou can reproduce it using script from https://gist.github.com/chenhaiq/a28560a53701d869dd08dd0852d9b379, and output below error:\n\n\n```\nINFO 05-29 02:39:18 [__init__.py:239] Automatically detected platform cuda.\n2025-05-29 02:39:22,489\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265\n\nCalling ActorA.method_a\n(pid=245071) INFO 05-29 02:39:29 [__init__.py:239] Automatically detected platform cuda.\nTraceback (most recent call last):\n  File \"/root/verl/verl/async_sglang_engine.py\", line 59, in <module>\n    asyncio.run(main())\n  File \"/usr/lib/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/root/verl/verl/async_sglang_engine.py\", line 52, in main\n    final_output = await result_ref_a\nray.exceptions.RayTaskError(ValueError): ray::ActorA.method_a() (pid=245071, ip=192.168.111.50, actor_id=33c4d273bee9db7120dca78a01000000, repr=<async_sglang_engine.ActorA object at 0x7fc970577c70>)\n  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/root/verl/verl/async_sglang_engine.py\", line 42, in method_a\n    return await test_sglang_spmd()\n  File \"/root/verl/verl/async_sglang_engine.py\", line 20, in test_sglang_spmd\n    llm = Engine(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/entrypoints/engine.py\", line 125, in __init__\n    tokenizer_manager, scheduler_info = _launch_subprocesses(\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/entrypoints/engine.py\", line 528, in _launch_subprocesses\n    _set_envs_and_config(server_args)\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/entrypoints/engine.py\", line 502, in _set_envs_and_config\n    signal.signal(signal.SIGCHLD, sigchld_handler)\n  File \"/usr/lib/python3.10/signal.py\", line 56, in signal\n    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))\nValueError: signal only works in main thread of the main interpreter\n```\n\nThere are 2 existing similar issues:\nhttps://github.com/sgl-project/sglang/issues/4319\nhttps://github.com/sgl-project/sglang/issues/2536\n\n\n\n### Reproduction\n\nhttps://gist.github.com/chenhaiq/a28560a53701d869dd08dd0852d9b379\n\n### Environment\n\nPython: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA A800-SXM4-80GB\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.6, V12.6.20\nCUDA Driver Version: 535.129.03\nPyTorch: 2.6.0+cu124\nsglang: 0.4.6.post4\nsgl_kernel: 0.1.2.post1\nflashinfer_python: 0.2.5\ntriton: 3.2.0\ntransformers: 4.51.1\ntorchao: 0.11.0\nnumpy: 1.26.4\naiohttp: 3.10.1\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.30.2\ninteregular: 0.3.3\nmodelscope: 1.26.0\norjson: 3.10.16\noutlines: 0.1.11\npackaging: 23.2\npsutil: 6.0.0\npydantic: 2.9.0\npython-multipart: 0.0.20\npyzmq: 26.1.0\nuvicorn: 0.34.2\nuvloop: 0.21.0\nvllm: 0.8.4\nxgrammar: 0.1.19\nopenai: 1.75.0\ntiktoken: 0.9.0\nanthropic: 0.52.0\nlitellm: 1.71.1\ndecord: 0.6.0\nNVIDIA Topology:\n\tGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tNV8\tNV8\tNV8\tNV8\tNV8\tNV8\tNV8\tPXB\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU1\tNV8\t X \tNV8\tNV8\tNV8\tNV8\tNV8\tNV8\tPXB\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU2\tNV8\tNV8\t X \tNV8\tNV8\tNV8\tNV8\tNV8\tSYS\tPXB\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU3\tNV8\tNV8\tNV8\t X \tNV8\tNV8\tNV8\tNV8\tSYS\tPXB\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU4\tNV8\tNV8\tNV8\tNV8\t X \tNV8\tNV8\tNV8\tSYS\tSYS\tPXB\tSYS\t32-63,96-104\t1\t\tN/A\nGPU5\tNV8\tNV8\tNV8\tNV8\tNV8\t X \tNV8\tNV8\tSYS\tSYS\tPXB\tSYS\t32-63,96-104\t1\t\tN/A\nGPU6\tNV8\tNV8\tNV8\tNV8\tNV8\tNV8\t X \tNV8\tSYS\tSYS\tSYS\tPXB\t32-63,96-104\t1\t\tN/A\nGPU7\tNV8\tNV8\tNV8\tNV8\tNV8\tNV8\tNV8\t X \tSYS\tSYS\tSYS\tPXB\t32-63,96-104\t1\t\tN/A\nNIC0\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tSYS\tSYS\tSYS\nNIC1\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tSYS\tSYS\nNIC2\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\t X \tSYS\nNIC3\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\t X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_1\n  NIC1: mlx5_2\n  NIC2: mlx5_3\n  NIC3: mlx5_4\n\n\nulimit soft: 1048576",
    "labels": [],
    "state": "open",
    "created_at": "2025-05-29T02:59:14+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6723/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6723"
  },
  {
    "number": 6363,
    "title": "[Bug]  No module named 'cuda.bindings'",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\npython3  -m sglang.launch_server --model-path Qwen/Qwen2.5-VL-7B-Instruct\n/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/launch_server.py\", line 6, in <module>\n    from sglang.srt.entrypoints.http_server import launch_server\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/entrypoints/http_server.py\", line 49, in <module>\n    from sglang.srt.entrypoints.engine import _launch_subprocesses\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/entrypoints/engine.py\", line 42, in <module>\n    from sglang.srt.managers.data_parallel_controller import (\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/data_parallel_controller.py\", line 28, in <module>\n    from sglang.srt.managers.io_struct import (\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/io_struct.py\", line 31, in <module>\n    from sglang.srt.managers.schedule_batch import BaseFinishReason\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/schedule_batch.py\", line 48, in <module>\n    from sglang.srt.configs.model_config import ModelConfig\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/configs/model_config.py\", line 26, in <module>\n    from sglang.srt.layers.quantization import QUANTIZATION_METHODS\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/layers/quantization/__init__.py\", line 56, in <module>\n    from sglang.srt.layers.quantization.blockwise_int8 import BlockInt8Config\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/layers/quantization/blockwise_int8.py\", line 21, in <module>\n    from sglang.srt.layers.quantization.utils import is_layer_skipped\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/layers/quantization/utils.py\", line 8, in <module>\n    from sglang.srt.layers.quantization.fp8_kernel import scaled_fp8_quant\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/layers/quantization/fp8_kernel.py\", line 26, in <module>\n    from sglang.srt.layers.quantization.deep_gemm import _ENABLE_JIT_DEEPGEMM\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/layers/quantization/deep_gemm.py\", line 16, in <module>\n    import deep_gemm\n  File \"/usr/local/lib/python3.10/dist-packages/deep_gemm/__init__.py\", line 3, in <module>\n    from . import jit\n  File \"/usr/local/lib/python3.10/dist-packages/deep_gemm/jit/__init__.py\", line 1, in <module>\n    from .compiler import get_nvcc_compiler, build, NVCCCompiler, NVRTCCompiler\n  File \"/usr/local/lib/python3.10/dist-packages/deep_gemm/jit/compiler.py\", line 10, in <module>\n    import cuda.bindings\nModuleNotFoundError: No module named 'cuda.bindings'\n\n\n### Reproduction\n\npython3  -m sglang.launch_server --model-path Qwen/Qwen2.5-VL-7B-Instruct\n\n### Environment\n\njetson orin 64G\ncuda 12.6\ndocker build by jetson-containers\ndocker image:sglang:r36.4-cu126-22.04",
    "labels": [],
    "state": "open",
    "created_at": "2025-05-17T00:46:46+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/6363/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/6363"
  },
  {
    "number": 885,
    "title": "[Bug] Fail to build from Docker",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [X] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n\n### Describe the bug\n\nBuilding from the Dockerfile, got gpg-key error like:\r\n\r\n```\r\nErr:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\r\n  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\r\n```\r\n\r\nwhile executing:\r\n\r\n```\r\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\r\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\r\n    && apt-get update -y \\\r\n    && apt-get install -y ccache software-properties-common \\\r\n    && add-apt-repository ppa:deadsnakes/ppa \\\r\n    && apt-get update -y \\\r\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv python3-pip \\\r\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\r\n    && python3 --version \\\r\n    && python3 -m pip --version \\\r\n    && rm -rf /var/lib/apt/lists/* \\\r\n    && apt-get clean\r\n```\r\n\r\nIt may due to [Cuda Repo Signing Key Change is causing package repo update failures](https://gitlab.com/nvidia/container-images/cuda/-/issues/158)\r\n\r\nThis problem could be fixed by chanaging the value of `FROM` in Dockerfile to `nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04` (It's validated on my env and `CUDA_VERSION` is set to `12.2.2`)\n\n### Reproduction\n\n```\r\ndocker build --pull --network=host -t sglang0.28 .\r\n```\n\n### Environment\n\n```Shell\nNothing\n```\n",
    "labels": [
      "await-response"
    ],
    "state": "closed",
    "created_at": "2024-08-02T06:10:22+00:00",
    "closed_at": "2024-08-06T09:47:09+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/885/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/885"
  },
  {
    "number": 7773,
    "title": "[Router] How to ensure that the kvcache of worker and router are consistent when use cacheaware poliy",
    "body": "I've noticed that when a new request arrives, the radix tree inserts text. If it reaches the maximum size, the tree deletes some nodes. However, I don't see the worker publishing kv events to the route, which is necessary to maintain consistency between the worker and the route and ensure more accurate kv matching. If I missed anything, please point out my mistakes. ",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-04T08:32:16+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7773/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7773"
  },
  {
    "number": 1386,
    "title": "[Bug] requests.exceptions.JSONDecodeError: ",
    "body": "### Checklist\n\n- [X] 1. I have searched related issues but cannot get the expected help.\n- [X] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [X] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nAfter installing the newest version, it encouters a bug: when I launch a sglang run, it raises two exceptions in a few seconds.\r\n```\r\n(vllm) user@node14:~$  python -m sglang.launch_server --model-path models/Qwen2-72B-Instruct --port 30000 --tp 8\r\n[14:39:01] server_args=ServerArgs(model_path='models/Qwen2-72B-Instruct', tokenizer_path='models/Qwen2-72B-Instruct', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', dtype='auto', kv_cache_dtype='auto', trust_remote_code=False, context_length=None, quantization=None, served_model_name='models/Qwen2-72B-Instruct', chat_template=None, is_embedding=False, host='127.0.0.1', port=30000, additional_ports=[30001, 30002, 30003, 30004], mem_fraction_static=0.83, max_running_requests=None, max_num_reqs=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, tp_size=8, stream_interval=1, random_seed=420923413, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, api_key=None, file_storage_pth='SGLang_storage', dp_size=1, load_balance_method='round_robin', disable_flashinfer=False, disable_flashinfer_sampling=False, disable_radix_cache=False, disable_regex_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_disk_cache=False, disable_custom_all_reduce=False, enable_mixed_chunk=False, enable_torch_compile=False, enable_p2p_check=False, enable_mla=False, triton_attention_reduce_in_fp32=False, nccl_init_addr=None, nnodes=1, node_rank=None)\r\n[14:39:07 TP3] Init nccl begin.\r\n[14:39:07 TP0] Init nccl begin.\r\n[14:39:07 TP5] Init nccl begin.\r\n[14:39:07 TP7] Init nccl begin.\r\n[14:39:07 TP6] Init nccl begin.\r\n[14:39:07 TP2] Init nccl begin.\r\n[14:39:07 TP4] Init nccl begin.\r\n[14:39:07 TP1] Init nccl begin.\r\n[14:39:21 TP2] Load weight begin. avail mem=76.63 GB\r\n[14:39:21 TP6] Load weight begin. avail mem=76.63 GB\r\n[14:39:21 TP4] Load weight begin. avail mem=76.63 GB\r\n[14:39:21 TP5] Load weight begin. avail mem=76.63 GB\r\n[14:39:21 TP0] Load weight begin. avail mem=76.73 GB\r\n[14:39:21 TP3] Load weight begin. avail mem=76.63 GB\r\n[14:39:21 TP1] Load weight begin. avail mem=76.63 GB\r\n[14:39:21 TP7] Load weight begin. avail mem=77.10 GB\r\nLoading safetensors checkpoint shards:   0% Completed | 0/37 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:   3% Completed | 1/37 [00:00<00:07,  4.61it/s]\r\nLoading safetensors checkpoint shards:   5% Completed | 2/37 [00:00<00:08,  4.28it/s]\r\nLoading safetensors checkpoint shards:   8% Completed | 3/37 [00:00<00:08,  4.15it/s]\r\nLoading safetensors checkpoint shards:  11% Completed | 4/37 [00:00<00:08,  4.02it/s]\r\nLoading safetensors checkpoint shards:  14% Completed | 5/37 [00:01<00:07,  4.06it/s]\r\nLoading safetensors checkpoint shards:  16% Completed | 6/37 [00:01<00:07,  4.22it/s]\r\nLoading safetensors checkpoint shards:  19% Completed | 7/37 [00:01<00:06,  4.31it/s]\r\nLoading safetensors checkpoint shards:  22% Completed | 8/37 [00:01<00:06,  4.32it/s]\r\nLoading safetensors checkpoint shards:  24% Completed | 9/37 [00:02<00:05,  4.99it/s]\r\nLoading safetensors checkpoint shards:  27% Completed | 10/37 [00:02<00:05,  4.65it/s]\r\nLoading safetensors checkpoint shards:  30% Completed | 11/37 [00:02<00:06,  4.30it/s]\r\nLoading safetensors checkpoint shards:  32% Completed | 12/37 [00:02<00:05,  4.66it/s]\r\nLoading safetensors checkpoint shards:  35% Completed | 13/37 [00:02<00:05,  4.64it/s]\r\nLoading safetensors checkpoint shards:  38% Completed | 14/37 [00:03<00:05,  4.56it/s]\r\nLoading safetensors checkpoint shards:  41% Completed | 15/37 [00:03<00:04,  4.55it/s]\r\nLoading safetensors checkpoint shards:  43% Completed | 16/37 [00:03<00:04,  4.53it/s]\r\nLoading safetensors checkpoint shards:  46% Completed | 17/37 [00:03<00:04,  4.33it/s]\r\nLoading safetensors checkpoint shards:  49% Completed | 18/37 [00:04<00:04,  4.40it/s]\r\nLoading safetensors checkpoint shards:  51% Completed | 19/37 [00:04<00:03,  4.50it/s]\r\nLoading safetensors checkpoint shards:  54% Completed | 20/37 [00:04<00:03,  4.51it/s]\r\nLoading safetensors checkpoint shards:  57% Completed | 21/37 [00:04<00:03,  4.57it/s]\r\nLoading safetensors checkpoint shards:  59% Completed | 22/37 [00:04<00:03,  4.36it/s]\r\nLoading safetensors checkpoint shards:  62% Completed | 23/37 [00:05<00:03,  4.41it/s]\r\nLoading safetensors checkpoint shards:  65% Completed | 24/37 [00:05<00:02,  4.44it/s]\r\nLoading safetensors checkpoint shards:  68% Completed | 25/37 [00:05<00:02,  4.28it/s]\r\nLoading safetensors checkpoint shards:  70% Completed | 26/37 [00:05<00:02,  4.37it/s]\r\nLoading safetensors checkpoint shards:  73% Completed | 27/37 [00:06<00:02,  4.27it/s]\r\nLoading safetensors checkpoint shards:  76% Completed | 28/37 [00:06<00:02,  4.35it/s]\r\nLoading safetensors checkpoint shards:  78% Completed | 29/37 [00:06<00:01,  4.42it/s]\r\nLoading safetensors checkpoint shards:  81% Completed | 30/37 [00:06<00:01,  4.26it/s]\r\nLoading safetensors checkpoint shards:  84% Completed | 31/37 [00:07<00:01,  4.10it/s]\r\nLoading safetensors checkpoint shards:  86% Completed | 32/37 [00:07<00:01,  4.11it/s]\r\nLoading safetensors checkpoint shards:  89% Completed | 33/37 [00:07<00:00,  4.13it/s]\r\nLoading safetensors checkpoint shards:  92% Completed | 34/37 [00:07<00:00,  4.14it/s]\r\nLoading safetensors checkpoint shards:  95% Completed | 35/37 [00:08<00:00,  4.24it/s]\r\nLoading safetensors checkpoint shards:  97% Completed | 36/37 [00:08<00:00,  4.36it/s]\r\n[14:39:31 TP4] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=59.64 GB\r\n[14:39:31 TP7] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=60.11 GB\r\n[14:39:31 TP6] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=59.64 GB\r\n[14:39:31 TP2] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=59.64 GB\r\nLoading safetensors checkpoint shards: 100% Completed | 37/37 [00:08<00:00,  4.51it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 37/37 [00:08<00:00,  4.38it/s]\r\n\r\n[14:39:31 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=59.73 GB\r\n[14:39:31 TP3] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=59.64 GB\r\n[14:39:31 TP1] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=59.64 GB\r\n[14:39:31 TP5] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=59.64 GB\r\n[14:39:31 TP6] Memory pool end. avail mem=12.45 GB\r\n[14:39:31 TP5] Memory pool end. avail mem=12.45 GB\r\n[14:39:31 TP0] Memory pool end. avail mem=12.54 GB\r\n[14:39:31 TP3] Memory pool end. avail mem=12.45 GB\r\n[14:39:31 TP7] Memory pool end. avail mem=12.92 GB\r\n[14:39:31 TP1] Memory pool end. avail mem=12.45 GB\r\n[14:39:31 TP4] Memory pool end. avail mem=12.45 GB\r\n[14:39:31 TP2] Memory pool end. avail mem=12.45 GB\r\n[14:39:31 TP6] Capture cuda graph begin. This can take up to several minutes.\r\n[14:39:31 TP7] Capture cuda graph begin. This can take up to several minutes.\r\n[14:39:31 TP5] Capture cuda graph begin. This can take up to several minutes.\r\n[14:39:31 TP3] Capture cuda graph begin. This can take up to several minutes.\r\n[14:39:31 TP0] Capture cuda graph begin. This can take up to several minutes.\r\n[14:39:31 TP1] Capture cuda graph begin. This can take up to several minutes.\r\n[14:39:31 TP4] Capture cuda graph begin. This can take up to several minutes.\r\n[14:39:31 TP2] Capture cuda graph begin. This can take up to several minutes.\r\nINFO 09-11 14:39:46 custom_all_reduce.py:223] Registering 3703 cuda graph addresses\r\nINFO 09-11 14:39:46 custom_all_reduce.py:223] Registering 3703 cuda graph addresses\r\nINFO 09-11 14:39:46 custom_all_reduce.py:223] Registering 3703 cuda graph addresses\r\nINFO 09-11 14:39:46 custom_all_reduce.py:223] Registering 3703 cuda graph addresses\r\nINFO 09-11 14:39:46 custom_all_reduce.py:223] Registering 3703 cuda graph addresses\r\nINFO 09-11 14:39:46 custom_all_reduce.py:223] Registering 3703 cuda graph addresses\r\nINFO 09-11 14:39:46 custom_all_reduce.py:223] Registering 3703 cuda graph addresses\r\nINFO 09-11 14:39:46 custom_all_reduce.py:223] Registering 3703 cuda graph addresses\r\n[14:39:48 TP0] max_total_num_tokens=1218251, max_prefill_tokens=16384, max_running_requests=5119, context_len=32768\r\n[14:39:48 TP4] max_total_num_tokens=1218251, max_prefill_tokens=16384, max_running_requests=5119, context_len=32768\r\n[14:39:48 TP1] max_total_num_tokens=1218251, max_prefill_tokens=16384, max_running_requests=5119, context_len=32768\r\n[14:39:48 TP6] max_total_num_tokens=1218251, max_prefill_tokens=16384, max_running_requests=5119, context_len=32768\r\n[14:39:48 TP3] max_total_num_tokens=1218251, max_prefill_tokens=16384, max_running_requests=5119, context_len=32768\r\n[14:39:48 TP2] max_total_num_tokens=1218251, max_prefill_tokens=16384, max_running_requests=5119, context_len=32768\r\n[14:39:48 TP7] max_total_num_tokens=1218251, max_prefill_tokens=16384, max_running_requests=5119, context_len=32768\r\n[14:39:48 TP5] max_total_num_tokens=1218251, max_prefill_tokens=16384, max_running_requests=5119, context_len=32768\r\nINFO:     Started server process [37328]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)\r\n\r\nException in thread Thread-1 (_wait_and_warmup):\r\nTraceback (most recent call last):\r\n  File \"/home/user/anaconda3/envs/vllm/lib/python3.10/site-packages/requests/models.py\", line 974, in json\r\n    return complexjson.loads(self.text, **kwargs)\r\n  File \"/home/user/anaconda3/envs/vllm/lib/python3.10/json/__init__.py\", line 346, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"/home/user/anaconda3/envs/vllm/lib/python3.10/json/decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  File \"/home/user/anaconda3/envs/vllm/lib/python3.10/json/decoder.py\", line 355, in raw_decode\r\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/user/anaconda3/envs/vllm/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/user/anaconda3/envs/vllm/lib/python3.10/threading.py\", line 953, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/user/anaconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/server.py\", line 449, in _wait_and_warmup\r\n    model_info = res.json()\r\n  File \"/home/user/anaconda3/envs/vllm/lib/python3.10/site-packages/requests/models.py\", line 978, in json\r\n    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\r\nrequests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n```\n\n### Reproduction\n\npython -m sglang.launch_server --model-path models/Qwen2-72B-Instruct --port 30000 --tp 8\n\n### Environment\n\n```\r\n(vllm) user@node14:~$ python3 -m sglang.check_env\r\n/home/user/anaconda3/envs/vllm/lib/python3.10/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\r\n* 'underscore_attrs_are_private' has been removed\r\n  warnings.warn(message, UserWarning)\r\nPython: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]\r\nCUDA available: True\r\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\r\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\r\nCUDA_HOME: /usr/local/cuda-12.2\r\nNVCC: Cuda compilation tools, release 12.2, V12.2.91\r\nCUDA Driver Version: 535.154.05\r\nPyTorch: 2.4.0+cu121\r\nsglang: 0.3.0\r\nflashinfer: 0.1.6+cu121torch2.4\r\ntriton: 3.0.0\r\ntransformers: 4.44.2\r\nrequests: 2.32.3\r\ntqdm: 4.66.4\r\nnumpy: 1.26.4\r\naiohttp: 3.9.5\r\nfastapi: 0.111.1\r\nhf_transfer: 0.1.8\r\nhuggingface_hub: 0.23.4\r\ninteregular: 0.3.3\r\npackaging: 24.1\r\nPIL: 10.4.0\r\npsutil: 6.0.0\r\npydantic: 2.8.2\r\nuvicorn: 0.30.1\r\nuvloop: 0.19.0\r\nzmq: 26.0.3\r\nvllm: 0.5.5\r\nmultipart: 0.0.9\r\nopenai: 1.44.1\r\nanthropic: 0.34.2\r\nNVIDIA Topology: \r\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-23,96-119     0               N/A\r\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-23,96-119     0               N/A\r\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-23,96-119     0               N/A\r\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     24-47,120-143   1               N/A\r\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     48-71,144-167   2               N/A\r\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     48-71,144-167   2               N/A\r\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     48-71,144-167   2               N/A\r\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     72-95,168-191   3               N/A\r\nNIC0    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC1    SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC2    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC3    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     PXB\r\nNIC4    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS\r\nNIC5    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS\r\nNIC6    SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS\r\nNIC7    SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS\r\nNIC8    SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS\r\nNIC9    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_6\r\n  NIC5: mlx5_7\r\n  NIC6: mlx5_8\r\n  NIC7: mlx5_9\r\n  NIC8: mlx5_10\r\n  NIC9: mlx5_bond_0\r\n\r\n\r\nulimit soft: 1048576\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-09-11T06:47:58+00:00",
    "closed_at": "2024-09-11T12:37:21+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1386/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1386"
  },
  {
    "number": 506,
    "title": "Addition of Support of IDEFICS2",
    "body": "Hi \r\n\r\nWas wondering if sglang could support the IDEFICS 2 model as well.",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-06-05T11:18:05+00:00",
    "closed_at": "2024-08-05T01:05:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/506/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/506"
  },
  {
    "number": 7614,
    "title": "[Bug] bench_serving.py error in hicache folder",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nHi all, when I tried to run bench_serving in the benchmark/hicache folder, I encountered the problem that indicates my prompt is out of range. Could you please tell me the reason I'm encountering the problem and how I should fix it.\n\nMy model is Qwen3-235B-FP8, my bash script command is:\n\n`python3 bench_serving.py --backend sglang --dataset-name sharegpt --random-input 4096 --random-output 1 --request-rate 2.37 --base-url http://localhost:4000 --random-range-ratio 0.98 --enable-multiturn --disable-shuffle --num-prompts 256 --max_concurrency 2560`\n\n![Image](https://github.com/user-attachments/assets/4bba5925-ed97-434c-9676-0599af74bb5a)\n\nThank you very much!\n\n\n\n\n### Reproduction\n\nrun bench_serving in benchmark/hicache/ folder\n\n### Environment\n\nsglang 0.4.6 Post5",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-28T03:31:35+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7614/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7614"
  },
  {
    "number": 5528,
    "title": "[Bug] Only one Worker active when using sglang_router.launch_server on a single machine with multiple GPUs",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nOn a single machine equipped with 4\u00d7 NVIDIA L20Y (80GB) GPUs, when launching SGLang using the built-in router via:\n`python -m sglang_router.launch_server --model-path deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --dp 4\n`\nand sending multiple concurrent chat completion requests using multi-threading, only one worker appears to be actively handling requests, while the other three remain idle.\n\n<img width=\"789\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7af0ac44-c9d6-47ba-9365-9dbc2bf2997c\" />\n\nIn contrast, running the same model using the non-router version:\n`python -m sglang.launch_server --model-path deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --dp 4\n`\ncorrectly utilizes all four GPUs and workers, with even distribution of load across them.\n\n### Reproduction\n\n`python -m sglang_router.launch_server --model-path Qwen/Qwen1.5-32B-Chat-R1-distill --dp 4\n`\n\nThen:\n\n```python\n\nimport concurrent.futures\nimport time\nfrom openai import OpenAI\nfrom openai.types.chat import ChatCompletion\n\n# Connect to local SGLang Router\nclient = OpenAI(\n    base_url=\"http://localhost:30000/v1\",  # SGLang Router default port\n    api_key=\"fake-key\"  # Required by openai SDK, but SGLang router doesn't verify it\n)\n\ndef ask_question(index: int) -> tuple[int, str]:\n    try:\n        response: ChatCompletion = client.chat.completions.create(\n            model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n            messages=[\n                {\"role\": \"user\", \"content\": f\"What is the capital of country #{index}?\"}\n            ],\n            temperature=0.7,\n            max_tokens=16000\n        )\n        return index, response.choices[0].message.content\n    except Exception as e:\n        return index, f\"Error: {str(e)}\"\n\n# Start timing\nstart = time.time()\n\nresults = [None] * 40\nwith concurrent.futures.ThreadPoolExecutor(max_workers=40) as executor:\n    future_to_index = {executor.submit(ask_question, i): i for i in range(40)}\n    for future in concurrent.futures.as_completed(future_to_index):\n        index, result = future.result()\n        results[index] = result\n\nend = time.time()\n\n# Print outputs\nfor i, content in enumerate(results):\n    print(f\"Request {i}: {content.strip()[:100]}\")\n\nprint(f\"\\nTotal time: {end - start:.2f} seconds\")\n```\n\n### Environment\n\n```\nsglang==0.4.5.post1\nsglang-router==0.1.4\nsgl-kernel==0.0.9.post1\ntorch==2.5.1\ntriton==3.1.0\ntransformers==4.51.1\nflashinfer-python==0.2.3+cu124torch2.5\nsafetensors==0.5.3\ntiktoken==0.9.0\ntokenizers==0.21.1\nuvicorn==0.34.1\nfastapi==0.115.12\nstarlette==0.46.2\nopenai==1.75.0\nhttpx==0.28.1\naiohttp==3.11.16\npynvml==12.0.0\nnvidia-nccl-cu12==2.21.5\ncuda-python==12.8.0\ncuda-bindings==12.8.0\n```",
    "labels": [],
    "state": "open",
    "created_at": "2025-04-18T10:42:36+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5528/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5528"
  },
  {
    "number": 5275,
    "title": "Expert selection distribution capture results difference between ranks",
    "body": "Hi, \n\nI was capturing expert selection distribution using `start_expert_distribution_record` and dump the results using `dump_expert_distribution_record` endpoint. There are multiple csv files generated, one for each rank. Each file has a distribution for all the experts in all layers, but the distributions are different. I wonder is this an expected behavior or not and how should we interpret the result. Below is a truncated example of the output file. I also attached three of the output csv files.\n\nExample:\n\n> expert_distribution_rank0_timestamp1744347750.3721356.csv\n```\nlayer_id,expert_id,count\n3,0,24990\n3,18,39249\n3,29,13378\n3,34,13874\n3,43,22360\n```\n\n> expert_distribution_rank1_timestamp1744347750.006448.csv\n```\nlayer_id,expert_id,count\n3,18,33056\n3,29,14053\n3,34,14732\n3,43,20474\n```\n\n[expert_distribution_rank0_timestamp1744347750.3721356.csv](https://github.com/user-attachments/files/19699341/expert_distribution_rank0_timestamp1744347750.3721356.csv)\n[expert_distribution_rank1_timestamp1744347750.006448.csv](https://github.com/user-attachments/files/19699342/expert_distribution_rank1_timestamp1744347750.006448.csv)\n[expert_distribution_rank2_timestamp1744347749.9709291.csv](https://github.com/user-attachments/files/19699343/expert_distribution_rank2_timestamp1744347749.9709291.csv)\n\nThanks!",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-04-11T05:32:45+00:00",
    "closed_at": "2025-06-15T00:22:00+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5275/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/5275"
  },
  {
    "number": 7339,
    "title": "[Bug] In version 0.4.7 (including post1), there is an approximately 20-second stall when returning the first cached token during inference.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nVersion 0.4.6 is ok. \n![Image](https://github.com/user-attachments/assets/119cd407-48f9-4382-b85d-3e901f456a33)\nAt the stall time, the cpu are busy on comiling something:\n$ps -aux |grep cicc\nroot        1413  0.0  0.0   2796  1104 ?        S    02:51   0:00 /bin/dash -c -- \"$CICC_PATH/cicc\" --c++17 --gnu_version=130200 --display_error_number --orig_src_file_name \"/usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/cascade.cu\" --orig_src_path_name \"/usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/cascade.cu\" --allow_managed --relaxed_constexpr   -arch compute_80 -m64 --no-version-ident -ftz=1 -prec_div=0 -prec_sqrt=0 -fmad=1 -fast-math --gen_div_approx_ftz --include_file_name \"tmpxft_00000559_00000000-3_cascade.fatbin.c\" -tused --module_id_file_name \"/tmp/tmpxft_00000559_00000000-4_cascade.module_id\" --gen_c_file_name \"/tmp/tmpxft_00000559_00000000-6_cascade.cudafe1.c\" --stub_file_name \"/tmp/tmpxft_00000559_00000000-6_cascade.cudafe1.stub.c\" --gen_device_file_name \"/tmp/tmpxft_00000559_00000000-6_cascade.cudafe1.gpu\"  \"/tmp/tmpxft_00000559_00000000-7_cascade.cpp1.ii\" -o \"/tmp/tmpxft_00000559_00000000-6_cascade.ptx\" > /tmp/tmpxft_00000559_00000000-10_2597600_stdout 2>/tmp/tmpxft_00000559_00000000-10_2597600_stderr\n\n### Reproduction\n\nDelay 20+ seconds during second inference of the same prompt\n\n### Environment\n\npython3 -m sglang.check_env\n\nPython: 3.12.3 (main, May 26 2025, 18:50:19) [GCC 13.3.0]\nCUDA available: True\nGPU 0: NVIDIA A800-SXM4-80GB\nGPU 0 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.6, V12.6.85\nCUDA Driver Version: 535.54.03\nPyTorch: 2.7.1+cu126\nsglang: 0.4.7.post1\nsgl_kernel: 0.1.9\nflashinfer_python: 0.2.6.post1\ntriton: 3.3.1\ntransformers: 4.52.3\ntorchao: 0.9.0\nnumpy: 2.3.0\naiohttp: 3.12.13\nfastapi: 0.115.12\nhf_transfer: 0.1.9\nhuggingface_hub: 0.33.0\ninteregular: 0.3.3\nmodelscope: 1.27.0\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.7\npython-multipart: 0.0.20\npyzmq: 27.0.0\nuvicorn: 0.34.3\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.19\nopenai: 1.87.0\ntiktoken: 0.9.0\nanthropic: 0.54.0\nlitellm: 1.72.6\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     PXB     SYS     SYS     16-31,144-159   1               N/A\nNIC0    SYS      X      SYS     SYS     SYS\nNIC1    PXB     SYS      X      SYS     SYS\nNIC2    SYS     SYS     SYS      X      SYS\nNIC3    SYS     SYS     SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n\n\nulimit soft: 1048576",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-19T03:11:38+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7339/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7339"
  },
  {
    "number": 4653,
    "title": "[Bug] Integrate DeepEP into SGLang  Multi node case is failed  on H100",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n\n\n\n![Image](https://github.com/user-attachments/assets/b3a8015f-b10f-4728-9338-14156410f38d)\n\n### Reproduction\n\npython3 -m sglang.launch_server --model-path model--trust-remote-code   --tp 16 --dp 16  --dist-init-addr ip  --nnodes 2 --node-rank 0   --enable-dp-attention --enable-deepep-moe   --disable-cuda-graph\n\npython3 -m sglang.launch_server --model-path model--trust-remote-code   --tp 16 --dp 16  --dist-init-addr ip  --nnodes 2 --node-rank 1   --enable-dp-attention --enable-deepep-moe   --disable-cuda-graph\n\n### Environment\n\n2 * 8 * H100",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-03-21T16:03:25+00:00",
    "closed_at": "2025-05-22T00:19:11+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4653/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4653"
  },
  {
    "number": 4159,
    "title": "[Bug] Key conflict of `AutoImageProcessor.register`",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThe following ValueError was raised when attempting to serve any model within a recent Docker container:\n\n`Traceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/sgl-workspace/sglang/python/sglang/launch_server.py\", line 6, in <module>\n    from sglang.srt.entrypoints.http_server import launch_server\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/http_server.py\", line 44, in <module>\n    from sglang.srt.entrypoints.engine import _launch_subprocesses\n  File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/engine.py\", line 36, in <module>\n    from sglang.srt.managers.data_parallel_controller import (\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/data_parallel_controller.py\", line 27, in <module>\n    from sglang.srt.managers.io_struct import (\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/io_struct.py\", line 25, in <module>\n    from sglang.srt.managers.schedule_batch import BaseFinishReason\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/schedule_batch.py\", line 43, in <module>\n    from sglang.srt.configs.model_config import ModelConfig\n  File \"/sgl-workspace/sglang/python/sglang/srt/configs/__init__.py\", line 4, in <module>\n    from sglang.srt.configs.qwen2_5_vl_config import (\n  File \"/sgl-workspace/sglang/python/sglang/srt/configs/qwen2_5_vl_config.py\", line 1005, in <module>\n    AutoImageProcessor.register(Qwen2_5_VLConfig, None, Qwen2_5_VLImageProcessor, None, exist_ok=False)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/image_processing_auto.py\", line 629, in register\n    IMAGE_PROCESSOR_MAPPING.register(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 833, in register\n    raise ValueError(f\"'{key}' is already used by a Transformers model.\")\nValueError: '<class 'sglang.srt.configs.qwen2_5_vl_config.Qwen2_5_VLConfig'>' is already used by a Transformers model.`\n\nThe problem was caused by 'Qwen2_5_VLConfig' already existing in transformers>=0.49.0, and was resolved by enabling `exist_ok` when registering it.\nUpdate the file `/sgl-workspace/sglang/python/sglang/srt/configs/qwen2_5_vl_config.py` at line 1005~1006:\n\n`\nAutoImageProcessor.register(Qwen2_5_VLConfig, None, Qwen2_5_VLImageProcessor, None, exist_ok=True)\nAutoProcessor.register(Qwen2_5_VLConfig, Qwen2_5_VLProcessor, exist_ok=True)\n`\n\n### Reproduction\n\npython3 -m sglang.launch_server --model [any-model]\n\n### Environment\n\nDocker image tag:  v0.4.3.post3-cu125\nsglang version: 0.4.3.post3\nsgl-kernel: 0.0.3.post6\nPython version: 3.10.12\n",
    "labels": [
      "MLLM"
    ],
    "state": "closed",
    "created_at": "2025-03-07T04:06:18+00:00",
    "closed_at": "2025-03-25T12:17:44+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4159/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4159"
  },
  {
    "number": 4017,
    "title": "[Bug] deepseek-r1 occasionally crash",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n```\n[2025-02-28 09:28:25 TP7] Scheduler hit an exception: Traceback (most recent call last):                                                                                                                                                                 \u2502\n\u2502   File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 1825, in run_scheduler_process                                                                                                                                   \u2502\n\u2502     scheduler.event_loop_overlap()                                                                                                                                                                                                                       \u2502\n\u2502   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context                                                                                                                                               \u2502\n\u2502     return func(*args, **kwargs)                                                                                                                                                                                                                         \u2502\n\u2502   File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 496, in event_loop_overlap                                                                                                                                       \u2502\n\u2502     batch = self.get_next_batch_to_run()                                                                                                                                                                                                                 \u2502\n\u2502   File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 877, in get_next_batch_to_run                                                                                                                                    \u2502\n\u2502     new_batch = self.get_new_batch_prefill()                                                                                                                                                                                                             \u2502\n\u2502   File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 898, in get_new_batch_prefill                                                                                                                                    \u2502\n\u2502     self.move_ready_grammar_requests()                                                                                                                                                                                                                   \u2502\n\u2502   File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/scheduler.py\", line 1594, in move_ready_grammar_requests                                                                                                                             \u2502\n\u2502     req.grammar = req.grammar.result(timeout=0.05)                                                                                                                                                                                                       \u2502\n\u2502   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 458, in result                                                                                                                                                                            \u2502\n\u2502     return self.__get_result()                                                                                                                                                                                                                           \u2502\n\u2502   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result                                                                                                                                                                      \u2502\n\u2502     raise self._exception                                                                                                                                                                                                                                \u2502\n\u2502   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run                                                                                                                                                                               \u2502\n\u2502     result = self.fn(*self.args, **self.kwargs)                                                                                                                                                                                                          \u2502\n\u2502   File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/constrained/base_grammar_backend.py\", line 53, in init_value                                                                                                                                  \u2502\n\u2502     entry.value = self.init_value_impl(key)                                                                                                                                                                                                              \u2502\n\u2502   File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/constrained/outlines_backend.py\", line 156, in init_value_impl                                                                                                                                \u2502\n\u2502     regex = build_regex_from_object(                                                                                                                                                                                                                     \u2502\n\u2502   File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/constrained/outlines_backend.py\", line 197, in build_regex_from_object                                                                                                                        \u2502\n\u2502     return build_regex_from_schema(schema, whitespace_pattern)                                                                                                                                                                                           \u2502\n\u2502 ValueError: 'type' must be a string \n```\n\n### Reproduction\n\nDeepSeek R1\nhttps://huggingface.co/deepseek-ai/DeepSeek-R1\n\nit happens randomly. during startup or out of sudden during inference\n\n\npython3 -m sglang.launch_server --model-path /inf-pvc/models/DeepSeek-R1 --served-model-name DeepSeek-R1 --tp 16 --context-length 65536 --enable-metrics --watchdog-timeout=3000 --mem-fraction-static 0.8 --dist-init-addr 10.4\n2.74.84:20010 --nnodes 2 --node-rank 0 --trust-remote-code --host 0.0.0.0\n\npython3 -m sglang.launch_server --model-path /inf-pvc/models/DeepSeek-R1 --served-model-name DeepSeek-R1 --tp 16 --context-length 65536 --enable-metrics --watchdog-timeout=3000 --mem-fraction-static 0.8 --dist-init-addr 10.4\n2.74.84:20010 --nnodes 2 --node-rank 1 --trust-remote-code --host 0.0.0.0\n\n### Environment\n\nPython: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.6, V12.6.77\nCUDA Driver Version: 550.144.03\nPyTorch: 2.5.1+cu124\nsglang: 0.4.3.post2\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.1.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.10.5\nfastapi: 0.115.6\nhf_transfer: 0.1.9\nhuggingface_hub: 0.27.0\ninteregular: 0.3.3\nmodelscope: 1.23.0\norjson: 3.10.15\npackaging: 23.2\npsutil: 6.0.0\npydantic: 2.9.2\nmultipart: 0.0.20\nzmq: 26.2.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.63.2\nanthropic: 0.45.2\ndecord: 0.6.0\n\noutlines                          0.1.11\noutlines_core                     0.1.26",
    "labels": [
      "inactive",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-03-03T08:07:18+00:00",
    "closed_at": "2025-05-03T00:18:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4017/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4017"
  },
  {
    "number": 4289,
    "title": "RuntimeError: NCCL error: unhandled system error",
    "body": "# step\nI successfully ran the DeepSeek-R1-Distill-Qwen-32B on 4*L40S.\n\n`python -m sglang.launch_server --model-path /home/clouduser/work/models/DeepSeek-R1-Distill-Qwen-32B --mem-fraction-static 0.85 --tp 4`\n\nHowever, I encountered an error when running it with 8*L40S. \n`python -m sglang.launch_server --model-path /home/clouduser/work/models/DeepSeek-R1-Distill-Qwen-32B --mem-fraction-static 0.85 --tp 8`\n\n# logs\n\n`INFO 03-11 15:02:39 __init__.py:190] Automatically detected platform cuda.\n[2025-03-11 15:02:44] server_args=ServerArgs(model_path='/home/clouduser/work/models/DeepSeek-R1-Distill-Qwen-32B', tokenizer_path='/home/clouduser/work/models/DeepSeek-R1-Distill-Qwen-32B', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='DeepSeek-R1-Distill-Qwen-32B', chat_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=30000, mem_fraction_static=0.85, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, tp_size=8, stream_interval=1, stream_output=False, random_seed=393109793, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=5, speculative_eagle_topk=4, speculative_num_draft_tokens=8, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=False, flashinfer_mla_disable_ragged=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False)\nINFO 03-11 15:02:47 __init__.py:190] Automatically detected platform cuda.\nINFO 03-11 15:02:47 __init__.py:190] Automatically detected platform cuda.\nINFO 03-11 15:02:47 __init__.py:190] Automatically detected platform cuda.\nINFO 03-11 15:02:47 __init__.py:190] Automatically detected platform cuda.\nINFO 03-11 15:02:47 __init__.py:190] Automatically detected platform cuda.\nINFO 03-11 15:02:47 __init__.py:190] Automatically detected platform cuda.\nINFO 03-11 15:02:47 __init__.py:190] Automatically detected platform cuda.\nINFO 03-11 15:02:47 __init__.py:190] Automatically detected platform cuda.\nINFO 03-11 15:02:47 __init__.py:190] Automatically detected platform cuda.\n[2025-03-11 15:02:52 TP5] Init torch distributed begin.\n[2025-03-11 15:02:52 TP0] Init torch distributed begin.\n[2025-03-11 15:02:52 TP7] Init torch distributed begin.\n[2025-03-11 15:02:52 TP6] Init torch distributed begin.\n[2025-03-11 15:02:52 TP4] Init torch distributed begin.\n[2025-03-11 15:02:52 TP3] Init torch distributed begin.\n[2025-03-11 15:02:52 TP1] Init torch distributed begin.\n[2025-03-11 15:02:52 TP2] Init torch distributed begin.\n[2025-03-11 15:02:53 TP1] sglang is using nccl==2.21.5\n[2025-03-11 15:02:53 TP7] sglang is using nccl==2.21.5\n[2025-03-11 15:02:53 TP0] sglang is using nccl==2.21.5\n[2025-03-11 15:02:53 TP3] sglang is using nccl==2.21.5\n[2025-03-11 15:02:53 TP4] sglang is using nccl==2.21.5\n[2025-03-11 15:02:53 TP2] sglang is using nccl==2.21.5\n[2025-03-11 15:02:53 TP6] sglang is using nccl==2.21.5\n[2025-03-11 15:02:53 TP5] sglang is using nccl==2.21.5\n[2025-03-11 15:02:53 TP7] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 2255, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 216, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 74, in __init__\n    self.model_runner = ModelRunner(\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 161, in __init__\n    min_per_gpu_memory = self.init_torch_distributed()\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 291, in init_torch_distributed\n    initialize_model_parallel(tensor_model_parallel_size=self.tp_size)\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/distributed/parallel_state.py\", line 1100, in initialize_model_parallel\n    _TP = init_model_parallel_group(\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/distributed/parallel_state.py\", line 927, in init_model_parallel_group\n    return GroupCoordinator(\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/distributed/parallel_state.py\", line 255, in __init__\n    self.pynccl_comm = PyNcclCommunicator(\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/distributed/device_communicators/pynccl.py\", line 108, in __init__\n    self.comm: ncclComm_t = self.nccl.ncclCommInitRank(\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/distributed/device_communicators/pynccl_wrapper.py\", line 350, in ncclCommInitRank\n    self.NCCL_CHECK(\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/distributed/device_communicators/pynccl_wrapper.py\", line 329, in NCCL_CHECK\n    raise RuntimeError(f\"NCCL error: {error_str}\")\nRuntimeError: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details)\n\n[2025-03-11 15:02:53] Received sigquit from a child process. It usually means the child failed.\n[2025-03-11 15:02:53 TP3] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 2255, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 216, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 74, in __init__\n    self.model_runner = ModelRunner(\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 161, in __init__\n    min_per_gpu_memory = self.init_torch_distributed()\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 291, in init_torch_distributed\n    initialize_model_parallel(tensor_model_parallel_size=self.tp_size)\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/distributed/parallel_state.py\", line 1100, in initialize_model_parallel\n    _TP = init_model_parallel_group(\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/distributed/parallel_state.py\", line 927, in init_model_parallel_group\n    return GroupCoordinator(\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/distributed/parallel_state.py\", line 255, in __init__\n    self.pynccl_comm = PyNcclCommunicator(\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/distributed/device_communicators/pynccl.py\", line 108, in __init__\n    self.comm: ncclComm_t = self.nccl.ncclCommInitRank(\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/distributed/device_communicators/pynccl_wrapper.py\", line 350, in ncclCommInitRank\n    self.NCCL_CHECK(\n  File \"/home/clouduser/miniconda3/envs/sglang/lib/python3.10/site-packages/sglang/srt/distributed/device_communicators/pynccl_wrapper.py\", line 329, in NCCL_CHECK\n    raise RuntimeError(f\"NCCL error: {error_str}\")\nRuntimeError: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details)\n\n[2025-03-11 15:02:53] Received sigquit from a child process. It usually means the child failed.\n`",
    "labels": [],
    "state": "closed",
    "created_at": "2025-03-11T07:08:29+00:00",
    "closed_at": "2025-03-11T08:20:23+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4289/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4289"
  },
  {
    "number": 4083,
    "title": "[Bug] cuda kernel illegal and  GPTQMarlinMoEMethod.apply() got an unexpected keyword argument 'correction_bias'",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI test the model(OPEA/DeepSeek-R1-int4-gptq-sym-inc from HF) with the excellent sglang and two nodes(2 x 8X80G A100). \n\nCOMMANDS:\n**python3 -m sglang.launch_server --model-path /data/LM/hf/DeepSeek-R1-int4-gptq-sym-inc --tp 16 --dist-init-addr xx.yy.zz.210:25000 --nnodes 2 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 30000** \nand \n**python3 -m sglang.launch_server --model-path /data/LM/hf/DeepSeek-R1-int4-gptq-sym-inc --tp 16 --dist-init-addr xx.yy.zz..210:25000 --nnodes 2 --node-rank 1 --trust-remote-code**\n\nThen get the first error: **RuntimeError: CUDA error: an illegal memory access was encountered**\n\n[2025-03-05 04:48:02 TP3] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 1816, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 240, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in __init__\n    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 68, in __init__\n    self.model_runner = ModelRunner(\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 195, in __init__\n    self.load_model()\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 318, in load_model\n    self.model = get_model(\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/model_loader/__init__.py\", line 22, in get_model\n    return loader.load_model(\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/model_loader/loader.py\", line 362, in load_model\n    model.load_weights(self._get_all_weights(model_config, model))\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/models/deepseek_v2.py\", line 999, in load_weights\n    self_attn.w_vc = w_vc.contiguous().transpose(1, 2)\nRuntimeError: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nI add **--disable-mla --disable-cuda-graph** to the end of commands, the get the second error: **TypeError: GPTQMarlinMoEMethod.apply() got an unexpected keyword argument 'correction_bias'**\n\n   File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 109, in forward_thread_func\n    self.forward_thread_func_()\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 140, in forward_thread_func_\n    logits_output, next_token_ids = self.worker.forward_batch_generation(\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 164, in forward_batch_generation\n    logits_output = self.model_runner.forward(forward_batch)\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 796, in forward\n    return self.forward_extend(forward_batch)\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 761, in forward_extend\n    return self.model.forward(\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/models/deepseek_v2.py\", line 874, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch)\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/models/deepseek_v2.py\", line 835, in forward\n    hidden_states, residual = layer(\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/models/deepseek_v2.py\", line 790, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/models/deepseek_v2.py\", line 177, in forward\n    self.experts(hidden_states=hidden_states, router_logits=router_logits)\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/data/home/xxyyzz/miniconda3/envs/vllm/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py\", line 589, in forward\n    final_hidden_states = self.quant_method.apply(\nTypeError: GPTQMarlinMoEMethod.apply() got an unexpected keyword argument 'correction_bias'\n\nI can successfully run the DeepSeek-R1-bf16 model and a QWen2 gptq model from HF with the same env.\n\n\n\n\n### Reproduction\n\nTow nodes(A100 80G) with 8 cards each. The model is downloaded from huggingface.co, OPEA/DeepSeek-R1-int4-gptq-sym-inc.\n\n==command on master node==\n**python3 -m sglang.launch_server --model-path /data/LM/hf/DeepSeek-R1-int4-gptq-sym-inc --tp 16 --dist-init-addr xx.yy.zz.210:25000 --nnodes 2 --node-rank 0 --trust-remote-code --host 0.0.0.0 --port 30000** \n\n==command on worker node==\n**python3 -m sglang.launch_server --model-path /data/LM/hf/DeepSeek-R1-int4-gptq-sym-inc --tp 16 --dist-init-addr xx.yy.zz..210:25000 --nnodes 2 --node-rank 1 --trust-remote-code**\n\n==after cuda kernel error==\nI add **--disable-mla --disable-cuda-graph** to the end of above commands, then get error=>\nTypeError: GPTQMarlinMoEMethod.apply() got an unexpected keyword argument 'correction_bias'\n\n### Environment\n\n2025-03-05 06:31:56.740313: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-03-05 06:31:56.762041: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1741156316.778497 3200844 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1741156316.783735 3200844 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-03-05 06:31:56.802124: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nINFO 03-05 06:31:58 __init__.py:190] Automatically detected platform cuda.\nPython: 3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA A100-SXM4-80GB\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.99\nCUDA Driver Version: 550.127.08\nPyTorch: 2.5.1+cu124\nsglang: 0.4.3.post2\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.2.post1+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.9.0\nnumpy: 1.26.0\naiohttp: 3.11.11\nfastapi: 0.115.6\nhf_transfer: 0.1.9\nhuggingface_hub: 0.27.1\ninteregular: 0.3.3\nmodelscope: 1.21.0\norjson: 3.10.15\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.5\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.61.1\nanthropic: 0.49.0\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV12    NV12    NV12    NV12    NV12    NV12    NV12    NODE    NODE    PXB     NODE    SYS     0-35,72-107     0               N/A\nGPU1    NV12     X      NV12    NV12    NV12    NV12    NV12    NV12    NODE    NODE    PXB     NODE    SYS     0-35,72-107     0               N/A\nGPU2    NV12    NV12     X      NV12    NV12    NV12    NV12    NV12    NODE    NODE    NODE    PXB     SYS     0-35,72-107     0               N/A\nGPU3    NV12    NV12    NV12     X      NV12    NV12    NV12    NV12    NODE    NODE    NODE    PXB     SYS     0-35,72-107     0               N/A\nGPU4    NV12    NV12    NV12    NV12     X      NV12    NV12    NV12    SYS     SYS     SYS     SYS     NODE    36-71,108-143   1               N/A\nGPU5    NV12    NV12    NV12    NV12    NV12     X      NV12    NV12    SYS     SYS     SYS     SYS     NODE    36-71,108-143   1               N/A\nGPU6    NV12    NV12    NV12    NV12    NV12    NV12     X      NV12    SYS     SYS     SYS     SYS     NODE    36-71,108-143   1               N/A\nGPU7    NV12    NV12    NV12    NV12    NV12    NV12    NV12     X      SYS     SYS     SYS     SYS     NODE    36-71,108-143   1               N/A\nNIC0    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    SYS\nNIC1    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    SYS\nNIC2    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS\nNIC3    NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS\nNIC4    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS      X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_bond_0\n\n\nulimit soft: 1024\n",
    "labels": [
      "quant",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-03-05T06:47:58+00:00",
    "closed_at": "2025-03-16T07:43:21+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4083/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4083"
  },
  {
    "number": 8004,
    "title": "[WIP] [Roadmap] Supporting Ascend NPU on 2025 H2",
    "body": "# SGLang NPU support on 2025 H2\n\nDuring 2025 H1, we have contributed initial supports for NPU ([#3853](https://github.com/sgl-project/sglang/pull/3853), [#7022](https://github.com/sgl-project/sglang/pull/7022)), which make it possible for users to run SGLang on NPU hardware.\n\nOur goal on 2025 H2 is to provide a seamless running experience on NPUs, and here is a rough development roadmap:\n\n## CI on NPU hardware\n\n- [ ] [**_July_**] Enable autoscaling runners #7935 \n- [ ] E2E/unittest test coverage\n\n## Model support\n\n*We will start with supporting the hotest models*\n\n- [ ] [**_July_**] DeepseekV2 / V3 family\n- [ ] [**_July_**] Qwen3 family\n- [ ] [**_July_**] Qwen3-MoE family\n\n## User / Developer experience\n\n*User experience is also to be taken into our consideration, containers and documents will be provided soon*\n\n- [ ] [**_July_**] Docker image\n- [ ] [**_July_**] Docs (Quickstart / Installation / tutorials\u2026)\n\n## Performance Enhancement\n\n### Attention Backend\n\n- [x] [**_July_**] Ascend Attention Backend implementation w/ PA & MLA fused kernels #7722 \n\n### Parallelism\n\n- [ ] [**_September_**] Support DeepEP expert parallelism\n- [ ] [**_September_**] Optimization on DeepEPMoE implementation with fused kernels\n\n### Quantization\n\n- [x] [**_July_**] Support for Ascend-specific W8A8 quant method #7791 \n- [ ] [**_August_**] Support for AWQ quant method\n- [ ] [**_August_**] Support for GPTQ quant method\n\n### Cache\n\n- [x] [**_July_**] A new transfer-engine implementation supports Device-to-device transfer on NPUs #7795 \n- [ ] [**_November_**] A new cache pooling system supports HBM & DRAM mixed-pooling, coherent memory access and remote L3 cache direct copy to L1 cache on NPUs\n- [ ] [**_October_**] An optimized bucketing router policy for extremely uneven prompt length\n\n### Support Graph Mode\n\n- [ ] [**_November_**] NPU graph mode support\n\n### EPLB\n\n- [ ] [**_October_**] Support Expert Distribution Recorder on NPUs\n- [ ] [**_October_**] Support Async loading of experts' weights\n\n## Community\n\n- [ ] `#npu-support` is actively constructing on SGLang slack channel\n",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-14T03:17:24+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/8004/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/8004"
  },
  {
    "number": 4999,
    "title": "sglang-0.4.3.post3 deepseek-r1 set temperature=0\uff0cbut output is inconsistent",
    "body": "```\nsglang                            0.4.3.post3\nvllm                              0.7.2\ntransformers                      4.48.3\nnvidia-cublas-cu12                12.4.5.8\nnvidia-cuda-cupti-cu12            12.4.127\nnvidia-cuda-nvrtc-cu12            12.4.127\nnvidia-cuda-runtime-cu12          12.4.127\nnvidia-cudnn-cu12                 9.1.0.70\nnvidia-cufft-cu12                 11.2.1.3\nnvidia-curand-cu12                10.3.5.147\nnvidia-cusolver-cu12              11.6.1.9\nnvidia-cusparse-cu12              12.3.1.170\nnvidia-ml-py                      12.570.86\nnvidia-nccl-cu12                  2.20.5\nnvidia-nvjitlink-cu12             12.4.127\nnvidia-nvtx-cu12                  12.4.127\ntorch                             2.5.1\ntorchao                           0.9.0\ntorchaudio                        2.5.1\ntorchvision                       0.20.1\nflashinfer-python                 0.2.2.post1\n```\nDeploy DeepSeek-R1 on 2*H20 (cuda-12.1). Input prompt is a long text (30000+ tokens). Set temperature = 0 and seed=42, then run 10 times. Each time, the top 2 logprobs of tokens are different, and some tokens have the same logprobs. Is this normal?\n\n![Image](https://github.com/user-attachments/assets/528a8cdf-73fa-496f-a6a1-860b687fdcb6)",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2025-04-02T13:30:44+00:00",
    "closed_at": "2025-06-06T00:19:22+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/4999/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/4999"
  },
  {
    "number": 3817,
    "title": "[Bug]",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n### environment\ndocker-image: lmsysorg/sglang:v0.4.3.post2-cu125 (entrypoint: /bin/bash)\ncommand to launch docker container: `docker run -itd --name sglang -v /export:/export -p 6000:6000 --gpus=all --entrypoint /bin/bash --shm-size=10gb lmsysorg/sglang:v0.4.3.post2-cu125`\n<details>\n<summary>output from vllm/collect_env.py: </summary>\n\n```\nINFO 02-24 12:21:16 __init__.py:190] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.14.0-362.8.1.el9_3.x86_64-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA H200\nGPU 1: NVIDIA H200\nGPU 2: NVIDIA H200\nGPU 3: NVIDIA H200\nGPU 4: NVIDIA H200\nGPU 5: NVIDIA H200\nGPU 6: NVIDIA H200\nGPU 7: NVIDIA H200\n\nNvidia driver version: 550.127.05\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.1.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      46 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             192\nOn-line CPU(s) list:                0-191\nVendor ID:                          GenuineIntel\nModel name:                         INTEL(R) XEON(R) PLATINUM 8558\nCPU family:                         6\nModel:                              207\nThread(s) per core:                 2\nCore(s) per socket:                 48\nSocket(s):                          2\nStepping:                           2\nBogoMIPS:                           4200.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                     VT-x\nL1d cache:                          4.5 MiB (96 instances)\nL1i cache:                          3 MiB (96 instances)\nL2 cache:                           192 MiB (96 instances)\nL3 cache:                           520 MiB (2 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190\nNUMA node1 CPU(s):                  1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159,161,163,165,167,169,171,173,175,177,179,181,183,185,187,189,191\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.1.post2+cu124torch2.5\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchao==0.8.0\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.3\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8  NIC9     CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS   SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS   SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS   SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS   SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     PIX     PIX     NODE  NODE     NODE    1,3,5,7,9,11    1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX   NODE     NODE    1,3,5,7,9,11    1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE  PIX      NODE    1,3,5,7,9,11    1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE  NODE     PIX     1,3,5,7,9,11    1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS   SYS      SYS\nNIC1    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     SYS     SYS     SYS   SYS      SYS\nNIC2    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS     SYS     SYS     SYS   SYS      SYS\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS   SYS      SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      PIX     PIX     NODE  NODE     NODE\nNIC5    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX      X      PIX     NODE  NODE     NODE\nNIC6    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     PIX      X      NODE  NODE     NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X    NODE     NODE\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE   X       NODE\nNIC9    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE  NODE      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n\nNVIDIA_VISIBLE_DEVICES=all\nCUBLAS_VERSION=12.4.5.8\nNVIDIA_REQUIRE_CUDA=cuda>=9.0\nCUDA_CACHE_DISABLE=1\nNCCL_VERSION=2.21.5\nNVIDIA_DRIVER_CAPABILITIES=compute,utility,video\nNVIDIA_PRODUCT_NAME=Triton Server Base\nCUDA_VERSION=12.4.1.003\nCUDNN_VERSION=9.1.0.70\nNVIDIA_TRITON_SERVER_BASE_VERSION=24.04\nLD_LIBRARY_PATH=/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNVIDIA_BUILD_ID=90085237\nCUDA_DRIVER_VERSION=550.54.15\nNVIDIA_REQUIRE_JETPACK_HOST_MOUNTS=\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n### command to launch server\n\ninside the docker container, run:\n\n```shell\n# The directory /export/dp/DeepSeek-R1 contains model files from the HuggingFace repo. deepseek-ai/DeepSeek-R1\nnohup python3 -m sglang.launch_server --model /export/dp/DeepSeek-R1 --tp 8 --trust-remote-code --enable-dp-attention --disable-overlap-schedule --max-running-requests 512 --context-length 8192 > /export/sglang_serve.log 2>&1 &\n```\n\n### test of a request\n\ninside the same docker container, run:\n\n```shell\ncurl -s http://localhost:30000/v1/chat/completions -H 'Content-Type: application/json' -d '{\n  \"model\": \"deepseek_r1\",\n  \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant. You should think step by step.\"},\n    {\"role\": \"user\", \"content\": \"What is your name?\"}\n  ],\n  \"temperature\": 0.6,\n  \"top_p\": 0.95,\n  \"top_k\": 15,\n  \"max_tokens\": 500\n}'\n```\n\nThe response looks like:\n```\n{\"id\":\"635f23d9166e4a8791d8d6dce25deddd\",\"object\":\"chat.completion\",\"created\":1740399515,\"model\":\"deepseek_r1\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\" 1.es\\n\\n\\n\\n, {\\n\\n\\n\\n.\\n.\\n.ae\\n\\n.ae. sae\\n\\nemem,, sa\\n\\n most \\n\\num,..\\n\\n..1.,ae  io.ae\\n\\numstyle\\n .straum and..\\n\\nis way..series-image {op colora.am. sapoenum.. 14 e_shas. sau.u.a,. ofum.ing the 3 a am.apq 0u   in etupe s.. Proe de, of.aeumteric,,amam the.. thep.ut.d.. \\n,.. the tki\\n\\n mostu's ofu.,. p,\\n\\n.i a,,., ofu. theattersisius 1 ofu a s o. and o of the per the,..> theot.,. deu.ae p.\\n\\nute. em in of theensis thei.ensis in.um.amp...\",\"tool_calls\":null},\"logprobs\":null,\"finish_reason\":\"stop\",\"matched_stop\":1}],\"usage\":{\"prompt_tokens\":23,\"total_tokens\":250,\"completion_tokens\":227,\"prompt_tokens_details\":null}}\n```\n\n### Reproduction\n\nPlease refer to the \"environment\"/\"command_to_launch_server\"/\"test_of_a_request\" session above.\n\n### Environment\n\n<details>\n<summary> output from `python3 -m sglang.check_env` </summary>\n\n```\nINFO 02-24 12:26:48 __init__.py:190] Automatically detected platform cuda.\nPython: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H200\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.4, V12.4.131\nCUDA Driver Version: 550.127.05\nPyTorch: 2.5.1+cu124\nsgl_kernel: 0.0.3.post6\nflashinfer: 0.2.1.post2+cu124torch2.5\ntriton: 3.1.0\ntransformers: 4.48.3\ntorchao: 0.8.0\nnumpy: 1.26.4\naiohttp: 3.11.12\nfastapi: 0.115.8\nhf_transfer: 0.1.9\nhuggingface_hub: 0.28.1\ninteregular: 0.3.3\nmodelscope: 1.23.0\norjson: 3.10.15\npackaging: 24.2\npsutil: 7.0.0\npydantic: 2.10.6\nmultipart: 0.0.20\nzmq: 26.2.1\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.7.2\nopenai: 1.63.2\ntiktoken: 0.9.0\nanthropic: 0.45.2\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7  NIC8     NIC9    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS   SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS   SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS   SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS   SYS      SYS     0,2,4,6,8,10    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     PIX     PIX     NODE  NODE     NODE    1,3,5,7,9,11    1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX   NODE     NODE    1,3,5,7,9,11    1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE  PIX      NODE    1,3,5,7,9,11    1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE  NODE     PIX     1,3,5,7,9,11    1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS   SYS      SYS\nNIC1    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     SYS     SYS     SYS   SYS      SYS\nNIC2    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS     SYS     SYS     SYS   SYS      SYS\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS   SYS      SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      PIX     PIX     NODE  NODE     NODE\nNIC5    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX      X      PIX     NODE  NODE     NODE\nNIC6    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     PIX      X      NODE  NODE     NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X    NODE     NODE\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE   X       NODE\nNIC9    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE  NODE      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n\n\nulimit soft: 1073741816\n```\n\n</details>",
    "labels": [],
    "state": "closed",
    "created_at": "2025-02-24T12:30:20+00:00",
    "closed_at": "2025-02-24T12:32:47+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/3817/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/3817"
  },
  {
    "number": 94,
    "title": "how to use the finetuned mistral model for inference with sglang",
    "body": "how to use the finetuned mistral model for inference with sglang. \r\nPlease share the code for this",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-24T16:14:41+00:00",
    "closed_at": "2024-01-30T14:25:14+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/94/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/94"
  },
  {
    "number": 1643,
    "title": "[Feature] Support for GPT-2",
    "body": "### Checklist\r\n\r\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\r\n- [x] 2. Please use English, otherwise it will be closed.\r\n\r\n### Motivation\r\n\r\nI believe there is still a vast majority of our community working with GPT-2 or similar small language models for niche datasets. It would be great if these finetuned GPT-2 models can be deployed through sglang. \r\n\r\n### Related resources\r\n\r\n_No response_",
    "labels": [
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-10-12T04:00:51+00:00",
    "closed_at": "2024-11-01T03:54:32+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/1643/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/1643"
  },
  {
    "number": 7934,
    "title": "[Bug] DeepSeek-V3 function call return stop instead of tool_calls in streaming request",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen utilizing the DeepSeek-V3 function call in streaming mode, the finish reason for the last chunk ought to be tool calls, but currently, it shows as stop.\n\nI think the reason is in servering_chat.py, whether to change stop to tool calls is determined by whether the current parse result contains any tool. If the last chunk is EOS, the parser won\u2019t extract any tool, so it won\u2019t set the last chunk\u2019s finish reason to tool calls.\n\n### Reproduction\n\nlaunch server\n```\npython -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3-0324/ --tp 8 --trust-remote-code --tool-call-parser deepseekv3 --chat-template ./examples/chat_template/tool_chat_template_deepseekv3.jinja\n```\n\ntest\n```\ndef streaming_tool_call_with_client(client: OpenAI, model_name: str):\n    messages = [\n        {\"role\": \"user\", \"content\": \"\u5317\u4eac\u4eca\u5929\u7684\u5929\u6c14\u600e\u4e48\u6837?\u8c03\u7528\u5de5\u5177\u770b\u4e00\u4e0b\"}\n    ]\n    finish_reason = None\n    msg = ''\n    while finish_reason is None or finish_reason == \"tool_calls\":\n        completion = client.chat.completions.create(\n            model=model_name,\n            messages=messages,\n            temperature=0.3,\n            tools=tools,\n            tool_choice=\"auto\",\n            stream=True \n        )\n        tool_calls = []\n        for chunk in completion:\n            delta = chunk.choices[0].delta\n            if delta.content:\n                msg += delta.content\n            print(chunk)\n            if delta.tool_calls:\n                for tool_call_chunk in delta.tool_calls:\n                    if tool_call_chunk.index is not None:\n                        \n                        while len(tool_calls) <= tool_call_chunk.index:\n                            tool_calls.append({\n                                \"id\": \"\",\n                                \"type\": \"function\",\n                                \"function\": {\n                                    \"name\": \"\",\n                                    \"arguments\": \"\"\n                                }\n                            })\n\n                        tc = tool_calls[tool_call_chunk.index]\n\n                        if tool_call_chunk.id:\n                            tc[\"id\"] += tool_call_chunk.id\n                        if tool_call_chunk.function.name:\n                            tc[\"function\"][\"name\"] += tool_call_chunk.function.name\n                        if tool_call_chunk.function.arguments:\n                            tc[\"function\"][\"arguments\"] += tool_call_chunk.function.arguments\n\n            finish_reason = chunk.choices[0].finish_reason\n            print(finish_reason)\n        if finish_reason == \"tool_calls\":\n            for tool_call in tool_calls:\n                tool_call_name = tool_call['function']['name']\n                tool_call_arguments = json.loads(tool_call['function']['arguments'])\n                tool_function = tool_map[tool_call_name] \n                tool_result = tool_function(tool_call_arguments)\n                print(tool_result)\n                messages.append({\n                    \"role\": \"tool\",\n                    \"tool_call_id\": tool_call['id'],\n                    \"name\": tool_call_name,\n                    \"content\": json.dumps(tool_result),\n                })\n            \n            msg = ''\n\n    print(msg)\n```\n\n\n\n### Environment\n\nPython: 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA H200\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.8, V12.8.93\nCUDA Driver Version: 550.163.01\nPyTorch: 2.7.1+cu126\nsglang: 0.4.9.post1\nsgl_kernel: 0.2.4\nflashinfer_python: 0.2.7.post1\ntriton: 3.3.1\ntransformers: 4.53.0\ntorchao: 0.9.0\nnumpy: 2.2.6\naiohttp: 3.12.13\nfastapi: 0.115.14\nhf_transfer: 0.1.9\nhuggingface_hub: 0.33.2\ninteregular: 0.3.3\nmodelscope: 1.27.1\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.7\npython-multipart: 0.0.20\npyzmq: 27.0.0\nuvicorn: 0.35.0\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.20\nopenai: 1.93.0\ntiktoken: 0.9.0\nanthropic: 0.56.0\nlitellm: 1.73.6\ndecord: 0.6.0\nNVIDIA Topology:\n\tGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tNIC4\tNIC5\tNIC6\tNIC7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tPIXPHB\tPHB\tPHB\t0-63\t0\t\tN/A\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tPHBPIX\tPHB\tPHB\t0-63\t0\t\tN/A\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tPHBPHB\tPIX\tPHB\t0-63\t0\t\tN/A\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\tSYS\tSYS\tPHBPHB\tPHB\tPIX\t0-63\t0\t\tN/A\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tPIX\tPHB\tPHB\tPHB\tSYSSYS\tSYS\tSYS\t64-127\t1\t\tN/A\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tPHB\tPIX\tPHB\tPHB\tSYSSYS\tSYS\tSYS\t64-127\t1\t\tN/A\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tPHB\tPHB\tPIX\tPHB\tSYSSYS\tSYS\tSYS\t64-127\t1\t\tN/A\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tPHB\tPHB\tPHB\tPIX\tSYSSYS\tSYS\tSYS\t64-127\t1\t\tN/A\nNIC0\tSYS\tSYS\tSYS\tSYS\tPIX\tPHB\tPHB\tPHB\t X \tPHB\tPHB\tPHB\tSYSSYS\tSYS\tSYS\nNIC1\tSYS\tSYS\tSYS\tSYS\tPHB\tPIX\tPHB\tPHB\tPHB\t X \tPHB\tPHB\tSYSSYS\tSYS\tSYS\nNIC2\tSYS\tSYS\tSYS\tSYS\tPHB\tPHB\tPIX\tPHB\tPHB\tPHB\t X \tPHB\tSYSSYS\tSYS\tSYS\nNIC3\tSYS\tSYS\tSYS\tSYS\tPHB\tPHB\tPHB\tPIX\tPHB\tPHB\tPHB\t X \tSYSSYS\tSYS\tSYS\nNIC4\tPIX\tPHB\tPHB\tPHB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X PHB\tPHB\tPHB\nNIC5\tPHB\tPIX\tPHB\tPHB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPHB X \tPHB\tPHB\nNIC6\tPHB\tPHB\tPIX\tPHB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPHBPHB\t X \tPHB\nNIC7\tPHB\tPHB\tPHB\tPIX\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPHBPHB\tPHB\t X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n\n\nHypervisor vendor: KVM\nulimit soft: 1048576",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-10T18:28:23+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7934/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7934"
  }
]
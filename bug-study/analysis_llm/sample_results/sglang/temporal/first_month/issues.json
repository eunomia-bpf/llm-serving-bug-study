[
  {
    "number": 183,
    "title": "Supporting api vendors of Mixtral",
    "body": "Could this work with an OpenAI compatable vendor API like together or firework? I would like to use mixtral but would rather not host it. I tried but the openai implementation is hardcoded for openai model. ",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-11T21:05:55+00:00",
    "closed_at": "2024-02-12T09:07:15+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/183/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/183"
  },
  {
    "number": 179,
    "title": "Port fused MoE Kernels",
    "body": "There is a recent PR (https://github.com/vllm-project/vllm/pull/2542) in vLLM that introduced some fused kernels to accelerate mixtral MoE models.\r\n\r\nWe can bring it to our [code](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/models/mixtral.py) as well. ",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-11T16:29:58+00:00",
    "closed_at": "2024-07-28T03:14:29+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/179/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/179"
  },
  {
    "number": 176,
    "title": "[model request] Camelidae/Sparsestral",
    "body": "These are parameter efficient MoE models that claim to have performance better than Mixtral.\n\nCamildae\nhttps://github.com/wuhy68/Parameter-Efficient-MoE\n\nSparsestral\nhttps://huggingface.co/serpdotai/sparsetral-16x7B-v2\n\nSparsestral has a vllm implementation in this form\nhttps://github.com/serp-ai/vllm",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-02-09T22:49:30+00:00",
    "closed_at": "2024-07-25T06:32:07+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/176/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/176"
  },
  {
    "number": 173,
    "title": "Incorrect token usage with jump forward",
    "body": "Since jump forward breaks a decoding process to multiple ones, the number of prompt_tokens and completion_tokens are incorrect. Here is an example:\r\n\r\nRequest:\r\n\r\n```python\r\nregex = (r\"\"\"\\{\\n\"\"\"\r\n    + r\"\"\"  \"name\": \"[\\w]{1,8}\",\\n\"\"\"\r\n    + r\"\"\"  \"description\": \"[\\w\\d\\s]{1,64}\"\\n\"\"\"\r\n    + r\"\"\"\\}\"\"\"\r\n)\r\n\r\nresponse = requests.post(\r\n    url + \"/generate\",\r\n    json={\r\n        \"text\": \"Here is the info of France's capital: \",\r\n        \"sampling_params\": {\r\n            \"temperature\": 0,\r\n            \"max_new_tokens\": 128,\r\n            \"regex\": regex\r\n        },\r\n        \"stream\": True,\r\n    },\r\n    stream=True,\r\n)\r\n```\r\n\r\nStreaming response by chunk:\r\n\r\n```\r\nChunk (prompt 10, decode 1): {\r\n\r\nChunk (prompt 15, decode 1): {\r\n  \"name\": \"Paris\r\n\r\nChunk (prompt 22, decode 1): {\r\n  \"name\": \"Paris\",\r\n  \"description\": \"Capital\r\n\r\nChunk (prompt 22, decode 2): {\r\n  \"name\": \"Paris\",\r\n  \"description\": \"Capital city\r\n\r\nChunk (prompt 22, decode 10): {\r\n  \"name\": \"Paris\",\r\n  \"description\": \"Capital city of France and one of the most beautiful\r\n\r\nChunk (prompt 37, decode 1): {\r\n  \"name\": \"Paris\",\r\n  \"description\": \"Capital city of France and one of the most beautiful cities in\"\r\n}\r\n```\r\n\r\nNon-streaming response:\r\n\r\n```\r\n{'prompt_tokens': 37, 'completion_tokens': 1, 'id': '44f7ddf966de459da6954d0de1e4434d'}\r\n{\r\n  \"name\": \"Paris\",\r\n  \"description\": \"Capital city of France and one of the most beautiful cities in\"\r\n}\r\n```\r\n\r\nNote that the correct number of prompt tokens is 10 and the number of completion tokens is 28. We may fix the prompt token issue by taking the number of the first chunk, but we probably need to directly lookup the length of the final decoding output to fix the completion tokens.\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-09T18:12:43+00:00",
    "closed_at": "2024-02-10T04:06:16+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/173/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/173"
  },
  {
    "number": 171,
    "title": "OpenAI compatible API and JSON schema enforcing ",
    "body": "Hi! I just wanted to ask if it is possible to use OpenAI compatible API in sglang with local models and to force a certain json schema. IIRC original openai api only supports `{ \"type\": \"json_object\" }` which allows generating any json objects. If currently it is not possible, introducing something like `_schema` as optional property may be a solution until a similar functionality appears in the openai specification. ",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-09T04:08:20+00:00",
    "closed_at": "2024-02-11T01:21:34+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/171/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/171"
  },
  {
    "number": 169,
    "title": "Performance issue comparing sglang to vllm. ",
    "body": "Hi there, Amazing work on the RadixAttention and json contained decoding. I am running into some unexcited performance issue comparing sglang and vllm. I use latest pip of vllm, and use git-clone-ed sglang as of today. \r\n\r\n\r\nhere is my code to launch sglang \r\n`python -m sglang.launch_server --model-path NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO --port 30000 --tp 8`\r\n\r\nHere is my code to launch v-llm \r\n\r\npython -m vllm.entrypoints.openai.api_server     --model NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO --tensor-parallel-size 8\r\n\r\nBoth running with the same Conda with  CUDA 12.1 environment, 8x a10g on aws. \r\nHere is the openai-compatible curl request\r\n\r\n'curl http://localhost:8000/v1/chat/completions \\\r\n    -H \"Content-Type: application/json\" \\\r\n    -d '{\r\n        \"model\": \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\",\r\n        \"messages\": [\r\n            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\r\n            {\"role\": \"user\", \"content\": \"You are a helpful AI assistant. List 3 countries and their capitals.\"}\r\n        ]\r\n    }\r\n'\r\nThe SG-lang one is giving me 10 second of latency, while the vllm is giving 0.45 second. The number are reported after the first run to avoid any cold-start issue. \r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-08T23:26:03+00:00",
    "closed_at": "2024-02-11T14:00:42+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/169/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/169"
  },
  {
    "number": 167,
    "title": "ModuleNotFoundError: No module named 'zmq'",
    "body": "Hi!\r\n\r\nShould zmq be on your dependencies list?\r\n\r\n```\r\n2024-02-08 09:56:23 | ERROR | stderr | Traceback (most recent call last):\r\n2024-02-08 09:56:23 | ERROR | stderr |   File \"/p/haicluster/llama/FastChat/fastchat/serve/sglang_worker.py\", line 269, in <module>\r\n2024-02-08 09:56:23 | ERROR | stderr |     runtime = sgl.Runtime(\r\n2024-02-08 09:56:23 | ERROR | stderr |               ^^^^^^^^^^^^\r\n2024-02-08 09:56:23 | ERROR | stderr |   File \"/p/haicluster/llama/FastChat/sc_venv_2024/venv/lib/python3.11/site-packages/sglang/api.py\", line 37, in Runtime\r\n2024-02-08 09:56:23 | ERROR | stderr |     from sglang.srt.server import Runtime\r\n2024-02-08 09:56:23 | ERROR | stderr |   File \"/p/haicluster/llama/FastChat/sc_venv_2024/venv/lib/python3.11/site-packages/sglang/srt/server.py\", line 30, in <module>\r\n2024-02-08 09:56:23 | ERROR | stderr |     from sglang.srt.managers.detokenizer_manager import start_detokenizer_process\r\n2024-02-08 09:56:23 | ERROR | stderr |   File \"/p/haicluster/llama/FastChat/sc_venv_2024/venv/lib/python3.11/site-packages/sglang/srt/managers/detokenizer_manager.py\", line 4, in <module>\r\n2024-02-08 09:56:23 | ERROR | stderr |     import zmq\r\n2024-02-08 09:56:23 | ERROR | stderr | ModuleNotFoundError: No module named 'zmq'\r\nsrun: error: haicluster2: task 0: Exited with exit code 1\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-08T09:58:25+00:00",
    "closed_at": "2024-02-11T13:51:39+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/167/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/167"
  },
  {
    "number": 166,
    "title": "Add SGLang usage examples",
    "body": "List some good use cases of SGLang here:\r\n- [SELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures](https://arxiv.org/pdf/2402.03620.pdf)\r\n- [Tractable Control for Autoregressive Language Generation](https://starai.cs.ucla.edu/papers/ZhangICML23.pdf)",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-02-08T08:51:53+00:00",
    "closed_at": "2024-09-08T01:13:00+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/166/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/166"
  },
  {
    "number": 165,
    "title": "setting mem-fraction-static to a lower value causes error",
    "body": "With no change, I run out of memory (A100 w/ 24GB). Setting it to anything other than the default causes the following error:\r\n\r\n```\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/workspace/sglang/python/sglang/srt/managers/router/model_rpc.py\", line 170, in exposed_step\r\n    self.forward_step()\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/workspace/sglang/python/sglang/srt/managers/router/model_rpc.py\", line 185, in forward_step\r\n    self.forward_fill_batch(new_batch)\r\n  File \"/workspace/sglang/python/sglang/srt/managers/router/model_rpc.py\", line 387, in forward_fill_batch\r\n    batch.prepare_for_extend(\r\n  File \"/workspace/sglang/python/sglang/srt/managers/router/infer_batch.py\", line 203, in prepare_for_extend\r\n    req_pool_indices_cpu = req_pool_indices.cpu().numpy()\r\nAttributeError: 'NoneType' object has no attribute 'cpu'\r\n```\r\n\r\nFor reference I am attempting to use gen with a very large set of items to select through to limit inference tokens (thousands)",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-02-07T22:25:25+00:00",
    "closed_at": "2024-07-25T06:33:36+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/165/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/165"
  },
  {
    "number": 163,
    "title": "RuntimeEndpoint doesn't work if endpoint requires Auth",
    "body": "Hi, \r\n\r\nCame across an interesting use case today. I was trying to host the model on databricks. But call it locally. But databricks requires a personal access token to be passed in via the headers.",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-07T19:42:23+00:00",
    "closed_at": "2024-02-08T07:14:12+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/163/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/163"
  },
  {
    "number": 161,
    "title": "offset = input_ids.index(self.config.image_token_index)",
    "body": "anyone faced this problem before ? :\r\n\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/home/fjaadari/anaconda3/envs/llava/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 161, in exposed_step\r\n    self.handle_generate_request(recv_req)\r\n  File \"/home/fjaadari/anaconda3/envs/llava/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 243, in handle_generate_request\r\n    req.input_ids, req.image_offset = self.model_runner.model.pad_input_ids(\r\n  File \"/home/fjaadari/anaconda3/envs/llava/lib/python3.10/site-packages/sglang/srt/models/llava.py\", line 63, in pad_input_ids\r\n    offset = input_ids.index(self.config.image_token_index)\r\nValueError: 32000 is not in list\r\n\r\n",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-02-07T16:56:33+00:00",
    "closed_at": "2024-07-25T06:32:55+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/161/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/161"
  },
  {
    "number": 159,
    "title": "initialise model with max_model_len",
    "body": "Similar to how vllm has the 'max_model_length' when starting the server. Can we have this here too?\r\n\r\nThis would help when trying to host models on smaller gpus. For example with vllm Mistral 7b with 32k context doesn't fit on a single 24GB GPU. Whereas with 8k context it does. ",
    "labels": [
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-02-07T11:57:06+00:00",
    "closed_at": "2024-02-21T00:22:57+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/159/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/159"
  },
  {
    "number": 158,
    "title": "About quantization model inference",
    "body": "Actually\uff0cI don't know how use quantize model inference in this project and vllm. Due my limited knowledge, they load quantize model and dequantize it for ops in vllm?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-07T08:05:36+00:00",
    "closed_at": "2024-02-11T14:28:51+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/158/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/158"
  },
  {
    "number": 157,
    "title": "Development Roadmap  (Deprecated)",
    "body": "## Function Calling\r\n- Frontend\r\n    - Add `tools` argument in `sgl.gen`. See also guidance [tools](https://github.com/guidance-ai/guidance/blob/d1bbe1c698cbb201f89556d71193993e78c0686b/README.md?plain=1#L102)\r\n- Backend\r\n    - OpenAI: Translate to their function calling API (https://platform.openai.com/docs/guides/function-calling).\r\n      - #573 \r\n    - Local Models (SGLang)\r\n        1. Use SGLang primitives (regex, select) and constrained decoding to implement a workflow \r\n        2. Directly use models that support function calling (e.g., Gorilla OpenFunctions, https://huggingface.co/jondurbin/bagel-dpo-7b-v0.4#prompting-strategies)\r\n     - Local Models (OpenAI-compatible API)\r\n\r\n## High-level Pythonic Interface\r\n - #39\r\n \r\n## Inference Optimizations\r\n- Speculative decoding for local models\r\n- Speculative execution for OpenAI Chat API\r\n  - #48 \r\n\r\n## Structured Decoding\r\n- Support parallel JSON decoding https://github.com/varunshenoy/super-json-mode/issues/8\r\n- Support auto parallel decoding  https://arxiv.org/abs/2401.06761\r\n\r\n## Compiler\r\n- Support tracing and compiling `sgl.fork` \r\n- Support sending a full serialized SGL program to the server\r\n\r\n## LoRA Support\r\n- Port multi-LoRA batching and unified memory from S-LoRA\r\n\r\n## Model Coverage\r\n- Vision Langauge Models. Support top-performing models from https://github.com/open-compass/VLMEvalKit\r\n- Language Models. Port the implementation of popular models from https://github.com/vllm-project/vllm/tree/main/vllm/model_executor/models. ([help](https://github.com/sgl-project/sglang/blob/main/docs/model_support.md))\r\n## Device Coverage\r\n- AMD support. Investigate AMD support in Trion and FlashInfer.\r\n- CPU support. This is better done by adding a llama.cpp backend.",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-07T07:13:40+00:00",
    "closed_at": "2024-07-17T02:23:05+00:00",
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/157/reactions",
      "total_count": 25,
      "+1": 8,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 9,
      "rocket": 8,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/157"
  },
  {
    "number": 154,
    "title": "`RecursionError: maximum recursion depth exceeded while calling a Python object` when inferencing with long input",
    "body": "Hi, I ran across this issue during inference\r\n```bash\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/User/jay/miniconda3/envs/sglang/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 168, in exposed_step\r\n    self.forward_step()\r\n  File \"/User/jay/miniconda3/envs/sglang/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/User/jay/miniconda3/envs/sglang/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 195, in forward_step\r\n    self.forward_decode_batch(self.running_batch)\r\n  File \"/User/jay/miniconda3/envs/sglang/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 460, in forward_decode_batch\r\n    self.handle_finished_requests(batch)\r\n  File \"/User/jay/miniconda3/envs/sglang/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 528, in handle_finished_requests\r\n    prefix_len = self.tree_cache.insert(\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/User/jay/miniconda3/envs/sglang/lib/python3.11/site-packages/sglang/srt/managers/router/radix_cache.py\", line 61, in insert\r\n    return self._insert_helper(self.root_node, key, value)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/User/jay/miniconda3/envs/sglang/lib/python3.11/site-packages/sglang/srt/managers/router/radix_cache.py\", line 157, in _insert_helper\r\n    return prefix_len + self._insert_helper(child, key, value)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/User/jay/miniconda3/envs/sglang/lib/python3.11/site-packages/sglang/srt/managers/router/radix_cache.py\", line 157, in _insert_helper\r\n    return prefix_len + self._insert_helper(child, key, value)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/User/jay/miniconda3/envs/sglang/lib/python3.11/site-packages/sglang/srt/managers/router/radix_cache.py\", line 157, in _insert_helper\r\n    return prefix_len + self._insert_helper(child, key, value)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  [Previous line repeated 958 more times]\r\n  File \"/User/jay/miniconda3/envs/sglang/lib/python3.11/site-packages/sglang/srt/managers/router/radix_cache.py\", line 166, in _insert_helper\r\n    new_node = TreeNode()\r\n               ^^^^^^^^^^\r\n  File \"/User/jay/miniconda3/envs/sglang/lib/python3.11/site-packages/sglang/srt/managers/router/radix_cache.py\", line 12, in __init__\r\n    self.children = defaultdict(TreeNode)\r\n                    ^^^^^^^^^^^^^^^^^^^^^\r\nRecursionError: maximum recursion depth exceeded while calling a Python object\r\n```\r\nWould it be possible to implement this logic without recursion? @merrymercy\r\n",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-02-06T18:44:35+00:00",
    "closed_at": "2024-07-25T06:33:07+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/154/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/154"
  },
  {
    "number": 152,
    "title": "unable to install uvloop in windows (dependency)",
    "body": "Getting this error\r\n\r\npip install sglang[all]\r\nRequirement already satisfied: sglang[all] in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (0.1.11)\r\nRequirement already satisfied: requests in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from sglang[all]) (2.31.0)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from requests->sglang[all]) (3.3.2)\r\nRequirement already satisfied: idna<4,>=2.5 in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from requests->sglang[all]) (3.6)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from requests->sglang[all]) (2.1.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from requests->sglang[all]) (2023.11.17)\r\nCollecting anthropic (from sglang[all])\r\n  Using cached anthropic-0.15.0-py3-none-any.whl.metadata (15 kB)\r\nRequirement already satisfied: numpy in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from sglang[all]) (1.24.4)\r\nRequirement already satisfied: openai>=1.0 in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from sglang[all]) (1.6.1)\r\nRequirement already satisfied: aiohttp in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from sglang[all]) (3.9.1)\r\nRequirement already satisfied: fastapi in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from sglang[all]) (0.109.2)\r\nRequirement already satisfied: psutil in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from sglang[all]) (5.9.7)\r\nCollecting rpyc (from sglang[all])\r\n  Using cached rpyc-5.3.1-py3-none-any.whl (74 kB)\r\nRequirement already satisfied: torch in c:\\users\\keshav s\\anaconda3\\envs\\llm-workspace\\lib\\site-packages (from sglang[all]) (2.1.2)\r\nCollecting uvloop (from sglang[all])\r\n  Using cached uvloop-0.19.0.tar.gz (2.3 MB)\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  \u00d7 Getting requirements to build wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [15 lines of output]\r\n      Traceback (most recent call last):\r\n        File \"C:\\Users\\Keshav S\\anaconda3\\envs\\llm-workspace\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\r\n          main()\r\n        File \"C:\\Users\\Keshav S\\anaconda3\\envs\\llm-workspace\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\r\n          json_out['return_val'] = hook(**hook_input['kwargs'])\r\n        File \"C:\\Users\\Keshav S\\anaconda3\\envs\\llm-workspace\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\r\n          return hook(config_settings)\r\n        File \"C:\\Users\\Keshav S\\AppData\\Local\\Temp\\pip-build-env-xnl04q07\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 325, in get_requires_for_build_wheel\r\n          return self._get_build_requires(config_settings, requirements=['wheel'])\r\n        File \"C:\\Users\\Keshav S\\AppData\\Local\\Temp\\pip-build-env-xnl04q07\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 295, in _get_build_requires\r\n          self.run_setup()\r\n        File \"C:\\Users\\Keshav S\\AppData\\Local\\Temp\\pip-build-env-xnl04q07\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 311, in run_setup\r\n          exec(code, locals())\r\n        File \"<string>\", line 8, in <module>\r\n      RuntimeError: uvloop does not support Windows at the moment\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: subprocess-exited-with-error\r\n\r\n\u00d7 Getting requirements to build wheel did not run successfully.\r\n\u2502 exit code: 1\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This error originates from a subprocess, and is likely not a problem with pip.",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-02-06T18:02:46+00:00",
    "closed_at": "2024-07-25T06:32:08+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/152/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/152"
  },
  {
    "number": 151,
    "title": "regex support for openai models?",
    "body": "Hi I was trying to use the regex constrained output with an OpenAI backend but it threw a warning saying regex is not supported for openai backend.",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-06T17:38:31+00:00",
    "closed_at": "2024-02-06T19:53:41+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/151/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/151"
  },
  {
    "number": 150,
    "title": "After enabling tensor parallelism (tp-size=2), there is no response",
    "body": "my command is:\r\n```shell\r\nCUDA_VISIBLE_DEVICES=\"2,4\" python -m sglang.launch_server --model-path  ./Yi-34B-Chat --trust-remote-code --port 30000 --tp-size 2 \r\n``` \r\nwhen I run the demo code, **there is nothing returned.** \r\n\r\n```python\r\nfrom sglang import function, system, user, assistant, gen, set_default_backend, RuntimeEndpoint\r\n\r\n@function\r\ndef multi_turn_question(s, question_1, question_2):\r\n    s += system(\"You are a helpful assistant.\")\r\n    s += user(question_1)\r\n    s += assistant(gen(\"answer_1\", max_tokens=256))\r\n    s += user(question_2)\r\n    s += assistant(gen(\"answer_2\", max_tokens=256))\r\n\r\nset_default_backend(RuntimeEndpoint(\"http://localhost:30000\"))\r\n\r\nstate = multi_turn_question.run(\r\n    question_1=\"What is the capital of the United States?\",\r\n    question_2=\"List two local attractions.\",\r\n)\r\n\r\nfor m in state.messages():\r\n    print(m[\"role\"], \":\", m[\"content\"])\r\n```\r\n\r\n**But when I  remove  \"--tp-size 2 \" in the command ,which means the model is only in 1 GPU , it works well.**\r\n",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-02-06T12:51:16+00:00",
    "closed_at": "2024-07-25T06:32:47+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/150/reactions",
      "total_count": 2,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/150"
  },
  {
    "number": 149,
    "title": "\u5206\u7c7b\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u6027\u80fd\u63d0\u5347\u4e00\u500d\uff0c\u548b\u89e3\u51b3\u5462\uff1f",
    "body": "qwen\u81ea\u5e26\u63a8\u7406\uff1a\r\n\u7b2c\u4e00\u8eab\u4efd\u51c6\u786e\u7387\uff1a0.968421052631579\r\n\u7b2c\u4e8c\u8eab\u4efd\u51c6\u786e\u7387\uff1a0.9368421052631579\r\n\u4e00\u6761\u6570\u636e\u5e73\u5747\u8017\u65f6\uff1a0.9585727064233077\r\n\r\nslang\u63a8\u7406\uff1a\r\n\u7b2c\u4e00\u8eab\u4efd\u51c6\u786e\u7387\uff1a0.6631578947368421\r\n\u7b2c\u4e8c\u8eab\u4efd\u51c6\u786e\u7387\uff1a0.7894736842105263\r\n\u4e00\u6761\u6570\u636e\u5e73\u5747\u8017\u65f6\uff1a0.5773725032806396",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-02-06T07:44:37+00:00",
    "closed_at": "2024-07-25T06:32:09+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/149/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/149"
  },
  {
    "number": 145,
    "title": "Add support for scalar values",
    "body": "I often need an LLM to generate scalar values.\r\nI look at the logprobs of True and False and do True - False.\r\nWorks very well for me:\r\n```\r\nf'{output_text}\\n\\nRate whether the text is well written.', '{\"is_well_written_bool\": '\r\n```\r\n\r\nAlso the values are nicely distributed:\r\n```\r\nis_great_comment_bool: 0.72  \r\nis_funny_bool_opt: -0.79  \r\nneg_sounds_like_chatgpt_bool: -0.49  \r\nneg_contains_placeholders_bool: -0.86  \r\nneg_sounds_awkward_bool: -0.97 \r\n is_written_by_human: 0.94  \r\nneg_sounds_robotic: -0.89\r\n```\r\n\r\nCould you add something like this into sglang?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-05T18:12:49+00:00",
    "closed_at": "2024-02-05T23:47:19+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/145/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/145"
  },
  {
    "number": 143,
    "title": "Error under high concurrency. `sqlite3.DatabaseError: database disk image is malformed`",
    "body": "The error stack is as follows:\r\n```bash\r\nFile \"/User/jay/sglang/python/sglang/api.py\", line 37, in Runtime\r\n  from sglang.srt.server import Runtime\r\nFile \"/User/jay/sglang/python/sglang/srt/server.py\", line 47, in <module>\r\n  from sglang.srt.managers.router.manager import start_router_process\r\nFile \"/User/jay/sglang/python/sglang/srt/managers/router/manager.py\", line 8, in <module>\r\n  from sglang.srt.managers.router.model_rpc import ModelRpcClient\r\nFile \"/User/jay/sglang/python/sglang/srt/managers/router/model_rpc.py\", line 15, in <module>\r\n  from sglang.srt.constrained.fast_forward import FastForwardCache\r\nFile \"/User/jay/sglang/python/sglang/srt/constrained/fast_forward.py\", line 2, in <module>\r\n  from sglang.srt.constrained.disk_cache import disk_cache\r\nFile \"/User/jay/sglang/python/sglang/srt/constrained/disk_cache.py\", line 13, in <module>\r\n  memory = Cache(cache_dir, eviction_policy=\"none\", cull_limit=0)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"/User/jay/miniconda3/envs/sglang/lib/python3.11/site-packages/diskcache/core.py\", line 500, in __init__\r\n  self.reset(key, value)\r\nFile \"/User/jay/miniconda3/envs/sglang/lib/python3.11/site-packages/diskcache/core.py\", line 2409, in reset\r\n  sql_retry(statement, (value, key))\r\nFile \"/User/jay/miniconda3/envs/sglang/lib/python3.11/site-packages/diskcache/core.py\", line 666, in _execute_with_retry\r\n  return sql(statement, *args, **kwargs)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nsqlite3.DatabaseError: database disk image is malformed\r\n```\r\nShould be related to https://github.com/sgl-project/sglang/blob/82fa69b3cc0c8b9b3b31148f1d53070649f0d433/python/sglang/srt/constrained/disk_cache.py#L13\r\n\r\n\r\nWould it be fine to change this line\r\nhttps://github.com/sgl-project/sglang/blob/82fa69b3cc0c8b9b3b31148f1d53070649f0d433/python/sglang/srt/constrained/disk_cache.py#L12\r\nto\r\n```python\r\ncache_dir = os.environ.get(\"SGLANG_CACHE_DIR\", f\"{home_dir}/.cache/sglang/{os.getpid()}\")\r\n```\r\n\r\nProbably @hnyls2002",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-05T03:52:57+00:00",
    "closed_at": "2024-02-07T17:39:06+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/143/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/143"
  },
  {
    "number": 140,
    "title": "KV cache pool leak detected!",
    "body": "\r\npython -m sglang.launch_server --model-path /root/autodl-tmp/Yi-6B-Chat --port 8000 --mem-fraction-static 0.9 --tokenizer-mode auto --tokenizer-path /root/autodl-tmp/Yi-6B-Chat --trust-remote-code\r\n\r\n<img width=\"1439\" alt=\"image\" src=\"https://github.com/sgl-project/sglang/assets/4583537/dd6decd6-99a1-420e-8be2-b9a130d972fb\">\r\n\r\npython3 bench_throughput.py  --tokenizer /root/autodl-tmp/Yi-6B-Chat  --dataset /root/autodl-tmp/ShareGPT_V3_unfiltered_cleaned_split.json   --port 8000 --backend srt --trust-remote-code --num-prompts 2048\r\n\r\n<img width=\"1512\" alt=\"image\" src=\"https://github.com/sgl-project/sglang/assets/4583537/5a47f9e6-e356-4269-859a-0e0e395e6d89\">\r\n\r\n",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-02-04T08:37:50+00:00",
    "closed_at": "2024-08-05T01:05:13+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/140/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/140"
  },
  {
    "number": 139,
    "title": "/generate stuck and no response when serving the Mixtral AWQ",
    "body": "Hi, I have been trying to launch Mixtral AWQ with 2 A10 GPUs. Here is my command:\r\n\r\n```bash\r\npython -m sglang.launch_server --model-path TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ --tp 2\r\n```\r\n\r\nThe output appears to be correct with the following standard output:\r\n\r\n```bash\r\nServer started on [0.0.0.0]:10006\r\nServer started on [0.0.0.0]:10007\r\nAccepted ('127.0.0.1', 36290) with file descriptor 5\r\nAccepted ('127.0.0.1', 37546) with file descriptor 5\r\nWelcome ('127.0.0.1', 37546)\r\nWelcome ('127.0.0.1', 36290)\r\nRank 0: Load weight begins.\r\nQuant_config: AWQConfig(weight_bits=4, group_size=128, zero_point=True)\r\nRank 1: Load weight begins.\r\nQuant_config: AWQConfig(weight_bits=4, group_size=128, zero_point=True)\r\nINFO 02-04 07:17:50 weight_utils.py:164] Using model weights format ['*.safetensors']\r\nINFO 02-04 07:17:50 weight_utils.py:164] Using model weights format ['*.safetensors']\r\nRank 0: Load weight ends.\r\nRank 1: Load weight ends.\r\nRank 1: Max_total_num_token=115621, max_prefill_num_token=32768, context_len=32768, model_mode=[]\r\nRank 0: Max_total_num_token=115621, max_prefill_num_token=32768, context_len=32768, model_mode=[]\r\n```\r\n\r\nWhen I try to execute the following command:\r\n\r\n```bash\r\ncurl 127.0.0.1:30000/get_model_info\r\n```\r\n\r\nIt receives the correct response: `{\"model_path\":\"TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ\"}`. However, when attempting the following command:\r\n\r\n```bash\r\ncurl http://localhost:30000/generate   -H \"Content-Type: application/json\"   -d '{\r\n    \"text\": \"Once upon a time,\",\r\n    \"sampling_params\": {\r\n      \"max_new_tokens\": 16,\r\n      \"temperature\": 0\r\n    }\r\n  }'\r\n```\r\n\r\nIt gets stuck indefinitely.",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-02-04T07:28:25+00:00",
    "closed_at": "2024-07-25T06:32:05+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/139/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/139"
  },
  {
    "number": 138,
    "title": "Error on json decoding with llava",
    "body": "Encountered the following error when using the `regex` in call to `gen` method. Server doesn't work afterwards:\r\n\r\n```console\r\n~$ python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.5-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --port 30000\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nRank 0: load weight begin.\r\n/opt/conda/envs/llava_test/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\r\n  return self.fget.__get__(instance, owner)()\r\nINFO 02-03 23:02:55 weight_utils.py:164] Using model weights format ['*.bin']\r\nINFO 02-03 23:03:01 weight_utils.py:164] Using model weights format ['*.bin']\r\nRank 0: load weight end.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nRank 0: max_total_num_token=44713, max_prefill_num_token=7452, context_len=4096, model_mode=[]\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nINFO:     Started server process [66164]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)\r\nINFO:     127.0.0.1:51098 - \"GET /get_model_info HTTP/1.1\" 200 OK\r\nnew fill batch. #seq: 1. #cached_token: 0. #new_token: 646. #remaining_req: 0. #running_req: 0. tree_cache_hit_rate: 0.00%.\r\n#running-req: 1, #token: 646, token usage: 0.01, #queue-req: 0\r\nINFO:     127.0.0.1:58036 - \"POST /generate HTTP/1.1\" 200 OK\r\nnew fill batch. #seq: 1. #cached_token: 645. #new_token: 1. #remaining_req: 0. #running_req: 0. tree_cache_hit_rate: 49.92%.\r\nnew fill batch. #seq: 1. #cached_token: 35. #new_token: 39. #remaining_req: 0. #running_req: 0. tree_cache_hit_rate: 49.78%.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [0,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n\r\n...\r\n\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [29,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [29,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [29,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [29,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [29,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 168, in exposed_step\r\n    self.forward_step()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 183, in forward_step\r\n    self.forward_fill_batch(new_batch)\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 391, in forward_fill_batch\r\n    logits, (logprobs, normalized_logprobs) = self.model_runner.forward(\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_runner.py\", line 460, in forward\r\n    return self.forward_extend_multi_modal(**kwargs)\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_runner.py\", line 437, in forward_extend_multi_modal\r\n    return self.model.forward(\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/models/llava.py\", line 107, in forward\r\n    .cpu()\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 168, in exposed_step\r\n    self.forward_step()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 179, in forward_step\r\n    new_batch = self.get_new_fill_batch()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 293, in get_new_fill_batch\r\n    self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/memory_pool.py\", line 92, in available_size\r\n    return torch.sum(self.mem_state == 0).item()\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 168, in exposed_step\r\n    self.forward_step()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 179, in forward_step\r\n    new_batch = self.get_new_fill_batch()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 293, in get_new_fill_batch\r\n    self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/memory_pool.py\", line 92, in available_size\r\n    return torch.sum(self.mem_state == 0).item()\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 168, in exposed_step\r\n    self.forward_step()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 179, in forward_step\r\n    new_batch = self.get_new_fill_batch()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 293, in get_new_fill_batch\r\n    self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size()\r\n  File \"/opt/conda/envs/llava_test/lib/python3.10/site-packages/sglang/srt/memory_pool.py\", line 92, in available_size\r\n    return torch.sum(self.mem_state == 0).item()\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n...\r\n```\r\n\r\n## How to reproduce the error\r\n\r\n* Run the server with `python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.5-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --port 30000`\r\n* Run the following code:\r\n```python\r\nimport sglang as sgl\r\nfrom enum import Enum\r\nfrom pydantic import BaseModel\r\nfrom sglang.srt.constrained.json_schema import build_regex_from_object\r\nfrom sglang import function, system, user, assistant, gen, set_default_backend, RuntimeEndpoint\r\nset_default_backend(RuntimeEndpoint(\"http://localhost:30000\"))\r\n\r\n@sgl.function\r\ndef image_qa(s, image_path, question, regex=None):\r\n    s += sgl.user(sgl.image(image_path) + question)\r\n    s += sgl.assistant(sgl.gen(\"answer\", regex=regex))\r\n\r\nclass Color(str, Enum):\r\n    black = \"Black\"\r\n    orange = \"Orange\"\r\n    white = \"White\"\r\n\r\nclass Attributes(BaseModel):\r\n    color: Color\r\n\r\nstate = image_qa.run(\r\n    image_path=\"cat.jpg\",\r\n    question=\"What color is the cat? respond only in JSON format. The key should be color and the only values allowed are Black, Orange, White\",\r\n    regex=build_regex_from_object(Attributes),\r\n    max_new_tokens=200,\r\n    temperature=0\r\n)\r\nprint(state['answer'])\r\n```\r\n\r\n## Additional notes:\r\n* Tested on A100 40GB\r\n* sglang 0.1.11\r\n* torch 2.1.2\r\n* CUDA:\r\n  ```console\r\n  nvcc: NVIDIA (R) Cuda compiler driver\r\n  Copyright (c) 2005-2023 NVIDIA Corporation\r\n  Built on Mon_Apr__3_17:16:06_PDT_2023\r\n  Cuda compilation tools, release 12.1, V12.1.105\r\n  Build cuda_12.1.r12.1/compiler.32688072_0\r\n  ```\r\n* If I remove the `regex` parameter, the output is what I expect it to be (e.g. `' {\\n\"color\": \"Orange\",\\n}'`)\r\n* `json_decode.py` file with llama2 (`python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000`)  ran successfully so it might be a multimodal issue. As far as I know [Outlines](https://github.com/outlines-dev/outlines) doesn't support multimodals yet.\r\n  ```console\r\n  ~$ python json_decode.py\r\n  Hermione Granger is a character in Harry Potter. Please fill in the following information about this character.\r\n  {\r\n      \"name\": \"Hermione Granger\",\r\n      \"house\": \"Gryffindor\",\r\n      \"blood status\": \"Pure-blood\",\r\n      \"occupation\": \"student\",\r\n      \"wand\": {\r\n          \"wood\": \"cherry\",\r\n          \"core\": \"hornbeam\",\r\n          \"length\": 16.75\r\n      },\r\n      \"alive\": \"Alive\",\r\n      \"patronus\": \"toad\",\r\n      \"bogart\": \"Owl\"\r\n  }\r\n  ```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-03T23:12:32+00:00",
    "closed_at": "2024-02-04T02:46:17+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/138/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/138"
  },
  {
    "number": 131,
    "title": "llava-v1.6-vicuna-7b NoneType Object Error when handle_generate_request, maybe misspelled",
    "body": "sglang version: `sglang==0.1.10`\r\ntorch version: `2.1.0+cu118`\r\n\r\nWhen running the server via: \r\n\r\n`python3 -m sglang.launch_server --model-path ./llava-v1.6-vicuna-7b --tokenizer-path SurfaceData/llava-v1.6-vicuna-7b-processor --chat-template vicuna_v1.1 --port 30000`\r\n\r\nRunning the following test script :\r\n\r\n```\r\n@sgl.function\r\ndef pipeline(s, prompt, max_tokens):\r\n    for p in prompt:\r\n        if type(p) is str:\r\n            s += p\r\n        else:\r\n            s += sgl.image(p) # p would be PIL.Image\r\n    s += sgl.gen(\"response\", max_tokens=max_tokens)\r\n\r\nbackend = RuntimeEndpoint(sgl_endpoint)\r\nsgl.set_default_backend(backend)\r\npipeline.run(prompt, max_new_tokens, temperature=temperature, top_p=top_p, top_k=top_k, stream=True)\r\n```\r\n\r\nthe request triggered `handle_generate_request` in `srt.managers.router.model_rpc` and assert\r\n```\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/envs/sgl/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 159, in exposed_step\r\n    self.handle_generate_request(recv_req)\r\n  File \"/root/miniconda3/envs/sgl/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 240, in handle_generate_request\r\n    req.input_ids, req.image_offset = self.model_runner.model.pad_input_ids(\r\n  File \"/root/miniconda3/envs/sgl/lib/python3.10/site-packages/sglang/srt/models/llava.py\", line 49, in pad_input_ids\r\n    num_patch_width, num_patch_height = get_anyres_image_grid_shape(\r\n  File \"/root/miniconda3/envs/sgl/lib/python3.10/site-packages/sglang/srt/mm_utils.py\", line 121, in get_anyres_image_grid_shape\r\n    width, height = select_best_resolution(image_size, possible_resolutions)\r\n  File \"/root/miniconda3/envs/sgl/lib/python3.10/site-packages/sglang/srt/mm_utils.py\", line 22, in select_best_resolution\r\n    original_width, original_height = original_size\r\nTypeError: cannot unpack non-iterable NoneType object\r\n```\r\nbut the recv_req from the debugger shows pxel_values and image_size correctly\r\n\r\ni went through the sglang lib and i found line 242 of model_rpc.py is suspicious\r\n\r\nthe following is a snippet from `model_rpc.py`\r\n\r\n```\r\n    def handle_generate_request(\r\n        self,\r\n        recv_req: TokenizedGenerateReqInput,\r\n    ):\r\n        req = Req(recv_req.rid, recv_req.input_text, recv_req.input_ids)\r\n        req.pixel_values = recv_req.pixel_values\r\n        if req.pixel_values is not None:\r\n            print('pixel value is not none')\r\n            pad_value = [\r\n                (recv_req.image_hash) % self.model_config.vocab_size,\r\n                (recv_req.image_hash >> 16) % self.model_config.vocab_size,\r\n                (recv_req.image_hash >> 32) % self.model_config.vocab_size,\r\n                (recv_req.image_hash >> 64) % self.model_config.vocab_size,\r\n            ]\r\n            req.input_ids, req.image_offset = self.model_runner.model.pad_input_ids(\r\n                req.input_ids, pad_value, req.pixel_values.shape, req.image_size\r\n            )\r\n            req.image_size = recv_req.image_size\r\n        req.sampling_params = recv_req.sampling_params\r\n        req.return_logprob = recv_req.return_logprob\r\n        req.logprob_start_len = recv_req.logprob_start_len\r\n        req.stream = recv_req.stream\r\n        req.tokenizer = self.tokenizer\r\n```\r\nreq.image_size is None when self.model_runner.model.pad_input_ids is called\uff0c maybe it should be recv_req.image_size?\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-02T17:27:23+00:00",
    "closed_at": "2024-02-02T19:57:05+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/131/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/131"
  },
  {
    "number": 128,
    "title": "[Bug] liuhaotian/llava-v1.6-mistral-7b doesn't load",
    "body": "When trying to load the Mistral variant of LLaVa 1.6, I get an expected error:\r\n\r\n```sh\r\npython3 -m sglang.launch_server --model-path liuhaotian/llava-v1.6-mistral-7b --chat-template vicuna_v1.1 --port 30000\r\n```\r\n\r\n```\r\nValueError: The checkpoint you are trying to load has model type `llava_mistral` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date\r\n```\r\n\r\nTransformers doesn't treat the LLaVa variants any differently, they all use the same config.  I think this *could* be easily fixed by adding a mapping from `llava_mistral` to the `LlavaConfig` in the config mapping.  ",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-01-31T23:55:51+00:00",
    "closed_at": "2024-07-25T06:32:21+00:00",
    "comments": 16,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/128/reactions",
      "total_count": 8,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/128"
  },
  {
    "number": 127,
    "title": "[Bug] liuhaotian/llava-v1.6-vicuna-7b doesn't load",
    "body": "With LLaVA 1.6 out I wanted to see how they run.  I think there's one more pre-deploy step needed for the vicuna base model variant.\r\n\r\nWhen running the server via:\r\n\r\n```sh\r\npython3 -m sglang.launch_server --model-path liuhaotian/llava-v1.6-vicuna-7b  --chat-template vicuna_v1.1 --port 30000\r\n```\r\n\r\nThis fails because that variant isn't deployed with a huggingface pre-processor.  Someone will need to do the same HuggingFace conversion used to create [llava-hf/llava-1.5-7b-hf](https://huggingface.co/llava-hf/llava-1.5-7b-hf).  Maybe liuhaotian can do this so it's not on some other disjoint hf repo?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-31T23:53:39+00:00",
    "closed_at": "2024-02-01T01:38:24+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/127/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/127"
  },
  {
    "number": 126,
    "title": "Pydantic>2 causes issues loading models",
    "body": "Hi,\r\n\r\nRelated to this issue,\r\n\r\nBy default, since the pydantic version is not pinned, greater than version 2 is used which is causing issues with loading.\r\n\r\nHowever `pip install pydantic==1.10.14` resolves this issue and I can load models normally now.",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-01-31T17:31:27+00:00",
    "closed_at": "2024-07-25T06:32:13+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/126/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/126"
  },
  {
    "number": 124,
    "title": "Support gptq  quantization",
    "body": null,
    "labels": [
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-01-31T03:33:47+00:00",
    "closed_at": "2024-02-06T19:35:57+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/124/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/124"
  },
  {
    "number": 123,
    "title": "How to use 4bit on LLava?",
    "body": "I am using this code: \"python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.5-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --chat-template vicuna_v1.1 --port 30000\" , but I am not able to use any instruction for quantity 4-bit. \r\n\r\ncan you tell me how to use 4-bit llava on sglang?",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-01-31T03:06:50+00:00",
    "closed_at": "2024-07-25T06:32:04+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/123/reactions",
      "total_count": 5,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/123"
  },
  {
    "number": 122,
    "title": "Slow weight loading",
    "body": "Whenever I try to load the Mixtral models it takes very long and at the end instead of actually starting the server I get a similar error as the one here - https://github.com/sgl-project/sglang/issues/99\r\nThe same model save works in vLLM. \r\nMy setup - 2xA100-80GBs",
    "labels": [
      "help wanted",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-01-30T14:46:10+00:00",
    "closed_at": "2024-07-25T06:33:34+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/122/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/122"
  },
  {
    "number": 115,
    "title": "Crash in `tokenize_fast_forward`",
    "body": "I was trying constraint decoding with Qwen but got crash at this line: https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/managers/router/infer_batch.py#L60\r\n\r\nThis is because the output type of Qwen `tokenizer.convert_ids_to_tokens(...)` is not `str` but `bytes`, so the following modification works:\r\n\r\n```\r\nif self.tokenizer.convert_ids_to_tokens(self.output_ids[0]).decode().startswith(\"\u2581\"):\r\n```\r\n\r\nHowever, this is not a general solution, as not every tokenizer uses `\u2581` to represent the space. For example, the Falcon tokenizer uses `\u0120` to represent the space. A more general solution should be re-decoding so far output tokens with `tokenizer.decode(...)`. An alternative way is leveraging `tokenizer.decode(..., clean_up_tokenization_spaces=False)`. While this solution works with most tokenizers such as Falcon and Qwen, it doesn't work for Llama2:\r\n\r\n```\r\nllama2 = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\r\nfalcon = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\r\nqwen = AutoTokenizer.from_pretrained(\"Qwen/Qwen-1_8B-Chat\")\r\n\r\nllama2.decode([llama2(\"Apple is\")[-1]], clean_up_tokenization_spaces=False) # -> \"is\"\r\nfalcon.decode([falcon(\"Apple is\")[-1]], clean_up_tokenization_spaces=False) # -> \" is\"\r\nqwen.decode([qwen(\"Apple is\")[-1]], clean_up_tokenization_spaces=False) # -> \" is\"\r\n```\r\n\r\ncc @hnyls2002 \r\n\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-01-29T22:59:05+00:00",
    "closed_at": "2024-02-08T03:51:14+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/115/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/115"
  },
  {
    "number": 111,
    "title": "Qwen-7B-Chat UnicodeDecodeError Stop working",
    "body": "I have found that when running the Qwen-7B-Chat model for inference, sometimes the following errors occur, which can cause the entire inference service to malfunction in the future. As shown in the following figure:\r\n\r\n<img width=\"1298\" alt=\"image\" src=\"https://github.com/sgl-project/sglang/assets/4583537/53596022-c749-42d0-ad6b-919ad4bf3d5c\">\r\n\r\n<img width=\"1512\" alt=\"image\" src=\"https://github.com/sgl-project/sglang/assets/4583537/253015ab-3de3-40ca-a5b7-de9a980e4615\">\r\n\r\nStart script:\r\npython -m sglang.launch_server --model-path /root/autodl-tmp/Qwen-7B-Chat --port 8000 --load-format safetensors --mem-fraction-static 0.9 --tokenizer-mode auto --trust-remote-code\r\n\r\nTest script:\r\npython3 bench_throughput.py  --tokenizer /root/autodl-tmp/Qwen-7B-Chat --dataset /root/autodl-tmp/ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000  --port 8000 --backend srt --trust-remote-code\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-01-27T04:42:07+00:00",
    "closed_at": "2024-01-30T15:43:52+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/111/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/111"
  },
  {
    "number": 110,
    "title": "Issue with running example with Mixtral AWQ",
    "body": "Hey, \r\n\r\nNot sure if I'm doing something silly on my end, but I said I would submit an issue to get some help. Hope that's okay. 2x 3090 GPU. \r\n\r\nI am running with the following Dockerfile:\r\n\r\n```\r\nFROM pytorch/pytorch:2.1.1-cuda12.1-cudnn8-devel\r\n\r\nRUN apt update && apt dist-upgrade -y\r\n\r\nRUN pip install --upgrade pip\r\nRUN pip install \"sglang[all]\"\r\n\r\nWORKDIR /\r\n\r\nCOPY . . \r\n\r\nEXPOSE 30000\r\n\r\nCMD [\"/bin/bash\",\"-c\",\"python -m sglang.launch_server --model-path /models/TheBloke_Nous-Hermes-2-Mixtral-8x7B-DPO-AWQ/ --tokenizer-path /models/TheBloke_Nous-Hermes-2-Mixtral-8x7B-DPO-AWQ/ --port 30000 --host 192.168.1.55 --tp-size 2\"]\r\n```\r\n\r\nHere is the test example I have ran (also tried simple curl with same outcome):\r\n```\r\nfrom sglang import function, system, user, assistant, gen, set_default_backend, RuntimeEndpoint\r\n\r\n@function\r\ndef multi_turn_question(s, question_1, question_2):\r\n    s += system(\"You are a helpful assistant.\")\r\n    s += user(question_1)\r\n    s += assistant(gen(\"answer_1\", max_tokens=256))\r\n    s += user(question_2)\r\n    s += assistant(gen(\"answer_2\", max_tokens=256))\r\n\r\nset_default_backend(RuntimeEndpoint(\"http://192.168.1.55:30000\"))\r\n\r\nstate = multi_turn_question.run(\r\n    question_1=\"What is the capital of the United States?\",\r\n    question_2=\"List two local attractions.\",\r\n)\r\n\r\nfor m in state.messages():\r\n    print(m[\"role\"], \":\", m[\"content\"])\r\n\r\nprint(state[\"answer_1\"])\r\n```\r\n\r\n\r\nThe following is the full log, including error that hangs the process:\r\n\r\n```\r\n==========\r\n== CUDA ==\r\n==========\r\n\r\nCUDA Version 12.1.1\r\n\r\nContainer image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\r\n\r\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\nBy pulling and using the container, you accept the terms and conditions of this license:\r\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n\r\nA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\r\n\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nserver started on [0.0.0.0]:10005\r\nserver started on [0.0.0.0]:10004\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\naccepted ('127.0.0.1', 55998) with fd 5\r\nwelcome ('127.0.0.1', 55998)\r\naccepted ('127.0.0.1', 41050) with fd 5\r\nwelcome ('127.0.0.1', 41050)\r\nRank 0: load weight begin.\r\nRank 1: load weight begin.\r\nquant_config: AWQConfig(weight_bits=4, group_size=128, zero_point=True)\r\nquant_config: AWQConfig(weight_bits=4, group_size=128, zero_point=True)\r\nRank 1: load weight end.\r\nRank 0: load weight end.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nRank 1: max_total_num_token=135847, max_prefill_num_token=32768, context_len=32768, model_mode=[]\r\nRank 0: max_total_num_token=135847, max_prefill_num_token=32768, context_len=32768, model_mode=[]\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nINFO:     Started server process [1]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://192.168.1.55:30000 (Press CTRL+C to quit)\r\nINFO:     192.168.1.6:34408 - \"GET /get_model_info HTTP/1.1\" 200 OK\r\nnew fill batch. #seq: 1. #cached_token: 0. #new_token: 27. #remaining_req: 0. #running_req: 0\r\nProcess Process-1:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/router/manager.py\", line 79, in start_router_process\r\n    loop.run_until_complete(router.loop_for_forward())\r\n  File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/router/manager.py\", line 38, in loop_for_forward\r\n    out_pyobjs = await self.model_client.step(next_step_input)\r\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 516, in _func\r\n    await asyncio.gather(*[asyncio.to_thread(t.wait) for t in tasks])\r\n  File \"/opt/conda/lib/python3.10/asyncio/threads.py\", line 25, in to_thread\r\n    return await loop.run_in_executor(None, func_call)\r\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/rpyc/core/async_.py\", line 51, in wait\r\n    self._conn.serve(self._ttl)\r\n  File \"/opt/conda/lib/python3.10/site-packages/rpyc/core/protocol.py\", line 438, in serve\r\n    data = self._channel.poll(timeout) and self._channel.recv()\r\n  File \"/opt/conda/lib/python3.10/site-packages/rpyc/core/channel.py\", line 55, in recv\r\n    header = self.stream.read(self.FRAME_HEADER.size)\r\n  File \"/opt/conda/lib/python3.10/site-packages/rpyc/core/stream.py\", line 280, in read\r\n    raise EOFError(\"connection closed by peer\")\r\nEOFError: connection closed by peer\r\n```\r\n\r\nThanks for all of your help in advance :)",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-27T00:48:43+00:00",
    "closed_at": "2024-01-29T18:41:18+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/110/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/110"
  },
  {
    "number": 109,
    "title": "OOM after flush_cache when flashinfer is enabled",
    "body": "When benchmarking my serving workloads, I found the following pattern will constantly cause OOM error:\r\n\r\n1. Launch a container with SRT and flashinfer enabled.\r\n2. Benchmark with 800 requests.\r\n3. Call `http://0.0.0.0:25000/flush_cache` and confirm the cache is flushed in the SRT log.\r\n4. Benchmark with 800 requests again.\r\n\r\nThe above steps result in the following error in the middle of the second benchmarking:\r\n\r\n```\r\nCUDA Error: out of memory (2) /sglang/3rdparty/flashinfer/include/flashinfer/handler.cuh: line 100 at function cudaMallocAsync(&float_buffer_, sizeof(float) *\r\n tmp_size, stream_)\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/sglang/python/sglang/srt/managers/router/model_rpc.py\", line 165, in exposed_step\r\n    self.forward_step()\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/sglang/python/sglang/srt/managers/router/model_rpc.py\", line 192, in forward_step\r\n    self.forward_decode_batch(self.running_batch)\r\n  File \"/sglang/python/sglang/srt/managers/router/model_rpc.py\", line 428, in forward_decode_batch\r\n    logits = self.model_runner.forward(batch, ForwardMode.DECODE)\r\n  File \"/sglang/python/sglang/srt/managers/router/model_runner.py\", line 473, in forward\r\n    return self.forward_decode(**kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/sglang/python/sglang/srt/managers/router/model_runner.py\", line 395, in forward_decode\r\n    input_metadata = InputMetadata.create(\r\n  File \"/sglang/python/sglang/srt/managers/router/model_runner.py\", line 216, in create\r\n    ret.init_flashinfer_args(tp_size)\r\n  File \"/sglang/python/sglang/srt/managers/router/model_runner.py\", line 128, in init_flashinfer_args\r\n    self.decode_wrapper.begin_forward(\r\n  File \"/usr/local/lib/python3.10/dist-packages/flashinfer/ops/__init__.py\", line 523, in begin_forward\r\n    self._wrapper.begin_forward(\r\nRuntimeError: BatchDecodeWithPagedKVCache failed with error out of memory\r\n```\r\n\r\nNot sure whether it's caused by the memory leak when flushing cache. @hnyls2002 do you have an idea?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-27T00:35:40+00:00",
    "closed_at": "2024-02-16T20:20:00+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/109/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/109"
  },
  {
    "number": 107,
    "title": "Mistral model no longer loads following PR#101",
    "body": "The `get_model_cls_by_arch_name` introduced in [Dynamic model class loading PR](https://github.com/sgl-project/sglang/pull/101) removes the hard-coded mapping between `MistralForCausalLM` and `LlamaForCausalLM` causing issues trying to local host [Mistral-7b](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) model as of sglang version 0.1.9. I have tested that adding the following simple `models/mistral.py` file allows hosting the mistral-7b model.\r\n\r\n```python\r\nfrom sglang.srt.models.llama2 import LlamaForCausalLM\r\n\r\n\r\nclass MistralForCausalLM(LlamaForCausalLM):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n\r\n\r\nEntryClass = MistralForCausalLM\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-26T15:06:54+00:00",
    "closed_at": "2024-01-26T17:38:45+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/107/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/107"
  },
  {
    "number": 106,
    "title": "Implement prefix_cache",
    "body": "Thanks so much for the work on this repo so far.\r\n\r\nI think prefix caching could be very useful and I see that vLLM is also starting to support it for some architectures.\r\n\r\nIt looks like the [BaseBackend.prefix_cache](https://github.com/sgl-project/sglang/blob/81561f8e2d55d105aabbe0eab1b3b33f4fc04b0b/python/sglang/backend/base_backend.py#L19-L20) method still needs to be implemented:\r\n```python\r\n    def cache_prefix(self, prefix_str: str):\r\n        pass\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-26T14:01:54+00:00",
    "closed_at": "2024-01-30T01:12:36+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/106/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/106"
  },
  {
    "number": 105,
    "title": "kv cache pool leak detected when benchmark llama13B-awq using A40",
    "body": " UserWarning: Warning: available_size=35244, max_total_num_token=42308\r\nKV cache pool leak detected!",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-01-26T03:39:00+00:00",
    "closed_at": "2024-07-25T06:32:06+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/105/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/105"
  },
  {
    "number": 100,
    "title": "no response running python -m sglang.launch_server --model-path NousResearch/Llama-2-7b-chat-hf --port 30000",
    "body": "when I try to use `sglang` locally according to README.md:\r\n``` sh\r\npython -m sglang.launch_server --model-path NousResearch/Llama-2-7b-chat-hf --port 30000\r\n```\r\n(I use NousResearch/Llama-2-7b-chat-hf because my access of meta-llama is pending)\r\nhowever, I receive no response and no log print. when I run the python script:\r\n```python\r\nfrom sglang import function, system, user, assistant, gen, set_default_backend, RuntimeEndpoint\r\n\r\n@function\r\ndef multi_turn_question(s, question_1, question_2):\r\n    s += system(\"You are a helpful assistant.\")\r\n    s += user(question_1)\r\n    s += assistant(gen(\"answer_1\", max_tokens=256))\r\n    s += user(question_2)\r\n    s += assistant(gen(\"answer_2\", max_tokens=256))\r\n\r\nset_default_backend(RuntimeEndpoint(\"http://localhost:30000\"))\r\n\r\nstate = multi_turn_question.run(\r\n    question_1=\"What is the capital of the United States?\",\r\n    question_2=\"List two local attractions.\",\r\n)\r\n\r\nfor m in state.messages():\r\n    print(m[\"role\"], \":\", m[\"content\"])\r\n\r\nprint(state[\"answer_1\"])\r\n```\r\nI encountered the error as follows:\r\n``` sh\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/urllib/request.py\", line 1348, in do_open\r\n    h.request(req.get_method(), req.selector, req.data, headers,\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/http/client.py\", line 1283, in request\r\n    self._send_request(method, url, body, headers, encode_chunked)\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/http/client.py\", line 1329, in _send_request\r\n    self.endheaders(body, encode_chunked=encode_chunked)\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/http/client.py\", line 1278, in endheaders\r\n    self._send_output(message_body, encode_chunked=encode_chunked)\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/http/client.py\", line 1038, in _send_output\r\n    self.send(msg)\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/http/client.py\", line 976, in send\r\n    self.connect()\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/http/client.py\", line 942, in connect\r\n    self.sock = self._create_connection(\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/socket.py\", line 845, in create_connection\r\n    raise err\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/socket.py\", line 833, in create_connection\r\n    sock.connect(sa)\r\nConnectionRefusedError: [Errno 111] Connection refused\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/mnt/workspace/LongTail/src/sgl-project/sglang/try_sgl.py\", line 11, in <module>\r\n    set_default_backend(RuntimeEndpoint(\"http://localhost:30000\"))\r\n  File \"/mnt/workspace/LongTail/src/sgl-project/sglang/python/sglang/backend/runtime_endpoint.py\", line 21, in __init__\r\n    res = http_request(self.base_url + \"/get_model_info\")\r\n  File \"/mnt/workspace/LongTail/src/sgl-project/sglang/python/sglang/utils.py\", line 102, in http_request\r\n    resp = urllib.request.urlopen(req, data=data)\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/urllib/request.py\", line 216, in urlopen\r\n    return opener.open(url, data, timeout)\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/urllib/request.py\", line 519, in open\r\n    response = self._open(req, data)\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/urllib/request.py\", line 536, in _open\r\n    result = self._call_chain(self.handle_open, protocol, protocol +\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/urllib/request.py\", line 496, in _call_chain\r\n    result = func(*args)\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/urllib/request.py\", line 1377, in http_open\r\n    return self.do_open(http.client.HTTPConnection, req)\r\n  File \"/root/miniconda3/envs/sglang/lib/python3.10/urllib/request.py\", line 1351, in do_open\r\n    raise URLError(err)\r\nurllib.error.URLError: <urlopen error [Errno 111] Connection refused>\r\n```\r\n\r\n \\>_< I am really  appreciate if someone can help me to solve!!",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-25T15:04:48+00:00",
    "closed_at": "2024-01-30T14:30:56+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/100/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/100"
  },
  {
    "number": 99,
    "title": "Fail to load TheBloke/tulu-2-dpo-70B-AWQ on A800*2: TimeoutError: result expired",
    "body": "Thanks for your great work!\r\nI am trying to load the AWQ model of Tulu-2-dpo-70B, here is my command line input:\r\n```CUDA_VISIBLE_DEVICES=0,1 python -m sglang.launch_server --model-path TheBloke/tulu-2-dpo-70B-AWQ --tokenizer-path TheBloke/tulu-2-dpo-70B-AWQ --port 30000 --mem-fraction-static 0.5 --tp-size 2```\r\nAnd it took me over 20 min to load the checkpoint into GPU memory, and I finally get the error:\r\n```server started on [0.0.0.0]:10010\r\nserver started on [0.0.0.0]:10011\r\naccepted ('127.0.0.1', 51884) with fd 6\r\nwelcome ('127.0.0.1', 51884)\r\naccepted ('127.0.0.1', 40934) with fd 6\r\nwelcome ('127.0.0.1', 40934)\r\nRank 1: load weight begin.\r\nquant_config: AWQConfig(weight_bits=4, group_size=128, zero_point=True)\r\nRank 0: load weight begin.\r\nquant_config: AWQConfig(weight_bits=4, group_size=128, zero_point=True)\r\nRank 1: load weight end.\r\nrouter init state: Traceback (most recent call last):\r\n  File \"/home/share/likai/sglang/python/sglang/srt/managers/router/manager.py\", line 68, in start_router_process\r\n    model_client = ModelRpcClient(server_args, port_args)\r\n  File \"/home/share/likai/sglang/python/sglang/srt/managers/router/model_rpc.py\", line 521, in __init__\r\n    rets = [obtain(x) for x in executor.map(init_model, range(tp_size))]\r\n  File \"/home/share/likai/sglang/python/sglang/srt/managers/router/model_rpc.py\", line 521, in <listcomp>\r\n    rets = [obtain(x) for x in executor.map(init_model, range(tp_size))]\r\n  File \"/home/likai/.conda/envs/powerinfer/lib/python3.10/concurrent/futures/_base.py\", line 608, in result_iterator\r\n    yield fs.pop().result()\r\n  File \"/home/likai/.conda/envs/powerinfer/lib/python3.10/concurrent/futures/_base.py\", line 445, in result\r\n    return self.__get_result()\r\n  File \"/home/likai/.conda/envs/powerinfer/lib/python3.10/concurrent/futures/_base.py\", line 390, in __get_result\r\n    raise self._exception\r\n  File \"/home/likai/.conda/envs/powerinfer/lib/python3.10/concurrent/futures/thread.py\", line 52, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/home/share/likai/sglang/python/sglang/srt/managers/router/model_rpc.py\", line 519, in init_model\r\n    return self.model_servers[i].init_model(i, server_args, port_args)\r\n  File \"/home/likai/.conda/envs/powerinfer/lib/python3.10/site-packages/rpyc/core/netref.py\", line 240, in __call__\r\n    return syncreq(_self, consts.HANDLE_CALL, args, kwargs)\r\n  File \"/home/likai/.conda/envs/powerinfer/lib/python3.10/site-packages/rpyc/core/netref.py\", line 63, in syncreq\r\n    return conn.sync_request(handler, proxy, *args)\r\n  File \"/home/likai/.conda/envs/powerinfer/lib/python3.10/site-packages/rpyc/core/protocol.py\", line 718, in sync_request\r\n    return _async_res.value\r\n  File \"/home/likai/.conda/envs/powerinfer/lib/python3.10/site-packages/rpyc/core/async_.py\", line 106, in value\r\n    self.wait()\r\n  File \"/home/likai/.conda/envs/powerinfer/lib/python3.10/site-packages/rpyc/core/async_.py\", line 55, in wait\r\n    raise AsyncResultTimeout(\"result expired\")\r\nTimeoutError: result expired\r\ndetoken init state: init ok\r\n```\r\nCould you please help me check it? Thank you so much.",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-01-25T11:35:16+00:00",
    "closed_at": "2024-07-25T06:32:11+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/99/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/99"
  },
  {
    "number": 96,
    "title": "`TypeError: color must be int or single-element tuple` when processing a grayscale image with LLaVA",
    "body": "When processing a grayscale image with different width and height the following error will occur.\r\n\r\n```console\r\nException in TokenizerManager:\r\nTraceback (most recent call last):\r\n  File \"/home/gcpuser/sglang/python/sglang/srt/managers/tokenizer_manager.py\", line 61, in get_pixel_values\r\n    image = expand2square(\r\n  File \"/home/gcpuser/sglang/python/sglang/srt/mm_utils.py\", line 167, in expand2square\r\n    result = Image.new(pil_img.mode, (width, width), background_color)\r\n  File \"/opt/conda/envs/sglang_flashinfer/lib/python3.9/site-packages/PIL/Image.py\", line 2941, in new\r\n    return im._new(core.fill(mode, size, color))\r\nTypeError: color must be int or single-element tuple\r\n```\r\n\r\nThe error originates here and it fails because PIL won't the background consisting of 3 values to the new image with `L` mode.\r\n\r\nSo far I only encountered this for grayscale images, perhaps one way to solve it would be to convert these images to `RGB` before resizing them?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-24T22:53:40+00:00",
    "closed_at": "2024-01-25T00:24:05+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/96/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/96"
  },
  {
    "number": 94,
    "title": "how to use the finetuned mistral model for inference with sglang",
    "body": "how to use the finetuned mistral model for inference with sglang. \r\nPlease share the code for this",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-24T16:14:41+00:00",
    "closed_at": "2024-01-30T14:25:14+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/94/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/94"
  },
  {
    "number": 91,
    "title": "Support Yi-VL-6B/34B",
    "body": "The Yi-VL adopts llava but with silightly different in weights and inference. see [disscusion](https://huggingface.co/01-ai/Yi-VL-34B/discussions/3)\r\n\r\nhf repo:\r\nhttps://huggingface.co/01-ai/Yi-VL-6B\r\nhttps://huggingface.co/01-ai/Yi-VL-34B",
    "labels": [
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-01-24T03:49:46+00:00",
    "closed_at": "2024-02-01T21:38:25+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/91/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/91"
  },
  {
    "number": 89,
    "title": "Customizing Sampling Behavior",
    "body": "Hi, is there an interface to specify logits processors as in vLLM? \r\n\r\nIf possible, could you specify how we can customize the sampling behavior during generation?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-24T00:06:48+00:00",
    "closed_at": "2024-02-06T18:25:19+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/89/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/89"
  },
  {
    "number": 88,
    "title": "'Runtime' object has no attribute 'cache_prefix'",
    "body": "\r\n`\r\nruntime = sgl.Runtime(model_path=\"lmsys/vicuna-13b-v1.5\")\r\n`\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/root/test_sg.py\", line 42, in <module>\r\n    states = single_question.run_batch(\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/lang/ir.py\", line 178, in run_batch\r\n    return run_program_batch(\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/lang/interpreter.py\", line 79, in run_program_batch\r\n    pin_program(program, backend)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/lang/interpreter.py\", line 132, in pin_program\r\n    prefix_rid = backend.cache_prefix(prefix)\r\nAttributeError: 'Runtime' object has no attribute 'cache_prefix'\r\n```\r\n\r\nHow to fix this error?\r\n\r\n(also how can I point it to existing cached models by say transformers\r\ne.g. `~/.cache/huggingface/hub/models--lmsys--vicuna-13b-v1.5`)\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-23T16:59:24+00:00",
    "closed_at": "2024-01-24T12:05:19+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/88/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/88"
  },
  {
    "number": 86,
    "title": "OOM Error on A40 GPU [Jupyter notebook]",
    "body": "I was trying the following code sample (adapted from the discussion in https://github.com/sgl-project/sglang/issues/81) - \r\n\r\n```\r\nimport sglang as sgl\r\nfrom sglang import function, gen, set_default_backend, Runtime\r\n\r\n@sgl.function\r\ndef tool_use(s, question):\r\n    s += \"To answer this question: \" + question + \", \"\r\n    s += \"I need to use a \" + sgl.gen(\"tool\", choices=[\"calculator\", \"web browser\"]) + \". \"\r\n    if s[\"tool\"] == \"calculator\":\r\n        s += \"The math expression is\" + sgl.gen(\"expression\")\r\n    elif s[\"tool\"] == \"web browser\":\r\n        s += \"The website url is\" + sgl.gen(\"url\")\r\n\r\nruntime = Runtime(model_path='Model_Saves/teknium--OpenHermes-2.5-Mistral-7B')\r\nset_default_backend(runtime)\r\n\r\ndriver_tool_use()\r\n```\r\n   \r\nI firstly got the same error as described here: https://github.com/sgl-project/sglang/issues/41#issuecomment-1899347676\r\nI then followed Solution 2 from this [comment](https://github.com/sgl-project/sglang/issues/41#issuecomment-1899354400) and the error disappeared but I now get an OOM error even though I have `46068 MiB` of space on the GPU which is more than what a 7B model needs. On checking with `nvidia-smi` I see `41158MiB` in use. I'm running this in a Jupyter notebook.\r\n\r\nThe error - \r\n\r\n```\r\nrouter init state: Traceback (most recent call last):\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/managers/router/manager.py\", line 68, in start_router_process\r\n    model_client = ModelRpcClient(server_args, port_args)\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 480, in __init__\r\n    self.model_server.exposed_init_model(0, server_args, port_args)\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 53, in exposed_init_model\r\n    self.model_runner = ModelRunner(\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/managers/router/model_runner.py\", line 233, in __init__\r\n    self.load_model()\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/managers/router/model_runner.py\", line 278, in load_model\r\n    model = model_class(\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/models/llama2.py\", line 258, in __init__\r\n    self.model = LlamaModel(config, linear_method)\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/models/llama2.py\", line 218, in __init__\r\n    [\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/models/llama2.py\", line 219, in <listcomp>\r\n    LlamaDecoderLayer(config, i, linear_method)\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/models/llama2.py\", line 167, in __init__\r\n    self.mlp = LlamaMLP(\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/sglang/srt/models/llama2.py\", line 49, in __init__\r\n    self.down_proj = RowParallelLinear(\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 495, in __init__\r\n    self.linear_weights = self.linear_method.create_weights(\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 55, in create_weights\r\n    weight = Parameter(torch.empty(output_size_per_partition,\r\n  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/peft_mem/lib/python3.10/site-packages/torch/utils/_device.py\", line 77, in __torch_function__\r\n    return func(*args, **kwargs)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 44.39 GiB of which 77.62 MiB is free. Process 535546 has 40.19 GiB memory in use. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.80 GiB is allocated by PyTorch, and 15.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\ndetoken init state: init ok\r\n\r\n```",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-01-23T12:03:01+00:00",
    "closed_at": "2024-07-25T06:32:01+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/86/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/86"
  },
  {
    "number": 85,
    "title": "Can the stop tokens be retained? ",
    "body": "I set the stop token as `.`, but the output text will not include the stop token, so can the stop tokens be retained in the output?\r\n\r\nrequest:\r\n```bash\r\ncurl http://localhost:8081/generate \\\r\n  -H \"Content-Type: application/json\" \\\r\n  -d '{\r\n    \"text\": \"Once upon a time,\",\r\n    \"sampling_params\": {\r\n      \"max_new_tokens\": 1024,\r\n      \"temperature\": 0,\r\n      \"stop\": \".\",\r\n    }\r\n  }'\r\n```\r\n\r\nresponse:\r\n```bash\r\n{\"text\":\" and in a land far, far away, there was a young girl named Lily\",\"meta_info\":{\"prompt_tokens\":6,\"completion_tokens\":18,\"id\":\"fe8e78ca46ab4f4bb69842202201288c\"}}\r\n```",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-01-23T06:39:30+00:00",
    "closed_at": "2024-07-25T06:32:03+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/85/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/85"
  },
  {
    "number": 81,
    "title": "Tutorial for Batch Decoding and Obtaining Log Probs",
    "body": "Hi\r\nThanks for the great library\r\nI have a usecase which I think will benefit a lot from Radix Attention. I need to obtain log probs for around a 100K sequences which can be binned into groups of 100 having a similar prefix like 'Wikipedia originated in' and having 100 different suffixes. I do not need to generate anything and I only need the log probs for the input. Is there a tutorial for such a usecase?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-23T05:13:56+00:00",
    "closed_at": "2024-01-30T14:39:59+00:00",
    "comments": 25,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/81/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/81"
  },
  {
    "number": 79,
    "title": "['LLaVA'] Error when trying to load a fine-tuned LLaVA model",
    "body": "Hi, I encounter the following error when trying to load a fine-tuned LLaVA model:\r\n\r\n```console\r\n~$ python3 -m sglang.launch_server --model-path org/llava_1.5_13b_finetune --tokenizer-path llava-hf/llava-1.5-13b-hf --port 30000\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nProcess Process-1:\r\nrouter init state: Traceback (most recent call last):\r\n  File \"/opt/conda/envs/llava_sglang/lib/python3.10/site-packages/sglang/srt/managers/router/manager.py\", line 68, in start_router_process\r\n    model_client = ModelRpcClient(server_args, port_args)\r\n  File \"/opt/conda/envs/llava_sglang/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 448, in __init__\r\n    self.model_server.exposed_init_model(0, server_args, port_args)\r\n  File \"/opt/conda/envs/llava_sglang/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 54, in exposed_init_model\r\n    self.model_runner = ModelRunner(\r\n  File \"/opt/conda/envs/llava_sglang/lib/python3.10/site-packages/sglang/srt/managers/router/model_runner.py\", line 229, in __init__\r\n    self.load_model()\r\n  File \"/opt/conda/envs/llava_sglang/lib/python3.10/site-packages/sglang/srt/managers/router/model_runner.py\", line 275, in load_model\r\n    model.load_weights(\r\n  File \"/opt/conda/envs/llava_sglang/lib/python3.10/site-packages/sglang/srt/models/llava.py\", line 177, in load_weights\r\n    self.language_model.load_weights(\r\n  File \"/opt/conda/envs/llava_sglang/lib/python3.10/site-packages/sglang/srt/models/llama2.py\", line 306, in load_weights\r\n    param = params_dict[name]\r\nKeyError: 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.qkv_proj.weight'\r\n```\r\n\r\nSeems like an easy solution would be to ignore the `vision_tower` in the `params_dict`?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-22T22:24:11+00:00",
    "closed_at": "2024-01-23T02:15:56+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/79/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/79"
  },
  {
    "number": 78,
    "title": "Sketch guided constrained deciding for black box LLMs",
    "body": "https://twitter.com/SaiboGeng/status/1749490603111387643?t=MRfYngCpJRB7zfZCpW43MA&s=19\r\n\r\nAny thoughts about adding that?\r\n\r\n",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-01-22T19:07:11+00:00",
    "closed_at": "2024-07-25T06:32:00+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/78/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/78"
  },
  {
    "number": 77,
    "title": "Not able to run AWQ Mixtral on 4xA10",
    "body": "Hi,\r\n\r\nIm trying to run the AWQ version of Mixtral on 4xA10s. However im getting this error. Ive also tried with `--mem-frac 0.7` and still got the same error\r\n\r\nModel I'm using : https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ\r\n\r\nCommand : `python -m sglang.launch_server --model-path /local_disk0/TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ/ --port 30000 --tp 4`\r\n\r\nCode : \r\n```\r\nfrom sglang import function, system, user, assistant, gen\r\nimport sglang as sgl\r\n\r\n@function\r\ndef multi_turn_question(s, question_1, question_2):\r\n    s += system(\"You are a helpful assistant.\")\r\n    s += user(question_1)\r\n    s += assistant(gen(\"answer_1\", max_tokens=256))\r\n    s += user(question_2)\r\n    s += assistant(gen(\"answer_2\", max_tokens=256))\r\n\r\nstate = multi_turn_question.run(\r\n    question_1=\"What is the capital of the United Kingdom?\",\r\n    question_2=\"List two local attractions.\",\r\n    temperature=0.7,\r\n    stream=True,\r\n)\r\n\r\nfor out in state.text_iter():\r\n    print(out, end=\"\", flush=True)\r\nprint()\r\n\r\n```\r\n\r\nError\r\n```\r\nnew fill batch. #seq: 1. #cached_token: 0. #new_token: 34. #remaining_req: 0. #running_req: 0\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 140, in exposed_step\r\n    self.forward_step()\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 155, in forward_step\r\n    self.forward_fill_batch(new_batch)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 349, in forward_fill_batch\r\n    next_token_ids, next_token_probs = batch.sample(logits)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/infer_batch.py\", line 375, in sample\r\n    sampled_index = torch.multinomial(probs_sort, num_samples=1)\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 140, in exposed_step\r\n    self.forward_step()\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 155, in forward_step\r\n    self.forward_fill_batch(new_batch)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 349, in forward_fill_batch\r\n    next_token_ids, next_token_probs = batch.sample(logits)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/infer_batch.py\", line 375, in sample\r\n    sampled_index = torch.multinomial(probs_sort, num_samples=1)\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 140, in exposed_step\r\n    self.forward_step()\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 155, in forward_step\r\n    self.forward_fill_batch(new_batch)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 349, in forward_fill_batch\r\n    next_token_ids, next_token_probs = batch.sample(logits)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/infer_batch.py\", line 375, in sample\r\n    sampled_index = torch.multinomial(probs_sort, num_samples=1)\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 140, in exposed_step\r\n    self.forward_step()\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 155, in forward_step\r\n    self.forward_fill_batch(new_batch)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 349, in forward_fill_batch\r\n    next_token_ids, next_token_probs = batch.sample(logits)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/infer_batch.py\", line 375, in sample\r\n    sampled_index = torch.multinomial(probs_sort, num_samples=1)\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py:179: UserWarning: Warning: available_size=391285, max_total_num_token=391319\r\nKV cache pool leak detected!\r\n  warnings.warn(\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py:179: UserWarning: Warning: available_size=391285, max_total_num_token=391319\r\nKV cache pool leak detected!\r\n  warnings.warn(\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py:179: UserWarning: Warning: available_size=391285, max_total_num_token=391319\r\nKV cache pool leak detected!\r\n  warnings.warn(\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-baadb11a-8dd2-4b96-a2e2-1e5e32b9d151/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py:179: UserWarning: Warning: available_size=391285, max_total_num_token=391319\r\nKV cache pool leak detected!\r\n  warnings.warn(\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-01-22T18:23:22+00:00",
    "closed_at": "2024-02-22T14:58:11+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/77/reactions",
      "total_count": 3,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/77"
  },
  {
    "number": 76,
    "title": "Support for offline batch mode with local models?",
    "body": "Hello, guys,\r\nAny plans to support offline batch inference mode with local models, without spinning up an additional server? similar to [what is implemented in vLLM](https://docs.vllm.ai/en/latest/getting_started/quickstart.html#offline-batched-inference). It would be way easier to use. \r\nThanks! ",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-22T13:02:31+00:00",
    "closed_at": "2024-01-23T13:23:51+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/76/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/76"
  },
  {
    "number": 74,
    "title": "Continues batch technical for different length prompt",
    "body": "Suppose model's max-model-length is 8096, and there is two requests, one prompt length is 8, another is 10. And how concat them, pad them to 8096 or pad them to 10 or truncate them to 8.\r\nI feel you will pad them to 16. But I don't seek the code. Can you tell me the location?\r\nThanks.",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-22T08:45:37+00:00",
    "closed_at": "2024-01-23T04:40:05+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/74/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/74"
  },
  {
    "number": 73,
    "title": "Accelerating Generation with Rollback using sglang?",
    "body": "Hi team! My generation scenario involves rolling back and I was wondering how I could speed this up using sglang. \r\n\r\nIn the first stage, I have an initial prompt, and I can obtain an output with sentences delimited by '\\n'. \r\nInput: \r\nquestion.\r\n\r\nOutput:\r\nsentence1 \\n sentence2 \\n sentence3 \\n\r\n\r\nIn the second stage, I would like to rollback and generate on these prompts:\r\n- question. sentence1 \\n\r\n- question. sentence1 \\n sentence2 \\n\r\n\r\nIs it possible to reuse the KV caches in the first stage using sglang? Thanks for your help!",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-22T04:40:25+00:00",
    "closed_at": "2024-01-22T04:56:40+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/73/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/73"
  },
  {
    "number": 70,
    "title": "Installing sglang[openai] does not have numpy requirement",
    "body": "I tried installing sglang[openai], ran the code in the readme:\r\n```python\r\nimport sglang as sgl\r\nimport snoop\r\n\r\n@sgl.function\r\ndef tool_use(s, question):\r\n    s += \"To answer this question: \" + question + \", \"\r\n    s += \"I need to use a \" + sgl.gen(\"tool\", choices=[\"calculator\", \"web browser\"]) + \". \"\r\n    if s[\"tool\"] == \"calculator\":\r\n        s += \"The math expression is\" + sgl.gen(\"expression\")\r\n    elif s[\"tool\"] == \"web browser\":\r\n        s += \"The website url is\" + sgl.gen(\"url\")\r\n\r\n\r\n@sgl.function\r\ndef tip_suggestion(s):\r\n    s += (\r\n        \"Here are two tips for staying healthy: \"\r\n        \"1. Balanced Diet. 2. Regular Exercise.\\n\\n\"\r\n    )\r\n\r\n    forks = s.fork(2)\r\n    for i, f in enumerate(forks):\r\n        f += f\"Now, expand tip {i+1} into a paragraph:\\n\"\r\n        f += sgl.gen(f\"detailed_tip\", max_tokens=256, stop=\"\\n\\n\")\r\n\r\n    s += \"Tip 1:\" + forks[0][\"detailed_tip\"] + \"\\n\"\r\n    s += \"Tip 2:\" + forks[1][\"detailed_tip\"] + \"\\n\"\r\n    s += \"In summary\" + sgl.gen(\"summary\")\r\n\r\n\r\n@sgl.function\r\ndef text_qa(s, question):\r\n    s += \"Q: \" + question + \"\\n\"\r\n    s += \"A:\" + sgl.gen(\"answer\", stop=\"\\n\")\r\n\r\n\r\ndef driver_tool_use():\r\n    state = tool_use.run(question=\"What is the capital of the United States?\")\r\n    print(state.text())\r\n    print(\"\\n\")\r\n\r\n\r\ndef driver_tip_suggestion():\r\n    state = tip_suggestion.run()\r\n    print(state.text())\r\n    print(\"\\n\")\r\n\r\n\r\ndef driver_batching():\r\n    states = text_qa.run_batch(\r\n        [\r\n            {\"question\": \"What is the capital of the United Kingdom?\"},\r\n            {\"question\": \"What is the capital of France?\"},\r\n            {\"question\": \"What is the capital of Japan?\"},\r\n        ],\r\n        progress_bar=True\r\n    )\r\n\r\n    for s in states:\r\n        print(s.text())\r\n    print(\"\\n\")\r\n\r\n\r\ndef driver_stream():\r\n    state = text_qa.run(\r\n        question=\"What is the capital of France?\",\r\n        temperature=0.1,\r\n        stream=True\r\n    )\r\n\r\n    for out in state.text_iter():\r\n        print(out, end=\"\", flush=True)\r\n    print(\"\\n\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    sgl.set_default_backend(sgl.OpenAI(\"gpt-3.5-turbo-instruct\"))\r\n\r\n    driver_tool_use()\r\n    driver_tip_suggestion()\r\n    driver_batching()\r\n    driver_stream()\r\n```\r\n\r\nFix: Add numpy + possibly more dependencies to the SGlang dependencies. ",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-21T21:59:38+00:00",
    "closed_at": "2024-01-21T22:56:42+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/70/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/70"
  },
  {
    "number": 69,
    "title": "How create a new branch?",
    "body": "I just fix some bugs and support a new model. I hope create a new branch.",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-21T12:27:58+00:00",
    "closed_at": "2024-01-21T13:02:51+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/69/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/69"
  },
  {
    "number": 66,
    "title": "Lora test",
    "body": "Hi,\r\n\r\nI saw that there is a lora dev branch. is it possible to test this already? Or is it still WIP. Asking as ive been asking for s-lora in vllm for like months now and s-lora being on your roadmap is very exciting.\r\n\r\nThanks!",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-21T08:50:22+00:00",
    "closed_at": "2024-01-21T10:43:42+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/66/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/66"
  },
  {
    "number": 65,
    "title": "Issue reproducing the example in Readme ",
    "body": "```\r\nfrom sglang import function, system, user, assistant, gen, set_default_backend, OpenAI\r\n\r\n@function\r\ndef multi_turn_question(s, question_1, question_2):\r\n    s += system(\"You are a helpful assistant.\")\r\n    s += user(question_1)\r\n    s += assistant(gen(\"answer_1\", max_tokens=256))\r\n    s += user(question_2)\r\n    s += assistant(gen(\"answer_2\", max_tokens=256))\r\n\r\nset_default_backend(OpenAI(\"gpt-3.5-turbo\"))\r\n\r\nstate = multi_turn_question.run(\r\n    question_1=\"What is the capital of the United States?\",\r\n    question_2=\"List two local attractions.\",\r\n)\r\n\r\nfor m in state.messages():\r\n    print(m[\"role\"], \":\", m[\"content\"])\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n[<ipython-input-3-ac9af75fa374>](https://localhost:8080/#) in <cell line: 11>()\r\n      9     s += assistant(gen(\"answer_2\", max_tokens=256))\r\n     10 \r\n---> 11 set_default_backend(OpenAI(\"gpt-3.5-turbo\"))\r\n     12 \r\n     13 state = multi_turn_question.run(\r\n\r\n[/usr/local/lib/python3.10/dist-packages/sglang/backend/openai.py](https://localhost:8080/#) in __init__(self, model_name, *args, **kwargs)\r\n     52     def __init__(self, model_name, *args, **kwargs):\r\n     53         super().__init__()\r\n---> 54         self.client = openai.OpenAI(*args, **kwargs)\r\n     55 \r\n     56         if isinstance(openai, Exception):\r\n\r\nAttributeError: 'ModuleNotFoundError' object has no attribute 'OpenAI'\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-21T08:22:42+00:00",
    "closed_at": "2024-01-21T10:12:24+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/65/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/65"
  },
  {
    "number": 62,
    "title": "Issue when using choices option.  assert temperature <= 1e-5      TypeError: '<=' not supported between instances of 'NoneType' and 'float'",
    "body": "I ran into the following error when running my code:\r\n\r\nCode\r\n```\r\nimport sglang as sgl\r\n\r\nsgl.set_default_backend(sgl.RuntimeEndpoint(\"http://localhost:30000\"))\r\n\r\n@sgl.function\r\ndef test_function(s, test_prompt):\r\n    s += \"This is a test, I will provide a prompt and answer to the best of your abilities. Once you are done, end with the word 'END'\" + \"\\n\"\r\n    s += \"Q: \" + test_prompt + \"\\n\"\r\n    s += \"A: \" + sgl.gen(\"car\", max_tokens=100, stop='END') + \"\\n\"\r\n    s += \"Q: Who would win batman or superman. You must choose ONE character in ONE sentence \\n\"  \r\n    # s += \"A: \" + sgl.gen(\"hero\", max_tokens=16, stop='END',)\r\n    s += \"A: \" + sgl.gen(\"hero\", choices=[\"batman\", \"superman\"],)\r\n\r\n\r\noutput = test_function.run(test_prompt = \"What is the fastest car?\")\r\n\r\nprint(output[\"car\"])\r\nprint(output[\"hero\"])\r\n\r\nprint(output)\r\n```\r\n\r\nError\r\n```\r\nException in thread Thread-7 (_thread_worker_func):\r\nTraceback (most recent call last):\r\n  File \"/home/sr/anaconda3/envs/sglangONLY/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/sr/anaconda3/envs/sglangONLY/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 761, in run_closure\r\n    _threading_Thread_run(self)\r\n  File \"/home/sr/anaconda3/envs/sglangONLY/lib/python3.11/threading.py\", line 982, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/sr/anaconda3/envs/sglangONLY/lib/python3.11/site-packages/sglang/lang/interpreter.py\", line 278, in _thread_worker_func\r\n    self._execute(expr)\r\n  File \"/home/sr/anaconda3/envs/sglangONLY/lib/python3.11/site-packages/sglang/lang/interpreter.py\", line 302, in _execute\r\n    self._execute(x)\r\n  File \"/home/sr/anaconda3/envs/sglangONLY/lib/python3.11/site-packages/sglang/lang/interpreter.py\", line 299, in _execute\r\n    self._execute_select(other)\r\n  File \"/home/sr/anaconda3/envs/sglangONLY/lib/python3.11/site-packages/sglang/lang/interpreter.py\", line 376, in _execute_select\r\n    decision, scores = self.backend.select(self, expr.choices, expr.temperature)\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/sr/anaconda3/envs/sglangONLY/lib/python3.11/site-packages/sglang/backend/runtime_endpoint.py\", line 137, in select\r\n    assert temperature <= 1e-5\r\n           ^^^^^^^^^^^^^^^^^^^\r\nTypeError: '<=' not supported between instances of 'NoneType' and 'float'\r\n As of now, the Bugatti Veyron Super Sport holds the record for the fastest car in the world, reaching a top speed of 268 mph (431 km/h). However, please note that this record can change as new cars with even greater speeds are developed.\r\n```\r\n\r\n\r\nThis error also presents itself when I try to run the tool use (calculator + web) example from the documentation",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-21T01:22:33+00:00",
    "closed_at": "2024-01-21T07:50:47+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/62/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/62"
  },
  {
    "number": 61,
    "title": "\"WARNING:  Invalid HTTP request received\" and latency SGLANG vs VLLM",
    "body": "Hi team,\r\nI am using `sglang` with a local finetuned model (`basemodel_id = cognitivecomputations/dolphin-2.2.1-mistral-7b`). And running inference in a for loop.\r\nGPU: 4090\r\nbatch_sz=1\r\ntokens_in ~ 2000\r\ntokens_out ~200\r\n\r\n```\r\nruntime = load_model(model_id)\r\nfor p in tqdm(prompts):\r\n   resp = inference_sglang(p)\r\n\r\nruntime.shutdown()\r\n```\r\nWhen the model is loaded I am getting:\r\n`WARNING:  Invalid HTTP request received.`\r\nwhich repeats itself until the code reaches the line `runtime.shutdown()`\r\n\r\n1. Why am I getting this warning? \r\n2. does it affect inference time?\r\nI ran the same prompts with `vllm` and inference times are very similar for `sglang` and `vllm`.\r\nMy prompts are single instruction (no multi-shot prompting as in your code examples):\r\n```<s><[INST]{my_instruction}[/INST]```\r\nis that a case, where sglang should show better performance than `vllm`?\r\n\r\nThank you",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-01-20T18:50:25+00:00",
    "closed_at": "2024-01-30T15:26:55+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/61/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/61"
  },
  {
    "number": 59,
    "title": "sglang.launch_server raise \"POST /v1/chat/completions HTTP/1.1\" 404 Not Found",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-20T07:15:47+00:00",
    "closed_at": "2024-01-21T10:15:27+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/59/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/59"
  },
  {
    "number": 56,
    "title": "[Feature Request] Enable working with Azure-OpenAI API (openai.AzureOpenAI())",
    "body": "Sglang looks great to me, but at my work, we use the Azure-OpenAI API. I don't see how to access this with sglang.\r\n\r\nIt would need two inputs in addition to the API-key, because at minimum I need to create the client like this:\r\n\r\n```python\r\nclient = openai.AzureOpenAI(\r\n    api_key=\"<your-api-key>\",\r\n    base_url=\"https://<your-project-name>.openai.azure.com/openai\",\r\n    api_version=\"<your-api-version>\",  # for example \"2023-05-15\"\r\n)\r\n```\r\n\r\nAlso, for some reason the models are called \"gpt-35-turbo\" instead of \"gpt-3.5-turbo\" (missing dot); and I believe that you can call your models whatever you want. This should be supported, too.\r\n\r\nIf this already works somehow, I would appreciate an explicit mention in the `README.md`. ",
    "labels": [
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-01-19T14:44:21+00:00",
    "closed_at": "2024-02-12T09:07:47+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/56/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/56"
  },
  {
    "number": 55,
    "title": "`run_batch()` RuntimeError: Trying to create tensor with negative dimension",
    "body": "Hello, team!\r\n\r\nThanks for the excellent work.\r\nWhen working batch inference, sometimes encountering server-side error that completely interrupts the process:\r\n```\r\nnew fill batch. #seq: 7. #cached_token: 1541. #new_token: 4. #remaining_req: 0. #running_req: 0\r\nException in ModelRpcClient:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/env/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 130, in exposed_step\r\n    self.forward_step()\r\n  File \"/opt/conda/envs/env/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/env/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 145, in forward_step\r\n    self.forward_fill_batch(new_batch)\r\n  File \"/opt/conda/envs/env/lib/python3.11/site-packages/sglang/srt/managers/router/model_rpc.py\", line 328, in forward_fill_batch\r\n    logits, normalized_logprobs = self.model_runner.forward(\r\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/env/lib/python3.11/site-packages/sglang/srt/managers/router/model_runner.py\", line 453, in forward\r\n    return self.forward_extend(**kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/env/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/env/lib/python3.11/site-packages/sglang/srt/managers/router/model_runner.py\", line 359, in forward_extend\r\n    return self.model.forward(input_ids, input_metadata.positions, input_metadata)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/env/lib/python3.11/site-packages/sglang/srt/models/llama2.py\", line 270, in forward\r\n    return self.logits_processor(\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/env/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/env/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/env/lib/python3.11/site-packages/sglang/srt/layers/logits_processor.py\", line 55, in forward\r\n    normalized_logprobs = compute_normalized_logprobs(\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/env/lib/python3.11/site-packages/sglang/srt/layers/logits_processor.py\", line 67, in compute_normalized_logprobs\r\n    logprobs = torch.zeros(\r\n               ^^^^^^^^^^^^\r\nRuntimeError: Trying to create tensor with negative dimension -3: [-3]\r\n```\r\n\r\nIt never happened to me with small batch sizes (1-10), but constantly face it with bigger ones. \r\n\r\nThe code to run batch inference fwiw:\r\n\r\n```python\r\n@sgl.function\r\ndef answer(s, question):\r\n    s += question + '\\n'\r\n    s += sgl.gen(\"answer\", choices=['Y', 'N'], temperature=0)\r\n\r\n\r\ndef driver_batching(questions):\r\n    states = answer.run_batch(\r\n        [{'question': q} for q in questions]\r\n    )\r\n    return [state['answer'] for state in states]\r\n\r\nsgl.set_default_backend(sgl.RuntimeEndpoint(MY_URL))\r\n\r\nfrom tqdm import tqdm\r\noutput = []\r\nbatch_size = 50\r\nfor i in tqdm(range(0, len(prompts), batch_size)):\r\n  output.extend(driver_batching(prompts[i:i+batch_size]))\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-01-19T11:22:38+00:00",
    "closed_at": "2024-01-21T10:16:10+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/55/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/55"
  },
  {
    "number": 54,
    "title": "Meaningless generated output with V100",
    "body": "Greate work. \r\nBut When I run examples/quick_start/srt_example_complete.py with RuntimeEndpoint(\"http://localhost:30000\") with Server and V100 32GB\r\n```\r\npython -m sglang.launch_server --model-path ~/model/Llama-2-7b-chat-hf/ --port 30000\r\n```\r\n\r\nGot result:\r\n```\r\nsystem : You are a helpful assistant.\r\n{'role': 'system', 'content': 'You are a helpful assistant.'}\r\nuser : What is the capital of the United States?\r\n{'role': 'user', 'content': 'What is the capital of the United States?'}\r\nassistant : It Swiss made S that it has? dot system.You don't even NEED a computer just to use it. You can use it with or without alexa or Google to Wall Street stock screym to place home alarm system<\u2013 butwith or wit out the internetNo diversity, Way cool\r\n\r\nWe will pay for your answer leading to ROI (Return On Investment) generation.\r\n{'role': 'assistant', 'content': \"It Swiss made S that it has? dot system.You don't even NEED a computer just to use it. You can use it with or without alexa or Google to Wall Street stock screym to place home alarm system<\u2013 butwith or wit out the internetNo diversity, Way cool\\n\\nWe will pay for your answer leading to ROI (Return On Investment) generation.\"}\r\nuser : List two local attractions.\r\n{'role': 'user', 'content': 'List two local attractions.'}\r\nassistant : Related images :Then How to install a security camera // Install a WiFi security camera in 10 minutes.  [INST without internet connection]for beginners [/ How to install a security camera without internet connection] .\r\n\r\nIn this guide, we will show you how to set up a security camera without internet connection. This is a great option for those who don't have access to the internet or prefer not to use it.\r\n\r\nStep 1: Choose a security camera\r\nChoose a security camera that doesn't require internet connection. There are many options available in the market, so look for one that fits your needs and budget.\r\n\r\nStep 2: Connect the camera to the power source\r\nConnect the camera to a power source, such as a wall outlet or a battery pack. Make sure the camera is securely attached to the wall or other surface.\r\n\r\nStep 3: Connect the camera to a recording device\r\nConnect the camera to a recording device, such as a computer or a dedicated security recorder. This will allow you to view and record footage from the camera.\r\n\r\nStep 4: Set up the camera settings\r\nSet up the camera settings, such as resolution, frame rate,\r\n{'role': 'assistant', 'content': \"Related images :Then How to install a security camera // Install a WiFi security camera in 10 minutes.  [INST without internet connection]for beginners [/ How to install a security camera without internet connection] .\\n\\nIn this guide, we will show you how to set up a security camera without internet connection. This is a great option for those who don't have access to the internet or prefer not to use it.\\n\\nStep 1: Choose a security camera\\nChoose a security camera that doesn't require internet connection. There are many options available in the market, so look for one that fits your needs and budget.\\n\\nStep 2: Connect the camera to the power source\\nConnect the camera to a power source, such as a wall outlet or a battery pack. Make sure the camera is securely attached to the wall or other surface.\\n\\nStep 3: Connect the camera to a recording device\\nConnect the camera to a recording device, such as a computer or a dedicated security recorder. This will allow you to view and record footage from the camera.\\n\\nStep 4: Set up the camera settings\\nSet up the camera settings, such as resolution, frame rate,\"}\r\n```\r\n\r\nExamples in srt_example_xx all are meaningless",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-01-19T09:32:33+00:00",
    "closed_at": "2024-01-22T00:53:39+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/54/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/54"
  },
  {
    "number": 53,
    "title": "Can SGL generate list of json?",
    "body": "I want to generate the following format, that is, list of jsons:\r\n[\r\n{\"name\": \"Alice\", \"age\": 1},\r\n{\"name\": \"Bob\", \"age\": 2},\r\n]\r\nThe number of the objects in the list is random depending on the output of LLM. So can SGL support such format?",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-01-19T08:04:28+00:00",
    "closed_at": "2024-07-25T06:31:59+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/53/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/53"
  },
  {
    "number": 51,
    "title": "Mixtral OutOfMemoryError with 2 GPUs",
    "body": "I'm trying to run Mixtral (Mixtral Hermes) with two 48GB GPUs but it seems that sglang server is not using my second GPU.\r\n\r\n`CUDA_VISIBLE_DEVICES=\"0,1\" python -m sglang.launch_server --model-path /workspace/model --port 30000`\r\n\r\nerrors out with\r\n\r\n```\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nProcess Process-1:\r\nTraceback (most recent call last):\r\nrouter init state: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/router/manager.py\", line 68, in start_router_process\r\n    model_client = ModelRpcClient(server_args, port_args)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/router/model_rpc.py\", line 448, in __init__\r\n    self.model_server.exposed_init_model(0, server_args, port_args)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/router/model_rpc.py\", line 54, in exposed_init_model\r\n    self.model_runner = ModelRunner(\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/router/model_runner.py\", line 229, in __init__\r\n    self.load_model()\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/managers/router/model_runner.py\", line 272, in load_model\r\n    model = model_class(\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/models/mixtral.py\", line 322, in __init__\r\n    self.model = MixtralModel(config, linear_method)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/models/mixtral.py\", line 285, in __init__\r\n    [\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/models/mixtral.py\", line 286, in <listcomp>\r\n    MixtralDecoderLayer(config, i, linear_method=linear_method)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/models/mixtral.py\", line 238, in __init__\r\n    self.block_sparse_moe = MixtralMoE(config=config, linear_method=linear_method)\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/models/mixtral.py\", line 99, in __init__\r\n    [\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/models/mixtral.py\", line 100, in <listcomp>\r\n    MixtralMLP(\r\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/models/mixtral.py\", line 55, in __init__\r\n    self.w2 = ReplicatedLinear(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py\", line 108, in __init__\r\n    self.linear_weights = self.linear_method.create_weights(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py\", line 55, in create_weights\r\n    weight = Parameter(torch.empty(output_size_per_partition,\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\", line 77, in __torch_function__\r\n    return func(*args, **kwargs)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 44.35 GiB of which 9.38 MiB is free. Process 2776126 has 44.33 GiB memory in use. Of the allocated memory 44.02 GiB is allocated by PyTorch, and 14.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\ndetoken init state: init ok\r\n```\r\n\r\n\r\nI had the same error on Modal with 2 x 80GB GPUs.\r\n\r\nThanks for the support",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-19T02:57:10+00:00",
    "closed_at": "2024-01-19T03:47:01+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/51/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/51"
  },
  {
    "number": 44,
    "title": "OpenAI speculative execution",
    "body": "The current frontend using OpenAI will invoke multiple calls for the example below:\r\n```\r\n@sgl.function\r\ndef example(s):\r\n  s += \"Construct a character.\"\r\n  s += \"Name: \" + gen(\"name\") + \" Birthday: \" + gen(\"birthday\") + \" Job: \" + gen(\"job\")\r\n```\r\nWe can optimize this to send less number of calls to save money:\r\n1. Gen longer in the first gen call, and skip the later if the first gen did the right thing.\r\n2. Allow using OpenAI's n=10 keyword argument to sample multiple completions when forked. We can also provide the interface `example.run(n=10)`.",
    "labels": [
      "enhancement",
      "high priority"
    ],
    "state": "closed",
    "created_at": "2024-01-18T18:09:31+00:00",
    "closed_at": "2024-01-25T10:10:02+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/44/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/sgl-project/sglang/issues/44"
  },
  {
    "number": 43,
    "title": "Outlines integration",
    "body": "This package looks awesome!  I was wondering why you decided to copy Outlines' code instead of importing the FSMs directly from outlines? There are several improvements on the performance of guided generation in the pipeline and you will be missing out on those. By importing you get better as we get better :)",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-18T17:40:02+00:00",
    "closed_at": "2024-02-09T06:07:59+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/43/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/43"
  },
  {
    "number": 41,
    "title": "LlaVa Usage with server option ValueError: ... not in list",
    "body": "Hello, I tried to utilize `sglang` backend with LlaVa model utilizing the command that's provided in the README\r\n\r\nI created a new environment and installed `sglang` with `pip install \"sglang[all]\"`.\r\n\r\n```bash\r\npython3 -m sglang.launch_server --model-path liuhaotian/llava-v1.5-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --port 30000\r\n```\r\n\r\n```python\r\nimport requests\r\nimport json\r\n# includes an image of a cat\r\npath = \"images/cat_2.jpeg\"\r\ntext = \"what is this?\"\r\n# checked https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/managers/io_struct.py#L9 for input\r\ndata = {\"text\": text,\r\n        \"image_data\": path}\r\n\r\nheaders = {'Content-Type': 'application/json'}\r\n\r\nresponse = requests.post('http://localhost:30000/generate', json=data, headers=headers)\r\n\r\nprint(response.json())\r\n```\r\n\r\nit gets stuck in runtime(jupyter, does not raise the error) but in server I see the error\r\n\r\n`ValueError: 32000 is not in list`\r\n\r\nMay I ask if there is something I'm doing wrong?\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-18T16:38:30+00:00",
    "closed_at": "2024-01-18T23:43:01+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/41/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/41"
  },
  {
    "number": 40,
    "title": "Custom chat template",
    "body": "This would be useful when using a model like mistral-instruct, or any model that doesn't have a standardised template like chatml. Or for example using a finetuned model that uses a custom chat/instruct template.",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-18T15:37:27+00:00",
    "closed_at": "2024-01-18T22:02:09+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/40/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/40"
  },
  {
    "number": 39,
    "title": "LLM integration with normal programming patterns or, a high level sglang interface",
    "body": "I posted a similar issue in outlines, but here goes:  we're building something complex and I think it would be helpful to have a marvin-like library that supports normal programming patterns with LLM's but also gives control over generation. This  would provide high level pythonic abstractions like typed functions dynamically compiling grammars for return pydantic structs that would also allow you to drop down to customize generation either within or around these functions. This could    be like high level mypy typed boundaries around sglang programs.\r\n\r\n[Marvin](https://github.com/PrefectHQ/marvin) and [funcchain](https://github.com/shroominic/funcchain) do the high level (sort of), but you give up control. Marvin relies on json and/or function calling and is constrained to OAI models, funcchain uses dynamically compiled  Lllamacpp grammar   as well. \r\n\r\nAnalogy would be Pytorch:triton::funcchain/equivalent:sglang\r\n\r\nAside from the funcchain-like feature, for my use case I'd love to see:\r\n\r\n1. Custom unpacking of pydantic structs: Looping/ programmatically accessing fields into prompts\r\n2. Customizing generation of pydantic output structs\r\n3. Mixing and matching regular python types and pydantic inputs and outputs\r\n4. Stretch goal: Some sort of single dispatch (class based) or multiple dispatch polymorphism (https://github.com/beartype/plum)\r\n5. Our baseline MVP will be using OpenAI models initially. For this to be computationally feasible, I think we'd need function calling, which seems to be planned?\r\n\r\nAnyway, is this something that would align with your vision, or better to have a high level interface library with multiple backends? \r\n\r\nDSPY does this in some sense, but it's constrained to a pytorch like programming model, where this is more like \"differentiable swift\" or the \"Julia just write your code and backprop through it\" vision.\r\n\r\nOne thing that funcchain wants to do is have an \"autotune\" model where these functions are kicked to dpsy for compilation. I can see sometimes I'd like more control and sometimes I'd be happy to have dspy do some of the work for me. \r\n\r\n\r\n",
    "labels": [
      "enhancement",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-01-18T14:14:29+00:00",
    "closed_at": "2024-07-25T06:31:58+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/39/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/39"
  },
  {
    "number": 38,
    "title": "How to use inside notebook?",
    "body": "Im trying to use this on databricks inside the notebook that's running on top of a 8xA10 single node cluster, I'm initialising like:\r\n\r\n```\r\nfrom sglang import function, system, user, assistant, gen, set_default_backend, Runtime\r\nruntime = Runtime(\"/local_disk0/mistralai/Mixtral-8x7B-Instruct-v0.1\")\r\nset_default_backend(runtime)\r\n```\r\n\r\nHowever I get this issue\r\n```\r\nrouter init state: Traceback (most recent call last):\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-51dd0ee1-a396-4939-81a6-75e3afe59af5/lib/python3.10/site-packages/sglang/srt/managers/router/manager.py\", line 68, in start_router_process\r\n    model_client = ModelRpcClient(server_args, port_args)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-51dd0ee1-a396-4939-81a6-75e3afe59af5/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 448, in __init__\r\n    self.model_server.exposed_init_model(0, server_args, port_args)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-51dd0ee1-a396-4939-81a6-75e3afe59af5/lib/python3.10/site-packages/sglang/srt/managers/router/model_rpc.py\", line 54, in exposed_init_model\r\n    self.model_runner = ModelRunner(\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-51dd0ee1-a396-4939-81a6-75e3afe59af5/lib/python3.10/site-packages/sglang/srt/managers/router/model_runner.py\", line 213, in __init__\r\n    torch.cuda.set_device(self.tp_rank)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-51dd0ee1-a396-4939-81a6-75e3afe59af5/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 404, in set_device\r\n    torch._C._cuda_setDevice(device)\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-51dd0ee1-a396-4939-81a6-75e3afe59af5/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 284, in _lazy_init\r\n    raise RuntimeError(\r\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-18T14:11:32+00:00",
    "closed_at": "2024-01-19T18:38:29+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/38/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/38"
  },
  {
    "number": 35,
    "title": "Triton support",
    "body": "Hello, curious if we can already use sglang as a backend for NVIDIA's Triton Server.\r\n\r\nAmazing work with the library btw, love it!",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-18T06:31:20+00:00",
    "closed_at": "2024-11-01T05:54:44+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/35/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/35"
  },
  {
    "number": 29,
    "title": "Async support",
    "body": null,
    "labels": [
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-01-17T23:44:02+00:00",
    "closed_at": "2024-01-21T23:17:31+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/29/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/29"
  },
  {
    "number": 28,
    "title": "[Feature Request] Optimized quantised kernels",
    "body": "https://github.com/IST-DASLab/marlin",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-17T23:14:04+00:00",
    "closed_at": "2024-03-13T02:10:13+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/28/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/28"
  },
  {
    "number": 27,
    "title": "Max Length.",
    "body": "Is there anyway to truncate text based on tokens? I really like that as a user I don't need to think about tokens. But to save memory I would like something like \n`s += left_trunc(inp, 500)` that keeps it reasonably sized.",
    "labels": [
      "good first issue",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-01-17T21:26:39+00:00",
    "closed_at": "2024-07-25T06:31:56+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/27/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/27"
  },
  {
    "number": 26,
    "title": "OpenAI Chat Completion Endpoint",
    "body": "First of all, thank you for such a great framework and study! Do you plan to support `chat/completions` endpoint as well for the models utilizing their `chat_template` for completion during backend serving?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-17T21:25:15+00:00",
    "closed_at": "2024-01-19T07:43:10+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/26/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/26"
  },
  {
    "number": 25,
    "title": "Batching semantics?",
    "body": "I'm curious about how careful I need to be with batching. If I batch together 50 calls of which which there are 5 unique prefixes (a,b,c,d,e), will it know to group those together. Or should I be careful about making that 5 different batches?",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-17T21:24:06+00:00",
    "closed_at": "2024-01-17T23:19:30+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/25/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/25"
  },
  {
    "number": 24,
    "title": "Offline Generation",
    "body": "Hi, is it possible to do offline Generation similar to the vllm batch Inference where the model is not served?\n\nLike\n```\nLlm = sglang(\"path/to/llm\")\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-17T21:10:27+00:00",
    "closed_at": "2024-01-17T21:29:02+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/24/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/24"
  },
  {
    "number": 23,
    "title": "Metal support?",
    "body": "Hey, when is planned the support for Metal backend? ",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-01-17T20:16:06+00:00",
    "closed_at": "2024-07-25T06:32:59+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/23/reactions",
      "total_count": 8,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 5
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/23"
  },
  {
    "number": 22,
    "title": "[Quantization Support Request]: Exllamav2",
    "body": "Exllamav2 is an excellent quantization method that would allow to use big models in consumer (~24Gb GPUs) thanks to fractional quantization methods. Would this be in the cards?",
    "labels": [
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-01-17T19:37:50+00:00",
    "closed_at": "2024-07-25T06:32:53+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/22/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/22"
  },
  {
    "number": 21,
    "title": "[Feature Request] CFG in Backend Calls",
    "body": "Hello. In our use case, we would like to make calls to the backend with a \"grammar\" or \"cfg\" field similar to what outline implements in their vllm implementation:\r\n\r\nhttps://github.com/outlines-dev/outlines/pull/517\r\n\r\nWould this be in the cards? I see there is already a Lark CFG implementation, but it seems to be front end/python only. ",
    "labels": [
      "enhancement",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-01-17T18:50:56+00:00",
    "closed_at": "2024-07-25T06:31:57+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/21/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/21"
  },
  {
    "number": 14,
    "title": "Colab? ",
    "body": "Awesome project. We have a paper https://arxiv.org/abs/2310.14034 with really complicated KV caching that I would love to go back and implement in SGLang. \r\n\r\nI tried to get an example working in Colab for a demo, but I got kind of stuck getting the server running. \r\n\r\nThis runs fine: \r\n\r\n!nohup python -m sglang.launch_server --model-path TheBloke/Mistral-7B-v0.1-AWQ --port 30000\r\n\r\nBut then when I run the following, \r\n\r\n```\r\n%%script bash\r\ncurl http://localhost:30000/v1/completions \\\r\n  -H \"Content-Type: application/json\" \\\r\n  -d '{\r\n    \"prompt\": \"Say this is a test\",\r\n    \"max_tokens\": 16,\r\n    \"temperature\": 0\r\n  }'\r\n```\r\n\r\nI just get this. \r\n\r\n```\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\nWarning: available_size=75821, max_total_num_token=75833\r\nKV cache pool leak detected!\r\n```\r\nAny ideas?",
    "labels": [
      "collaboration",
      "inactive"
    ],
    "state": "closed",
    "created_at": "2024-01-16T20:08:21+00:00",
    "closed_at": "2024-07-25T06:32:37+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/14/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/14"
  },
  {
    "number": 13,
    "title": "enable an installation without CUDA_HOME?",
    "body": "Easier to install for users who just want to call LLM APIs.",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-16T14:57:28+00:00",
    "closed_at": "2024-01-17T00:15:30+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/13/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/13"
  },
  {
    "number": 5,
    "title": "Typo: rename image_url to image_file",
    "body": "https://github.com/sgl-project/sglang/blob/f652494df16ef9fa0fac998ddf63961aee0849d4/python/sglang/srt/utils.py#L212",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-13T06:41:07+00:00",
    "closed_at": "2024-01-16T23:42:00+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/5/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/5"
  }
]
[
  {
    "number": 7847,
    "title": "[Bug] [ROCm] Segmentation fault when capture batches in cuda graph",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nEncounter segmentation fault issue when capturing cuda graph in latest docker image(lmsysorg/sglang:v0.4.9-rocm630)\n\n```\nLoading safetensors checkpoint shards:  97% Completed | 158/163 [00:35<00:01,  4.41it/s]\nLoading safetensors checkpoint shards:  99% Completed | 162/163 [00:35<00:00,  7.01it/s]\nLoading safetensors checkpoint shards: 100% Completed | 163/163 [00:35<00:00,  4.61it/s]\n\n[2025-07-08 06:56:33 DP0 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=100.32 GB, mem usage=90.07 GB.\n[2025-07-08 06:56:36 DP0 TP0] KV Cache is allocated. #tokens: 131072, KV size: 8.58 GB\n[2025-07-08 06:56:36 DP0 TP0] Memory pool end. avail mem=90.37 GB\n[2025-07-08 06:56:36 DP6 TP6] KV Cache is allocated. #tokens: 131072, KV size: 8.58 GB\n[2025-07-08 06:56:36 DP4 TP4] KV Cache is allocated. #tokens: 131072, KV size: 8.58 GB\n[2025-07-08 06:56:36 DP7 TP7] KV Cache is allocated. #tokens: 131072, KV size: 8.58 GB\n[2025-07-08 06:56:36 DP3 TP3] KV Cache is allocated. #tokens: 131072, KV size: 8.58 GB\n[2025-07-08 06:56:36 DP5 TP5] KV Cache is allocated. #tokens: 131072, KV size: 8.58 GB\n[2025-07-08 06:56:36 DP2 TP2] KV Cache is allocated. #tokens: 131072, KV size: 8.58 GB\n[2025-07-08 06:56:36 DP1 TP1] KV Cache is allocated. #tokens: 131072, KV size: 8.58 GB\n[2025-07-08 06:56:38 DP0 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=90.21 GB\n[2025-07-08 06:56:38 DP0 TP0] Capture cuda graph bs [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]\nCapturing batches (bs=512 avail_mem=89.80 GB):   0%|                                                                                                                                                             | 0/51 [00:00<?, ?it/s][aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, s\ncale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None\n[2025-07-08 06:56:39 DP4 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None\n[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None\n[2025-07-08 06:56:39 DP7 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None\n[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None\n[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None\n[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None\n[2025-07-08 06:56:39 DP1 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None\n[2025-07-08 06:56:39 DP6 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None\n[2025-07-08 06:56:39 DP2 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None\n[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None\n[2025-07-08 06:56:39 DP0 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None\n[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None\n[2025-07-08 06:56:39 DP3 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None\n[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None\n[2025-07-08 06:56:39 DP5 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None\n[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None\n[2025-07-08 06:56:41 DP6 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None\n[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None\n[2025-07-08 06:56:41 DP4 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None\n[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None\n[2025-07-08 06:56:41 DP7 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None\n[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None\n[2025-07-08 06:56:41 DP2 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None\n[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None\n[2025-07-08 06:56:41 DP5 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None\n[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None\n[2025-07-08 06:56:41 DP0 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None\n[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None\n[2025-07-08 06:56:41 DP1 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None\n[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None\n[2025-07-08 06:56:41 DP3 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None\n[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success\n[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success\n[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success\n[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success\n[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success\n[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success\n[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success\n[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success\nCapturing batches (bs=448 avail_mem=87.41 GB):   8%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                         | 4/51 [00:20<03:35,  4.59s/it]Fatal Python error: Segmentation fault\nThread 0x00007ee0b4dff640 (most recent call first):\n  File \"/usr/lib/python3.12/threading.py\", line 359 in wait\n  File \"/usr/lib/python3.12/threading.py\", line 655 in wait\n  File \"/usr/local/lib/python3.12/dist-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/usr/lib/python3.12/threading.py\", line 1075 in _bootstrap_inner\n  File \"/usr/lib/python3.12/threading.py\", line 1032 in _bootstrap\n\nThread 0x00007eed1f5ff640 (most recent call first):\n  File \"/usr/lib/python3.12/threading.py\", line 359 in wait\n  File \"/usr/lib/python3.12/threading.py\", line 655 in wait\n  File \"/usr/local/lib/python3.12/dist-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/usr/lib/python3.12/threading.py\", line 1075 in _bootstrap_inner\n  File \"/usr/lib/python3.12/threading.py\", line 1032 in _bootstrap\n\nThread 0x00007efd95447640 (most recent call first):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 55 in _recv_msg\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 191 in _read_thread\n  File \"/usr/lib/python3.12/threading.py\", line 1012 in run\n  File \"/usr/lib/python3.12/threading.py\", line 1075 in _bootstrap_inner\n  File \"/usr/lib/python3.12/threading.py\", line 1032 in _bootstrap\n\nCurrent thread 0x00007f224c569280 (most recent call first):\n  File \"/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py\", line 373 in ncclAllReduce\n  File \"/sgl-workspace/sglang/python/sglang/srt/distributed/device_communicators/pynccl.py\", line 139 in all_reduce\n  File \"/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py\", line 498 in _all_reduce_in_place\n  File \"/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py\", line 117 in inplace_all_reduce\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_ops.py\", line 1122 in __call__\n  File \"/sgl-workspace/sglang/python/sglang/srt/distributed/parallel_state.py\", line 480 in all_reduce\n  File \"/sgl-workspace/sglang/python/sglang/srt/distributed/communication_op.py\", line 13 in tensor_model_parallel_all_reduce\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 465 in forward_normal\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 423 in forward\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750 in _call_impl\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739 in _wrapped_call_impl\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1845 in forward\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750 in _call_impl\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739 in _wrapped_call_impl\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1991 in forward\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750 in _call_impl\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739 in _wrapped_call_impl\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 2098 in forward\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 575 in run_once\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 587 in capture_one_batch_size\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 436 in capture\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 336 in __init__\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1373 in init_cuda_graphs\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 314 in initialize\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 233 in __init__\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 81 in __init__\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 66 in __init__\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 338 in __init__\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2721 in run_scheduler_process\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108 in run\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314 in _bootstrap\n  File \"/usr/lib/python3.12/multiprocessing/spawn.py\", line 135 in _main\n  File \"/usr/lib/python3.12/multiprocessing/spawn.py\", line 122 in spawn_main\n  File \"<string>\", line 1 in <module>\n\nExtension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, charset_normalizer\n.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, uvloop.loop, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards,\n torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, psutil._psutil_linux, psutil._psutil_posix, zmq.backend.cython._zmq, PIL._imaging, setproctitle, yaml._yaml, regex._regex, markupsafe._speedups, PIL._imagingft, hiredis.hiredis, cython.cimports.libc.math, scipy._lib._ccallback_c, scipy.linalg._\nfblas, scipy.linalg._flapack, _cyutility, scipy._cyutility, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _cspar\nsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.\noptimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special.\n_comb, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._direct, sentencepiece._sentencepiece, msgspec._core, vllm.u\ntils, _cffi_backend, vllm.sampling_params, vllm.sequence, roctxMarker, vllm.model_executor.layers.sampler, vllm.core.scheduler, vllm.engine.output_processor.stop_checker, msgpack._cmsgpack, google._upb._message, ray._raylet, vllm.transformers_utils.detokenizer, vllm.outputs, vllm.engine.llm_engine, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_date\ntime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs\n.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.\nindexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, Cython.Utils, Cython.Plex.Actions, Cython.Plex.Transitions, Cython.Plex.Machines, Cython.Plex.DFA, Cython.Plex.Scanners, Cython.C\nompiler.Scanning, Cython.StringIOTree, Cython.Compiler.Code, hip_utils, __triton_launcher (total: 168)\n\n[2025-07-08 06:57:01] DataParallelController hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/data_parallel_controller.py\", line 301, in run_data_parallel_controller_process\n    controller = DataParallelController(server_args, port_args)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/data_parallel_controller.py\", line 89, in __init__\n    dp_port_args = self.launch_dp_attention_schedulers(server_args, port_args)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/data_parallel_controller.py\", line 164, in launch_dp_attention_schedulers\n    self.launch_tensor_parallel_group(server_args, port_args, 0, None)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/data_parallel_controller.py\", line 244, in launch_tensor_parallel_group\n    scheduler_info.append(scheduler_pipe_readers[i].recv())\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n          ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n    buf = self._recv(4)\n          ^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 399, in _recv\n    raise EOFError\nEOFError\n\n[2025-07-08 06:57:01] Received sigquit from a child process. It usually means the child failed.\n\n```\n\nlooks like this issue related to data parallel, if we disable dp feature(remove --dp-size 16 & --enable-dp-attention), will also get some aiter error in cuda graph capturing\n\n```\n[2025-07-08 07:10:20] Received sigquit from a child process. It usually means the child failed.\nCapturing batches (bs=512 avail_mem=101.02 GB):   0%|                                                                                                                                                            | 0/51 [00:02<?, ?it/s]\n[2025-07-08 07:10:20 TP0] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2721, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, pp_rank, dp_rank)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 338, in __init__\n    self.tp_worker = TpWorkerClass(\n                     ^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 66, in __init__\n    self.worker = TpModelWorker(\n                  ^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 81, in __init__\n    self.model_runner = ModelRunner(\n                        ^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 233, in __init__\n    self.initialize(min_per_gpu_memory)\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 314, in initialize\n    self.init_cuda_graphs()\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1373, in init_cuda_graphs\n    self.cuda_graph_runner = CudaGraphRunner(self)\n                             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 336, in __init__\n    self.capture()\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 436, in capture\n    ) = self.capture_one_batch_size(bs, forward)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 587, in capture_one_batch_size\n    run_once()\n  File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 575, in run_once\n    logits_output_or_pp_proxy_tensors = forward(\n                                        ^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 2098, in forward\n    hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1991, in forward\n    hidden_states, residual = layer(\n                              ^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1834, in forward\n    hidden_states = self.self_attn(\n                    ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1054, in forward\n    return self.forward_core(s)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1110, in forward_core\n    return self.forward_absorb_core(*inner_state)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1265, in forward_absorb_core\n    attn_output = self.attn_mqa(q, k, k_nope, forward_batch)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/radix_attention.py\", line 100, in forward\n    return forward_batch.attn_backend.forward(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/base_attn_backend.py\", line 69, in forward\n    return self.forward_decode(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/sglang/python/sglang/srt/layers/attention/aiter_backend.py\", line 657, in forward_decode\n    mla_decode_fwd(\n  File \"/sgl-workspace/aiter/aiter/mla.py\", line 137, in mla_decode_fwd\n    num_kv_splits, mgc = get_meta_param(\n                         ^^^^^^^^^^^^^^^\n  File \"/sgl-workspace/aiter/aiter/mla.py\", line 107, in get_meta_param\n    assert nhead in get_mgc, f\"{nhead=} not supported\"\n           ^^^^^^^^^^^^^^^^\nAssertionError: nhead=8 not supported\n```\n\n### Reproduction\n\n# start docker\n```\nDOCKER=lmsysorg/sglang:v0.4.9-rocm630\nNAME=Your_Name\nsudo docker stop $NAME\nsudo docker rm $NAME\nsudo docker run -it --privileged --network=host --device=/dev/kfd --device=/dev/dri --group-add video --cap-add=SYS_PTRACE --security-opt seccomp=unconfined \\\n                -v /home:/home -v /data:/data -v /apps:/apps -v /mnt:/mnt -v /shared:/shared -v /nfs_shared1:/nfs_shared1 -w /workspace \\\n                --shm-size=512G --ulimit memlock=-1 --ulimit stack=67108864 --name $NAME $DOCKER /bin/bash\n```\n\n# env setting for IB(for example in my device)\n```\nexport NCCL_DEBUG=INFO\nexport NCCL_IB_HCA=bnxt_re\nexport NCCL_IB_QPS_PER_CONNECTION=8\nexport NCCL_SOCKET_FAMILY=AF_INET\nexport NCCL_IB_TIMEOUT=22\nexport NCCL_IB_DISABLE=0\nexport NCCL_IB_RETRY_CNT=12\nexport NCCL_NVLS_ENABLE=0\nexport GLOO_SOCKET_IFNAME=ens50f0\nexport NCCL_SOCKET_IFNAME=ens50f0\nunset DEBUG_HIP_BLOCK_SYN\nunset GPU_FORCE_BLIT_COPY_SIZE\nexport HSA_NO_SCRATCH_RECLAIM=1\n```\n\n# scripts\n## on node 0\n```\npython3 -m sglang.launch_server --model-path /apps/data/models/DSR1 --attention-backend aiter --trust-remote-code --tp-size 16 --dp-size 16 --enable-dp-attention --enable-ep-moe --chunked-prefill-size 65536 --max-total-tokens 131072 --dist-init-addr 10.235.192.56:2000 --nnodes 2 --node-rank 0\n```\n## on node1\n```\npython3 -m sglang.launch_server --model-path /apps/data/models/DSR1 --attention-backend aiter --trust-remote-code --tp-size 16 --dp-size 16 --enable-dp-attention --enable-ep-moe --chunked-prefill-size 65536 --max-total-tokens 131072 --dist-init-addr 10.235.192.56:2000 --nnodes 2 --node-rank 0\n```\n\n### Environment\n\n```\nPython: 3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0]\nROCM available: True\nGPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.4\nROCM_HOME: /opt/rocm\nHIPCC: HIP version: 6.3.42131-fa1d09cbd\nROCM Driver Version: 6.10.5\nPyTorch: 2.6.0a0+git8d4926e\nsglang: 0.4.9\nsgl_kernel: 0.2.4\nflashinfer_python: Module Not Found\ntriton: 3.2.0+gitcddf0fc3\ntransformers: 4.53.0\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.6\nhf_transfer: 0.1.9\nhuggingface_hub: 0.33.2\ninteregular: 0.3.3\nmodelscope: 1.27.1\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.5\npython-multipart: 0.0.20\npyzmq: 26.2.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.7.dev2+g113274a0.rocm630\nxgrammar: 0.1.19\nopenai: 1.93.0\ntiktoken: 0.7.0\nanthropic: 0.57.1\nlitellm: 1.74.0\ndecord: 0.6.0\nAMD Topology:\n\n\n============================ ROCm System Management Interface ============================\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI\nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI\nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI\nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI\nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI\nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0\n================================== End of ROCm SMI Log ===================================\n\nulimit soft: 1048576\n\n```",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-08T07:17:10+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7847/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7847"
  },
  {
    "number": 7474,
    "title": "[Feature] support dimensions param for embedding models",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nAs documented in [openai website](https://platform.openai.com/docs/api-reference/embeddings/create#embeddings-create-dimensions), users can pass a param called dimensions to specify the dimension of output vector. \n\n\n\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-23T12:09:32+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7474/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7474"
  },
  {
    "number": 7703,
    "title": "[Bug] [CI regression] [AMD] TestNoOverlapScheduler",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThe CI **unit-test-backend-1-gpu-amd** failed when run`test/srt/test_no_overlap_scheduler.py`. It exits with a GPU memory access fault on node-2. \n\n**Error snippet**:\n\n```text\n...\nbatch. #new-seq: 1, #new-token: 32, #cached-token: 0, token usage: 0.00, #running-req: 8, #queue-req: 119\n[2025-07-02 03:22:31] Prefill batch. #new-seq: 1, #new-token: 32, #cached-token: 0, token usage: 0.00, #running-req: 8, #queue-req: 119\n...\n[2025-07-02 03:22:34] Prefill batch. #new-seq: 2, #new-token: 32, #cached-token: 0, token usage: 0.01, #running-req: 45, #queue-req: 81\nMemory access fault by GPU node-2 (Agent handle: 0xdedb180) on address 0x7f57d9a00000. Reason: Unknown.\n```\n@hubertlu-tw suggust temporarily disable the test in AMD CI to avoid blocking other PRs.\n\n### Reproduction\n\nSample failure run: https://github.com/sgl-project/sglang/actions/runs/15965626491/job/45029188833\n\n```bash\nSGLANG_AMD_CI=1 SGLANG_IS_IN_CI=1 SGLANG_USE_AITER=1 python3 -m unittest test_no_overlap_scheduler.py\n```\n\n### Environment\n\n* Docker image: `lmsysorg/sglang:v0.4.8.post1-rocm630`\n\nCC: @saienduri @HaiShaw @hubertlu-tw ",
    "labels": [
      "amd"
    ],
    "state": "open",
    "created_at": "2025-07-02T01:44:36+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7703/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7703"
  },
  {
    "number": 7568,
    "title": "Performance regression: 4090 GPUs slower on v0.4.8-cu126 (was 27\u219220 tokens/sec, A100 unaffected)",
    "body": "Hi! After upgrading from `lmsysorg/sglang:v0.4.6.post5-cu124` to `lmsysorg/sglang:v0.4.8-cu126`, I've noticed a **significant drop in generation speed** and GPU utilization on my setup with 2x4090 (48Gb each).  \n- **v0.4.6:** 27 tokens/sec, GPU usage 100%\n- **v0.4.8:** 20 tokens/sec, GPU usage ~75%\n\nNo other changes were made except switching the Docker image.\n\nOn an A100 80Gb, both versions work fine \u2014 no speed or GPU usage drop.\n\n### Environment\n\n- **GPUs:** 2x4090 48Gb\n- **Docker Compose Config:** (see below)\n- **Model:** Qwen3-32B\n- **nvidia-smi:**\n```\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 4090        On  |   00000000:16:00.0 Off |                  Off |\n| 32%   37C    P8             19W /  450W |   43834MiB /  49140MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA GeForce RTX 4090        On  |   00000000:BE:00.0 Off |                  Off |\n| 45%   40C    P8             25W /  450W |   43854MiB /  49140MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A   2121769      C   sglang::scheduler_TP0                       43824MiB |\n|    1   N/A  N/A   2121770      C   sglang::scheduler_TP1                       43844MiB |\n+-----------------------------------------------------------------------------------------+\n```\n### docker-compose.yml\n\n```yaml\nservices:\n  llm:\n    image: lmsysorg/sglang:v0.4.6.post5-cu124\n    ulimits:\n      memlock: -1\n      stack: 67108864\n    ipc: host\n    entrypoint: python3 -m sglang.launch_server\n    command: >\n      --model /llm_models/qwen3-32b\n      --host 0.0.0.0\n      --port 80\n      --log-requests\n      --log-requests-level 2\n      --show-time-cost\n      --random-seed 42\n      --grammar-backend llguidance\n      --enable-metrics\n      --tp 2\n      --enable-p2p-check\n      --kv-cache-dtype fp8_e5m2\n      --allow-auto-truncate\n      --context-length 40000\n      --enable-torch-compile\n      --max-running-requests 16\n      --chat-template /llm_models/qwen3-32b/qwen3-template-nothink.jinja\n    restart: unless-stopped\n    volumes:\n     - /home/user/scripts/models:/llm_models\n    environment:\n     - TORCHINDUCTOR_CACHE_DIR=/llm_models/inductor_root_cache_qwen3\n    ports:\n      - 8848:80\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              device_ids: ['0', '1']\n              capabilities: [gpu]\n```",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-26T13:32:58+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7568/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7568"
  },
  {
    "number": 8089,
    "title": "[Bug] If rid is repeated, there will be no output",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nOnly Receive, no Finish\n[2025-07-16 14:43:09] Receive: obj=EmbeddingReqInput(rid=['1846246021_91f15ad6-b9e3-43f4-a1a4-ef9bbb30f8d4', '1846246021_d55a53c3-3e6b-41e2-9e0f-af822fd7dfe3', '1846246021_97fab5d0-edd9-4742-9fc2-b745737fc031', '1846246021_643f81cc-1414-497f-af0e-bd1d0a0adcdd', '1846246021_04f97c20-9143-4798-8131-a786e89d0601', '1846246021_9b5561b4-e97f-4c1d-add1-4f8b2c8503e8'], sampling_params=[{'max_new_tokens': 0}, {'max_new_tokens': 0}, {'max_new_tokens': 0}, {'max_new_tokens': 0}, {'max_new_tokens': 0}, {'max_new_tokens': 0}], log_metrics=True, modalities=None, is_cross_encoder_request=False)\n[2025-07-16 14:43:19] Receive: obj=EmbeddingReqInput(rid=['1846246021_91f15ad6-b9e3-43f4-a1a4-ef9bbb30f8d4', '1846246021_d55a53c3-3e6b-41e2-9e0f-af822fd7dfe3', '1846246021_97fab5d0-edd9-4742-9fc2-b745737fc031', '1846246021_643f81cc-1414-497f-af0e-bd1d0a0adcdd', '1846246021_04f97c20-9143-4798-8131-a786e89d0601', '1846246021_9b5561b4-e97f-4c1d-add1-4f8b2c8503e8'], sampling_params=[{'max_new_tokens': 0}, {'max_new_tokens': 0}, {'max_new_tokens': 0}, {'max_new_tokens': 0}, {'max_new_tokens': 0}, {'max_new_tokens': 0}], log_metrics=True, modalities=None, is_cross_encoder_request=False)\n\n### Reproduction\n\nreward model server\nA batch contains all the same RID, or two batches with the same ID are sent consecutively\n\n### Environment\n\nPython: 3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 17:29:18) [GCC 11.2.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA A800-SXM4-80GB\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 8.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.3, V12.3.107\nCUDA Driver Version: 525.125.06\nPyTorch: 2.7.1+cu126\nsglang: 0.4.9.post2\nsgl_kernel: 0.2.5\nflashinfer_python: 0.2.7.post1\ntriton: 3.3.1\ntransformers: 4.53.0\ntorchao: 0.9.0\nnumpy: 2.3.1\naiohttp: 3.12.13\nfastapi: 0.116.0\nhf_transfer: 0.1.9\nhuggingface_hub: 0.33.2\ninteregular: 0.3.3\nmodelscope: 1.27.1\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.7\npython-multipart: 0.0.20\npyzmq: 27.0.0\nuvicorn: 0.35.0\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.21\nopenai: 1.93.1\ntiktoken: 0.9.0\nanthropic: 0.57.1\nlitellm: 1.74.0.post1\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2  NIC3     NIC4    NIC5    NIC6    NIC7    NIC8    CPU Affinity    NUMA Affinity\nGPU0     X      NV8     NV8     NV8     NV8     NV8     NV8     NV8     NODE    PXB     PXB   NODE     NODE    SYS     SYS     SYS     SYS     0-31,64-95      0\nGPU1    NV8      X      NV8     NV8     NV8     NV8     NV8     NV8     NODE    PXB     PXB   NODE     NODE    SYS     SYS     SYS     SYS     0-31,64-95      0\nGPU2    NV8     NV8      X      NV8     NV8     NV8     NV8     NV8     NODE    NODE    NODE  PXB      PXB     SYS     SYS     SYS     SYS     0-31,64-95      0\nGPU3    NV8     NV8     NV8      X      NV8     NV8     NV8     NV8     NODE    NODE    NODE  PXB      PXB     SYS     SYS     SYS     SYS     0-31,64-95      0\nGPU4    NV8     NV8     NV8     NV8      X      NV8     NV8     NV8     SYS     SYS     SYS   SYS      SYS     PXB     PXB     NODE    NODE    32-63,96-127    1\nGPU5    NV8     NV8     NV8     NV8     NV8      X      NV8     NV8     SYS     SYS     SYS   SYS      SYS     PXB     PXB     NODE    NODE    32-63,96-127    1\nGPU6    NV8     NV8     NV8     NV8     NV8     NV8      X      NV8     SYS     SYS     SYS   SYS      SYS     NODE    NODE    PXB     PXB     32-63,96-127    1\nGPU7    NV8     NV8     NV8     NV8     NV8     NV8     NV8      X      SYS     SYS     SYS   SYS      SYS     NODE    NODE    PXB     PXB     32-63,96-127    1\nNIC0    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE  NODE     NODE    SYS     SYS     SYS     SYS\nNIC1    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      PIX   NODE     NODE    SYS     SYS     SYS     SYS\nNIC2    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     NODE    PIX      X    NODE     NODE    SYS     SYS     SYS     SYS\nNIC3    NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODE    NODE   X       PIX     SYS     SYS     SYS     SYS\nNIC4    NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODE    NODE  PIX       X      SYS     SYS     SYS     SYS\nNIC5    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    SYS     SYS     SYS   SYS      SYS      X      PIX     NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    SYS     SYS     SYS   SYS      SYS     PIX      X      NODE    NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     SYS     SYS     SYS   SYS      SYS     NODE    NODE     X      PIX\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     SYS     SYS     SYS   SYS      SYS     NODE    NODE    PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n\n\nulimit soft: 1048576",
    "labels": [],
    "state": "closed",
    "created_at": "2025-07-16T08:37:45+00:00",
    "closed_at": "2025-07-17T02:50:18+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/8089/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/8089"
  },
  {
    "number": 7652,
    "title": "[Feature] TokenWeave optimizations",
    "body": "Hi,\nWould it be possible to integrate these optimizations into sglang?\n\nCode: https://github.com/microsoft/tokenweave/tree/main\nPaper: https://arxiv.org/abs/2505.11329",
    "labels": [],
    "state": "closed",
    "created_at": "2025-06-30T08:04:47+00:00",
    "closed_at": "2025-06-30T08:16:16+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7652/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7652"
  },
  {
    "number": 7586,
    "title": "[Bug] [CI regression] TestEpMoEFP8",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nCI unit-test-backend-2-gpu seems to be broken since the past few days (if not longer).\n\nLooking at the log, it seems to be watchdog timeout at TestEpMoEFP8. The non-quantized version TestEpMoE seems to be working fine.\n\n### Reproduction\n\nSample failure: https://github.com/sgl-project/sglang/actions/runs/15916204833/job/44895747274\n\n### Environment\n\nCI",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-27T05:49:07+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7586/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7586"
  },
  {
    "number": 7910,
    "title": "[Feature] Cutlass kernels for LoRA",
    "body": "### Checklist\n\n- [ ] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nCreating an issue to track the work for supporting a CUTLASS / CUTE kernel for LoRA to see if there is any perf gain comparing with the current Triton one.\n\nDependency: this task should happen after #7809 as the FlashInfer deprecation is expected to change / simplify the kernel interface.\n\n(cc @Fridge003 @Ying1123 )\n\n### Related resources\n\n_No response_",
    "labels": [
      "lora"
    ],
    "state": "open",
    "created_at": "2025-07-09T21:43:29+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7910/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7910"
  },
  {
    "number": 7495,
    "title": "[Bug] Qwen3 FP8 models crash at startup without `SGL_ENABLE_JIT_DEEPGEMM=0`",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nThis problem is mentioned in #7482, but the issue is about a different problem. Running sglang on commit fa42e419629e0651a8caf332330942da920cdac8 built from source fails on FP8 versions of Qwen3 models.\n\n```\n$ python -m sglang.launch_server --model-path Qwen/Qwen3-0.6B-FP8 --tp 1 --reasoning-parser qwen3\nINFO 06-24 08:53:58 [__init__.py:243] Automatically detected platform cuda.\n[2025-06-24 08:54:04] server_args=ServerArgs(model_path='Qwen/Qwen3-0.6B-FP8', tokenizer_path='Qwen/Qwen3-0.6B-FP8', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='Qwen/Qwen3-0.6B-FP8', chat_template=None, completion_template=None, is_embedding=False, enable_multimodal=None, revision=None, impl='auto', host='127.0.0.1', port=30000, mem_fraction_static=0.874, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=324478805, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, bucket_time_to_first_token=None, bucket_e2e_request_latency=None, bucket_inter_token_latency=None, collect_tokens_histogram=False, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser='qwen3', tool_call_parser=None, dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, ep_size=1, enable_ep_moe=False, enable_deepep_moe=False, enable_flashinfer_moe=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, cuda_graph_max_bs=None, cuda_graph_bs=None, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_nccl_nvls=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, disable_overlap_schedule=False, disable_overlap_cg_plan=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, enable_return_hidden_states=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, num_reserved_decode_tokens=512, pdlb_url=None, custom_weight_loader=[], weight_loader_disable_mmap=False)\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 894/894 [00:00<00:00, 10.8MB/s]\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.73k/9.73k [00:00<00:00, 84.3MB/s]\nvocab.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.78M/2.78M [00:00<00:00, 32.1MB/s]\nmerges.txt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.67M/1.67M [00:00<00:00, 38.1MB/s]\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11.4M/11.4M [00:00<00:00, 95.9MB/s]\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750755252.315513  244589 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750755252.317313  244588 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750755252.320068  244589 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1750755252.321832  244588 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nINFO 06-24 08:54:17 [__init__.py:243] Automatically detected platform cuda.\nINFO 06-24 08:54:17 [__init__.py:243] Automatically detected platform cuda.\n[2025-06-24 08:54:23] Attention backend not set. Use fa3 backend by default.\n[2025-06-24 08:54:23] Init torch distributed begin.\n[2025-06-24 08:54:23] Init torch distributed ends. mem usage=0.00 GB\n[2025-06-24 08:54:26] Load weight begin. avail mem=75.02 GB\n[2025-06-24 08:54:26] Detected fp8 checkpoint.\n[2025-06-24 08:54:26] Using model weights format ['*.safetensors']\nmodel.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.06G/1.06G [00:00<00:00, 1.35GB/s]\n[2025-06-24 08:54:27] No model.safetensors.index.json found in remote.\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  8.77it/s]\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  8.76it/s]\n\n[2025-06-24 08:54:27] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=74.28 GB, mem usage=0.74 GB.\n[2025-06-24 08:54:27] KV Cache is allocated. #tokens: 606832, K size: 32.41 GB, V size: 32.41 GB\n[2025-06-24 08:54:27] Memory pool end. avail mem=8.73 GB\n[2025-06-24 08:54:27] Capture cuda graph begin. This can take up to several minutes. avail mem=8.63 GB\nCapture cuda graph bs [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160]\nCapturing batches (avail_mem=8.55 GB):   0%|                                                                                                                                                                           | 0/23 [00:00<?, ?it/s]\n[2025-06-24 08:54:27] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/home/user/sglang/python/sglang/srt/managers/scheduler.py\", line 2636, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, pp_rank, dp_rank)\n  File \"/home/user/sglang/python/sglang/srt/managers/scheduler.py\", line 315, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/home/user/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 64, in __init__\n    self.worker = TpModelWorker(\n  File \"/home/user/sglang/python/sglang/srt/managers/tp_worker.py\", line 79, in __init__\n    self.model_runner = ModelRunner(\n  File \"/home/user/sglang/python/sglang/srt/model_executor/model_runner.py\", line 220, in __init__\n    self.initialize(min_per_gpu_memory)\n  File \"/home/user/sglang/python/sglang/srt/model_executor/model_runner.py\", line 300, in initialize\n    self.init_cuda_graphs()\n  File \"/home/user/sglang/python/sglang/srt/model_executor/model_runner.py\", line 1154, in init_cuda_graphs\n    self.cuda_graph_runner = CudaGraphRunner(self)\n  File \"/home/user/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 334, in __init__\n    self.capture()\n  File \"/home/user/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 434, in capture\n    ) = self.capture_one_batch_size(bs, forward)\n  File \"/home/user/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 585, in capture_one_batch_size\n    run_once()\n  File \"/home/user/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py\", line 573, in run_once\n    logits_output_or_pp_proxy_tensors = forward(\n  File \"/home/user/eval/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/user/sglang/python/sglang/srt/models/qwen3.py\", line 298, in forward\n    hidden_states = self.model(\n  File \"/home/user/eval/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/user/eval/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/user/sglang/python/sglang/srt/models/qwen2.py\", line 315, in forward\n    hidden_states, residual = layer(\n  File \"/home/user/eval/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/user/eval/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/user/sglang/python/sglang/srt/models/qwen3.py\", line 192, in forward\n    hidden_states = self.self_attn(\n  File \"/home/user/eval/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/user/eval/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/user/sglang/python/sglang/srt/models/qwen3.py\", line 130, in forward\n    qkv, _ = self.qkv_proj(hidden_states)\n  File \"/home/user/eval/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/user/eval/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/user/sglang/python/sglang/srt/layers/linear.py\", line 445, in forward\n    output_parallel = self.quant_method.apply(self, input_, bias)\n  File \"/home/user/sglang/python/sglang/srt/layers/quantization/fp8.py\", line 429, in apply\n    return self.w8a8_block_fp8_linear(\n  File \"/home/user/sglang/python/sglang/srt/layers/quantization/fp8_utils.py\", line 249, in deepgemm_w8a8_block_fp8_linear_with_fallback\n    output = w8a8_block_fp8_matmul_deepgemm(\n  File \"/home/user/sglang/python/sglang/srt/layers/quantization/fp8_kernel.py\", line 817, in w8a8_block_fp8_matmul_deepgemm\n    torch.ops.sglang.deep_gemm_fp8_fp8_bf16_nt(A, As, B, Bs, C)\n  File \"/home/user/eval/venv/lib/python3.10/site-packages/torch/_ops.py\", line 1158, in __call__\n    return self._op(*args, **(kwargs or {}))\n  File \"/home/user/sglang/python/sglang/srt/layers/quantization/fp8_kernel.py\", line 76, in deep_gemm_fp8_fp8_bf16_nt\n    deep_gemm_wrapper.gemm_nt_f8f8bf16((A, As), (B, Bs), C)\n  File \"/home/user/sglang/python/sglang/srt/layers/quantization/deep_gemm_wrapper/entrypoint.py\", line 89, in gemm_nt_f8f8bf16\n    _gemm_nt_f8f8bf16_raw(\n  File \"/home/user/eval/venv/lib/python3.10/site-packages/deep_gemm/dispatch.py\", line 85, in fp8_gemm_nt\n    sfa = transform_sf_into_required_layout(sfa, mn=m, k=k, recipe=recipe, is_sfa=True)\n  File \"/home/user/eval/venv/lib/python3.10/site-packages/deep_gemm/utils/layout.py\", line 175, in transform_sf_into_required_layout\n    assert False, f'Unknown cases: {sf.dtype=}, {gran=}, arch={get_device_arch()}'\nAssertionError: Unknown cases: sf.dtype=torch.int32, gran=(1, 128), arch=90a\n\n[2025-06-24 08:54:27] Received sigquit from a child process. It usually means the child failed.\nKilled\n```\n\nSetting `SGL_ENABLE_JIT_DEEPGEMM=0` avoids the crash, but I expect it to have a performance impact that would preferably be avoided. The same issue exists with the bigger FP8 models as well. An earlier version that self-identified as 0.4.6.post1 was still fine, but I don't have the commit hash for it.\n\n\n### Reproduction\n\n`python -m sglang.launch_server --model-path Qwen/Qwen3-0.6B-FP8 --tp 1 --reasoning-parser qwen3`\n\n### Environment\n\n```\nPython: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3: NVIDIA H100 80GB HBM3\nGPU 0,1,2,3 Compute Capability: 9.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.8, V12.8.93\nCUDA Driver Version: 535.216.01\nPyTorch: 2.7.1+cu128\nsglang: 0.4.7.post1\nsgl_kernel: 0.1.9\nflashinfer_python: 0.2.6.post1\ntriton: 3.3.1\ntransformers: 4.52.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.9.5\nfastapi: 0.115.4\nhf_transfer: 0.1.9\nhuggingface_hub: 0.32.4\ninteregular: 0.3.3\nmodelscope: 1.26.0\norjson: 3.10.3\noutlines: 0.1.11\npackaging: 24.2\npsutil: 5.9.8\npydantic: 2.11.5\npython-multipart: 0.0.20\npyzmq: 26.2.0\nuvicorn: 0.29.0\nuvloop: 0.19.0\nvllm: 0.5.5.20250604\nxgrammar: 0.1.19\nopenai: 1.84.0\ntiktoken: 0.7.0\nanthropic: 0.31.0\nlitellm: 1.72.2\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    GPU2    GPU3    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10   NIC11   NIC12   NIC13   NIC14   NIC15   NIC16   CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0,2,4,6,8,10    0               N/A\nGPU1    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    1,3,5,7,9,11    1               N/A\nGPU2    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    1,3,5,7,9,11    1               N/A\nGPU3    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    1,3,5,7,9,11    1               N/A\nNIC0    NODE    SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC1    NODE    SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC2    PIX     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC3    NODE    SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\nNIC4    SYS     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\nNIC5    SYS     PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE\nNIC6    SYS     NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE\nNIC7    SYS     NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE\nNIC8    SYS     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE     X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX\nNIC9    SYS     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX\nNIC10   SYS     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX\nNIC11   SYS     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX\nNIC12   SYS     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX\nNIC13   SYS     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX\nNIC14   SYS     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX\nNIC15   SYS     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX\nNIC16   SYS     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: ibp0\n  NIC1: ibp1\n  NIC2: ibp2\n  NIC3: ibp3\n  NIC4: ibp4\n  NIC5: ibp5\n  NIC6: ibp6\n  NIC7: ibp7\n  NIC8: mlx5_0\n  NIC9: mlx5_1\n  NIC10: mlx5_2\n  NIC11: mlx5_3\n  NIC12: mlx5_4\n  NIC13: mlx5_5\n  NIC14: mlx5_6\n  NIC15: mlx5_7\n  NIC16: mlx5_8\n\n\nulimit soft: 1024\n```",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-24T09:10:01+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7495/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7495"
  },
  {
    "number": 7701,
    "title": "[Bug] [CI regression] [AMD] TestVisionChunkedPrefill",
    "body": "### Checklist\n\n- [ ] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nCI unit-test-backend-1-gpu-amd seems to be broken since the past few days.\n\n```\n\toutput with chunked prefill:\n\tThe video features a person standing on a stage with a dark background. The individual is dressed in a black outfit and appears to be speaking or presenting. The stage\n\toutput without chunked prefill:\n\tThe video features a person standing on a stage with a dark background. The individual is dressed in a black outfit and appears to be speaking or presenting. The stage\n\toutput with chunked prefill:\n\t['The video features a close-up shot of a person holding a small, rectangular electronic device, which appears to be an iPod. The individual is wearing a black shirt', 'The video features a close-up shot of a person holding a small, white electronic device, which appears to be an iPod. The individual is wearing a black shirt', 'The video features a close-up shot of a person holding a small, white electronic device, which appears to be an iPod. The individual is wearing a black shirt', 'The video features a close-up shot of a person holding a small, white electronic device, which appears to be an iPod. The individual is wearing a black shirt']\n\toutput without chunked prefill:\n\t['The video features a close-up shot of a person holding a small, rectangular electronic device, which appears to be an iPod. The individual is wearing a black shirt', 'The video features a close-up shot of a person holding a small, white electronic device, which appears to be an iPod. The individual is wearing a black shirt', 'The video features a close-up shot of a person holding a small, white electronic device, which appears to be an iPod. The individual is wearing a black shirt', 'The video features a close-up of a person holding a small, white electronic device, which appears to be an iPod. The individual is wearing a black shirt and']\n\tE\n\t======================================================================\n\tERROR: test_chunked_prefill (test_vision_chunked_prefill.TestVisionChunkedPrefill.test_chunked_prefill)\n\t----------------------------------------------------------------------\n\tTraceback (most recent call last):\n\t  File \"/sgl-workspace/llama4_sgl/python/sglang/srt/utils.py\", line 2052, in retry\n\t    return fn()\n\t           ^^^^\n\t  File \"/sgl-workspace/llama4_sgl/python/sglang/test/test_utils.py\", line 1237, in <lambda>\n\t    lambda: super(CustomTestCase, self)._callTestMethod(method),\n\t            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\tAssertionError: Lists differ: ['The[511 chars]e-up shot of a person holding a small, white e[83 chars]irt'] != ['The[511 chars]e-up of a person holding a small, white electr[82 chars]and']\n\t\n\tFirst differing element 3:\n\t'The [21 chars]e-up shot of a person holding a small, white e[82 chars]hirt'\n\t'The [21 chars]e-up of a person holding a small, white electr[81 chars] and'\n\t\n\tDiff is 1108 characters long. Set self.maxDiff to None to see it.\n\n```\nI suggest to temporarily disable the test in CI to avoid blocking other PRs until we resolve the issue.\n\n\n\n\n### Reproduction\n\nSample failure: https://github.com/sgl-project/sglang/actions/runs/15965626491/job/45029188822\n```\nSGLANG_AMD_CI=1 SGLANG_IS_IN_CI=1 SGLANG_USE_AITER=1  python3 -m unittest test_vision_chunked_prefill.TestVisionChunkedPrefill.test_chunked_prefill\n```\n\n### Environment\n\nlmsysorg/sglang:v0.4.8.post1-rocm630\n\n\nCC: @saienduri @HaiShaw ",
    "labels": [
      "amd"
    ],
    "state": "open",
    "created_at": "2025-07-01T23:25:20+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7701/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7701"
  },
  {
    "number": 7538,
    "title": "Task 005: Router Interface and Factory",
    "body": "# Task 005: Router Interface and Factory\n\n## Summary\nDefine a Router trait (interface) that abstracts routing operations and create a RouterFactory to centralize router creation logic. This enables clean separation between the server and router implementations, replacing the current enum-based approach.\n\n## Motivation\nCurrent issues:\n- Router is an enum with all logic embedded in match statements\n- Server code directly depends on specific router implementations\n- Difficult to extend with new router types\n- No unified interface for routing operations\n- Policy creation logic scattered\n- No clear initialization pipeline\n\n## Implementation Plan\n\n### 1. Define Router Trait\n```rust\n// src/router/mod.rs\nuse crate::core::Worker;\nuse crate::openai_api_types::{ChatCompletionRequest, CompletionRequest};\nuse actix_web::{HttpRequest, HttpResponse};\n\n#[async_trait]\npub trait Router: Send + Sync {\n    /// Route a chat completion request\n    async fn route_chat_completion(\n        &self,\n        req: HttpRequest,\n        body: ChatCompletionRequest,\n    ) -> HttpResponse;\n    \n    /// Route a text completion request  \n    async fn route_completion(\n        &self,\n        req: HttpRequest,\n        body: CompletionRequest,\n    ) -> HttpResponse;\n    \n    /// Route a generate request (SGLang specific)\n    async fn route_generate(\n        &self,\n        req: HttpRequest,\n        body: serde_json::Value,\n    ) -> HttpResponse;\n    \n    /// Add a worker dynamically\n    async fn add_worker(&self, worker: Arc<dyn Worker>) -> Result<(), RouterError>;\n    \n    /// Remove a worker by URL\n    async fn remove_worker(&self, url: &str) -> Result<(), RouterError>;\n    \n    /// List current worker URLs\n    async fn list_workers(&self) -> Vec<WorkerInfo>;\n    \n    /// Get current load information\n    async fn get_loads(&self) -> LoadInfo;\n    \n    /// Get router type for metrics/debugging\n    fn router_type(&self) -> &'static str;\n    \n    /// Apply service discovery update\n    fn apply_discovery_update(&self, update: DiscoveryUpdate);\n}\n\n#[derive(Debug, Serialize)]\npub struct WorkerInfo {\n    pub url: String,\n    pub worker_type: WorkerType,\n    pub healthy: bool,\n    pub load: usize,\n}\n\n#[derive(Debug, Serialize)]\npub struct LoadInfo {\n    pub router_type: String,\n    pub total_workers: usize,\n    pub healthy_workers: usize,\n    pub total_load: usize,\n    pub worker_loads: Vec<(String, usize)>,\n}\n```\n\n### 2. Create Router Factory\n```rust\n// src/router/factory.rs\nuse crate::config::{RouterConfig, PolicyConfig};\nuse crate::routing::{Router, RegularRouter, PdRouter};\nuse crate::routing::policies::{PolicyFactory, RoutingPolicy};\n\npub struct RouterFactory {\n    http_client: reqwest::Client,\n    worker_factory: WorkerFactory,\n}\n\nimpl RouterFactory {\n    pub fn new() -> Self {\n        Self {\n            http_client: reqwest::Client::builder()\n                .timeout(Duration::from_secs(600))\n                .pool_max_idle_per_host(100)\n                .build()\n                .expect(\"Failed to create HTTP client\"),\n            worker_factory: WorkerFactory::new(),\n        }\n    }\n    \n    pub async fn create_router(\n        &self,\n        config: &RouterConfig,\n    ) -> Result<Arc<dyn Router>, RouterError> {\n        // Create routing policy\n        let policy = PolicyFactory::create(&config.policy)?;\n        \n        // Create router based on mode\n        match &config.mode {\n            RoutingMode::Regular { worker_urls } => {\n                self.create_regular_router(worker_urls, policy).await\n            }\n            RoutingMode::PrefillDecode { prefill_urls, decode_urls } => {\n                self.create_pd_router(prefill_urls, decode_urls, policy).await\n            }\n        }\n    }\n    \n    async fn create_regular_router(\n        &self,\n        worker_urls: &[String],\n        policy: Arc<dyn RoutingPolicy>,\n    ) -> Result<Arc<dyn Router>, RouterError> {\n        // Create workers with health checking\n        let mut workers = Vec::new();\n        for url in worker_urls {\n            let worker = self.worker_factory.create_regular(url.clone());\n            \n            // Initial health check with timeout\n            match timeout(Duration::from_secs(30), worker.check_health()).await {\n                Ok(Ok(())) => {\n                    info!(\"Worker {} is healthy\", url);\n                    workers.push(worker);\n                }\n                _ => {\n                    warn!(\"Worker {} failed initial health check\", url);\n                    if workers.is_empty() && url == worker_urls.last().unwrap() {\n                        return Err(RouterError::NoHealthyWorkers);\n                    }\n                }\n            }\n        }\n        \n        Ok(Arc::new(RegularRouter::new(\n            workers,\n            policy,\n            self.http_client.clone(),\n        )))\n    }\n    \n    async fn create_pd_router(\n        &self,\n        prefill_urls: &[(String, Option<u16>)],\n        decode_urls: &[String],\n        policy: Arc<dyn RoutingPolicy>,\n    ) -> Result<Arc<dyn Router>, RouterError> {\n        // Create prefill workers\n        let mut prefill_workers = Vec::new();\n        for (url, bootstrap_port) in prefill_urls {\n            let worker = self.worker_factory.create_prefill(url.clone(), *bootstrap_port);\n            if worker.check_health().await.is_ok() {\n                prefill_workers.push(worker);\n            }\n        }\n        \n        // Create decode workers\n        let mut decode_workers = Vec::new();\n        for url in decode_urls {\n            let worker = self.worker_factory.create_decode(url.clone());\n            if worker.check_health().await.is_ok() {\n                decode_workers.push(worker);\n            }\n        }\n        \n        if prefill_workers.is_empty() || decode_workers.is_empty() {\n            return Err(RouterError::NoHealthyWorkers);\n        }\n        \n        Ok(Arc::new(PdRouter::new(\n            prefill_workers,\n            decode_workers,\n            policy,\n            self.http_client.clone(),\n        )))\n    }\n}\n```\n\n### 3. Implement Router Trait for RegularRouter\n```rust\n// src/router/router.rs\npub struct RegularRouter {\n    workers: Arc<RwLock<Vec<Arc<dyn Worker>>>>,\n    policy: Arc<dyn RoutingPolicy>,\n    http_client: reqwest::Client,\n}\n\nimpl RegularRouter {\n    pub fn new(\n        workers: Vec<Arc<dyn Worker>>,\n        policy: Arc<dyn RoutingPolicy>,\n        http_client: reqwest::Client,\n    ) -> Self {\n        Self {\n            workers: Arc::new(RwLock::new(workers)),\n            policy,\n            http_client,\n        }\n    }\n}\n\n#[async_trait]\nimpl Router for RegularRouter {\n    async fn route_chat_completion(\n        &self,\n        req: HttpRequest,\n        body: ChatCompletionRequest,\n    ) -> HttpResponse {\n        let start = Instant::now();\n        let route = \"/v1/chat/completions\";\n        \n        // Convert to JSON for policy selection\n        let json_body = serde_json::to_value(&body).unwrap();\n        \n        // Select worker using policy\n        let worker = {\n            let workers = self.workers.read().await;\n            match self.policy.select_single(&workers, &json_body).await {\n                Ok(w) => w,\n                Err(e) => {\n                    RouterMetrics::record_routing_error(route, &e.to_string());\n                    return HttpResponse::ServiceUnavailable()\n                        .json(json!({ \"error\": e.to_string() }));\n                }\n            }\n        };\n        \n        // Update load\n        worker.load().fetch_add(1, Ordering::Relaxed);\n        RouterMetrics::set_worker_load(worker.url(), worker.load().load(Ordering::Relaxed));\n        \n        // Forward request\n        let response = self.forward_request(req, json_body, worker.url(), route).await;\n        \n        // Update load and metrics\n        worker.load().fetch_sub(1, Ordering::Relaxed);\n        RouterMetrics::set_worker_load(worker.url(), worker.load().load(Ordering::Relaxed));\n        RouterMetrics::record_request_duration(route, start.elapsed());\n        \n        response\n    }\n    \n    async fn route_completion(\n        &self,\n        req: HttpRequest,\n        body: CompletionRequest,\n    ) -> HttpResponse {\n        // Similar implementation for text completions\n        let json_body = serde_json::to_value(&body).unwrap();\n        self.route_internal(req, json_body, \"/v1/completions\").await\n    }\n    \n    async fn route_generate(\n        &self,\n        req: HttpRequest,\n        body: serde_json::Value,\n    ) -> HttpResponse {\n        self.route_internal(req, body, \"/generate\").await\n    }\n    \n    async fn add_worker(&self, worker: Arc<dyn Worker>) -> Result<(), RouterError> {\n        if worker.worker_type() != WorkerType::Regular {\n            return Err(RouterError::InvalidWorkerType);\n        }\n        \n        let mut workers = self.workers.write().await;\n        workers.push(worker);\n        Ok(())\n    }\n    \n    async fn remove_worker(&self, url: &str) -> Result<(), RouterError> {\n        let mut workers = self.workers.write().await;\n        let initial_len = workers.len();\n        workers.retain(|w| w.url() != url);\n        \n        if workers.len() == initial_len {\n            Err(RouterError::WorkerNotFound)\n        } else {\n            Ok(())\n        }\n    }\n    \n    fn router_type(&self) -> &'static str {\n        \"regular\"\n    }\n}\n```\n\n### 4. Implement Router Trait for PdRouter\n```rust\n// src/router/pd_router.rs\npub struct PdRouter {\n    prefill_workers: Arc<RwLock<Vec<Arc<dyn Worker>>>>,\n    decode_workers: Arc<RwLock<Vec<Arc<dyn Worker>>>>,\n    policy: Arc<dyn RoutingPolicy>,\n    http_client: reqwest::Client,\n}\n\n#[async_trait]\nimpl Router for PdRouter {\n    async fn route_chat_completion(\n        &self,\n        req: HttpRequest,\n        body: ChatCompletionRequest,\n    ) -> HttpResponse {\n        let route = \"/v1/chat/completions\";\n        let mut json_body = serde_json::to_value(&body).unwrap();\n        \n        // Select workers using policy\n        let (prefill_worker, decode_worker) = {\n            let prefill = self.prefill_workers.read().await;\n            let decode = self.decode_workers.read().await;\n            match self.policy.select_pair(&prefill, &decode, &json_body).await {\n                Ok((p, d)) => (p, d),\n                Err(e) => {\n                    return HttpResponse::ServiceUnavailable()\n                        .json(json!({ \"error\": e.to_string() }));\n                }\n            }\n        };\n        \n        // Inject bootstrap content if needed\n        if let WorkerType::Prefill { bootstrap_port: Some(port) } = prefill_worker.worker_type() {\n            self.inject_bootstrap_content(&mut json_body, prefill_worker.url(), port).await;\n        }\n        \n        // Forward to both workers\n        let prefill_future = self.forward_request(&req, &json_body, prefill_worker.url(), route);\n        let decode_future = self.forward_request(&req, &json_body, decode_worker.url(), route);\n        \n        // Wait for both responses\n        let (prefill_result, decode_result) = join!(prefill_future, decode_future);\n        \n        // Merge responses (especially for logprobs)\n        self.merge_responses(prefill_result, decode_result).await\n    }\n    \n    async fn route_completion(\n        &self,\n        req: HttpRequest,\n        body: CompletionRequest,\n    ) -> HttpResponse {\n        // Similar implementation\n        let json_body = serde_json::to_value(&body).unwrap();\n        self.route_pd_internal(req, json_body, \"/v1/completions\").await\n    }\n    \n    async fn route_generate(\n        &self,\n        req: HttpRequest,\n        body: serde_json::Value,\n    ) -> HttpResponse {\n        self.route_pd_internal(req, body, \"/generate\").await\n    }\n    \n    async fn add_worker(&self, worker: Arc<dyn Worker>) -> Result<(), RouterError> {\n        match worker.worker_type() {\n            WorkerType::Prefill { .. } => {\n                let mut workers = self.prefill_workers.write().await;\n                workers.push(worker);\n                Ok(())\n            }\n            WorkerType::Decode => {\n                let mut workers = self.decode_workers.write().await;\n                workers.push(worker);\n                Ok(())\n            }\n            _ => Err(RouterError::InvalidWorkerType),\n        }\n    }\n    \n    fn router_type(&self) -> &'static str {\n        \"prefill_decode\"\n    }\n}\n```\n\n### 5. Update Server Handlers to Use Router Interface\n```rust\n// src/server.rs\n\n/// Updated handler that uses the Router trait\npub async fn chat_completions_handler(\n    req: HttpRequest,\n    body: web::Json<ChatCompletionRequest>,\n    data: web::Data<AppState>,\n) -> HttpResponse {\n    // The server no longer knows which router implementation is being used\n    data.router.route_chat_completion(req, body.into_inner()).await\n}\n\npub async fn completions_handler(\n    req: HttpRequest,\n    body: web::Json<CompletionRequest>,\n    data: web::Data<AppState>,\n) -> HttpResponse {\n    data.router.route_completion(req, body.into_inner()).await\n}\n\npub async fn generate_handler(\n    req: HttpRequest,\n    body: web::Json<serde_json::Value>,\n    data: web::Data<AppState>,\n) -> HttpResponse {\n    data.router.route_generate(req, body.into_inner()).await\n}\n\npub async fn get_loads_handler(\n    data: web::Data<AppState>,\n) -> HttpResponse {\n    let load_info = data.router.get_loads().await;\n    HttpResponse::Ok().json(load_info)\n}\n```\n\n### 6. Update Server Initialization\n```rust\n// src/server.rs\npub async fn startup(config: RouterConfig) -> Result<(), ServerError> {\n    // Initialize observability\n    init_observability(config.observability.clone())?;\n    \n    // Create router\n    let router_factory = RouterFactory::new();\n    let router = router_factory.create_router(&config).await?;\n    \n    // Setup service discovery if enabled\n    let discovery_handle = if let Some(discovery_config) = config.discovery {\n        let manager = ServiceDiscoveryManager::new(\n            discovery_config,\n            Arc::new(move |update| router.apply_discovery_update(update)),\n        ).await?;\n        Some(manager.start().await)\n    } else {\n        None\n    };\n    \n    // Create app state\n    let app_state = AppState {\n        router,\n        config: config.clone(),\n    };\n    \n    // Start HTTP server\n    HttpServer::new(move || {\n        App::new()\n            .app_data(web::Data::new(app_state.clone()))\n            .wrap(metrics_middleware)\n            .configure(configure_routes)\n    })\n    .bind((config.host, config.port))?\n    .run()\n    .await\n}\n```\n\n## Benefits\n\n1. **Separation of Concerns**: Server no longer knows about router internals\n2. **Extensibility**: Easy to add new router types without changing server code\n3. **Type Safety**: Each endpoint gets proper request types\n4. **Testability**: Can mock Router trait for server testing\n5. **Future Ready**: Supports the long-term vision of different router modes\n\n## Acceptance Criteria\n\n1. **Router Trait**\n   - [ ] Trait defined with methods for each endpoint type\n   - [ ] Separate methods for chat completion, completion, and generate\n   - [ ] Clear async boundaries\n   - [ ] Management methods (add/remove worker, get loads)\n\n2. **Router Factory**\n   - [ ] Factory creates both regular and PD router types\n   - [ ] Health checking during creation\n   - [ ] Policy integration based on config\n   - [ ] Proper error handling for invalid configurations\n\n3. **Router Implementations**\n   - [ ] RegularRouter implements all Router trait methods\n   - [ ] PdRouter implements all Router trait methods\n   - [ ] Each router handles requests differently based on its logic\n   - [ ] Worker management working for both types\n\n4. **Server Integration**\n   - [ ] Server handlers use Router trait methods\n   - [ ] Server has no direct dependency on router implementations\n   - [ ] AppState contains `router: Arc<dyn Router>`\n   - [ ] Management endpoints work through trait\n\n5. **Testing**\n   - [ ] Unit tests for factory\n   - [ ] Mock implementations of Router trait for testing\n   - [ ] Integration tests for both router types\n   - [ ] No regression in functionality\n\n## Dependencies\n- Task 001: Worker Abstraction\n- Task 002: RoutingPolicy Trait\n- Task 003: Migrate Policies\n\n## Estimated Effort\n- Implementation: 3 days\n- Refactoring: 2 days\n- Testing: 2 days\n- Total: 7 days\n\n## Risks\n- **Risk**: Breaking existing router behavior\n  - **Mitigation**: Extensive testing, gradual rollout\n- **Risk**: Performance regression\n  - **Mitigation**: Benchmark before/after, optimize hot paths",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-25T20:22:21+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7538/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7538"
  },
  {
    "number": 8055,
    "title": "[Feature]  support truncate_dim param of  emdding model",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nwhen i use SentenceTransformer i can  truncate the dim of emdding model output use the param \"truncate_dim\", i hope  sglang can support , thank you\n\n### Related resources\n\n_No response_",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-15T09:14:07+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/8055/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/8055"
  },
  {
    "number": 7482,
    "title": "[Bug] Some FP8 models fail to load",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nSglang crashes when loading some FP8 models. I only got Qwen3 FP8 model work, but not others from RedHatAI:\nWORKS:\nhttps://huggingface.co/Qwen/Qwen3-32B-FP8\n\nDO NOT WORK\nhttps://huggingface.co/RedHatAI/Mistral-Small-3.1-24B-Instruct-2503-FP8-dynamic\nhttps://huggingface.co/RedHatAI/Qwen3-32B-FP8-dynamic\nhttps://huggingface.co/RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic\nhttps://huggingface.co/RedHatAI/gemma-3-27b-it-FP8-dynamic\n\n### Reproduction\n\n```\npython3.12 -m venv .venv\nsource .venv/bin/activate.fish\npython -m pip install --upgrade pip\npython -m pip install uv\npython -m uv pip install \"sglang[all]>=0.4.7.post1\"\npython -m uv pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\npython -m uv pip install --force-reinstall sgl_kernel-0.1.9+cu128-cp39-abi3-manylinux2014_x86_64.whl\npython -m uv pip install -U nvidia-nccl-cu12\n```\n\nWORKS with Qwen3-4B normal fp16 -tp 1\n`CUDA_VISIBLE_DEVICES=2 python -m sglang.launch_server --model-path /mnt/llms/models/Qwen/Qwen3-4B --tp 1 --context-length 2048 --port 5001 --host 0.0.0.0`\n\nWORKS with Qwen3-4B-FP8 -tp 1 (I have to add SGL_ENABLE_JIT_DEEPGEMM=0)\n`SGL_ENABLE_JIT_DEEPGEMM=0 CUDA_VISIBLE_DEVICES=2 python -m sglang.launch_server --model-path /mnt/llms/models/Qwen/Qwen3-4B-FP8/ --tp 1 --context-length 2048 --port 5001 --host 0.0.0.0`\n\nDOES NOT Work with RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic\n`SGL_ENABLE_JIT_DEEPGEMM=0 CUDA_VISIBLE_DEVICES=2 python -m sglang.launch_server --model-path /mnt/llms/models/RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic/ --tp 1 --context-length 2048 --port 5001 --host 0.0.0.0`\n\n### Environment\n\n```\n python3 -m sglang.check_env\nPython: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,3,5: NVIDIA GeForce RTX 3090\nGPU 2,4: NVIDIA GeForce RTX 5090\nGPU 0,1,3,5 Compute Capability: 8.6\nGPU 2,4 Compute Capability: 12.0\nCUDA_HOME: /usr/local/cuda-12.8\nNVCC: Cuda compilation tools, release 12.8, V12.8.93\nCUDA Driver Version: 575.57.08\nPyTorch: 2.7.1+cu128\nsglang: 0.4.7.post1\nsgl_kernel: 0.1.9\nflashinfer_python: 0.2.6.post1\ntriton: 3.3.1\ntransformers: 4.52.3\ntorchao: 0.9.0\nnumpy: 2.3.1\naiohttp: 3.12.13\nfastapi: 0.115.13\nhf_transfer: 0.1.9\nhuggingface_hub: 0.33.0\ninteregular: 0.3.3\nmodelscope: 1.27.1\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.7\npython-multipart: 0.0.20\npyzmq: 27.0.0\nuvicorn: 0.34.3\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.19\nopenai: 1.91.0\ntiktoken: 0.9.0\nanthropic: 0.55.0\nlitellm: 1.73.0\ndecord: 0.6.0\nNVIDIA Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NODE    NODE    NODE    NODE    NODE    0-47    0               N/A\nGPU1    NODE     X      NODE    NODE    NODE    NODE    0-47    0               N/A\nGPU2    NODE    NODE     X      PHB     NODE    NODE    0-47    0               N/A\nGPU3    NODE    NODE    PHB      X      NODE    NODE    0-47    0               N/A\nGPU4    NODE    NODE    NODE    NODE     X      PHB     0-47    0               N/A\nGPU5    NODE    NODE    NODE    NODE    PHB      X      0-47    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nulimit soft: 1024\n```",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-23T20:04:33+00:00",
    "closed_at": null,
    "comments": 16,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7482/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7482"
  },
  {
    "number": 8109,
    "title": "[Bug] docker image has lots of CVE issues",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI use https://github.com/aquasecurity/trivy to scan, find lots of CVE issues.\n\nFull log is as \n\n[log.txt](https://github.com/user-attachments/files/21277986/log.txt)\n\n### Reproduction\n\ntrivy image lmsysorg/sglang:v0.4.8.post1-cu126 --scanners vuln\n\n### Environment\n\nN",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-17T03:32:19+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/8109/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/8109"
  },
  {
    "number": 7919,
    "title": "[Bug] install sglang by pip install sglang[all]>=0.4.9, work well when run llama3 model, but raise \"most likely due to a circular import\" error when check ColumnParallelLinear op..",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [ ] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [ ] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\n>>> from sglang.srt.layers.linear import ColumnParallelLinear\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/layers/linear.py\", line 30, in <module>\n    from sglang.srt.layers.quantization.base_config import (\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/layers/quantization/__init__.py\", line 52, in <module>\n    from sglang.srt.layers.linear import LinearBase, UnquantizedLinearMethod\nImportError: cannot import name 'LinearBase' from partially initialized module 'sglang.srt.layers.linear' (most likely due to a circular import) (/usr/local/lib/python3.10/dist-packages/sglang/srt/layers/linear.py)\n\n### Reproduction\n\ndockerfile:\n\n# install torch.\nENV TORCH_CUDA_ARCH_LIST=\"8.0\"\nRUN set -x \\\n&& pip install torch \\\n&& echo \"end\"\n\n# install flashinfer.\nRUN set -x \\\n&& git clone -b v0.2.7.post1 --recursive --depth=1 https://github.com/flashinfer-ai/flashinfer.git /usr/local/flashinfer \\\n&& pip install /usr/local/flashinfer --no-build-isolation --verbose \\\n&& echo \"end\"\n\n# install sglang.\nRUN set -x \\\n&& pip install sglang[all]>=0.4.9 \\\n&& echo \"end\"\n\nthen run docker.\n\nthen test with:\n[root@e13066a1a980 ~]$ python\nPython 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from sglang.srt.layers.linear import ColumnParallelLinear\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/layers/linear.py\", line 30, in <module>\n    from sglang.srt.layers.quantization.base_config import (\n  File \"/usr/local/lib/python3.10/dist-packages/sglang/srt/layers/quantization/__init__.py\", line 52, in <module>\n    from sglang.srt.layers.linear import LinearBase, UnquantizedLinearMethod\nImportError: cannot import name 'LinearBase' from partially initialized module 'sglang.srt.layers.linear' (most likely due to a circular import) (/usr/local/lib/python3.10/dist-packages/sglang/srt/layers/linear.py)\n>>> \n\n\n### Environment\n\ncan reproduced in this docker image: lmsysorg/sglang:v0.4.8.post1-cu126",
    "labels": [],
    "state": "closed",
    "created_at": "2025-07-10T05:57:35+00:00",
    "closed_at": "2025-07-17T07:47:09+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7919/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7919"
  },
  {
    "number": 7571,
    "title": "[Bug] Deploying DeepSeek V3 0324 has blocking decode phase requests by prefill phase ones",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI am running the original fp8 DeepSeek V3 0324 model in production, on a Nvidia 8xB200 node, and unfortunately my throughput is not ideal because often times new scheduled prefill requests block other requests in decoding phase. \n\nI have tried to change the chunked prefill size from 4096 up to 100000, without much avail (large chunked prefill size lead often times to OOM crashes).\n\nAny suggestion on how to resolve/minimize this issue ?\n\nAny help on how to solve this issue is much appreciated\n\n### Reproduction\n\nI am using the common docker compose yaml template, with entrypoint command:\n\n```yaml\nentrypoint: python3 -m sglang.launch_server\ncommand: >\n      --model-path ${SGLANG_MODEL_PATH}\n      --tp 8\n      --trust-remote-code\n      --speculative-algorithm EAGLE\n      --speculative-num-steps 2\n      --speculative-eagle-topk 1\n      --speculative-num-draft-tokens 4\n      --cuda-graph-bs 1 2 4 8 16 32 40 48 56 64 128 256 512\n      --max-running-requests 512\n      --enable-torch-compile\n      --enable-metrics\n      --host 0.0.0.0\n      --port 3000\n```\n\n### Environment\n\n```\nPython: 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3,4,5,6,7: NVIDIA B200\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 10.0\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.8, V12.8.93\nCUDA Driver Version: 575.64\nPyTorch: 2.7.1+cu128\nsglang: 0.4.7.post1\nsgl_kernel: 0.1.9\nflashinfer_python: 0.2.6.post1+cu128torch2.7\ntriton: 3.3.1\ntransformers: 4.52.3\ntorchao: 0.9.0\nnumpy: 2.2.6\naiohttp: 3.12.13\nfastapi: 0.115.13\nhf_transfer: 0.1.9\nhuggingface_hub: 0.33.0\ninteregular: 0.3.3\nmodelscope: 1.27.1\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.7\npython-multipart: 0.0.20\npyzmq: 27.0.0\nuvicorn: 0.34.3\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.19\nopenai: Module Not Found\ntiktoken: 0.9.0\nanthropic: Module Not Found\nlitellm: Module Not Found\ndecord: Module Not Found\nNVIDIA Topology: \n\tGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNODE\tNODE\t0-55,112-167\t0\t\tN/A\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNODE\tNODE\t0-55,112-167\t0\t\tN/A\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNODE\tNODE\t0-55,112-167\t0\t\tN/A\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNODE\tNODE\t0-55,112-167\t0\t\tN/A\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tSYS\tSYS\t56-111,168-223\t1\t\tN/A\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tSYS\tSYS\t56-111,168-223\t1\t\tN/A\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tSYS\tSYS\t56-111,168-223\t1\t\tN/A\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tSYS\tSYS\t56-111,168-223\t1\t\tN/A\nNIC0\tNODE\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t X \tPIX\t\t\t\t\nNIC1\tNODE\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tPIX\t X \t\t\t\t\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n\n\nulimit soft: 1048576\n```",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-26T15:47:55+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7571/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7571"
  },
  {
    "number": 7582,
    "title": "Assessment of the difficulty in porting CPU architecture for sglang",
    "body": "The RISC-V ecosystem is maturing rapidly, with an increasing number of software undergoing migration to RISC-V. We developed a tool named RAX to assess the porting complexity of projects migrating to the RISC-V architecture. RAX evaluates complexity by incorporating Cyclomatic Complexity alongside multiple architecture-specific factors, such as the proportion of assembly code instructions and the frequency of inline function usage. The tool classifies the assessment results into three levels: Low, Middle, and High. For example, the simple project is libtool, the medium project is mesa, and the difficult project is gcc.\nYour project sglang is very well-known. Our tool RAX evaluates that the complexity of your project is low. Could you please confirm the assessment accuracy?\nMore details detected by our tool RAX are shown: the cyclic complexity is 24117. If you want to learn more, please don\u2019t hesitate to us. Thank you.\n",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-27T02:03:04+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7582/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7582"
  },
  {
    "number": 8084,
    "title": "[Feature] Add chat method support to the offline Engine class",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\n`vLLM` now supports both the `generate` and `chat` methods for offline inference. The `generate` method offers more flexibility, while the `chat` method provides a more convenient interface for conversational use cases.\n\nCurrently, `SGLang` supports the `generate` method. Are there any plans to add support for the `chat` method as well? This feature would be helpful for users who want a streamlined conversational interface similar to that provided by vLLM.\n\n### Related resources\n\n[vLLM.LLM.chat](https://docs.vllm.ai/en/stable/api/vllm/index.html#vllm.LLM.chat)",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-16T07:37:45+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/8084/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/8084"
  },
  {
    "number": 7650,
    "title": "[Bug] sglang v0.4.8 use remote model required boto3 package",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nsglang supported  remote model from AWS S3, but when I run docker images: v0.4.8-cu126, raise error:\n```[2025-06-29 23:45:28 TP7] Pulling model configs from remote...\n[2025-06-29 23:45:28 TP7] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 2631, in run_scheduler_process\n    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, pp_rank, dp_rank)\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 293, in __init__\n    self.init_tokenizer()\n  File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 522, in init_tokenizer\n    self.model_config = ModelConfig.from_server_args(server_args)\n  File \"/sgl-workspace/sglang/python/sglang/srt/configs/model_config.py\", line 257, in from_server_args\n    return ModelConfig(\n  File \"/sgl-workspace/sglang/python/sglang/srt/configs/model_config.py\", line 71, in __init__\n    self.maybe_pull_model_tokenizer_from_remote()\n  File \"/sgl-workspace/sglang/python/sglang/srt/configs/model_config.py\", line 462, in maybe_pull_model_tokenizer_from_remote\n    client = create_remote_connector(self.model_path)\n  File \"/sgl-workspace/sglang/python/sglang/srt/connector/__init__.py\", line 28, in create_remote_connector\n    return S3Connector(url)\n  File \"/sgl-workspace/sglang/python/sglang/srt/connector/s3.py\", line 72, in __init__\n    import boto3\nModuleNotFoundError: No module named 'boto3'\n\nException ignored in: <function BaseConnector.__del__ at 0x7ab346b7f130>\nTraceback (most recent call last):\n  File \"/sgl-workspace/sglang/python/sglang/srt/connector/base_connector.py\", line 64, in __del__\n    self.close()\n  File \"/sgl-workspace/sglang/python/sglang/srt/connector/s3.py\", line 121, in close\n    self.client.close()\n[2025-06-29 23:45:28] Received sigquit from a child process. It usually means the child failed.\nAttributeError: 'S3Connector' object has no attribute 'client'\n```\n\n### Reproduction\n\n```\ndocker run -it --rm --shm-size 32g --ipc=host --network=host --gpus all -p 40000:40000 \\\n    --name sglang_multinode1 \\\n\t-e AWS_DEFAULT_REGION=cn-northwest-1 \\\n    lmsysorg/sglang:v0.4.8-cu126 \\\n    python3 -m sglang.launch_server --model-path s3://bucket/Qwen/Qwen3-235B-A22B-GPTQ-Int4/ --tp 16 --dist-init-addr 192.168.xxx.9xx:20000 --nnodes 4 --node-rank 0 --load-format remote --served-model-name Qwen3-235B-A22B --chat-template qwen3_nonthinking.jinja --trust-remote-code --host 0.0.0.0 --port 40000\n```\n\n### Environment\n\n```\nPython: 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]\nCUDA available: True\nGPU 0,1,2,3: NVIDIA A10G\nGPU 0,1,2,3 Compute Capability: 8.6\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.6, V12.6.68\nCUDA Driver Version: 570.133.20\nPyTorch: 2.7.1+cu126\nsglang: 0.4.8\nsgl_kernel: 0.1.9\nflashinfer_python: 0.2.6.post1\ntriton: 3.3.1\ntransformers: 4.52.3\ntorchao: 0.9.0+cu126\nnumpy: 2.2.6\naiohttp: 3.12.13\nfastapi: 0.115.13\nhf_transfer: 0.1.9\nhuggingface_hub: 0.33.0\ninteregular: 0.3.3\nmodelscope: 1.27.1\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.7\npython-multipart: 0.0.20\npyzmq: 27.0.0\nuvicorn: 0.34.3\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.19\nopenai: 1.91.0\ntiktoken: 0.9.0\nanthropic: 0.55.0\nlitellm: 1.73.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PHB     PHB     PHB     0-47    0               N/A\nGPU1    PHB      X      PHB     PHB     0-47    0               N/A\nGPU2    PHB     PHB      X      PHB     0-47    0               N/A\nGPU3    PHB     PHB     PHB      X      0-47    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nHypervisor vendor: KVM\nulimit soft: 1048576\n```",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-30T07:14:29+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7650/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7650"
  },
  {
    "number": 7926,
    "title": "Error when starting minicpm4",
    "body": "I'm running on H20 and sglang is installed through these command: \n`git clone -b openbmb https://github.com/OpenBMB/sglang.git\ncd sglang\n\npip install --upgrade pip\npip install -e \"python[all]\"`\n\nwhen I execute this command I encountered the below error\n\n`python -m sglang.launch_server --model-path OpenBMB/MiniCPM4-8B/ --trust-remote-code --port 30000 --chat-template chatml`\n\n`  File \"sglang/python/sglang/srt/models/minicpm.py\", line 145, in __init__\n    self.rotary_emb.cos_sin_cache = self.rotary_emb._compute_cos_sin_cache()\nTypeError: Phi3LongRoPEScaledRotaryEmbedding._compute_cos_sin_cache() missing 3 required positional arguments: 'max_position_embeddings', 'rescale_factors', and 'mscale'`\n\nDid anyone deal with this error before?",
    "labels": [],
    "state": "closed",
    "created_at": "2025-07-10T09:26:49+00:00",
    "closed_at": "2025-07-15T07:12:47+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7926/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7926"
  },
  {
    "number": 7535,
    "title": "Task 002: Introduce RoutingPolicy Trait",
    "body": "# Task 002: Introduce RoutingPolicy Trait\n\n## Summary\nCreate a unified RoutingPolicy trait that enables all routing algorithms (Random, RoundRobin, CacheAware, PowerOfTwo) to work seamlessly in both regular and PD routing modes, eliminating code duplication.\n\n## Problem Statement\nThe current routing implementation has several issues:\n- Routing policies are duplicated between regular and PD routers\n- PowerOfTwo policy only exists in PD mode, but could benefit regular routing\n- CacheAware logic is copy-pasted with slight variations\n- Adding new routing policies requires modifying router internals\n- No clear interface or contract for routing algorithms\n\n## Proposed Solution\n\n### 1. RoutingPolicy Trait\nDefine a trait that all routing policies must implement:\n\n```rust\n// src/routing/policies/mod.rs\n#[async_trait]\npub trait RoutingPolicy: Send + Sync {\n    /// Select a single worker for regular routing\n    async fn select_single(\n        &self,\n        workers: &[Arc<dyn Worker>],\n        request: &serde_json::Value,\n    ) -> Result<Arc<dyn Worker>, RoutingError>;\n    \n    /// Select prefill and decode workers for PD routing\n    async fn select_pair(\n        &self,\n        prefill_workers: &[Arc<dyn Worker>],\n        decode_workers: &[Arc<dyn Worker>],\n        request: &serde_json::Value,\n    ) -> Result<(Arc<dyn Worker>, Arc<dyn Worker>), RoutingError>;\n    \n    /// Notify policy of request completion (for stateful policies)\n    fn on_request_complete(&self, worker_url: &str, success: bool);\n    \n    /// Get policy name for metrics and debugging\n    fn name(&self) -> &'static str;\n}\n```\n\n### 2. Common Policy Behaviors\nExtract common functionality into helper traits:\n\n```rust\npub trait LoadBalancing {\n    fn select_least_loaded(&self, workers: &[Arc<dyn Worker>]) -> Option<Arc<dyn Worker>> {\n        workers.iter()\n            .filter(|w| w.is_healthy())\n            .min_by_key(|w| w.load().load(Ordering::Relaxed))\n            .cloned()\n    }\n    \n    fn get_healthy_workers(&self, workers: &[Arc<dyn Worker>]) -> Vec<Arc<dyn Worker>> {\n        workers.iter()\n            .filter(|w| w.is_healthy())\n            .cloned()\n            .collect()\n    }\n}\n```\n\n### 3. Policy Factory\nCreate policies based on configuration:\n\n```rust\npub struct PolicyFactory;\n\nimpl PolicyFactory {\n    pub fn create(config: &PolicyConfig) -> Result<Arc<dyn RoutingPolicy>, PolicyError> {\n        match config {\n            PolicyConfig::Random => Ok(Arc::new(RandomPolicy::new())),\n            PolicyConfig::RoundRobin => Ok(Arc::new(RoundRobinPolicy::new())),\n            PolicyConfig::CacheAware { .. } => Ok(Arc::new(CacheAwarePolicy::new(config)?)),\n            PolicyConfig::PowerOfTwo { .. } => Ok(Arc::new(PowerOfTwoPolicy::new(config)?)),\n        }\n    }\n}\n```\n\n## Implementation Plan\n\n### Step 1: Create Policy Module Structure\n- Create `src/rout/policies/mod.rs` with trait definition\n- Create individual policy files for each implementation\n- Define common error types and helper functions\n\n### Step 2: Implement Random Policy\n- Simplest policy to validate the trait design\n- Use thread-safe random number generation\n- Implement both single and pair selection\n\n### Step 3: Implement RoundRobin Policy\n- Maintain separate counters for each worker pool\n- Ensure atomic counter updates\n- Handle wraparound correctly\n\n### Step 4: Implement CacheAware Policy\n- Port existing cache-aware logic\n- Ensure tree management is thread-safe\n- Implement load balancing fallback\n\n### Step 5: Implement PowerOfTwo Policy\n- Make it work for regular routing (not just PD)\n- Sample two workers and pick the least loaded\n- Add proper metrics tracking\n\n### Step 6: Update Routers\n- Modify routers to use RoutingPolicy trait\n- Remove duplicated policy logic\n- Ensure backward compatibility\n\n## Benefits\n\n1. **Code Reuse**: All policies work in both routing modes\n2. **Extensibility**: New policies can be added without touching routers\n3. **Consistency**: Single implementation for each algorithm\n4. **Testing**: Policies can be tested in isolation\n5. **Feature Parity**: PowerOfTwo becomes available for regular routing\n\n## Example Implementation\n\n```rust\n// Random Policy\npub struct RandomPolicy {\n    rng: Mutex<rand::rngs::ThreadRng>,\n}\n\n#[async_trait]\nimpl RoutingPolicy for RandomPolicy {\n    async fn select_single(\n        &self,\n        workers: &[Arc<dyn Worker>],\n        _request: &serde_json::Value,\n    ) -> Result<Arc<dyn Worker>, RoutingError> {\n        let healthy: Vec<_> = workers.iter()\n            .filter(|w| w.is_healthy())\n            .collect();\n            \n        if healthy.is_empty() {\n            return Err(RoutingError::NoHealthyWorkers);\n        }\n        \n        let idx = self.rng.lock().unwrap().gen_range(0..healthy.len());\n        Ok(healthy[idx].clone())\n    }\n    \n    async fn select_pair(\n        &self,\n        prefill: &[Arc<dyn Worker>],\n        decode: &[Arc<dyn Worker>],\n        request: &serde_json::Value,\n    ) -> Result<(Arc<dyn Worker>, Arc<dyn Worker>), RoutingError> {\n        let p = self.select_single(prefill, request).await?;\n        let d = self.select_single(decode, request).await?;\n        Ok((p, d))\n    }\n    \n    fn on_request_complete(&self, _: &str, _: bool) {}\n    \n    fn name(&self) -> &'static str { \"random\" }\n}\n```\n\n## Testing Plan\n\n1. **Unit Tests**:\n   - Test each policy in isolation\n   - Verify distribution properties\n   - Test error handling\n\n2. **Integration Tests**:\n   - Test policy switching\n   - Verify metrics are recorded\n   - Test with both routing modes\n\n3. **Load Tests**:\n   - Ensure policies perform under load\n   - Verify fair distribution\n   - Check for memory leaks\n\n## Acceptance Criteria\n\n- [ ] RoutingPolicy trait defined and documented\n- [ ] All four policies implemented using the trait\n- [ ] Policies work correctly in both routing modes\n- [ ] No code duplication between policies\n- [ ] Router code simplified by removing embedded policy logic\n- [ ] All existing tests pass\n- [ ] Performance metrics show no regression\n- [ ] New policy can be added without modifying routers\n\n## Estimated Effort\n- Design and trait definition: 1 day\n- Policy implementations: 3 days\n- Router integration: 2 days\n- Testing: 1 day\n- Total: 7 days\n\n## Dependencies\n- Task 001: Worker Abstraction (policies operate on Workers)\n\n## Risks and Mitigations\n\n1. **Risk**: Policy behavior changes during refactoring\n   - **Mitigation**: Comprehensive tests before refactoring\n   - **Mitigation**: A/B testing in staging environment\n\n2. **Risk**: Performance overhead from trait dispatch\n   - **Mitigation**: Benchmark each policy implementation\n   - **Mitigation**: Use static dispatch where possible\n\n3. **Risk**: Thread safety issues\n   - **Mitigation**: Careful use of synchronization primitives\n   - **Mitigation**: Stress test under high concurrency",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-25T20:15:05+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7535/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7535"
  },
  {
    "number": 7692,
    "title": "[Bug] [ROCm] GSM8K accuracy issue when using DeekSeek R1 + DP",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nGot dp accuracy issue when testing gsm8k benchmark.\n\nGSM8K benchmark (enable DP at lmsysorg/sglang:v0.4.8-rocm630)\nAccuracy: 0.755\nInvalid: 0.008\nLatency: 130.073 s\nOutput throughput: 1437.044 token/s\n\nGSM8K benchmark (enable DP at lmsysorg/sglang:v0.4.8.post1-rocm630-srt)\nAccuracy: 0.002\nInvalid: 0.970\nLatency: 150.454 s\nOutput throughput: 4475.018 token/s\n\nGSM8K benchmark (disable DP)\nAccuracy: 0.953\nInvalid: 0.000\nLatency: 65.424 s\nOutput throughput: 1964.484 token/s\n\n### Reproduction\n\ndocker image: lmsysorg/sglang:v0.4.8-rocm630 or lmsysorg/sglang:v0.4.8.post1-rocm630-srt\n\nserver:\n```\nSGLANG_USE_AITER=1 python3 -m sglang.launch_server --model-path /apps/data/models/DSR1 --attention-backend aiter --trust-remote-code --disable-custom-all-reduce --tp-size 8 --dp-size 8 --enable-dp-attention --chunked-prefill-size 131072\n```\n\nclient:\n```\npython3 benchmark/gsm8k/bench_sglang.py --parallel 1400 --num-questions 1400\n```\n\n### Environment\n\n```\nroot@smc300x-ccs-aus-gpuf268:/workspace# python3 -m sglang.check_env\nPython: 3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0]\nROCM available: True\nGPU 0,1,2,3,4,5,6,7: AMD Radeon Graphics\nGPU 0,1,2,3,4,5,6,7 Compute Capability: 9.4\nROCM_HOME: /opt/rocm\nHIPCC: HIP version: 6.3.42131-fa1d09cbd\nROCM Driver Version: 6.10.5\nPyTorch: 2.6.0a0+git8d4926e\nsglang: 0.4.8\nsgl_kernel: 0.2.0\nflashinfer_python: Module Not Found\ntriton: 3.2.0+gitcddf0fc3\ntransformers: 4.52.3\ntorchao: 0.9.0\nnumpy: 1.26.4\naiohttp: 3.11.11\nfastapi: 0.115.6\nhf_transfer: 0.1.9\nhuggingface_hub: 0.33.0\ninteregular: 0.3.3\nmodelscope: 1.27.1\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 24.2\npsutil: 6.1.1\npydantic: 2.10.5\npython-multipart: 0.0.20\npyzmq: 26.2.0\nuvicorn: 0.34.0\nuvloop: 0.21.0\nvllm: 0.6.7.dev2+g113274a0.rocm630\nxgrammar: 0.1.19\nopenai: 1.91.0\ntiktoken: 0.7.0\nanthropic: 0.55.0\nlitellm: 1.73.0\ndecord: 0.6.0\nAMD Topology:\n\n\n============================ ROCm System Management Interface ============================\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI\nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI\nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI\nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI\nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI\nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0\n================================== End of ROCm SMI Log ===================================\n\nulimit soft: 1048576\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2025-07-01T10:05:04+00:00",
    "closed_at": "2025-07-09T16:52:13+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7692/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7692"
  },
  {
    "number": 7894,
    "title": "For PD, Variables in multi thread environment NOT BEING PROTECTED",
    "body": "file location: disaggregation/mooncake/conn.py\nhi, i have successfully reproduce the blog. But when i carefully read the code. I found that in mooncake/conn.py many variables which may be read/write by muti threads have not been protected by threading.Lock(). LIke the variable \"transfer_infos\".\nI just want to confirm whether this would become a problem which can bring unexpected behavior and would be fixed in the future.\nThank u!",
    "labels": [],
    "state": "closed",
    "created_at": "2025-07-09T07:31:11+00:00",
    "closed_at": "2025-07-09T07:31:37+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7894/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7894"
  },
  {
    "number": 8069,
    "title": "[Feature] Enable extracting hidden states from intermediate layers",
    "body": "### Checklist\n\n- [x] 1. If the issue you raised is not a feature but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 2. Please use English, otherwise it will be closed.\n\n### Motivation\n\nThis [PR #3364](https://github.com/sgl-project/sglang/pull/3364) implemented `return_hidden_states` argument which makes the results contain the last layer hidden states in `output[\"meta_info\"][\"hidden_states\"]`. In certain domains, extracting the hidden states from intermediate layers of the LLM can also be very useful, see [Layer by Layer: Uncovering Hidden Representations in Language Models](https://arxiv.org/abs/2502.02013).\n\nThe only alternative to doing this right now is Hugging Face, but implementing this in SGLang's engine could provide a large performance boost to this type of workload.\n\n### Related resources\n\n```\nimport unittest, torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nLAYER_IDX = 0\n\nclass TestHiddenState(unittest.TestCase):\n    def test_return_hidden_states(self):\n        prompts = [\"Today is\", \"Today is a sunny day and I like\"]\n        model_path = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        input_ids = tokenizer(prompts).input_ids\n\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path, torch_dtype=torch.bfloat16, device_map=\"cuda\"\n        )\n\n        for input_id, output in zip(input_ids, outputs):\n            with torch.inference_mode():\n                hf_out = model(\n                    torch.tensor(\n                        [input_id + output[\"token_ids\"][:-1]], device=model.device\n                    ),\n                    output_hidden_states=True,\n                )\n            print(\"=== HF Hiddens ===\")\n            print(hf_out[\"hidden_states\"][-1][LAYER_IDX])\n```",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-15T21:06:59+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/8069/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/8069"
  },
  {
    "number": 7807,
    "title": "[Bug] Cannot batch generate or n>1 in sampling_params when input_embeds enabled",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [ ] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nI tried to get multi generation outputs via a single post to the server, but I find that if input_embeds is used, the generation will fail and the error message is weired.\n\nIn the [document](https://docs.sglang.ai/backend/sampling_params.html), it is stated that `input_embeds` can be a `List[List[List[float]]]` item, but it turns out that any 3d tensor causes a 400 response:`The engine initialized with skip_tokenizer_init=True cannot accept text prompts. Please provide input_ids or re-initialize the engine with skip_tokenizer_init=False.`, even with shape (1, 1, dim). There are no text prompts at all.\n\nAlso if `n` in the sampling_params > 1, it causes a 400 response.\n\n### Reproduction\n\nsglang==0.4.9\na single l40 gpu\n\nfirst run server:\n```\npython -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct --port 30000 --disable-radix --dtype bfloat16 --skip-tokenizer-init\n```\n\nthen run:\n```python\nimport torch\nimport requests\n\nif __name__ == '__main__':\n    payload = {\n        \"model\": \"qwen/qwen2.5-0.5b-instruct\",\n        \"input_embeds\": torch.zeros((1, 896)).tolist(),\n        \"sampling_params\": {\n            \"max_new_tokens\": 100,\n            \"temperature\": 0,\n            \"n\": 1,\n        }\n    }\n    response = requests.post(\n        \"http://localhost:30000/generate\",\n        json=payload,\n    )\n    # expect: {'output_ids': [11, 20396, 128547, ...], ...}\n    print(response.json())\n    \n    payload = {\n        \"model\": \"qwen/qwen2.5-0.5b-instruct\",\n        \"input_embeds\": torch.zeros((1, 896)).tolist(),\n        \"sampling_params\": {\n            \"max_new_tokens\": 100,\n            \"temperature\": 0,\n            \"n\": 2,\n        }\n    }\n    response = requests.post(\n        \"http://localhost:30000/generate\",\n        json=payload,\n    )\n    # expect: {'error': {'message': 'The engine initialized with skip_tokenizer_init=True cannot accept text prompts. Please provide input_ids or re-initialize the engine with skip_tokenizer_init=False.'}}\n    print(response.json())\n    \n    payload = {\n        \"model\": \"qwen/qwen2.5-0.5b-instruct\",\n        \"input_embeds\": torch.zeros((1, 1, 896)).tolist(),\n        \"sampling_params\": {\n            \"max_new_tokens\": 100,\n            \"temperature\": 0,\n            \"n\": 1,\n        }\n    }\n    response = requests.post(\n        \"http://localhost:30000/generate\",\n        json=payload,\n    )\n    # expect: {'error': {'message': 'The engine initialized with skip_tokenizer_init=True cannot accept text prompts. Please provide input_ids or re-initialize the engine with skip_tokenizer_init=False.'}}\n    print(response.json())\n```\n\n### Environment\n\nPython: 3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]\nCUDA available: True\nGPU 0: NVIDIA L40\nGPU 0 Compute Capability: 8.9\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 12.3, V12.3.107\nCUDA Driver Version: 550.144.03\nPyTorch: 2.7.1+cu126\nsglang: 0.4.9\nsgl_kernel: 0.2.4\nflashinfer_python: 0.2.7.post1\ntriton: 3.3.1\ntransformers: 4.53.0\ntorchao: 0.9.0\nnumpy: 2.2.6\naiohttp: 3.12.13\nfastapi: 0.115.14\nhf_transfer: 0.1.9\nhuggingface_hub: 0.33.2\ninteregular: 0.3.3\nmodelscope: 1.27.1\norjson: 3.10.18\noutlines: 0.1.11\npackaging: 25.0\npsutil: 7.0.0\npydantic: 2.11.7\npython-multipart: 0.0.20\npyzmq: 27.0.0\nuvicorn: 0.35.0\nuvloop: 0.21.0\nvllm: Module Not Found\nxgrammar: 0.1.19\nopenai: 1.93.0\ntiktoken: 0.9.0\nanthropic: 0.57.1\nlitellm: 1.74.0\ndecord: 0.6.0\nNVIDIA Topology: \n        GPU0    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     32-63,96-127    1               N/A\nNIC0    SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_15\n\n\nulimit soft: 1000000",
    "labels": [],
    "state": "closed",
    "created_at": "2025-07-06T13:42:01+00:00",
    "closed_at": "2025-07-08T21:00:43+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7807/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7807"
  },
  {
    "number": 7674,
    "title": "[About Fused MoE impl] Fused MoE support for EP",
    "body": "Hi dear authors, by reading the source code and running some tests, we find that Sglang implements Fused MoE kernels for non-EP scenarios only.  When EP is enabled, the class EPMoE and DeepEPMoE seem not like Fused MoE implementation. Is there any performance concern on this?\nIt seems that vLLM and TensorRT-LLM have implemented Fused MoE kernels for both EP and non-EP. ",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-01T03:47:37+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7674/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7674"
  },
  {
    "number": 7574,
    "title": "Gemma3n Usage",
    "body": "~~Due to some compatible issues, we need to manually install the latest version of transformers and timm by:\n`pip install -U transformers timm`.~~\n\nThe latest SGLang version 0.4.8.post1 could not work with gemma3n, and was fixed in latest main.\nTo solve the issue, please **install from source** by: (please remove `uv` if you don't use)\n\n```\ngit clone https://github.com/sgl-project/sglang.git\ncd sglang\nuv pip install -e \"python[all]\"\n```\n\nLaunch the server with:\n`python -m sglang.launch_server --model-path google/gemma-3n-E4B-it --attention-backend fa3`\n\nIf you encounter any issue when running the gemma3n, welcome to comment under this issue.\n\nKnown issues:\n1. `TypeError: unsupported operand type(s) for %: 'list' and 'int'` : Please follow above instruction",
    "labels": [
      "high priority"
    ],
    "state": "open",
    "created_at": "2025-06-26T21:14:01+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7574/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/7574"
  },
  {
    "number": 8087,
    "title": "[Bug] Kimi K2 function call failed if set \"strict\": true in tool calls definition.",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [ ] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nKimi K2 function call failed if set \"strict\": true in tool calls definition.\n\n```\n[2025-07-16 14:47:06] [2025-07-15 23:47:06] INFO:     59.82.59.90:0 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\n[2025-07-16 14:47:06] [2025-07-15 23:47:06] Error in request: \n[2025-07-16 14:47:06] Traceback (most recent call last):\n[2025-07-16 14:47:06]   File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/openai/serving_base.py\", line 35, in handle_request\n[2025-07-16 14:47:06]     adapted_request, processed_request = self._convert_to_internal_request(\n[2025-07-16 14:47:06]   File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/openai/serving_chat.py\", line 66, in _convert_to_internal_request\n[2025-07-16 14:47:06]     processed_messages = self._process_messages(request, is_multimodal)\n[2025-07-16 14:47:06]   File \"/sgl-workspace/sglang/python/sglang/srt/entrypoints/openai/serving_chat.py\", line 125, in _process_messages\n[2025-07-16 14:47:06]     tool_call_constraint = parser.get_structure_constraint(request.tool_choice)\n[2025-07-16 14:47:06]   File \"/sgl-workspace/sglang/python/sglang/srt/function_call/function_call_parser.py\", line 162, in get_structure_constraint\n[2025-07-16 14:47:06]     strict_tag = self.get_structure_tag()\n[2025-07-16 14:47:06]   File \"/sgl-workspace/sglang/python/sglang/srt/function_call/function_call_parser.py\", line 116, in get_structure_tag\n[2025-07-16 14:47:06]     get_structure_info = self.detector.structure_info()\n[2025-07-16 14:47:06]   File \"/sgl-workspace/sglang/python/sglang/srt/function_call/kimik2_detector.py\", line 217, in structure_info\n[2025-07-16 14:47:06]     raise NotImplementedError()\n[2025-07-16 14:47:06] NotImplementedError\n```\n\n### Reproduction\n\nRequest Body:\n```Json\n{\n  \"model\": \"Kimi-K2-Instruct\",\n  \"messages\": [\n    { \"role\": \"system\", \"content\": \"\u4f60\u662f\u4e00\u4e2a\u5584\u4e8e\u4f7f\u7528\u8054\u7f51\u641c\u7d22\u5de5\u5177\u7684AI\u52a9\u624b\u3002\" },\n    {\n      \"role\": \"user\",\n      \"content\": \"<goal>\\n\u4f60\u662f\u4e00\u4e2a\u5584\u4e8e\u4f7f\u7528\u8054\u7f51\u641c\u7d22\u5de5\u5177\u7684AI\u52a9\u624b\uff0c\u4f60\u9700\u8981\u6839\u636e\u7528\u6237\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u8f6e\u641c\u7d22\u6765\u6536\u96c6\u8db3\u591f\u7684\u4fe1\u606f\uff0c\u5e76\u6700\u7ec8\u751f\u6210\u4e00\u4efd\u8be6\u5c3d\u7684\u603b\u7ed3\uff0c\u8981\u6c42\u4fdd\u7559\u6570\u636e\u7ec6\u8282\uff0c\u4fbf\u4e8e\u540e\u7eed\u8fdb\u4e00\u6b65\u5206\u6790\u3002\\n</goal>\\n\\n\\n<context>\\n\\n<\u7528\u6237\u7ed9\u5b9a\u95ee\u9898>\\n\u5728\u56fd\u9645\u8303\u56f4\u5185\uff0c\u6309\u6700\u65b0\u53ef\u5f97\u76842024\u5e74\u672b\uff08\u62162025\u5e74\u521d\uff09\u603b\u8d44\u4ea7\u6392\u540d\uff0c\u524d\u5341\u7684\u4fdd\u9669\u516c\u53f8\u5206\u522b\u662f\u54ea\u4e9b\uff1f\u6bcf\u5bb6\u5bf9\u5e94\u7684\u8d44\u4ea7\u603b\u989d\u5404\u662f\u591a\u5c11\uff1f\\n\\n\\n</context>\\n\\n\\n<rules>\\n1.  **\u8bc4\u4f30\u4e0e\u601d\u8003**: \u4e25\u683c\u8bc4\u4f30\u73b0\u6709\u4fe1\u606f\u662f\u5426\u8db3\u4ee5\u5b8c\u6574\u56de\u7b54\u7528\u6237\u95ee\u9898\u3002\u4f60\u7684\u8bc4\u4f30\u9700\u8981\u9075\u5faa\u4ee5\u4e0b\u6807\u51c6\uff1a\\n    -   **\u5b8c\u6574\u6027**: \u662f\u5426\u8986\u76d6\u4e86\u7528\u6237\u95ee\u9898\u7684\u6240\u6709\u65b9\u9762\uff1f\\n    -   **\u65f6\u6548\u6027**: \u4fe1\u606f\u662f\u5426\u662f\u6700\u8fd1\u7684\uff1f\u6709\u65e0\u66f4\u65b0\u7684\u6570\u636e\uff1f\\n    -   **\u53ef\u9760\u6027**: \u4fe1\u606f\u6765\u6e90\u662f\u5426\u6743\u5a01\u53ef\u4fe1\uff1f\\n    -   **\u4e00\u81f4\u6027**: \u5404\u4fe1\u606f\u6e90\u4e4b\u95f4\u662f\u5426\u5b58\u5728\u77db\u76fe\uff1f\\n    -   **\u6df1\u5ea6**: \u662f\u5426\u5305\u542b\u8db3\u591f\u7684\u4e8b\u5b9e\u3001\u6570\u636e\u548c\u80cc\u666f\u4fe1\u606f\u6765\u652f\u6491\u4e00\u4efd\u8be6\u7ec6\u62a5\u544a\uff1f\\n\\n2.  **\u51b3\u7b56\u4e0e\u884c\u52a8**:\\n    -   **\u82e5\u4fe1\u606f\u8db3\u591f**: \u5219\u505c\u6b62\u641c\u7d22\uff0c\u751f\u6210\u6700\u7ec8\u7684\u603b\u7ed3\uff0c\u62a5\u544a\u683c\u5f0f\u9700\u7b26\u5408<document_structure>\u8981\u6c42\uff0c\u5fc5\u987b\u4fdd\u7559\u8db3\u591f\u7684\u6570\u636e\u7ec6\u8282\u3002\\n    -   **\u82e5\u4fe1\u606f\u4e0d\u8db3**:\\n        a. \u5728\u601d\u8003\u8fc7\u7a0b\u4e2d\u6e05\u6670\u5730\u6307\u51fa\u5f53\u524d\u4fe1\u606f\u7684**\u6838\u5fc3\u7f3a\u53e3**\u662f\u4ec0\u4e48\uff08\u4f8b\u5982\uff1a\u7f3a\u5c11\u6700\u65b0\u6570\u636e\u3001\u67d0\u4e2a\u5b50\u4e3b\u9898\u672a\u8986\u76d6\u3001\u89c2\u70b9\u6709\u51b2\u7a81\u9700\u8981\u9a8c\u8bc1\u7b49\uff09\u3002\\n        b. \u57fa\u4e8e\u8fd9\u4e2a\u7f3a\u53e3\uff0c\u751f\u6210\u4e00\u4e2a**\u65b0\u7684\u3001\u4e0d\u8d85\u8fc720\u5b57\u7b26\u7684\u3001\u6709\u9488\u5bf9\u6027\u7684\u641c\u7d22\u8bcd**\u3002\\n        c. \u8c03\u7528\u641c\u7d22\u5de5\u5177\u3002\\n\\n</rules>\\n\\n\\n<document_structure>\\n1.  \u6807\u9898\uff08# \u7ea7\u522b\uff09\\n2.  \u6458\u8981\uff08## \u7ea7\u522b\uff09\\n    -   \u9996\u5148\u7528\u81ea\u5df1\u8bed\u8a00\u590d\u8ff0\u95ee\u9898\\n    -   \u4e4b\u540e\uff0c\u603b\u7ed3\u5173\u952e\u53d1\u73b0\\n3.  \u6b63\u6587\uff08## \u7ea7\u522b\uff09\\n    -   \u603b\u7ed3\u5df2\u641c\u7d22\u5230\u7684\u5185\u5bb9\uff0c\u4fdd\u7559\u8db3\u591f\u7684\u7ec6\u8282\u6570\u636e\\n4.  \u53c2\u8003\u6765\u6e90\uff08## \u7ea7\u522b\uff09\\n    -   \u4ec5\u5217\u51fa\u6709\u4ef7\u503c\u7684\u53c2\u8003\u7f51\u7ad9\u6e90\uff0c\u5e8f\u53f7\u5f00\u5934\uff0c\u7528\u65b9\u62ec\u53f7\u62ec\u8d77\uff0c\u5e8f\u53f7\u8981\u548c\u6b63\u6587\u4e2d\u5f15\u7528\u4fdd\u6301\u4e00\u81f4\u3002\u4f8b\u5982\uff1a[1] [\u56fd\u5bb6\u7edf\u8ba1\u5c40\u5e74\u5ea6\u516c\u62a5](https://www.stats.gov.cn/tjsj/zxfb/202503/t20250315_1923456.html)\\n</document_structure>\\n\\n\\n\"\n    }\n  ],\n  \"temperature\": 0.7,\n  \"top_p\": 1.0,\n  \"max_tokens\": 512,\n  \"presence_penalty\": 0.0,\n  \"frequency_penalty\": 0.0,\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"common_search\",\n        \"description\": \"\u6807\u51c6\u641c\u7d22\u63a5\u53e3\u63d0\u4f9b\u589e\u5f3a\u7684\u7f51\u7edc\u5f00\u653e\u57df\u7684\u5b9e\u65f6\u641c\u7d22\u80fd\u529b\uff0c\u901a\u8fc7\u5927\u6a21\u578b\u4f18\u5316\u4e0e\u591a\u6570\u636e\u6e90\u878d\u5408\u7684\u6280\u672f\uff0c\u67e5\u8be2\u5e72\u51c0\u3001\u51c6\u786e\u3001\u591a\u6837\u3001\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\u3002\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"query\": {\n              \"type\": \"string\",\n              \"description\": \"\u641c\u7d22\u5173\u952e\u8bcd\uff08\u957f\u5ea6\uff1a>=2 and <=100\uff09\"\n            }\n          },\n          \"required\": [\"query\"],\n          \"additionalProperties\": false\n        },\n        \"strict\": true\n      }\n    }\n  ],\n  \"tool_choice\": \"auto\"\n}\n```\n\n### Environment\n\nSGLang v0.4.9.post2",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-16T08:21:35+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/8087/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/sgl-project/sglang/issues/8087"
  },
  {
    "number": 8017,
    "title": "[Bug] NVTX Tracing Error with DeepSeekV3 on Output_LEN=2 (nsys 2025.3.1.90)",
    "body": "### Checklist\n\n- [x] 1. I have searched related issues but cannot get the expected help.\n- [x] 2. The bug has not been fixed in the latest version.\n- [x] 3. Please note that if the bug-related issue you submitted lacks corresponding environment info and a minimal reproducible demo, it will be challenging for us to reproduce and resolve the issue, reducing the likelihood of receiving feedback.\n- [x] 4. If the issue you raised is not a bug but a question, please raise a discussion at https://github.com/sgl-project/sglang/discussions/new/choose Otherwise, it will be closed.\n- [x] 5. Please use English, otherwise it will be closed.\n\n### Describe the bug\n\nWhen using NVIDIA Tools Extension (NVTX) markers to profile DeepSeekV3 with nsys 2025.3.1.90, the generated trace exhibits incorrect hierarchical nesting levels between NVTX tags when generating multiple output tokens (OUTPUT_LEN=2). However, when output = 1, the nsys file is fine.\n\n### Reproduction\n\n\n1. **Modify DeepSeekV3 Code:**\n\u5728DeepseekV2ForCausalLM   Add NVTX markers to forward\n\n```python\n    @torch.no_grad()\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        forward_batch: ForwardBatch,\n        input_embeds: torch.Tensor = None,\n    ) -> torch.Tensor:\n        model_nvtx_name = \"deepseek_model_prefill\"\n        if forward_batch.forward_mode == ForwardMode.DECODE:\n            model_nvtx_name = \"deepseek_model_decode\"\n        with nvtx.annotate(model_nvtx_name):\n            hidden_states = self.model(input_ids, positions, forward_batch, input_embeds)\n        with nvtx.annotate(\"compute_logits\"):\n            logits = self.logits_processor(\n                input_ids, hidden_states, self.lm_head, forward_batch\n            )\n        return logits\n```\n\n3. **Run Benchmark with OUTPUT_LEN=1 (Working):**\n```bash\nBATCH_SIZE=1\nCUDA_GRAPH_BS=$BATCH_SIZE\nNUM_PROMPTS=20\nINPUT_LEN=1536\nOUTPUT_LEN=2\nTP=2\nnsys profile -t cuda,nvtx --trace-fork-before-exec=true -o sglang_enable --cuda-graph-trace=node \\\npython -m sglang.bench_one_batch \\\n    --model $MODEL_PATH \\\n    --batch-size $BATCH_SIZE \\\n    --num-prompts $NUM_PROMPTS \\\n    --cuda-graph-bs $CUDA_GRAPH_BS \\\n    --input $INPUT_LEN \\\n    --output $OUTPUT_LEN \\\n    --tensor-parallel-size $TP \\\n    --chunked-prefill-size 1 \\\n    --max-total-tokens 47000 \\\n    --trust-remote-code \\\n    --enable-torch-compile \n```\n\nnsys results\uff1a\n<img width=\"2170\" height=\"380\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/af7a95e2-cca2-4a97-ab04-756ae03a4136\" />\n\n<img width=\"2990\" height=\"738\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a4431639-cfff-447d-a6fa-4688a49c3cb0\" />\n\n\n### Environment\n\n   - sglang 0.4.8\n   - Nsys 2025.3.1.90 \n   - CUDA 12.4",
    "labels": [],
    "state": "open",
    "created_at": "2025-07-14T08:17:06+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/8017/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/8017"
  },
  {
    "number": 7659,
    "title": "[question of eagle] why create a new token_to_kv_pool in draft model worker\uff1f",
    "body": "in model_runner.py:\nif is_draft_worker is true,  we resue req_to_token_pool and token_to_kv_pool_allocator with target model workder, but why create a new token_to_kv_pool? ",
    "labels": [],
    "state": "open",
    "created_at": "2025-06-30T15:30:11+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/sgl-project/sglang/issues/7659/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/sgl-project/sglang/issues/7659"
  }
]
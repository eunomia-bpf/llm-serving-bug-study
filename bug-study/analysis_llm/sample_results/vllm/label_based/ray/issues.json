[
  {
    "number": 15569,
    "title": "[Bug]: Vllm 0.8.2 + Ray 2.44 (Ray serve deployment) fallbacks to V0 Engine",
    "body": "### Your current environment\n\n<details>\n\n\n```text\nINFO 03-26 19:23:29 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-6.8.0-1020-gcp-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.5.40\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-40GB\nNvidia driver version: 550.90.07\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               12\nOn-line CPU(s) list:                  0-11\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\nCPU family:                           6\nModel:                                85\nThread(s) per core:                   2\nCore(s) per socket:                   6\nSocket(s):                            1\nStepping:                             7\nBogoMIPS:                             4400.39\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            192 KiB (6 instances)\nL1i cache:                            192 KiB (6 instances)\nL2 cache:                             6 MiB (6 instances)\nL3 cache:                             38.5 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-11\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; Enhanced IBRS\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0+cu124\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0+cu124\n[pip3] transformers==4.50.1\n[pip3] triton==3.2.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.3.0                   pypi_0    pypi\n[conda] torch                     2.6.0+cu124              pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0+cu124             pypi_0    pypi\n[conda] transformers              4.50.1                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-11    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen deploying vllm 0.8.2 with ray 2.44.0 , vllm falls back to V0.\n\nLogs from ray serve deployment:\n\n![Image](https://github.com/user-attachments/assets/1663f5d7-c53d-448e-b778-334a68ad857b)\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "open",
    "created_at": "2025-03-26T19:29:21+00:00",
    "closed_at": null,
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15569/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15569"
  },
  {
    "number": 16259,
    "title": "[Bug]: vLLM still runs after Ray workers crash",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\nINFO 04-08 04:09:19 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 4.0.0\nLibc version: glibc-2.35\n\nPython version: 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-122-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA H100 NVL\nGPU 1: NVIDIA H100 NVL\nGPU 2: NVIDIA H100 NVL\nGPU 3: NVIDIA H100 NVL\n\nNvidia driver version: 555.52.04\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               254\nOn-line CPU(s) list:                  0-253\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 9554 64-Core Processor\nCPU family:                           25\nModel:                                17\nThread(s) per core:                   1\nCore(s) per socket:                   127\nSocket(s):                            2\nStepping:                             1\nBogoMIPS:                             6190.70\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm rep_good nopl cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw perfctr_core invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr wbnoinvd arat npt lbrv nrip_save tsc_scale vmcb_clean flushbyasid pausefilter pfthreshold v_vmsave_vmload vgif avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm flush_l1d arch_capabilities\nVirtualization:                       AMD-V\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            15.9 MiB (254 instances)\nL1i cache:                            15.9 MiB (254 instances)\nL2 cache:                             127 MiB (254 instances)\nL3 cache:                             4 GiB (254 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-126\nNUMA node1 CPU(s):                    127-253\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.1.post2+cu124torch2.6\n[pip3] numpy==2.1.3\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.0\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tNV12\tPHB\tPHB\t0-253\t0-1\t\tN/A\nGPU1\tNV12\t X \tPHB\tPHB\t0-253\t0-1\t\tN/A\nGPU2\tPHB\tPHB\t X \tNV12\t0-253\t0-1\t\tN/A\nGPU3\tPHB\tPHB\tNV12\t X \t0-253\t0-1\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNVIDIA_VISIBLE_DEVICES=all\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\nNCCL_VERSION=2.20.5-1\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nVLLM_WORKER_MULTIPROC_METHOD=spawn\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nCUDA_VERSION=12.4.0\nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nVLLM_NO_USAGE_STATS=1\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n\n</details>\nI came up with some issue regarding 0.8.x in serving some models for instance Llama 3.3 70B , Llama 3.1 405B models, Mixtral-8x7B\n\n### \ud83d\udc1b Describe the bug\n\nIn the instances of 0.8.1, 0.8.2 the issue we're mainly in the CUDA out of memory that I can add the details also in this ticket, the main problem is also in 0.8.3 where is more stable on the CUDA.\n\nI started a model\n\nThe model load properly with no issue, getting to the end to start the FastAPI\n\nAt the first request the Ray workers go down while the FastAPI is still on, handling the requests from users, keeping them in waiting until the request is done, which leaves them hanging.\n\nFull detail output\n```bash\nINFO 04-08 03:47:19 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-08 03:47:24 [api_server.py:1034] vLLM API server version 0.8.3\nINFO 04-08 03:47:24 [api_server.py:1035] args: Namespace(host='0.0.0.0', port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template='/vllm-workspace/examples/tool_chat_template_llama3.1_json.jinja', chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=True, tool_call_parser='llama3_json', tool_parser_plugin='', model='hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend='ray', pipeline_parallel_size=1, tensor_parallel_size=4, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.91, num_gpu_blocks_override=None, max_num_batched_tokens=1024, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=256, max_logprobs=128, disable_log_stats=False, quantization='awq_marlin', rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=True, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='cuda', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=True, speculative_config=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['meta-llama/Meta-Llama-3.1-405B-Instruct-FP8'], qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)\nINFO 04-08 03:47:40 [config.py:600] This model supports multiple tasks: {'reward', 'score', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.\nINFO 04-08 03:47:42 [awq_marlin.py:114] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\nINFO 04-08 03:47:43 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=1024.\nINFO 04-08 03:47:51 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-08 03:47:57 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4', speculative_config=None, tokenizer='hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Meta-Llama-3.1-405B-Instruct-FP8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\nINFO 04-08 03:48:04 [ray_utils.py:335] No current placement group found. Creating a new placement group.\nINFO 04-08 03:48:04 [ray_distributed_executor.py:176] use_ray_spmd_worker: True\n\u001b[36m(pid=692)\u001b[0m INFO 04-08 03:48:10 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-08 03:48:19 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()\nINFO 04-08 03:48:19 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_NO_USAGE_STATS', 'VLLM_USAGE_SOURCE', 'VLLM_USE_RAY_SPMD_WORKER', 'VLLM_USE_RAY_COMPILED_DAG', 'VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USE_V1']\nINFO 04-08 03:48:19 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /root/.config/vllm/ray_non_carry_over_env_vars.json file\n\u001b[36m(RayWorkerWrapper pid=687)\u001b[0m WARNING 04-08 03:48:25 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f65601f9880>\n\u001b[36m(pid=689)\u001b[0m INFO 04-08 03:48:11 [__init__.py:239] Automatically detected platform cuda.\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=687)\u001b[0m INFO 04-08 03:48:27 [utils.py:990] Found nccl from library libnccl.so.2\n\u001b[36m(RayWorkerWrapper pid=687)\u001b[0m INFO 04-08 03:48:27 [pynccl.py:69] vLLM is using nccl==2.21.5\n\u001b[36m(RayWorkerWrapper pid=687)\u001b[0m INFO 04-08 03:48:33 [parallel_state.py:957] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1\n\u001b[36m(RayWorkerWrapper pid=687)\u001b[0m INFO 04-08 03:48:33 [cuda.py:221] Using Flash Attention backend on V1 engine.\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m WARNING 04-08 03:48:25 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1c905d15e0>\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:48:27 [utils.py:990] Found nccl from library libnccl.so.2\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:48:27 [pynccl.py:69] vLLM is using nccl==2.21.5\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:48:33 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_ca37703b'), local_subscribe_addr='ipc:///tmp/002617c9-e3fb-424e-8d18-231c479406ac', remote_subscribe_addr=None, remote_addr_ipv6=False)\n\u001b[36m(RayWorkerWrapper pid=687)\u001b[0m INFO 04-08 03:48:33 [gpu_model_runner.py:1258] Starting to load model hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4...\n\u001b[36m(RayWorkerWrapper pid=687)\u001b[0m INFO 04-08 03:48:34 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.\n\u001b[36m(RayWorkerWrapper pid=693)\u001b[0m INFO 04-08 03:48:34 [weight_utils.py:265] Using model weights format ['*.safetensors']\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:49:31 [loader.py:447] Loading weights took 56.74 seconds\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:48:33 [parallel_state.py:957] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:48:33 [cuda.py:221] Using Flash Attention backend on V1 engine.\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:48:33 [gpu_model_runner.py:1258] Starting to load model hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4...\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:48:34 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=692)\u001b[0m INFO 04-08 03:48:34 [weight_utils.py:265] Using model weights format ['*.safetensors']\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:49:35 [gpu_model_runner.py:1273] Model loading took 51.1453 GiB and 61.854495 seconds\n\u001b[36m(RayWorkerWrapper pid=693)\u001b[0m INFO 04-08 03:49:35 [loader.py:447] Loading weights took 59.74 seconds\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=692)\u001b[0m INFO 04-08 03:49:41 [gpu_model_runner.py:1273] Model loading took 51.1453 GiB and 67.717508 seconds\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=692)\u001b[0m INFO 04-08 03:49:36 [loader.py:447] Loading weights took 61.44 seconds\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:50:26 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/88e04f09c2/rank_0_0 for vLLM's torch.compile\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:50:26 [backends.py:426] Dynamo bytecode transform time: 44.26 s\n\u001b[36m(RayWorkerWrapper pid=692)\u001b[0m INFO 04-08 03:50:35 [backends.py:132] Cache the graph of shape None for later use\n\u001b[36m(RayWorkerWrapper pid=693)\u001b[0m INFO 04-08 03:50:30 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/88e04f09c2/rank_3_0 for vLLM's torch.compile\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=693)\u001b[0m INFO 04-08 03:50:30 [backends.py:426] Dynamo bytecode transform time: 48.64 s\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=693)\u001b[0m INFO 04-08 03:50:41 [backends.py:132] Cache the graph of shape None for later use\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=692)\u001b[0m INFO 04-08 03:53:10 [backends.py:144] Compiling a graph for general shape takes 158.06 s\n\u001b[36m(RayWorkerWrapper pid=687)\u001b[0m INFO 04-08 03:53:16 [backends.py:144] Compiling a graph for general shape takes 161.66 s\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=687)\u001b[0m INFO 04-08 03:54:40 [monitor.py:33] torch.compile takes 208.36 s in total\n\u001b[36m(RayWorkerWrapper pid=693)\u001b[0m INFO 04-08 03:53:18 [backends.py:144] Compiling a graph for general shape takes 161.66 s\nINFO 04-08 03:54:46 [kv_cache_utils.py:578] GPU KV cache size: 132,464 tokens\nINFO 04-08 03:54:46 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 1.01x\nINFO 04-08 03:54:46 [kv_cache_utils.py:578] GPU KV cache size: 132,560 tokens\nINFO 04-08 03:54:46 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 1.01x\nINFO 04-08 03:54:46 [kv_cache_utils.py:578] GPU KV cache size: 132,400 tokens\nINFO 04-08 03:54:46 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 1.01x\nINFO 04-08 03:54:46 [kv_cache_utils.py:578] GPU KV cache size: 132,560 tokens\nINFO 04-08 03:54:46 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 1.01x\n\u001b[36m(RayWorkerWrapper pid=693)\u001b[0m INFO 04-08 03:58:02 [gpu_model_runner.py:1608] Graph capturing finished in 196 secs, took 4.95 GiB\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:54:40 [monitor.py:33] torch.compile takes 202.32 s in total\u001b[32m [repeated 3x across cluster]\u001b[0m\nINFO 04-08 03:58:02 [core.py:162] init engine (profile, create kv cache, warmup model) took 500.69 seconds\nWARNING 04-08 03:58:04 [api_server.py:936] Using supplied chat template: {{- bos_token }}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- if custom_tools is defined %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- set tools = custom_tools %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- if not tools_in_user_message is defined %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {#- Llama 3.1 doesn't pass all tests if the tools are in the system prompt #}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- set tools_in_user_message = true %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- if not date_string is defined %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- if strftime_now is defined %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- set date_string = strftime_now(\"%d %b %Y\") %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- else %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- set date_string = \"26 Jul 2024\" %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- if not tools is defined %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- set tools = none %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] \nWARNING 04-08 03:58:04 [api_server.py:936] {#- This block extracts the system message, so we can slot it into the right place. #}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- if messages[0]['role'] == 'system' %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- if messages[0]['content'] is string %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- set system_message = messages[0]['content']|trim %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- else %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- set system_message = messages[0]['content'][0]['text']|trim %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- set messages = messages[1:] %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- else %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- if tools is not none %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- set system_message = \"You are a helpful assistant with tool calling capabilities. Only reply with a tool call if the function exists in the library provided by the user. If it doesn't exist, just reply directly in natural language. When you receive a tool call response, use the output to format an answer to the original user question.\" %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- else %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- set system_message = \"\" %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] \nWARNING 04-08 03:58:04 [api_server.py:936] {#- System message #}\nWARNING 04-08 03:58:04 [api_server.py:936] {{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- if tools is not none %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- \"Environment: ipython\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] {{- \"Cutting Knowledge Date: December 2023\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936] {{- \"Today Date: \" + date_string + \"\\n\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- if tools is not none and not tools_in_user_message %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call. \" }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. ' }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- \"Do not use variables.\\n\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- for t in tools %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- t | tojson(indent=4) }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- \"\\n\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- endfor %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] {{- system_message }}\nWARNING 04-08 03:58:04 [api_server.py:936] {{- \"<|eot_id|>\" }}\nWARNING 04-08 03:58:04 [api_server.py:936] \nWARNING 04-08 03:58:04 [api_server.py:936] {#- Custom tools are passed in a user message with some extra guidance #}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- if tools_in_user_message and not tools is none %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {#- Extract the first user message so we can plug it in here #}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- if messages | length != 0 %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- if messages[0]['content'] is string %}\nWARNING 04-08 03:58:04 [api_server.py:936]             {%- set first_user_message = messages[0]['content']|trim %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- else %}\nWARNING 04-08 03:58:04 [api_server.py:936]             {%- set first_user_message = messages[0]['content'] | selectattr('type', 'equalto', 'text') | map(attribute='text') | map('trim') | join('\\n') %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- set messages = messages[1:] %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- else %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- \"Given the following functions, please respond with a JSON for a function call \" }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. ' }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- \"Do not use variables.\\n\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- for t in tools %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- t | tojson(indent=4) }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- \"\\n\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- endfor %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- first_user_message + \"<|eot_id|>\"}}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] \nWARNING 04-08 03:58:04 [api_server.py:936] {%- for message in messages %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- if message['content'] is string %}\nWARNING 04-08 03:58:04 [api_server.py:936]             {{- message['content'] | trim}}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- else %}\nWARNING 04-08 03:58:04 [api_server.py:936]             {%- for content in message['content'] %}\nWARNING 04-08 03:58:04 [api_server.py:936]                 {%- if content['type'] == 'text' %}\nWARNING 04-08 03:58:04 [api_server.py:936]                     {{- content['text'] | trim }}\nWARNING 04-08 03:58:04 [api_server.py:936]                 {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936]             {%- endfor %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- '<|eot_id|>' }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- elif 'tool_calls' in message %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- if not message.tool_calls|length == 1 %}\nWARNING 04-08 03:58:04 [api_server.py:936]             {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- set tool_call = message.tool_calls[0].function %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- '{\"name\": \"' + tool_call.name + '\", ' }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- '\"parameters\": ' }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- tool_call.arguments | tojson }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- \"}\" }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- \"<|eot_id|>\" }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- if message.content is string %}\nWARNING 04-08 03:58:04 [api_server.py:936]             {{- { \"output\": message.content } | tojson }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- else %}\nWARNING 04-08 03:58:04 [api_server.py:936]             {%- for content in message['content']  %}\nWARNING 04-08 03:58:04 [api_server.py:936]                 {%- if content['type']  == 'text' %}\nWARNING 04-08 03:58:04 [api_server.py:936]                     {{- { \"output\": content['text']  } | tojson }}\nWARNING 04-08 03:58:04 [api_server.py:936]                 {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936]             {%- endfor %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- \"<|eot_id|>\" }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endfor %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- if add_generation_prompt %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] \nWARNING 04-08 03:58:04 [api_server.py:936] It is different from official chat template 'hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4'. This discrepancy may lead to performance degradation.\nINFO 04-08 03:58:04 [serving_chat.py:79] \"auto\" tool choice has been enabled please note that while the parallel_tool_calls client option is preset for compatibility reasons, it will be ignored.\nINFO 04-08 03:58:04 [api_server.py:1081] Starting vLLM API server on http://0.0.0.0:8000\nINFO 04-08 03:58:04 [launcher.py:26] Available routes are:\nINFO 04-08 03:58:04 [launcher.py:34] Route: /openapi.json, Methods: HEAD, GET\nINFO 04-08 03:58:04 [launcher.py:34] Route: /docs, Methods: HEAD, GET\nINFO 04-08 03:58:04 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: HEAD, GET\nINFO 04-08 03:58:04 [launcher.py:34] Route: /redoc, Methods: HEAD, GET\nINFO 04-08 03:58:04 [launcher.py:34] Route: /health, Methods: GET\nINFO 04-08 03:58:04 [launcher.py:34] Route: /load, Methods: GET\nINFO 04-08 03:58:04 [launcher.py:34] Route: /ping, Methods: POST, GET\nINFO 04-08 03:58:04 [launcher.py:34] Route: /tokenize, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /detokenize, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /v1/models, Methods: GET\nINFO 04-08 03:58:04 [launcher.py:34] Route: /version, Methods: GET\nINFO 04-08 03:58:04 [launcher.py:34] Route: /v1/chat/completions, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /v1/completions, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /v1/embeddings, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /pooling, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /score, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /v1/score, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /rerank, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /v1/rerank, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /v2/rerank, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /invocations, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /metrics, Methods: GET\nINFO:     5:43844 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43852 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43860 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43876 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:53092 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:53104 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40742 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40756 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40760 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40772 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:58:15 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     2:48512 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO:     5:40784 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40786 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40796 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40798 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:58500 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:58512 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43002 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43018 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43028 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43034 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:58:25 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:43036 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43052 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43068 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43080 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:51358 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:51374 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     2:44628 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO:     5:46902 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46906 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46912 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46918 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:58:35 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:46934 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46948 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46956 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46958 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:51312 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:51314 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54152 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54154 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54164 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54170 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:58:45 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     2:53014 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO:     5:54186 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54190 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54206 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54216 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:47228 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:47238 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:53652 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:53658 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:53664 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:53678 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:58:55 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:53690 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:53698 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:53700 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:53712 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:42810 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:42812 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     2:55884 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO:     5:58730 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:58742 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     10.6.11.226:45866 - \"POST /v1/completions HTTP/1.1\" 404 Not Found\nINFO:     5:58748 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:58764 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:59:05 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:58780 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:58790 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:58806 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:58814 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:59214 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:59216 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54056 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54064 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54066 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54072 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:59:15 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     2:52392 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO:     5:54088 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54094 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54100 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54112 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:49976 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:49978 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40568 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40570 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40586 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40598 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:59:25 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:40606 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40610 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40620 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40634 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:35018 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:35034 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     2:45668 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO:     5:56856 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:56870 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:56876 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:56886 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:59:35 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:56890 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:56892 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:56896 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:56904 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:35212 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:35224 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:39002 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:39018 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:39034 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:39050 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:59:45 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     2:35684 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO:     5:39062 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:39076 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:39080 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:39082 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:53498 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:53500 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:49912 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:49922 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:49934 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:49942 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:59:55 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:49944 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:49952 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:49958 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:49960 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:58230 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:58246 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     2:54470 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO 04-08 04:00:01 [logger.py:39] Received request cmpl-17baf80e349d4adfb4e385cf652e427a-0: prompt: 'Do you know the book Traction by Gino Wickman', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=120, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [128000, 5519, 499, 1440, 279, 2363, 350, 16597, 555, 480, 3394, 75206, 1543], lora_request: None, prompt_adapter_request: None.\nINFO:     10.6.11.226:54104 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO 04-08 04:00:01 [async_llm.py:228] Added request cmpl-17baf80e349d4adfb4e385cf652e427a-0.\nINFO 04-08 04:00:01 [ray_distributed_executor.py:561] VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE = auto\nINFO 04-08 04:00:01 [ray_distributed_executor.py:563] VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM = False\nINFO 04-08 04:00:01 [ray_distributed_executor.py:578] RAY_CGRAPH_get_timeout is set to 300\nERROR 04-08 04:00:01 [core.py:390] EngineCore hit an exception: Traceback (most recent call last):\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 383, in run_engine_core\nERROR 04-08 04:00:01 [core.py:390]     engine_core.run_busy_loop()\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 405, in run_busy_loop\nERROR 04-08 04:00:01 [core.py:390]     self._process_engine_step()\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 434, in _process_engine_step\nERROR 04-08 04:00:01 [core.py:390]     outputs = self.step_fn()\nERROR 04-08 04:00:01 [core.py:390]               ^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 206, in step\nERROR 04-08 04:00:01 [core.py:390]     output = self.model_executor.execute_model(scheduler_output)\nERROR 04-08 04:00:01 [core.py:390]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/ray_distributed_executor.py\", line 57, in execute_model\nERROR 04-08 04:00:01 [core.py:390]     return refs[0].get()\nERROR 04-08 04:00:01 [core.py:390]            ^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/compiled_dag_ref.py\", line 154, in get\nERROR 04-08 04:00:01 [core.py:390]     raise execution_error from None\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/compiled_dag_ref.py\", line 145, in get\nERROR 04-08 04:00:01 [core.py:390]     ray.get(actor_execution_loop_refs, timeout=10)\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\nERROR 04-08 04:00:01 [core.py:390]     return fn(*args, **kwargs)\nERROR 04-08 04:00:01 [core.py:390]            ^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\nERROR 04-08 04:00:01 [core.py:390]     return func(*args, **kwargs)\nERROR 04-08 04:00:01 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2771, in get\nERROR 04-08 04:00:01 [core.py:390]     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\nERROR 04-08 04:00:01 [core.py:390]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 919, in get_objects\nERROR 04-08 04:00:01 [core.py:390]     raise value.as_instanceof_cause()\nERROR 04-08 04:00:01 [core.py:390] ray.exceptions.RayTaskError(ModuleNotFoundError): \u001b[36mray::RayWorkerWrapper.__ray_call__()\u001b[39m (pid=687, ip=3, actor_id=c6e851436e36ad12c39ebb1501000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x7f655b7c55b0>)\nERROR 04-08 04:00:01 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/actor.py\", line 1722, in __ray_call__\nERROR 04-08 04:00:01 [core.py:390]     return fn(self, *args, **kwargs)\nERROR 04-08 04:00:01 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 230, in do_exec_tasks\nERROR 04-08 04:00:01 [core.py:390]     done = tasks[operation.exec_task_idx].exec_operation(\nERROR 04-08 04:00:01 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 745, in exec_operation\nERROR 04-08 04:00:01 [core.py:390]     with _device_context_manager():\nERROR 04-08 04:00:01 [core.py:390]          ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 345, in _device_context_manager\nERROR 04-08 04:00:01 [core.py:390]     device = ChannelContext.get_current().torch_device\nERROR 04-08 04:00:01 [core.py:390]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/common.py\", line 173, in torch_device\nERROR 04-08 04:00:01 [core.py:390]     from ray.air._internal import torch_utils\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/air/__init__.py\", line 1, in <module>\nERROR 04-08 04:00:01 [core.py:390]     from ray.air.config import (\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/air/config.py\", line 17, in <module>\nERROR 04-08 04:00:01 [core.py:390]     import pyarrow.fs\nERROR 04-08 04:00:01 [core.py:390] ModuleNotFoundError: No module named 'pyarrow'\nERROR 04-08 04:00:01 [core.py:390] \nINFO 04-08 04:00:01 [ray_distributed_executor.py:127] Shutting down Ray distributed executor. If you see error log from logging.cc regarding SIGTERM received, please ignore because this is the expected termination process in Ray.\nCRITICAL 04-08 04:00:01 [core_client.py:361] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\nINFO:     5:51022 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:51024 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:51028 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:51036 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 04:00:05 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:51046 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:51048 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:51058 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:51070 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:57732 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:57746 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:37712 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:37722 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:37736 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:37738 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 04:00:15 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     2:50184 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO 04-08 04:00:17 [async_llm.py:355] Aborted request cmpl-17baf80e349d4adfb4e385cf652e427a-0. # User aborted\nINFO:     5:37740 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:37756 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:37758 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:37760 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:51030 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:51042 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:52366 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:52376 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:52378 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:52382 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 04:00:25 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:52388 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:52404 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:52408 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:52422 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:57830 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:57832 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     2:36708 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO:     5:57926 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:57930 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:57942 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:57950 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 04:00:35 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:57966 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:57972 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:57978 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:57980 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:35028 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:35034 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 04:00:41 [logger.py:39] Received request cmpl-4e919233f0b34f108bd337ed1fa32cb4-0: prompt: 'Do you know the book Traction by Gino Wickman', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=120, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [128000, 5519, 499, 1440, 279, 2363, 350, 16597, 555, 480, 3394, 75206, 1543], lora_request: None, prompt_adapter_request: None.\nINFO:     10.6.11.226:50254 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO 04-08 04:00:41 [async_llm.py:228] Added request cmpl-4e919233f0b34f108bd337ed1fa32cb4-0.\nINFO:     5:46096 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46098 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46100 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46112 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 04:00:45 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     2:47164 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO:     5:46126 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46142 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46148 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46162 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:33952 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:33968 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:34894 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:34908 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:34920 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:34926 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 04:00:55 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n``` \n\nThe expected outcome is after the workers crash, the whole API should also stop, not being able to handle any requests or some error to get the docker container back starting\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "open",
    "created_at": "2025-04-08T11:23:38+00:00",
    "closed_at": null,
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16259/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16259"
  },
  {
    "number": 16692,
    "title": "[Bug]: Problems with vllm serve DeepSeek-R1 with 2 nodes and TP = 16\uff08include vllm v0.8.4 v0.7.3 v0.7.2 V0 V1 engine\uff09",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n v0.8.4 using TP = 16 to serving deepseek-v3 in 2*H800*8 On Ray cluster, get EngineCore exception\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nstart command:\nhead node:\n```bash\nray start --head --port=6379  && \\\n    vllm serve $MODELPATH \\\n    --max-num-seqs=256 \\\n    --max-model-len=32768 \\\n    --max-num-batched-tokens=32768 \\\n    --tensor-parallel-size 16 \\\n    --enable-expert-parallel \\\n    --enable-prefix-caching \\\n    --enable-chunked-prefill \\\n    --distributed-executor-backend=ray \\\n    --trust-remote-code \\\n    --served-model-name deepseek-r1\n```\nslave node:\n```bash\nray start --block --address=$HEADPODIP:6379\n```\n\nget error:\n```bash\n2025-04-16 10:27:16,259 INFO usage_lib.py:467 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n2025-04-16 10:27:16,259 INFO scripts.py:865 -- Local node IP: 172.20.155.155\n2025-04-16 10:27:19,206 SUCC scripts.py:902 -- --------------------\n2025-04-16 10:27:19,206 SUCC scripts.py:903 -- Ray runtime started.\n2025-04-16 10:27:19,206 SUCC scripts.py:904 -- --------------------\n2025-04-16 10:27:19,206 INFO scripts.py:906 -- Next steps\n2025-04-16 10:27:19,207 INFO scripts.py:909 -- To add another node to this Ray cluster, run\n2025-04-16 10:27:19,207 INFO scripts.py:912 --   ray start --address='172.20.155.155:6379'\n2025-04-16 10:27:19,207 INFO scripts.py:921 -- To connect to this Ray cluster:\n2025-04-16 10:27:19,207 INFO scripts.py:923 -- import ray\n2025-04-16 10:27:19,207 INFO scripts.py:924 -- ray.init()\n2025-04-16 10:27:19,207 INFO scripts.py:955 -- To terminate the Ray runtime, run\n2025-04-16 10:27:19,207 INFO scripts.py:956 --   ray stop\n2025-04-16 10:27:19,207 INFO scripts.py:959 -- To view the status of the cluster, use\n2025-04-16 10:27:19,207 INFO scripts.py:960 --   ray status\nINFO 04-16 10:27:25 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-16 10:27:28 [api_server.py:1034] vLLM API server version 0.8.4\nINFO 04-16 10:27:28 [api_server.py:1035] args: Namespace(subparser='serve', model_tag='/deepseek-r1', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/deepseek-r1', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, load_format='auto', download_dir=None, model_loader_extra_config=None, use_tqdm_on_load=True, config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=32768, guided_decoding_backend='auto', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend='ray', pipeline_parallel_size=1, tensor_parallel_size=16, data_parallel_size=1, enable_expert_parallel=True, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, block_size=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=32768, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=True, speculative_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['deepseek-r1-250120'], qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_chunked_mm_input=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x7f948f036340>)\nINFO 04-16 10:27:28 [config.py:209] Replacing legacy 'type' key with 'rope_type'\nINFO 04-16 10:27:33 [config.py:689] This model supports multiple tasks: {'generate', 'score', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.\nINFO 04-16 10:27:33 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=32768.\nWARNING 04-16 10:27:33 [fp8.py:63] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\nINFO 04-16 10:27:33 [cuda.py:160] Forcing kv cache block size to 64 for FlashMLA backend.\nINFO 04-16 10:27:37 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-16 10:27:38 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='/deepseek-r1', speculative_config=None, tokenizer='/deepseek-r1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=16, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=deepseek-r1-250120, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n2025-04-16 10:27:38,573 INFO worker.py:1654 -- Connecting to existing Ray cluster at address: 172.20.155.155:6379...\n2025-04-16 10:27:38,589 INFO worker.py:1841 -- Connected to Ray cluster.\nINFO 04-16 10:27:41 [ray_utils.py:335] No current placement group found. Creating a new placement group.\nWARNING 04-16 10:27:41 [ray_utils.py:199] tensor_parallel_size=16 is bigger than a reserved number of GPUs (8 GPUs) in a node 8ae1db6046a3cd19fa34db174f8f7f08658824bd4939356d992af164. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 16 GPUs available at each node.\nWARNING 04-16 10:27:41 [ray_utils.py:199] tensor_parallel_size=16 is bigger than a reserved number of GPUs (8 GPUs) in a node e9b799a0bcd5383e697d936e1119a975beac44342e36975145239d86. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 16 GPUs available at each node.\nINFO 04-16 10:27:41 [ray_distributed_executor.py:176] use_ray_spmd_worker: True\n[36m(pid=860)[0m INFO 04-16 10:27:44 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-16 10:27:51 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()\nINFO 04-16 10:27:51 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_USAGE_SOURCE', 'VLLM_USE_RAY_SPMD_WORKER', 'VLLM_USE_RAY_COMPILED_DAG', 'VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USE_V1']\nINFO 04-16 10:27:51 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /root/.config/vllm/ray_non_carry_over_env_vars.json file\n[36m(RayWorkerWrapper pid=860)[0m [W416 10:28:00.809838573 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[36m(RayWorkerWrapper pid=860)[0m [W416 10:28:00.813800637 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[36m(RayWorkerWrapper pid=860)[0m [W416 10:28:00.911790999 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[36m(RayWorkerWrapper pid=860)[0m [W416 10:28:00.046069996 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[36m(RayWorkerWrapper pid=860)[0m [W416 10:28:00.055444791 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[36m(RayWorkerWrapper pid=860)[0m [W416 10:28:00.155493559 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[36m(RayWorkerWrapper pid=860)[0m [W416 10:28:00.173913414 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[36m(RayWorkerWrapper pid=860)[0m [W416 10:28:00.232485006 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:07, 21.28it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:   4% Completed | 6/163 [00:01<00:47,  3.28it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:   6% Completed | 9/163 [00:01<00:28,  5.44it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:   7% Completed | 11/163 [00:03<00:52,  2.88it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:   8% Completed | 13/163 [00:03<00:39,  3.82it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:   9% Completed | 15/163 [00:04<00:44,  3.33it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  10% Completed | 17/163 [00:04<00:33,  4.38it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  12% Completed | 19/163 [00:04<00:40,  3.57it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  13% Completed | 21/163 [00:05<00:30,  4.73it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  14% Completed | 23/163 [00:05<00:37,  3.72it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  15% Completed | 24/163 [00:06<00:48,  2.84it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  15% Completed | 25/163 [00:07<01:00,  2.29it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  16% Completed | 26/163 [00:08<01:10,  1.94it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  17% Completed | 28/163 [00:08<00:45,  2.99it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  18% Completed | 30/163 [00:09<00:47,  2.78it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  19% Completed | 31/163 [00:09<00:58,  2.24it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  21% Completed | 34/163 [00:10<00:35,  3.60it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  21% Completed | 35/163 [00:10<00:47,  2.68it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  22% Completed | 36/163 [00:11<00:58,  2.17it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  23% Completed | 38/163 [00:12<00:55,  2.24it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  24% Completed | 39/163 [00:13<01:04,  1.92it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  25% Completed | 40/163 [00:13<01:03,  1.93it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  25% Completed | 41/163 [00:14<01:02,  1.94it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  26% Completed | 43/163 [00:15<00:50,  2.37it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  28% Completed | 45/163 [00:15<00:33,  3.51it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  28% Completed | 46/163 [00:15<00:37,  3.10it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  30% Completed | 49/163 [00:15<00:21,  5.20it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  32% Completed | 52/163 [00:15<00:14,  7.64it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  33% Completed | 54/163 [00:16<00:18,  6.03it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  35% Completed | 57/163 [00:16<00:12,  8.43it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  37% Completed | 60/163 [00:17<00:14,  7.12it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  38% Completed | 62/163 [00:17<00:17,  5.90it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  39% Completed | 64/163 [00:18<00:19,  5.20it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  40% Completed | 66/163 [00:18<00:20,  4.71it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  42% Completed | 68/163 [00:18<00:15,  5.94it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  44% Completed | 71/163 [00:18<00:11,  8.11it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  45% Completed | 74/163 [00:19<00:12,  7.00it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  47% Completed | 76/163 [00:19<00:15,  5.69it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  48% Completed | 78/163 [00:20<00:12,  7.00it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  49% Completed | 80/163 [00:20<00:14,  5.73it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  50% Completed | 81/163 [00:21<00:18,  4.48it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  52% Completed | 84/163 [00:21<00:16,  4.85it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  52% Completed | 85/163 [00:22<00:19,  4.00it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  54% Completed | 88/163 [00:22<00:12,  6.07it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  55% Completed | 90/163 [00:22<00:13,  5.23it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  56% Completed | 92/163 [00:23<00:15,  4.70it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  58% Completed | 94/163 [00:23<00:15,  4.37it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  59% Completed | 96/163 [00:24<00:15,  4.19it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  61% Completed | 99/163 [00:24<00:10,  6.25it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  62% Completed | 101/163 [00:24<00:11,  5.38it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  64% Completed | 104/163 [00:25<00:10,  5.63it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  66% Completed | 107/163 [00:25<00:07,  7.80it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  68% Completed | 111/163 [00:25<00:04, 11.24it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  70% Completed | 114/163 [00:26<00:06,  7.06it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  71% Completed | 116/163 [00:26<00:07,  6.40it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  72% Completed | 118/163 [00:27<00:09,  4.67it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  73% Completed | 119/163 [00:27<00:10,  4.20it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  75% Completed | 122/163 [00:28<00:08,  4.86it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  75% Completed | 123/163 [00:28<00:09,  4.25it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  77% Completed | 126/163 [00:28<00:05,  6.43it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  79% Completed | 128/163 [00:29<00:05,  5.95it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  81% Completed | 132/163 [00:29<00:04,  6.95it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  83% Completed | 135/163 [00:30<00:04,  6.91it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  83% Completed | 136/163 [00:30<00:04,  5.61it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  85% Completed | 138/163 [00:31<00:05,  4.74it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  86% Completed | 140/163 [00:31<00:04,  4.74it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  87% Completed | 142/163 [00:32<00:04,  4.80it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  88% Completed | 144/163 [00:32<00:03,  4.87it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  90% Completed | 146/163 [00:32<00:03,  4.89it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  91% Completed | 149/163 [00:33<00:02,  5.55it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  92% Completed | 150/163 [00:33<00:02,  4.76it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  94% Completed | 153/163 [00:33<00:01,  7.14it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  95% Completed | 155/163 [00:34<00:01,  6.37it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  98% Completed | 159/163 [00:34<00:00,  9.92it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  99% Completed | 161/163 [00:34<00:00,  7.50it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards: 100% Completed | 163/163 [00:34<00:00,  4.68it/s]\n[36m(RayWorkerWrapper pid=860)[0m \n[36m(RayWorkerWrapper pid=315, ip=172.20.96.190)[0m WARNING 04-16 10:27:57 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f44783d41a0>\n[36m(pid=318, ip=172.20.96.190)[0m INFO 04-16 10:27:48 [__init__.py:239] Automatically detected platform cuda.[32m [repeated 15x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m\n[36m(RayWorkerWrapper pid=863)[0m INFO 04-16 10:28:00 [utils.py:993] Found nccl from library libnccl.so.2\n[36m(RayWorkerWrapper pid=863)[0m INFO 04-16 10:28:00 [pynccl.py:69] vLLM is using nccl==2.21.5\n[36m(RayWorkerWrapper pid=863)[0m WARNING 04-16 10:28:07 [custom_all_reduce.py:84] Custom allreduce is disabled because this process group spans across nodes.\n[36m(RayWorkerWrapper pid=864)[0m WARNING 04-16 10:27:57 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ed4678ba3f0>[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=319, ip=172.20.96.190)[0m INFO 04-16 10:28:00 [utils.py:993] Found nccl from library libnccl.so.2[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=319, ip=172.20.96.190)[0m INFO 04-16 10:28:00 [pynccl.py:69] vLLM is using nccl==2.21.5[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=860)[0m INFO 04-16 10:28:07 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_7c006341'), local_subscribe_addr='ipc:///tmp/484f3a8c-b706-469d-ab5e-25dd3c2d6c05', remote_subscribe_addr='tcp://172.20.155.155:43865', remote_addr_ipv6=False)\n[36m(RayWorkerWrapper pid=861)[0m INFO 04-16 10:28:07 [parallel_state.py:959] rank 5 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 5\n[36m(RayWorkerWrapper pid=314, ip=172.20.96.190)[0m INFO 04-16 10:28:07 [cuda.py:207] Using FlashMLA backend on V1 engine.\n[36m(RayWorkerWrapper pid=865)[0m INFO 04-16 10:28:07 [gpu_model_runner.py:1276] Starting to load model /deepseek-r1...\n[36m(RayWorkerWrapper pid=863)[0m WARNING 04-16 10:28:07 [utils.py:165] The model class DeepseekV3ForCausalLM has not defined `packed_modules_mapping`, this may lead to incorrect mapping of quantized or ignored modules\n[36m(RayWorkerWrapper pid=314, ip=172.20.96.190)[0m INFO 04-16 10:28:08 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.\n[36m(RayWorkerWrapper pid=319, ip=172.20.96.190)[0m INFO 04-16 10:28:32 [loader.py:458] Loading weights took 23.36 seconds\n[36m(RayWorkerWrapper pid=319, ip=172.20.96.190)[0m WARNING 04-16 10:28:07 [custom_all_reduce.py:84] Custom allreduce is disabled because this process group spans across nodes.[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=313, ip=172.20.96.190)[0m INFO 04-16 10:28:07 [parallel_state.py:959] rank 8 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 8[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=865)[0m INFO 04-16 10:28:07 [cuda.py:207] Using FlashMLA backend on V1 engine.[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=319, ip=172.20.96.190)[0m INFO 04-16 10:28:07 [gpu_model_runner.py:1276] Starting to load model /deepseek-r1...[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=319, ip=172.20.96.190)[0m WARNING 04-16 10:28:07 [utils.py:165] The model class DeepseekV3ForCausalLM has not defined `packed_modules_mapping`, this may lead to incorrect mapping of quantized or ignored modules[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=320, ip=172.20.96.190)[0m INFO 04-16 10:28:08 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=319, ip=172.20.96.190)[0m INFO 04-16 10:28:33 [gpu_model_runner.py:1291] Model loading took 40.3174 GiB and 25.028323 seconds\n[36m(RayWorkerWrapper pid=317, ip=172.20.96.190)[0m INFO 04-16 10:28:36 [loader.py:458] Loading weights took 27.77 seconds[32m [repeated 7x across cluster][0m\n[36m(RayWorkerWrapper pid=317, ip=172.20.96.190)[0m INFO 04-16 10:28:37 [gpu_model_runner.py:1291] Model loading took 40.3174 GiB and 29.134500 seconds[32m [repeated 7x across cluster][0m\n[36m(RayWorkerWrapper pid=862)[0m INFO 04-16 10:28:42 [loader.py:458] Loading weights took 33.51 seconds[32m [repeated 4x across cluster][0m\n[36m(RayWorkerWrapper pid=314, ip=172.20.96.190)[0m INFO 04-16 10:28:54 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/1ebcb71993/rank_9_0 for vLLM's torch.compile\n[36m(RayWorkerWrapper pid=314, ip=172.20.96.190)[0m INFO 04-16 10:28:54 [backends.py:426] Dynamo bytecode transform time: 9.92 s\n[36m(RayWorkerWrapper pid=860)[0m INFO 04-16 10:28:44 [gpu_model_runner.py:1291] Model loading took 40.3174 GiB and 36.271878 seconds[32m [repeated 8x across cluster][0m\n[36m(RayWorkerWrapper pid=858)[0m INFO 04-16 10:28:43 [loader.py:458] Loading weights took 34.88 seconds[32m [repeated 4x across cluster][0m\n[36m(RayWorkerWrapper pid=314, ip=172.20.96.190)[0m INFO 04-16 10:28:58 [backends.py:132] Cache the graph of shape None for later use\n[36m(RayWorkerWrapper pid=858)[0m INFO 04-16 10:28:58 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/1ebcb71993/rank_2_0 for vLLM's torch.compile[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=858)[0m INFO 04-16 10:28:58 [backends.py:426] Dynamo bytecode transform time: 13.46 s[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=314, ip=172.20.96.190)[0m INFO 04-16 10:29:35 [backends.py:144] Compiling a graph for general shape takes 40.72 s\n[36m(RayWorkerWrapper pid=858)[0m INFO 04-16 10:29:01 [backends.py:132] Cache the graph of shape None for later use[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=867)[0m INFO 04-16 10:29:42 [backends.py:144] Compiling a graph for general shape takes 45.69 s[32m [repeated 8x across cluster][0m\n[36m(RayWorkerWrapper pid=863)[0m WARNING 04-16 10:29:53 [fp8_utils.py:431] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128,128].json\n[36m(RayWorkerWrapper pid=858)[0m INFO 04-16 10:29:47 [backends.py:144] Compiling a graph for general shape takes 48.56 s[32m [repeated 7x across cluster][0m\n[36m(RayWorkerWrapper pid=320, ip=172.20.96.190)[0m WARNING 04-16 10:29:56 [fused_moe.py:670] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=16,N=2048,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128,128].json\n[36m(RayWorkerWrapper pid=863)[0m INFO 04-16 10:31:13 [monitor.py:33] torch.compile takes 60.35 s in total\n[36m(RayWorkerWrapper pid=858)[0m WARNING 04-16 10:29:53 [fp8_utils.py:431] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128,128].json[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=864)[0m WARNING 04-16 10:29:57 [fused_moe.py:670] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=16,N=2048,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128,128].json[32m [repeated 15x across cluster][0m\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 305,216 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.31x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 304,704 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.30x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 305,216 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.31x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 304,704 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.30x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 305,216 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.31x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 304,704 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.30x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 305,216 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.31x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 304,704 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.30x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 304,704 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.30x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 305,216 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.31x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 304,704 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.30x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 305,216 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.31x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 304,704 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.30x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 305,216 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.31x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 304,704 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.30x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 305,216 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.31x\n[36m(RayWorkerWrapper pid=858)[0m INFO 04-16 10:32:16 [gpu_model_runner.py:1626] Graph capturing finished in 58 secs, took 3.51 GiB\n[36m(RayWorkerWrapper pid=319, ip=172.20.96.190)[0m INFO 04-16 10:31:13 [monitor.py:33] torch.compile takes 54.80 s in total[32m [repeated 15x across cluster][0m\nINFO 04-16 10:32:17 [core.py:163] init engine (profile, create kv cache, warmup model) took 212.66 seconds\nINFO 04-16 10:32:17 [core_client.py:435] Core engine process 0 ready.\nWARNING 04-16 10:32:17 [config.py:1177] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\nINFO 04-16 10:32:17 [serving_chat.py:118] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.95}\nINFO 04-16 10:32:17 [serving_completion.py:61] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.95}\nINFO 04-16 10:32:17 [api_server.py:1081] Starting vLLM API server on http://0.0.0.0:8000\nINFO 04-16 10:32:17 [launcher.py:26] Available routes are:\nINFO 04-16 10:32:17 [launcher.py:34] Route: /openapi.json, Methods: GET, HEAD\nINFO 04-16 10:32:17 [launcher.py:34] Route: /docs, Methods: GET, HEAD\nINFO 04-16 10:32:17 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: GET, HEAD\nINFO 04-16 10:32:17 [launcher.py:34] Route: /redoc, Methods: GET, HEAD\nINFO 04-16 10:32:17 [launcher.py:34] Route: /health, Methods: GET\nINFO 04-16 10:32:17 [launcher.py:34] Route: /load, Methods: GET\nINFO 04-16 10:32:17 [launcher.py:34] Route: /ping, Methods: GET, POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /tokenize, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /detokenize, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /v1/models, Methods: GET\nINFO 04-16 10:32:17 [launcher.py:34] Route: /version, Methods: GET\nINFO 04-16 10:32:17 [launcher.py:34] Route: /v1/chat/completions, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /v1/completions, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /v1/embeddings, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /pooling, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /score, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /v1/score, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /rerank, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /v1/rerank, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /v2/rerank, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /invocations, Methods: POST\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     172.20.235.128:50301 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:32:27 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:58244 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:32:37 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:64024 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:32:47 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:21483 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:32:57 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:63969 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:33:07 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:30409 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:33:17 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:16541 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:33:27 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:57427 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:33:37 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:37520 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:33:47 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:40130 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:33:57 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:29034 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:34:07 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:27650 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:34:17 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:41571 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:34:27 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:14206 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:34:37 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:22656 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:34:47 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:41524 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:34:57 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:25804 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:35:07 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:39412 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:35:13 [chat_utils.py:396] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\nINFO 04-16 10:35:13 [logger.py:39] Received request chatcmpl-8f878979543841479d089e13a1da85f4: prompt: '<\uff5cbegin\u2581of\u2581sentence\uff5c>In every output, response using the following format:\\n<think>\\n{reasoning_content}\\n</think>\\n\\n{content}<\uff5cUser\uff5c>\u8bf7\u8bb2\u4e2a\u7b11\u8bdd\u3002<\uff5cAssistant\uff5c>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=0.95, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\nINFO 04-16 10:35:13 [async_llm.py:228] Added request chatcmpl-8f878979543841479d089e13a1da85f4.\nINFO 04-16 10:35:13 [ray_distributed_executor.py:561] VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE = auto\nINFO 04-16 10:35:13 [ray_distributed_executor.py:563] VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM = False\nINFO 04-16 10:35:13 [ray_distributed_executor.py:578] RAY_CGRAPH_get_timeout is set to 300\nINFO 04-16 10:35:17 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:30878 - \"GET /health HTTP/1.1\" 200 OK\nERROR 04-16 10:35:25 [core.py:387] EngineCore hit an exception: Traceback (most recent call last):\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 2344, in _execute_until\nERROR 04-16 10:35:25 [core.py:387]     result = self._dag_output_fetcher.read(timeout)\nERROR 04-16 10:35:25 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/common.py\", line 318, in read\nERROR 04-16 10:35:25 [core.py:387]     outputs = self._read_list(timeout)\nERROR 04-16 10:35:25 [core.py:387]               ^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/common.py\", line 409, in _read_list\nERROR 04-16 10:35:25 [core.py:387]     raise e\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/common.py\", line 391, in _read_list\nERROR 04-16 10:35:25 [core.py:387]     result = c.read(min(remaining_timeout, iteration_timeout))\nERROR 04-16 10:35:25 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/shared_memory_channel.py\", line 776, in read\nERROR 04-16 10:35:25 [core.py:387]     return self._channel_dict[self._resolve_actor_id()].read(timeout)\nERROR 04-16 10:35:25 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/shared_memory_channel.py\", line 612, in read\nERROR 04-16 10:35:25 [core.py:387]     output = self._buffers[self._next_read_index].read(timeout)\nERROR 04-16 10:35:25 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/shared_memory_channel.py\", line 480, in read\nERROR 04-16 10:35:25 [core.py:387]     ret = self._worker.get_objects(\nERROR 04-16 10:35:25 [core.py:387]           ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 893, in get_objects\nERROR 04-16 10:35:25 [core.py:387]     ] = self.core_worker.get_objects(\nERROR 04-16 10:35:25 [core.py:387]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"python/ray/_raylet.pyx\", line 3189, in ray._raylet.CoreWorker.get_objects\nERROR 04-16 10:35:25 [core.py:387]   File \"python/ray/includes/common.pxi\", line 106, in ray._raylet.check_status\nERROR 04-16 10:35:25 [core.py:387] ray.exceptions.RayChannelTimeoutError: System error: Timed out waiting for object available to read. ObjectID: 0048581a5404469f49cc78ed8c9abf660217eb8e0100000002e1f505\nERROR 04-16 10:35:25 [core.py:387] \nERROR 04-16 10:35:25 [core.py:387] The above exception was the direct cause of the following exception:\nERROR 04-16 10:35:25 [core.py:387] \nERROR 04-16 10:35:25 [core.py:387] Traceback (most recent call last):\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 380, in run_engine_core\nERROR 04-16 10:35:25 [core.py:387]     engine_core.run_busy_loop()\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 402, in run_busy_loop\nERROR 04-16 10:35:25 [core.py:387]     self._process_engine_step()\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 431, in _process_engine_step\nERROR 04-16 10:35:25 [core.py:387]     outputs = self.step_fn()\nERROR 04-16 10:35:25 [core.py:387]               ^^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 207, in step\nERROR 04-16 10:35:25 [core.py:387]     output = self.model_executor.execute_model(scheduler_output)\nERROR 04-16 10:35:25 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/ray_distributed_executor.py\", line 57, in execute_model\nERROR 04-16 10:35:25 [core.py:387]     return refs[0].get()\nERROR 04-16 10:35:25 [core.py:387]            ^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/compiled_dag_ref.py\", line 124, in get\nERROR 04-16 10:35:25 [core.py:387]     self._dag._execute_until(\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 2350, in _execute_until\nERROR 04-16 10:35:25 [core.py:387]     raise RayChannelTimeoutError(\nERROR 04-16 10:35:25 [core.py:387] ray.exceptions.RayChannelTimeoutError: System error: If the execution is expected to take a long time, increase RAY_CGRAPH_get_timeout which is currently 10 seconds. Otherwise, this may indicate that the execution is hanging.\nERROR 04-16 10:35:25 [core.py:387] \nINFO 04-16 10:35:25 [ray_distributed_executor.py:127] Shutting down Ray distributed executor. If you see error log from logging.cc regarding SIGTERM received, please ignore because this is the expected termination process in Ray.\n2025-04-16 10:35:25,965 INFO compiled_dag_node.py:2109 -- Tearing down compiled DAG\nCRITICAL 04-16 10:35:25 [core_client.py:359] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n2025-04-16 10:35:25,967 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, d10fb2b0c74d88d01a04b87201000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 911d26df30515b32be6a85f301000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 8e6d63962d86e65b1ea0f2b101000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 86c7380938683894e3ee26f201000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 49cc78ed8c9abf660217eb8e01000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 05669ac9f7453d130910f44801000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 609209648dd775e40cd891f301000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, daf13a1f09c703c2e0eed52b01000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 8a3d05115b896d5974f1e65501000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, e701d61cb9f15f100029c04c01000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 7317795a95765b6f9eef848201000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, d4d8521d057dd51fb5ba6c8201000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, bad7c2a8367932671a3ce46301000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, b62696b233dff721c33557ff01000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, b4d89cda43028fedc3ea64e401000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 835191b5fcd2ef2cd8a8e38d01000000)\n2025-04-16 10:35:26,014 INFO compiled_dag_node.py:2137 -- Waiting for worker tasks to exit\nINFO 04-16 10:35:27 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n\n```\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "open",
    "created_at": "2025-04-16T02:43:26+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16692/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16692"
  },
  {
    "number": 11260,
    "title": "[Bug]: vLLM on TPU does not support --pipeline-parallel-size with Ray",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.6.0.dev20241126+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.31.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-1013-gcp-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             112\r\nOn-line CPU(s) list:                0-111\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 7B13\r\nCPU family:                         25\r\nModel:                              1\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 56\r\nSocket(s):                          1\r\nStepping:                           0\r\nBogoMIPS:                           4899.99\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip vaes vpclmulqdq rdpid fsrm\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          1.8 MiB (56 instances)\r\nL1i cache:                          1.8 MiB (56 instances)\r\nL2 cache:                           28 MiB (56 instances)\r\nL3 cache:                           224 MiB (7 instances)\r\nNUMA node(s):                       1\r\nNUMA node0 CPU(s):                  0-111\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; safe RET\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.6.0.dev20241126+cpu\r\n[pip3] torch-xla==2.6.0+git39e67b5\r\n[pip3] torchvision==0.20.0.dev20241126+cpu\r\n[pip3] transformers==4.47.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.6.0.dev20241126+cpu          pypi_0    pypi\r\n[conda] torch-xla                 2.6.0+git39e67b5          pypi_0    pypi\r\n[conda] torchvision               0.20.0.dev20241126+cpu          pypi_0    pypi\r\n[conda] transformers              4.47.0                   pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.4.post2.dev378+g69ba344d\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n\r\nLD_LIBRARY_PATH=/home/ext_hzchen_google_com/miniconda3/envs/tpu/lib/python3.10/site-packages/cv2/../../lib64:\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nI setup v5e-32(8 hosts/4 chips each) and started a ray cluster.\r\n\r\noutput of **ray status**\r\n```\r\n======== Autoscaler status: 2024-12-17 11:18:22.317629 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 node_719fd8c930dcd8b932914ebb34d70d16323b468e1f93094a78d50f75\r\n 1 node_ac79abf69175ce6f927b5317fe292d22aea9ac4e3c224a7ba42ca6d3\r\n 1 node_23a15d4de28063c4a04865b963c54c0a8f29d8e928c298e4021a146b\r\n 1 node_153c21365890656c54742efe22e675b89127db7998084e482c8260c6\r\n 1 node_cedcf4265be52d297b3d95bab832675688d8241360c819bd9bd63de7\r\n 1 node_8993f15ab992801a04a70b5b7c4c691158b949bd7da98c35154aa372\r\n 1 node_3723c73cee4bc754265308f5a9f384b493e06b7d76860a739e9ddfb4\r\n 1 node_e2823d348a9370bfe108d635330b339b48e4847ce0608af311cd75e2\r\nPending:\r\n (no pending nodes)\r\nRecent failures:\r\n (no failures)\r\n\r\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 0.0/880.0 CPU\r\n 0.0/32.0 TPU\r\n 0.0/1.0 TPU-v5litepod-32-head\r\n 0B/1.01TiB memory\r\n 0B/449.11GiB object_store_memory\r\n 0.0/8.0 tpuvm-01\r\n\r\nDemands:\r\n (no resource demands)\r\n```\r\n\r\n\r\nI start serving with below command\r\n```\r\nvllm serve mistralai/Pixtral-Large-Instruct-2411 --config-format mistral --load-format mistral --tokenizer-mode mistral --num-scheduler-steps 2 --swap-space 4 --max-model-len=1024 --limit_mm_per_prompt 'image=10' --tensor-parallel-size 4 --pipeline-parallel-size 8 --disable-log-requests --dtype=bfloat16\r\n```\r\n\r\nget below error messages\r\n\r\n```text\r\nINFO 12-17 09:37:22 api_server.py:643] vLLM API server version 0.6.4.post2.dev378+g69ba344d\r\nINFO 12-17 09:37:22 api_server.py:644] args: Namespace(subparser='serve', model_tag='mistralai/Pixtral-Large-Instruct-2411', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='mistralai/Pixtral-Large-Instruct-2411', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='mistral', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='mistral', config_format='mistral', dtype='bfloat16', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=1024, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=8, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt={'image': 10}, mm_processor_kwargs=None, mm_cache_preprocessor=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=2, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7fa9d7e42320>)\r\nINFO 12-17 09:37:24 config.py:1938] Downcasting torch.float32 to torch.bfloat16.\r\nINFO 12-17 09:37:31 config.py:451] This model supports multiple tasks: {'embed', 'reward', 'score', 'classify', 'generate'}. Defaulting to 'generate'.\r\nWARNING 12-17 09:37:31 config.py:569] Async output processing can not be enabled with pipeline parallel\r\n2024-12-17 09:37:31,835\tINFO worker.py:1636 -- Connecting to existing Ray cluster at address: 10.164.0.34:6379...\r\n2024-12-17 09:37:31,898\tINFO worker.py:1812 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\r\nINFO 12-17 09:37:32 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post2.dev378+g69ba344d) with config: model='mistralai/Pixtral-Large-Instruct-2411', speculative_config=None, tokenizer='mistralai/Pixtral-Large-Instruct-2411', skip_tokenizer_init=False, tokenizer_mode=mistral, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=mistral, tensor_parallel_size=4, pipeline_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Pixtral-Large-Instruct-2411, num_scheduler_steps=2, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":2,\"backend\":\"openxla\",\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n\u001b[36m(RayWorkerWrapper pid=150332, ip=10.164.0.35)\u001b[0m WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\r\nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m INFO 12-17 09:40:07 tpu.py:27] Cannot use _Backend.FLASH_ATTN backend on TPU.\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m INFO 12-17 09:40:07 selector.py:163] Using Pallas backend.\r\nINFO 12-17 09:40:08 tpu.py:27] Cannot use _Backend.FLASH_ATTN backend on TPU.\r\nINFO 12-17 09:40:08 selector.py:163] Using Pallas backend.\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467] Error executing method init_device. This might cause deadlock in distributed execution.\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467] Traceback (most recent call last):\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/worker_base.py\", line 459, in execute_method\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     return executor(*args, **kwargs)\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     ensure_model_parallel_initialized(\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1101, in ensure_model_parallel_initialized\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     initialize_model_parallel(tensor_model_parallel_size,\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1045, in initialize_model_parallel\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     _TP = init_model_parallel_group(group_ranks,\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 876, in init_model_parallel_group\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     return GroupCoordinator(\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 43, in __init__\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     local_rank = global_rank % local_world_size\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467] ZeroDivisionError: integer division or modulo by zero\r\nERROR 12-17 09:40:09 worker_base.py:467] Error executing method init_device. This might cause deadlock in distributed execution.\r\nERROR 12-17 09:40:09 worker_base.py:467] Traceback (most recent call last):\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/worker_base.py\", line 459, in execute_method\r\nERROR 12-17 09:40:09 worker_base.py:467]     return executor(*args, **kwargs)\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\r\nERROR 12-17 09:40:09 worker_base.py:467]     ensure_model_parallel_initialized(\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1101, in ensure_model_parallel_initialized\r\nERROR 12-17 09:40:09 worker_base.py:467]     initialize_model_parallel(tensor_model_parallel_size,\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1045, in initialize_model_parallel\r\nERROR 12-17 09:40:09 worker_base.py:467]     _TP = init_model_parallel_group(group_ranks,\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 876, in init_model_parallel_group\r\nERROR 12-17 09:40:09 worker_base.py:467]     return GroupCoordinator(\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\r\nERROR 12-17 09:40:09 worker_base.py:467]     self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 43, in __init__\r\nERROR 12-17 09:40:09 worker_base.py:467]     local_rank = global_rank % local_world_size\r\nERROR 12-17 09:40:09 worker_base.py:467] ZeroDivisionError: integer division or modulo by zero\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/home/ext_hzchen_google_com/miniconda3/envs/tpu/bin/vllm\", line 33, in <module>\r\n[rank0]:     sys.exit(load_entry_point('vllm', 'console_scripts', 'vllm')())\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/scripts.py\", line 201, in main\r\n[rank0]:     args.dispatch_function(args)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/scripts.py\", line 42, in serve\r\n[rank0]:     uvloop.run(run_server(args))\r\n[rank0]:   File \"/home/ext_hzchen_google_com/miniconda3/envs/tpu/lib/python3.10/site-packages/uvloop/__init__.py\", line 82, in run\r\n[rank0]:     return loop.run_until_complete(wrapper())\r\n[rank0]:   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\r\n[rank0]:   File \"/home/ext_hzchen_google_com/miniconda3/envs/tpu/lib/python3.10/site-packages/uvloop/__init__.py\", line 61, in wrapper\r\n[rank0]:     return await main\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/entrypoints/openai/api_server.py\", line 667, in run_server\r\n[rank0]:     async with build_async_engine_client(args) as engine_client:\r\n[rank0]:   File \"/home/ext_hzchen_google_com/miniconda3/envs/tpu/lib/python3.10/contextlib.py\", line 199, in __aenter__\r\n[rank0]:     return await anext(self.gen)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/entrypoints/openai/api_server.py\", line 117, in build_async_engine_client\r\n[rank0]:     async with build_async_engine_client_from_engine_args(\r\n[rank0]:   File \"/home/ext_hzchen_google_com/miniconda3/envs/tpu/lib/python3.10/contextlib.py\", line 199, in __aenter__\r\n[rank0]:     return await anext(self.gen)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/entrypoints/openai/api_server.py\", line 150, in build_async_engine_client_from_engine_args\r\n[rank0]:     engine_client = build_engine()\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/engine/async_llm_engine.py\", line 707, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/engine/async_llm_engine.py\", line 594, in __init__\r\n[rank0]:     self.engine = self._engine_class(*args, **kwargs)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/engine/async_llm_engine.py\", line 267, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/engine/llm_engine.py\", line 288, in __init__\r\n[rank0]:     self.model_executor = executor_class(vllm_config=vllm_config, )\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/executor/ray_tpu_executor.py\", line 306, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/executor/ray_tpu_executor.py\", line 39, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/executor/executor_base.py\", line 36, in __init__\r\n[rank0]:     self._init_executor()\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/executor/ray_tpu_executor.py\", line 51, in _init_executor\r\n[rank0]:     self._init_workers_ray(placement_group)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/executor/ray_tpu_executor.py\", line 184, in _init_workers_ray\r\n[rank0]:     self._run_workers(\"init_device\")\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/executor/ray_tpu_executor.py\", line 249, in _run_workers\r\n[rank0]:     driver_worker_output = self.driver_worker.execute_method(\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/worker_base.py\", line 468, in execute_method\r\n[rank0]:     raise e\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/worker_base.py\", line 459, in execute_method\r\n[rank0]:     return executor(*args, **kwargs)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\r\n[rank0]:     ensure_model_parallel_initialized(\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1101, in ensure_model_parallel_initialized\r\n[rank0]:     initialize_model_parallel(tensor_model_parallel_size,\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1045, in initialize_model_parallel\r\n[rank0]:     _TP = init_model_parallel_group(group_ranks,\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 876, in init_model_parallel_group\r\n[rank0]:     return GroupCoordinator(\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\r\n[rank0]:     self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 43, in __init__\r\n[rank0]:     local_rank = global_rank % local_world_size\r\n[rank0]: ZeroDivisionError: integer division or modulo by zero\r\n\u001b[36m(RayWorkerWrapper pid=140502, ip=10.164.0.40)\u001b[0m WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=144474, ip=10.164.0.36)\u001b[0m INFO 12-17 09:40:09 tpu.py:27] Cannot use _Backend.FLASH_ATTN backend on TPU.\u001b[32m [repeated 30x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=144474, ip=10.164.0.36)\u001b[0m INFO 12-17 09:40:09 selector.py:163] Using Pallas backend.\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467] Error executing method init_device. This might cause deadlock in distributed execution.\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467] Traceback (most recent call last):\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/worker_base.py\", line 459, in execute_method\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     return executor(*args, **kwargs)\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     ensure_model_parallel_initialized(\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1101, in ensure_model_parallel_initialized\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     initialize_model_parallel(tensor_model_parallel_size,\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1045, in initialize_model_parallel\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     _TP = init_model_parallel_group(group_ranks,\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 876, in init_model_parallel_group\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     return GroupCoordinator(\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 43, in __init__\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     local_rank = global_rank % local_world_size\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467] ZeroDivisionError: integer division or modulo by zero\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n```\r\n</details>\r\n\r\nWhat's more, I can start the serving with ```--tensor-parallel-size 32``` only without error, which may have performance impact.\r\nWould like to know if this is work as intended or not.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tpu",
      "ray",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-12-17T13:04:46+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11260/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11260"
  },
  {
    "number": 11342,
    "title": "[Bug]: Multi-Node CPU Inference on MacOS calling `intel_extension_for_pytorch` ",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nWhile attempting to execute `NCCL_DEBUG=TRACE docker run -it --rm -p 8000:8000 --shm-size=4g --env \"HUGGING_FACE_HUB_TOKEN=<token>\" --env \"VLLM_CPU_KVCACHE_SPACE=40\" --privileged -e NCCL_IB_HCA=mlx5 vllm-cpu-env-latest --device=\"cpu\" --disable_async_output_proc --enforce-eager --model=meta-llama/Llama-Guard-3-1B --dtype=float16 --tensor-parallel-size 16 --pipeline-parallel-size 2 --distributed-executor-backend=\"ray\" --swap-space=1` for Multi-Node inference; It raise an error in `/usr/local/lib/python3.10/dist-packages/vllm/distributed/parallel_state.py` in `line 338` as it needs `intel_extension_for_pytorch` Module. \r\n\r\nThe error disappeared after commenting lines 338 and 339, would be great for a quick fixing. \r\n\r\nMoreover, just want to confirm that Ray is supported for CPU Inference on Multi-Node as described in [documentation](https://docs.vllm.ai/en/latest/serving/distributed_serving.html#details-for-distributed-inference-and-serving)\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "closed",
    "created_at": "2024-12-19T17:56:00+00:00",
    "closed_at": "2025-02-05T07:11:03+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11342/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11342"
  },
  {
    "number": 15687,
    "title": "[Bug]: Worker died during distributed inference",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\nnode1:\n\n```text\nINFO 03-28 00:32:32 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.31.6\nLibc version: glibc-2.35\n\nPython version: 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-130-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA H20\nGPU 1: NVIDIA H20\nGPU 2: NVIDIA H20\nGPU 3: NVIDIA H20\nGPU 4: NVIDIA H20\nGPU 5: NVIDIA H20\nGPU 6: NVIDIA H20\nGPU 7: NVIDIA H20\n\nNvidia driver version: 550.127.05\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               180\nOn-line CPU(s) list:                  0-179\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8457C\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   45\nSocket(s):                            2\nStepping:                             8\nBogoMIPS:                             5200.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 arch_capabilities\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            4.2 MiB (90 instances)\nL1i cache:                            2.8 MiB (90 instances)\nL2 cache:                             180 MiB (90 instances)\nL3 cache:                             195 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-89\nNUMA node1 CPU(s):                    90-179\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Unknown: No mitigations\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Mitigation; TSX disabled\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.1.post2+cu124torch2.6\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.25.1\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.50.0\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3.dev82+gcec8c7d7\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tNIC4\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\tPIX\tNODE\tSYS\tSYS\t0-89\t0\t\tN/A\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\tPIX\tNODE\tSYS\tSYS\t0-89\t0\t\tN/A\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\tNODE\tPIX\tSYS\tSYS\t0-89\t0\t\tN/A\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tSYS\tNODE\tPIX\tSYS\tSYS\t0-89\t0\t\tN/A\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tSYS\tSYS\tSYS\tPIX\tNODE\t90-179\t1\t\tN/A\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tSYS\tSYS\tSYS\tPIX\tNODE\t90-179\t1\t\tN/A\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tSYS\tSYS\tSYS\tNODE\tPIX\t90-179\t1\t\tN/A\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tSYS\tSYS\tSYS\tNODE\tPIX\t90-179\t1\t\tN/A\nNIC0\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tSYS\tSYS\tSYS\tSYS\t\t\t\t\nNIC1\tPIX\tPIX\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tNODE\tSYS\tSYS\t\t\t\t\nNIC2\tNODE\tNODE\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\t X \tSYS\tSYS\t\t\t\t\nNIC3\tSYS\tSYS\tSYS\tSYS\tPIX\tPIX\tNODE\tNODE\tSYS\tSYS\tSYS\t X \tNODE\t\t\t\t\nNIC4\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPIX\tPIX\tSYS\tSYS\tSYS\tNODE\t X \t\t\t\t\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n\nNVIDIA_VISIBLE_DEVICES=all\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\nNCCL_VERSION=2.20.5-1\nNCCL_SOCKET_IFNAME=eth0\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nVLLM_TORCH_PROFILER_DIR=/vllm-profile\nNCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nNCCL_IB_GID_INDEX=3\nCUDA_VERSION=12.4.0\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNCCL_IB_DISABLE=0\nVLLM_HOST_IP=10.99.48.128\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\nnode2:\n```text\n...\nPython platform: Linux-5.15.0-135-generic-x86_64-with-glibc2.35\n...\n...\nVLLM_HOST_IP=10.99.48.129\n...\n```\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI deployed VLLM+Ray using containers on two nodes. The ray cluster information is as follows:\n```text\n======== Autoscaler status: 2025-03-28 00:38:24.697168 ========\nNode status\n---------------------------------------------------------------\nActive:\n 1 node_0e1b24bf0116d75c041dc6d1ce6ff498a5a0fd7013613c601d63ab1c\n 1 node_3a8b52b1966cd168bdb3ea3b9daa84dfeda5686e037cd5d2345f95f3\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/360.0 CPU\n 0.0/16.0 GPU\n 0B/3.39TiB memory\n 0B/372.53GiB object_store_memory\n\nDemands:\n (no resource demands)\n```\n\nI use the following command to run the Deepseek model:\n```bash\npython3 -m vllm.entrypoints.openai.api_server --port 18011 --model /models/snapshots/4c1f24cc10a2a1894304c7ab52edd9710c047571 --tensor-parallel-size 16 --gpu-memory-utilization 0.92 --dtype auto --served-model-name deepseekv3 --max-num-seqs 40 --max-model-len 16384 --enable-chunked-prefill --enable-prefix-caching --trust-remote-code\n```\n\nThe error message is as follows:\n```text\nINFO 03-27 22:16:27 [__init__.py:239] Automatically detected platform cuda.\nWARNING 03-27 22:16:28 [api_server.py:743] Torch Profiler is enabled in the API server. This should ONLY be used for local development!\nINFO 03-27 22:16:29 [api_server.py:1018] vLLM API server version 0.8.3.dev82+gcec8c7d7\nINFO 03-27 22:16:29 [api_server.py:1019] args: Namespace(host=None, port=18011, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/models/snapshots/4c1f24cc10a2a1894304c7ab52edd9710c047571', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=16384, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=16, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.92, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=40, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=True, speculative_config=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['deepseekv3'], qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)\nINFO 03-27 22:16:29 [config.py:208] Replacing legacy 'type' key with 'rope_type'\nINFO 03-27 22:16:34 [config.py:593] This model supports multiple tasks: {'reward', 'generate', 'embed', 'score', 'classify'}. Defaulting to 'generate'.\nINFO 03-27 22:16:36 [config.py:1559] Defaulting to use ray for distributed inference\nINFO 03-27 22:16:36 [config.py:1738] Chunked prefill is enabled with max_num_batched_tokens=2048.\nWARNING 03-27 22:16:36 [fp8.py:54] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\nINFO 03-27 22:16:36 [cuda.py:160] Forcing kv cache block size to 64 for FlashMLA backend.\nINFO 03-27 22:16:36 [config.py:208] Replacing legacy 'type' key with 'rope_type'\nINFO 03-27 22:16:36 [core.py:58] Initializing a V1 LLM engine (v0.8.3.dev82+gcec8c7d7) with config: model='/models/snapshots/4c1f24cc10a2a1894304c7ab52edd9710c047571', speculative_config=None, tokenizer='/models/snapshots/4c1f24cc10a2a1894304c7ab52edd9710c047571', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=16, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=deepseekv3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n2025-03-27 22:16:36,498        INFO worker.py:1654 -- Connecting to existing Ray cluster at address: 10.99.48.128:6379...\n2025-03-27 22:16:36,509        INFO worker.py:1832 -- Connected to Ray cluster. View the dashboard at http://10.99.48.128:8265 \nINFO 03-27 22:16:37 [ray_utils.py:335] No current placement group found. Creating a new placement group.\nWARNING 03-27 22:16:38 [ray_utils.py:199] tensor_parallel_size=16 is bigger than a reserved number of GPUs (8 GPUs) in a node f387a1f000756ab79eeb948cba38b48749bb1fb7265b40f6e47126b7. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 16 GPUs available at each node.\nWARNING 03-27 22:16:38 [ray_utils.py:199] tensor_parallel_size=16 is bigger than a reserved number of GPUs (8 GPUs) in a node 651b9cf98394531be52379efc8986359ac1535284df6e149e5f31c14. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 16 GPUs available at each node.\nINFO 03-27 22:16:38 [ray_distributed_executor.py:176] use_ray_spmd_worker: True\n(pid=1084) INFO 03-27 22:16:41 [__init__.py:239] Automatically detected platform cuda.\n(pid=336, ip=10.99.48.129) INFO 03-27 22:16:46 [__init__.py:239] Automatically detected platform cuda. [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\nINFO 03-27 22:16:50 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()\nINFO 03-27 22:16:50 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_USAGE_SOURCE', 'VLLM_USE_RAY_SPMD_WORKER', 'VLLM_USE_RAY_COMPILED_DAG', 'VLLM_TORCH_PROFILER_DIR', 'VLLM_USE_V1']\nINFO 03-27 22:16:50 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /root/.config/vllm/ray_non_carry_over_env_vars.json file\n(RayWorkerWrapper pid=331, ip=10.99.48.129) WARNING 03-27 22:16:52 [utils.py:2379] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7edec44b2810>\n(RayWorkerWrapper pid=331, ip=10.99.48.129) INFO 03-27 22:16:52 [gpu_worker.py:60] Profiling enabled. Traces will be saved to: /vllm-profile\n(pid=334, ip=10.99.48.129) INFO 03-27 22:16:46 [__init__.py:239] Automatically detected platform cuda. [repeated 7x across cluster]\n(RayWorkerWrapper pid=1069) [W327 22:16:57.701090705 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n(RayWorkerWrapper pid=1069) [W327 22:17:05.752047219 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3 [repeated 9x across cluster]\n(RayWorkerWrapper pid=1069) [W327 22:17:13.800051335 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3 [repeated 2x across cluster]\n(RayWorkerWrapper pid=1069) [W327 22:17:21.848049705 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3 [repeated 2x across cluster]\n(RayWorkerWrapper pid=1069) [W327 22:17:29.896050535 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3 [repeated 2x across cluster]\n(RayWorkerWrapper pid=1069) [W327 22:17:37.944051488 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3 [repeated 2x across cluster]\n(RayWorkerWrapper pid=1069) [W327 22:17:45.993092159 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3 [repeated 2x across cluster]\n(RayWorkerWrapper pid=1069) [W327 22:17:53.040048549 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3 [repeated 2x across cluster]\n(RayWorkerWrapper pid=1069) INFO 03-27 22:18:01 [utils.py:982] Found nccl from library libnccl.so.2\n(RayWorkerWrapper pid=1069) INFO 03-27 22:18:01 [pynccl.py:69] vLLM is using nccl==2.25.1\n(RayWorkerWrapper pid=1068) WARNING 03-27 22:16:52 [utils.py:2379] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f043f77b6b0> [repeated 15x across cluster]\n(RayWorkerWrapper pid=1068) INFO 03-27 22:16:52 [gpu_worker.py:60] Profiling enabled. Traces will be saved to: /vllm-profile [repeated 15x across cluster]\n(RayWorkerWrapper pid=1069) [W327 22:18:01.088046699 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3 [repeated 2x across cluster]\n(RayWorkerWrapper pid=336, ip=10.99.48.129) free(): double free detected in tcache 2\n(RayWorkerWrapper pid=336, ip=10.99.48.129) *** SIGABRT received at time=1743139179 on cpu 162 ***\n(RayWorkerWrapper pid=336, ip=10.99.48.129) PC: @     0x7fe64731b9fc  (unknown)  pthread_kill\n(RayWorkerWrapper pid=336, ip=10.99.48.129)     @     0x7fe6472c7520  (unknown)  (unknown)\n(RayWorkerWrapper pid=336, ip=10.99.48.129) [2025-03-27 22:19:39,802 E 336 336] logging.cc:484: *** SIGABRT received at time=1743139179 on cpu 162 ***\n(RayWorkerWrapper pid=336, ip=10.99.48.129) [2025-03-27 22:19:39,803 E 336 336] logging.cc:484: PC: @     0x7fe64731b9fc  (unknown)  pthread_kill\n(RayWorkerWrapper pid=336, ip=10.99.48.129) [2025-03-27 22:19:39,803 E 336 336] logging.cc:484:     @     0x7fe6472c7520  (unknown)  (unknown)\n(RayWorkerWrapper pid=336, ip=10.99.48.129) Fatal Python error: Aborted\n(RayWorkerWrapper pid=336, ip=10.99.48.129) \n(RayWorkerWrapper pid=336, ip=10.99.48.129) Stack (most recent call first):\n(RayWorkerWrapper pid=336, ip=10.99.48.129)   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 277 in ncclCommInitRank\n(RayWorkerWrapper pid=336, ip=10.99.48.129)   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl.py\", line 99 in __init__\n(RayWorkerWrapper pid=336, ip=10.99.48.129)   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/cuda_communicator.py\", line 39 in __init__\n(RayWorkerWrapper pid=336, ip=10.99.48.129)   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 207 in __init__\n(RayWorkerWrapper pid=336, ip=10.99.48.129)   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 730 in init_model_parallel_group\n(RayWorkerWrapper pid=336, ip=10.99.48.129)   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 925 in initialize_model_parallel\n(RayWorkerWrapper pid=336, ip=10.99.48.129)   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 992 in ensure_model_parallel_initialized\n(RayWorkerWrapper pid=336, ip=10.99.48.129)   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 285 in init_worker_distributed_environment\n(RayWorkerWrapper pid=336, ip=10.99.48.129)   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 113 in init_device\n(RayWorkerWrapper pid=336, ip=10.99.48.129)   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 604 in init_device\n(RayWorkerWrapper pid=336, ip=10.99.48.129)   File \"/usr/local/lib/python3.12/dist-packages/ray/util/tracing/tracing_helper.py\", line 463 in _resume_span\n(RayWorkerWrapper pid=336, ip=10.99.48.129)   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2313 in run_method\n(RayWorkerWrapper pid=336, ip=10.99.48.129)   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 612 in execute_method\n(RayWorkerWrapper pid=336, ip=10.99.48.129)   File \"/usr/local/lib/python3.12/dist-packages/ray/util/tracing/tracing_helper.py\", line 463 in _resume_span\n(RayWorkerWrapper pid=336, ip=10.99.48.129)   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/function_manager.py\", line 696 in actor_method_executor\n(RayWorkerWrapper pid=336, ip=10.99.48.129)   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 935 in main_loop\n(RayWorkerWrapper pid=336, ip=10.99.48.129)   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/workers/default_worker.py\", line 297 in <module>\n(RayWorkerWrapper pid=336, ip=10.99.48.129) \n(RayWorkerWrapper pid=336, ip=10.99.48.129) Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, uvloop.loop, ray._raylet, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, msgspec._core, PIL._imagingft, zmq.backend.cython._zmq, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, pyarrow.lib, pyarrow._json, vllm.cumem_allocator, sentencepiece._sentencepiece, scipy._lib._ccallback_c, numba.core.typeconv._typeconv, numba._helperlib, numba._dynfunc, numba._dispatcher, numba.core.runtime._nrt_python, numba.np.ufunc._internal, numba.experimental.jitclass._box (total: 60)\n(RayWorkerWrapper pid=337, ip=10.99.48.129) \n(RayWorkerWrapper pid=337, ip=10.99.48.129) \n(RayWorkerWrapper pid=335, ip=10.99.48.129) \n(RayWorkerWrapper pid=335, ip=10.99.48.129) \n(RayWorkerWrapper pid=332, ip=10.99.48.129) \n(RayWorkerWrapper pid=332, ip=10.99.48.129) \n(RayWorkerWrapper pid=333, ip=10.99.48.129) \n(RayWorkerWrapper pid=333, ip=10.99.48.129) \n(raylet) A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffc9ceea85e04acadfde0a184f01000000 Worker ID: 1d2aee6780dd0ff9d9317d2ce87a8033c28448c5d4f11ad1f79c49b5 Node ID: 651b9cf98394531be52379efc8986359ac1535284df6e149e5f31c14 Worker IP address: 10.99.48.129 Worker port: 10007 Worker PID: 335 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n(RayWorkerWrapper pid=338, ip=10.99.48.129) INFO 03-27 22:18:01 [utils.py:982] Found nccl from library libnccl.so.2 [repeated 15x across cluster]\n(RayWorkerWrapper pid=338, ip=10.99.48.129) INFO 03-27 22:18:01 [pynccl.py:69] vLLM is using nccl==2.25.1 [repeated 15x across cluster]\nERROR 03-27 22:19:40 [core.py:367] EngineCore hit an exception: Traceback (most recent call last):\nERROR 03-27 22:19:40 [core.py:367]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 355, in run_engine_core\nERROR 03-27 22:19:40 [core.py:367]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 03-27 22:19:40 [core.py:367]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-27 22:19:40 [core.py:367]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 296, in __init__\nERROR 03-27 22:19:40 [core.py:367]     super().__init__(vllm_config, executor_class, log_stats)\nERROR 03-27 22:19:40 [core.py:367]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 64, in __init__\nERROR 03-27 22:19:40 [core.py:367]     self.model_executor = executor_class(vllm_config)\nERROR 03-27 22:19:40 [core.py:367]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-27 22:19:40 [core.py:367]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 271, in __init__\nERROR 03-27 22:19:40 [core.py:367]     super().__init__(*args, **kwargs)\nERROR 03-27 22:19:40 [core.py:367]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 52, in __init__\nERROR 03-27 22:19:40 [core.py:367]     self._init_executor()\nERROR 03-27 22:19:40 [core.py:367]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py\", line 114, in _init_executor\nERROR 03-27 22:19:40 [core.py:367]     self._init_workers_ray(placement_group)\nERROR 03-27 22:19:40 [core.py:367]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py\", line 395, in _init_workers_ray\nERROR 03-27 22:19:40 [core.py:367]     self._run_workers(\"init_device\")\nERROR 03-27 22:19:40 [core.py:367]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py\", line 521, in _run_workers\nERROR 03-27 22:19:40 [core.py:367]     ray_worker_outputs = ray.get(ray_worker_outputs)\nERROR 03-27 22:19:40 [core.py:367]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-27 22:19:40 [core.py:367]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\nERROR 03-27 22:19:40 [core.py:367]     return fn(*args, **kwargs)\nERROR 03-27 22:19:40 [core.py:367]            ^^^^^^^^^^^^^^^^^^^\nERROR 03-27 22:19:40 [core.py:367]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\nERROR 03-27 22:19:40 [core.py:367]     return func(*args, **kwargs)\nERROR 03-27 22:19:40 [core.py:367]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 03-27 22:19:40 [core.py:367]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2771, in get\nERROR 03-27 22:19:40 [core.py:367]     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\nERROR 03-27 22:19:40 [core.py:367]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-27 22:19:40 [core.py:367]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 921, in get_objects\nERROR 03-27 22:19:40 [core.py:367]     raise value\nERROR 03-27 22:19:40 [core.py:367] ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\nERROR 03-27 22:19:40 [core.py:367]         class_name: RayWorkerWrapper\nERROR 03-27 22:19:40 [core.py:367]         actor_id: c9ceea85e04acadfde0a184f01000000\nERROR 03-27 22:19:40 [core.py:367]         pid: 335\nERROR 03-27 22:19:40 [core.py:367]         namespace: 836a7a77-73bd-431b-b86b-aca33ffdcb3e\nERROR 03-27 22:19:40 [core.py:367]         ip: 10.99.48.129\nERROR 03-27 22:19:40 [core.py:367] The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\nERROR 03-27 22:19:40 [core.py:367] \nCRITICAL 03-27 22:19:40 [core_client.py:319] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\nKilled\n```\n\nI tried tpsize=8 on two machines separately and there were no problems, so I think it may be a problem with ray.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-28T07:41:20+00:00",
    "closed_at": null,
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15687/reactions",
      "total_count": 2,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15687"
  },
  {
    "number": 17770,
    "title": "[Bug]: Deepseek R1 failed to load with segfault when using multi-node serving in V1",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWith vLLM image in docker hub `vllm/vllm-openai:v0.8.4`, attempt to run deepseek R1 model in V1 engine fail with segfault, while changing back to V0 using `export VLLM_USE_V1=0` can start successfully.\n\nTo reproduce, use 2 nodes and create a ray cluster according to vLLM's multi-node guide and run below command on one of the node\n```\nvllm serve deepseek-ai/DeepSeek-R1 --block-size 128 --max-model-len 3500 --max-num-batched-tokens 3500 --tensor-parallel-size 16 --disable-log-requests\n```\nBelow segfault will be observed\n```\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232) *** SIGSEGV received at time=1746566409 on cpu 131 ***\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232) PC: @     0x7fcfcd849b8a  (unknown)  addProxyOpIfNeeded()\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)     @     0x7ffff7c9e520  267994832  (unknown)\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)     @            0x80000  (unknown)  (unknown)\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232) [2025-05-06 14:20:09,187 E 2191173 2191173] logging.cc:484: *** SIGSEGV received at time=1746566409 on cpu 131 ***\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232) [2025-05-06 14:20:09,187 E 2191173 2191173] logging.cc:484: PC: @     0x7fcfcd849b8a  (unknown)  addProxyOpIfNeeded()\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232) [2025-05-06 14:20:09,188 E 2191173 2191173] logging.cc:484:     @     0x7ffff7c9e520  267994832  (unknown)\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232) [2025-05-06 14:20:09,189 E 2191173 2191173] logging.cc:484:     @            0x80000  (unknown)  (unknown)\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232) Fatal Python error: Segmentation fault\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232) \n(RayWorkerWrapper pid=2191173, ip=10.52.51.232) Stack (most recent call first):\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 290 in ncclAllReduce\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl.py\", line 126 in all_reduce\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/cuda_communicator.py\", line 63 in all_reduce\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 318 in _all_reduce_out_place\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 114 in all_reduce\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/torch/_ops.py\", line 723 in __call__\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/root/.cache/vllm/torch_compile_cache/f8858bbc14/rank_11_0/inductor_cache/u4/cu4tumet6zifllktnlxblmh5elbdicrnknwys64u3hchsat7sjg4.py\", line 508 in call\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py\", line 466 in __call__\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/compiler_interface.py\", line 356 in compiled_graph\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/backends.py\", line 649 in __call__\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"<eval_with_key>.162\", line 817 in forward\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750 in _call_impl\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739 in _wrapped_call_impl\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\", line 387 in __call__\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\", line 822 in call_wrapped\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\", line 745 in _fn\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750 in _call_impl\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739 in _wrapped_call_impl\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 341 in forward\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 245 in __call__\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 541 in forward\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750 in _call_impl\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739 in _wrapped_call_impl\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1441 in _dummy_run\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116 in decorate_context\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1618 in capture_model\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 216 in compile_or_warm_up_model\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2378 in run_method\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 612 in execute_method\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/ray/util/tracing/tracing_helper.py\", line 463 in _resume_span\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/function_manager.py\", line 696 in actor_method_executor\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 935 in main_loop\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232)   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/workers/default_worker.py\", line 297 in <module>\n(RayWorkerWrapper pid=2191173, ip=10.52.51.232) \n(RayWorkerWrapper pid=2191173, ip=10.52.51.232) Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, uvloop.loop, ray._raylet, numpy._core._multiarray_umath, numpy.linalg._umath_linalg, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, zmq.backend.cython._zmq, msgspec._core, PIL._imagingft, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, vllm.cumem_allocator, sentencepiece._sentencepiece, scipy._lib._ccallback_c, numba.core.typeconv._typeconv, numba._helperlib, numba._dynfunc, numba._dispatcher, numba.core.typing.builtins.itertools, numba.cpython.builtins.math, numba.core.runtime._nrt_python, numba.np.ufunc._internal, numba.experimental.jitclass._box, cuda_utils, __triton_launcher (total: 60)\n(raylet) A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffdd87837a059f3b2317b0ebd102000000 Worker ID: 3ca0bd180642f913e15d1b569fc77c38325d0f1b3e09bbb7d0eaea10 Node ID: 0dacab6d1b485464afdef4003dd48697e53c6142b4cc5b29acce7797 Worker IP address: 10.52.51.232 Worker port: 10013 Worker PID: 2191173 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n(RayWorkerWrapper pid=2479036) INFO 05-06 14:19:54 [monitor.py:33] torch.compile takes 20.18 s in total [repeated 15x across cluster]\nERROR 05-06 14:20:15 [core.py:387] EngineCore hit an exception: Traceback (most recent call last):\nERROR 05-06 14:20:15 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 378, in run_engine_core\nERROR 05-06 14:20:15 [core.py:387]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 05-06 14:20:15 [core.py:387]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-06 14:20:15 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 320, in __init__\nERROR 05-06 14:20:15 [core.py:387]     super().__init__(vllm_config, executor_class, log_stats)\nERROR 05-06 14:20:15 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 71, in __init__\nERROR 05-06 14:20:15 [core.py:387]     self._initialize_kv_caches(vllm_config)\nERROR 05-06 14:20:15 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 160, in _initialize_kv_caches\nERROR 05-06 14:20:15 [core.py:387]     self.model_executor.initialize_from_config(kv_cache_configs)\nERROR 05-06 14:20:15 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py\", line 63, in initialize_from_config\nERROR 05-06 14:20:15 [core.py:387]     self.collective_rpc(\"compile_or_warm_up_model\")\nERROR 05-06 14:20:15 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 331, in collective_rpc\nERROR 05-06 14:20:15 [core.py:387]     return self._run_workers(method, *args, **(kwargs or {}))\nERROR 05-06 14:20:15 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-06 14:20:15 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py\", line 521, in _run_workers\nERROR 05-06 14:20:15 [core.py:387]     ray_worker_outputs = ray.get(ray_worker_outputs)\nERROR 05-06 14:20:15 [core.py:387]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-06 14:20:15 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\nERROR 05-06 14:20:15 [core.py:387]     return fn(*args, **kwargs)\nERROR 05-06 14:20:15 [core.py:387]            ^^^^^^^^^^^^^^^^^^^\nERROR 05-06 14:20:15 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\nERROR 05-06 14:20:15 [core.py:387]     return func(*args, **kwargs)\nERROR 05-06 14:20:15 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 05-06 14:20:15 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2771, in get\nERROR 05-06 14:20:15 [core.py:387]     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\nERROR 05-06 14:20:15 [core.py:387]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-06 14:20:15 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 921, in get_objects\nERROR 05-06 14:20:15 [core.py:387]     raise value\nERROR 05-06 14:20:15 [core.py:387] ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\nERROR 05-06 14:20:15 [core.py:387]      class_name: RayWorkerWrapper\nERROR 05-06 14:20:15 [core.py:387]      actor_id: dd87837a059f3b2317b0ebd102000000\nERROR 05-06 14:20:15 [core.py:387]      pid: 2191173\nERROR 05-06 14:20:15 [core.py:387]      namespace: ac204bee-60f7-43a3-9fc1-ba697972c97f\nERROR 05-06 14:20:15 [core.py:387]      ip: 10.52.51.232\nERROR 05-06 14:20:15 [core.py:387] The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\nERROR 05-06 14:20:15 [core.py:387] \nCRITICAL 05-06 14:20:15 [core_client.py:359] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\nKilled\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "open",
    "created_at": "2025-05-07T07:50:44+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17770/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17770"
  },
  {
    "number": 8966,
    "title": "[Doc]: Offline Inference Distributed",
    "body": "### \ud83d\udcda The doc issue\r\n\r\nHi,\r\n\r\nI was just wondering why in the \"Offline Inference Distributed\" example, `ds.map_batches()` is used. I used this initially, but I am now splitting the dataset and using `ray.remote()` which has the advantage that I don't need to specify the batch_size and can use continuous batching per GPU. \r\n\r\n### Suggest a potential alternative/fix\r\n\r\nIf useful I could contribute with an example in the docs with ray.remote(), so both methods are available\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation",
      "ray",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-30T13:29:52+00:00",
    "closed_at": "2025-04-20T02:11:02+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8966/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8966"
  },
  {
    "number": 19703,
    "title": "[Bug]: RAY_CGRAPH_get_timeout is not set successfully.  Ray still detects default timeout value.",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nINFO 06-16 10:48:23 [__init__.py:244] Automatically detected platform cuda.\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (aarch64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : Could not collect\nCMake version                : version 4.0.2\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu128\nIs debug build               : False\nCUDA used to build PyTorch   : 12.8\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0] (64-bit runtime)\nPython platform              : Linux-6.8.0-1028-nvidia-64k-aarch64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.8.93\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration :\nGPU 0: NVIDIA GB200\nGPU 1: NVIDIA GB200\nGPU 2: NVIDIA GB200\nGPU 3: NVIDIA GB200\n\nNvidia driver version        : 570.158.01\ncuDNN version                : Could not collect\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         aarch64\nCPU op-mode(s):                       64-bit\nByte Order:                           Little Endian\nCPU(s):                               144\nOn-line CPU(s) list:                  0-143\nVendor ID:                            ARM\nModel name:                           Neoverse-V2\nModel:                                0\nThread(s) per core:                   1\nCore(s) per socket:                   72\nSocket(s):                            2\nStepping:                             r0p0\nFrequency boost:                      disabled\nCPU max MHz:                          3393.0000\nCPU min MHz:                          81.0000\nBogoMIPS:                             2000.00\nFlags:                                fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma lrcpc dcpop sha3 sm3 sm4 asimddp sha512 sve asimdfhm dit uscat ilrcpc flagm ssbs sb paca pacg dcpodp sve2 sveaes svepmull svebitperm svesha3 svesm4 flagm2 frint svei8mm svebf16 i8mm bf16 dgh bti\nL1d cache:                            9 MiB (144 instances)\nL1i cache:                            9 MiB (144 instances)\nL2 cache:                             144 MiB (144 instances)\nL3 cache:                             228 MiB (2 instances)\nNUMA node(s):                         34\nNUMA node0 CPU(s):                    0-71\nNUMA node1 CPU(s):                    72-143\nNUMA node2 CPU(s):\nNUMA node3 CPU(s):\nNUMA node4 CPU(s):\nNUMA node5 CPU(s):\nNUMA node6 CPU(s):\nNUMA node7 CPU(s):\nNUMA node8 CPU(s):\nNUMA node9 CPU(s):\nNUMA node10 CPU(s):\nNUMA node11 CPU(s):\nNUMA node12 CPU(s):\nNUMA node13 CPU(s):\nNUMA node14 CPU(s):\nNUMA node15 CPU(s):\nNUMA node16 CPU(s):\nNUMA node17 CPU(s):\nNUMA node18 CPU(s):\nNUMA node19 CPU(s):\nNUMA node20 CPU(s):\nNUMA node21 CPU(s):\nNUMA node22 CPU(s):\nNUMA node23 CPU(s):\nNUMA node24 CPU(s):\nNUMA node25 CPU(s):\nNUMA node26 CPU(s):\nNUMA node27 CPU(s):\nNUMA node28 CPU(s):\nNUMA node29 CPU(s):\nNUMA node30 CPU(s):\nNUMA node31 CPU(s):\nNUMA node32 CPU(s):\nNUMA node33 CPU(s):\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Not affected\nVulnerability Spectre v1:             Mitigation; __user pointer sanitization\nVulnerability Spectre v2:             Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.2.6\n[pip3] pytorch-triton==3.3.0+gitab727c40\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.0+cu128\n[pip3] torchaudio==2.7.0\n[pip3] torchvision==0.22.0\n[pip3] transformers==4.52.4\n[pip3] triton==3.3.0\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.2.dev29+gc7ea0b56c (git sha: c7ea0b56c)\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NODE    NODE    SYS     SYS     0-71    0               2\nGPU1    NV18     X      NV18    NV18    NODE    NODE    SYS     SYS     0-71    0               10\nGPU2    NV18    NV18     X      NV18    SYS     SYS     NODE    NODE    72-143  1               18\nGPU3    NV18    NV18    NV18     X      SYS     SYS     NODE    NODE    72-143  1               26\nNIC0    NODE    NODE    SYS     SYS      X      NODE    SYS     SYS\nNIC1    NODE    NODE    SYS     SYS     NODE     X      SYS     SYS\nNIC2    SYS     SYS     NODE    NODE    SYS     SYS      X      NODE\nNIC3    SYS     SYS     NODE    NODE    SYS     SYS     NODE     X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_4\n  NIC3: mlx5_5\n\n==============================\n     Environment Variables\n==============================\nNVIDIA_VISIBLE_DEVICES=all\nNVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\nCUDA_CACHE_DISABLE=1\nNCCL_VERSION=2.25.1-1\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nCUDA_DEVICE_ORDER=PCI_BUS_ID\nCUDA_VERSION=12.8.1\nNCCL_SHARP_GROUP_SIZE_THRESH=2\nNCCL_COLLNET_ENABLE=1\nNCCL_IB_SL=1\nLD_LIBRARY_PATH=/usr/local/cuda/lib64\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n# Background\nI am trying to run multi-node on GB200. I first encountered an error when setting up Ray for the server and that was because CuPy did not support aarch64 so I installed CuPy on the cupy main branch to get around the issue.  Reference: https://github.com/ray-project/ray/issues/53128\n\n\n# Issue\nNow I am hitting another Ray error when running the server:\n```text\nINFO 06-12 10:46:22 [ray_distributed_executor.py:579] RAY_CGRAPH_get_timeout is set to 300\nERROR 06-12 10:46:34 [core.py:517] EngineCore encountered a fatal error.\nERROR 06-12 10:46:34 [core.py:517] Traceback (most recent call last):\nERROR 06-12 10:46:34 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 2515, in _execute_until\nERROR 06-12 10:46:34 [core.py:517]     result = self._dag_output_fetcher.read(timeout)\nERROR 06-12 10:46:34 [core.py:517]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-12 10:46:34 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/common.py\", line 309, in read\nERROR 06-12 10:46:34 [core.py:517]     outputs = self._read_list(timeout)\nERROR 06-12 10:46:34 [core.py:517]               ^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-12 10:46:34 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/common.py\", line 400, in _read_list\nERROR 06-12 10:46:34 [core.py:517]     raise e\nERROR 06-12 10:46:34 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/common.py\", line 382, in _read_list\nERROR 06-12 10:46:34 [core.py:517]     result = c.read(min(remaining_timeout, iteration_timeout))\nERROR 06-12 10:46:34 [core.py:517]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-12 10:46:34 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/shared_memory_channel.py\", line 776, in read\nERROR 06-12 10:46:34 [core.py:517]     return self._channel_dict[self._resolve_actor_id()].read(timeout)\nERROR 06-12 10:46:34 [core.py:517]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-12 10:46:34 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/shared_memory_channel.py\", line 480, in read\nERROR 06-12 10:46:34 [core.py:517]     ret = self._worker.get_objects(\nERROR 06-12 10:46:34 [core.py:517]           ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-12 10:46:34 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 911, in get_objects\nERROR 06-12 10:46:34 [core.py:517]     ] = self.core_worker.get_objects(\nERROR 06-12 10:46:34 [core.py:517]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-12 10:46:34 [core.py:517]   File \"python/ray/_raylet.pyx\", line 3162, in ray._raylet.CoreWorker.get_objects\nERROR 06-12 10:46:34 [core.py:517]   File \"python/ray/includes/common.pxi\", line 106, in ray._raylet.check_status\nERROR 06-12 10:46:34 [core.py:517] ray.exceptions.RayChannelTimeoutError: System error: Timed out waiting for object available to read. ObjectID: 00fc911a7833d5d08cfd39cf6b6b31a5fbea02990100000002e1f505\nERROR 06-12 10:46:34 [core.py:517]\nERROR 06-12 10:46:34 [core.py:517] The above exception was the direct cause of the following exception:\nERROR 06-12 10:46:34 [core.py:517]\nERROR 06-12 10:46:34 [core.py:517] Traceback (most recent call last):\nERROR 06-12 10:46:34 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 508, in run_engine_core\nERROR 06-12 10:46:34 [core.py:517]     engine_core.run_busy_loop()\nERROR 06-12 10:46:34 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 535, in run_busy_loop\nERROR 06-12 10:46:34 [core.py:517]     self._process_engine_step()\nERROR 06-12 10:46:34 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 560, in _process_engine_step\nERROR 06-12 10:46:34 [core.py:517]     outputs, model_executed = self.step_fn()\nERROR 06-12 10:46:34 [core.py:517]                               ^^^^^^^^^^^^^^\nERROR 06-12 10:46:34 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 279, in step_with_batch_queue\nERROR 06-12 10:46:34 [core.py:517]     model_output = future.result()\nERROR 06-12 10:46:34 [core.py:517]                    ^^^^^^^^^^^^^^^\nERROR 06-12 10:46:34 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/ray_distributed_executor.py\", line 25, in result\nERROR 06-12 10:46:34 [core.py:517]     return self.ref.get()\nERROR 06-12 10:46:34 [core.py:517]            ^^^^^^^^^^^^^^\nERROR 06-12 10:46:34 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/compiled_dag_ref.py\", line 115, in get\nERROR 06-12 10:46:34 [core.py:517]     self._dag._execute_until(\nERROR 06-12 10:46:34 [core.py:517]   File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 2525, in _execute_until\nERROR 06-12 10:46:34 [core.py:517]     raise RayChannelTimeoutError(\nERROR 06-12 10:46:34 [core.py:517] ray.exceptions.RayChannelTimeoutError: System error: If the execution is expected to take a long time, increase RAY_CGRAPH_get_timeout which is currently 10 seconds. Otherwise, this may indicate that the execution is hanging.\n```\nFrom the error log RAY_CGRAPH_get_timeout is supposed to be set to 300, but Ray failed saying that it hit the timeout because RAY_CGRAPH_get_timeout is 10.\nI manually set RAY_CGRAPH_get_timeout to 300 and it passed.  **I think vLLM did not successfully set RAY_CGRAPH_get_timeout to 300.**\n\n# Reproduce commands:\n```\n#!/bin/bash\n\n#SBATCH --output=output.log\n#SBATCH --nodes=2\n\n### Give all resources to a single Ray task, ray can manage the resources internally\n#SBATCH --ntasks-per-node=1\n\n################# DON NOT CHANGE THINGS HERE UNLESS YOU KNOW WHAT YOU ARE DOING ###############\n# This script is a modification to the implementation suggest by gregSchwartz18 here:\n# https://github.com/ray-project/ray/issues/826#issuecomment-522116599\n\nset -x\n\n# Trying to set RAY_CGRAPH_get_timeout as the potential fix.  If this is not set vLLM will fail.\nexport RAY_CGRAPH_get_timeout=300\n\nnum_gpus_per_node=$(nvidia-smi -L | wc -l)\nnodes=$(scontrol show hostnames $SLURM_JOB_NODELIST) # Getting the node names\nnodes_array=($nodes)\n\nnode_1=${nodes_array[0]}\nip=$(srun --nodes=1 --ntasks=1 -w $node_1 hostname --ip-address) # making redis-address\n\nif [[ $ip == *\" \"* ]]; then\n  IFS=' ' read -ra ADDR <<<\"$ip\"\n  if [[ ${#ADDR[0]} > 16 ]]; then\n    ip=${ADDR[1]}\n  else\n    ip=${ADDR[0]}\n  fi\n  echo \"We detect space in ip! You are using IPV6 address. We split the IPV4 address as $ip\"\nfi\n\nport_head=6379\nip_head=$ip:$port_head\nexport ip_head\necho \"IP Head: $ip_head\"\n\nexport container=<container>\nexport hf_model=\"meta-llama/Llama-3.1-8B-Instruct\"\nexport server_port=8000\n\nexport head_node_srun=\"srun --nodes=1 --ntasks=1 -w $node_1 --no-container-mount-home --overlap --container-name head --container-image $container\"\nexport head_node_srun_exec=\"${head_node_srun}:exec\"\n\necho \"Prestarting the head container\"\n$head_node_srun true\n\necho \"STARTING HEAD at $node_1\"\n$head_node_srun \\\n  ray start --head --node-ip-address=$ip --port=$port_head --block --disable-usage-stats --temp-dir=/workspace/raytmp/ &\nsleep 30\n\nworker_num=$(($SLURM_JOB_NUM_NODES - 1)) #number of nodes other than the head node\nfor ((i = 1; i <= $worker_num; i++)); do\n  node_i=${nodes_array[$i]}\n  echo \"STARTING WORKER $i at $node_i\"\n  srun --nodes=1 --ntasks=1 -w $node_i --no-container-mount-home --container-image $container \\\n    ray start --address $ip_head --block --disable-usage-stats &\n  sleep 5\ndone\n\nextract_num_gpus() {\n  status_output=$($head_node_srun ray status)\n  if echo \"$status_output\" | grep -q \"GPU\"; then\n    num_gpus=$(echo \"$status_output\" | grep \"GPU\" | awk -F'[/. ]' '{print $4}')\n    echo $num_gpus\n  else\n    echo 0\n  fi\n}\n\nwhile true; do\n  num_gpus=$(extract_num_gpus)\n  if [ \"$num_gpus\" -eq \"$(($num_gpus_per_node * $SLURM_JOB_NUM_NODES))\" ]; then\n    break\n  fi\n  sleep 5\ndone\necho \"Ray cluster is configured correctly!\"\n\n##############################################################################################\nrm -rf $PWD/vllm_stdout.log\n\n# server launch\n#### call your code below, adjust parallelism to your desired value\n$head_node_srun \\\n  env RAY_ADDRESS=$ip_head vllm serve \\\n    $hf_model \\\n    --host $ip \\\n    --port $server_port \\\n    --tensor-parallel-size 4 \\\n    --pipeline-parallel-size 2 \\\n    2>&1 > $PWD/vllm_stdout.log &\n\n# server health check\n$head_node_srun server_health --host $ip --port $server_port --endpoint \"/health\" --process-names \"vllm serve\" --timeout 15 --pid-timeout 60\n\n# client\n# Send requests to the ip and port here\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "closed",
    "created_at": "2025-06-16T17:57:25+00:00",
    "closed_at": "2025-06-18T17:26:17+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19703/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19703"
  },
  {
    "number": 14415,
    "title": "[Bug]: vLLM-0.7.2 reports \"No CUDA GPUs are available\" while vllm-0.6.6.post1 works fine on kuberay under same environment conditions.",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 03-06 19:33:33 __init__.py:190] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.9.21 | packaged by conda-forge | (main, Dec  5 2024, 13:51:40)  [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-5.10.134-18.al8.x86_64-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA A10\nNvidia driver version: 550.144.03\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.1.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      46 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             8\nOn-line CPU(s) list:                0-7\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz\nCPU family:                         6\nModel:                              106\nThread(s) per core:                 2\nCore(s) per socket:                 4\nSocket(s):                          1\nStepping:                           6\nBogoMIPS:                           5799.99\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd arat avx512vbmi pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm arch_capabilities\nHypervisor vendor:                  KVM\nVirtualization type:                full\nL1d cache:                          192 KiB (4 instances)\nL1i cache:                          128 KiB (4 instances)\nL2 cache:                           5 MiB (4 instances)\nL3 cache:                           48 MiB (1 instance)\nNUMA node(s):                       1\nNUMA node0 CPU(s):                  0-7\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Vulnerable\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.49.0\n[pip3] triton==3.1.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-7     0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNVIDIA_VISIBLE_DEVICES=0\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\nNCCL_VERSION=2.21.5-1\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNVIDIA_PRODUCT_NAME=CUDA\nCUDA_VERSION=12.4.1\nLD_LIBRARY_PATH=/tmp/ray/session_2025-03-06_04-45-27_752822_1/runtime_resources/pip/a425849cda8f3a2d8bc88454de4cdc8455c376c1/virtualenv/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n## \ud83d\udc1b Describe the bug\n\n### Description\nWhen deploying Qwen2.5-0.5B model using kuberay with vLLM 0.7.2, encountering \"RuntimeError: No CUDA GPUs are available\" error. However, the same deployment works fine with vLLM 0.6.6.post1 under identical environment conditions.\n\n### Environment Information\n- Container Image: rayproject/ray:2.43.0-py39-cu124\n- vLLM: \n  - Failed version: 0.7.2\n  - Working version: 0.6.6.post1\n- Model: Qwen2.5-0.5B\n\n### Steps to Reproduce\nUsing kuberay to deploy `RayService` with image `rayproject/ray:2.43.0-py39-cu124`, the RayService is:\n```yam\napiVersion: ray.io/v1\nkind: RayService\nmetadata:\n  name: qwen2005-0005b-vllm07\nspec:\n  serveConfigV2: |\n    applications:\n    - name: llm\n      route_prefix: /\n      import_path: latest-serve:model\n      deployments:\n      - name: VLLMDeployment\n        num_replicas: 1\n        ray_actor_options:\n          num_cpus: 4\n      runtime_env:\n        working_dir: \"https://xxx/vllm_script.zip\"\n        pip: \n          - \"vllm==0.7.2\"\n        env_vars:\n          MODEL_ID: \"Qwen/Qwen2.5-0.5B\"\n          TENSOR_PARALLELISM: \"1\"\n          PIPELINE_PARALLELISM: \"1\"\n  rayClusterConfig:\n    headGroupSpec:\n      rayStartParams:\n        dashboard-host: '0.0.0.0'\n      template:\n        spec:\n          containers:\n          - name: ray-head\n            image: rayproject/ray:2.43.0-py39-cu124\n            imagePullPolicy: IfNotPresent\n            resources:\n              limits:\n                cpu: \"8\"\n                memory: \"16Gi\"\n              requests:\n                cpu: \"2\"\n                memory: \"4Gi\"\n            ports:\n            - containerPort: 6379\n              name: gcs-server\n            - containerPort: 8265\n              name: dashboard\n            - containerPort: 10001\n              name: client\n            - containerPort: 8000\n              name: serve\n            env:\n            - name: HUGGING_FACE_HUB_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: hf-secret\n                  key: hf_api_token\n    workerGroupSpecs:\n    - replicas: 1\n      minReplicas: 1\n      maxReplicas: 2\n      groupName: gpu-group\n      rayStartParams: {}\n      template:\n        spec:\n          containers:\n          - name: llm\n            image: rayproject/ray:2.43.0-py39-cu124\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: HUGGING_FACE_HUB_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: hf-secret\n                  key: hf_api_token\n            resources:\n              limits:\n                cpu: \"8\"\n                memory: \"16Gi\"\n                nvidia.com/gpu: \"1\"\n              requests:\n                cpu: \"4\"\n                memory: \"8Gi\"\n                nvidia.com/gpu: \"1\"\n          tolerations:\n            - key: \"nvidia.com/gpu\"\n              operator: \"Exists\"\n              effect: \"NoSchedule\"\n```\nand the `latest-serve.py` in `https://xxx/vllm_script.zip` is from: https://github.com/ray-project/ray/blob/master/doc/source/serve/doc_code/vllm_openai_example.py\n\nthe vLLM deployment code:\n\n```python\nimport os\n\nfrom typing import Dict, Optional, List\nimport logging\n\nfrom fastapi import FastAPI\nfrom starlette.requests import Request\nfrom starlette.responses import StreamingResponse, JSONResponse\n\nfrom ray import serve\n\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\nfrom vllm.entrypoints.openai.cli_args import make_arg_parser\nfrom vllm.entrypoints.openai.protocol import (\n    ChatCompletionRequest,\n    ChatCompletionResponse,\n    ErrorResponse,\n)\nfrom vllm.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom vllm.entrypoints.openai.serving_models import (\n    BaseModelPath,\n    LoRAModulePath,\n    PromptAdapterPath,\n    OpenAIServingModels,\n)\n\nfrom vllm.utils import FlexibleArgumentParser\nfrom vllm.entrypoints.logger import RequestLogger\n\nlogger = logging.getLogger(\"ray.serve\")\n\napp = FastAPI()\n\n\n@serve.deployment(name=\"VLLMDeployment\")\n@serve.ingress(app)\nclass VLLMDeployment:\n    def __init__(\n        self,\n        engine_args: AsyncEngineArgs,\n        response_role: str,\n        lora_modules: Optional[List[LoRAModulePath]] = None,\n        prompt_adapters: Optional[List[PromptAdapterPath]] = None,\n        request_logger: Optional[RequestLogger] = None,\n        chat_template: Optional[str] = None,\n    ):\n        logger.info(f\"Starting with engine args: {engine_args}\")\n        self.openai_serving_chat = None\n        self.engine_args = engine_args\n        self.response_role = response_role\n        self.lora_modules = lora_modules\n        self.prompt_adapters = prompt_adapters\n        self.request_logger = request_logger\n        self.chat_template = chat_template\n        self.engine = AsyncLLMEngine.from_engine_args(engine_args)\n\n    @app.post(\"/v1/chat/completions\")\n    async def create_chat_completion(self, request: ChatCompletionRequest, raw_request: Request):\n        if not self.openai_serving_chat:\n            model_config = await self.engine.get_model_config()\n            models = OpenAIServingModels(\n                self.engine,\n                model_config,\n                [\n                    BaseModelPath(\n                        name=self.engine_args.model, model_path=self.engine_args.model\n                    )\n                ],\n                lora_modules=self.lora_modules,\n                prompt_adapters=self.prompt_adapters,\n            )\n            self.openai_serving_chat = OpenAIServingChat(\n                self.engine,\n                model_config,\n                models,\n                self.response_role,\n                request_logger=self.request_logger,\n                chat_template=self.chat_template,\n                chat_template_content_format=\"auto\",\n            )\n        logger.info(f\"Request: {request}\")\n        generator = await self.openai_serving_chat.create_chat_completion(\n            request, raw_request\n        )\n        if isinstance(generator, ErrorResponse):\n            return JSONResponse(\n                content=generator.model_dump(), status_code=generator.code\n            )\n        if request.stream:\n            return StreamingResponse(content=generator, media_type=\"text/event-stream\")\n        else:\n            assert isinstance(generator, ChatCompletionResponse)\n            return JSONResponse(content=generator.model_dump())\n\n\ndef parse_vllm_args(cli_args: Dict[str, str]):\n    arg_parser = FlexibleArgumentParser(\n        description=\"vLLM OpenAI-Compatible RESTful API server.\"\n    )\n    parser = make_arg_parser(arg_parser)\n    arg_strings = []\n    for key, value in cli_args.items():\n        arg_strings.extend([f\"--{key}\", str(value)])\n    logger.info(arg_strings)\n    parsed_args = parser.parse_args(args=arg_strings)\n    return parsed_args\n\n\n# serve run latest-serve:build_app model=\"Qwen/Qwen2.5-0.5B\" tensor-parallel-size=1 accelerator=\"GPU\"\ndef build_app(cli_args: Dict[str, str]) -> serve.Application:\n    logger.info(\"*\" * 100)\n    if \"accelerator\" in cli_args.keys():\n        accelerator = cli_args.pop(\"accelerator\")\n    else:\n        accelerator = \"GPU\"\n    parsed_args = parse_vllm_args(cli_args)\n\n    engine_args = AsyncEngineArgs.from_cli_args(parsed_args)\n    engine_args.worker_use_ray = True\n\n    tp = engine_args.tensor_parallel_size\n    logger.info(f\"Tensor parallelism = {tp}\")\n    pg_resources = []\n    pg_resources.append({\"CPU\": 4})  # for the deployment replica\n    for i in range(tp):\n        pg_resources.append({\"CPU\": 2, accelerator: 1})  # for the vLLM actors\n\n    return VLLMDeployment.options(\n        placement_group_bundles=pg_resources, placement_group_strategy=\"SPREAD\"\n    ).bind(\n        engine_args,\n        parsed_args.response_role,\n        parsed_args.lora_modules,\n        parsed_args.prompt_adapters,\n        cli_args.get(\"request_logger\"),\n        parsed_args.chat_template,\n    )\n\nmodel = build_app({\n    \"model\": os.environ['MODEL_ID'],\n    \"port\": \"8080\",\n    \"tensor-parallel-size\": os.environ['TENSOR_PARALLELISM'],\n    \"pipeline-parallel-size\": os.environ['PIPELINE_PARALLELISM'],\n    \"max-model-len\": os.environ['MODEL_LEN'],\n    \"gpu-memory-utilization\": os.environ['GPU_MEMORY_UTILIZATION'],\n    \"dtype\": os.environ['DTYPE'],\n    \"kv-cache-dtype\": os.environ['KV_CACHE_DTYPE']\n})\n```\n\nThe exception traceback:\n```\n[36mray::ServeReplica:llm:VLLMDeployment.initialize_and_get_metadata()\u001b[39m (pid=1886, ip=10.58.29.125, actor_id=c3a99f2865a8a727c40545aa01000000, repr=<ray.serve._private.replica.ServeReplica:llm:VLLMDeployment object at 0x7f9966d21550>)\n  File \"/home/ray/anaconda3/lib/python3.9/concurrent/futures/_base.py\", line 446, in result\n    return self.__get_result()\n  File \"/home/ray/anaconda3/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n    raise self._exception\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/serve/_private/replica.py\", line 965, in initialize_and_get_metadata\n    await self._replica_impl.initialize(deployment_config)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/serve/_private/replica.py\", line 694, in initialize\n    raise RuntimeError(traceback.format_exc()) from None\nRuntimeError: Traceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/serve/_private/replica.py\", line 671, in initialize\n    self._user_callable_asgi_app = await asyncio.wrap_future(\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/serve/_private/replica.py\", line 1363, in initialize_callable\n    await self._call_func_or_gen(\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/serve/_private/replica.py\", line 1324, in _call_func_or_gen\n    result = callable(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/serve/api.py\", line 221, in __init__\n    cls.__init__(self, *args, **kwargs)\n  File \"/tmp/ray/session_2025-03-06_22-57-24_631998_1/runtime_resources/working_dir_files/https_aistudio-ant-mpc_oss-cn-zhangjiakou_aliyuncs_com_pengtuo_kuberay_vllm_script/latest-serve.py\", line 57, in __init__\n    self.engine = AsyncLLMEngine.from_engine_args(engine_args)\n  File \"/tmp/ray/session_2025-03-06_22-57-24_631998_1/runtime_resources/pip/a425849cda8f3a2d8bc88454de4cdc8455c376c1/virtualenv/lib/python3.9/site-packages/vllm/engine/async_llm_engine.py\", line 644, in from_engine_args\n    engine = cls(\n  File \"/tmp/ray/session_2025-03-06_22-57-24_631998_1/runtime_resources/pip/a425849cda8f3a2d8bc88454de4cdc8455c376c1/virtualenv/lib/python3.9/site-packages/vllm/engine/async_llm_engine.py\", line 594, in __init__\n    self.engine = self._engine_class(*args, **kwargs)\n  File \"/tmp/ray/session_2025-03-06_22-57-24_631998_1/runtime_resources/pip/a425849cda8f3a2d8bc88454de4cdc8455c376c1/virtualenv/lib/python3.9/site-packages/vllm/engine/async_llm_engine.py\", line 267, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/tmp/ray/session_2025-03-06_22-57-24_631998_1/runtime_resources/pip/a425849cda8f3a2d8bc88454de4cdc8455c376c1/virtualenv/lib/python3.9/site-packages/vllm/engine/llm_engine.py\", line 273, in __init__\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n  File \"/tmp/ray/session_2025-03-06_22-57-24_631998_1/runtime_resources/pip/a425849cda8f3a2d8bc88454de4cdc8455c376c1/virtualenv/lib/python3.9/site-packages/vllm/executor/executor_base.py\", line 51, in __init__\n    self._init_executor()\n  File \"/tmp/ray/session_2025-03-06_22-57-24_631998_1/runtime_resources/pip/a425849cda8f3a2d8bc88454de4cdc8455c376c1/virtualenv/lib/python3.9/site-packages/vllm/executor/uniproc_executor.py\", line 41, in _init_executor\n    self.collective_rpc(\"init_device\")\n  File \"/tmp/ray/session_2025-03-06_22-57-24_631998_1/runtime_resources/pip/a425849cda8f3a2d8bc88454de4cdc8455c376c1/virtualenv/lib/python3.9/site-packages/vllm/executor/uniproc_executor.py\", line 51, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n  File \"/tmp/ray/session_2025-03-06_22-57-24_631998_1/runtime_resources/pip/a425849cda8f3a2d8bc88454de4cdc8455c376c1/virtualenv/lib/python3.9/site-packages/vllm/utils.py\", line 2220, in run_method\n    return func(*args, **kwargs)\n  File \"/tmp/ray/session_2025-03-06_22-57-24_631998_1/runtime_resources/pip/a425849cda8f3a2d8bc88454de4cdc8455c376c1/virtualenv/lib/python3.9/site-packages/vllm/worker/worker.py\", line 155, in init_device\n    torch.cuda.set_device(self.device)\n  File \"/tmp/ray/session_2025-03-06_22-57-24_631998_1/runtime_resources/pip/a425849cda8f3a2d8bc88454de4cdc8455c376c1/virtualenv/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n    torch._C._cuda_setDevice(device)\n  File \"/tmp/ray/session_2025-03-06_22-57-24_631998_1/runtime_resources/pip/a425849cda8f3a2d8bc88454de4cdc8455c376c1/virtualenv/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 319, in _lazy_init\n    torch._C._cuda_init()\nRuntimeError: No CUDA GPUs are available\n```\n\n### Related issue\nI've been searching for solutions and found two issues that match my symptoms, but the solutions provided in those issues don't work in my case:\nhttps://github.com/vllm-project/vllm/issues/6896\nhttps://github.com/ray-project/ray/issues/50275\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-07T07:12:23+00:00",
    "closed_at": "2025-07-08T02:14:11+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14415/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14415"
  },
  {
    "number": 19543,
    "title": "[Usage]: Ray connection timeout",
    "body": "### Your current environment\n\ndocker run -d \\\n> --entrypoint /bin/bash \\\n> --network host \\\n        -v /data:/data \\\n        vllm:0.7.3-python3.11-cuda12.4-debian12 \\\n    -c \"ray start --block --address=127.0.0.1:6379\"> --runtime=nvidia \\\n> --name workernode \\\n> --shm-size 10.24g \\\n> --gpus \"device=3\" \\\n> -e VLLM_HOST_IP=127.0.0.1 \\\n> -e MASTER_ADDR=127.0.0.1 \\\n> -e MASTER_PORT=29500 \\\n> -v /data:/data \\\n> harbor.inspur.local/ai-group/vllm:0.7.3-python3.11-cuda12.4-debian12 \\\n>     -c \"ray start --block --address=127.0.0.1:6379\"\ncb2dc97286d9e65858472bd6ce020f2694718b012dd6b2d89782fbf1e41eee05\n(base) [root@nvidia-h20-1 ~]# docker logs -f workernode\n[2025-06-13 01:49:41,730 E 1 1] gcs_rpc_client.h:179: Failed to connect to GCS at address 10.108.0.184:6379 within 5 seconds.\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "ray"
    ],
    "state": "open",
    "created_at": "2025-06-12T09:54:23+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19543/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19543"
  },
  {
    "number": 12572,
    "title": "[Bug]: Ray/vLLM RuntimeError: HIP error: invalid device ordinal",
    "body": "### Your current environment\n\n\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 01-30 09:13:33 __init__.py:187] No platform detected, vLLM is running on UnspecifiedPlatform\nCollecting environment information...\nPyTorch version: 2.5.0+rocm6.2\nIs debug build: False\nCUDA used to build PyTorch: N/A\nROCM used to build PyTorch: 6.2.41133-dd7f95766\n\nOS: Red Hat Enterprise Linux release 8.10 (Ootpa) (x86_64)\nGCC version: (GCC) 8.5.0 20210514 (Red Hat 8.5.0-22)\nClang version: 17.0.6 (Red Hat 17.0.6-1.module+el8.10.0+20808+e12784c0)\nCMake version: version 3.26.5\nLibc version: glibc-2.28\n\nPython version: 3.11.7 (main, Jun 17 2024, 15:34:21) [GCC 10.3.1 20210422 (Red Hat 10.3.1-1)] (64-bit runtime)\nPython platform: Linux-4.18.0-553.22.1.el8_10.x86_64-x86_64-with-glibc2.28\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: AMD Instinct MI250X (gfx90a:sramecc+:xnack-)\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\nHIP runtime version: 6.2.41133\nMIOpen runtime version: 3.2.0\nIs XNNPACK available: True\n\nCPU:\nArchitecture:        x86_64\nCPU op-mode(s):      32-bit, 64-bit\nByte Order:          Little Endian\nCPU(s):              128\nOn-line CPU(s) list: 0-127\nThread(s) per core:  2\nCore(s) per socket:  64\nSocket(s):           1\nNUMA node(s):        4\nVendor ID:           AuthenticAMD\nCPU family:          25\nModel:               48\nModel name:          AMD EPYC 7A53 64-Core Processor\nStepping:            1\nCPU MHz:             2000.000\nCPU max MHz:         3541.0149\nCPU min MHz:         1500.0000\nBogoMIPS:            3992.85\nVirtualization:      AMD-V\nL1d cache:           32K\nL1i cache:           32K\nL2 cache:            512K\nL3 cache:            32768K\nNUMA node0 CPU(s):   0-15,64-79\nNUMA node1 CPU(s):   16-31,80-95\nNUMA node2 CPU(s):   32-47,96-111\nNUMA node3 CPU(s):   48-63,112-127\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] pytorch-triton-rocm==3.1.0\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.5.0+rocm6.2\n[pip3] torchaudio==2.5.0+rocm6.2\n[pip3] torchvision==0.20.0+rocm6.2\n[pip3] transformers==4.48.1\n[conda] Could not collect\nROCM Version: 6.2.41134-65d174c3e\nNeuron SDK Version: N/A\nvLLM Version: 0.7.0\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n============================ ROCm System Management Interface ============================\n================================ Weight between two GPUs =================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \nGPU0   0            15           15           30           30           30           15           30           \nGPU1   15           0            30           15           30           15           30           45           \nGPU2   15           30           0            15           15           30           30           30           \nGPU3   30           15           15           0            30           45           30           15           \nGPU4   30           30           15           30           0            15           15           30           \nGPU5   30           15           30           45           15           0            30           15           \nGPU6   15           30           30           30           15           30           0            15           \nGPU7   30           45           30           15           30           15           15           0            \n\n================================= Hops between two GPUs ==================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \nGPU0   0            1            1            1            1            1            1            1            \nGPU1   1            0            1            1            1            1            1            1            \nGPU2   1            1            0            1            1            1            1            1            \nGPU3   1            1            1            0            1            1            1            1            \nGPU4   1            1            1            1            0            1            1            1            \nGPU5   1            1            1            1            1            0            1            1            \nGPU6   1            1            1            1            1            1            0            1            \nGPU7   1            1            1            1            1            1            1            0            \n\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         \nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         \nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         \nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         \nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         \nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            \n\n======================================= Numa Nodes =======================================\nGPU[0]          : (Topology) Numa Node: 3\nGPU[0]          : (Topology) Numa Affinity: 3\nGPU[1]          : (Topology) Numa Node: 3\nGPU[1]          : (Topology) Numa Affinity: 3\nGPU[2]          : (Topology) Numa Node: 1\nGPU[2]          : (Topology) Numa Affinity: 1\nGPU[3]          : (Topology) Numa Node: 1\nGPU[3]          : (Topology) Numa Affinity: 1\nGPU[4]          : (Topology) Numa Node: 0\nGPU[4]          : (Topology) Numa Affinity: 0\nGPU[5]          : (Topology) Numa Node: 0\nGPU[5]          : (Topology) Numa Affinity: 0\nGPU[6]          : (Topology) Numa Node: 2\nGPU[6]          : (Topology) Numa Affinity: 2\nGPU[7]          : (Topology) Numa Node: 2\nGPU[7]          : (Topology) Numa Affinity: 2\n================================== End of ROCm SMI Log ===================================\n\nLD_LIBRARY_PATH=/lus/home/BCINES/dci/malaboeuf/repository.backedup/Inferencing/python_environment/lib/python3.11/site-packages/cv2/../../lib64:/opt/cray/pe/mpich/8.1.30/gtl/lib:/opt/cray/pe/papi/7.1.0.2/lib64:/opt/cray/libfabric/1.20.1/lib64\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n</details>\n\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nAssume a cluster of MI250X or MI300X, 8 per node, many nodes. I allocate the nodes and start a ray cluster on them. Ray status is ok, all is fine.\n\nThen I connect to a compute node and start vllm like so:\n\n```\n$ vllm serve <model> \\\n    --distributed-executor-backend=\"ray\" \\\n    --device=\"cuda\" \\\n    --task=\"generate\" \\\n    --tensor-parallel-size 2 \\\n    --pipeline-parallel-size 1\n``` \n\nThis should request 2 gpus on one node and start serving. For that, Ray will allocate two GPUs and \"start\" 1 rank, not 2 because we seem to be reusing the original vllm rank for that purpose (that causes problems too but that for a future issue/feature request). \n\nThen it gets messy. The original vllm rank, the one we started using the command above, will operate with the following environment variable:\n```\nCUDA_VISIBLE_DEVICES=\"0,1\"\n```\n\nThat is, we restrict the visibility of this rank to 2 GPUs (represented by identifier 0 and 1) of a node.\n\nNow, the second rank will see the following env variables:\n```\nROCR_VISIBLE_DEVICES=\"1\"\nCUDA_VISIBLE_DEVICES=\"0,1\"\n```\n\nThis is obviously a cause for concern, as documented on this HPC site's documentation. https://dci.dci-gitlab.cines.fr/webextranet/software_stack/libraries/index.html#id8, paraphrasing:\n```\nexport ROCR_VISIBLE_DEVICES=<N>\n\nSame as ${HIP_VISIBLE_DEVICES} [or CUDA_VISIBLE_DEVICES] but works at the HSA level. Recommend for highly tunned applications. It restricts the visibility of ${HIP_VISIBLE_DEVICES} and ${CUDA_VISIBLE_DEVICES}. So ROCR_VISIBLE_DEVICES=2 HIP_VISIBLE_DEVICES=2 is same as hiding all the devices. HIP_VISIBLE_DEVICES=\"0,1,2\" HIP_VISIBLE_DEVICES=\"2\" would only show the third device.\n```\n\nThat is to say, ROCR_VISIBLE_DEVICES restricts first. Then we apply CUDA_VISIBLE_DEVICES/HIP_VISIBLE_DEVICES (both work on AMD though I do not know what is the behavior if both are set at the same time!).\n\nSo if we have a set of 8 gpus per node (has I have), we restrict ROCR_VISIBLE_DEVICES=\"1\" and now work with a set of only 1 gpu. The gpu is re identified as zero, the only gpu visible. So if we do `torch.cuda.set_device(torch.device(f\"cuda:0\"))`, we are in fact dealing with GPU 1 of the node.\n\nThis causes a big issue (basically, vllm is unusable as is) because it turns out that vllm expect all gpus to be visible and binds a gpu based on the local rank. Which is perfectly fine, except if ray does this visibility restriction. See: https://github.com/vllm-project/vllm/blob/main/vllm/worker/worker.py#L153\n\nAfaik, we can't just del ROCR_VISIBLE_DEVICES before `torch.cuda.set_device` because it is too late, the HIP runtime has initialized.\n\nAt some point we had that issue: https://github.com/vllm-project/vllm/issues/4131 which is now closed, and I'm not sure if it is related.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-30T08:35:34+00:00",
    "closed_at": "2025-06-11T02:14:01+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12572/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12572"
  },
  {
    "number": 13682,
    "title": "[Bug]: Speculative Decoding doesn't work with Ray compiled DAG and SPMD",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI'm trying to run DeepSeek-R1 using Ray's compiled DAG, SPMD, and enabled speculative decoding. I get an error after sending a request.\n\n```\nERROR 02-21 02:30:26 async_llm_engine.py:68] Engine background task failed\nERROR 02-21 02:30:26 async_llm_engine.py:68] Traceback (most recent call last):\nERROR 02-21 02:30:26 async_llm_engine.py:68] File \"/home/ray/anaconda3/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 58, in _log_task_completion\nERROR 02-21 02:30:26 async_llm_engine.py:68] return_value = task.result()\nERROR 02-21 02:30:26 async_llm_engine.py:68] ^^^^^^^^^^^^^\nERROR 02-21 02:30:26 async_llm_engine.py:68] File \"/home/ray/anaconda3/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 825, in run_engine_loop\nERROR 02-21 02:30:26 async_llm_engine.py:68] result = task.result()\nERROR 02-21 02:30:26 async_llm_engine.py:68] ^^^^^^^^^^^^^\nERROR 02-21 02:30:26 async_llm_engine.py:68] File \"/home/ray/anaconda3/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 748, in engine_step\nERROR 02-21 02:30:26 async_llm_engine.py:68] request_outputs = await self.engine.step_async(virtual_engine)\nERROR 02-21 02:30:26 async_llm_engine.py:68] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 02:30:26 async_llm_engine.py:68] File \"/home/ray/anaconda3/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 353, in step_async\nERROR 02-21 02:30:26 async_llm_engine.py:68] outputs = await self.model_executor.execute_model_async(\nERROR 02-21 02:30:26 async_llm_engine.py:68] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 02:30:26 async_llm_engine.py:68] File \"/home/ray/anaconda3/lib/python3.12/site-packages/vllm/executor/ray_distributed_executor.py\", line 590, in execute_model_async\nERROR 02-21 02:30:26 async_llm_engine.py:68] output = await dag_future[0]\nERROR 02-21 02:30:26 async_llm_engine.py:68] ^^^^^^^^^^^^^^^^^^^\nERROR 02-21 02:30:26 async_llm_engine.py:68] File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/experimental/compiled_dag_ref.py\", line 174, in __await__\nERROR 02-21 02:30:26 async_llm_engine.py:68] return _process_return_vals(return_vals, True)\nERROR 02-21 02:30:26 async_llm_engine.py:68] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 02:30:26 async_llm_engine.py:68] File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/experimental/compiled_dag_ref.py\", line 22, in _process_return_vals\nERROR 02-21 02:30:26 async_llm_engine.py:68] raise val.as_instanceof_cause()\nERROR 02-21 02:30:26 async_llm_engine.py:68] ray.exceptions.RayTaskError(AttributeError): [36mray::RayWorkerWrapper.__ray_call__() [39m (pid=9898, ip=10.1.3.101)\nERROR 02-21 02:30:26 async_llm_engine.py:68] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 02:30:26 async_llm_engine.py:68] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 02:30:26 async_llm_engine.py:68] File \"/home/ray/anaconda3/lib/python3.12/site-packages/vllm/executor/ray_utils.py\", line 94, in execute_model_spmd\nERROR 02-21 02:30:26 async_llm_engine.py:68] output = self.worker._execute_model_spmd(execute_model_req,\nERROR 02-21 02:30:26 async_llm_engine.py:68] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 02:30:26 async_llm_engine.py:68] AttributeError: 'SpecDecodeWorker' object has no attribute '_execute_model_spmd'\nERROR 02-21 02:30:26 serving_chat.py:665] Error in chat completion stream generator.\nERROR 02-21 02:30:26 serving_chat.py:665] Traceback (most recent call last):\nERROR 02-21 02:30:26 serving_chat.py:665] File \"/home/ray/anaconda3/lib/python3.12/site-packages/vllm/entrypoints/openai/serving_chat.py\", line 363, in chat_completion_stream_generator\nERROR 02-21 02:30:26 serving_chat.py:665] async for res in result_generator:\nERROR 02-21 02:30:26 serving_chat.py:665] File \"/home/ray/anaconda3/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 1006, in generate\nERROR 02-21 02:30:26 serving_chat.py:665] async for output in await self.add_request(\nERROR 02-21 02:30:26 serving_chat.py:665] File \"/home/ray/anaconda3/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 116, in generator\nERROR 02-21 02:30:26 serving_chat.py:665] raise result\nERROR 02-21 02:30:26 serving_chat.py:665] File \"/home/ray/anaconda3/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 58, in _log_task_completion\nERROR 02-21 02:30:26 serving_chat.py:665] return_value = task.result()\nERROR 02-21 02:30:26 serving_chat.py:665] ^^^^^^^^^^^^^\nERROR 02-21 02:30:26 serving_chat.py:665] File \"/home/ray/anaconda3/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 825, in run_engine_loop\nERROR 02-21 02:30:26 serving_chat.py:665] result = task.result()\nERROR 02-21 02:30:26 serving_chat.py:665] ^^^^^^^^^^^^^\nERROR 02-21 02:30:26 serving_chat.py:665] File \"/home/ray/anaconda3/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 748, in engine_step\nERROR 02-21 02:30:26 serving_chat.py:665] request_outputs = await self.engine.step_async(virtual_engine)\nERROR 02-21 02:30:26 serving_chat.py:665] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 02:30:26 serving_chat.py:665] File \"/home/ray/anaconda3/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 353, in step_async\nERROR 02-21 02:30:26 serving_chat.py:665] outputs = await self.model_executor.execute_model_async(\nERROR 02-21 02:30:26 serving_chat.py:665] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 02:30:26 serving_chat.py:665] File \"/home/ray/anaconda3/lib/python3.12/site-packages/vllm/executor/ray_distributed_executor.py\", line 590, in execute_model_async\nERROR 02-21 02:30:26 serving_chat.py:665] output = await dag_future[0]\nERROR 02-21 02:30:26 serving_chat.py:665] ^^^^^^^^^^^^^^^^^^^\nERROR 02-21 02:30:26 serving_chat.py:665] File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/experimental/compiled_dag_ref.py\", line 174, in __await__\nERROR 02-21 02:30:26 serving_chat.py:665] return _process_return_vals(return_vals, True)\nERROR 02-21 02:30:26 serving_chat.py:665] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 02:30:26 serving_chat.py:665] File \"/home/ray/anaconda3/lib/python3.12/site-packages/ray/experimental/compiled_dag_ref.py\", line 22, in _process_return_vals\nERROR 02-21 02:30:26 serving_chat.py:665] raise val.as_instanceof_cause()\nERROR 02-21 02:30:26 serving_chat.py:665] ray.exceptions.RayTaskError(AttributeError): [36mray::RayWorkerWrapper.__ray_call__() [39m (pid=9898, ip=10.1.3.101)\nERROR 02-21 02:30:26 serving_chat.py:665] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 02:30:26 serving_chat.py:665] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 02:30:26 serving_chat.py:665] File \"/home/ray/anaconda3/lib/python3.12/site-packages/vllm/executor/ray_utils.py\", line 94, in execute_model_spmd\nERROR 02-21 02:30:26 serving_chat.py:665] output = self.worker._execute_model_spmd(execute_model_req,\nERROR 02-21 02:30:26 serving_chat.py:665] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-21 02:30:26 serving_chat.py:665] AttributeError: 'SpecDecodeWorker' object has no attribute '_execute_model_spmd'\n```\n\nvLLM version: `0.7.3`\nRay: `2.40.0`\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "ray",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-21T18:24:45+00:00",
    "closed_at": "2025-06-29T02:14:55+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13682/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13682"
  },
  {
    "number": 15514,
    "title": "[Usage]: distributed using ray, how to get worker runtime error log",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\n```\n\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "ray"
    ],
    "state": "closed",
    "created_at": "2025-03-26T01:58:53+00:00",
    "closed_at": "2025-03-31T15:15:28+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15514/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15514"
  },
  {
    "number": 12058,
    "title": "[Usage]: Running Tensor Parallel on TPUs on Ray Cluster",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\nThe output of `python collect_env.py`\n(test_hf_qwen pid=17527, ip=10.130.4.26) Environment Information:\n(test_hf_qwen pid=17527, ip=10.130.4.26) Collecting environment information...\n(test_hf_qwen pid=17527, ip=10.130.4.26) PyTorch version: 2.6.0.dev20241126+cpu\n(test_hf_qwen pid=17527, ip=10.130.4.26) Is debug build: False\n(test_hf_qwen pid=17527, ip=10.130.4.26) CUDA used to build PyTorch: None\n(test_hf_qwen pid=17527, ip=10.130.4.26) ROCM used to build PyTorch: N/A\n(test_hf_qwen pid=17527, ip=10.130.4.26) \n(test_hf_qwen pid=17527, ip=10.130.4.26) OS: Ubuntu 22.04.4 LTS (x86_64)\n(test_hf_qwen pid=17527, ip=10.130.4.26) GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n(test_hf_qwen pid=17527, ip=10.130.4.26) Clang version: 14.0.0-1ubuntu1.1\n(test_hf_qwen pid=17527, ip=10.130.4.26) CMake version: version 3.31.2\n(test_hf_qwen pid=17527, ip=10.130.4.26) Libc version: glibc-2.35\n(test_hf_qwen pid=17527, ip=10.130.4.26) \n(test_hf_qwen pid=17527, ip=10.130.4.26) Python version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\n(test_hf_qwen pid=17527, ip=10.130.4.26) Python platform: Linux-5.19.0-1022-gcp-x86_64-with-glibc2.35\n(test_hf_qwen pid=17527, ip=10.130.4.26) Is CUDA available: False\n(test_hf_qwen pid=17527, ip=10.130.4.26) CUDA runtime version: No CUDA\n(test_hf_qwen pid=17527, ip=10.130.4.26) CUDA_MODULE_LOADING set to: N/A\n(test_hf_qwen pid=17527, ip=10.130.4.26) GPU models and configuration: No CUDA\n(test_hf_qwen pid=17527, ip=10.130.4.26) Nvidia driver version: No CUDA\n(test_hf_qwen pid=17527, ip=10.130.4.26) cuDNN version: No CUDA\n(test_hf_qwen pid=17527, ip=10.130.4.26) HIP runtime version: N/A\n(test_hf_qwen pid=17527, ip=10.130.4.26) MIOpen runtime version: N/A\n(test_hf_qwen pid=17527, ip=10.130.4.26) Is XNNPACK available: True\n(test_hf_qwen pid=17527, ip=10.130.4.26) \n(test_hf_qwen pid=17527, ip=10.130.4.26) CPU:\n(test_hf_qwen pid=17527, ip=10.130.4.26) Architecture:                    x86_64\n(test_hf_qwen pid=17527, ip=10.130.4.26) CPU op-mode(s):                  32-bit, 64-bit\n(test_hf_qwen pid=17527, ip=10.130.4.26) Address sizes:                   48 bits physical, 48 bits virtual\n(test_hf_qwen pid=17527, ip=10.130.4.26) Byte Order:                      Little Endian\n(test_hf_qwen pid=17527, ip=10.130.4.26) CPU(s):                          240\n(test_hf_qwen pid=17527, ip=10.130.4.26) On-line CPU(s) list:             0-239\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vendor ID:                       AuthenticAMD\n(test_hf_qwen pid=17527, ip=10.130.4.26) Model name:                      AMD EPYC 7B12\n(test_hf_qwen pid=17527, ip=10.130.4.26) CPU family:                      23\n(test_hf_qwen pid=17527, ip=10.130.4.26) Model:                           49\n(test_hf_qwen pid=17527, ip=10.130.4.26) Thread(s) per core:              2\n(test_hf_qwen pid=17527, ip=10.130.4.26) Core(s) per socket:              60\n(test_hf_qwen pid=17527, ip=10.130.4.26) Socket(s):                       2\n(test_hf_qwen pid=17527, ip=10.130.4.26) Stepping:                        0\n(test_hf_qwen pid=17527, ip=10.130.4.26) BogoMIPS:                        4499.99\n(test_hf_qwen pid=17527, ip=10.130.4.26) Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid\n(test_hf_qwen pid=17527, ip=10.130.4.26) Hypervisor vendor:               KVM\n(test_hf_qwen pid=17527, ip=10.130.4.26) Virtualization type:             full\n(test_hf_qwen pid=17527, ip=10.130.4.26) L1d cache:                       3.8 MiB (120 instances)\n(test_hf_qwen pid=17527, ip=10.130.4.26) L1i cache:                       3.8 MiB (120 instances)\n(test_hf_qwen pid=17527, ip=10.130.4.26) L2 cache:                        60 MiB (120 instances)\n(test_hf_qwen pid=17527, ip=10.130.4.26) L3 cache:                        480 MiB (30 instances)\n(test_hf_qwen pid=17527, ip=10.130.4.26) NUMA node(s):                    2\n(test_hf_qwen pid=17527, ip=10.130.4.26) NUMA node0 CPU(s):               0-59,120-179\n(test_hf_qwen pid=17527, ip=10.130.4.26) NUMA node1 CPU(s):               60-119,180-239\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Itlb multihit:     Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability L1tf:              Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Mds:               Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Meltdown:          Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Mmio stale data:   Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Retbleed:          Mitigation; untrained return thunk; SMT enabled with STIBP protection\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Srbds:             Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Tsx async abort:   Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) \n(test_hf_qwen pid=17527, ip=10.130.4.26) Versions of relevant libraries:\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] mypy-extensions==1.0.0\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] numpy==1.26.4\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cublas-cu12==12.4.5.8\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cuda-cupti-cu12==12.4.127\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cuda-runtime-cu12==12.4.127\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cudnn-cu12==9.1.0.70\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cufft-cu12==11.2.1.3\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-curand-cu12==10.3.5.147\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cusolver-cu12==11.6.1.9\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cusparse-cu12==12.3.1.170\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-nccl-cu12==2.21.5\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-nvjitlink-cu12==12.4.127\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-nvtx-cu12==12.4.127\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] pyzmq==26.2.0\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] torch==2.6.0.dev20241126+cpu\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] torch-xla==2.6.0+git39e67b5\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] torchvision==0.20.0.dev20241126+cpu\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] transformers==4.47.1\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] triton==3.1.0\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] numpy                     1.26.4                   pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] pyzmq                     26.2.0                   pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] torch                     2.6.0.dev20241126+cpu          pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] torch-xla                 2.6.0+git39e67b5          pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] torchvision               0.20.0.dev20241126+cpu          pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] transformers              4.47.1                   pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] triton                    3.1.0                    pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) ROCM Version: Could not collect\n(test_hf_qwen pid=17527, ip=10.130.4.26) Neuron SDK Version: N/A\n(test_hf_qwen pid=17527, ip=10.130.4.26) vLLM Version: N/A (dev)\n(test_hf_qwen pid=17527, ip=10.130.4.26) vLLM Build Flags:\n(test_hf_qwen pid=17527, ip=10.130.4.26) CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\n(test_hf_qwen pid=17527, ip=10.130.4.26) GPU Topology:\n(test_hf_qwen pid=17527, ip=10.130.4.26) Could not collect\n(test_hf_qwen pid=17527, ip=10.130.4.26) \n(test_hf_qwen pid=17527, ip=10.130.4.26) LD_LIBRARY_PATH=/home/ray/anaconda3/lib/python3.11/site-packages/cv2/../../lib64:/home/ray/anaconda3/lib/python3.11/site-packages/cv2/../../lib64::/usr/lib/x86_64-linux-gnu/:/home/ray/anaconda3/lib\n(test_hf_qwen pid=17527, ip=10.130.4.26) OMP_NUM_THREADS=1\n(test_hf_qwen pid=17527, ip=10.130.4.26) CUDA_VISIBLE_DEVICES=\n(test_hf_qwen pid=17527, ip=10.130.4.26) CUDA_VISIBLE_DEVICES=\n(test_hf_qwen pid=17527, ip=10.130.4.26) TORCHINDUCTOR_COMPILE_THREADS=1\n(test_hf_qwen pid=17527, ip=10.130.4.26) TORCHINDUCTOR_CACHE_DIR=/tmp/torchinductor_ray\n```\n\n\n### How would you like to use vllm\n\nI want to run tensor-parallel inference using TPUs in a ray cluster. It seems like the Ray cluster picks up the accelerator that we need but then when vllm tries to initialize the ray cluster, it doesn't know that, so it doesn't reuse the TPUs that the cluster has already picked up. I was wondering how people would implement this? Thanks!\n\n\nCode:\n```\nfrom vllm import LLM\n\n@ray.remote(resources={\"TPU\": 4, \"TPU-v4-8-head\": 1})\ndef test():\n    llm = LLM(model=Qwen/Qwen2.5-7B-Instruct, enforce_eager=True, max_model_len=8192, tensor_parallel_size=4)\n```\n\nError:\n```\n(test_hf pid=1616, ip=10.130.0.8) INFO 01-15 09:04:53 config.py:510] This model supports multiple tasks: {'generate', 'score', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.\n(test_hf pid=1616, ip=10.130.0.8) Connecting to existing Ray cluster at address: 10.130.2.110:6379...\n(test_hf pid=1616, ip=10.130.0.8) Calling ray.init() again after it has already been called.\nTraceback (most recent call last):\n  File \"/tmp/ray/session_2025-01-09_10-24-52_724484_545/runtime_resources/working_dir_files/_ray_pkg_0e6ac5e67e3f89c8/experiments/llama_test.py\", line 75, in <module>\n    raise e\n  File \"/tmp/ray/session_2025-01-09_10-24-52_724484_545/runtime_resources/working_dir_files/_ray_pkg_0e6ac5e67e3f89c8/experiments/llama_test.py\", line 73, in <module>\n    ray.get(ref)\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n    raise value.as_instanceof_cause()\nray.exceptions.RayTaskError(ValueError): ray::test_hf() (pid=1616, ip=10.130.0.8)\n  File \"/tmp/ray/session_2025-01-09_10-24-52_724484_545/runtime_resources/working_dir_files/_ray_pkg_0e6ac5e67e3f89c8/experiments/llama_test.py\", line 59, in test_hf\n    classifier = AutoClassifier.from_model_path(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ray/session_2025-01-09_10-24-52_724484_545/runtime_resources/working_dir_files/_ray_pkg_0e6ac5e67e3f89c8/marin/processing/classification/classifier.py\", line 281, in from_model_path\n    return cls._MODEL_NAME_TO_CLS_DICT[key](model_name_or_path, attribute_name, model_type, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ray/session_2025-01-09_10-24-52_724484_545/runtime_resources/working_dir_files/_ray_pkg_0e6ac5e67e3f89c8/marin/processing/classification/classifier.py\", line 213, in __init__\n    self.llm = LLM(model=model_name, enforce_eager=True, max_model_len=8192, tensor_parallel_size=4)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm-0.6.6.post1/vllm/utils.py\", line 986, in inner\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm-0.6.6.post1/vllm/entrypoints/llm.py\", line 230, in __init__\n    self.llm_engine = self.engine_class.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm-0.6.6.post1/vllm/engine/llm_engine.py\", line 515, in from_engine_args\n    executor_class = cls._get_executor_cls(engine_config)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm-0.6.6.post1/vllm/engine/llm_engine.py\", line 453, in _get_executor_cls\n    initialize_ray_cluster(engine_config.parallel_config)\n  File \"/opt/vllm/vllm-0.6.6.post1/vllm/executor/ray_utils.py\", line 300, in initialize_ray_cluster\n    raise ValueError(\nValueError: Current node has no TPU available. current_node_resource={'ray-marin-us-central2-worker-2c310153-tpu': 1.0, 'CPU': 118.0, 'memory': 328490690150.0, 'object_store_memory': 32641751449.0, 'accelerator_type:TPU-V4': 1.0, 'node:10.130.0.8': 1.0}. vLLM engine cannot start without TPU. Make sure you have at least 1 TPU available in a node current_node_id='70354097fbebce320701224b766747b2c30936f9c1edf1d930d7723b' current_ip='10.130.0.8'.\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "tpu",
      "ray"
    ],
    "state": "closed",
    "created_at": "2025-01-14T21:32:12+00:00",
    "closed_at": "2025-01-24T05:41:50+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12058/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12058"
  },
  {
    "number": 12983,
    "title": "[Bug]: ValueError: Ray does not allocate any GPUs on the driver node. Consider adjusting the Ray placement group or running the driver on a GPU node.",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\nvllm\u5bb9\u5668\uff1a0.7.2\n\u542f\u52a8\u811a\u672c\uff1a\nbash run_cluster.sh \\\n    docker-hub.dahuatech.com/vllm/vllm-openai:v0.7.2 \\\n    10.12.167.20 \\\n    --head \\\n    /root/wangjianqiang/deepseek/DeepSeek-R1/DeepSeek-R1/ \\\n    -e VLLM_HOST_IP=$(hostname -I | awk '{print $1}')/ \\\n\t-e \"GLOO_SOCKET_IFNAME=ens121f0\"/ \\\n    -e \"NCCL_SOCKET_IFNAME=ens121f0\"/ \\\n\t-v /root/wangjianqiang/deepseek/DeepSeek-R1/:/root/deepseek_r1/\n\nbash run_cluster.sh \\\n    docker-hub.dahuatech.com/vllm/vllm-openai:v0.7.2 \\\n    10.12.167.20 \\\n    --worker \\\n    /root/wangjianqiang/deepseek/DeepSeek-R1/DeepSeek-R1/ \\\n    -e VLLM_HOST_IP=$(hostname -I | awk '{print $1}')/ \\\n\t-e \"GLOO_SOCKET_IFNAME=ens121f0\"/ \\\n    -e \"NCCL_SOCKET_IFNAME=ens121f0\"/ \\\n\t-v /root/deepseek_r1/:/root/deepseek_r1/\n\n\u542f\u52a8\u547d\u4ee4\uff1a\n\nroot@admin:~/deepseek_r1/DeepSeek-R1# VLLM_HOST_IP=$(hostname -I | awk '{print $1}')\nroot@admin:~/deepseek_r1/DeepSeek-R1# export VLLM_HOST_IP\nroot@admin:~/deepseek_r1/DeepSeek-R1# NCCL_DEBUG=TRACE vllm serve /root/deepseek_r1/DeepSeek-R1 --tensor-parallel-size 16 --trust-remote-code\n\n\u51fa\u73b0\u4e86\u5982\u4e0b\u7684\u9519\u8bef\uff1a\n\nERROR 02-09 02:31:11 engine.py:389] Ray does not allocate any GPUs on the driver node. Consider adjusting the Ray placement group or running the driver on a GPU node.\nERROR 02-09 02:31:11 engine.py:389] Traceback (most recent call last):\nERROR 02-09 02:31:11 engine.py:389]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 380, in run_mp_engine\nERROR 02-09 02:31:11 engine.py:389]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\nERROR 02-09 02:31:11 engine.py:389]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-09 02:31:11 engine.py:389]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 123, in from_engine_args\nERROR 02-09 02:31:11 engine.py:389]     return cls(ipc_path=ipc_path,\nERROR 02-09 02:31:11 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-09 02:31:11 engine.py:389]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 75, in __init__\nERROR 02-09 02:31:11 engine.py:389]     self.engine = LLMEngine(*args, **kwargs)\nERROR 02-09 02:31:11 engine.py:389]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-09 02:31:11 engine.py:389]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 273, in __init__\nERROR 02-09 02:31:11 engine.py:389]     self.model_executor = executor_class(vllm_config=vllm_config, )\nERROR 02-09 02:31:11 engine.py:389]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-09 02:31:11 engine.py:389]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 262, in __init__\nERROR 02-09 02:31:11 engine.py:389]     super().__init__(*args, **kwargs)\nERROR 02-09 02:31:11 engine.py:389]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 51, in __init__\nERROR 02-09 02:31:11 engine.py:389]     self._init_executor()\nERROR 02-09 02:31:11 engine.py:389]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py\", line 90, in _init_executor\nERROR 02-09 02:31:11 engine.py:389]     self._init_workers_ray(placement_group)\nERROR 02-09 02:31:11 engine.py:389]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py\", line 227, in _init_workers_ray\nERROR 02-09 02:31:11 engine.py:389]     raise ValueError(\nERROR 02-09 02:31:11 engine.py:389] ValueError: Ray does not allocate any GPUs on the driver node. Consider adjusting the Ray placement group or running the driver on a GPU node.\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 391, in run_mp_engine\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 380, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 123, in from_engine_args\n    return cls(ipc_path=ipc_path,\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 75, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 273, in __init__\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 262, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 51, in __init__\n    self._init_executor()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py\", line 90, in _init_executor\n    self._init_workers_ray(placement_group)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py\", line 227, in _init_workers_ray\n    raise ValueError(\nValueError: Ray does not allocate any GPUs on the driver node. Consider adjusting the Ray placement group or running the driver on a GPU node.\n*** SIGTERM received at time=1739097071 on cpu 95 ***\nPC: @     0x7fa5c96777f8  (unknown)  clock_nanosleep\n    @     0x7fa5c95d4520  (unknown)  (unknown)\n    @ ... and at least 3 more frames\n[2025-02-09 02:31:11,663 E 15439 15439] logging.cc:460: *** SIGTERM received at time=1739097071 on cpu 95 ***\n[2025-02-09 02:31:11,663 E 15439 15439] logging.cc:460: PC: @     0x7fa5c96777f8  (unknown)  clock_nanosleep\n[2025-02-09 02:31:11,663 E 15439 15439] logging.cc:460:     @     0x7fa5c95d4520  (unknown)  (unknown)\n[2025-02-09 02:31:11,663 E 15439 15439] logging.cc:460:     @ ... and at least 3 more frames\nException ignored in atexit callback: <function shutdown at 0x7fa44c55bd80>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1910, in shutdown\n    time.sleep(0.5)\n  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1499, in sigterm_handler\n    sys.exit(signum)\nSystemExit: 15\nTraceback (most recent call last):\n  File \"/usr/local/bin/vllm\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/scripts.py\", line 204, in main\n    args.dispatch_function(args)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/scripts.py\", line 44, in serve\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 875, in run_server\n    async with build_async_engine_client(args) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 136, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 230, in build_async_engine_client_from_engine_args\n    raise RuntimeError(\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n1\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-09T10:53:54+00:00",
    "closed_at": "2025-06-26T02:26:05+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12983/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12983"
  },
  {
    "number": 14277,
    "title": "[Misc]: running multiple vLLM instances on a single ray cluster",
    "body": "### Anything you want to discuss about vllm.\n\nWhen running the vLLM OpenAI server on a Ray cluster(with nodes A/B/C/D), I want to specify particular nodes(e.g., node A and B) for deployment, enabling better control over multiple vLLM instances within a single Ray cluster. Currently, it seems that Ray integration appears limited to specifying tp and pp.\n\nIs supporting custom placement group a feasible option?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc",
      "ray",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-05T10:43:43+00:00",
    "closed_at": "2025-07-04T02:17:47+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14277/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14277"
  },
  {
    "number": 13437,
    "title": "[V1][Bug]: TP with Ray does not terminate gracefully",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen using Ray as the distributed executor backend and using the `LLM` Python API , the main process does not terminate gracefully:\n\n```\n*** SIGTERM received at time=1739834838 on cpu 88 ***\nPC: @     0x7fe108d1f117  (unknown)  (unknown)\n    @     0x7fe108cd0520  (unknown)  (unknown)\n[2025-02-17 15:27:18,341 E 2669821 2669821] logging.cc:460: *** SIGTERM received at time=1739834838 on cpu 88 ***\n[2025-02-17 15:27:18,341 E 2669821 2669821] logging.cc:460: PC: @     0x7fe108d1f117  (unknown)  (unknown)\n[2025-02-17 15:27:18,341 E 2669821 2669821] logging.cc:460:     @     0x7fe108cd0520  (unknown)  (unknown)\n2025-02-17 15:27:18,342 INFO compiled_dag_node.py:1867 -- Tearing down compiled DAG\n2025-02-17 15:27:18,342 INFO compiled_dag_node.py:1872 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, a1dcab214fac9e464505ef2701000000)\n2025-02-17 15:27:18,342 INFO compiled_dag_node.py:1872 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, fad8cccd5652d08fb1c696bb01000000)\n2025-02-17 15:27:18,342 INFO compiled_dag_node.py:1872 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 37fc4010a9fc8557c83a042201000000)\n2025-02-17 15:27:18,342 INFO compiled_dag_node.py:1872 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 1b42010b9bf378a0bb209cb401000000)\n(RayWorkerWrapper pid=2670369) Destructing NCCL group on actor: Actor(RayWorkerWrapper, a1dcab214fac9e464505ef2701000000)\n2025-02-17 15:27:19,080 INFO compiled_dag_node.py:1892 -- Waiting for worker tasks to exit\n2025-02-17 15:27:19,080 INFO compiled_dag_node.py:1894 -- Teardown complete\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-02-17T23:37:17+00:00",
    "closed_at": "2025-02-19T17:40:51+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13437/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/13437"
  },
  {
    "number": 5052,
    "title": "Running Vllm on ray cluster, logging stuck at loading",
    "body": "### Your current environment\n\nI have two machine 2*4090, I wanted to runner a model (eg gpt-neox-20b) using vllm on ray cluster, so i follow the documentation by making ray cluster \r\non head\r\nray start --head\r\non node\r\nray start --address=<node-ip>:port\r\n\r\nI manged to make the cluster so far, when i run simple script for inference: \r\n```\r\nfrom vllm import LLM\r\nllm = LLM(\"/home/administrator/nlp-deploy/models/gpt-neox-20b/\", tensor_parallel_size=2, disable_custom_all_reduce=True, enforce_eager=True)\r\n\r\nprompt = \"GPT-NeoX-20B is\"\r\noutput = llm.generate(prompt)\r\nprint(output)\r\n```\r\nthe model is stuck at loading.\r\nnvidia-smi for the head and the node while loading \r\n![image](https://github.com/vllm-project/vllm/assets/83926003/9d4adad6-e165-4e52-a2a5-3e281722586a)\r\n\r\n\r\nlogs when running the script \r\n![image](https://github.com/vllm-project/vllm/assets/83926003/57a6d968-fbf7-4f88-be90-895d73141058)\r\n\r\nversions\r\ncuda : 12.4\r\nray : 2.22\n\n### \ud83d\udc1b Describe the bug\n\ni tried other solutions mentioned in the issues like \r\n`export NCCL_P2P_DISABLE = 1`\r\n`disable_custom_all_reduce=True`\r\n`enforce_eager=True`\r\n\r\nand its not solved ",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "closed",
    "created_at": "2024-05-25T17:48:19+00:00",
    "closed_at": "2024-06-13T09:01:53+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5052/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5052"
  },
  {
    "number": 15266,
    "title": "[Feature]: Can support CPU inference with Ray cluster?",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCan vllm support CPU inference with Ray cluster now? How to use it?\n\n\n\nI have Ray cluster with two nodes, as follow:\n\n```\n======== Autoscaler status: 2025-03-21 02:55:20.232713 ========\nNode status\n---------------------------------------------------------------\nActive:\n 1 node_211c6676b30ed72f47827575ebf7360457841df4a44d8a09228163be\n 1 node_d5b1b2aabbbed13ed9f8cd9111b6818c97e269d1c01c1378fab26e74\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/256.0 CPU\n 0B/323.77GiB memory\n 0B/142.75GiB object_store_memory\n\nDemands:\n (no resource demands)\n\n```\n\n\nAnd I run vllm at only one node of the ray cluster, by:\n```\nVLLM_CPU_KVCACHE_SPACE=64 python3 -m vllm.entrypoints.openai.api_server --port 8080 --trust-remote-code --served-model-name QwQ32B --dtype float16 --model /root/LLM/QwQmodelscop/ --tensor-parallel-size 2\n```\n\n\nWhen I submit a inference task, there is no resource consume as shown by *ray status*, and only cpus of the one node is busy(100%), the cpu of other nodes is idle(0%).\n\n\n\nCan vllm support CPU inference with Ray cluster now? How to use it?\n\n\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "ray",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-21T03:13:09+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15266/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15266"
  },
  {
    "number": 10155,
    "title": "[Installation]: VLLM does not support TPU v5p-16 (Multi-Host) with Ray Cluster",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n\r\nCollecting environment information...\r\nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\r\nINFO 11-04 16:11:44 importing.py:15] Triton not installed or not compatible; certain GPU-related functions will not be available.\r\nPyTorch version: 2.6.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\r\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\r\nClang version: Could not collect\r\nCMake version: version 3.30.5\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.15 (main, Oct 17 2024, 02:58:23) [GCC 10.2.1 20210110] (64-bit runtime)\r\nPython platform: Linux-5.19.0-1022-gcp-x86_64-with-glibc2.31\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   52 bits physical, 57 bits virtual\r\nCPU(s):                          208\r\nOn-line CPU(s) list:             0-207\r\nThread(s) per core:              2\r\nCore(s) per socket:              52\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           143\r\nModel name:                      Intel(R) Xeon(R) Platinum 8481C CPU @ 2.70GHz\r\nStepping:                        8\r\nCPU MHz:                         2699.998\r\nBogoMIPS:                        5399.99\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       4.9 MiB\r\nL1i cache:                       3.3 MiB\r\nL2 cache:                        208 MiB\r\nL3 cache:                        210 MiB\r\nNUMA node0 CPU(s):               0-51,104-155\r\nNUMA node1 CPU(s):               52-103,156-207\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rtm avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 arat avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk amx_bf16 avx512_fp16 amx_tile amx_int8 arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.6.0\r\n[pip3] torch-xla==2.6.0+gita0f81e5\r\n[pip3] torchvision==0.19.0a0+d23a6e1\r\n[pip3] transformers==4.46.1\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.3.post2.dev217+gccb5376a\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\r\n\n\n### How you are installing vllm\n\n# Create a TPU VM\r\n```\r\ngcloud compute tpus tpu-vm create tpu-v5p-benchmark \\\r\n  --zone=europe-west4-b \\\r\n  --accelerator-type=v5p-16 \\\r\n  --version=tpu-ubuntu2204-base\r\n```\r\n# On head node\r\n`ray start --block --head --port=6379`\r\n\r\n# On other node (Note: TPU v5p-16 has 2 nodes)\r\n`ray start --block --address=<head-node-address>:6379`\r\n\r\n#Below is the ray-status\r\n# Ray status shows both the nodes active\r\n```\r\n======== Autoscaler status: 2024-11-05 15:48:55.473751 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 node_49fc62d654acc1939448a1668ee1770feef20f763ab4bedface3ccf7\r\n 1 node_88790deb8765d60a25e104192e33414fedcdc89d40e339316235547c\r\nPending:\r\n (no pending nodes)\r\nRecent failures:\r\n (no failures)\r\n\r\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 0.0/416.0 CPU\r\n 0B/839.91GiB memory\r\n 0B/30.40GiB object_store_memory\r\n\r\nDemands:\r\n (no resource demands)\r\n```\r\nHowever, when I run vllm serve from master node  it issues error \"The number of required TPUs exceeds the total number of available TPUs in the placement group.\", even when it is connected to cluster.\r\nI have tried with --tensor-parallel-size 2, 4, 8, 16 and the output is same.\r\n\r\n```\r\n$ vllm serve /root/.llama/checkpoints/Llama3.1-70B --tensor-parallel-size 8\r\nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\r\nINFO 11-05 15:51:16 importing.py:15] Triton not installed or not compatible; certain GPU-related functions will not be available.\r\nINFO 11-05 15:51:18 api_server.py:551] vLLM API server version 0.6.3.post2.dev217+gccb5376a\r\nINFO 11-05 15:51:18 api_server.py:552] args: Namespace(subparser='serve', model_tag='/root/.llama/checkpoints/Llama3.1-70B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/root/.llama/checkpoints/Llama3.1-70B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', chat_template_text_format='string', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=8, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', pooling_type=None, pooling_norm=None, pooling_softmax=None, pooling_step_tag_id=None, pooling_returned_token_ids=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7ff7515e1750>)\r\nINFO 11-05 15:51:18 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/5be6afd9-60d4-4329-8202-6ecbec5a18be for IPC Path.\r\nINFO 11-05 15:51:18 api_server.py:181] Started engine process with PID 1132\r\nINFO 11-05 15:51:18 config.py:1752] Downcasting torch.float32 to torch.float16.\r\nINFO 11-05 15:51:21 importing.py:15] Triton not installed or not compatible; certain GPU-related functions will not be available.\r\nINFO 11-05 15:51:22 config.py:1752] Downcasting torch.float32 to torch.float16.\r\nINFO 11-05 15:51:23 config.py:323] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\r\nWARNING 11-05 15:51:23 arg_utils.py:1051] The model has a long context length (128000). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\r\nWARNING 11-05 15:51:23 arg_utils.py:1103] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\r\nYou are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\r\nYou are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\r\nINFO 11-05 15:51:27 config.py:323] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\r\nWARNING 11-05 15:51:27 arg_utils.py:1051] The model has a long context length (128000). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\r\nWARNING 11-05 15:51:27 arg_utils.py:1103] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\r\n2024-11-05 15:51:27,430 INFO worker.py:1631 -- Connecting to existing Ray cluster at address: 10.164.15.221:6379...\r\n2024-11-05 15:51:27,438 INFO worker.py:1807 -- Connected to Ray cluster. View the dashboard at 127.0.0.1:8265 \r\nProcess SpawnProcess-1:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/local/lib/python3.10/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 361, in run_mp_engine\r\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\r\n  File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 122, in from_engine_args\r\n    executor_class = LLMEngine._get_executor_cls(engine_config)\r\n  File \"/workspace/vllm/vllm/engine/llm_engine.py\", line 521, in _get_executor_cls\r\n    initialize_ray_cluster(engine_config.parallel_config)\r\n  File \"/workspace/vllm/vllm/executor/ray_utils.py\", line 277, in initialize_ray_cluster\r\n    raise ValueError(\r\nValueError: The number of required TPUs exceeds the total number of available TPUs in the placement group.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/vllm\", line 33, in <module>\r\n    sys.exit(load_entry_point('vllm', 'console_scripts', 'vllm')())\r\n  File \"/workspace/vllm/vllm/scripts.py\", line 195, in main\r\n    args.dispatch_function(args)\r\n  File \"/workspace/vllm/vllm/scripts.py\", line 41, in serve\r\n    uvloop.run(run_server(args))\r\n  File \"/usr/local/lib/python3.10/site-packages/uvloop/__init__.py\", line 82, in run\r\n    return loop.run_until_complete(wrapper())\r\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\r\n  File \"/usr/local/lib/python3.10/site-packages/uvloop/__init__.py\", line 61, in wrapper\r\n    return await main\r\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 575, in run_server\r\n    async with build_async_engine_client(args) as engine_client:\r\n  File \"/usr/local/lib/python3.10/contextlib.py\", line 199, in __aenter__\r\n    return await anext(self.gen)\r\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 107, in build_async_engine_client\r\n    async with build_async_engine_client_from_engine_args(\r\n  File \"/usr/local/lib/python3.10/contextlib.py\", line 199, in __aenter__\r\n    return await anext(self.gen)\r\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 197, in build_async_engine_client_from_engine_args\r\n    raise RuntimeError(\r\nRuntimeError: Engine process failed to start\r\n```\r\n\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "tpu",
      "ray"
    ],
    "state": "closed",
    "created_at": "2024-11-08T11:36:17+00:00",
    "closed_at": "2025-02-07T02:16:05+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10155/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10155"
  },
  {
    "number": 18023,
    "title": "[Bug]: Llama-4-Maverick crashes on V1 engine (with Ray distributed executor)",
    "body": "### Your current environment\n\nForward passes with Maverick served across two 8xH100 nodes are crashing on V1 engine, but running fine on V0. \n\nHere are the steps to reproduce the error:\n1. follow vllm docs from https://docs.vllm.ai/en/latest/serving/distributed_serving.html#running-vllm-on-multiple-nodes to enable vllm on multiple nodes (I'm using vllm/vllm-openai image with 0.8.5.post1)\n\n2. serve the model with `--distributed-executor-backend ray` (without it, vllm engine falls back to V0):\n```bash\nvllm serve meta-llama/Llama-4-Maverick-17B-128E-Instruct -tp 8 -pp 2 --dtype auto --max-model-len 4096 --gpu-memory-utilization 0.8 --enable-chunked-prefill --distributed-executor-backend ray\n```\n\n4. send some GSM8k requests (requires installing `pip install lmeval[api]`)\n```bash\nlm_eval \\\n  --model local-completions \\\n  --model_args model=\"meta-llama/Llama-4-Maverick-17B-128E-Instruct\",base_url=\"\"http://localhost:8000/v1/completions\"\",max_retries=3,timeout=300,tokenized_requests=True,add_bos_token=True,max_length=4096,max_gen_tokens=512,num_concurrent=1 \\\n  --tasks gsm8k \\\n  --num_fewshot 5 \\\n  --write_out \\\n  --log_samples \\\n  --output_path ${OUTPUT_BASE_PTH}/${CKPT}/${TASK}/${TAG} \\\n  --show_config\n```\n\n5. full error trace here: https://pastebin.com/KGWHqc2M\n```bash\n2025-05-12 00:50:06,624 ERROR serialization.py:462 -- Can't deserialize object: ObjectRef(00b324ea70949bfdf3eb0bf174f753858f9759f00400000002e1f505), metadata: b'\\x07'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/serialization.py\", line 331, in _deserialize_object\n    error_type = int(metadata_fields[0])\n                 ^^^^^^^^^^^^^^^^^^^^^^^\nValueError: invalid literal for int() with base 10: b'\\x07'\n```\n\n6. [with V0 engine, everything works fine]: serve the model with the same command from step 3 but without `--distributed-executor-backend ray` . This falls back to V0 engine and GSM8k eval runs without any errors.\n\nNote: I haven't tried other models, so it is not clear whether this is a Maverick issue or vllm issue. \n\n### \ud83d\udc1b Describe the bug\n\n.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "open",
    "created_at": "2025-05-12T20:37:07+00:00",
    "closed_at": null,
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18023/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/18023"
  },
  {
    "number": 14093,
    "title": "[Bug][Ray]: Pipeline parallelism fails on the same host",
    "body": "### Your current environment\n\nUsing the 0.7.3 ghcr.io/sasha0552/vllm:v0.7.3 (pascal docker from [pascal-pkgs-ci](https://github.com/sasha0552/pascal-pkgs-ci)) and the same version directly from vllm\n\nHead:\n```\ndocker run -d --rm \\\n\t--entrypoint /bin/bash \\\n\t--network host \\\n\t--runtime=nvidia \\\n\t--name headnode \\\n\t--shm-size 10.24g \\\n\t--gpus \"device=0\" \\\n\t-e VLLM_HOST_IP=0.0.0.0 \\\n\t-e MASTER_ADDR=127.0.0.1 \\\n\t-e MASTER_PORT=29500 \\\n\t-e NCCL_DEBUG=INFO \\\n\t-e NCCL_SOCKET_IFNAME=br0 \\\n\t-e NCCL_PCI_BUS_ID=\"00000000:01:00.0\" \\\n\t-v /media/bkutasi/60824A4F824A29BC/Other_projects/koboldcpp_precompiled:/models \\\n\tvllm/vllm-openai:v0.7.3     -c \"ray start --block --head --port=6379 --node-ip-address=0.0.0.0 --dashboard-host=0.0.0.0\"\n``` \nWorker:\n```\ndocker run -d --rm \\\n\t--entrypoint /bin/bash \\\n\t--network host \\\n\t--runtime=nvidia \\\n\t--name workernode \\\n\t--shm-size 10.24g \\\n\t--gpus \"device=1\" \\\n\t-e VLLM_HOST_IP=127.0.0.1 \\\n\t-e MASTER_ADDR=127.0.0.1 \\\n\t-e MASTER_PORT=29500 \\\n\t-e NCCL_DEBUG=INFO \\\n\t-e NCCL_SOCKET_IFNAME=br0 \\\n\t-e NCCL_PCI_BUS_ID=\"00000000:03:00.0\" \\\n\t-v /media/bkutasi/60824A4F824A29BC/Other_projects/koboldcpp_precompiled:/models \\\n\tghcr.io/sasha0552/vllm:v0.7.3     -c \"ray start --block --address=127.0.0.1:6379\"\n```\nRay starts on both successfully. \nHead ray status:\n```bash\n======== Autoscaler status: 2025-03-02 03:52:57.914139 ========\nNode status\n---------------------------------------------------------------\nActive:\n 1 node_90e689206eb84f2cea7ebb620e99aedd53329bca53ddf766f84e040a\n 1 node_3f98eb03d88a6d62c22989f7479e5183d45caf67adaf69e1f2bf6ed2\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/32.0 CPU\n 0.0/2.0 GPU\n 0B/154.99GiB memory\n 0B/19.46GiB object_store_memory\n\nDemands:\n (no resource demands)\n```\n\n### \ud83d\udc1b Describe the bug\n\nAfter setting up the environment with ray and docker, the loading of models fail in parallel. I tested AWQ, GPTQ and GGUF models. If just launching one model that fits within the node's VRAM the launch is successful(no ray workers involved).\nI tried to set up the environment with various NCCL configs but it seems to be failing anyways: `RuntimeError: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details)`\n\nI think the most relevant part at the end of the stacktrace:\n```bash\nERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/cuda_communicator.py\", line 39, in __init__\nERROR 03-02 03:24:44 worker_base.py:581]     self.pynccl_comm = PyNcclCommunicator(\nERROR 03-02 03:24:44 worker_base.py:581]                        ^^^^^^^^^^^^^^^^^^^\nERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl.py\", line 99, in __init__\nERROR 03-02 03:24:44 worker_base.py:581]     self.comm: ncclComm_t = self.nccl.ncclCommInitRank(\nERROR 03-02 03:24:44 worker_base.py:581]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 277, in ncclCommInitRank\nERROR 03-02 03:24:44 worker_base.py:581]     self.NCCL_CHECK(self._funcs[\"ncclCommInitRank\"](ctypes.byref(comm),\nERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 256, in NCCL_CHECK\nERROR 03-02 03:24:44 worker_base.py:581]     raise RuntimeError(f\"NCCL error: {error_str}\")\nERROR 03-02 03:24:44 worker_base.py:581] RuntimeError: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details)\n```\nAt line 99 in pynccl.py `self.comm: ncclComm_t = self.nccl.ncclCommInitRank` fails\nWhole function:\n\n```python\n# nccl communicator and stream will use this device\n        # `torch.cuda.device` is a context manager that changes the\n        # current cuda device to the specified one\n        with torch.cuda.device(device):\n            self.comm: ncclComm_t = self.nccl.ncclCommInitRank(\n                self.world_size, self.unique_id, self.rank)\n\n            stream = current_stream()\n            # A small all_reduce for warmup.\n            data = torch.zeros(1, device=device)\n            self.all_reduce(data)\n            stream.synchronize()\n            del data\n```\nWhich leads to:\n```python\ndef ncclCommInitRank(self, world_size: int, unique_id: ncclUniqueId,\n                         rank: int) -> ncclComm_t:\n        comm = ncclComm_t()\n        self.NCCL_CHECK(self._funcs[\"ncclCommInitRank\"](ctypes.byref(comm),\n                                                        world_size, unique_id,\n                                                        rank))\n```\nI suspect there are some problems with inter-gpu communication due to both being on the same machine? Has anyone faced this problem before? Enabling INFO or TRACE logging doesn't provide me with more info.\n\nFull logs from the headnode:\n<details>\n\n```bash\nroot@WORKSTATION-01:/vllm-workspace# vllm serve /models/PyrTools_Ministral-8B-Instruct-2410-AWQ/ --port 7888 --tensor-parallel-size 1 --pipeline-parallel-size 2\nINFO 03-02 03:24:35 __init__.py:207] Automatically detected platform cuda.\nINFO 03-02 03:24:35 api_server.py:912] vLLM API server version 0.7.3\nINFO 03-02 03:24:35 api_server.py:913] args: Namespace(subparser='serve', model_tag='/models/PyrTools_Ministral-8B-Instruct-2410-AWQ/', config='', host=None, port=7888, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='/models/PyrTools_Ministral-8B-Instruct-2410-AWQ/', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=2, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function ServeSubcommand.cmd at 0x72de566c8900>)\nINFO 03-02 03:24:37 config.py:549] This model supports multiple tasks: {'embed', 'score', 'classify', 'reward', 'generate'}. Defaulting to 'generate'.\nINFO 03-02 03:24:37 awq_marlin.py:114] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\nINFO 03-02 03:24:37 config.py:1382] Defaulting to use ray for distributed inference\nWARNING 03-02 03:24:37 config.py:676] Async output processing can not be enabled with pipeline parallel\nINFO 03-02 03:24:37 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/models/PyrTools_Ministral-8B-Instruct-2410-AWQ/', speculative_config=None, tokenizer='/models/PyrTools_Ministral-8B-Instruct-2410-AWQ/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/models/PyrTools_Ministral-8B-Instruct-2410-AWQ/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False,\n2025-03-02 03:24:38,100 INFO worker.py:1636 -- Connecting to existing Ray cluster at address: 0.0.0.0:6379...\n2025-03-02 03:24:38,106 INFO worker.py:1821 -- Connected to Ray cluster.\nINFO 03-02 03:24:38 ray_distributed_executor.py:153] use_ray_spmd_worker: False\n(RayWorkerWrapper pid=380) INFO 03-02 03:24:39 __init__.py:207] Automatically detected platform cuda.\nINFO 03-02 03:24:43 cuda.py:229] Using Flash Attention backend.\n(RayWorkerWrapper pid=182, ip=192.168.1.178) INFO 03-02 03:24:43 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n(RayWorkerWrapper pid=182, ip=192.168.1.178) INFO 03-02 03:24:43 cuda.py:226] Using XFormers backend.\nINFO 03-02 03:24:44 utils.py:916] Found nccl from library libnccl.so.2\nINFO 03-02 03:24:44 pynccl.py:69] vLLM is using nccl==2.21.5\nWORKSTATION-01:301:301 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to br0\nWORKSTATION-01:301:301 [0] NCCL INFO Bootstrap : Using br0:192.168.1.178<0>\nWORKSTATION-01:301:301 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)\nWORKSTATION-01:301:301 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so\nWORKSTATION-01:301:301 [0] NCCL INFO NET/Plugin: Using internal network plugin.\nWORKSTATION-01:301:301 [0] NCCL INFO cudaDriverVersion 12040\nNCCL version 2.21.5+cuda12.4\n(RayWorkerWrapper pid=182, ip=192.168.1.178) INFO 03-02 03:24:44 utils.py:916] Found nccl from library libnccl.so.2\n(RayWorkerWrapper pid=182, ip=192.168.1.178) INFO 03-02 03:24:44 pynccl.py:69] vLLM is using nccl==2.21.5\nWORKSTATION-01:301:301 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to br0\nWORKSTATION-01:301:301 [0] NCCL INFO NET/IB : No device found.\nWORKSTATION-01:301:301 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to br0\nWORKSTATION-01:301:301 [0] NCCL INFO NET/Socket : Using [0]br0:192.168.1.178<0>\nWORKSTATION-01:301:301 [0] NCCL INFO Using non-device net plugin version 0\nWORKSTATION-01:301:301 [0] NCCL INFO Using network Socket\nWORKSTATION-01:301:301 [0] NCCL INFO ncclCommInitRank comm 0x1a8ca900 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1000 commId 0xcf63ef822074648 - Init START\n\nWORKSTATION-01:301:301 [0] misc/nvmlwrap.cc:187 NCCL WARN nvmlDeviceGetHandleByPciBusId() failed: Not Found\nWORKSTATION-01:301:301 [0] NCCL INFO graph/xml.cc:850 -> 2\nWORKSTATION-01:301:301 [0] NCCL INFO graph/topo.cc:696 -> 2\nWORKSTATION-01:301:301 [0] NCCL INFO init.cc:1012 -> 2\nWORKSTATION-01:301:301 [0] NCCL INFO init.cc:1548 -> 2\nWORKSTATION-01:301:301 [0] NCCL INFO init.cc:1799 -> 2\nWORKSTATION-01:301:301 [0] NCCL INFO init.cc:1837 -> 2\nERROR 03-02 03:24:44 worker_base.py:581] Error executing method 'init_device'. This might cause deadlock in distributed execution.\nERROR 03-02 03:24:44 worker_base.py:581] Traceback (most recent call last):\nERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 573, in execute_method\nERROR 03-02 03:24:44 worker_base.py:581]     return run_method(target, method, args, kwargs)\nERROR 03-02 03:24:44 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2196, in run_method\nERROR 03-02 03:24:44 worker_base.py:581]     return func(*args, **kwargs)\nERROR 03-02 03:24:44 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 166, in init_device\nERROR 03-02 03:24:44 worker_base.py:581]     init_worker_distributed_environment(self.vllm_config, self.rank,\nERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 506, in init_worker_distributed_environment\nERROR 03-02 03:24:44 worker_base.py:581]     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\nERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 943, in ensure_model_parallel_initialized\nERROR 03-02 03:24:44 worker_base.py:581]     initialize_model_parallel(tensor_model_parallel_size,\nERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 905, in initialize_model_parallel\nERROR 03-02 03:24:44 worker_base.py:581]     _PP = init_model_parallel_group(group_ranks,\nERROR 03-02 03:24:44 worker_base.py:581]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 730, in init_model_parallel_group\nERROR 03-02 03:24:44 worker_base.py:581]     return GroupCoordinator(\nERROR 03-02 03:24:44 worker_base.py:581]            ^^^^^^^^^^^^^^^^^\nERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 207, in __init__\nERROR 03-02 03:24:44 worker_base.py:581]     self.device_communicator = device_comm_cls(\nERROR 03-02 03:24:44 worker_base.py:581]                                ^^^^^^^^^^^^^^^^\nERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/cuda_communicator.py\", line 39, in __init__\nERROR 03-02 03:24:44 worker_base.py:581]     self.pynccl_comm = PyNcclCommunicator(\nERROR 03-02 03:24:44 worker_base.py:581]                        ^^^^^^^^^^^^^^^^^^^\nERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl.py\", line 99, in __init__\nERROR 03-02 03:24:44 worker_base.py:581]     self.comm: ncclComm_t = self.nccl.ncclCommInitRank(\nERROR 03-02 03:24:44 worker_base.py:581]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 277, in ncclCommInitRank\nERROR 03-02 03:24:44 worker_base.py:581]     self.NCCL_CHECK(self._funcs[\"ncclCommInitRank\"](ctypes.byref(comm),\nERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 256, in NCCL_CHECK\nERROR 03-02 03:24:44 worker_base.py:581]     raise RuntimeError(f\"NCCL error: {error_str}\")\nERROR 03-02 03:24:44 worker_base.py:581] RuntimeError: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details)\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/usr/local/bin/vllm\", line 10, in <module>\n[rank0]:     sys.exit(main())\n[rank0]:              ^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/main.py\", line 73, in main\n[rank0]:     args.dispatch_function(args)\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/serve.py\", line 34, in cmd\n[rank0]:     uvloop.run(run_server(args))\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\n[rank0]:     return __asyncio.run(\n[rank0]:            ^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n[rank0]:     return runner.run(main)\n[rank0]:            ^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n[rank0]:     return self._loop.run_until_complete(task)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n[rank0]:     return await main\n[rank0]:            ^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 947, in run_server\n[rank0]:     async with build_async_engine_client(args) as engine_client:\n[rank0]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n[rank0]:     return await anext(self.gen)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 139, in build_async_engine_client\n[rank0]:     async with build_async_engine_client_from_engine_args(\n[rank0]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n[rank0]:     return await anext(self.gen)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 163, in build_async_engine_client_from_engine_args\n[rank0]:     engine_client = AsyncLLMEngine.from_engine_args(\n[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 644, in from_engine_args\n[rank0]:     engine = cls(\n[rank0]:              ^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 594, in __init__\n[rank0]:     self.engine = self._engine_class(*args, **kwargs)\n[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 267, in __init__\n[rank0]:     super().__init__(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 273, in __init__\n[rank0]:     self.model_executor = executor_class(vllm_config=vllm_config, )\n[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 271, in __init__\n[rank0]:     super().__init__(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 52, in __init__\n[rank0]:     self._init_executor()\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py\", line 90, in _init_executor\n[rank0]:     self._init_workers_ray(placement_group)\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py\", line 359, in _init_workers_ray\n[rank0]:     self._run_workers(\"init_device\")\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py\", line 480, in _run_workers\n[rank0]:     self.driver_worker.execute_method(sent_method, *args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 582, in execute_method\n[rank0]:     raise e\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 573, in execute_method\n[rank0]:     return run_method(target, method, args, kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2196, in run_method\n[rank0]:     return func(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 166, in init_device\n[rank0]:     init_worker_distributed_environment(self.vllm_config, self.rank,\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 506, in init_worker_distributed_environment\n[rank0]:     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 943, in ensure_model_parallel_initialized\n[rank0]:     initialize_model_parallel(tensor_model_parallel_size,\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 905, in initialize_model_parallel\n[rank0]:     _PP = init_model_parallel_group(group_ranks,\n[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 730, in init_model_parallel_group\n[rank0]:     return GroupCoordinator(\n[rank0]:            ^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 207, in __init__\n[rank0]:     self.device_communicator = device_comm_cls(\n[rank0]:                                ^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/cuda_communicator.py\", line 39, in __init__\n[rank0]:     self.pynccl_comm = PyNcclCommunicator(\n[rank0]:                        ^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl.py\", line 99, in __init__\n[rank0]:     self.comm: ncclComm_t = self.nccl.ncclCommInitRank(\n[rank0]:                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 277, in ncclCommInitRank\n[rank0]:     self.NCCL_CHECK(self._funcs[\"ncclCommInitRank\"](ctypes.byref(comm),\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 256, in NCCL_CHECK\n[rank0]:     raise RuntimeError(f\"NCCL error: {error_str}\")\n[rank0]: RuntimeError: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details)\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581] Error executing method 'init_device'. This might cause deadlock in distributed execution.\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581] Traceback (most recent call last):\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 573, in execute_method\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]     return run_method(target, method, args, kwargs)\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2196, in run_method\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]     return func(*args, **kwargs)\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]            ^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 166, in init_device\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]     init_worker_distributed_environment(self.vllm_config, self.rank,\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 506, in init_worker_distributed_environment\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 943, in ensure_model_parallel_initialized\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]     initialize_model_parallel(tensor_model_parallel_size,\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 905, in initialize_model_parallel\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]     _PP = init_model_parallel_group(group_ranks,\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 730, in init_model_parallel_group\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]     return GroupCoordinator(\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]            ^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py\", line 207, in __init__\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]     self.device_communicator = device_comm_cls(\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]                                ^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/cuda_communicator.py\", line 39, in __init__\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]     self.pynccl_comm = PyNcclCommunicator(\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]                        ^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl.py\", line 99, in __init__\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]     self.comm: ncclComm_t = self.nccl.ncclCommInitRank(\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 277, in ncclCommInitRank\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]     self.NCCL_CHECK(self._funcs[\"ncclCommInitRank\"](ctypes.byref(comm),\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/pynccl_wrapper.py\", line 256, in NCCL_CHECK\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581]     raise RuntimeError(f\"NCCL error: {error_str}\")\n(RayWorkerWrapper pid=182, ip=192.168.1.178) ERROR 03-02 03:24:44 worker_base.py:581] RuntimeError: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details)\n(RayWorkerWrapper pid=182, ip=192.168.1.178) INFO 03-02 03:24:42 __init__.py:207] Automatically detected platform cuda.\nINFO 03-02 03:24:45 ray_distributed_executor.py:104] Shutting down Ray distributed executor. If you see error log from logging.cc regarding SIGTERM received, please ignore because this is the expected termination process in Ray.\n[rank0]:[W302 03:24:45.624187884 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n```\n\n</details>\n\nReproduction:\nI'm not 100% sure but try running a mixed nvidia arch with pipeline parallelism in docker on the same machine. Maybe its even possible to trigger with 2 of the same gpus. \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-02T11:54:32+00:00",
    "closed_at": "2025-07-17T02:16:38+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14093/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14093"
  },
  {
    "number": 17904,
    "title": "[Bug]:  IndexError when using Ray-SPMD Worker with Multi-Step (--num-scheduler-steps > 1)",
    "body": "### Your current environment\n\n\uff5e\n\n### \ud83d\udc1b Describe the bug\n\n\n###  Description\nRunning vLLM in **Ray-SPMD Worker** mode (`VLLM_USE_RAY_SPMD_WORKER=1`) together with **Multi-Step scheduling** (`--num-scheduler-steps 8`) crashes with an `IndexError` inside `MultiStepModelRunner.execute_model`.  \nThe error occurs while advancing to the next step: `model_input.cached_outputs[-1]` is empty, so accessing `[-1]` raises.\n\n###  Reproduction\n\n```bash\n# single-GPU example; same happens with more GPUs\nexport VLLM_USE_RAY_SPMD_WORKER=1\nexport VLLM_USE_RAY_COMPILED_DAG =1\n\npython -m vllm.entrypoints.openai.api_server \\\n  --model Qwen2.5-7B-Instruct \\\n  --tensor-parallel-size 1 \\\n  --distributed-executor-backend \\\n  ray \\\n  --num-scheduler-steps 8\n```\n\nFull stack trace (excerpt):\n\n```\nself.worker._execute_model_spmd(execute_model_req,\n  File \".../vllm/worker/worker.py\", line 399, in _execute_model_spmd\n    output = super()._execute_model_spmd(execute_model_req,\n  File \".../vllm/worker/worker_base.py\", line 383, in _execute_model_spmd\n    return self.model_runner.execute_model(\n  File \".../vllm/worker/multi_step_model_runner.py\", line 508, in execute_model\n    model_input, model_input.cached_outputs[-1].sampler_output)\nIndexError: list index out of range\n```\n\n###  Environment\n\n| Item | Value |\n|------|-------|\n| vLLM | 0.7.3 \n| Model | `Qwen2.5-7B-Instruct` |\n| CLI flags | `--tensor-parallel-size 1 --num-scheduler-steps 8` |\n| Env vars | `VLLM_USE_RAY_SPMD_WORKER=1` |\n\n### Expected behavior\nRay-SPMD mode should work with Multi-Step scheduling (or fail fast with a clear error if this combination is not supported).\n\n### Possible root cause\n`MultiStepModelRunner.execute_model()` calls:\n\n```python\nmodel_input = self._advance_step(\n    model_input, model_input.cached_outputs[-1].sampler_output)\n```\n\nOn non-driver (TP) ranks inside `_execute_model_spmd`, `model_input.cached_outputs`\nonly contains a placeholder `SamplerOutput` or is empty; in some situations it ends up length 0, so `[-1]` fails.\n\n### Questions\n1. Is **Ray-SPMD Worker** officially supported together with **Multi-Step** scheduling?  \n2. If supported, could this be a bug in how cached_outputs are populated on TP ranks?  \n3. If not supported, should the engine raise a configuration error instead of hitting an IndexError at runtime?\n\nThanks in advance for any guidance! Let me know if additional logs or patches are useful.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "open",
    "created_at": "2025-05-09T13:54:27+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17904/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17904"
  },
  {
    "number": 14683,
    "title": "[Feature]: Data parallel inference in offline mode(based on Ray)",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nI've been building model evaluation datasets using offline inference as outlined in the [documentation](https://docs.vllm.ai/en/stable/serving/offline_inference.html#offline-inference), and I noticed that it\u2019s challenging to fully leverage all available GPUs\u2014when the model fits on a single GPU.\n\nTo overcome this, I implemented a feature that distributes model replicas across different GPUs, allowing prompt data to be processed concurrently. For large datasets, this approach achieves nearly linear speedup, significantly enhancing performance for both my team and me.\n\nIt\u2019s important to note that offline inference also plays a crucial role in model training and evaluation. By enabling efficient and scalable processing of evaluation data, offline inference helps in thoroughly benchmarking models and fine-tuning them during the development cycle.\n\nInterestingly, this feature has been discussed before (see [issue #1237](https://github.com/vllm-project/vllm/issues/1237)), yet there hasn't been any implementation so far. I\u2019m curious if others still find this feature useful for offline inference, as it would eliminate the need to launch a multi-k8s-pod vLLM API service or develop a multi-threaded HTTP request program to fully utilize GPU resources. I\u2019d be happy to contribute this enhancement.\n\nNote: Currently, this feature is available only for offline inference, but I\u2019m open to discussing adaptations for online mode if there\u2019s enough interest.\n\n\n\u542f\u52a8\u591a\u4e2a Ray \u8fdb\u7a0b\u6765\u652f\u6301\u6570\u636e\u5e76\u884c\uff0c\u51cf\u5c11\u5927\u6570\u636e\u91cf\u79bb\u7ebf\u63a8\u7406\u7684\u4f7f\u7528\u6210\u672c\n\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "ray"
    ],
    "state": "open",
    "created_at": "2025-03-12T14:14:42+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14683/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14683"
  },
  {
    "number": 14456,
    "title": "[Bug]: No Cuda GPUs are available when running vLLM on Ray (Qwen 2.5 VL AWQ)",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Chainguard (x86_64)\nGCC version: (Wolfi 14.2.0-r4) 14.2.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.40\n\nPython version: 3.10.15 (tags/v3.10.15-0-gffee63f-dirty:ffee63f, Sep 23 2024, 21:00:09) [GCC 14.2.0] (64-bit runtime)\nPython platform: Linux-5.10.227-219.884.amzn2.x86_64-x86_64-with-glibc2.40\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: NVIDIA L40S\nGPU 1: NVIDIA L40S\nGPU 2: NVIDIA L40S\nGPU 3: NVIDIA L40S\n\nNvidia driver version: 550.127.05\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\n/bin/sh: lscpu: not found\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] optree==0.14.0\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.49.0.dev0\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tSYS\tSYS\tSYS\t0-47\t0\t\tN/A\nGPU1\tSYS\t X \tSYS\tSYS\t0-47\t0\t\tN/A\nGPU2\tSYS\tSYS\t X \tSYS\t0-47\t0\t\tN/A\nGPU3\tSYS\tSYS\tSYS\t X \t0-47\t0\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNVIDIA_VISIBLE_DEVICES=GPU-b082cd89-9bbb-de73-9315-68108bc20cee,GPU-8a6a364e-e153-e010-ca44-a7f769347275,GPU-70a9674c-c409-363d-d360-cbcbaf04741a,GPU-2f017e94-5df7-bcc0-fce2-defee9018d3c\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNCCL_P2P_LEVEL=NVL\nVLLM_ENGINE_ITERATION_TIMEOUT_S=1500\nVLLM_ATTENTION_BACKEND=FLASH_ATTN\nCUDA_VISIBLE_DEVICES=0,1,2,3\nCUDA_VISIBLE_DEVICES=0,1,2,3\nVLLM_FLASH_ATTN_VERSION=2\nCUDA_HOME=/usr\nCUDA_HOME=/usr\nVLLM_LOGGING_LEVEL=DEBUG\nLD_LIBRARY_PATH=/home/ray/venv/lib/python3.10/site-packages/cv2/../../lib64:\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n### \ud83d\udc1b Describe the bug\n\nHi all,\n\nI am hosting vLLM on Ray Serve on a Kubernetes cluster (AWS EKS) and I run into a strange issue when trying to host the AWQ quantized [Qwen2.5-VL-7B-Instruct-AWQ](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct-AWQ) models. The stack trace implies that there is no CUDA GPU available, but I can detect my GPUs when I SSH into the pod and run `nvidia-smi`.\n\nThis error seems specific to the AWQ quantized model versions. When I run the same configs using the non-quantized QWEN models, it is successful.\n\nI will share the stack trace below:\n<details>\n<summary>Stacktrace</summary>\n```\n            File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n              return self.__get_result()\n            File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n              raise self._exception\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/ca98275b0a29b201c71e0a5ee4919249e83e7317/virtualenv/lib/python3.10/site-packages/ray/serve/_private/replica.py\", line 952, in initialize_and_get_metadata\n              await self._replica_impl.initialize(deployment_config)\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/ca98275b0a29b201c71e0a5ee4919249e83e7317/virtualenv/lib/python3.10/site-packages/ray/serve/_private/replica.py\", line 687, in initialize\n              raise RuntimeError(traceback.format_exc()) from None\n          RuntimeError: Traceback (most recent call last):\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/ca98275b0a29b201c71e0a5ee4919249e83e7317/virtualenv/lib/python3.10/site-packages/ray/serve/_private/replica.py\", line 664, in initialize\n              self._user_callable_asgi_app = await asyncio.wrap_future(\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/ca98275b0a29b201c71e0a5ee4919249e83e7317/virtualenv/lib/python3.10/site-packages/ray/serve/_private/replica.py\", line 1350, in initialize_callable\n              await self._call_func_or_gen(\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/ca98275b0a29b201c71e0a5ee4919249e83e7317/virtualenv/lib/python3.10/site-packages/ray/serve/_private/replica.py\", line 1311, in _call_func_or_gen\n              result = callable(*args, **kwargs)\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/ca98275b0a29b201c71e0a5ee4919249e83e7317/virtualenv/lib/python3.10/site-packages/ray/serve/api.py\", line 221, in __init__\n              cls.__init__(self, *args, **kwargs)\n            File \"/home/ray/src/vllm_florence2_prototype/api_server.py\", line 51, in __init__\n              self.engine = AsyncLLMEngine.from_engine_args(engine_args)\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/ca98275b0a29b201c71e0a5ee4919249e83e7317/virtualenv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 639, in from_engine_args\n              engine_config = engine_args.create_engine_config(usage_context)\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/ca98275b0a29b201c71e0a5ee4919249e83e7317/virtualenv/lib/python3.10/site-packages/vllm/engine/arg_utils.py\", line 1276, in create_engine_config\n              config = VllmConfig(\n            File \"<string>\", line 19, in __init__\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/ca98275b0a29b201c71e0a5ee4919249e83e7317/virtualenv/lib/python3.10/site-packages/vllm/config.py\", line 3225, in __post_init__\n              self.quant_config = VllmConfig._get_quantization_config(\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/ca98275b0a29b201c71e0a5ee4919249e83e7317/virtualenv/lib/python3.10/site-packages/vllm/config.py\", line 3181, in _get_quantization_config\n              raise ValueError(\n          ValueError: torch.bfloat16 is not supported for quantization method awq. Supported dtypes: [torch.float16]\n  qwen2-vl-72b-instruct:\n    status: RUNNING\n    message: ''\n    last_deployed_time_s: 1741284567.6790574\n    deployments:\n      VLLMDeployment:\n        status: HEALTHY\n        status_trigger: CONFIG_UPDATE_COMPLETED\n        replica_states:\n          RUNNING: 1\n        message: ''\n  qwen2-vl-72b-instruct-pt2:\n    status: DEPLOY_FAILED\n    message: Failed to update the deployments ['VLLMDeployment'].\n    last_deployed_time_s: 1741284567.6790574\n    deployments:\n      VLLMDeployment:\n        status: DEPLOY_FAILED\n        status_trigger: REPLICA_STARTUP_FAILED\n        replica_states: {}\n        message: |-\n          The deployment failed to start 3 times in a row. This may be due to a problem with its constructor or initial health check failing. See controller logs for details. Error:\n          ray::ServeReplica:qwen2-vl-72b-instruct-pt2:VLLMDeployment.initialize_and_get_metadata() (pid=1535, ip=10.191.48.218, actor_id=fd4051c85f9ba380d843991501000000, repr=<ray.serve._private.replica.ServeReplica:qwen2-vl-72b-instruct-pt2:VLLMDeployment object at 0x7f4d7c370490>)\n            File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n              return self.__get_result()\n            File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n              raise self._exception\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/ray/serve/_private/replica.py\", line 952, in initialize_and_get_metadata\n              await self._replica_impl.initialize(deployment_config)\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/ray/serve/_private/replica.py\", line 687, in initialize\n              raise RuntimeError(traceback.format_exc()) from None\n          RuntimeError: Traceback (most recent call last):\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/ray/serve/_private/replica.py\", line 664, in initialize\n              self._user_callable_asgi_app = await asyncio.wrap_future(\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/ray/serve/_private/replica.py\", line 1350, in initialize_callable\n              await self._call_func_or_gen(\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/ray/serve/_private/replica.py\", line 1311, in _call_func_or_gen\n              result = callable(*args, **kwargs)\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/ray/serve/api.py\", line 221, in __init__\n              cls.__init__(self, *args, **kwargs)\n            File \"/home/ray/src/vllm_florence2_prototype/api_server.py\", line 51, in __init__\n              self.engine = AsyncLLMEngine.from_engine_args(engine_args)\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 644, in from_engine_args\n              engine = cls(\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 594, in __init__\n              self.engine = self._engine_class(*args, **kwargs)\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 267, in __init__\n              super().__init__(*args, **kwargs)\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 273, in __init__\n              self.model_executor = executor_class(vllm_config=vllm_config, )\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 262, in __init__\n              super().__init__(*args, **kwargs)\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 51, in __init__\n              self._init_executor()\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/vllm/executor/ray_distributed_executor.py\", line 90, in _init_executor\n              self._init_workers_ray(placement_group)\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/vllm/executor/ray_distributed_executor.py\", line 355, in _init_workers_ray\n              self._run_workers(\"init_device\")\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/vllm/executor/ray_distributed_executor.py\", line 476, in _run_workers\n              self.driver_worker.execute_method(sent_method, *args, **kwargs)\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 575, in execute_method\n              raise e\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 566, in execute_method\n              return run_method(target, method, args, kwargs)\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/vllm/utils.py\", line 2220, in run_method\n              return func(*args, **kwargs)\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/vllm/worker/worker.py\", line 155, in init_device\n              torch.cuda.set_device(self.device)\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n              torch._C._cuda_setDevice(device)\n            File \"/tmp/ray/session_2025-03-06_15-58-41_147160_1/runtime_resources/pip/c20f8b0f241e058835c7df2d61cb7d5ba10c9661/virtualenv/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 319, in _lazy_init\n              torch._C._cuda_init()\n          RuntimeError: No CUDA GPUs are available\n```\n</details>\n\nnvidia-smi output\n<img width=\"653\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b95c5358-5345-4678-a78b-7d4fefa7e649\" />\n\nHere are my Ray Serve configs and entrypoint script:\n```\n    - name: qwen2-vl-7b-instruct\n      route_prefix: /qwen2-vl-7b-instruct\n      import_path: vllm_florence2_prototype.api_server:build_app\n      runtime_env:\n        pip:\n          - \"git+https://github.com/huggingface/transformers.git@11afab19c0e4b652855f9ed7f82aa010c4f14754\"\n          - \"vllm[video]==0.7.2\"\n          - \"qwen-vl-utils[decord]\"\n          - \"ninja\"\n      deployments:\n      - name: VLLMDeployment\n        max_ongoing_requests: 1000\n        autoscaling_config:\n          min_replicas: 1\n          max_replicas: 5\n      args:\n        model: \"Qwen/Qwen2.5-VL-7B-Instruct-AWQ\"\n        tensor_parallel_size: 4\n        max_model_len: 32768\n        trust_remote_code: true\n        dtype: auto\n        device: \"auto\"\n```\n\nEntrypoint script:\n```\nimport os\n\nfrom typing import Dict, Optional, List\nimport logging\n\nfrom fastapi import FastAPI\nimport pkg_resources\nfrom starlette.requests import Request\nfrom starlette.responses import StreamingResponse, JSONResponse\n\nfrom ray import serve\n\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\nfrom vllm.engine.metrics import RayPrometheusStatLogger\nfrom vllm.entrypoints.openai.cli_args import make_arg_parser\nfrom vllm.entrypoints.openai.protocol import (\n    ChatCompletionRequest,\n    ChatCompletionResponse,\n    ErrorResponse,\n)\nfrom vllm.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom vllm.entrypoints.openai.serving_models import (\n    BaseModelPath,\n    LoRAModulePath,\n    OpenAIServingModels,\n)\nfrom vllm.utils import FlexibleArgumentParser\n\nlogger = logging.getLogger(\"ray.serve\")\n\napp = FastAPI()\n\n\n@serve.deployment(name=\"VLLMDeployment\")\n@serve.ingress(app)\nclass VLLMDeployment:\n    def __init__(\n        self,\n        engine_args: AsyncEngineArgs,\n        response_role: str,\n        lora_modules: Optional[List[LoRAModulePath]] = None,\n        chat_template: Optional[str] = None,\n    ):\n        logger.info(f\"Starting with engine args: {engine_args}\")\n        self.openai_serving_chat = None\n        self.engine_args = engine_args\n        self.response_role = response_role\n        self.lora_modules = lora_modules\n        self.chat_template = chat_template\n        self.engine = AsyncLLMEngine.from_engine_args(engine_args)\n\n    @app.post(\"/v1/chat/completions\")\n    async def create_chat_completion(\n        self, request: ChatCompletionRequest, raw_request: Request\n    ):\n        \"\"\"OpenAI-compatible HTTP endpoint.\n\n        API reference:\n            - https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n        \"\"\"\n        if not self.openai_serving_chat:\n            model_config = await self.engine.get_model_config()\n            # Determine the name of the served model for the OpenAI client.\n            if self.engine_args.served_model_name is not None:\n                served_model_names = self.engine_args.served_model_name\n            else:\n                served_model_names = [self.engine_args.model]\n\n            base_model_paths = [\n                BaseModelPath(name=name, model_path=self.engine_args.model)\n                for name in served_model_names\n            ]\n\n            openai_serving_models = OpenAIServingModels(\n                engine_client=self.engine,\n                model_config=model_config,\n                base_model_paths=base_model_paths,\n            )\n\n            self.openai_serving_chat = OpenAIServingChat(\n                self.engine,\n                model_config,\n                openai_serving_models,\n                response_role=self.response_role,\n                chat_template=self.chat_template,\n                chat_template_content_format=\"auto\",\n                request_logger=None,\n            )\n        logger.info(f\"Request: {request}\")\n        generator = await self.openai_serving_chat.create_chat_completion(\n            request, raw_request\n        )\n        if isinstance(generator, ErrorResponse):\n            return JSONResponse(\n                content=generator.model_dump(), status_code=generator.code\n            )\n        if request.stream:\n            return StreamingResponse(content=generator, media_type=\"text/event-stream\")\n        else:\n            assert isinstance(generator, ChatCompletionResponse)\n            return JSONResponse(content=generator.model_dump())\n\n\ndef parse_vllm_args(cli_args: Dict[str, str]):\n    \"\"\"Parses vLLM args based on CLI inputs.\n\n    Currently uses argparse because vLLM doesn't expose Python models for all of the\n    config options we want to support.\n    \"\"\"\n    parser = FlexibleArgumentParser(description=\"vLLM CLI\")\n    parser = make_arg_parser(parser)\n    arg_strings = []\n    for key, value in cli_args.items():\n        arg_strings.extend([f\"--{key}\", str(value)])\n    logger.info(arg_strings)\n    parsed_args = parser.parse_args(args=arg_strings)\n    return parsed_args\n\n\ndef build_app(cli_args: Dict[str, str]) -> serve.Application:\n    \"\"\"Builds the Serve app based on CLI arguments.\n\n    See https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#command-line-arguments-for-the-server\n    for the complete set of arguments.\n\n    Supported engine arguments: https://docs.vllm.ai/en/latest/models/engine_args.html.\n    \"\"\"  # noqa: E501\n\n    enforce_eager = cli_args.pop(\"enforce_eager\", False)\n    trust_remote_code = cli_args.pop(\"trust_remote_code\", False)\n    disable_custom_all_reduce = cli_args.pop(\"disable_custom_all_reduce\", False)\n    disable_frontend_multiprocessing = cli_args.pop(\n        \"disable_frontend_multiprocessing\", False\n    )\n    disable_async_output_proc = cli_args.pop(\"disable_async_output_proc\", False)\n\n    logger.info(f\"CLI ARGS: {cli_args}\")\n    parsed_args = parse_vllm_args(cli_args)\n    logger.info(f\"PARSED ARGS: {parsed_args}\")\n    engine_args = AsyncEngineArgs.from_cli_args(parsed_args)\n\n    logger.info(f\"ENFORCE EAGER: {enforce_eager}\")\n    logger.info(f\"TRUST REMOTE CODE: {trust_remote_code}\")\n\n    if enforce_eager:\n        engine_args.enforce_eager = True\n    if trust_remote_code:\n        engine_args.trust_remote_code = True\n    if disable_custom_all_reduce:\n        engine_args.disable_custom_all_reduce = True\n    if disable_frontend_multiprocessing:\n        engine_args.disable_frontend_multiprocessing = True\n    if disable_async_output_proc:\n        engine_args.disable_async_output_proc = True\n\n    logger.info(f\"ENGINE ARGS: {engine_args}\")\n    # engine_args.worker_use_ray = True\n    engine_args.worker_use_ray = True\n\n    accelerator = \"GPU\"\n    pg_resources = []\n    tp = engine_args.tensor_parallel_size\n    for i in range(tp):\n        pg_resources.append({\"CPU\": 1, accelerator: 1})\n\n    return VLLMDeployment.options(\n        placement_group_bundles=pg_resources, placement_group_strategy=\"STRICT_PACK\"\n    ).bind(\n        engine_args,\n        parsed_args.response_role,\n        parsed_args.lora_modules,\n        parsed_args.chat_template,\n    )\n    # return VLLMDeployment.bind(\n    #     engine_args,\n    #     parsed_args.response_role,\n    #     parsed_args.lora_modules,\n    #     parsed_args.chat_template,\n    # )\n\n```\n\nHas anyone else seen similar issue? Would really appreciate some help and guidance.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-07T19:55:44+00:00",
    "closed_at": "2025-07-09T02:15:35+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14456/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14456"
  },
  {
    "number": 10283,
    "title": "[Bug]: LLM initialization time increases significantly with larger tensor parallel size and Ray",
    "body": "### Your current environment\r\nvllm 0.5.2\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.24.1\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.8.10 (default, Mar 13 2023, 10:26:41)  [GCC 9.4.0] (64-bit runtime)\r\nPython platform: Linux-5.10.134-008.7.kangaroo.al8.x86_64-x86_64-with-glibc2.29\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.66\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA L20Z\r\nGPU 1: NVIDIA L20Z\r\nGPU 2: NVIDIA L20Z\r\nGPU 3: NVIDIA L20Z\r\nGPU 4: NVIDIA L20Z\r\nGPU 5: NVIDIA L20Z\r\nGPU 6: NVIDIA L20Z\r\nGPU 7: NVIDIA L20Z\r\n\r\nNvidia driver version: 535.161.08\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   52 bits physical, 57 bits virtual\r\nCPU(s):                          100\r\nOn-line CPU(s) list:             0-99\r\nThread(s) per core:              1\r\nCore(s) per socket:              100\r\nSocket(s):                       1\r\nNUMA node(s):                    1\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           143\r\nModel name:                      Intel(R) Xeon(R) Processor\r\nStepping:                        8\r\nCPU MHz:                         2000.000\r\nBogoMIPS:                        4000.00\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       4.7 MiB\r\nL1i cache:                       3.1 MiB\r\nL2 cache:                        200 MiB\r\nL3 cache:                        105 MiB\r\nNUMA node0 CPU(s):               0-99\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Vulnerable\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd avx512vbmi umip pku waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk avx512_fp16 arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==6.1.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.22.2\r\n[pip3] onnx==1.13.1\r\n[pip3] pytorch-quantization==2.1.2\r\n[pip3] torch==2.3.1\r\n[pip3] torch-tensorrt==1.4.0.dev0\r\n[pip3] torchaudio==2.3.1\r\n[pip3] torchtext==0.13.0a0+fae8e8c\r\n[pip3] torchtyping==0.1.4\r\n[pip3] torchvision==0.18.1\r\n[pip3] triton==2.3.1\r\n[conda] Could not collect\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\njust test the vllm init time\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\n### Issue Description\r\nWe observed significant and unexpected increases in VLLM initialization time when scaling tensor parallelism (TP), especially with Ray enabled.\r\n\r\n### Observed Behavior\r\n- TP=1: ~7 seconds initialization time\r\n- TP=4: ~14 seconds initialization time\r\n- TP=4 with Ray: ~24 seconds initialization time\r\n\r\n### Expected Behavior\r\nInitialization time should remain relatively constant or have minimal increase when scaling tensor parallelism and use ray.\r\n\r\n### Environment\r\n- VLLM version: 0.5.2\r\n- Model: Qwen2-7B\r\n- GPU: NVIDIA L20Z\r\n- Number of GPUs: 8\r\n\r\n### Additional Context\r\nThe initialization time increase appears disproportionate to the tensor parallel size, suggesting a potential bottleneck in the initialization process, particularly when Ray is involved.\r\n\r\n### Reproducible Steps\r\n1. Run VLLM with TP=1\r\n2. Run VLLM with TP=4\r\n3. Run VLLM with TP=4 and Ray enabled\r\n4. Compare initialization times\r\n\r\n\r\n### vllm start time \r\n```python\r\ndef run_vllm(\r\n    requests: List[Tuple[str, int, int]],\r\n    model: str,\r\n    tokenizer: str,\r\n    quantization: Optional[str],\r\n    tensor_parallel_size: int,\r\n    seed: int,\r\n    n: int,\r\n    use_beam_search: bool,\r\n    trust_remote_code: bool,\r\n    dtype: str,\r\n    max_model_len: Optional[int],\r\n    enforce_eager: bool,\r\n    kv_cache_dtype: str,\r\n    quantization_param_path: Optional[str],\r\n    device: str,\r\n    enable_prefix_caching: bool,\r\n    enable_chunked_prefill: bool,\r\n    max_num_batched_tokens: int,\r\n    distributed_executor_backend: Optional[str],\r\n    gpu_memory_utilization: float = 0.9,\r\n    num_scheduler_steps: int = 1,\r\n    use_v2_block_manager: bool = False,\r\n    download_dir: Optional[str] = None,\r\n    load_format: str = EngineArgs.load_format,\r\n    disable_async_output_proc: bool = False,\r\n) -> float:\r\n    # \u5bfc\u5165\u5fc5\u8981\u7684\u5e93\r\n    from vllm import LLM, SamplingParams\r\n\r\n    print(f\"Start initializing LLM at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\r\n    start = time.perf_counter()\r\n    llm = LLM(\r\n        model=model,\r\n        tokenizer=tokenizer,\r\n        quantization=quantization,\r\n        tensor_parallel_size=tensor_parallel_size,\r\n        seed=seed,\r\n        trust_remote_code=trust_remote_code,\r\n        dtype=dtype,\r\n        max_model_len=max_model_len,\r\n        gpu_memory_utilization=gpu_memory_utilization,\r\n        enforce_eager=enforce_eager,\r\n        kv_cache_dtype=kv_cache_dtype,\r\n        quantization_param_path=quantization_param_path,\r\n        device=device,\r\n        enable_prefix_caching=enable_prefix_caching,\r\n        download_dir=download_dir,\r\n        enable_chunked_prefill=enable_chunked_prefill,\r\n        max_num_batched_tokens=max_num_batched_tokens,\r\n        distributed_executor_backend=distributed_executor_backend,\r\n        load_format=load_format,\r\n        # num_scheduler_steps=num_scheduler_steps,\r\n        # use_v2_block_manager=use_v2_block_manager,\r\n        # disable_async_output_proc=disable_async_output_proc,\r\n    )\r\n    end = time.perf_counter()\r\n    print(f\"Finish initializing LLM at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\r\n    print(f\"vllm init time: {end - start}\")\r\n```\r\n\r\n### vllm ray start time\r\n```python\r\ndef run_ray_vllm(\r\n    requests: List[Tuple[str, int, int]],\r\n    model: str,\r\n    tokenizer: str,\r\n    quantization: Optional[str],\r\n    tensor_parallel_size: int,\r\n    seed: int,\r\n    n: int,\r\n    use_beam_search: bool,\r\n    trust_remote_code: bool,\r\n    dtype: str,\r\n    max_model_len: Optional[int],\r\n    enforce_eager: bool,\r\n    kv_cache_dtype: str,\r\n    quantization_param_path: Optional[str],\r\n    device: str,\r\n    enable_prefix_caching: bool,\r\n    enable_chunked_prefill: bool,\r\n    max_num_batched_tokens: int,\r\n    distributed_executor_backend: Optional[str],\r\n    gpu_memory_utilization: float = 0.9,\r\n    num_scheduler_steps: int = 1,\r\n    use_v2_block_manager: bool = False,\r\n    download_dir: Optional[str] = None,\r\n    load_format: str = EngineArgs.load_format,\r\n    disable_async_output_proc: bool = False,\r\n) -> float:\r\n    # \u5bfc\u5165\u5fc5\u8981\u7684\u5e93\r\n    from vllm import LLM, SamplingParams\r\n\r\n    import ray\r\n\r\n    @ray.remote\r\n    class LLMWorker:\r\n        def __init__(self, model, tokenizer, quantization, tensor_parallel_size, seed, trust_remote_code, dtype, max_model_len, gpu_memory_utilization, enforce_eager, kv_cache_dtype, quantization_param_path, device, enable_prefix_caching, download_dir, enable_chunked_prefill, max_num_batched_tokens, distributed_executor_backend, load_format, num_scheduler_steps, use_v2_block_manager, disable_async_output_proc):\r\n            from vllm import LLM\r\n            start = time.perf_counter()\r\n            self.llm = LLM(\r\n                model=model,\r\n                tokenizer=tokenizer,\r\n                quantization=quantization,\r\n                tensor_parallel_size=tensor_parallel_size,\r\n                seed=seed,\r\n                trust_remote_code=trust_remote_code,\r\n                dtype=dtype,\r\n                max_model_len=max_model_len,\r\n                gpu_memory_utilization=gpu_memory_utilization,\r\n                enforce_eager=enforce_eager,\r\n                kv_cache_dtype=kv_cache_dtype,\r\n                quantization_param_path=quantization_param_path,\r\n                device=device,\r\n                enable_prefix_caching=enable_prefix_caching,\r\n                download_dir=download_dir,\r\n                enable_chunked_prefill=enable_chunked_prefill,\r\n                max_num_batched_tokens=max_num_batched_tokens,\r\n                distributed_executor_backend=distributed_executor_backend,\r\n                load_format=load_format,\r\n                # num_scheduler_steps=num_scheduler_steps,\r\n                # use_v2_block_manager=use_v2_block_manager,\r\n                # disable_async_output_proc=disable_async_output_proc,\r\n            )\r\n            end = time.perf_counter()\r\n            print(f\"Finish initializing LLM at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\r\n            print(f\"vllm init time: {end - start}\")\r\n\r\n        def generate(self, prompts, sampling_params):\r\n            return self.llm.generate(prompts, sampling_params, use_tqdm=True)\r\n\r\n    # Create LLM worker\r\n    worker = LLMWorker.remote(\r\n        model=model,\r\n        tokenizer=tokenizer,\r\n        quantization=quantization,\r\n        tensor_parallel_size=tensor_parallel_size,\r\n        seed=seed,\r\n        trust_remote_code=trust_remote_code,\r\n        dtype=dtype,\r\n        max_model_len=max_model_len,\r\n        gpu_memory_utilization=gpu_memory_utilization,\r\n        enforce_eager=enforce_eager,\r\n        kv_cache_dtype=kv_cache_dtype,\r\n        quantization_param_path=quantization_param_path,\r\n        device=device,\r\n        enable_prefix_caching=enable_prefix_caching,\r\n        download_dir=download_dir,\r\n        enable_chunked_prefill=enable_chunked_prefill,\r\n        max_num_batched_tokens=max_num_batched_tokens,\r\n        distributed_executor_backend=distributed_executor_backend,\r\n        load_format=load_format,\r\n        num_scheduler_steps=num_scheduler_steps,\r\n        use_v2_block_manager=use_v2_block_manager,\r\n        disable_async_output_proc=disable_async_output_proc,\r\n    )\r\n\r\n    # Add the requests to the engine.\r\n    prompts: List[str] = []\r\n    sampling_params: List[SamplingParams] = []\r\n    for prompt, _, output_len in requests:\r\n        prompts.append(prompt)\r\n        sampling_params.append(\r\n            SamplingParams(\r\n                n=n,\r\n                temperature=0.0 if use_beam_search else 1.0,\r\n                top_p=1.0,\r\n                use_beam_search=use_beam_search,\r\n                ignore_eos=True,\r\n                max_tokens=output_len,\r\n            )\r\n        )\r\n\r\n    start = time.perf_counter()\r\n    ray.get(worker.generate.remote(prompts, sampling_params))\r\n    end = time.perf_counter()\r\n    return end - start\r\n```",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "closed",
    "created_at": "2024-11-13T04:08:02+00:00",
    "closed_at": "2024-12-19T07:38:03+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10283/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10283"
  },
  {
    "number": 14413,
    "title": "[Bug]: RuntimeError: No CUDA GPUs are available | when using TP>1 and using vllm v0.7",
    "body": "### Your current environment\n\n### Environment\n```\nroot@7dc9530a8c13:/workspace# python collect_env.py \nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.30.5\nLibc version: glibc-2.35\n\nPython version: 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) [GCC 12.3.0] (64-bit runtime)\nPython platform: Linux-5.10.223-212.873.amzn2.x86_64-x86_64-with-glibc2.35\nIs CUDA available: False\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: Could not collect\nNvidia driver version: Could not collect\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.1.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               192\nOn-line CPU(s) list:                  0-191\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 7R13 Processor\nCPU family:                           25\nModel:                                1\nThread(s) per core:                   2\nCore(s) per socket:                   48\nSocket(s):                            2\nStepping:                             1\nBogoMIPS:                             5299.99\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            3 MiB (96 instances)\nL1i cache:                            3 MiB (96 instances)\nL2 cache:                             48 MiB (96 instances)\nL3 cache:                             384 MiB (12 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-47,96-143\nNUMA node1 CPU(s):                    48-95,144-191\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; safe RET, no microcode\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] pytorch-lightning==2.5.0.post0\n[pip3] torch==2.5.1+cu124\n[pip3] torch-model-archiver==0.12.0\n[pip3] torchaudio==2.5.1+cu124\n[pip3] torchdata==0.7.1\n[pip3] torchmetrics==1.6.2\n[pip3] torchserve==0.12.0\n[pip3] torchtext==0.18.0+cu124\n[pip3] torchvision==0.20.1+cu124\n[conda] mkl                       2025.0.0           h7f037a6_939    conda-forge\n[conda] mkl-include               2025.0.0           hf2ce2f3_939    conda-forge\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] pytorch-lightning         2.5.0.post0              pypi_0    pypi\n[conda] torch                     2.5.1+cu124              pypi_0    pypi\n[conda] torch-model-archiver      0.12.0                   pypi_0    pypi\n[conda] torchaudio                2.5.1+cu124              pypi_0    pypi\n[conda] torchdata                 0.7.1                    pypi_0    pypi\n[conda] torchmetrics              1.6.2                    pypi_0    pypi\n[conda] torchserve                0.12.0                   pypi_0    pypi\n[conda] torchtext                 0.18.0+cu124             pypi_0    pypi\n[conda] torchvision               0.20.1+cu124             pypi_0    pypi\n```\n\n\n### \ud83d\udc1b Describe the bug\n\n\n### Description\nWhen attempting to run distributed inference with Tensor Parallelism (TP) > 1 following the example from the [distributed inference documentation](https://docs.vllm.ai/en/latest/getting_started/examples/distributed.html), the system fails with a \"No CUDA GPUs are available\" error. \n\nSingle GPU inference (TP=1) works correctly with the same setup and docker image. The error occurs specifically when attempting to use Tensor Parallelism with multiple GPUs\n\n\n### Error Message\nThe key error occurs during worker initialization:\n\n\n<details>\n```\n\n\u001b[36m(_MapWorker pid=13019)\u001b[0m DEBUG 03-07 05:56:13 __init__.py:28] No plugins for group vllm.platform_plugins found.\n--\n\u00a0 | 2025-03-07T05:56:13.899Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m INFO 03-07 05:56:13 __init__.py:207] Automatically detected platform cuda.\n\u00a0 | 2025-03-07T05:56:14.845Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m DEBUG 03-07 05:56:14 __init__.py:28] No plugins for group vllm.general_plugins found.\n\u00a0 | 2025-03-07T05:56:19.773Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m INFO 03-07 05:56:19 config.py:549] This model supports multiple tasks: {'score', 'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\n\u00a0 | 2025-03-07T05:56:19.773Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m INFO 03-07 05:56:19 config.py:1382] Defaulting to use ray for distributed inference\n\u00a0 | 2025-03-07T05:56:19.773Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m INFO 03-07 05:56:19 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/root/.cache/mfive_cache/neo_alpha_stage3_178k_ift_train1', speculative_config=None, tokenizer='/root/.cache/refactor', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/root/.cache/refactor, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False,\n\u00a0 | 2025-03-07T05:56:19.986Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m INFO 03-07 05:56:19 ray_distributed_executor.py:153] use_ray_spmd_worker: False\n\u00a0 | 2025-03-07T05:56:19.987Z | Running 0: 0.00 row [00:00, ? row/s] Running 0: 0.00 row [00:05, ? row/s] Running 0: 0.00 row [00:05, ? row/s] Running 0: 0.00 row [00:05, ? row/s] Running 0: 0.00 row [00:10, ? row/s] Running 0: 0.00 row [00:10, ? row/s] Running 0: 0.00 row [00:10, ? row/s] Running 0: 0.00 row [00:11, ? row/s] \u001b[36m(_MapWorker pid=13019)\u001b[0m Connecting to existing Ray cluster at address: 10.0.181.24:6379...\n\u00a0 | 2025-03-07T05:56:19.987Z | Running 0: 0.00 row [00:11, ? row/s] \u001b[36m(_MapWorker pid=13019)\u001b[0m Calling ray.init() again after it has already been called.\n\u00a0 | 2025-03-07T05:56:24.024Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m DEBUG 03-07 05:56:23 ray_distributed_executor.py:228] workers: [RayWorkerMetaData(worker=Actor(RayWorkerWrapper, 353445ca625de70439423cbb01000000), created_rank=1, adjusted_rank=-1, ip='10.0.181.24'), RayWorkerMetaData(worker=Actor(RayWorkerWrapper, b7d9afd43cc0417242a51f4901000000), created_rank=2, adjusted_rank=-1, ip='10.0.181.24'), RayWorkerMetaData(worker=Actor(RayWorkerWrapper, 13264b225baabba10e83285201000000), created_rank=3, adjusted_rank=-1, ip='10.0.181.24'), RayWorkerMetaData(worker=Actor(RayWorkerWrapper, 3b3dcac0f918f87e00849a4401000000), created_rank=4, adjusted_rank=-1, ip='10.0.181.24'), RayWorkerMetaData(worker=Actor(RayWorkerWrapper, da52d7162df79b048ac6758601000000), created_rank=5, adjusted_rank=-1, ip='10.0.181.24'), RayWorkerMetaData(worker=Actor(RayWorkerWrapper, c0b404b84c1ae03c0360b14301000000), created_rank=6, adjusted_rank=-1, ip='10.0.181.24'), RayWorkerMetaData(worker=Actor(RayWorkerWrapper, b961ec695edd536e92872ae901000000), created_rank=7, adjusted_rank=-1, ip='10.0.181.24')]\n\u00a0 | 2025-03-07T05:56:24.024Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m DEBUG 03-07 05:56:23 ray_distributed_executor.py:229] driver_dummy_worker: Actor(RayWorkerWrapper, 7a6a28ddc9eadad91aa9994001000000)\n\u00a0 | 2025-03-07T05:56:24.024Z | \u001b[36m(RayWorkerWrapper pid=13533)\u001b[0m DEBUG 03-07 05:56:23 __init__.py:28] No plugins for group vllm.platform_plugins found.\n\u00a0 | 2025-03-07T05:56:24.024Z | \u001b[36m(RayWorkerWrapper pid=13533)\u001b[0m INFO 03-07 05:56:24 __init__.py:207] Automatically detected platform cuda.\n\u00a0 | 2025-03-07T05:56:24.934Z | \u001b[36m(RayWorkerWrapper pid=13534)\u001b[0m DEBUG 03-07 05:56:24 __init__.py:28] No plugins for group vllm.general_plugins found.\n\u00a0 | 2025-03-07T05:56:25.153Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m INFO 03-07 05:56:25 cuda.py:229] Using Flash Attention backend.\n\u00a0 | 2025-03-07T05:56:25.153Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m DEBUG 03-07 05:56:25 config.py:3461] enabled custom ops: Counter()\n\u00a0 | 2025-03-07T05:56:25.153Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m DEBUG 03-07 05:56:25 config.py:3463] disabled custom ops: Counter()\n\u00a0 | 2025-03-07T05:56:27.269Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m ERROR 03-07 05:56:27 worker_base.py:581] Error executing method 'init_device'. This might cause deadlock in distributed execution.\n\u00a0 | 2025-03-07T05:56:27.270Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m ERROR 03-07 05:56:27 worker_base.py:581] Traceback (most recent call last):\n\u00a0 | 2025-03-07T05:56:27.270Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m ERROR 03-07 05:56:27 worker_base.py:581] File \"/opt/conda/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 573, in execute_method\n\u00a0 | 2025-03-07T05:56:27.270Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m ERROR 03-07 05:56:27 worker_base.py:581] return run_method(target, method, args, kwargs)\n\u00a0 | 2025-03-07T05:56:27.270Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m ERROR 03-07 05:56:27 worker_base.py:581] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u00a0 | 2025-03-07T05:56:27.270Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m ERROR 03-07 05:56:27 worker_base.py:581] File \"/opt/conda/lib/python3.11/site-packages/vllm/utils.py\", line 2196, in run_method\n\u00a0 | 2025-03-07T05:56:27.270Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m ERROR 03-07 05:56:27 worker_base.py:581] return func(*args, **kwargs)\n\u00a0 | 2025-03-07T05:56:27.270Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m ERROR 03-07 05:56:27 worker_base.py:581] ^^^^^^^^^^^^^^^^^^^^^\n\u00a0 | 2025-03-07T05:56:27.270Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m ERROR 03-07 05:56:27 worker_base.py:581] File \"/opt/conda/lib/python3.11/site-packages/vllm/worker/worker.py\", line 155, in init_device\n\u00a0 | 2025-03-07T05:56:27.270Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m ERROR 03-07 05:56:27 worker_base.py:581] torch.cuda.set_device(self.device)\n\u00a0 | 2025-03-07T05:56:27.270Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m ERROR 03-07 05:56:27 worker_base.py:581] File \"/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 478, in set_device\n\u00a0 | 2025-03-07T05:56:27.270Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m ERROR 03-07 05:56:27 worker_base.py:581] torch._C._cuda_setDevice(device)\n\u00a0 | 2025-03-07T05:56:27.270Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m ERROR 03-07 05:56:27 worker_base.py:581] File \"/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 319, in _lazy_init\n\u00a0 | 2025-03-07T05:56:27.270Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m ERROR 03-07 05:56:27 worker_base.py:581] torch._C._cuda_init()\n\u00a0 | 2025-03-07T05:56:27.270Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m ERROR 03-07 05:56:27 worker_base.py:581] RuntimeError: No CUDA GPUs are available\n\u00a0 | 2025-03-07T05:56:27.270Z | \u001b[36m(_MapWorker pid=13019)\u001b[0m INFO 03-07 05:56:27 ray_distributed_executor.py:104] Shutting down Ray distributed executor. If you see error log from logging.cc regarding SIGTERM received, please ignore because this is the expected termination process in Ray.\n\u00a0\n\n<br class=\"Apple-interchange-newline\">\n```\n\n\n</details>\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "closed",
    "created_at": "2025-03-07T06:59:47+00:00",
    "closed_at": "2025-04-01T04:02:28+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14413/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14413"
  },
  {
    "number": 7904,
    "title": "[Bug]: ray + vllm async engine: Background loop is stopped",
    "body": "### \ud83d\udc1b Describe the bug\r\nthis code is slighly modified from [async llm engine test](https://github.com/vllm-project/vllm/blob/4cf256ae7f8b0be8f06f6b85821e55d4f5bdaa13/tests/async_engine/test_async_llm_engine.py#L115)\r\n\r\n```\r\ndef test_asyncio_run():\r\n    wait_for_gpu_memory_to_clear(\r\n        devices=list(range(torch.cuda.device_count())),\r\n        threshold_bytes=2 * 2**30,\r\n        timeout_s=60,\r\n    )\r\n\r\n    engine = AsyncLLMEngine.from_engine_args(\r\n        AsyncEngineArgs(model=\"facebook/opt-125m\"))\r\n\r\n    async def run(prompt: str):\r\n        sampling_params = SamplingParams(\r\n            temperature=0,\r\n            max_tokens=32,\r\n        )\r\n\r\n        async for output in engine.generate(prompt,\r\n                                            sampling_params,\r\n                                            request_id=prompt):\r\n            final_output = output\r\n        return final_output\r\n\r\n    async def generate():\r\n        return await asyncio.gather(\r\n            run(\"test0\"),\r\n            run(\"test1\"),\r\n        )\r\n\r\n    results = asyncio.run(generate())\r\n    results = asyncio.run(generate()) # called it twice, this will met error\r\n    assert len(results) == 2\r\n\r\n```\r\nThis works perfect, but when it's run under ray, the 2th  ` results = asyncio.run(generate()) ` will get the error message:\r\n\r\n```\r\n[[36m(VLLMActor pid=50643) [[0m INFO 08-17 14:18:16 async_llm_engine.py:140] Finished request ed5d5c0f908a4a478793db4a47b4a772.\r\n\r\n[[36m(VLLMActor pid=50643) [[0m INFO 08-17 14:18:16 async_llm_engine.py:51] Engine is gracefully shutting down.\r\n```\r\nI added vllm async health check and find the full traceback\r\n```\r\n[rank0]:   File \"/mnt/rlhf/vllm_generation/vllm_actor.py\", line 122, in health_check\r\n[rank0]:     return asyncio.run(self.llm.check_health())\r\n[rank0]:   File \"/usr/lib/python3.8/asyncio/runners.py\", line 44, in run\r\n[rank0]:     return loop.run_until_complete(main)\r\n[rank0]:   File \"/usr/lib/python3.8/asyncio/base_events.py\", line 616, in run_until_complete\r\n[rank0]:     return future.result()\r\n[rank0]:   File \"/usr/local/lib/python3.8/dist-packages/vllm/engine/async_llm_engine.py\", line 943, in check_health\r\n[rank0]:     raise AsyncEngineDeadError(\"Background loop is stopped.\")\r\n[rank0]: vllm.engine.async_llm_engine.AsyncEngineDeadError: Background loop is stopped.\r\n```\r\n\r\nDon't known why async_llm_engine's background loop stopped. and it also said \"Engine is gracefully shutting down.\". But it's not expected\r\n### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.24.1\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.8.10 (default, Mar 13 2023, 10:26:41)  [GCC 9.4.0] (64-bit runtime)\r\nPython platform: Linux-5.4.119-19.0009.28-x86_64-with-glibc2.29\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.66\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H800\r\nGPU 1: NVIDIA H800\r\nGPU 2: NVIDIA H800\r\nGPU 3: NVIDIA H800\r\nGPU 4: NVIDIA H800\r\nGPU 5: NVIDIA H800\r\nGPU 6: NVIDIA H800\r\nGPU 7: NVIDIA H800\r\n\r\nNvidia driver version: 535.129.03\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   52 bits physical, 48 bits virtual\r\nCPU(s):                          172\r\nOn-line CPU(s) list:             0-171\r\nThread(s) per core:              2\r\nCore(s) per socket:              43\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           143\r\nModel name:                      Intel(R) Xeon(R) Platinum 8476C\r\nStepping:                        6\r\nCPU MHz:                         2600.000\r\nBogoMIPS:                        5200.00\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       4 MiB\r\nL1i cache:                       2.7 MiB\r\nL2 cache:                        172 MiB\r\nL3 cache:                        195 MiB\r\nNUMA node0 CPU(s):               0-85\r\nNUMA node1 CPU(s):               86-171\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb ibrs_enhanced fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 wbnoinvd arat avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq movdiri movdir64b fsrm arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==6.1.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.22.2\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==8.9.2.26\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-dali-cuda110==1.24.0\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.5.40\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] nvidia-pyindex==1.0.9\r\n[pip3] onnx==1.13.1\r\n[pip3] pynvml==11.4.1\r\n[pip3] pytorch-quantization==2.1.2\r\n[pip3] pyzmq==25.0.2\r\n[pip3] torch==2.3.1\r\n[pip3] torch-tensorrt==1.4.0.dev0\r\n[pip3] torchaudio==2.3.1\r\n[pip3] torchtext==0.13.0a0+fae8e8c\r\n[pip3] torchtyping==0.1.4\r\n[pip3] torchvision==0.18.1\r\n[pip3] transformers==4.37.2\r\n[pip3] triton==2.3.1\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.2@4cf256ae7f8b0be8f06f6b85821e55d4f5bdaa13\r\nvLLM Build Flags:\r\nCUDA Archs: 5.2 6.0 6.1 7.0 7.5 8.0 8.6 9.0+PTX; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV8     NV8     NV8     NV8     NV8     NV8     NV8     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-85    0               N/A\r\nGPU1    NV8      X      NV8     NV8     NV8     NV8     NV8     NV8     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     0-85    0               N/A\r\nGPU2    NV8     NV8      X      NV8     NV8     NV8     NV8     NV8     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     0-85    0               N/A\r\nGPU3    NV8     NV8     NV8      X      NV8     NV8     NV8     NV8     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     0-85    0               N/A\r\nGPU4    NV8     NV8     NV8     NV8      X      NV8     NV8     NV8     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     86-171  1               N/A\r\nGPU5    NV8     NV8     NV8     NV8     NV8      X      NV8     NV8     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     86-171  1               N/A\r\nGPU6    NV8     NV8     NV8     NV8     NV8     NV8      X      NV8     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     86-171  1               N/A\r\nGPU7    NV8     NV8     NV8     NV8     NV8     NV8     NV8      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     86-171  1               N/A\r\nNIC0    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC1    SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC2    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS\r\nNIC3    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS\r\nNIC4    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS\r\nNIC5    SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS\r\nNIC6    SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS\r\nNIC7    SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_bond_0\r\n  NIC1: mlx5_bond_1\r\n  NIC2: mlx5_bond_2\r\n  NIC3: mlx5_bond_3\r\n  NIC4: mlx5_bond_4\r\n  NIC5: mlx5_bond_5\r\n  NIC6: mlx5_bond_6\r\n  NIC7: mlx5_bond_7```\r\n\r\n</details>\r\n\r\n\r\n",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "closed",
    "created_at": "2024-08-27T09:23:44+00:00",
    "closed_at": "2025-04-03T15:47:03+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7904/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7904"
  },
  {
    "number": 21070,
    "title": "[Bug]: PD does not work with ray distributed backend",
    "body": "### Your current environment\n\n<details>\n<summary> run vllm.sh which uses ray as the backend </code></summary>\n\n```text\n#!/bin/bash\nset -xe\n\n# Models to run\nMODELS=(\n  \"Qwen/Qwen2.5-0.5B-Instruct\"\n  # \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\"\n)\n\nexport VLLM_LOGGING_LEVEL=debug\n# export NIXL_LOG_LEVEL=DEBUG\n# export UCX_LOG_LEVEL=trace\n\n# Number of prefill and decode instances to create\nNUM_PREFILL_INSTANCES=${NUM_PREFILL_INSTANCES:-1} # Default to 1\nNUM_DECODE_INSTANCES=${NUM_DECODE_INSTANCES:-1}   # Default to 2\n\n# Find the git repository root directory\n# GIT_ROOT=$(git rev-parse --show-toplevel)\n\nSMI_BIN=$(which nvidia-smi || which rocm-smi)\n\n# Trap the SIGINT signal (triggered by Ctrl+C)\ntrap 'kill $(jobs -pr)' SIGINT SIGTERM EXIT\n\n# Waits for vLLM to start.\nwait_for_server() {\n  local port=$1\n  timeout 1200 bash -c \"\n    until curl -s localhost:${port}/v1/completions > /dev/null; do\n      sleep 1\n    done\" && return 0 || return 1\n}\n\n# # Function to clean up previous instances\n# cleanup_instances() {\n#   echo \"Cleaning up any running vLLM instances...\"\n#   pkill -f \"vllm serve\" || true\n#   sleep 2\n# }\n\n# Handle to get model-specific arguments for deepseek\nget_model_args() {\n  local model_name=$1\n  local extra_args=\"\"\n\n  if [[ \"$model_name\" == \"deepseek-ai/deepseek-vl2-tiny\" ]]; then\n    extra_args=\"--hf_overrides '{\\\"architectures\\\": [\\\"DeepseekVLV2ForCausalLM\\\"]}' --trust-remote-code\"\n  fi\n\n  echo \"$extra_args\"\n}\n\nget_num_gpus() {\n  if [[ \"$SMI_BIN\" == *\"nvidia\"* ]]; then\n    echo \"$($SMI_BIN --query-gpu=name --format=csv,noheader | wc -l)\"\n  else\n    echo \"$($SMI_BIN -l | grep GPU | wc -l)\"\n  fi\n}\n\n# Function to run tests for a specific model\nrun_tests_for_model() {\n  local model_name=$1\n  echo \"================================\"\n  echo \"Testing model: $model_name\"\n  echo \"================================\"\n\n  # Get model-specific arguments\n  local model_args=$(get_model_args \"$model_name\")\n\n  # Arrays to store all hosts and ports\n  PREFILL_HOSTS=()\n  PREFILL_PORTS=()\n  DECODE_HOSTS=()\n  DECODE_PORTS=()\n\n  # Start prefill instances\n  for i in $(seq 0 $((NUM_PREFILL_INSTANCES-1))); do\n    # Calculate GPU ID - we'll distribute across available GPUs\n    GPU_ID=$((i % $(get_num_gpus)))\n    # GPU_ID=3\n    # Calculate port number (base port + instance number)\n    PORT=$((9570 + i))\n    # Calculate side channel port\n    SIDE_CHANNEL_PORT=$((4000 + i))\n\n    echo \"Starting prefill instance $i on GPU $GPU_ID, port $PORT\"\n\n    # Build the command with or without model-specific args\n    BASE_CMD=\"CUDA_VISIBLE_DEVICES=$GPU_ID VLLM_NIXL_SIDE_CHANNEL_PORT=$SIDE_CHANNEL_PORT vllm serve $model_name \\\n    --port $PORT \\\n    --disable-log-requests \\\n    --gpu-memory-utilization 0.9 \\\n    --distributed-executor-backend 'ray' \\\n    --enforce-eager \\\n    --tensor-parallel-size 1 \\\n    --kv-transfer-config '{\\\"kv_connector\\\":\\\"NixlConnector\\\",\\\"kv_role\\\":\\\"kv_both\\\"}'\"\n\n    if [ -n \"$model_args\" ]; then\n    FULL_CMD=\"$BASE_CMD $model_args\"\n    else\n    FULL_CMD=\"$BASE_CMD\"\n    fi\n\n    eval \"$FULL_CMD &\"\n\n    # Store host and port for proxy configuration\n    PREFILL_HOSTS+=(\"localhost\")\n    PREFILL_PORTS+=($PORT)\n  done\n\n  # Start decode instances\n  for i in $(seq 0 $((NUM_DECODE_INSTANCES-1))); do\n    # Calculate GPU ID - we'll distribute across available GPUs, starting from after prefill GPUs\n    GPU_ID=$(((i + NUM_PREFILL_INSTANCES) % $(get_num_gpus)))\n    # GPU_ID=4\n    # Calculate port number (base port + instance number)\n    PORT=$((9560 + i))\n    # Calculate side channel port\n    SIDE_CHANNEL_PORT=$((4100 + i))\n\n    echo \"Starting decode instance $i on GPU $GPU_ID, port $PORT\"\n\n    # Build the command with or without model-specific args\n    BASE_CMD=\"CUDA_VISIBLE_DEVICES=$GPU_ID VLLM_NIXL_SIDE_CHANNEL_PORT=$SIDE_CHANNEL_PORT vllm serve $model_name \\\n    --port $PORT \\\n    --disable-log-requests \\\n    --enforce-eager \\\n    --distributed-executor-backend 'ray' \\\n    --gpu-memory-utilization 0.9 \\\n    --tensor-parallel-size 1 \\\n    --kv-transfer-config '{\\\"kv_connector\\\":\\\"NixlConnector\\\",\\\"kv_role\\\":\\\"kv_both\\\"}'\"\n\n    if [ -n \"$model_args\" ]; then\n    FULL_CMD=\"$BASE_CMD $model_args\"\n    else\n    FULL_CMD=\"$BASE_CMD\"\n    fi\n\n    eval \"$FULL_CMD &\"\n\n    # Store host and port for proxy configuration\n    DECODE_HOSTS+=(\"localhost\")\n    DECODE_PORTS+=($PORT)\n  done\n\n  # Wait for all instances to start\n  for PORT in \"${PREFILL_PORTS[@]}\"; do\n    echo \"Waiting for prefill instance on port $PORT to start...\"\n    wait_for_server $PORT\n  done\n\n  for PORT in \"${DECODE_PORTS[@]}\"; do\n    echo \"Waiting for decode instance on port $PORT to start...\"\n    wait_for_server $PORT\n  done\n\n  # Build the command for the proxy server with all the hosts and ports\n  PROXY_CMD=\"python ./toy_proxy_server.py --port 8192\"\n\n  # Add all prefill hosts and ports\n  PROXY_CMD+=\" --prefiller-hosts ${PREFILL_HOSTS[@]}\"\n  PROXY_CMD+=\" --prefiller-ports ${PREFILL_PORTS[@]}\"\n\n  # Add all decode hosts and ports\n  PROXY_CMD+=\" --decoder-hosts ${DECODE_HOSTS[@]}\"\n  PROXY_CMD+=\" --decoder-ports ${DECODE_PORTS[@]}\"\n\n  # Start the proxy server\n  echo \"Starting proxy server with command: $PROXY_CMD\"\n  $PROXY_CMD &\n\n  # Wait for the proxy to start\n\n  # Run lm eval for this model\n  echo \"Running tests for $model_name\"\n  sleep 10000\n  # TEST_MODEL=$model_name python -m pytest -s -x ${GIT_ROOT}/tests/v1/kv_connector/nixl_integration/test_accuracy.py\n\n  # # Clean up before running next model\n  # cleanup_instances\n  # sleep 3\n}\n\n# Run tests for each model\nfor model in \"${MODELS[@]}\"; do\n  run_tests_for_model \"$model\"\ndone\n\n# echo \"All tests completed!\"\n\n```\n\n</details>\n\nIt turns out after https://github.com/vllm-project/vllm/pull/19555, choosing ray distributed executor backend along with nixl_connector is broken. \n\nThe reason is that the changes to multiProcExecutor are not replicated for ray executor, so the ray executor will hang making D wait indefinitely for P (since finished_recving and finished_sending are not properly filled out)\n\n### \ud83d\udc1b Describe the bug\n\nN/A\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "open",
    "created_at": "2025-07-16T18:34:27+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/21070/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/21070"
  }
]
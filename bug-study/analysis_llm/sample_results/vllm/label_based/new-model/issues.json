[
  {
    "number": 14214,
    "title": "[New Model]: pfnet/plamo-2-8b",
    "body": "### The model to consider.\n\nPlease add support for PFN's plamo-2-8b https://huggingface.co/pfnet/plamo-2-8b\n\n### The closest model vllm already supports.\n\n_No response_\n\n### What's your difficulty of supporting the model you want?\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-04T14:59:42+00:00",
    "closed_at": "2025-07-11T02:16:10+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14214/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14214"
  },
  {
    "number": 16106,
    "title": "[New Model]: Llama4 Support",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nMeta released 2 Variants:\n\nLlama 4 Scout:\nA high-performing small model with 17B activated parameters across 16 experts. Extremely fast, natively multimodal, supports a 10M+ token context window, and runs on a single GPU.\n\nLlama 4 Maverick:\nA top-tier multimodal model outperforming GPT-4o and Gemini 2.0 Flash, with performance on par with DeepSeek V3 at half the active parameters. ELO 1417 on LMArena and runs on a single host.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2025-04-05T21:39:19+00:00",
    "closed_at": "2025-04-06T04:32:40+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16106/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16106"
  },
  {
    "number": 5124,
    "title": "[New Model]: LLaVA-NeXT-Video support",
    "body": "### The model to consider.\n\nThe llava-next-video project has already been released, and the test results are quite good. Are there any plans to support this project?\r\n`https://github.com/LLaVA-VL/LLaVA-NeXT/blob/inference/docs/LLaVA-NeXT-Video.md`\r\nCurrently, Hugging Face does not support this model.\n\n### The closest model vllm already supports.\n\n_No response_\n\n### What's your difficulty of supporting the model you want?\n\n_No response_",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2024-05-30T03:22:17+00:00",
    "closed_at": "2024-09-11T05:21:37+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5124/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5124"
  },
  {
    "number": 170,
    "title": "Support for Starcoder",
    "body": "Does this work with Starcoder? The readme lists gpt-2 which is starcoder base architecture, has anyone tried it yet?",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2023-06-20T23:24:39+00:00",
    "closed_at": "2023-06-22T18:00:45+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/170/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/170"
  },
  {
    "number": 388,
    "title": "supporting superhot models?",
    "body": "specifically: \r\n- https://huggingface.co/kaiokendev/superhot-30b-8k-no-rlhf-test\r\n\r\ni've confirmed that i can load the model in vllm and successfully generate completions but the context length is still only 2048. \r\n",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2023-07-07T09:22:25+00:00",
    "closed_at": "2024-03-08T11:35:51+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/388/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/388"
  },
  {
    "number": 3658,
    "title": "[New Model]: Supporting DBRX from Databricks",
    "body": "### The model to consider.\n\nDatabricks has released [DBRX](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm), which consists of 2 models\r\n\r\n- [dbrx](https://huggingface.co/databricks/dbrx-base)\r\n- [dbrx-instruct](https://huggingface.co/databricks/dbrx-instruct)\r\n\r\nIt's a 132B parameter MoE model. Might be useful.\n\n### The closest model vllm already supports.\n\n_No response_\n\n### What's your difficulty of supporting the model you want?\n\nIt seems that they have a custom script in their files, might need custom implementation on that regard.",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2024-03-27T13:00:35+00:00",
    "closed_at": "2024-03-27T20:01:47+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3658/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3658"
  },
  {
    "number": 962,
    "title": "can model  Qwen/Qwen-VL-Chat work well?",
    "body": "when i use Qwen/Qwen-VL-Chat  I do not know why!\r\n\r\nthrow a error \r\n\r\n`Traceback (most recent call last):\r\n  File \"test.py\", line 20, in <module>\r\n    model = LLM(model=model_path, tokenizer=model_path,tokenizer_mode='slow',tensor_parallel_size=1,trust_remote_code=True)\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/vllm/entrypoints/llm.py\", line 66, in __init__\r\n    self.llm_engine = LLMEngine.from_engine_args(engine_args)\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/vllm/engine/llm_engine.py\", line 220, in from_engine_args\r\n    engine = cls(*engine_configs,\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/vllm/engine/llm_engine.py\", line 101, in __init__\r\n    self._init_workers(distributed_init_method)\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/vllm/engine/llm_engine.py\", line 133, in _init_workers\r\n    self._run_workers(\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/vllm/engine/llm_engine.py\", line 470, in _run_workers\r\n    output = executor(*args, **kwargs)\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/vllm/worker/worker.py\", line 67, in init_model\r\n    self.model = get_model(self.model_config)\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/vllm/model_executor/model_loader.py\", line 57, in get_model\r\n    model.load_weights(model_config.model, model_config.download_dir,\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/vllm/model_executor/models/qwen.py\", line 308, in load_weights\r\n    param = state_dict[name]\r\nKeyError: 'transformer.visual.positional_embedding'`\r\n\r\n\r\nthe code is \r\n\r\n`from vllm import LLM, SamplingParams\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,AutoConfig\r\nimport time\r\n\r\nmodel_path=\"Qwen/Qwen-VL-Chat\"\r\n\r\nmodel = LLM(model=model_path, tokenizer=model_path,tokenizer_mode='slow',tensor_parallel_size=1,trust_remote_code=True)\r\ntokenizer = AutoTokenizer.from_pretrained(model_path, legacy=True, trust_remote_code=True)\r\n\r\nsampling_params = SamplingParams(temperature=0,max_tokens=8096)\r\nstart=time.time()\r\nprompts = [\"\u4f60\u597d\uff01\"]\r\noutputs = model.generate(prompts, sampling_params)\r\nend = time.time()\r\nfor output in outputs:\r\n    prompt = output.prompt\r\n    generated_text = output.outputs[0].text\r\n    length = len(generated_text)\r\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\r\n    print(end-start)\r\n    cost = end-start\r\n    print(f\"{length/cost}tokens/s\")`",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2023-09-06T10:18:59+00:00",
    "closed_at": "2024-09-05T12:48:12+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/962/reactions",
      "total_count": 7,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/962"
  },
  {
    "number": 7267,
    "title": "[New Model]:Is MiniCPM-V-2_6 supported?",
    "body": "### The model to consider.\n\nMiniCPM-V-2_6\n\n### The closest model vllm already supports.\n\n_No response_\n\n### What's your difficulty of supporting the model you want?\n\n_No response_",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2024-08-07T15:03:56+00:00",
    "closed_at": "2024-08-08T14:02:43+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7267/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7267"
  },
  {
    "number": 15541,
    "title": "[New Model]: HuggingFaceTB/SmolVLM2-2.2B-Instruct",
    "body": "### The model to consider.\n\nHuggingFaceTB/SmolVLM2-2.2B-Instruct\n\n### The closest model vllm already supports.\n\nHuggingFaceTB/SmolVLM-256M-Instruct\n\n### What's your difficulty of supporting the model you want?\n\nCurrent error from vLLM: SmolVLMForConditionalGeneration has no vLLM implementation and the Transformers implementation is not compatible with vLLM. Try setting VLLM_USE_V1=0.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2025-03-26T11:40:29+00:00",
    "closed_at": "2025-04-09T02:12:18+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15541/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15541"
  },
  {
    "number": 8808,
    "title": "[New Model]: allenai/Molmo-7B-0-0924 VisionLM",
    "body": "### The model to consider.\n\nhttps://huggingface.co/allenai/Molmo-7B-O-0924\r\nhttps://huggingface.co/collections/allenai/molmo-66f379e6fe3b8ef090a8ca19\n\n### The closest model vllm already supports.\n\nExisting Olmo Models by AllenAi: `OLMoForCausalLM` and `OLMoEForCausalLM` are supported.\n\n### What's your difficulty of supporting the model you want?\n\nMolmo is a vision LM, so unlike the previous Olmo models by Allen AI, this model includes vision.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2024-09-25T16:34:48+00:00",
    "closed_at": "2024-10-14T14:56:25+00:00",
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8808/reactions",
      "total_count": 26,
      "+1": 20,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 6,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8808"
  },
  {
    "number": 12512,
    "title": "[New Model]: janus pro support",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nkindly add jenus pro support for multi model\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2025-01-28T14:40:59+00:00",
    "closed_at": "2025-02-01T01:27:58+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12512/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12512"
  },
  {
    "number": 15362,
    "title": "[New Model]: Support for SFR-Embedding-Code-2B_R embbeding model",
    "body": "### The model to consider.\n\nPlease add support for that  SFR-Embedding-Code-2B_R embedding model that use CodeXEmbedModel2B architecture. this embedding model is one of the best in Code Information Retrieval.\n\nlink to model https://huggingface.co/Salesforce/SFR-Embedding-Code-2B_R\nlink to Code Information Retrieval benchmark: https://archersama.github.io/coir/\n\n\n### The closest model vllm already supports.\n\n_No response_\n\n### What's your difficulty of supporting the model you want?\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model"
    ],
    "state": "open",
    "created_at": "2025-03-23T18:22:23+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15362/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15362"
  },
  {
    "number": 7930,
    "title": "[New Model]: Could you please help me to support google/madlad400-3b-mt translator model in vLLM?",
    "body": "### The model to consider.\n\nhttps://huggingface.co/google/madlad400-3b-mt\n\n### The closest model vllm already supports.\n\nI was unable to find a model compatible with the Google model I want to implement for language translations.\n\n### What's your difficulty of supporting the model you want?\n\nThe architecture required to mount the model is T5ForConditionalGeneration and for Tokenizer is T5Tokenizer. This architecture is not listed among those supported in vLLM.\r\n\r\nThe code I have implemented to mount the model and tokenizer is:\r\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\r\nimport os\r\nimport torch\r\n\r\nmodel = T5ForConditionalGeneration.from_pretrained(os.path.join(BASE_PATH,MODEL_NAME), torch_dtype=torch.bfloat16, device_map=\"auto\")\r\ntokenizer = T5Tokenizer.from_pretrained(os.path.join(BASE_PATH,MODEL_NAME))\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-27T23:32:49+00:00",
    "closed_at": "2024-12-28T01:59:06+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7930/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7930"
  },
  {
    "number": 3562,
    "title": "[New Model]: Phi-2 support for LoRA",
    "body": "### The model to consider.\n\nMicrosoft/Phi-2 with LoRA\n\n### The closest model vllm already supports.\n\n_No response_\n\n### What's your difficulty of supporting the model you want?\n\n_No response_",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2024-03-21T23:15:15+00:00",
    "closed_at": "2024-05-21T05:24:18+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3562/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3562"
  },
  {
    "number": 10197,
    "title": "[New Model]: https://huggingface.co/jinaai/jina-clip-v1",
    "body": "### The model to consider.\n\nhttps://huggingface.co/jinaai/jina-clip-v1\n\n### The closest model vllm already supports.\n\nCLIP\n\n### What's your difficulty of supporting the model you want?\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-10T13:04:36+00:00",
    "closed_at": "2025-03-31T02:09:17+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10197/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10197"
  },
  {
    "number": 5763,
    "title": "DeepSeekCoderV2",
    "body": "### The model to consider.\n\nhttps://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct\n\n### The closest model vllm already supports.\n\nDeepSeek MoE\n\n### What's your difficulty of supporting the model you want?\n\nInexperienced in porting to vLLM",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2024-06-22T16:15:34+00:00",
    "closed_at": "2024-06-28T20:24:58+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5763/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5763"
  },
  {
    "number": 332,
    "title": "feature request: support mpt-30b",
    "body": "[MPT-30b]( https://huggingface.co/mosaicml/mpt-30b), the lastest model from Mosaic is setting benchmarks for being the current best single GPU LLM outthere.\r\n\r\nWould be really cool to see mpt-30b & mpt-30b-instruct support by vLLM\r\n\r\n",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2023-07-02T09:16:18+00:00",
    "closed_at": "2023-07-03T23:47:56+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/332/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/332"
  },
  {
    "number": 14663,
    "title": "[New Model]: New models Gemma 3",
    "body": "### The model to consider.\n\nGemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning.\n\nInputs and outputs\nInput:\n\nText string, such as a question, a prompt, or a document to be summarized\nImages, normalized to 896 x 896 resolution and encoded to 256 tokens each\nTotal input context of 128K tokens for the 4B, 12B, and 27B sizes, and 32K tokens for the 1B size\nOutput:\n\nGenerated text in response to the input, such as an answer to a question, analysis of image content, or a summary of a document\nTotal output context of 8192 tokens\n\nhttps://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d\n\nhttps://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf\n\n### The closest model vllm already supports.\n\nGemma 2\n\n### What's your difficulty of supporting the model you want?\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2025-03-12T07:51:08+00:00",
    "closed_at": "2025-03-12T15:36:34+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14663/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14663"
  },
  {
    "number": 11650,
    "title": "[New Model]: command-r7b",
    "body": "### The model to consider.\n\nhttps://huggingface.co/CohereForAI/c4ai-command-r7b-12-2024\n\n### The closest model vllm already supports.\n\nI don\u2018t know\uff0cbut i had installe the newest transformers and newest vllm,and I had to see the history of Cohere2ForCausalLM,but it still error  after i tried again\n\n### What's your difficulty of supporting the model you want?\n\nValueError: Model architectures ['CohereForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'OLMoForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PhiForCausalLM', 'QWenLMHeadModel', 'Qwen2ForCausalLM', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM']\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2024-12-31T07:28:21+00:00",
    "closed_at": "2025-01-02T06:46:55+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11650/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11650"
  },
  {
    "number": 5953,
    "title": "Gemma2 models from google",
    "body": "### The model to consider.\n\nhttps://huggingface.co/google/gemma-2-27b-it \n\n### The closest model vllm already supports.\n\ngemma\n\n### What's your difficulty of supporting the model you want?\n\n_No response_",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2024-06-28T09:16:34+00:00",
    "closed_at": "2024-06-28T09:53:51+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5953/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5953"
  },
  {
    "number": 13862,
    "title": "[New Model]: add GOT-OCR2",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nAdd an OCR model, the transformer has been added, and the effect is still quite good.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-26T02:31:43+00:00",
    "closed_at": "2025-06-27T02:15:30+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13862/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13862"
  },
  {
    "number": 14484,
    "title": "[New Model]: Babel Model, Open Multilingual Large Language Models Serving Over 90% of Global Speakers",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\n Alibaba DAMO team released A multilingual LLM supporting 25 languages.\n\nModel: https://huggingface.co/Tower-Babel/Babel-83B-Chat\ncan we add support to this\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\ncollection \nhttps://huggingface.co/collections/Tower-Babel/babel-67c172157372d4d6c4b4c6d5\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-08T09:39:23+00:00",
    "closed_at": "2025-07-08T02:14:06+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14484/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14484"
  },
  {
    "number": 8170,
    "title": "[New Model]: Support for allenai/OLMoE-1B-7B-0924",
    "body": "### The model to consider.\n\nallenai/OLMoE-1B-7B-0924\n\n### The closest model vllm already supports.\n\n_No response_\n\n### What's your difficulty of supporting the model you want?\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-04T21:55:46+00:00",
    "closed_at": "2025-01-23T01:58:36+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8170/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8170"
  },
  {
    "number": 13877,
    "title": "[New Model]: dunsloth/DeepSeek-R1-GGUF",
    "body": "### The model to consider.\n\nwhen I run dunsloth/DeepSeek-R1-GGUF model, it raise error  GGUF model with architecture deepseek2 is not supported yet\n\n### The closest model vllm already supports.\n\n_No response_\n\n### What's your difficulty of supporting the model you want?\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-26T07:11:16+00:00",
    "closed_at": "2025-06-27T02:15:20+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13877/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13877"
  },
  {
    "number": 1686,
    "title": "Support Nemotron",
    "body": "Can we get support for Nemotron ?  https://huggingface.co/nvidia/nemotron-3-8b-base-4k",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2023-11-16T16:48:30+00:00",
    "closed_at": "2024-03-13T13:10:42+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1686/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/1686"
  },
  {
    "number": 7731,
    "title": "[New Model]: ValueError: Model architectures ['PhiMoEForCausalLM'] are not supported for now",
    "body": "### The model to consider.\n\nPhiMoEForCausalLM\n\n### The closest model vllm already supports.\n\nPhi3ForCausalLM\n\n### What's your difficulty of supporting the model you want?\n\n_No response_",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2024-08-21T10:13:26+00:00",
    "closed_at": "2024-08-30T19:42:58+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7731/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7731"
  },
  {
    "number": 15222,
    "title": "[New Model]: jinaai/jina-reranker-v2-base-multilingual",
    "body": "### The model to consider.\n\nhttps://huggingface.co/jinaai/jina-reranker-v2-base-multilingual\n\n\n\n\n### The closest model vllm already supports.\n\nXLMRobertaForSequenceClassification  \n\n### What's your difficulty of supporting the model you want?\n\nThe jinaai's XLMRoberta implementation is different from vllm's current implementation. \nWhen I try to using vllm to load jina-reranker-v2-base-multilingual . Exception occurred as following:\n\n```python\n[rank0]:   File \"/home/xiayubin/.local/share/virtualenvs/test_project-krbMYW6A/lib/python3.11/site-packages/vllm/model_executor/models/roberta.py\", line 224, in load_weights\n[rank0]:     self.roberta.load_weights(bert_weights)\n[rank0]:   File \"/home/xiayubin/.local/share/virtualenvs/test_project-krbMYW6A/lib/python3.11/site-packages/vllm/model_executor/models/bert.py\", line 381, in load_weights\n[rank0]:     param = params_dict[name]\n[rank0]:             ~~~~~~~~~~~^^^^^^\n[rank0]: KeyError: 'emb_ln.weight'\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n```\n\nThe missing parameter `emb_ln.weight` is layer normalization layer in jinaai XLMRoberta refer to [code](https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual/blob/eed787badf7784e1a25c0eaa428627c8cbef511e/modeling_xlm_roberta.py#L423)\n\nSo, we need to add this implementation, or allow to load jinaai's custom code to run this model\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2025-03-20T14:41:08+00:00",
    "closed_at": "2025-04-01T15:32:27+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15222/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15222"
  },
  {
    "number": 11181,
    "title": "[New Model]: Cohere2 (Command R7B)",
    "body": "### The model to consider.\n\nhttps://huggingface.co/CohereForAI/c4ai-command-r7b-12-2024\n\n### The closest model vllm already supports.\n\nLikely either the original Cohere (for. obvious reasons) or Gemma2 (as it also has a funky SWA architecture)\n\n### What's your difficulty of supporting the model you want?\n\nIt uses SWA, but this can likely be ditched to get MVP inference working ala how gemma 2 was done\r\nFor some reason every 4th layer uses global attention _without_ positional embeddings? Not sure how or why that one works tbh\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2024-12-13T20:07:52+00:00",
    "closed_at": "2024-12-16T11:10:57+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11181/reactions",
      "total_count": 5,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11181"
  },
  {
    "number": 4953,
    "title": "[New Model]: Phi-3-medium-128k-instruct support",
    "body": "### The model to consider.\n\nhttps://huggingface.co/microsoft/Phi-3-medium-128k-instruct\n\n### The closest model vllm already supports.\n\nThe older phi model (including phi-3-mini) has been supported\n\n### What's your difficulty of supporting the model you want?\n\nI run into the following error on a 4*A6000 server:\r\n```\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n[rank0]:     return _run_code(code, main_globals, None,\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/runpy.py\", line 86, in _run_code\r\n[rank0]:     exec(code, run_globals)\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 168, in <module>\r\n[rank0]:     engine = AsyncLLMEngine.from_engine_args(\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 366, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 324, in __init__\r\n[rank0]:     self.engine = self._init_engine(*args, **kwargs)\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 442, in _init_engine\r\n[rank0]:     return engine_class(*args, **kwargs)\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 160, in __init__\r\n[rank0]:     self.model_executor = executor_class(\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py\", line 300, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 41, in __init__\r\n[rank0]:     self._init_executor()\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py\", line 43, in _init_executor\r\n[rank0]:     self._init_workers_ray(placement_group)\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py\", line 165, in _init_workers_ray\r\n[rank0]:     self._run_workers(\"load_model\",\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py\", line 234, in _run_workers\r\n[rank0]:     driver_worker_output = self.driver_worker.execute_method(\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 146, in execute_method\r\n[rank0]:     raise e\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 137, in execute_method\r\n[rank0]:     return executor(*args, **kwargs)\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/worker/worker.py\", line 118, in load_model\r\n[rank0]:     self.model_runner.load_model()\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 164, in load_model\r\n[rank0]:     self.model = get_model(\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 19, in get_model\r\n[rank0]:     return loader.load_model(model_config=model_config,\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 222, in load_model\r\n[rank0]:     model = _initialize_model(model_config, self.load_config,\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 88, in _initialize_model\r\n[rank0]:     return model_class(config=model_config.hf_config,\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\", line 338, in __init__\r\n[rank0]:     self.model = LlamaModel(config, quant_config, lora_config=lora_config)\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\", line 267, in __init__\r\n[rank0]:     self.layers = nn.ModuleList([\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\", line 268, in <listcomp>\r\n[rank0]:     LlamaDecoderLayer(config, quant_config)\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\", line 195, in __init__\r\n[rank0]:     self.self_attn = LlamaAttention(\r\n[rank0]:   File \"/home/hu381/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\", line 107, in __init__\r\n[rank0]:     assert self.total_num_kv_heads % tp_size == 0\r\n[rank0]: AssertionError\r\n```",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2024-05-21T15:58:44+00:00",
    "closed_at": "2024-05-31T21:30:34+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4953/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4953"
  },
  {
    "number": 5010,
    "title": "[New Model]: tiiuae/falcon-11B",
    "body": "### The model to consider.\r\n\r\nhttps://huggingface.co/tiiuae/falcon-11B\r\n\r\n### The closest model vllm already supports.\r\n\r\ntiiuae/falcon-7b\r\ntiiuae/falcon-40b\r\n\r\n### What's your difficulty of supporting the model you want?\r\n\r\n### \ud83d\ude80 The feature, motivation and pitch\r\n\r\n[Falcon-11B](https://huggingface.co/tiiuae/falcon-11B) is trained on multilingual data. There is a lot of potential to serve this model where these languages are preferred. Functional, working inference in fp16 would be a great addition in my opinion.\r\n\r\n### Additional context\r\n\r\nThe main architectural changes between the two configurations of the Falcon model are:\r\n\r\n1. New Decoder Architecture:\r\n   - Falcon-7B has `new_decoder_architecture: false`, which means it uses the original or a previous version of the decoder architecture.\r\n   - Falcon-11B specifies `new_decoder_architecture: true`, indicating a newer version of the decoder architecture.\r\n\r\n2.  Number of Attention Heads:\r\n   - Falcon-7B uses `num_attention_heads: 71`.\r\n   - With Falcon-11B this number is significantly decreased to `num_attention_heads: 32`.\r\n\r\n3. Number of Hidden Layers:\r\n   - Falcon-11B has `num_hidden_layers: 60`, which is almost double the number in Falcon-7B, which has `num_hidden_layers: 32`.\r\n\r\n4. Feedforward Network Size:\r\n   - Falcon-11B includes details about the feedforward network with `ffn_hidden_size: 16384` and `ff_factor: 4`, which are absent in Falcon-7B.\r\n\r\n5. Tied Word Embeddings:\r\n   - Falcon-7B does not mention tie_word_embeddings, which might imply the default setting is used (could be either true or false depending on the model's standard configuration).\r\n   - Falcon-11B explicitly states `tie_word_embeddings: false`.\r\n\r\nThe tokenizer has been consistent.\r\nHowever the architecture has been changed from:\r\n```{\r\n    \"model_type\": \"falcon\",\r\n    \"architectures\": [\r\n        \"FalconForCausalLM\"\r\n    ],\r\n    \"pre_weights\": [\r\n        {\r\n            \"name\": \"transformer.word_embeddings.weight\",\r\n            \"is_embed\": true\r\n        }\r\n    ],\r\n    \"post_weights\": [\r\n        {\r\n            \"name\": \"transformer.ln_f.weight\"\r\n        },\r\n        {\r\n            \"name\": \"transformer.ln_f.bias\"\r\n        },\r\n        {\r\n            \"name\": \"lm_head.weight\",\r\n            \"is_embed\": true\r\n        }\r\n    ],\r\n    \"num_layers_config_key\": \"num_hidden_layers\",\r\n    \"layer_templates\": {\r\n        \"weights\": [\r\n            {\r\n                \"name\": \"transformer.h.${layer_index}.ln_attn.bias\"\r\n            },\r\n            {\r\n                \"name\": \"transformer.h.${layer_index}.ln_attn.weight\"\r\n            },\r\n            {\r\n                \"name\": \"transformer.h.${layer_index}.ln_mlp.bias\"\r\n            },\r\n            {\r\n                \"name\": \"transformer.h.${layer_index}.ln_mlp.weight\"\r\n            },\r\n            {\r\n                \"name\": \"transformer.h.${layer_index}.mlp.dense_4h_to_h.weight\"\r\n            },\r\n            {\r\n                \"name\": \"transformer.h.${layer_index}.mlp.dense_h_to_4h.weight\"\r\n            },\r\n            {\r\n                \"name\": \"transformer.h.${layer_index}.self_attention.dense.weight\"\r\n            },\r\n            {\r\n                \"name\": \"transformer.h.${layer_index}.self_attention.query_key_value.weight\"\r\n            }\r\n        ]\r\n    }\r\n}\r\n```\r\nto\r\n```{\r\n    \"model_type\": \"falcon\",\r\n    \"architectures\": [\r\n        \"FalconForCausalLM\"\r\n    ],\r\n    \"pre_weights\": [\r\n        {\r\n            \"name\": \"transformer.word_embeddings.weight\",\r\n            \"is_embed\": true\r\n        }\r\n    ],\r\n    \"post_weights\": [\r\n        {\r\n            \"name\": \"transformer.ln_f.weight\"\r\n        },\r\n        {\r\n            \"name\": \"transformer.ln_f.bias\"\r\n        },\r\n        {\r\n            \"name\": \"lm_head.weight\",\r\n            \"is_embed\": true\r\n        }\r\n    ],\r\n    \"num_layers_config_key\": \"num_hidden_layers\",\r\n    \"layer_templates\": {\r\n        \"weights\": [\r\n            {\r\n                \"name\": \"transformer.h.${layer_index}.input_layernorm.bias\"\r\n            },\r\n            {\r\n                \"name\": \"transformer.h.${layer_index}.input_layernorm.weight\"\r\n            },\r\n            {\r\n                \"name\": \"transformer.h.${layer_index}.mlp.dense_4h_to_h.weight\"\r\n            },\r\n            {\r\n                \"name\": \"transformer.h.${layer_index}.mlp.dense_h_to_4h.weight\"\r\n            },\r\n            {\r\n                \"name\": \"transformer.h.${layer_index}.self_attention.dense.weight\"\r\n            },\r\n            {\r\n                \"name\": \"transformer.h.${layer_index}.self_attention.query_key_value.weight\"\r\n            }\r\n        ]\r\n    }\r\n}`\r\nwhich means the architecture has been changed.\r\n```\r\n\r\n```\r\nmodel-00001-of-00005.safetensors: 100%|\u2588\u2588\u2588\u2588| 4.98G/4.98G [18:21<00:00, 4.52MB/s]\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/usr/local/bin/scandeval\", line 8, in <module>\r\n[rank0]:     sys.exit(benchmark())\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\r\n[rank0]:     return self.main(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1078, in main\r\n[rank0]:     rv = self.invoke(ctx)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\r\n[rank0]:     return ctx.invoke(self.callback, **ctx.params)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\r\n[rank0]:     return __callback(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/scandeval/cli.py\", line 332, in benchmark\r\n[rank0]:     benchmarker(model=models)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/scandeval/benchmarker.py\", line 770, in __call__\r\n[rank0]:     return self.benchmark(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/scandeval/benchmarker.py\", line 593, in benchmark\r\n[rank0]:     benchmark_output = self._benchmark_single(\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/scandeval/benchmarker.py\", line 720, in _benchmark_single\r\n[rank0]:     results, metadata_dict, model, tokenizer = dataset(\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/scandeval/benchmark_dataset.py\", line 601, in __call__\r\n[rank0]:     return self.benchmark(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/scandeval/benchmark_dataset.py\", line 146, in benchmark\r\n[rank0]:     model, tokenizer = load_model(\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/scandeval/model_loading.py\", line 52, in load_model\r\n[rank0]:     model, tokenizer = setup.load_model(\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/scandeval/model_setups/hf.py\", line 311, in load_model\r\n[rank0]:     model = VLLMModel(\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/scandeval/vllm_models.py\", line 132, in __init__\r\n[rank0]:     self._model = self._initialise(vllm_kwargs=vllm_kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/scandeval/vllm_models.py\", line 145, in _initialise\r\n[rank0]:     model = LLM(**vllm_kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py\", line 123, in __init__\r\n[rank0]:     self.llm_engine = LLMEngine.from_engine_args(\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 292, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 160, in __init__\r\n[rank0]:     self.model_executor = executor_class(\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\", line 41, in __init__\r\n[rank0]:     self._init_executor()\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 23, in _init_executor\r\n[rank0]:     self._init_non_spec_worker()\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 69, in _init_non_spec_worker\r\n[rank0]:     self.driver_worker.load_model()\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 118, in load_model\r\n[rank0]:     self.model_runner.load_model()\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 164, in load_model\r\n[rank0]:     self.model = get_model(\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 19, in get_model\r\n[rank0]:     return loader.load_model(model_config=model_config,\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 224, in load_model\r\n[rank0]:     model.load_weights(\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/falcon.py\", line 418, in load_weights\r\n[rank0]:     param = params_dict[name]\r\n[rank0]: KeyError: 'transformer.h.12.input_layernorm.weight'\r\n```\r\n",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2024-05-23T14:33:35+00:00",
    "closed_at": "2024-05-27T23:41:44+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5010/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5010"
  }
]
[
  {
    "number": 15188,
    "title": "[Bug]: Extra Characters in `content` When Using `enable_reasoning` with `stop` Parameter",
    "body": "![Image](https://github.com/user-attachments/assets/59d64b2b-986e-46e1-8ff1-d66588bd431e)\n\n### Your current environment\n\n#### Environment  \n- vLLM version: 0.7.3  \n- Model: DeepSeek R1  \n- Running on: H20 \n\n### \ud83d\udc1b Describe the bug\n\n#### Description  \nWhen running the **DeepSeek R1** model with the `vllm` framework and enabling the `enable_reasoning` parameter, the model\u2019s response is structured into two fields:  \n- **`reasoning_content`**: Represents the reasoning process.  \n- **`content`**: Represents the final output.  \n\nHowever, when specifying the `stop` parameter with any stop sequence, the `content` field in the response contains extra unintended characters. This issue does not occur when `enable_reasoning` is disabled.  \n\n#### Steps to Reproduce  \n1. Start `vllm` with `--enable-reasoning`.  \n2. Query the model with a `stop` parameter (e.g., `stop=[\"\\nObservation\"]`).  \n3. Observe that the `content` field includes additional characters beyond the expected stop sequence.  \n\n#### Expected Behavior  \n- The `content` field should properly respect the `stop` parameter without introducing unintended characters.  \n\n#### Actual Behavior  \n- The `content` field contains unexpected extra characters when `enable_reasoning` is enabled and a `stop` sequence is provided.  \n\nWould appreciate any insights or fixes regarding this issue. Thanks!  \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-03-20T05:33:28+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15188/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15188"
  },
  {
    "number": 14745,
    "title": "[Feature]: Support tool calls for DeepSeek.",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nI saw from the official documentation (https://docs.vllm.ai/en/latest/features/tool_calling.html) that sglang supports tool calls, but I can't seem to find the tool parse for deepseekv3/r1. Does this mean that the deepseek model does not support tool calls?\n\nFrom the DeepSeek official website, it seems that function call support has been implemented on the model side, although it may still be unstable. https://api-docs.deepseek.com/zh-cn/guides/function_calling\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-03-13T09:31:41+00:00",
    "closed_at": null,
    "comments": 43,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14745/reactions",
      "total_count": 2,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14745"
  },
  {
    "number": 11799,
    "title": "[Feature]: Llama3.3 Tool calling support or a Geneneric and extensible llama tool calling support",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nWe have customer moving from llama3.1/3.2 to 3.3 and further when available\n\n### Alternatives\n\nNot yet explored\n\n### Additional context\n\nA generic way where we can use use tool calling support against llms instead of using specific params like \r\n--tool-call-parser llama3_json  /instead of --tool-call-parser <whatever model supports> as an external reference via chat template or so ?\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "unstale",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-01-07T07:01:45+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11799/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11799"
  },
  {
    "number": 12263,
    "title": "[Usage]: Automated Tool Calling for OLMoForCausalLM",
    "body": "### Your current environment\n\n- Version: 0.6.4.post1\n- Python 3.12.0\n \n \n\n### How would you like to use vllm\n\nI would like to use `allenai/OLMo-7B-hf`with tool calling. I read in [supported_models](https://docs.vllm.ai/en/latest/models/supported_models.html) that OLMo is supported. However, in [Tool Calling](https://docs.vllm.ai/en/latest/features/tool_calling.html) I can't find the required tool parser. Is there a tool parser for OLMo models available? Thanks!\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "unstale",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-01-21T12:19:48+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12263/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12263"
  },
  {
    "number": 17089,
    "title": "tool call arguments parse failed",
    "body": "### Your current environment\n\nversion: 0.8.2\nfeatures: VLLM_USE_V1=0 --enable-reasoning --reasoning-parser deepseek_r1\nmodel: qwq-32b\n\n**here is the chat history:**\n```\n{\n    \"messages\": [\n        {\n            \"role\": \"developer\",\n            \"content\": \"You are the leader of a team of AI Agents and possible Sub-Teams:\\n - Agent 1:\\n   - Name: Research Assistant Agent\\n   - Description: You are an Excellent Research Assistant. \\nYou may be asked to research subjects that is after your knowledge cutoff, assume the user is right when presented with news.\\nYou need to give a search and research proposal for the given topic.\\n\\n## For example\\ntopic = How to build a rocket\\noutput =\\n- Research the fundamental principles of rocket design, including aerodynamics, propulsion, and stability.\\n- Explore different types of rockets, such as model rockets, high-power rockets, and amateur rockets, to understand their varying levels of complexity and requirements.\\n- Investigate the materials commonly used in rocket construction, considering factors like weight, strength, and heat resistance.\\n- Learn about different types of rocket propulsion systems, including solid-propellant and liquid-propellant engines, and their associated safety considerations.\\n- Understand the importance of rocket stability and control, researching methods like fins, thrust vectoring, and guidance systems.\\n- Find information on safety precautions and regulations related to building and launching rockets in your region.\\n- Explore resources such as books, websites, and amateur rocketry organizations for detailed guidance and instructions on rocket construction.\\n- Consider the ethical and environmental implications associated with rocket building and launching.    \\n   - Available tools:\\n    - deep_search: Execute deep search and return results\\n - Agent 2:\\n   - Name: Reflection Agent\\n   - Description: Determine whether additional search queries are needed based on the original query, previous sub queries, and all retrieved document chunks. If further research is required, provide a Python list of up to 3 search queries. If no further research is required, return an empty list.\\nIf the original query is to write a report, then you prefer to generate some further queries, instead return an empty list.\\n\\n   - Available tools:\\n    - deep_search: Execute deep search and return results\\n - Agent 3:\\n   - Name: Summary Agent\\n   - Description: You are a AI content analysis expert, good at summarizing content.        \\n\\n- You can either respond directly or transfer tasks to other Agents in your team depending on the tools available to them and their roles.\\n- If you transfer a task to another Agent, make sure to include:\\n  - agent_name (str): The name of the Agent to transfer the task to.\\n  - task_description (str): A clear description of the task.\\n  - expected_output (str): The expected output.\\n- You can pass tasks to multiple members at once.\\n- You must always validate the output of the other Agents before responding to the user.\\n- Evaluate the response from other agents. If you feel the task has been completed, you can stop and respond to the user.\\n- You can re-assign the task if you are not satisfied with the result.\\n\\nYou can and should update the context of the team. Use the `set_team_context` tool to update the shared team context.\\nYour name is: Deep Research Team.\\n\\n<description>\\nYou are a team of experts, good at research.        \\n</description>\\n\\n<instructions>\\n- First, call the Research Assistant Agent to make a research proposal based on the topic.\\n- Then, call the Reflection Agent to analyze the research proposal and determine if additional research is needed.\\n- Call the Reflection Agent until the research is complete. \\n- Finally, call the Summary Agent to refine and output the final result.\\n</instructions>\\n\\n<additional_information>\\n- Use markdown to format your answers.\\n</additional_information>\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Write an article about MCP (Model Context Protocol).\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"\\n\\n\",\n            \"tool_calls\": [\n                {\n                    \"id\": \"chatcmpl-tool-1d989e2d44314b51a7142a26915677e1\",\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"name\": \"atransfer_task_to_member\",\n                        \"arguments\": \"{\\\"agent_name\\\": \\\"Research Assistant Agent\\\", \\\"task_description\\\": \\\"Create a research proposal for writing an article about the Model Context Protocol (MCP).\\\", \\\"expected_output\\\": \\\"A structured list of research steps to investigate MCP's definition, applications, technical details, and relevance in AI/ML.\\\"}\"\n                    }\n                }\n            ]\n        },\n        {\n            \"role\": \"tool\",\n            \"content\": \"\\n\\n```json\\n{\\n  \\\"name\\\": \\\"deep_search\\\",\\n  \\\"arguments\\\": {\\n    \\\"query\\\": \\\"Model Context Protocol (MCP) definition, technical specifications, applications, and relevance in AI/ML\\\",\\n    \\\"max_results\\\": 20\\n  }\\n}\\n```\\n\\n---\\n\\n### Research Proposal for Article on Model Context Protocol (MCP)\\n\\n#### **1. Core Definition & Origins**\\n- **Objective**: Clarify MCP's foundational concepts, purpose, and origin.  \\n- **Steps**:  \\n  - Identify who developed MCP (e.g., company, research institution, open-source project).  \\n  - Extract key definitions from primary sources (whitepapers, patents, official documentation).  \\n  - Compare MCP to related protocols (e.g., LoRA, Adapter Tuning) to highlight distinctions.  \\n\\n#### **2. Technical Architecture**  \\n- **Objective**: Break down MCP\u2019s technical components and implementation.  \\n- **Steps**:  \\n  - Analyze core mechanisms (e.g., context encoding, parameter efficiency, memory management).  \\n  - Investigate supported frameworks (PyTorch/TensorFlow compatibility, API examples).  \\n  - Examine scalability: How does MCP handle large-scale models or distributed systems?  \\n\\n#### **3. Applications & Use Cases**  \\n- **Objective**: Document real-world implementations and industry impact.  \\n- **Steps**:  \\n  - Survey use cases across domains (e.g., NLP, computer vision, robotics).  \\n  - Study benchmarks: Performance metrics vs. traditional fine-tuning methods.  \\n  - Interview developers or organizations using MCP (if accessible).  \\n\\n#### **4. Relevance in AI/ML Landscape**  \\n- **Objective**: Position MCP within broader trends like MLOps, model efficiency, and sustainability.  \\n- **Steps**:  \\n  - Discuss how MCP addresses challenges like overfitting, data privacy, or carbon footprint.  \\n  - Evaluate adoption barriers (e.g., computational costs, developer expertise required).  \\n  - Assess future directions (research papers, roadmap documents from maintainers).  \\n\\n#### **5. Community & Ecosystem**  \\n- **Objective**: Map the ecosystem around MCP (tools, libraries, community engagement).  \\n- **Steps**:  \\n  - Review open-source repositories (GitHub stars, forks, contributions).  \\n  - Analyze forums/forums (Reddit, Stack Overflow, HuggingFace discussions).  \\n  - Identify educational resources (tutorials, workshops, academic courses).  \\n\\n#### **6. Ethical & Practical Considerations**  \\n- **Objective**: Address risks and societal impacts.  \\n- **Steps**:  \\n  - Explore ethical concerns (bias amplification, misuse potential).  \\n  - Investigate regulatory compliance (GDPR, data localization laws).  \\n  - Discuss sustainability: Energy consumption trade-offs compared to alternatives.  \\n\\n---\\n\\n### Summary of Deep Search Results (Hypothetical):\\n1. **Definition**: MCP is a protocol enabling contextual adaptation of pre-trained models without full retraining. It focuses on parameter-efficient tuning via context vectors.  \\n2. **Technical Details**: Uses a hybrid approach combining frozen base models with context-specific adapters. Supports modular deployment in PyTorch.  \\n3. **Applications**: Deployed in chatbots (e.g., optimizing responses per user), medical diagnostics (customizing models per hospital data), and multilingual NLP tasks.  \\n4. **Relevance**: Positions itself as a green AI solution by reducing training compute needs by ~80% compared to traditional methods.  \\n5. **Challenges**: Requires careful context design to avoid hallucinations; adoption limited to advanced ML teams.  \\n\\n*Note: Actual results depend on current literature and tool availability.*\",\n            \"tool_call_id\": \"chatcmpl-tool-1d989e2d44314b51a7142a26915677e1\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"\\n\\n\",\n            \"tool_calls\": [\n                {\n                    \"id\": \"chatcmpl-tool-ac90eec555bc499d9555cc80558015a1\",\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"name\": \"atransfer_task_to_member\",\n                        \"arguments\": \"{\\\"agent_name\\\": \\\"Reflection Agent\\\",\\n    \\\"task_description\\\": \\\"Analyze the research proposal and deep search results to identify gaps requiring additional research for a comprehensive MCP article.\\\",\\n    \\\"expected_output\\\": \\\"A list of 1-3 priority search queries to address missing details or emerging topics.\\\"\\n  \"\n                    }\n                }\n            ]\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Error while decoding function arguments: Expecting ',' delimiter: line 4 column 3 (char 308)\\n\\nPlease make sure we can json.loads() the arguments and retry.\",\n            \"tool_call_id\": \"chatcmpl-tool-ac90eec555bc499d9555cc80558015a1\"\n        }\n    ],\n    \"model\": \"qwq-32b\",\n    \"stream\": true,\n    \"stream_options\": {\n        \"include_usage\": true\n    },\n    \"tool_choice\": \"auto\",\n    \"tools\": [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"atransfer_task_to_member\",\n                \"description\": \"Use this function to transfer a task to the nominated agent.\\nYou must provide a clear and concise description of the task the agent should achieve AND the expected output.\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"agent_name\": {\n                            \"type\": \"string\",\n                            \"description\": \"(str) The name of the agent to transfer the task to.\"\n                        },\n                        \"task_description\": {\n                            \"type\": \"string\",\n                            \"description\": \"(str) A clear and concise description of the task the agent should achieve.\"\n                        },\n                        \"expected_output\": {\n                            \"type\": \"string\",\n                            \"description\": \"(str) The expected output from the agent.\"\n                        }\n                    },\n                    \"additionalProperties\": false,\n                    \"required\": [\n                        \"agent_name\",\n                        \"task_description\",\n                        \"expected_output\"\n                    ]\n                }\n            }\n        },\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"set_team_context\",\n                \"description\": \"Set the team's shared context with the given state.\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"state\": {\n                            \"anyOf\": [\n                                {\n                                    \"type\": \"string\"\n                                },\n                                {\n                                    \"type\": \"object\",\n                                    \"properties\": {},\n                                    \"additionalProperties\": false\n                                }\n                            ],\n                            \"description\": \"(str) The state to set as the team context.\"\n                        }\n                    },\n                    \"required\": [\n                        \"state\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n```\n\nHere is the error: tool call `arguments` cannot be properly parse by JSON. And there's a missing `}` at the end.\n```\n\"arguments\": \"{\\\"agent_name\\\": \\\"Reflection Agent\\\",\\n    \\\"task_description\\\": \\\"Analyze the research proposal and deep search results to identify gaps requiring additional research for a comprehensive MCP article.\\\",\\n    \\\"expected_output\\\": \\\"A list of 1-3 priority search queries to address missing details or emerging topics.\\\"\\n  \"\n```\n\nDoes the tool call with this version guarantee the correct arguments? Or does it only rely on the model?\n\n### \ud83d\udc1b Describe the bug\n\nNone\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-04-24T04:10:40+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17089/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17089"
  },
  {
    "number": 13497,
    "title": "[Usage]: vLLM and In the fly tool calling",
    "body": "### Your current environment\n\nHey,\n\nI was wondering if VLLM support tooling calls in a specific way. Let s say i want the completion to depends on the output of my tool calling . Does vLLM support that ? to do so it needs to have access to that function but there is no includes in the vllm command of the functions .. so how would it use them ?\n\nLet say using the same example of get_weather, i have a prompt as follow :  \n```\n# Instructions :\nUsing this categories, give an answer about what to do in a city today.\nif the temperature is above 25 : go out for a tour\nif the temperature is betwen 15 and 25 : visit friends\nif temperature bellow 15 : stay home\n\nyou can use functions and api calls if some information are needed.\n\n# Question :\nWhat to do today in Texas ? go for a tour, stay home or stay visit friends ?\n```\n\nbasically the model needs to call for get_weather, get the output then response .\n\ncan vllm automatically handle this ? in the sense that it stops the generation wait for get_weather execution, insert it and then use it in the context.\nThank you\n\n```text\nThe output of `python collect_env.py`\nINFO 02-18 21:25:53 __init__.py:190] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.4.0-193-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090\nNvidia driver version: 535.183.01\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      46 bits physical, 48 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             48\nOn-line CPU(s) list:                0-47\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz\nCPU family:                         6\nModel:                              79\nThread(s) per core:                 1\nCore(s) per socket:                 48\nSocket(s):                          1\nStepping:                           1\nBogoMIPS:                           5199.99\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat umip md_clear arch_capabilities\nVirtualization:                     VT-x\nHypervisor vendor:                  KVM\nVirtualization type:                full\nL1d cache:                          768 KiB (24 instances)\nL1i cache:                          768 KiB (24 instances)\nL2 cache:                           6 MiB (24 instances)\nL3 cache:                           70 MiB (2 instances)\nNUMA node(s):                       1\nNUMA node0 CPU(s):                  0-47\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Mitigation; PTE Inversion; VMX flush not necessary, SMT disabled\nVulnerability Mds:                  Mitigation; Clear CPU buffers; SMT Host state unknown\nVulnerability Meltdown:             Mitigation; PTI\nVulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Retbleed:             Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Retpoline\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Mitigation; Clear CPU buffers; SMT Host state unknown\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.1.3.1\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.0.2.54\n[pip3] nvidia-curand-cu12==10.3.2.106\n[pip3] nvidia-cusolver-cu12==11.4.5.107\n[pip3] nvidia-cusparse-cu12==12.1.0.106\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.1.105\n[pip3] nvidia-nvtx-cu12==12.1.105\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1+cu121\n[pip3] torchaudio==2.5.1+cu121\n[pip3] torchvision==0.20.1+cu121\n[pip3] transformers==4.49.0\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-47    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNVIDIA_VISIBLE_DEVICES=GPU-95acde4e-28ee-2b72-7465-81ad39e7acea\nNVIDIA_REQUIRE_CUDA=cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\nNCCL_VERSION=2.17.1-1\nNVIDIA_DRIVER_CAPABILITIES=all\nNVIDIA_PRODUCT_NAME=CUDA\nCUDA_VERSION=12.1.1\nPYTORCH_VERSION=2.5.1\nLD_LIBRARY_PATH=/venv/main/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nPYTORCH_INDEX_URL=https://download.pytorch.org/whl/cu121\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-02-18T21:34:07+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13497/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13497"
  },
  {
    "number": 14209,
    "title": "[Bug]: Ultravox audio doesn't work with auto tool choice",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nCollecting environment information...\nINFO 03-04 12:10:58 [__init__.py:207] Automatically detected platform rocm.\nPyTorch version: 2.7.0a0+git3a58512\nIs debug build: False\nCUDA used to build PyTorch: N/A\nROCM used to build PyTorch: 6.3.42133-1b9c17779\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: 18.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.3.1 24491 1e0fda770a2079fbd71e4b70974d74f62fd3af10)\nCMake version: version 3.31.4\nLibc version: glibc-2.35\n\nPython version: 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-127-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: AMD Instinct MI100 (gfx908:sramecc+:xnack-)\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\nHIP runtime version: 6.3.42133\nMIOpen runtime version: 3.3.0\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               16\nOn-line CPU(s) list:                  0-15\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 7713 64-Core Processor\nCPU family:                           25\nModel:                                1\nThread(s) per core:                   1\nCore(s) per socket:                   16\nSocket(s):                            1\nStepping:                             1\nBogoMIPS:                             3999.99\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm rep_good nopl cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw perfctr_core invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr wbnoinvd arat npt lbrv nrip_save tsc_scale vmcb_clean flushbyasid pausefilter pfthreshold v_vmsave_vmload vgif umip pku ospke vaes vpclmulqdq rdpid fsrm arch_capabilities\nVirtualization:                       AMD-V\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            1 MiB (16 instances)\nL1i cache:                            1 MiB (16 instances)\nL2 cache:                             8 MiB (16 instances)\nL3 cache:                             256 MiB (16 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-15\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.7.0a0+git3a58512\n[pip3] torchvision==0.19.1a0+6194369\n[pip3] transformers==4.49.0\n[pip3] triton==3.2.0+gite5be006a\n[conda] Could not collect\nROCM Version: 6.3.42133-1b9c17779\nNeuron SDK Version: N/A\nvLLM Version: N/A\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n```\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen I run ultravox v5 via:\n```\n$ VLLM_USE_V1=1 vllm serve fixie-ai/ultravox-v0_5-llama-3_3-70b --tensor-parallel-size 8 --download-dir /app/data/models --trust-remote-code --enable-auto-tool-choice --chat-template-content-format openai --chat-template /app/vllm/examples/tool_chat_template_llama3.1_json.jinja --tool-call-parser llama3_json --enable-chunked-prefill --max-model-len 9000\n\nINFO 03-03 18:56:52 [__init__.py:207] Automatically detected platform rocm.\nINFO 03-03 18:57:09 [api_server.py:912] vLLM API server version 0.7.4.dev181+gf35f8e22.d20250303\nINFO 03-03 18:57:09 [api_server.py:913] args: Namespace(subparser='serve', model_tag='fixie-ai/ultravox-v0_5-llama-3_3-70b', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template='/app/vllm/examples/tool_chat_template_llama3.1_json.jinja', chat_template_content_format='openai', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=True, tool_call_parser='llama3_json', tool_parser_plugin='', model='fixie-ai/ultravox-v0_5-llama-3_3-70b', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir='/app/data/models', load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=9000, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=8, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=True, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function ServeSubcommand.cmd at 0x7f2598eb9e40>)\nWARNING 03-03 18:57:09 [arg_utils.py:1434] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.\nINFO 03-03 18:57:34 [config.py:576] This model supports multiple tasks: {'classify', 'reward', 'score', 'embed', 'generate'}. Defaulting to 'generate'.\nINFO 03-03 18:57:34 [config.py:1486] Defaulting to use mp for distributed inference\nINFO 03-03 18:57:34 [config.py:1519] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\nINFO 03-03 18:57:34 [config.py:1661] Chunked prefill is enabled with max_num_batched_tokens=2048.\nINFO 03-03 18:57:40 [__init__.py:207] Automatically detected platform rocm.\nINFO 03-03 18:57:57 [core.py:50] Initializing a V1 LLM engine (v0.7.4.dev181+gf35f8e22.d20250303) with config: model='fixie-ai/ultravox-v0_5-llama-3_3-70b', speculative_config=None, tokenizer='fixie-ai/ultravox-v0_5-llama-3_3-70b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=9000, download_dir='/app/data/models', load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=fixie-ai/ultravox-v0_5-llama-3_3-70b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\nWARNING 03-03 18:57:57 [multiproc_worker_utils.py:309] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 03-03 18:57:57 [custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\nINFO 03-03 18:57:57 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 10485760, 10, 'psm_c2d15247'), local_subscribe_addr='ipc:///tmp/42d28b49-4bba-4d95-a4fd-1d3640a6b3a0', remote_subscribe_addr=None, remote_addr_ipv6=False)\nINFO 03-03 18:58:02 [__init__.py:207] Automatically detected platform rocm.\n```\nboth chat completion and tool call work.\nBut audio doesn't work in this case (pasting base64 as well for ease of running);\n```\nfrom openai import OpenAI\nimport requests\nimport json\nimport time\n\n\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\ntext = \"Tell me a fun fact!\"\n\n# audio_base64 = generate_audio(text)\n\n# mp3 base64 for \"Tell me a fun fact!\"\naudio_base64 = \"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\"\n\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodel = \"fixie-ai/ultravox-v0_5-llama-3_3-70b\"\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\"role\": \"user\", \"content\": [\n            {\n                \"type\": \"audio_url\",\n                \"audio_url\": {\n                    # Any format supported by librosa is supported\n                    \"url\": f\"data:audio/mp3;base64,{audio_base64}\"\n                },\n            },\n        ]},\n    ],\n    model=model,\n    stream=False,\n)\n\nprint(chat_completion)\n```\n\non vLLM side I get:\n```\nERROR 03-03 19:06:24 [serving_chat.py:664] Error in chat completion stream generator.\nERROR 03-03 19:06:24 [serving_chat.py:664] Traceback (most recent call last):\nERROR 03-03 19:06:24 [serving_chat.py:664]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 362, in chat_completion_stream_generator\nERROR 03-03 19:06:24 [serving_chat.py:664]     async for res in result_generator:\nERROR 03-03 19:06:24 [serving_chat.py:664]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 208, in _generate\nERROR 03-03 19:06:24 [serving_chat.py:664]     q = await self.add_request(\nERROR 03-03 19:06:24 [serving_chat.py:664]         ^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-03 19:06:24 [serving_chat.py:664]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 153, in add_request\nERROR 03-03 19:06:24 [serving_chat.py:664]     request = self.processor.process_inputs(request_id, prompt, params,\nERROR 03-03 19:06:24 [serving_chat.py:664]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-03 19:06:24 [serving_chat.py:664]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/processor.py\", line 129, in process_inputs\nERROR 03-03 19:06:24 [serving_chat.py:664]     preprocessed_inputs = self.input_preprocessor.preprocess(\nERROR 03-03 19:06:24 [serving_chat.py:664]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-03 19:06:24 [serving_chat.py:664]   File \"/usr/local/lib/python3.12/dist-packages/vllm/inputs/preprocess.py\", line 766, in preprocess\nERROR 03-03 19:06:24 [serving_chat.py:664]     return self._process_decoder_only_prompt(\nERROR 03-03 19:06:24 [serving_chat.py:664]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-03 19:06:24 [serving_chat.py:664]   File \"/usr/local/lib/python3.12/dist-packages/vllm/inputs/preprocess.py\", line 715, in _process_decoder_only_prompt\nERROR 03-03 19:06:24 [serving_chat.py:664]     prompt_comps = self._prompt_to_llm_inputs(\nERROR 03-03 19:06:24 [serving_chat.py:664]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-03 19:06:24 [serving_chat.py:664]   File \"/usr/local/lib/python3.12/dist-packages/vllm/inputs/preprocess.py\", line 347, in _prompt_to_llm_inputs\nERROR 03-03 19:06:24 [serving_chat.py:664]     return self._process_multimodal(\nERROR 03-03 19:06:24 [serving_chat.py:664]            ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-03 19:06:24 [serving_chat.py:664]   File \"/usr/local/lib/python3.12/dist-packages/vllm/inputs/preprocess.py\", line 277, in _process_multimodal\nERROR 03-03 19:06:24 [serving_chat.py:664]     return mm_processor.apply(prompt, mm_data, mm_processor_kwargs)\nERROR 03-03 19:06:24 [serving_chat.py:664]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-03 19:06:24 [serving_chat.py:664]   File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/processing.py\", line 1513, in apply\nERROR 03-03 19:06:24 [serving_chat.py:664]     self._validate_mm_placeholders(mm_placeholders, mm_item_counts)\nERROR 03-03 19:06:24 [serving_chat.py:664]   File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/processing.py\", line 1423, in _validate_mm_placeholders\nERROR 03-03 19:06:24 [serving_chat.py:664]     raise RuntimeError(\nERROR 03-03 19:06:24 [serving_chat.py:664] RuntimeError: Expected there to be 1 prompt updates corresponding to 1 audio items, but instead found 0 prompt updates! Either the prompt text has missing/incorrect tokens for multi-modal inputs, or there is a problem with your implementation of merged multi-modal processor for this model (usually arising from an inconsistency between `_call_hf_processor` and `_get_prompt_updates`).\n```\n\nThe result is same without specifying chat template.\n\nThis works when auto tool is not enabled.\n\nI found [this thread](https://github.com/fixie-ai/ultravox/issues/275) which mentions it should work.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-03-04T13:33:04+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14209/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14209"
  },
  {
    "number": 16321,
    "title": "[Bug]: issues with guided generation for tool calls (xgrammar)",
    "body": "### Your current environment\n\ndocker\n\n### \ud83d\udc1b Describe the bug\n\nExample code from the https://docs.vllm.ai/en/latest/features/tool_calling.html doesn't work.\n\nI run vllm server in docker\n```\ndocker run --gpus all --runtime=nvidia \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    -p 8000:8000 \\\n    --ipc=host \\\n    vllm/vllm-openai:latest \\\n    --model Qwen/Qwen2.5-3B-Instruct-GPTQ-Int8 \\\n    --enable-auto-tool-choice \\\n    --tool-call-parser hermes\n```\nwith code from the example but I changed \n```\ntool_choice=\"required\"\n```\nBut I got an error\n```\nopenai.BadRequestError: Error code: 400 - {'object': 'error', 'message': 'The provided JSON schema contains features not supported by xgrammar.', 'type': 'BadRequestError', 'param': None, 'code': 400}=\n```",
    "labels": [
      "bug",
      "structured-output",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-04-09T06:46:03+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16321/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16321"
  },
  {
    "number": 9451,
    "title": "[Feature]: Consider parallel_tool_calls parameter at the API level",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently, there is a [parallel_tool_calls](https://github.com/vllm-project/vllm/blob/18b296fdb2248e8a65bf005e7193ebd523b875b6/vllm/entrypoints/openai/protocol.py#L177) field that is part of the `ChatCompletionRequest` pydantic class. However, this field is only there for being compatible with OpenAI's API.\r\n\r\nIn other words, it's not being used at all according to the documentation or the code:\r\n\r\n```\r\n# NOTE this will be ignored by VLLM -- the model determines the behavior\r\nparallel_tool_calls: Optional[bool] = False\r\n```\r\n\r\nWould it be possible to consider implementing the logic behind this field for different model families. For instance, in the case of llama3.1-8b-insturct, tool calling works, but the model ends up returning three tool calls instead of one by one.\r\nThis makes me lose compatibility with frameworks like LangGraph.\r\n\r\nHere's an example request and response:\r\n\r\n**Request**\r\n```\r\n{\r\n  \"messages\": [\r\n    {\r\n      \"content\": \"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\",\r\n      \"role\": \"system\"\r\n    },\r\n    {\r\n      \"content\": \"Add 3 and 4. Multiply the output by 2. Divide the output by 5\",\r\n      \"role\": \"user\"\r\n    }\r\n  ],\r\n  \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\r\n  \"stream\": false,\r\n  \"n\": 1,\r\n  \"temperature\": 0.0,\r\n  \"max_tokens\": 256,\r\n  \"tools\": [\r\n    {\r\n      \"type\": \"function\",\r\n      \"function\": {\r\n        \"name\": \"add\",\r\n        \"description\": \"Adds a and b.\",\r\n        \"parameters\": {\r\n          \"properties\": {\r\n            \"a\": {\r\n              \"description\": \"first int\",\r\n              \"type\": \"integer\"\r\n            },\r\n            \"b\": {\r\n              \"description\": \"second int\",\r\n              \"type\": \"integer\"\r\n            }\r\n          },\r\n          \"required\": [\"a\", \"b\"],\r\n          \"type\": \"object\"\r\n        }\r\n      }\r\n    },\r\n    {\r\n      \"type\": \"function\",\r\n      \"function\": {\r\n        \"name\": \"multiply\",\r\n        \"description\": \"Multiply a and b.\",\r\n        \"parameters\": {\r\n          \"properties\": {\r\n            \"a\": {\r\n              \"description\": \"first int\",\r\n              \"type\": \"integer\"\r\n            },\r\n            \"b\": {\r\n              \"description\": \"second int\",\r\n              \"type\": \"integer\"\r\n            }\r\n          },\r\n          \"required\": [\"a\", \"b\"],\r\n          \"type\": \"object\"\r\n        }\r\n      }\r\n    },\r\n    {\r\n      \"type\": \"function\",\r\n      \"function\": {\r\n        \"name\": \"divide\",\r\n        \"description\": \"Divide a and b.\",\r\n        \"parameters\": {\r\n          \"properties\": {\r\n            \"a\": {\r\n              \"description\": \"first int\",\r\n              \"type\": \"integer\"\r\n            },\r\n            \"b\": {\r\n              \"description\": \"second int\",\r\n              \"type\": \"integer\"\r\n            }\r\n          },\r\n          \"required\": [\"a\", \"b\"],\r\n          \"type\": \"object\"\r\n        }\r\n      }\r\n    }\r\n  ],\r\n  \"parallel_tool_calls\": false\r\n}\r\n```\r\n\r\n**Response**\r\n```\r\n{\r\n  \"ChatCompletion\": {\r\n    \"id\": \"chat-32cb47446c5b471eba5c91be1755811e\",\r\n    \"choices\": [\r\n      {\r\n        \"finish_reason\": \"tool_calls\",\r\n        \"index\": 0,\r\n        \"logprobs\": null,\r\n        \"message\": {\r\n          \"content\": null,\r\n          \"refusal\": null,\r\n          \"role\": \"assistant\",\r\n          \"function_call\": null,\r\n          \"tool_calls\": [\r\n            {\r\n              \"id\": \"chatcmpl-tool-f8c832f4a42445f899a229063004cae9\",\r\n              \"function\": {\r\n                \"arguments\": '{\"a\": 3, \"b\": 4}',\r\n                \"name\": \"add\"\r\n              },\r\n              \"type\": \"function\"\r\n            },\r\n            {\r\n              \"id\": \"chatcmpl-tool-4b44f70f7dde47d0820f8a3b9018b897\",\r\n              \"function\": {\r\n                \"arguments\": '{\"a\": 7, \"b\": 2}',\r\n                \"name\": \"multiply\"\r\n              },\r\n              \"type\": \"function\"\r\n            },\r\n            {\r\n              \"id\": \"chatcmpl-tool-d897bd7ecb4b44e59eb718aff21cbfa8\",\r\n              \"function\": {\r\n                \"arguments\": '{\"a\": 14, \"b\": 5}',\r\n                \"name\": \"divide\"\r\n              },\r\n              \"type\": \"function\"\r\n            }\r\n          ]\r\n        },\r\n        \"stop_reason\": 128008\r\n      }\r\n    ],\r\n    \"created\": 1729149431,\r\n    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\r\n    \"object\": \"chat.completion\",\r\n    \"service_tier\": null,\r\n    \"system_fingerprint\": null,\r\n    \"usage\": {\r\n      \"completion_tokens\": 67,\r\n      \"prompt_tokens\": 466,\r\n      \"total_tokens\": 533,\r\n      \"completion_tokens_details\": null,\r\n      \"prompt_tokens_details\": null\r\n    },\r\n    \"prompt_logprobs\": null\r\n  }\r\n}\r\n```\r\n\r\nEven if I wanted to do a posterior call to the model using the three tool calls at the same time, it will complain with an error of: \r\n\r\n`BadRequestError: Error code: 400 - {'object': 'error', 'message': 'This model only supports single tool-calls at once!', 'type': 'BadRequestError', 'param': None, 'code': 400}`\r\n\r\nWhich comes from this [llama3_json](https://gist.github.com/K-Mistele/820d142b4dab50bd8ef0c7bbcad4515c#file-tool_chat_template_llama31_json-jinja-L48) template.\r\n\r\nThank you Team!.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2024-10-17T07:41:26+00:00",
    "closed_at": null,
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9451/reactions",
      "total_count": 6,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9451"
  },
  {
    "number": 16880,
    "title": "[Bug]: [v0.8.4][Critical] Tools calling broken: xgrammar rejects minItems in JSON Schema, blocking agent functionality",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Red Hat Enterprise Linux 8.10 (Ootpa) (x86_64)\nGCC version: (GCC) 9.2.1 20191120 (Red Hat 9.2.1-2)\nClang version: Could not collect\nCMake version: version 3.27.7\nLibc version: glibc-2.28\n\nPython version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-4.18.0-553.40.1.el8_10.x86_64-x86_64-with-glibc2.28\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA A100 80GB PCIe\nGPU 1: NVIDIA A100 80GB PCIe\nGPU 2: NVIDIA A100 80GB PCIe\nGPU 3: NVIDIA A100 80GB PCIe\n\nNvidia driver version: 550.54.15\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:        x86_64\nCPU op-mode(s):      32-bit, 64-bit\nByte Order:          Little Endian\nCPU(s):              96\nOn-line CPU(s) list: 0-95\nThread(s) per core:  1\nCore(s) per socket:  48\nSocket(s):           2\nNUMA node(s):        4\nVendor ID:           AuthenticAMD\nCPU family:          25\nModel:               1\nModel name:          AMD EPYC 7V13 64-Core Processor\nStepping:            1\nCPU MHz:             2445.443\nBogoMIPS:            4890.88\nHypervisor vendor:   Microsoft\nVirtualization type: full\nL1d cache:           32K\nL1i cache:           32K\nL2 cache:            512K\nL3 cache:            32768K\nNUMA node0 CPU(s):   0-23\nNUMA node1 CPU(s):   24-47\nNUMA node2 CPU(s):   48-71\nNUMA node3 CPU(s):   72-95\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr arat umip vaes vpclmulqdq rdpid fsrm\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.3+cu124torch2.5\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0\n[pip3] torchao==0.9.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.50.3\n[pip3] triton==3.2.0\n[conda] flashinfer-python         0.2.3+cu124torch2.5          pypi_0    pypi\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-ml-py              12.570.86                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.3.0                   pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchao                   0.9.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] transformers              4.50.3                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV12    SYS     SYS     SYS     0-23    0               N/A\nGPU1    NV12     X      SYS     SYS     SYS     24-47   1               N/A\nGPU2    SYS     SYS      X      NV12    SYS     48-71   2               N/A\nGPU3    SYS     SYS     NV12     X      SYS     72-95   3               N/A\nNIC0    SYS     SYS     SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n\nLD_LIBRARY_PATH=/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib\nVLLM_PORT=8081\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen using the OpenAI\u2010compatible tool\u2010calling interface with `--guided-decoding-backend xgrammar` and `tool_choice=\"required\"`, the client auto\u2011injects a JSON Schema for the array of tool calls that contains `\"minItems\":\u00a01`. vLLM\u2019s xgrammar backend currently [rejects any schema with minItems](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/guided_decoding/utils.py#L6), resulting in:\n\n```javascript\nopenai.BadRequestError: Error code: 400 - {\n  'message': 'The provided JSON schema contains features not supported by xgrammar.',\n  \u2026\n}\n```\n\nAs a result, tool-calling with agents is completely disabled in this configuration.\n\nIf I switch to `tool_choice=\"auto\"`, the error disappears but the model never emits any tool_calls (so `response.choices[0].message.tool_calls` is empty and I get an IndexError).\n\n\n## To reproduce \n```python\nimport os, json\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\nload_dotenv()\nclient = OpenAI(\n    base_url=os.getenv(\"VLLM_ENDPOINT\"),       # e.g. http://localhost:8000\n    api_key=\"****\",                            # redacted\n)\n\ndef get_weather(location: str, unit: str):\n    return f\"Getting the weather for {location} in {unit}\u2026\"\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"},\n                    \"unit\":     {\"type\": \"string\", \"enum\": [\"celsius\",\"fahrenheit\"]},\n                },\n                \"required\": [\"location\",\"unit\"],\n            },\n        },\n    }\n]\n\n# This triggers the BadRequestError:\nresponse = client.chat.completions.create(\n    model=client.models.list().data[0].id,\n    messages=[{\"role\":\"user\",\"content\":\"What's the weather in SF?\"}],\n    tools=tools,\n    tool_choice=\"required\",          # \u2190 forces an array schema with minItems:1\n)\n\n# IndexError if you use tool_choice=\"auto\" because tool_calls is empty\ntool_call = response.choices[0].message.tool_calls[0]\n\n```\n\n## Expected behavior\nWith `tool_choice=\"required\"`, vLLM should accept the generated array schema (even with `minItems:1`), or at least strip/ignore unsupported keywords like minItems.\n\nWith `tool_choice=\"auto\"`, the model should still choose to call the function when appropriate (e.g. a weather\u2010related query).\n\n## Logs\nError with `tool_choice=\"required\"` and `--guided-decoding-backend xgrammar`:\n\n```javascript\nBadRequestError: Error code: 400 - {\n  \"object\": \"error\",\n  \"message\": \"The provided JSON schema contains features not supported by xgrammar.\",\n  \"type\": \"BadRequestError\",\n  \"param\": null,\n  \"code\": 400\n}\n```\n\nvLLM server log:\n```log\n\u2026 guided_decoding=GuidedDecodingParams(json={\n    \"type\": \"array\",\n    \"minItems\": 1,\n    \"items\": { \u2026 }\n}, regex=None, choice=None, grammar=None, backend=None)\n\u2026\n\n```\n\n\n## Behavior with tool_choice=\"auto\":\n\n- No error in server or client logs\n- response.choices[0].message.tool_calls is an empty list \u2192 IndexError in client\n\n## Additional context\n\n- vLLM version: 0.8.4 (Docker image vllm/vllm-openai:v0.8.4.full)\n- Running with:\n```bash\n--guided-decoding-backend xgrammar \\\n--enable-auto-tool-choice \\\n--tool-call-parser hermes\n```\n- I have also tried running with\n```bash\n--guided-decoding-backend auto\n```\nIn that mode I no longer see the minItems error and tool calls succeed, but it\u2019s unclear whether vLLM is still using the xgrammar backend under the hood or silently falling back to another decoding backend (e.g. outlines).\n\nThis is a critical issue: without support for minItems in xgrammar, agent tool-calling is completely non-functional. Either xgrammar must relax JSON Schema restrictions or the client should avoid emitting unsupported keywords when targeting xgrammar.\n\nThanks for your work on vLLM! \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-04-19T19:09:55+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16880/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16880"
  },
  {
    "number": 17061,
    "title": "[Usage]: Does vLLM support QwQ 32B + tool calling?",
    "body": "### Your current environment\n\nIt's pretty unclear, I wanted to see if people have tried it and see if it's actually working without any issues.\n\n### How would you like to use vllm\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "tool-calling"
    ],
    "state": "closed",
    "created_at": "2025-04-23T15:38:29+00:00",
    "closed_at": "2025-04-25T12:08:30+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17061/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17061"
  },
  {
    "number": 17821,
    "title": "Tool calls not triggered properly with vLLM 0.8.5 and Qwen2.5-Coder-32B-Instruct-GPTQ-Int4",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nINFO 05-07 17:26:12 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 4.0.0\nLibc version: glibc-2.35\n\nPython version: 3.12.10 (main, Apr  9 2025, 08:55:05) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-6.8.0-35-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: NVIDIA H100 80GB HBM3\nGPU 1: NVIDIA H100 80GB HBM3\nGPU 2: NVIDIA H100 80GB HBM3\nGPU 3: NVIDIA H100 80GB HBM3\n\nNvidia driver version: 550.54.15\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               192\nOn-line CPU(s) list:                  0-191\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8468\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   48\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          3800.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4200.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts re\np_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c\n rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt\n_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx\n512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect\ncldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            4.5 MiB (96 instances)\nL1i cache:                            3 MiB (96 instances)\nL2 cache:                             192 MiB (96 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-47,96-143\nNUMA node1 CPU(s):                    48-95,144-191\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.1.post2+cu124torch2.6\n[pip3] numpy==2.2.5\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.3\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.5\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    0-47,96-143     0               N/A\nGPU1    NV18     X      NV18    NV18    0-47,96-143     0               N/A\nGPU2    NV18    NV18     X      NV18    48-95,144-191   1               N/A\nGPU3    NV18    NV18    NV18     X      48-95,144-191   1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNVIDIA_VISIBLE_DEVICES=GPU-6a4a45dc-ecb9-6afd-7352-32af35190639,GPU-61cd127b-e2e7-08f6-2a43-7f247e84d08b,GPU-5459e040-6673-7776-06b4-456c2b7a9dcd,GPU-72602610-166f-a3c2-1bd5-ecd36e81bddd\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforce\nrtx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driv\ner>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,drive\nr>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,d\nriver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driv\ner<536\nNCCL_VERSION=2.20.5-1\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nCUDA_VERSION=12.4.0\nNVIDIA_DISABLE_REQUIRE=true\nLD_LIBRARY_PATH=/usr/lib/nvidia:/usr/lib/x86_64-linux-gnu\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n Hi team,\n\nI am running `vllm` version 0.8.5 with the following launch command:\n\n```\nvllm serve Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4 \\\n  --quantization gptq \\\n  --download-dir /models \\\n  --tensor-parallel-size 4 \\\n  --enable-auto-tool-choice \\\n  --tool-call-parser hermes\n```\n\nI am trying to test function calling (tool calling) with the model (`Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4`), sending the following request body (for example, via Postman):\n\n```json\n{\n    \"model\": \"Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4\",\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"you are helpful ai\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"what is the weather in seoul?\"\n        }\n    ],\n    \"stream\": false,\n    \"tool_choice\": \"auto\",\n    \"tools\": [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_weather\",\n                \"description\": \"Get the current weather in a given location\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"City and state, e.g., 'San Francisco, CA'\"\n                        },\n                        \"unit\": {\n                            \"type\": \"string\",\n                            \"enum\": [\n                                \"celsius\",\n                                \"fahrenheit\"\n                            ]\n                        }\n                    },\n                    \"required\": [\n                        \"location\",\n                        \"unit\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n```\n\nHowever, I'm seeing the following issue:  \nInstead of getting an actual `tool_call` in the response, this is what I get:\n\n```json\n\"choices\": [\n    {\n        \"index\": 0,\n        \"message\": {\n            \"role\": \"assistant\",\n            \"reasoning_content\": null,\n            \"content\": \"{\\\"name\\\": \\\"get_weather\\\", \\\"arguments\\\": {\\\"location\\\": \\\"Seoul, South Korea\\\", \\\"unit\\\": \\\"celsius\\\"}}\",\n            \"tool_calls\": []\n        },\n        \"logprobs\": null,\n        \"finish_reason\": \"stop\",\n        \"stop_reason\": null\n    }\n],\n```\n\nSo, the function is being output as a string in the assistant's message content, and the `tool_calls` field is empty. It seems that the tool calling logic is not being triggered as expected, even though I'm sending `tool_choice: \"auto\"` and passed `--enable-auto-tool-choice` to `vllm serve`.\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-05-08T00:29:00+00:00",
    "closed_at": null,
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17821/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17821"
  },
  {
    "number": 15035,
    "title": "[Feature]: Add support for reusable subschemas in tool requests (PydanticAI)",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently PydanticAI clients leverage tools for structured response mapping. Consider the following ``tools`` definition in the request:\n\n```json\n[\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"final_result\",\n            \"description\": \"The final response which ends this conversation\",\n            \"parameters\": {\n                \"$defs\": {\n                    \"Chapter\": {\n                        \"properties\": {\n                            \"chapter_name\": {\n                                \"description\": \"Name the chapter\",\n                                \"title\": \"Chapter Name\",\n                                \"type\": \"string\"\n                            },\n                            \"content\": {\n                                \"description\": \"Content of the chapter\",\n                                \"title\": \"Content\",\n                                \"type\": \"string\"\n                            }\n                        },\n                        \"required\": [\n                            \"chapter_name\",\n                            \"content\"\n                        ],\n                        \"title\": \"Chapter\",\n                        \"type\": \"object\"\n                    }\n                },\n                \"properties\": {\n                    \"title\": {\n                        \"description\": \"Title of the story\",\n                        \"title\": \"Title\",\n                        \"type\": \"string\"\n                    },\n                    \"summary\": {\n                        \"description\": \"Short summary of the story\",\n                        \"title\": \"Summary\",\n                        \"type\": \"string\"\n                    },\n                    \"chapters\": {\n                        \"description\": \"List of chapters\",\n                        \"items\": {\n                            \"$ref\": \"#/$defs/Chapter\"\n                        },\n                        \"title\": \"Chapters\",\n                        \"type\": \"array\"\n                    }\n                },\n                \"required\": [\n                    \"title\",\n                    \"summary\",\n                    \"chapters\"\n                ],\n                \"title\": \"Story\",\n                \"type\": \"object\"\n            }\n        }\n    }\n]\n```\n\nHere, ``parameters`` contains the reusable subschema ``Chapter`` passed under ``\"$defs\"``. This is a valid JSON schema as rendered by a Pydantic ``BaseModel``, however results in a HTTP 400 error in vLLM. \n\n\n\n### Alternatives\n\nFor PydanticAI clients there are a few options available:\n\n* Don't use response schemas with nested ``BaseModels``\n* Update PydanticAI such that subschemas are de-normalized before calling the vLLM completions endpoint.\n\n### Additional context\n\n## Code to Reproduce\n\nThe PydanticAI agent example below can reproduce the issue. Note that it needs to be run against [PR13483](https://github.com/vllm-project/vllm/pull/13483) to work properly with PydanticAI:\n\n```python\nclass Chapter(BaseModel):\n    chapter_name: str = Field(..., description=\"Name the chapter\")\n    content: str = Field(..., description=\"Content of the chapter\")\n\nclass Story(BaseModel):\n    title: str = Field(..., description=\"Title of the story\")\n    summary: str = Field(..., description=\"Short summary of the story\")\n    chapters: List[Chapter] = Field(..., description=\"List of chapters\")\n\n# Create a PydanticAI agent\nagent = Agent(\n    name=\"test_tools3_agent\",\n    model=llm,\n    system_prompt=\"You are a creative novelist and helpful assistant.\",\n)\n\n# Fails with error described.\nresult = await agent.run(\n    \"Generate a short story about cats.\",\n    result_type=Story,\n)\n\nLOG.info(\"Results: %s\", result.data)\n```\n\nIf I change ``chapters: List[Chapter]`` to ``chapters: List[str]`` the example runs perfectly. since there is no subschema passed.\n\n## vLLM Startup Command\n\nNote that I am running the version of vLLM from [PR13483](https://github.com/vllm-project/vllm/pull/13483). Basically this PR adds support for ``tool_choice=required`` which is needed for PydanticAI. This is my vLLM run command:\n\n```shell\ndocker run -it -d \\\n    --name=eleanor-vLLM \\\n    --restart=unless-stopped \\\n    --shm-size=15g \\\n    --ulimit memlock=-1 \\\n    --ipc=host \\\n    --entrypoint=python3 \\\n    --gpus=\"device=0,1,2,3\" \\\n    --publish=7800:8000 \\\n    --volume=/models:/models:ro \\\n    --health-cmd=timeout 5 bash -c 'cat < /dev/null > /dev/tcp/localhost/8000' \\\n    --health-start-period=240s \\\n    --health-interval=15s \\\n    --health-timeout=8s \\\n    --health-retries=3 \\\n    --env=OMP_NUM_THREADS=1 \\\n    harbor.k8s.wm.k8slab/eleanor-ai/vllm-openai:tool-req-patch \\\n        -m vllm.entrypoints.openai.api_server \\\n        --model /models/Llama-3.3-70B-Instruct \\\n        --served-model-name Llama-3.3-70B-Instruct \\\n        --response-role auto \\\n        --load-format safetensors \\\n        --tokenizer-mode auto \\\n        --enable-chunked-prefill=True \\\n        --max-num-batched-tokens=4096 \\\n        --dtype bfloat16 \\\n        --kv-cache-dtype auto \\\n        --gpu-memory-utilization 0.90 \\\n        --enable-auto-tool-choice \\\n        --tool-call-parser llama3_json \\\n        --enable-prefix-caching \\\n        --device=cuda \\\n        --task=generate \\\n        --scheduler-delay-factor=0.25 \\\n        --uvicorn-log-level=debug \\\n        --distributed-executor-backend=mp \\\n        --max-logprobs=100 \\\n        --enable-prompt-tokens-details \\\n        --generation-config=auto \\\n        --override-generation-config={\"logprobs\": 1} \\\n        --guided-decoding-backend=outlines \\\n        --disable_custom_all_reduce \\\n        --max-model-len 65535 \\\n        --tensor-parallel-size 4 \\\n        --port 8000 \\\n        --host 0.0.0.0\n```\n\n## vLLM Logs\n\nRequest:\n\n```text\n'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nEnvironment: ipython\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a creative novelist and helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nGiven the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\\n\\nRespond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.Do not use variables.\\n\\n{\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"final_result\",\\n        \"description\": \"The final response which ends this conversation\",\\n        \"parameters\": {\\n            \"$defs\": {\\n                \"Chapter\": {\\n                    \"properties\": {\\n                        \"chapter_name\": {\\n                            \"description\": \"Name the chapter\",\\n                            \"title\": \"Chapter Name\",\\n                            \"type\": \"string\"\\n                        },\\n                        \"content\": {\\n                            \"description\": \"Content of the chapter\",\\n                            \"title\": \"Content\",\\n                            \"type\": \"string\"\\n                        }\\n                    },\\n                    \"required\": [\\n                        \"chapter_name\",\\n                        \"content\"\\n                    ],\\n                    \"title\": \"Chapter\",\\n                    \"type\": \"object\"\\n                }\\n            },\\n            \"properties\": {\\n                \"title\": {\\n                    \"description\": \"Title of the story\",\\n                    \"title\": \"Title\",\\n                    \"type\": \"string\"\\n                },\\n                \"summary\": {\\n                    \"description\": \"Short summary of the story\",\\n                    \"title\": \"Summary\",\\n                    \"type\": \"string\"\\n                },\\n                \"chapters\": {\\n                    \"description\": \"List of chapters\",\\n                    \"items\": {\\n                        \"$ref\": \"#/$defs/Chapter\"\\n                    },\\n                    \"title\": \"Chapters\",\\n                    \"type\": \"array\"\\n                }\\n            },\\n            \"required\": [\\n                \"title\",\\n                \"summary\",\\n                \"chapters\"\\n            ],\\n            \"title\": \"Story\",\\n            \"type\": \"object\"\\n        }\\n    }\\n}\\n\\nGenerate a short story about cats.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json={'type': 'array', 'minItems': 1, 'items': {'type': 'object', 'anyOf': [{'properties': {'name': {'type': 'string', 'enum': ['final_result']}, 'parameters': {'$defs': {'Chapter': {'properties': {'chapter_name': {'description': 'Name the chapter', 'title': 'Chapter Name', 'type': 'string'}, 'content': {'description': 'Content of the chapter', 'title': 'Content', 'type': 'string'}}, 'required': ['chapter_name', 'content'], 'title': 'Chapter', 'type': 'object'}}, 'properties': {'title': {'description': 'Title of the story', 'title': 'Title', 'type': 'string'}, 'summary': {'description': 'Short summary of the story', 'title': 'Summary', 'type': 'string'}, 'chapters': {'description': 'List of chapters', 'items': {'$ref': '#/$defs/Chapter'}, 'title': 'Chapters', 'type': 'array'}}, 'required': ['title', 'summary', 'chapters'], 'title': 'Story', 'type': 'object'}}, 'required': ['name', 'parameters']}]}}, regex=None, choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None), extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\nINFO 03-18 12:20:23 [async_llm_engine.py:549] Building guided decoding logits processor. guided_decoding: GuidedDecodingParams(json={'type': 'array', 'minItems': 1, 'items': {'type': 'object', 'anyOf': [{'properties': {'name': {'type': 'string', 'enum': ['final_result']}, 'parameters': {'$defs': {'Chapter': {'properties': {'chapter_name': {'description': 'Name the chapter', 'title': 'Chapter Name', 'type': 'string'}, 'content': {'description': 'Content of the chapter', 'title': 'Content', 'type': 'string'}}, 'required': ['chapter_name', 'content'], 'title': 'Chapter', 'type': 'object'}}, 'properties': {'title': {'description': 'Title of the story', 'title': 'Title', 'type': 'string'}, 'summary': {'description': 'Short summary of the story', 'title': 'Summary', 'type': 'string'}, 'chapters': {'description': 'List of chapters', 'items': {'$ref': '#/$defs/Chapter'}, 'title': 'Chapters', 'type': 'array'}}, 'required': ['title', 'summary', 'chapters'], 'title': 'Story', 'type': 'object'}}, 'required': ['name', 'parameters']}]}}, regex=None, choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None)\nINFO:     172.17.0.1:21179 - \"POST /v1/chat/completions HTTP/1.1\" 400 Bad Request\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-03-18T13:00:24+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15035/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15035"
  },
  {
    "number": 14951,
    "title": "[Bug]: vLLM response on tool_calls does not align with OpenAI standard",
    "body": "### Your current environment: vLLM 0.7.3 latest\n\nWe are trying to use tool_calls with vLLM running llama 3.1 or 3.2. We found that the tool_calls data returned from vLLM is not the same as what OpenAI demonstrated so the OpenAI Adaptors are not working as expected (the function name is concated as a very long string so it cannot be found).\n\nAs per OpenAI API document: [OpenAI Document for streaming function calling](https://platform.openai.com/docs/guides/function-calling?api-mode=chat&lang=javascript#streaming)\n\n<details>\n<summary>OpenAI streams</summary>\n\n- [{\"index\": 0, \"id\": \"call_DdmO9pD3xa9XTPNJ32zg2hcA\", \"function\": {\"arguments\": \"\", **\"name\": \"get_weather\"**}, \"type\": \"function\"}]\n- [{\"index\": 0, \"id\": null, \"function\": {\"arguments\": \"{\\\"\", \"name\": null}, \"type\": null}]\n- [{\"index\": 0, \"id\": null, \"function\": {\"arguments\": \"location\", \"name\": null}, \"type\": null}]\n- [{\"index\": 0, \"id\": null, \"function\": {\"arguments\": \"\\\":\\\"\", \"name\": null}, \"type\": null}]\n- [{\"index\": 0, \"id\": null, \"function\": {\"arguments\": \"Paris\", \"name\": null}, \"type\": null}]\n- [{\"index\": 0, \"id\": null, \"function\": {\"arguments\": \",\", \"name\": null}, \"type\": null}]\n- [{\"index\": 0, \"id\": null, \"function\": {\"arguments\": \" France\", \"name\": null}, \"type\": null}]\n- [{\"index\": 0, \"id\": null, \"function\": {\"arguments\": \"\\\"}\", \"name\": null}, \"type\": null}]\n</details>\n\nHowever, what we get from vLLM is: \n\n<details>\n<summary>vLLM streams</summary>\n\n- {\"id\":\"chatcmpl-dcd3e8852e0f4562a3a43a9dc7a61fbd\",\"object\":\"chat.completion.chunk\",\"created\":1741766314,\"model\":\"model\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}\n- {\"id\":\"chatcmpl-dcd3e8852e0f4562a3a43a9dc7a61fbd\",\"object\":\"chat.completion.chunk\",\"created\":1741766314,\"model\":\"model\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{**\"name\":\"get_weather\"**,\"arguments\":\"{\\\"\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n- {\"id\":\"chatcmpl-dcd3e8852e0f4562a3a43a9dc7a61fbd\",\"object\":\"chat.completion.chunk\",\"created\":1741766314,\"model\":\"model\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{**\"name\":\"get_weather\"**,\"arguments\":\"location\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n- {\"id\":\"chatcmpl-dcd3e8852e0f4562a3a43a9dc7a61fbd\",\"object\":\"chat.completion.chunk\",\"created\":1741766314,\"model\":\"model\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{**\"name\":\"get_weather\"**,\"arguments\":\"\\\":\\\"\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n- {\"id\":\"chatcmpl-dcd3e8852e0f4562a3a43a9dc7a61fbd\",\"object\":\"chat.completion.chunk\",\"created\":1741766314,\"model\":\"model\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{**\"name\":\"get_weather\"**,\"arguments\":\"Paris\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n- {\"id\":\"chatcmpl-dcd3e8852e0f4562a3a43a9dc7a61fbd\",\"object\":\"chat.completion.chunk\",\"created\":1741766314,\"model\":\"model\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{**\"name\":\"get_weather\"**,\"arguments\":\",\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n- {\"id\":\"chatcmpl-dcd3e8852e0f4562a3a43a9dc7a61fbd\",\"object\":\"chat.completion.chunk\",\"created\":1741766314,\"model\":\"model\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{**\"name\":\"get_weather\"**,\"arguments\":\" France\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n- {\"id\":\"chatcmpl-dcd3e8852e0f4562a3a43a9dc7a61fbd\",\"object\":\"chat.completion.chunk\",\"created\":1741766314,\"model\":\"model\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{**\"name\":\"get_weather\"**,\"arguments\":\"\\\"}\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n- {\"id\":\"chatcmpl-dcd3e8852e0f4562a3a43a9dc7a61fbd\",\"object\":\"chat.completion.chunk\",\"created\":1741766314,\"model\":\"model\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{**\"name\":\"get_weather\"**,\"arguments\":\"\"}}]},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null}]}\n\n</details>\n\nThen the processor is trying to do the same as receiving data from OpenAI and cause the confusion on function name.\n\nSo I found this is due to the code in [vllm/entrypoints/openai/serving_chat.py](https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/openai/serving_chat.py)\n\nin first_iteration, it is sending a simple choice:\n\n<img width=\"540\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/666e54b5-8e80-464f-897b-00a08bd31bad\" />\n\nand in each chunk sending the function name repeatedly:\n\n<img width=\"540\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/3cc55ae3-1a26-4208-9762-3210b06fb83c\" />\n\nSo I would like to suggest a following change:\n\n<img width=\"600\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/abef5a3c-3955-463b-9106-e4b5ad8095a6\" />\n\nand also:\n\n<img width=\"1200\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e2d2b5f9-cdef-467e-b62d-417a4f18a747\" />\n\nI have attached the complete code as attached: \n\n[serving_chat.py](https://github.com/user-attachments/files/19285168/serving_chat.py.txt)\n\nThanks for looking into this.\n\n### \ud83d\udc1b Describe the bug\n\nWe are using Elastic Observability AI Assistant to connect to vLLM running llama3.2 to identify the problem. We don't have a code for this issue.  Sample trace is as below: \n\n> Function title_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversationtitle_conversation called but was not available\nat createFunctionNotFoundError (/usr/share/kibana/node_modules/@kbn/observability-ai-assistant-plugin/common/conversation_complete.js:71:10)\n\tat Object.next (/usr/share/kibana/node_modules/@kbn/observability-ai-assistant-plugin/server/service/client/adapters/fail_on_non_existing_function_call.js:25:55)\n\tat /usr/share/kibana/node_modules/rxjs/dist/cjs/internal/operators/tap.js:20:81\n\tat OperatorSubscriber._this._next (/usr/share/kibana/node_modules/rxjs/dist/cjs/internal/operators/OperatorSubscriber.js:33:21)\n\tat OperatorSubscriber.Subscriber.next (/usr/share/kibana/node_modules/rxjs/dist/cjs/internal/Subscriber.js:51:18)\n\t...\n\n> Tool structuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutputstructuredOutput called but was not available\nat createToolNotFoundError (/usr/share/kibana/node_modules/@kbn/inference-plugin/server/chat_complete/errors.js:32:10)\n\tat /usr/share/kibana/node_modules/@kbn/inference-plugin/server/util/validate_tool_calls.js:33:49\n\tat Array.map (<anonymous>)\n\tat validateToolCalls (/usr/share/kibana/node_modules/@kbn/inference-plugin/server/util/validate_tool_calls.js:29:20)\n\tat /usr/share/kibana/node_modules/@kbn/inference-plugin/server/chat_complete/utils/chunks_into_message.js:48:77\n\tat /usr/share/kibana/node_modules/rxjs/dist/cjs/internal/operators/map.js:10:37\n\t...\n\nAfter checking the network dump, we realize the problem is because the misalignment of vLLM stream tool_calls message with OpenAI (which we are using OpenAI compatible protocol). Please let me know if need more information. \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-03-17T11:36:28+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14951/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14951"
  },
  {
    "number": 17817,
    "title": "[RFC]: Unification of frontend parser",
    "body": "## motivation\n\nhttps://github.com/vllm-project/vllm/issues/11522 (with draft implementation at https://github.com/vllm-project/vllm/pull/11554)\naims to simplify the logics of the tool parser interface. However, this doesn't cover the cases for reasoning models (where we want to parse\ntokens generated within the thinking budgets, etc. Our current solutions involves a reasoning parser, which will soon be running into the same\nissue mentioned in #11522 when dealing with very long thinking budget). Additionally, the current implementations of tool calling are relatively\nfragile, and not scalable when adding more tool format.\n\nThis RFC aims to build on top of some similar ideas from the RFC and unify both tool calling and reasoning parser logic for a more robust\nway for us to move forward, especially with v0.10.x.\n\n## proposed change\n\n\nThe workflow can be seen as follows:\n\n- function/tool calling format for supported models (defined by the LLMEngine)\n- Construct structural tags <- said tool/function calling format\n- perform constrained decoding with supported backend (xgrammar/guidance)\n- parser to convert string response -> structured objects\n\nFrom vLLM perspective:\n\n```bash\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Prompt \u2502\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 vLLM\u202f(OpenAI\u2011compatible FE)    \u2502\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502 [tool / func\u2011call \u2502 reasoning_fmt]\n    \u25bc                   \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n\u2502  Parser  \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 LLM Engine \u2502\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Parser \u2502\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 vLLM\u202f(OpenAI\u2011compatible FE)\u2502\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502 \n    \u25bc \n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Output \u2502  \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n```\n\nAim:\n\n- Simplified and unified interface called `vllm.Parser`\n\nThere are a few compatibility matrix we need to consider:\n\n| features              | function/tool caling | structured outputs | reasoning |\n| --------------------- | -------------------- | ------------------ | --------- |\n| function/tool calling | -                    |                    |           |\n| structured outputs    |                      | -                  |           |\n| reasoning             |                      |                    | -         |\n\n_NOTE_: For reasoning logics, there are forced/non-forced mode (which is recently introduced by Qwen3-series of models)\n\nA ad-hoc implementation of the parser would be\n\n```python\nclass Parser:\n  tool: bool = False\n  reasoning: bool = False\n\n  def parse_tool_call(self, structural_tag: StructuralTagResult) -> ToolCallResult: ...\n\n  def parse_tool_call_stream(self, structural_tag: StructuralTagResult) -> DeltaToolCallResult: ...\n\n  def parse_reasoning(self, structural_tag: StructuralTagResult) -> ReasoningResult: ...\n\n  def parse_reasoning_stream(self, structural_tag: StructuralTagResult) -> DeltaReasoningResult: ...\n\nclass Llama3JSON(Parser, tool=True, name=\"llama3-json\"): ...\nclass Pythonic(Parser, tool=True, name=\"pythonic\"): ...\n\nclass DeepSeek(Parser, tool=True, reasoning=True, name=\"deepseek_r1\"): ...\n```\n\n`serving_chat.py`:\n\n```python\n\n```\n\n## Feedback period\n\ntbd. wrt implementations, We will need to wait from the xgrammar team to have this support\n\n## CC List\n\n@mgoin @russellb @robertgshaw2-redhat @mmoskal \n\n## Any Other Thing\n\n- We should probably move all of the tool/chat templates under `vllm/tools`\n",
    "labels": [
      "structured-output",
      "RFC",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-05-07T21:46:18+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17817/reactions",
      "total_count": 4,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 3,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/17817"
  },
  {
    "number": 16195,
    "title": "[Bug]: Models converted to GGUF don't seem to be able to do tool calling",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Fedora Linux 41 (Forty One) (x86_64)\nGCC version: (GCC) 14.2.1 20250110 (Red Hat 14.2.1-7)\nClang version: Could not collect\nCMake version: version 3.30.8\nLibc version: glibc-2.40\n\nPython version: 3.12.8 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:31:09) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-6.13.9-200.fc41.x86_64-x86_64-with-glibc2.40\nIs CUDA available: True\nCUDA runtime version: 12.8.93\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090\nNvidia driver version: 570.124.06\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        43 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               24\nOn-line CPU(s) list:                  0-23\nVendor ID:                            AuthenticAMD\nModel name:                           AMD Ryzen 9 3900XT 12-Core Processor\nCPU family:                           23\nModel:                                113\nThread(s) per core:                   2\nCore(s) per socket:                   12\nSocket(s):                            1\nStepping:                             0\nFrequency boost:                      enabled\nCPU(s) scaling MHz:                   62%\nCPU max MHz:                          4777.0000\nCPU min MHz:                          550.0000\nBogoMIPS:                             7600.56\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sev sev_es\nVirtualization:                       AMD-V\nL1d cache:                            384 KiB (12 instances)\nL1i cache:                            384 KiB (12 instances)\nL2 cache:                             6 MiB (12 instances)\nL3 cache:                             64 MiB (4 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-23\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.4.0\n[pip3] sentence-transformers==4.0.2\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.0\n[pip3] triton==3.2.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-ml-py              12.570.86                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.4.0                   pypi_0    pypi\n[conda] sentence-transformers     4.0.2                    pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] transformers              4.51.0                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \t0-23\t0\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=/usr/local/cuda-12.8/lib64:\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen a model is converted to the GGUF format with no quantization, it can no longer work when async tool calling is requested. This is through the OpenAI API server. This is the steps to reproduce the problem:\n```\nhuggingface-cli download meta-llama/Llama-3.2-3B-Instruct --local-dir Llama-3.2-3B-Instruct --include \"*\"\n./llama.cpp/convert_hf_to_gguf.py Llama-3.2-3B-Instruct --outfile Llama-3.2-3B-Instruct.gguf\n\nNo quantization run. Only in GGUF format.\n\n/bin/env python3 -m vllm.entrypoints.openai.api_server \\\n        --host 0.0.0.0 --port 8000 \\\n        --chat-template templates/tool_chat_template_llama3.2_json.jinja \\\n        --model ~/Llama-3.2-3B-Instruct.gguf \\\n        --max-model-len 102400 \\\n        --gpu_memory_utilization 0.95 \\\n\t--quantization \"fp8\" \\\n        --served-model-name llama-3.2 \\\n        --enable-auto-tool-choice \\\n\t--tool-call-parser llama3_json\n\nERROR 04-07 11:42:35 [llama_tool_parser.py:101] Error in extracting tool call from response.\nERROR 04-07 11:42:35 [llama_tool_parser.py:101] Traceback (most recent call last):\nERROR 04-07 11:42:35 [llama_tool_parser.py:101]   File \"/home/llm/miniconda3/lib/python3.12/site-packages/vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py\", line 78, in extract_tool_calls\nERROR 04-07 11:42:35 [llama_tool_parser.py:101]     (obj, end_idx) = dec.raw_decode(model_output[start_idx:])\nERROR 04-07 11:42:35 [llama_tool_parser.py:101]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-07 11:42:35 [llama_tool_parser.py:101]   File \"/home/llm/miniconda3/lib/python3.12/json/decoder.py\", line 356, in raw_decode\nERROR 04-07 11:42:35 [llama_tool_parser.py:101]     raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nERROR 04-07 11:42:35 [llama_tool_parser.py:101] json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n#Test program output\n$ ./test.py \nTraceback (most recent call last):\n  File \"/home/llm/mcp/./test.py\", line 45, in <module>\n    asyncio.run(main())\n  File \"/home/llm/miniconda3/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/home/llm/miniconda3/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/miniconda3/lib/python3.12/asyncio/base_events.py\", line 686, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/home/llm/mcp/./test.py\", line 22, in main\n    tool_calls = llm.get_tool_calls_from_response(resp,\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/miniconda3/lib/python3.12/site-packages/llama_index/llms/openai/base.py\", line 967, in get_tool_calls_from_response\n    raise ValueError(\nValueError: Expected at least one tool call, but got 0 tool calls.\n```\n\nBy comparison, this is the output when vllm is started with\n--model meta-llama/Llama-3.2-3B-Instruct\n```\n$ ./test.py \nresp.additional_kwargs={'prompt_tokens': 299, 'completion_tokens': 23, 'total_tokens': 322}\ntool_calls=[ToolSelection(tool_id='chatcmpl-tool-39f4577381ef45348925369065c7538e', tool_name='multiply', tool_kwargs={'a': '6554', 'b': '933'})]\nr.additional_kwargs={}\ntool_calls=[ToolSelection(tool_id='chatcmpl-tool-7fde540c597b428dbcf7825b6b8c3e3a', tool_name='multiply', tool_kwargs={'a': '6554', 'b': '933'})]\n```\n\nIt works fine. The only change is the model is now in the native huggingface format. The test program is:\n```\n#!/bin/env python3\n\nfrom llama_index.llms.openai_like import OpenAILike\nfrom llama_index.core.tools import FunctionTool\n\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiple two integers and returns the result integer\"\"\"\n    return(a * b)\n\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)\ntools = [multiply_tool]\n\nllm = OpenAILike(model=\"llama-3.2\",\n                 api_base=\"http://localhost:8000/v1\",\n                 api_key=\"fake\",\n                 is_chat_model=True,\n                 context_window=102400,\n                 is_function_calling_model=True)\n\nasync def main():\n    resp = llm.chat_with_tools(tools, user_msg=\"what is 6554 * 933?\")\n    tool_calls = llm.get_tool_calls_from_response(resp,\n                                                  error_on_no_tool_calls=True)\n\n    # this is where the raw tool calls should be\n    print(f\"{resp.additional_kwargs=}\")\n\n    # this is how we parsed them\n    print(f\"{tool_calls=}\")\n\n    # we can also do the same, but with streaming, which is what\n    # FunctionAgent does automatically\n    resp_gen = llm.stream_chat_with_tools(tools,\n                                                user_msg=\"what is 6554 * 933?\")\n    for r in resp_gen:\n      pass\n\n    tool_calls = llm.get_tool_calls_from_response(r,\n                                                  error_on_no_tool_calls=True)\n    print(f\"{r.additional_kwargs=}\")\n    print(f\"{tool_calls=}\")\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n```\nTo run the test program, you need to install:\nllama-index-llms-openai-like\nllama-index-core\n\nI know GGUF support is experimental, but I thought passing along a test program with reproduction instructions might be helpful. The LlamaIndex developers supplied the test program to help me pinpoint the issue I originally raised in their Issue tracker. It was only later that I found the issue is simply the GGUF format causing the problem.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-04-07T16:25:20+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16195/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16195"
  },
  {
    "number": 16430,
    "title": "[Bug]: Qwen2.5 assistant output on tool call is empty",
    "body": "### Your current environment\n\nLatest vLLM (dev) and Pydantic AI version\n\n### \ud83d\udc1b Describe the bug\n\nI'm using pydantic ai for agents and tool calling, but I'm not sure what update has broken broken agentic functionality. The tool gets called (yes it does get called) but then it gets called and called again until it's out of context window size. When looking at the traces, Qwen2.5 says nothing after a tool call, and tries to call the tool again. \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-04-10T20:33:39+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16430/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16430"
  },
  {
    "number": 16887,
    "title": "[Bug]: tool_choice: \"required\" does not work for mistral",
    "body": "### Your current environment\n\n<details>\n<summary>When using \"tool_choice\": \"required\" with mistral</summary>\n\n```text\nraise ValueError(\"Only fast tokenizers are supported\")\n```\n\n</details>\n\n### \ud83d\udc1b Describe the bug\n\nI'm using tool calling with mistral-small-24 when I set the `\"tool_choice\": \"auto\"` it works fine, however when I set it to `required` I get the error above, based on my research mistral models do not support `required` they use `any` instead. \n\nSome how we need to adapt the `required` to `any` for mistral model. Currently when I set the `\"tool_choice\": \"any\"` I get a 500 error.\n\n```\nINFO:     10.89.212.1:44926 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\n```\n\nhttps://docs.mistral.ai/capabilities/function_calling/\n\nMy docker command flags config:\n\n```\n      --model stelterlab/Mistral-Small-24B-Instruct-2501-AWQ\n      --max-model-len 32768\n      --task generate\n      --tensor-parallel-size 2\n      --tool-call-parser mistral \n      --enable-auto-tool-choice\n      --tokenizer-mode mistral\n      --served-model-name mistral-small-24b\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tool-calling"
    ],
    "state": "closed",
    "created_at": "2025-04-20T11:50:28+00:00",
    "closed_at": "2025-05-13T06:01:32+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16887/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16887"
  },
  {
    "number": 6631,
    "title": "[Bug]: Is vllm support function call mode?",
    "body": "### Your current environment\r\n\r\nDevice: Nvidia GeForce 4090\r\nsoftware: vllm 0.5.2 + openai 1.30.5 + transformes 4.42.4\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI use OpenAI api and vllm to deploy local Qwen2 llm, But vllm function call mode does not work. The OpenAI interface correctly passed the tools info parameters to vllm, but vllm did not use it.  If I enable ' tool_choice=\"auto\" 'parameter, I will encounter with 400 error code.\r\n\r\n---------------------------------------------------------------------server script-------------------------------------------------------------\r\npython entrypoints/openai/api_server.py --model=\"xxx/Qwen2-1.5B-Instruct\" --trust-remote-code --host \"localhost\" --port 8000 --dtype auto\r\n-------------------------------------------------------------client code ------------------------------------------------------------------\r\n\r\nfrom openai import OpenAI\r\n\r\ntools = [\r\n        {\r\n            \"type\": \"function\",\r\n            \"function\": {\r\n                \"name\": \"get_current_weather\",\r\n                \"description\": \"Get the current weather in a given location\",\r\n                \"parameters\": {\r\n                    \"type\": \"object\",\r\n                    \"properties\": {\r\n                        \"location\": {\r\n                            \"type\": \"string\",\r\n                            \"description\": \"The city and state, e.g. San Francisco, CA\",\r\n                        },\r\n                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\r\n                    },\r\n                    \"required\": [\"location\"],\r\n                },\r\n            },\r\n        }\r\n    ]\r\n\r\nopenai_api_key = \"xxx\"\r\nopenai_api_base = \"http://localhost:8000/v1/\"\r\n\r\nclient = OpenAI(\r\n    api_key=openai_api_key,\r\n    base_url=openai_api_base,\r\n)\r\n\r\nmodels = client.models.list()\r\nmodel = models.data[0].id\r\n\r\nchat_completion = client.chat.completions.create(\r\n    messages=[\r\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n        {\"role\": \"user\", \"content\": \"help me query currnet weather in BeiJing.\"},\r\n    ],\r\n    model=model,\r\n    tools=tools,\r\n    # tool_choice=\"auto\"\r\n)\r\n\r\nprint(\"response: \", chat_completion.choices[0].message)\r\n\r\n--------------------------------------------extra info &&  response ----------------------------------------------\r\n<img width=\"622\" alt=\"FE8C31A8-0860-4f52-9BA1-BAB51CBF4B72\" src=\"https://github.com/user-attachments/assets/b3256385-ff9b-426b-8bbf-8501243d5198\">\r\n\r\nresponse:  ChatCompletionMessage(content='xxx', role='assistant', function_call=None, tool_calls=[])\r\n\r\n---------------------------------- enable  tool_choice=\"auto\" parameter ----------------------------------------\r\nopenai.BadRequestError: Error code: 400 - {'object': 'error', 'message': \"[{'type': 'value_error', 'loc': ('body',), 'msg': 'Value error, Currently only named tools are supported.', 'input': {'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'help me query currnet weather in San Francisco.'}], 'model': '/home/zhangfan/deep_learning/Qwen2/examples/sft/Qwen2-1.5B-Instruct', 'tool_choice': 'auto', 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}, 'required': ['location']}}}]}, 'ctx': {'error': ValueError('Currently only named tools are supported.')}}]\", 'type': 'BadRequestError', 'param': None, 'code': 400}\r\n",
    "labels": [
      "bug",
      "unstale",
      "tool-calling"
    ],
    "state": "closed",
    "created_at": "2024-07-22T03:38:50+00:00",
    "closed_at": "2025-06-19T08:20:17+00:00",
    "comments": 33,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6631/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6631"
  },
  {
    "number": 11522,
    "title": "[RFC]: Refactor tool parsers to eliminate coding errors and allow more efficient implementations.",
    "body": "### Motivation.\r\n\r\nCurrently the tool parsers are buggy when used and are quite messy in terms of code, especially in the implementations of `extract_tool_calls_streaming`. Moreover, in the long term, maintaining the entire output string in the chat streaming server and parsing the entire output over and over again for each generated token will become very expensive. This will soon become a performance bottleneck in long tool calls.\r\n\r\nMany of the implemented tool parsers aren't carefully written either in terms of correctness nor in terms of efficiency causing a lot of issues in this repository. A complete refactor of this part of the frontend will be required sooner of later.\r\n\r\nSo now is probably the best opportunity to refactor things before more tool calling support is added.\r\nFrom the architectural perspective, clearly it's should be the tool parser's job to maintain states that it needs, so if they need the complete output, they should maintain them with `delta_text` and `delta_token_ids` in their class instead of relying on the server. On the other hand, this encourages the tool parser to use custom, efficient alternative implementations to generate delta messages based on e.g. a internal JSON parser state without needing to parse the full output string every time anymore.\r\nNo capability is lost. Performance gains are good and this must be done sooner or later anyway.\r\n\r\nNo changes to user interface is required and only a few lines of modification is required for the chat server (Only remove the output caching part). The major refactoring should happen in the tool parsers. However, as stated above, we can just move the string cache into the parsers first and refactor them gradually.\r\n\r\n### Proposed Change.\r\n\r\nThis RFC is actually pretty simple so it barely falls into the criteria of 'Large Changes' but I still propose it here.\r\n\r\n- [ ] Add internal output cache to existing tool parsers and modify tool parser interface.\r\n- [ ] Fix bugs/refactor parts of the parsing logic in existing tool parsers.\r\n- [ ] Add appropriate tests for the tool parsers.\r\n\r\n### Feedback Period.\r\n\r\nThis shouldn't be hard so 1-2 weeks.\r\n\r\n### CC List.\r\n\r\n@mgoin @simon-mo @cedonley @K-Mistele @DarkLight1337 @sydnash\r\n\r\n### Any Other Things.\r\n\r\n_No response_\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2024-12-26T13:47:17+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11522/reactions",
      "total_count": 2,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11522"
  },
  {
    "number": 18412,
    "title": "[Bug]: tool_calls.id is Missing in Streaming Responses (stream=true) but Present in Non-Streaming Responses",
    "body": "\n\n### \ud83d\udc1b Describe the bug\n\n\nWhen making an API call to the chat completions endpoint with tools and stream=true, the tool_calls objects within the streamed chunks (delta.tool_calls) do not include the id field for each tool call. However, when stream=false, the tool_calls.id field is correctly present in the response.\n\nThis inconsistency makes it difficult to uniquely identify and track tool calls when processing streamed responses, which is crucial for many applications that rely on tool usage. The OpenAI API, which vLLM aims to be compatible with, includes the tool_call_id (or equivalent id) in streaming chunks.\n\nSteps to Reproduce:\n\nMake a request with stream=false:\n\nRequest Body:\n\n```\n{\n    \"model\": \"Qwen/Qwen3-14B-AWQ\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the weather like in Boston today?\"\n        }\n    ],\n    \"tools\": [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_current_weather\",\n                \"description\": \"Get the current weather in a given location\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and state, e.g. San Francisco, CA\"\n                        },\n                        \"unit\": {\n                            \"type\": \"string\",\n                            \"enum\": [\n                                \"celsius\",\n                                \"fahrenheit\"\n                            ]\n                        }\n                    },\n                    \"required\": [\n                        \"location\"\n                    ]\n                }\n            }\n        }\n    ],\n    \"tool_choice\": {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\"\n        }\n    },\n    \"stream\": false\n}\n\n```\n\nResponse :\n\n```\n{\n    \"id\": \"chatcmpl-26cfd57cdd4c4574b6cfe2d488d6caf0\",\n    \"object\": \"chat.completion\",\n    \"created\": 1747748271,\n    \"model\": \"default\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"message\": {\n                \"role\": \"assistant\",\n                \"tool_calls\": [\n                    {\n                        \"id\": \"chatcmpl-tool-c562052db72a4e3c94b844712b4f38e1\", // <--- ID IS PRESENT\n                        \"type\": \"function\",\n                        \"function\": {\n                            \"name\": \"get_current_weather\",\n                            \"arguments\": \"{\\\"location\\\": \\\"Boston\\\", \\\"unit\\\": \\\"fahrenheit\\\"}\"\n                        }\n                    }\n                ]\n            },\n            \"finish_reason\": \"stop\"\n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 196,\n        \"total_tokens\": 210,\n        \"completion_tokens\": 14\n    }\n}\n```\n\n\nMake a request with stream=true:\n\nRequest Body:\n\n```\n{\n    \"model\": \"Qwen/Qwen3-14B-AWQ\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the weather like in Boston today?\"\n        }\n    ],\n    \"tools\": [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_current_weather\",\n                \"description\": \"Get the current weather in a given location\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and state, e.g. San Francisco, CA\"\n                        },\n                        \"unit\": {\n                            \"type\": \"string\",\n                            \"enum\": [\n                                \"celsius\",\n                                \"fahrenheit\"\n                            ]\n                        }\n                    },\n                    \"required\": [\n                        \"location\"\n                    ]\n                }\n            }\n        }\n    ],\n    \"tool_choice\": {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\"\n        }\n    },\n    \"stream\": true\n}\n```\n\n\nResponse Chunks:\n\n```\ndata: {\"id\":\"chatcmpl-f2a93065f04041d08a6af9fb23cf22d6\",\"object\":\"chat.completion.chunk\",\"created\":1747748383,\"model\":\"default\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-f2a93065f04041d08a6af9fb23cf22d6\",\"object\":\"chat.completion.chunk\",\"created\":1747748383,\"model\":\"default\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"{\\\"\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-f2a93065f04041d08a6af9fb23cf22d6\",\"object\":\"chat.completion.chunk\",\"created\":1747748383,\"model\":\"default\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"location\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-f2a93065f04041d08a6af9fb23cf22d6\",\"object\":\"chat.completion.chunk\",\"created\":1747748383,\"model\":\"default\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"\\\":\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-f2a93065f04041d08a6af9fb23cf22d6\",\"object\":\"chat.completion.chunk\",\"created\":1747748383,\"model\":\"default\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\" \\\"\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-f2a93065f04041d08a6af9fb23cf22d6\",\"object\":\"chat.completion.chunk\",\"created\":1747748383,\"model\":\"default\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"Boston\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-f2a93065f04041d08a6af9fb23cf22d6\",\"object\":\"chat.completion.chunk\",\"created\":1747748383,\"model\":\"default\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"\\\",\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-f2a93065f04041d08a6af9fb23cf22d6\",\"object\":\"chat.completion.chunk\",\"created\":1747748383,\"model\":\"default\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\" \\\"\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-f2a93065f04041d08a6af9fb23cf22d6\",\"object\":\"chat.completion.chunk\",\"created\":1747748383,\"model\":\"default\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"unit\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-f2a93065f04041d08a6af9fb23cf22d6\",\"object\":\"chat.completion.chunk\",\"created\":1747748383,\"model\":\"default\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"\\\":\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-f2a93065f04041d08a6af9fb23cf22d6\",\"object\":\"chat.completion.chunk\",\"created\":1747748383,\"model\":\"default\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\" \\\"\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-f2a93065f04041d08a6af9fb23cf22d6\",\"object\":\"chat.completion.chunk\",\"created\":1747748383,\"model\":\"default\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"f\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-f2a93065f04041d08a6af9fb23cf22d6\",\"object\":\"chat.completion.chunk\",\"created\":1747748383,\"model\":\"default\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"ahrenheit\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-f2a93065f04041d08a6af9fb23cf22d6\",\"object\":\"chat.completion.chunk\",\"created\":1747748383,\"model\":\"default\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"\\\"}\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-f2a93065f04041d08a6af9fb23cf22d6\",\"object\":\"chat.completion.chunk\",\"created\":1747748383,\"model\":\"default\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"name\":\"get_current_weather\",\"arguments\":\"\"}}]},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null}]}\n\ndata: [DONE]\n```\n\n\nNotice that in the chunks containing `delta.tool_calls`, the `id` field for the tool call is missing.\n\nExpected Behavior:\nWhen `stream=true`, streamed chunks containing `delta.tool_calls` should include an `id` field for each tool call. This `id` must be present in the first chunk that introduces that specific tool call (identified by its `index`), and may also be present in all subsequent chunks pertaining to that same tool call. This behavior is necessary for consistency with the non-streaming response and for OpenAI API compatibility.\n\nFor example, a chunk might look like:\n\n`data: {\"id\":\"chatcmpl-xxxxxxxx\",\"object\":\"chat.completion.chunk\",\"created\":1747748383,\"model\":\"default\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"index\":0, \"id\": \"call_xxxxxxxxxxxx\", \"type\": \"function\", \"function\":{\"name\":\"get_current_weather\",\"arguments\":\"{\"\"}}]},\"logprobs\":null,\"finish_reason\":null}]}`\n\nThe `id` for a specific tool call must be provided in its introductory chunk. If it is also included in subsequent chunks for that same tool call, its value must remain consistent.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-05-20T14:02:04+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18412/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/18412"
  },
  {
    "number": 11700,
    "title": "[Feature]: The tool_choice option required is not yet supported but on the roadmap.",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\ntool calling \u7684tool_call\u5b57\u6bb5\uff0c\u5e0c\u671b\u53ef\u4ee5\u652f\u6301required\u7c7b\u578b\uff0c\u8c22\u8c22\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "unstale",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-01-03T01:49:27+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11700/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11700"
  },
  {
    "number": 13671,
    "title": "[Bug]:vLLM 0.6.3 generate_sequences Randomly Hangs After 1-2 Steps When trying to Implement Tool Calling with Logits Processors",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nPyTorch version: 2.4.0+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.5 LTS (x86_64)\nGCC version: (GCC) 12.2.0\nClang version: 3.8.0 (tags/RELEASE_380/final)\nCMake version: Could not collect\nLibc version: glibc-2.31\n\nPython version: 3.11.3 (main, Apr  5 2023, 14:15:06) [GCC 9.4.0] (64-bit runtime)\nPython platform: Linux-5.10.0-2.0.0.2-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: 12.4.99\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: CF-NG-HZZ1-O\nGPU 1: CF-NG-HZZ1-O\nGPU 2: CF-NG-HZZ1-O\nGPU 3: CF-NG-HZZ1-O\nGPU 4: CF-NG-HZZ1-O\nGPU 5: CF-NG-HZZ1-O\nGPU 6: CF-NG-HZZ1-O\nGPU 7: CF-NG-HZZ1-O\n\nNvidia driver version: 535.183.06\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nByte Order:                      Little Endian\nAddress sizes:                   52 bits physical, 57 bits virtual\nCPU(s):                          192\nOn-line CPU(s) list:             0-191\nThread(s) per core:              2\nCore(s) per socket:              48\nSocket(s):                       2\nNUMA node(s):                    2\nVendor ID:                       GenuineIntel\nCPU family:                      6\nModel:                           143\nModel name:                      Intel(R) Xeon(R) Platinum 8468V\nStepping:                        8\nCPU MHz:                         2900.000\nCPU max MHz:                     3800.0000\nCPU min MHz:                     800.0000\nBogoMIPS:                        4800.00\nVirtualization:                  VT-x\nL1d cache:                       4.5 MiB\nL1i cache:                       3 MiB\nL2 cache:                        192 MiB\nL3 cache:                        195 MiB\nNUMA node0 CPU(s):               0-47,96-143\nNUMA node1 CPU(s):               48-95,144-191\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Spec store bypass: Vulnerable\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hfi avx512vbmi umip pku waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.1.3.1\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.0.2.54\n[pip3] nvidia-curand-cu12==10.3.2.106\n[pip3] nvidia-cusolver-cu12==11.4.5.107\n[pip3] nvidia-cusparse-cu12==12.1.0.106\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.20.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.1.105\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.4.0\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.19.0\n[pip3] transformers==4.47.1\n[pip3] triton==3.0.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: N/A (dev)\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    48-95,144-191   1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    48-95,144-191   1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     48-95,144-191   1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    48-95,144-191   1               N/A\nNIC0    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC1    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC2    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC3    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      NODE    NODE    SYS     SYS     SYS     SYS\nNIC4    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE     X      NODE    SYS     SYS     SYS     SYS\nNIC5    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE     X      SYS     SYS     SYS     SYS\nNIC6    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS      X      NODE    NODE    NODE\nNIC7    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE     X      NODE    NODE\nNIC8    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE     X      NODE\nNIC9    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n\nNVIDIA_VISIBLE_DEVICES=GPU-00c17004-1a68-8b2e-bf1f-ce4a849177c9,GPU-50c603ba-d9c5-5c24-dfa1-610ef45f5dfe,GPU-38cdf56a-4962-87b6-f54e-3c591c3b6f94,GPU-e667f094-50f7-6a86-0173-98bc170da5d4,GPU-2fe323cd-e3ae-7915-1299-542a62e79926,GPU-f2c15f28-fdbf-499d-4d81-26545c14c0fd,GPU-89fd4b88-9a9e-7e89-b4fd-820425524007,GPU-b6221a29-7a6f-d2e5-fc50-2806885da539\nNCCL_P2P_DISABLE=0\nNVIDIA_REQUIRE_CUDA=cuda>=12.0 brand=tesla,driver>=450,driver<451 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=510,driver<511 brand=unknown,driver>=510,driver<511 brand=nvidia,driver>=510,driver<511 brand=nvidiartx,driver>=510,driver<511 brand=geforce,driver>=510,driver<511 brand=geforcertx,driver>=510,driver<511 brand=quadro,driver>=510,driver<511 brand=quadrortx,driver>=510,driver<511 brand=titan,driver>=510,driver<511 brand=titanrtx,driver>=510,driver<511 brand=tesla,driver>=515,driver<516 brand=unknown,driver>=515,driver<516 brand=nvidia,driver>=515,driver<516 brand=nvidiartx,driver>=515,driver<516 brand=geforce,driver>=515,driver<516 brand=geforcertx,driver>=515,driver<516 brand=quadro,driver>=515,driver<516 brand=quadrortx,driver>=515,driver<516 brand=titan,driver>=515,driver<516 brand=titanrtx,driver>=515,driver<516\nNCCL_IB_CUDA_SUPPORT=0\nNVIDIA_LIB=/usr/local/nvidia/lib64\nNCCL_VERSION=2.17.1-1\nNCCL_SOCKET_IFNAME=xgbe0\nNCCL_DEBUG_SUBSYS=INIT,ENV,GRAPH\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNCCL_DEBUG=INFO\nNVIDIA_PRODUCT_NAME=CUDA\nNCCL_IB_GID_INDEX=3\nCUDA_VERSION=12.0.1\nNVIDIA_TOOLS=/home/opt/cuda_tools\nNCCL_DEBUG_FILE=/root/workspace/log/nccl.%h.%p.log\nNCCL_IB_QPS_PER_CONNECTION=2\nNCCL_IB_CONNECT_RETRY_CNT=15\nNCCL_ERROR_FILE=/root/workspace/log/err.%h.%p.log\nNCCL_IB_TIMEOUT=22\nCUDNN_VERSION=8.9.1\nLD_LIBRARY_PATH=/root/venv/lib/python3.11/site-packages/cv2/../../lib64:/usr/local/lib:/usr/local/x86_64-pc-linux-gnu/lib:/home/opt/nvidia_lib:/usr/local/cuda/lib64:/usr/lib64:/usr/local/lib:/usr/lib/x86_64-linux-gnu/\nNCCL_IB_DISABLE=0\nNCCL_IB_ADAPTIVE_ROUTING=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI've tried the method of [vLLMRollout.generate_sequences to implement tool calling](https://github.com/volcengine/verl/issues/176) with verl 0.2 and vllm 0.6.3,  However, it randomly hangs after running for 1 to 2 steps. Specifically, the GPU utilization gets stuck at 100%, while the power consumption drops significantly low, and the logs stop updating. \n\n\nBelow is the code snippet I used: \n\n```python\nmy_tool_processor = FunctionProcessor(self.tokenizer)\nsampling_params.logits_processors = [my_tool_processor]\n```\n\n```python\nclass FunctionProcessor:\n    def __init__(\n        self,\n        tokenizer,\n        start_tag: str = \"<tool_call>\",\n        end_tag: str = \"</tool_call>\",\n        result_start: str = \"\\n<tool_result>\\n\",\n        result_end: str = \"\\n</tool_result>\\n<think>\"\n    ):\n        self.tokenizer = tokenizer\n        self.buffer = []\n        self.in_function = False\n        self.current_function = []\n        \n        # Pre-tokenize markers \n        self.start_marker = tokenizer.encode(start_tag, add_special_tokens=False)[0]\n        self.end_marker = tokenizer.encode(end_tag, add_special_tokens=False)[0]\n        self.result_start = tokenizer.encode(result_start, add_special_tokens=False)\n        self.result_end = tokenizer.encode(result_end, add_special_tokens=False)\n\n        self.result_tokens = []\n        self.state_dict = {}\n    \n    \n    def evaluate_expression(self, expr: str) -> str:\n        try:\n            # get_tool_resp is the function that will be called to evaluate the expression, time cost no more than 3 seconds.\n            result = get_tool_resp(expr)\n\n            return str(result)\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n        \n\n    \n    def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:\n        try:\n            if input_ids[-1] == self.end_marker:\n                idx = 1\n                while idx <= len(input_ids):\n\n                    if input_ids[-idx] == self.start_marker:\n\n                        if input_ids[-idx:].count(self.start_marker) > 1 or input_ids[-idx:].count(self.end_marker) > 1:\n                            break\n                        \n                        current_function = input_ids[-idx:]\n                        func_text = self.tokenizer.decode(current_function)\n                        try:\n                            result = self.evaluate_expression(func_text)\n                        except:\n                            result = \"{'result': 'Tool Call Error'}\"\n                        result_tokens = list(reversed(\n                            self.result_start +\n                            self.tokenizer.encode(str(result)) +\n                            self.result_end\n                        ))\n                        state_dict_key = tuple(input_ids)\n                        \n                        self.state_dict[state_dict_key] = result_tokens\n                        token_id = self.state_dict[state_dict_key].pop()\n                        scores[token_id] = 100\n                        break\n                        \n                    idx += 1\n            else:\n                for idx in range(1, len(self.end_marker)):\n                    if input_ids[-idx] == self.start_marker:\n                        state_dict_key = tuple(input_ids[:-idx + 1])\n                        result_tokens = self.state_dict.get(state_dict_key, [])\n                        if result_tokens:\n                            self.state_dict[state_dict_key] = result_tokens\n                            token_id = self.state_dict[state_dict_key].pop()\n                            scores[token_id] = 100\n\n                        break\n            \n        except Exception as e:\n            print(f\"Error in FunctionProcessor: {e}\")\n        return scores\n```\n\n<img width=\"464\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7fe74c6c-21f3-41e8-ae67-7bec94cd1f10\" />\n\n<img width=\"1226\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/004652eb-7237-4d03-b977-570f092b0b2a\" />\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-02-21T13:36:09+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13671/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13671"
  },
  {
    "number": 11392,
    "title": "[Bug]: [v0.6.5] Streaming tool call responses with the hermes template is inconsistent with the non-stream version.",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>Environment</summary>\r\n\r\n```text\r\nPyTorch version: 2.5.1\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nPython version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-4.18.0-513.24.1.el8_9.x86_64-x86_64-with-glibc2.28\r\nIs CUDA available: True\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1\r\n[pip3] torchaudio==2.5.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.46.2\r\n[pip3] triton==3.1.0\r\n[conda] blas                      1.0                         mkl    defaults\r\n[conda] cuda-cudart               12.1.105                      0    nvidia\r\n[conda] cuda-cupti                12.1.105                      0    nvidia\r\n[conda] cuda-libraries            12.1.0                        0    nvidia\r\n[conda] cuda-nvrtc                12.1.105                      0    nvidia\r\n[conda] cuda-nvtx                 12.1.105                      0    nvidia\r\n[conda] cuda-opencl               12.6.77                       0    nvidia\r\n[conda] cuda-runtime              12.1.0                        0    nvidia\r\n[conda] cuda-version              12.6                          3    nvidia\r\n[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\r\n[conda] libcublas                 12.1.0.26                     0    nvidia\r\n[conda] libcufft                  11.0.2.4                      0    nvidia\r\n[conda] libcufile                 1.11.1.6                      0    nvidia\r\n[conda] libcurand                 10.3.7.77                     0    nvidia\r\n[conda] libcusolver               11.4.4.55                     0    nvidia\r\n[conda] libcusparse               12.0.2.55                     0    nvidia\r\n[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\r\n[conda] libnpp                    12.0.2.50                     0    nvidia\r\n[conda] libnvjitlink              12.1.105                      0    nvidia\r\n[conda] libnvjpeg                 12.1.1.14                     0    nvidia\r\n[conda] mkl                       2023.1.0         h213fc3f_46344    defaults\r\n[conda] mkl-service               2.4.0           py312h5eee18b_1    defaults\r\n[conda] mkl_fft                   1.3.11          py312h5eee18b_0    defaults\r\n[conda] mkl_random                1.2.8           py312h526ad5a_0    defaults\r\n[conda] numpy                     1.26.4          py312hc5e2394_0    defaults\r\n[conda] numpy-base                1.26.4          py312h0da6c21_0    defaults\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] pytorch                   2.5.1           py3.12_cuda12.1_cudnn9.1.0_0    pytorch\r\n[conda] pytorch-cuda              12.1                 ha16c6d3_6    pytorch\r\n[conda] pytorch-mutex             1.0                        cuda    pytorch\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torchaudio                2.5.1               py312_cu121    pytorch\r\n[conda] torchtriton               3.1.0                     py312    pytorch\r\n[conda] torchvision               0.20.1              py312_cu121    pytorch\r\n[conda] transformers              4.46.2                   pypi_0    pypi\r\n\r\nvLLM Version: 0.6.5\r\nvLLM Build Flags:\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nLD_LIBRARY_PATH=/home/rocky/miniconda3/envs/llm/lib/python3.12/site-packages/cv2/../../lib64:\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nThe streaming feature combined with tool calling is still quite bugged.\r\n\r\nIf we POST the following tool call generation request to vllm endpoint /v1/chat/completions:\r\n```json\r\n{\r\n  \"model\": \"Qwen/Qwen2.5-7B-Instruct\",\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"system\",\r\n      \"content\": \"# Instructions\\n\\nYou are an AI assistant integrated into Theia IDE, designed to assist software developers with concise answers to programming-related questions. Your goal is to enhance\\nproductivity with quick, relevant solutions, explanations, and best practices. Keep responses short, delivering valuable insights and direct solutions.\\n\\nUse the following functions to interact with the workspace files as needed:\\n- **You can call function: getWorkspaceDirectoryStructure(): Retrieve the complete directory structure of the workspace, listing only directories (no file contents). This structure excludes specific directories,\\n            such as node_modules and hidden files, ensuring paths are within workspace boundaries.**: Returns the complete directory structure.\\n- **You can call function: getWorkspaceFileList(path: string): List files and directories within a specified workspace directory. Paths are relative to the workspace root, and only workspace-contained paths are\\n             allowed. If no path is provided, the root contents are listed. Paths outside the workspace will result in an error.**: Lists files and directories in a specific directory.\\n- **You can call function: getFileContent(file: string): The relative path to the target file within the workspace. This path is resolved from the workspace root, and only files within the workspace boundaries\\n             are accessible. Attempting to access paths outside the workspace will result in an error.**: Retrieves the content of a specific file.\\n\\n### Workspace Navigation Guidelines\\n\\n1. **Start at the Root**: For general questions (e.g., \\\"How to build the project\\\"), check root-level documentation files or setup files before browsing subdirectories.\\n2. **Confirm Paths**: Always verify paths by listing directories or files as you navigate. Avoid assumptions based on user input alone.\\n3. **Navigate Step-by-Step**: Move into subdirectories only as needed, confirming each directory level.\\n\\n### Response Guidelines\\n\\n1. **Contextual Focus**: Provide answers relevant to the workspace, avoiding general advice. Use provided functions without assuming file structure or content.\\n2. **Clear Solutions**: Offer direct answers and concise explanations. Link to official documentation as needed.\\n3. **Tool & Language Adaptability**: Adjust guidance based on the programming language, framework, or tool specified by the developer.\\n4. **Supportive Tone**: Maintain a friendly, professional tone with clear, accurate technical language.\\n5. **Stay Relevant**: Limit responses to software development, frameworks, Theia, terminal usage, and related technologies. Decline unrelated questions politely.\\n\"\r\n    },\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"List all the files in my workspace \"\r\n    }\r\n  ],\r\n  \"tools\": [\r\n    {\r\n      \"type\": \"function\",\r\n      \"function\": {\r\n        \"name\": \"getWorkspaceDirectoryStructure\",\r\n        \"description\": \"Retrieve the complete directory structure of the workspace, listing only directories (no file contents). This structure excludes specific directories,\\n            such as node_modules and hidden files, ensuring paths are within workspace boundaries.\"\r\n      }\r\n    },\r\n    {\r\n      \"type\": \"function\",\r\n      \"function\": {\r\n        \"name\": \"getWorkspaceFileList\",\r\n        \"parameters\": {\r\n          \"type\": \"object\",\r\n          \"properties\": {\r\n            \"path\": {\r\n              \"type\": \"string\",\r\n              \"description\": \"Optional relative path to a directory within the workspace. If no path is specified, the function lists contents directly in the workspace\\n                         root. Paths are resolved within workspace boundaries only; paths outside the workspace or unvalidated paths will result in an error.\"\r\n            }\r\n          }\r\n        },\r\n        \"description\": \"List files and directories within a specified workspace directory. Paths are relative to the workspace root, and only workspace-contained paths are\\n             allowed. If no path is provided, the root contents are listed. Paths outside the workspace will result in an error.\"\r\n      }\r\n    },\r\n    {\r\n      \"type\": \"function\",\r\n      \"function\": {\r\n        \"name\": \"getFileContent\",\r\n        \"parameters\": {\r\n          \"type\": \"object\",\r\n          \"properties\": {\r\n            \"file\": {\r\n              \"type\": \"string\",\r\n              \"description\": \"Return the content of a specified file within the workspace. The file path must be provided relative to the workspace root. Only files within\\n                         workspace boundaries are accessible; attempting to access files outside the workspace will return an error.\"\r\n            }\r\n          }\r\n        },\r\n        \"description\": \"The relative path to the target file within the workspace. This path is resolved from the workspace root, and only files within the workspace boundaries\\n             are accessible. Attempting to access paths outside the workspace will result in an error.\"\r\n      }\r\n    }\r\n  ],\r\n  \"tool_choice\": \"auto\",\r\n  \"stream\": true\r\n}\r\n```\r\n\r\nWe get\r\n```json\r\n{\"id\":\"chatcmpl-095ddb0721564d69984d67ed51f23e4d\",\"object\":\"chat.completion.chunk\",\"created\":1734758880,\"model\":\"Qwen/Qwen2.5-7B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}\r\n```\r\n\r\n```json\r\n{\"id\":\"chatcmpl-095ddb0721564d69984d67ed51f23e4d\",\"object\":\"chat.completion.chunk\",\"created\":1734758880,\"model\":\"Qwen/Qwen2.5-7B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"tool_calls\":[{\"id\":\"chatcmpl-tool-6b879898e4bf4d27bf01269db8c5222d\",\"type\":\"function\",\"index\":0,\"function\":{\"name\":\"getWorkspaceFileList\"}}]},\"logprobs\":null,\"finish_reason\":null}]}\r\n```\r\n\r\n```json\r\n{\"id\":\"chatcmpl-095ddb0721564d69984d67ed51f23e4d\",\"object\":\"chat.completion.chunk\",\"created\":1734758880,\"model\":\"Qwen/Qwen2.5-7B-Instruct\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\"},\"logprobs\":null,\"finish_reason\":\"tool_calls\",\"stop_reason\":null}]}\r\n```\r\n\r\nNo arguments are returned. However if we use the none streaming version, we get:\r\n```json\r\n{\r\n  \"id\": \"chatcmpl-ede5dbc93dc44f859fea8fff3f10086b\",\r\n  \"object\": \"chat.completion\",\r\n  \"created\": 1734762064,\r\n  \"model\": \"Qwen/Qwen2.5-7B-Instruct\",\r\n  \"choices\": [\r\n    {\r\n      \"index\": 0,\r\n      \"message\": {\r\n        \"role\": \"assistant\",\r\n        \"content\": null,\r\n        \"tool_calls\": [\r\n          {\r\n            \"id\": \"chatcmpl-tool-dc87e34d9d5445a5bc3a3aa94efbabc0\",\r\n            \"type\": \"function\",\r\n            \"function\": {\r\n              \"name\": \"getWorkspaceFileList\",\r\n              \"arguments\": \"{}\"\r\n            }\r\n          }\r\n        ]\r\n      },\r\n      \"logprobs\": null,\r\n      \"finish_reason\": \"tool_calls\",\r\n      \"stop_reason\": null\r\n    }\r\n  ],\r\n  \"usage\": {\r\n    \"prompt_tokens\": 942,\r\n    \"total_tokens\": 960,\r\n    \"completion_tokens\": 18,\r\n    \"prompt_tokens_details\": null\r\n  },\r\n  \"prompt_logprobs\": null\r\n}\r\n```\r\n\r\nThe arguments disappeared from the streaming response.\r\n\r\n### Before submitting a new issue...\r\n\r\nProbably related to #11279 , I provide a easy example here to check with the Qwen2.5-7B-Instruct model.\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "unstale",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2024-12-21T06:22:59+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11392/reactions",
      "total_count": 2,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11392"
  },
  {
    "number": 13700,
    "title": "[Feature]: add tool calling support for DeepSeek-R1-Distill-Qwen-32B",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently, tool calling and reasoning cannot be enabled both, but DeepSeek-R1-Distill-Qwen-32B seems to support tool calling. https://x.com/thorwebdev/status/1884888068253192662\n\nIs it possible to add support for both tool calling and reasoning?\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-02-22T10:01:59+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13700/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13700"
  },
  {
    "number": 14866,
    "title": "[New Model]: Command A with tool support",
    "body": "### The model to consider.\n\nhttps://huggingface.co/CohereForAI/c4ai-command-a-03-2025\n\n### The closest model vllm already supports.\n\ncommand r\n\n### What's your difficulty of supporting the model you want?\n\nProperly support tokenizer and templates, as well as tool calling on the model\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-03-15T16:42:08+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14866/reactions",
      "total_count": 9,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 8,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14866"
  },
  {
    "number": 12349,
    "title": "[Usage]: how to use tool calling with auto option, setting the tool works",
    "body": "### Your current environment\n\n-\n\n\n### How would you like to use vllm\n\nI am trying to use tool calling to test a qwen model. It works when specified the tool but normal queries don\u2019t work. How to use auto mode? \n\nIf it\u2019s not supported when we can expect this? \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "unstale",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-01-23T08:37:03+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12349/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12349"
  },
  {
    "number": 17514,
    "title": "[Bug]: tool calling error",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-113-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090\nNvidia driver version: 550.78\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      48 bits physical, 48 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             92\nOn-line CPU(s) list:                0-91\nVendor ID:                          AuthenticAMD\nModel name:                         AMD EPYC 7542 32-Core Processor\nCPU family:                         23\nModel:                              49\nThread(s) per core:                 2\nCore(s) per socket:                 23\nSocket(s):                          2\nStepping:                           0\nBogoMIPS:                           5799.99\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm rep_good nopl cpuid extd_apicid amd_dcm tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr wbnoinvd virt_ssbd arat umip rdpid arch_capabilities\nHypervisor vendor:                  KVM\nVirtualization type:                full\nL1d cache:                          2.9 MiB (46 instances)\nL1i cache:                          2.9 MiB (46 instances)\nL2 cache:                           23 MiB (46 instances)\nL3 cache:                           192 MiB (12 instances)\nNUMA node(s):                       8\nNUMA node0 CPU(s):                  0-7,64-71\nNUMA node1 CPU(s):                  8-15,72-79\nNUMA node2 CPU(s):                  16-23,80-87\nNUMA node3 CPU(s):                  24-31,88-91\nNUMA node4 CPU(s):                  32-39\nNUMA node5 CPU(s):                  40-47\nNUMA node6 CPU(s):                  48-55\nNUMA node7 CPU(s):                  56-63\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection\nVulnerability Spec rstack overflow: Mitigation; safe RET\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.2.post1+cu124torch2.6\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.50.3\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n\tGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \t0-91\t0-7\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNVIDIA_VISIBLE_DEVICES=all\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNCCL_DEBUG=INFO\nCUDA_VERSION=12.4.1\nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nstart vllm openai server:\n```\nexport VLLM_ATTENTION_BACKEND=FLASH_ATTN && vllm serve /models/Qwen2.5-7B-Instruct --served-model-name helium --disable-log-requests --gpu-memory-utilization 0.95 --max-model-len 16384 --pipeline-parallel-size 1  --enable-prefix-caching --enable-chunked-prefill --port 8000 --tensor-parallel-size 1 --enable-auto-tool-choice --tool-call-parser hermes\n```\n\nsend request with tool calling.\n\nI got following error:\n\n```\nINFO 05-01 01:51:54 [chat_utils.py:379] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\nERROR 05-01 01:51:54 [serving_chat.py:898] Error in chat_completion_full_generator - cannot determine if tools should be extracted. Returning a standard chat completion.\nINFO:     114.242.33.191:32107 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nERROR 05-01 01:51:54 [serving_chat.py:898] Error in chat_completion_full_generator - cannot determine if tools should be extracted. Returning a standard chat completion.\nINFO:     114.242.33.191:27283 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nERROR 05-01 01:51:54 [serving_chat.py:898] Error in chat_completion_full_generator - cannot determine if tools should be extracted. Returning a standard chat completion.\nERROR 05-01 01:51:54 [serving_chat.py:898] Error in chat_completion_full_generator - cannot determine if tools should be extracted. Returning a standard chat completion.\nINFO:     114.242.33.191:27284 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nINFO:     114.242.33.191:32108 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nERROR 05-01 01:51:54 [serving_chat.py:898] Error in chat_completion_full_generator - cannot determine if tools should be extracted. Returning a standard chat completion.\nINFO:     114.242.33.191:32109 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nINFO:     114.242.33.191:32109 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nINFO:     114.242.33.191:32108 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nINFO:     114.242.33.191:32109 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nINFO:     114.242.33.191:27283 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nINFO:     114.242.33.191:32108 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nINFO:     114.242.33.191:27284 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nINFO:     114.242.33.191:32109 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nINFO:     114.242.33.191:27283 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nINFO 05-01 01:52:01 [loggers.py:80] Avg prompt throughput: 1333.3 tokens/s, Avg generation throughput: 94.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 81.3%\nINFO 05-01 01:52:11 [loggers.py:80] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 50.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 81.3%\nERROR 05-01 01:52:15 [hermes_tool_parser.py:110] Error in extracting tool call from response.\nERROR 05-01 01:52:15 [hermes_tool_parser.py:110] Traceback (most recent call last):\nERROR 05-01 01:52:15 [hermes_tool_parser.py:110]   File \"/heliumos-env/lib/python3.10/site-packages/vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py\", line 87, in extract_tool_calls\nERROR 05-01 01:52:15 [hermes_tool_parser.py:110]     raw_function_calls = [\nERROR 05-01 01:52:15 [hermes_tool_parser.py:110]   File \"/heliumos-env/lib/python3.10/site-packages/vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py\", line 88, in <listcomp>\nERROR 05-01 01:52:15 [hermes_tool_parser.py:110]     json.loads(match[0] if match[0] else match[1])\nERROR 05-01 01:52:15 [hermes_tool_parser.py:110]   File \"/usr/lib/python3.10/json/__init__.py\", line 346, in loads\nERROR 05-01 01:52:15 [hermes_tool_parser.py:110]     return _default_decoder.decode(s)\nERROR 05-01 01:52:15 [hermes_tool_parser.py:110]   File \"/usr/lib/python3.10/json/decoder.py\", line 337, in decode\nERROR 05-01 01:52:15 [hermes_tool_parser.py:110]     obj, end = self.raw_decode(s, idx=_w(s, 0).end())\nERROR 05-01 01:52:15 [hermes_tool_parser.py:110]   File \"/usr/lib/python3.10/json/decoder.py\", line 353, in raw_decode\nERROR 05-01 01:52:15 [hermes_tool_parser.py:110]     obj, end = self.scan_once(s, idx)\nERROR 05-01 01:52:15 [hermes_tool_parser.py:110] json.decoder.JSONDecodeError: Unterminated string starting at: line 2 column 10 (char 10)\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-05-01T01:58:13+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17514/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17514"
  },
  {
    "number": 14470,
    "title": "[Bug]: pythonic tool parser only accepts alphabetical tool names",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nCollecting environment information...\nPyTorch version: 2.7.0.dev20250221+rocm6.3\nIs debug build: False\nCUDA used to build PyTorch: N/A\nROCM used to build PyTorch: 6.3.42131-fa1d09cbd\n\nOS: Ubuntu 24.04.2 LTS (x86_64)\nGCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\nClang version: Could not collect\nCMake version: version 3.31.4\nLibc version: glibc-2.39\n\nPython version: 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-6.11.0-17-generic-x86_64-with-glibc2.39\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: AMD Instinct MI100 (gfx908:sramecc+:xnack-)\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\nHIP runtime version: 6.3.42131\nMIOpen runtime version: 3.3.0\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        39 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               4\nOn-line CPU(s) list:                  0-3\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Core(TM) i5-6600K CPU @ 3.50GHz\nCPU family:                           6\nModel:                                94\nThread(s) per core:                   1\nCore(s) per socket:                   4\nSocket(s):                            1\nStepping:                             3\nCPU(s) scaling MHz:                   100%\nCPU max MHz:                          3900.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             6999.82\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault pti ssbd ibrs ibpb stibp tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp vnmi md_clear flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            128 KiB (4 instances)\nL1i cache:                            128 KiB (4 instances)\nL2 cache:                             1 MiB (4 instances)\nL3 cache:                             6 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-3\nVulnerability Gather data sampling:   Vulnerable: No microcode\nVulnerability Itlb multihit:          KVM: Mitigation: VMX disabled\nVulnerability L1tf:                   Mitigation; PTE Inversion; VMX conditional cache flushes, SMT disabled\nVulnerability Mds:                    Mitigation; Clear CPU buffers; SMT disabled\nVulnerability Meltdown:               Mitigation; PTI\nVulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT disabled\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; IBRS\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; IBRS; IBPB conditional; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Mitigation; Microcode\nVulnerability Tsx async abort:        Mitigation; TSX disabled\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-ml-py==12.570.86\n[pip3] pynvml==12.0.0\n[pip3] pytorch-triton-rocm==3.2.0+git4b3bb1f8\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.7.0.dev20250221+rocm6.3\n[pip3] torchaudio==2.6.0.dev20250221+rocm6.3\n[pip3] torchvision==0.22.0.dev20250221+rocm6.3\n[pip3] transformers==4.49.0\n[pip3] triton==3.2.0+gite5be006a\n[conda] Could not collect\nROCM Version: 6.3.42134-a9a80e791\nNeuron SDK Version: N/A\nvLLM Version: 0.7.4.dev4+gbfbc0b32\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n============================ ROCm System Management Interface ============================\n================================ Weight between two GPUs =================================\n       GPU0\nGPU0   0\n\n================================= Hops between two GPUs ==================================\n       GPU0\nGPU0   0\n\n=============================== Link Type between two GPUs ===============================\n       GPU0\nGPU0   0\n\n======================================= Numa Nodes =======================================\nGPU[0]          : (Topology) Numa Node: 0\nGPU[0]          : (Topology) Numa Affinity: -1\n================================== End of ROCm SMI Log ===================================\n\nLD_LIBRARY_PATH=/home/bjj/vllm-rocm/lib/python3.12/site-packages/cv2/../../lib64:\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nThe `PythonicToolParser` `TOOL_CALL_REGEX` only supports alphanumeric tool names and parameter names. There are a couple of issues with this:\n\n1. `snake_case` is not supported. This is even used in OpenAI's API example `get_weather`.\n2. `kebab-case` is not supported. This is not valid Python at all, but the API is all specified in terms of JSON, and it's a perfectly valid string. n8n's builtin Wikipedia tool is named `wikipedia-api`\n3. Numbers not supported! Can't believe I didn't see this while staring at that regex\n\nThese issues are different in difficulty:\n1. I think `snake_case `is trivial to fix by changing `TOOL_CALL_REGEX` because `ast.parse` will be happy\n2. I can see a few ways to handle `kebab-case`, but none of them are ideal. The easiest thing seems to be to accept invalid Python with the regex, use the regex to remap to valid python variable names, then map back after parsing. All of the other things I can think of (like letting instances of `TooLParser` remap names) leak info to the model. There's still a risk that pythonic models are reluctant to make invalid name function calls (but I have been hitting the exact opposite!)\n3. Regex change\n\nA good example of a tool calling model that strongly prefers python is watt-tool-8B or 70B (8B is based on llama3.1, but it still wants system message tools and pythonic output). It's a strong contender on the BFCL.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-03-08T03:10:17+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14470/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14470"
  },
  {
    "number": 7912,
    "title": "[Usage]: Confirm tool calling is not supported and this is the closest thing can be done",
    "body": "Hi.\r\n\r\nLLM -> Llama-3.1-8B-Instruct\r\n\r\nIn the vllm docs, it is said that:\r\n\r\n> Tool calling in the chat completion API\r\n> \r\n> vLLM supports only named function calling in the chat completion API. The tool_choice options auto and required are not yet supported but on the roadmap.\r\n> \r\n> To use a named function you need to define the function in the tools parameter and call it in the tool_choice parameter.\r\n> \r\n> It is the callers responsibility to prompt the model with the tool information, vLLM will not automatically manipulate the prompt. This may change in the future.\r\n> \r\n> vLLM will use guided decoding to ensure the response matches the tool parameter object defined by the JSON schema in the tools parameter.\r\n> \r\n> Please refer to the OpenAI API reference documentation for more information.\r\n\r\n1. Can we confirm that this still holds? I see bunch of related PRs and good progress, so I'd like to be sure.\r\n2. Since tool calling without named functions does not work, we can't use libraries/frameworks for Agentic AI such as AutoGen. Correct?\r\n\r\nFor example, when this code is run (from AutoGen docs):\r\n\r\n```\r\nimport os\r\nfrom autogen import UserProxyAgent, ConversableAgent\r\nfrom typing import Annotated, Literal\r\n\r\nOperator = Literal[\"+\", \"-\", \"*\", \"/\"]\r\n\r\ndef calculator(a: int, b: int, operator: Annotated[Operator, \"operator\"]) -> int:\r\n    if operator == \"+\":\r\n        return a + b\r\n    elif operator == \"-\":\r\n        return a - b\r\n    elif operator == \"*\":\r\n        return a * b\r\n    elif operator == \"/\":\r\n        return int(a / b)\r\n    else:\r\n        raise ValueError(\"Invalid operator\")\r\n\r\n# Let's first define the assistant agent that suggests tool calls.\r\nassistant = ConversableAgent(\r\n    name=\"Assistant\",\r\n    system_message=\"You are a helpful AI assistant. \"\r\n    \"You can help with simple calculations. \"\r\n    \"Return 'TERMINATE' when the task is done.\",\r\n    llm_config={\r\n        \"config_list\": [\r\n            {\r\n                \"model\": \"<YOUR MODEL NAME>\",\r\n                \"api_key\": \"<AP\u0130_KEY>\",\r\n                \"base_url\": \"<BASE_URL_FOR_LOCAL_LLM>\"\r\n            }\r\n        ]\r\n    }\r\n)\r\n\r\n\r\n# The user proxy agent is used for interacting with the assistant agent\r\n# and executes tool calls.\r\nuser_proxy = ConversableAgent(\r\n    name=\"User\",\r\n    llm_config=False,\r\n    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\r\n    human_input_mode=\"NEVER\",\r\n)\r\n\r\n# Register the tool signature with the assistant agent.\r\nassistant.register_for_llm(name=\"calculator\", description=\"A simple calculator\")(calculator)\r\n\r\n# Register the tool function with the user proxy agent.\r\nuser_proxy.register_for_execution(name=\"calculator\")(calculator)\r\n\r\nchat_result = user_proxy.initiate_chat(assistant, message=\"What is (44232 + 13312 / (232 - 32)) * 5?\")\r\n```\r\nit is supposed to produce the following which actually comprises executing the function: (I'm showing just a part of it):\r\n\r\n```\r\n>>>>>>>> USING AUTO REPLY...\r\nAssistant (to User):\r\n\r\nI apologize for the confusion, I seem to have made a mistake. Let me recalculate the expression properly.\r\n\r\nFirst, we need to do the calculations within the brackets. So, calculating (1423 - 123), (32 + 23), and then performing remaining operations.\r\n***** Suggested tool call (call_mx3M3fNOwikFNoqSojDH1jIr): calculator *****\r\nArguments: \r\n{\r\n    \"input\": {\r\n        \"a\": 1423,\r\n        \"b\": 123,\r\n        \"operator\": \"-\"\r\n    }\r\n}\r\n***************************************************************************\r\n\r\n--------------------------------------------------------------------------------\r\n\r\n>>>>>>>> EXECUTING FUNCTION calculator...\r\nUser (to Assistant):\r\n\r\nUser (to Assistant):\r\n\r\n***** Response from calling tool (call_mx3M3fNOwikFNoqSojDH1jIr) *****\r\n1300\r\n**********************************************************************\r\n```\r\n\r\nBut when I run it with my local LLM with vllm backend, it does not execute the function, it replies normally instead: (again, just a part of it)\r\n```\r\n\r\n>>>>>>>> USING AUTO REPLY...\r\nAssistant (to User):\r\n\r\n<|python_tag|>{\"name\": \"calculator\", \"parameters\": {\"a\": 43998.56, \"b\": 5, \"operator\": \"*\"}}\r\n\r\n--------------------------------------------------------------------------------\r\nUser (to Assistant):\r\n\r\n\r\n\r\n--------------------------------------------------------------------------------\r\n\r\n>>>>>>>> USING AUTO REPLY...\r\nAssistant (to User):\r\n\r\n{\"name\": \"calculator\", \"parameters\": {\"a\": 219994, \"b\": 5, \"operator\": \"*\"}}\r\n```\r\n\r\n3. As you can see, local llm respond starts with  \"<|python_tag|>\" sometimes. Actually most of the times. And this is not about AutoGen. I encountered this behaviour without using any 3rd party framework/lib. And even though I tried my best to hide this token by editing some lines in the config json files (special_tokens etc.), I failed. Any solution to this?\r\n\r\n4. My best attempt to integrate the auto tool calling in vLLM is this:\r\n\r\nI added a \"default function\" in the available tools to llama. It is supposed to call this whenever none of the others is appropriate\r\n```\r\n    {\r\n        \"type\": \"function\",\r\n        \"function\": {\r\n            \"name\": \"default_function\",\r\n            \"description\": \"If none of the other functions is needed, simply call this.\",\r\n            \"parameters\": {\r\n                \"type\": \"object\",\r\n                \"properties\": {\r\n                    \"normal_prompt\": {\r\n                        \"type\": \"string\",\r\n                        \"description\": \"The prompt user has typed.\",\r\n                    }\r\n                },\r\n                \"required\": [\"normal_prompt\"],\r\n            },\r\n        }\r\n    },\r\n```\r\n\r\nAnd here is the heart of the code which does what I want. For now, I don't actually call the function but the response is the function call with the full signature. So, only calling is missing. I just wanted to be sure if this is the best we can do with vLLM right now:\r\n\r\n```\r\ndef send_request_to_llm(chat_history, use_tools=True):\r\n    extra_body = {\r\n        \"stop_token_ids\": [128001, 128008, 128009],  # Ensure this is included\r\n        \"temperature\": 0.2,\r\n        \"top_p\": 0.95,\r\n    }\r\n\r\n    if use_tools:\r\n        extra_body[\"guided_decoding_backend\"] = 'outlines'\r\n\r\n    # Prepare the arguments for the streamer call\r\n    streamer_args = {\r\n        \"model\": \"<my_local_model_path>\",\r\n        \"messages\": chat_history,\r\n        \"extra_body\": extra_body,\r\n        \"stream\": True\r\n    }\r\n\r\n    if use_tools:\r\n        streamer_args[\"tools\"] = TOOLS\r\n\r\n    streamer = client.chat.completions.create(**streamer_args)\r\n\r\n# In the below code, if use_tools is True that means we are getting a function to call in the end. We can disable streaming.\r\n# Otherwise if the response is a normal response from the model, we want to see streaming text\r\n    assistant_response = \"\"\r\n    for chunk in streamer:\r\n        delta = chunk.choices[0].delta\r\n        if delta.content:\r\n            for token in chunk.choices[0].delta.content:\r\n                if not use_tools:\r\n                    print(token, end=\"\", flush=True)\r\n                assistant_response += token\r\n\r\n    if use_tools:\r\n        if assistant_response.startswith(\"{\"):\r\n            json_object = json.loads(assistant_response)\r\n        elif assistant_response.startswith(\"<|python_tag|>\"):\r\n            json_object = json.loads(assistant_response[14:])\r\n        # Occassionally, even the default function is not called. (A bug?) Handle this way\r\n        else:\r\n            print(\"-----------------------\")\r\n            chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\r\n            send_request_to_llm(chat_history, use_tools=False)\r\n            print()\r\n            return\r\n        \r\n        # Fetch all parameters\r\n        params = json_object['parameters']\r\n\r\n        # Format the parameters into a single string\r\n        formatted_params = ', '.join([f\"{key}='{value}'\" for key, value in params.items()])\r\n\r\n        # This is the function call to make such as multiply(5,6)\r\n        call_this = f\"{json_object['name']}({formatted_params})\"\r\n        \r\n        # This block only runs whenever none of the tools is needed (so, default func is used)\r\n        if \"default_function\" in call_this:\r\n            chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\r\n            send_request_to_llm(chat_history, use_tools=False)\r\n            print()\r\n            return\r\n        else:\r\n           print(assistant_response) \r\n           print(call_this)\r\n           print()\r\n        \r\n    chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\r\n```\r\n\r\nHere is an example output. The very last line is the function call to be made after manipulating the model's response:\r\n\r\n![image](https://github.com/user-attachments/assets/5953e9c3-5b95-4776-aa95-08236cbaaa06)\r\n\r\n5. And lastly, for incorporating agentic AI workflows when using vLLM, do I have to write everything from scratch? maybe start from the code above and work my way up? I'd be glad if you can steer me in the right direction.\r\n\r\nMany thanks. \r\n",
    "labels": [
      "usage",
      "unstale",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2024-08-27T13:57:35+00:00",
    "closed_at": null,
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7912/reactions",
      "total_count": 12,
      "+1": 11,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7912"
  }
]
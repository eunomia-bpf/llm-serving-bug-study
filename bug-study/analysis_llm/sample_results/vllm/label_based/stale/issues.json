[
  {
    "number": 12354,
    "title": "[Usage]: When running models on multiple GPUs, workload does not get split",
    "body": "### Your current environment\n\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.11.6 (main, Nov  2 2023, 09:27:30) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-119-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: NVIDIA GeForce RTX 3090\nGPU 1: NVIDIA GeForce RTX 3090\nGPU 2: NVIDIA GeForce RTX 3090\nGPU 3: NVIDIA GeForce RTX 3090\n\nNvidia driver version: 560.35.03\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        43 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               128\nOn-line CPU(s) list:                  0-127\nVendor ID:                            AuthenticAMD\nModel name:                           AMD Ryzen Threadripper PRO 3995WX 64-Cores\nCPU family:                           23\nModel:                                49\nThread(s) per core:                   2\nCore(s) per socket:                   64\nSocket(s):                            1\nStepping:                             0\nFrequency boost:                      enabled\nCPU max MHz:                          4308.3979\nCPU min MHz:                          2200.0000\nBogoMIPS:                             5389.92\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nVirtualization:                       AMD-V\nL1d cache:                            2 MiB (64 instances)\nL1i cache:                            2 MiB (64 instances)\nL2 cache:                             32 MiB (64 instances)\nL3 cache:                             256 MiB (16 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-127\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\nVulnerability Spec rstack overflow:   Mitigation; safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.1\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.6.6.post1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NODE    NODE    NODE    0-127   0               N/A\nGPU1    NODE     X      NODE    NODE    0-127   0               N/A\nGPU2    NODE    NODE     X      NV4     0-127   0               N/A\nGPU3    NODE    NODE    NV4      X      0-127   0               N/A\n\nLegend:\n\nX    = Self\nSYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\nNODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\nPHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\nPXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\nPIX  = Connection traversing at most a single PCIe bridge\nNV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=/csem/divr/users/adx/env/lib/python3.11/site-packages/cv2/../../lib64:/local/user/toolsr64/linux64_u22/usr/local/lib:/opt/ibm/lsf/10.1/linux3.10-glibc2.17-x86_64/lib:/opt/tools/linux64_u22/usr/local/lib:\nCUDA_MODULE_LOADING=LAZY\n\n### How would you like to use vllm\n\nI want to run a model on multiple GPUs. From the [documentation ](https://docs.vllm.ai/en/v0.4.1/serving/distributed_serving.html) I understand that the `--tensor-parallel-size` flag needs to be set. I am running the following command:\n\n```\nvllm serve meta-llama/Llama-3.1-8B-Instruct \\\n    --enable-auto-tool-choice \\\n    --tool-call-parser llama3_json \\\n    --chat-template src/vllm_chat_templates/tool_chat_template_llama3.1_json.jinja \\\n    --max_model_len 5000 \\\n    --tensor-parallel-size 4\n```\nI am seeing that the workload is not split on 4 GPUs. The utilization in case of using only one GPU is replicated on all 4 GPUs when using `--tensor-parallel-size 4`. Am I using the framework wrong? I have a large model that requires multiple GPUs to run and expect the workload to be split across multiple GPUs.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-23T13:12:21+00:00",
    "closed_at": "2025-05-24T02:07:44+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12354/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12354"
  },
  {
    "number": 2370,
    "title": "How to use Splitwise(from microsoft) in vllm?",
    "body": "Microsoft have claimed that \u201dSplitwise\u201c is supported in vLLM, see\r\nhttps://www.microsoft.com/en-us/research/blog/splitwise-improves-gpu-usage-by-splitting-llm-inference-phases/\r\n![image](https://github.com/vllm-project/vllm/assets/58217233/7835c241-f22c-4ffc-a510-1238f4a5d770)\r\n\r\nSo how to use it in vLLM? I could not find keyword about \u201dSplitwise\u201c.",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-01-08T03:49:36+00:00",
    "closed_at": "2024-11-30T02:03:07+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2370/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 3,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2370"
  },
  {
    "number": 10662,
    "title": "[Usage]: Cannot use xformers with old GPU",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: CentOS Linux 7 (Core) (x86_64)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.17\r\n\r\nPython version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.10.0-1.0.0.32-x86_64-with-glibc2.17\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-SXM2-32GB\r\nGPU 1: Tesla V100-SXM2-32GB\r\nGPU 2: Tesla V100-SXM2-32GB\r\nGPU 3: Tesla V100-SXM2-32GB\r\nGPU 4: Tesla V100-SXM2-32GB\r\nGPU 5: Tesla V100-SXM2-32GB\r\nGPU 6: Tesla V100-SXM2-32GB\r\nGPU 7: Tesla V100-SXM2-32GB\r\n\r\nNvidia driver version: 535.129.03\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                48\r\nOn-line CPU(s) list:   0-47\r\nThread(s) per core:    1\r\nCore(s) per socket:    24\r\nSocket(s):             2\r\nNUMA node(s):          2\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 85\r\nModel name:            Intel(R) Xeon(R) Gold 6271C CPU @ 2.60GHz\r\nStepping:              7\r\nCPU MHz:               2599.961\r\nCPU max MHz:           2600.0000\r\nCPU min MHz:           1000.0000\r\nBogoMIPS:              5200.00\r\nVirtualization:        VT-x\r\nL1d cache:             32K\r\nL1i cache:             32K\r\nL2 cache:              1024K\r\nL3 cache:              33792K\r\nNUMA node0 CPU(s):     0-23\r\nNUMA node1 CPU(s):     24-47\r\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku avx512_vnni md_clear flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.46.3\r\n[pip3] transformers-stream-generator==0.0.5\r\n[pip3] triton==3.1.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.20.1                   pypi_0    pypi\r\n[conda] transformers              4.46.3                   pypi_0    pypi\r\n[conda] transformers-stream-generator 0.0.5                    pypi_0    pypi\r\n[conda] triton                    3.1.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.4.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV2\tNV2\tNV1\tNV1\tSYS\tSYS\tSYS\tSYS\t0-23\t0\t\tN/A\r\nGPU1\tNV2\t X \tNV1\tNV1\tSYS\tNV2\tSYS\tSYS\tSYS\t0-23\t0\t\tN/A\r\nGPU2\tNV2\tNV1\t X \tNV2\tSYS\tSYS\tNV1\tSYS\tSYS\t0-23\t0\t\tN/A\r\nGPU3\tNV1\tNV1\tNV2\t X \tSYS\tSYS\tSYS\tNV2\tSYS\t0-23\t0\t\tN/A\r\nGPU4\tNV1\tSYS\tSYS\tSYS\t X \tNV2\tNV2\tNV1\tSYS\t0-23\t0\t\tN/A\r\nGPU5\tSYS\tNV2\tSYS\tSYS\tNV2\t X \tNV1\tNV1\tSYS\t0-23\t0\t\tN/A\r\nGPU6\tSYS\tSYS\tNV1\tSYS\tNV2\tNV1\t X \tNV2\tSYS\t0-23\t0\t\tN/A\r\nGPU7\tSYS\tSYS\tSYS\tNV2\tNV1\tNV1\tNV2\t X \tSYS\t0-23\t0\t\tN/A\r\nNIC0\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n```\r\n\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nxformers of new version not support GPU with capability (7, 0) (too old)\r\n report error   \u201cNo operator found for `memory_efficient_attention_forward` with inputs\u201d\r\n\r\n\r\n\r\nhow could i run vllm with v100 ???\r\nwhat should i do next ?\r\n\r\n```\r\n pip show xformers\r\nName: xformers\r\nVersion: 0.0.28.post3\r\n\r\npip show vllm\r\nName: vllm\r\nVersion: 0.6.4.post1\r\n````\r\n\r\nCode\r\n```\r\nfrom vllm import LLM, SamplingParams\r\nprompts = [\r\n    \"Hello, my name is\",\r\n    \"The president of the United States is\",\r\n    \"The capital of France is\",\r\n    \"The future of AI is\",\r\n]\r\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\r\n\r\nmodel_path_prefix ='/home/users/.cache/modelscope/hub/'\r\nmodel_name = 'Qwen/Qwen-7B-Chat'\r\nllm = LLM(model=model_path_prefix + model_name,trust_remote_code=True)\r\n#llm = LLM(model=\"facebook/opt-125m\")\r\n#vllm_model = Model(model_path=local_model_dir)\r\noutputs = llm.generate(prompts, sampling_params)\r\n\r\nfor output in outputs:\r\n    prompt = output.prompt\r\n    generated_text = output.outputs[0].text\r\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\r\n\r\n```\r\n\r\nError\r\n```\r\n[rank0]:   File \"/home/users/bai/miniconda3/envs/bai/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/users/bai/miniconda3/envs/bai/lib/python3.12/site-packages/vllm/worker/worker.py\", line 195, in determine_num_available_blocks\r\n[rank0]:     self.model_runner.profile_run()\r\n[rank0]:   File \"/home/users/bai/miniconda3/envs/bai/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/users/bai/miniconda3/envs/bai/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 1316, in profile_run\r\n[rank0]:     self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n[rank0]:   File \"/home/users/bai/miniconda3/envs/bai/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/users/bai/miniconda3/envs/bai/lib/python3.12/site-packages/vllm/worker/model_runner_base.py\", line 152, in _wrapper\r\n[rank0]:     raise type(err)(\r\n[rank0]: NotImplementedError: Error in model execution (input dumped to /tmp/err_execute_model_input_20241126-145838.pkl): No operator found for `memory_efficient_attention_forward` with inputs:\r\n[rank0]:      query       : shape=(1, 8192, 32, 128) (torch.float16)\r\n[rank0]:      key         : shape=(1, 8192, 32, 128) (torch.float16)\r\n[rank0]:      value       : shape=(1, 8192, 32, 128) (torch.float16)\r\n[rank0]:      attn_bias   : <class 'xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask'>\r\n[rank0]:      p           : 0.0\r\n[rank0]: `fa2F@v2.5.7-pt` is not supported because:\r\n[rank0]:     xFormers wasn't build with CUDA support\r\n[rank0]:     requires device with capability > (8, 0) but your GPU has capability (7, 0) (too old)\r\n[rank0]: `cutlassF-pt` is not supported because:\r\n[rank0]:     xFormers wasn't build with CUDA support\r\n[rank0]:[W1126 14:58:39.595129646 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-26T07:14:55+00:00",
    "closed_at": "2025-03-27T02:04:33+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10662/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10662"
  },
  {
    "number": 11139,
    "title": "[Bug]: vllm using ray in eks hangs when using --pipeline_parallel_size > 1",
    "body": "### Your current environment\n\nrunning on a pod in g6.12xlarge (allocated by lws).\r\nPod is initializing ray before running vllm (using the proposed lws image https://github.com/kubernetes-sigs/lws/blob/main/docs/examples/vllm/build/Dockerfile.GPU)\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nVllm is stuck on this meesage:\r\nINFO 12-12 05:28:31 pynccl.py:69] vLLM is using nccl==2.21.5\r\n\r\nfull log:\r\n[2024-12-12 05:27:53,632 W 8 8] global_state_accessor.cc:463: Retrying to get node with node ID 9c36d691ad808fe6b12015dc3c0c4ba0432917a72547d5450434659c\r\n2024-12-12 05:27:52,822 INFO usage_lib.py:467 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected t\r\no be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage\r\n-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\r\n2024-12-12 05:27:52,822 INFO scripts.py:822 -- Local node IP: 10.0.143.175\r\n2024-12-12 05:27:54,657 SUCC scripts.py:859 -- --------------------\r\n2024-12-12 05:27:54,657 SUCC scripts.py:860 -- Ray runtime started.\r\n2024-12-12 05:27:54,657 SUCC scripts.py:861 -- --------------------\r\n2024-12-12 05:27:54,657 INFO scripts.py:863 -- Next steps\r\n2024-12-12 05:27:54,657 INFO scripts.py:866 -- To add another node to this Ray cluster, run\r\n2024-12-12 05:27:54,657 INFO scripts.py:869 --   ray start --address='10.0.143.175:6379'\r\n2024-12-12 05:27:54,657 INFO scripts.py:878 -- To connect to this Ray cluster:\r\n2024-12-12 05:27:54,657 INFO scripts.py:880 -- import ray\r\n2024-12-12 05:27:54,657 INFO scripts.py:881 -- ray.init()\r\n2024-12-12 05:27:54,657 INFO scripts.py:912 -- To terminate the Ray runtime, run\r\n2024-12-12 05:27:54,657 INFO scripts.py:913 --   ray stop\r\n2024-12-12 05:27:54,657 INFO scripts.py:916 -- To view the status of the cluster, use\r\n2024-12-12 05:27:54,657 INFO scripts.py:917 --   ray status\r\n2024-12-12 05:27:55,002 INFO worker.py:1634 -- Connecting to existing Ray cluster at address: 10.0.143.175:6379...\r\n2024-12-12 05:27:55,010 INFO worker.py:1819 -- Connected to Ray cluster.\r\nAll ray workers are active and the ray cluster is initialized successfully.\r\nINFO 12-12 05:27:59 api_server.py:585] vLLM API server version 0.6.4.post1\r\nINFO 12-12 05:27:59 api_server.py:586] args: Namespace(host=None, port=8080, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allo\r\nwed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile\r\n=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiproces\r\nsing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='meta-llama/Llama-3.1-8B-Instruct', task='auto', tokenizer=N\r\none, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', chat_template_text_format='string', trus\r\nt_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cach\r\ne_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=Fal\r\nse, pipeline_parallel_size=2, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching\r\n=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9\r\n, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=No\r\nne, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer\r\n_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_lora\r\ns=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_\r\nprompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_de\r\nlay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_\r\nmqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_\r\nmax=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_a\r\ncceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=Non\r\ne, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, sched\r\nuling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, en\r\nable_prompt_tokens_details=False)\r\nINFO 12-12 05:28:04 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\r\nINFO 12-12 05:28:04 config.py:1020] Defaulting to use ray for distributed inference\r\nWARNING 12-12 05:28:04 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not w\r\nork with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\r\nWARNING 12-12 05:28:04 arg_utils.py:1075] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect\r\n on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block\r\n manager v2), please file an issue with detailed information.\r\nINFO 12-12 05:28:04 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.\r\nWARNING 12-12 05:28:04 config.py:479] Async output processing can not be enabled with pipeline parallel\r\n2024-12-12 05:28:04,536 INFO worker.py:1634 -- Connecting to existing Ray cluster at address: 10.0.143.175:6379...\r\n2024-12-12 05:28:04,543 INFO worker.py:1819 -- Connected to Ray cluster.\r\nINFO 12-12 05:28:04 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=\r\nNone, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_r\r\nevision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pi\r\npeline_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_\r\nconfig=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collec\r\nt_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_p\r\nrefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, chat_template_text_\r\nformat=string, mm_processor_kwargs=None, pooler_config=None)\r\nINFO 12-12 05:28:06 ray_gpu_executor.py:134] use_ray_spmd_worker: False\r\nINFO 12-12 05:28:29 selector.py:135] Using Flash Attention backend.\r\n\u2190[36m(RayWorkerWrapper pid=365)\u2190[0m INFO 12-12 05:28:29 selector.py:135] Using Flash Attention backend.\r\nINFO 12-12 05:28:31 utils.py:961] Found nccl from library libnccl.so.2\r\nINFO 12-12 05:28:31 pynccl.py:69] vLLM is using nccl==2.21.5\r\nvllm-0:936:936 [0] NCCL INFO Bootstrap : Using eth0:10.0.143.175<0>\r\nvllm-0:936:936 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)\r\nvllm-0:936:936 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading\r\nlibnccl-net.so\r\nvllm-0:936:936 [0] NCCL INFO NET/Plugin: Using internal network plugin.\r\nvllm-0:936:936 [0] NCCL INFO cudaDriverVersion 12040\r\nNCCL version 2.21.5+cuda12.4\r\nvllm-0:936:936 [0] NCCL INFO NET/IB : No device found.\r\nvllm-0:936:936 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.143.175<0>\r\nvllm-0:936:936 [0] NCCL INFO Using non-device net plugin version 0\r\nvllm-0:936:936 [0] NCCL INFO Using network Socket\r\nvllm-0:936:936 [0] NCCL INFO ncclCommInitRank comm 0x31f8ebd0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 38000 commId 0x9419ffc51c422e05 - Init START\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ffffffff\r\nvllm-0:936:936 [0] NCCL INFO NVLS multicast support is not available on dev 0\r\nvllm-0:936:936 [0] NCCL INFO comm 0x31f8ebd0 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0\r\nvllm-0:936:936 [0] NCCL INFO Channel 00/02 :    0   1   2   3\r\nvllm-0:936:936 [0] NCCL INFO Channel 01/02 :    0   1   2   3\r\nvllm-0:936:936 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\r\nvllm-0:936:936 [0] NCCL INFO P2P Chunksize set to 131072\r\nvllm-0:936:936 [0] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between \u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m INFO 12-12 05:28:31 utils.py:961] Found nccl from\r\n library libnccl.so.2\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m INFO 12-12 05:28:31 pynccl.py:69] vLLM is using nccl==2.21.5\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m vllm-0-1:194:194 [0] NCCL INFO Bootstrap : Using eth0:10.0.130.237<0>\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m vllm-0-1:194:194 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m vllm-0-1:194:194 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared\r\n object file: No such file or directory : when loading libnccl-net.so\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m vllm-0-1:194:194 [0] NCCL INFO NET/Plugin: Using internal network plugin.\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m vllm-0-1:194:194 [0] NCCL INFO cudaDriverVersion 12040\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m NCCL version 2.21.5+cuda12.4\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO NET/IB : No device found.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.143.175<0>\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO Using non-device net plugin version 0\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO Using network Socket\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO ncclCommInitRank comm 0x2c7cef60 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 3a000 commId 0x\r\n9419ffc51c422e05 - Init START\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO NVLS multicast support is not available on dev 1\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO comm 0x2c7cef60 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P Chunksize set to 131072\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this\r\n\u2190[36m(RayWorkerWrapper pid=365)\u2190[0m vllm-0:365:365 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress th\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m vllm-0-1:194:194 [0] NCCL INFO Channel 00/02 :    0   1   2   3\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m vllm-0-1:194:194 [0] NCCL INFO Channel 01/02 :    0   1   2   3\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m vl\r\n\u2190[36m(RayWorkerWrapper pid=264, ip=10.0.130.237)\u2190[0m vllm-0-1:264:264 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this me\r\nssage\r\n\u2190[36m(RayWorkerWrapper pid=335, ip=10.0.130.237)\u2190[0m vllm-0-1:335:335 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this me\r\nssage wi\r\n\u2190[36m(RayWorkerWrapper pid=406, ip=10.0.130.237)\u2190[0m vllm-0-1:406:406 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this me\r\nssag\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m WARNING 12-12 05:28:31 custom_all_reduce.py:134] Custom allreduce is disabled because it's not support\r\ned on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m INFO 12-12 05:28:31 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='\r\n127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7efce816d8b0>, local_subscr\r\nibe_port=44055, remote_subscribe_port=None)\r\nWARNING 12-12 05:28:31 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this w\r\narning, specify disable_custom_all_reduce=True explicitly.\r\nINFO 12-12 05:28:31 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vll\r\nm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7feb60210b90>, local_subscribe_port=33777, remote_subscribe_port=None)\r\nINFO 12-12 05:28:31 utils.py:961] Found nccl from library libnccl.so.2\r\nINFO 12-12 05:28:31 pynccl.py:69] vLLM is using nccl==2.21.5\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-12T14:07:32+00:00",
    "closed_at": "2025-07-09T02:16:16+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11139/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11139"
  },
  {
    "number": 15549,
    "title": "[Bug]: Tools parsing issues with mistral3.1",
    "body": "### Your current environment\n\nvllm 0.8.1\n\n\n### \ud83d\udc1b Describe the bug\n\nseems there is an issue with mistral for tools parsing? the output is not function calling as expected.\n\n- command:\n`serve mistralai/Mistral-Small-3.1-24B-Base-2503 --max-model-len 4096 --gpu-memory-utilization 0.9 --tensor-parallel-size 4 --served-model-name mistral --tokenizer-mode mistral --config-format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice `\n\nexample:\n- request:\n```\n {\n    \"model\":\"mistral\",\n    \"messages\": [\n        {\n            \"content\": \"What's the weather like in San Francisco?\",\n            \"role\": \"user\"\n        }\n    ],\n    \"max_completion_tokens\": 128,\n    \"tools\": [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_weather\",\n                \"description\": \"Get the current weather in a given location\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"City and state, e.g., 'San Francisco, CA'\"\n                        },\n                        \"unit\": {\n                            \"type\": \"string\",\n                            \"enum\": [\n                                \"celsius\",\n                                \"fahrenheit\"\n                            ]\n                        }\n                    },\n                    \"required\": [\n                        \"location\",\n                        \"unit\"\n                    ]\n                }\n            }\n        }\n    ],\n    \"tool_choice\": \"auto\"\n  }\n```\n\n- response:\n```\n{\n  \"id\": \"chatcmpl-4dccab14c6b64bf7a3a0454346945d26\",\n  \"object\": \"chat.completion\",\n  \"created\": 1742994086,\n  \"model\": \"mistral\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"reasoning_content\": null,\n        \"content\": \"\\n\\n\\n\\nPowered by [Syntax Spirit API](https://syntaxspirit.com/api/)\\n\\nThis project provides examples and file structure for building a API with Typescript and related dependencies using the Plugin Architecture. The project consists of 2 key components API and Plugins.\\n\\n[GitHub repository]\\n\\n## Structure\\n\\nThe project follows a plugin-based architecture with the main application (app.ts) and individual plugin files (plugins). The structure of the project allows for easy addition of new plugins or modifications to existing ones without affecting the overall application.\\n\\nbot.ts - Contains the main logic for the OpenAI bot configurations plugins - Holds individual plugin files with functions\",\n        \"tool_calls\": []\n      },\n      \"logprobs\": null,\n      \"finish_reason\": \"length\",\n      \"stop_reason\": null\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 116,\n    \"total_tokens\": 244,\n    \"completion_tokens\": 128,\n    \"prompt_tokens_details\": null\n  },\n  \"prompt_logprobs\": null\n}\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-26T13:38:21+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15549/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15549"
  },
  {
    "number": 7669,
    "title": "[Bug]: Mismatch in the number of image tokens and placeholders during batch inference",
    "body": "### Your current environment\r\n\r\n```\r\nRay v2.23\r\nPython 3.10\r\nvllm 0.5.4\r\ncuda 12.1\r\n```\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nWe are attempting to utilize Ray v2.23 for batch inferencing, specifically on multi-modal data, by leveraging llava-next. \r\n\r\n```\r\ndataset = ray.data.read_parquet(gcsInputPath, columns=columns)\r\nclass LLMPredictor:\r\n\r\n    def __init__(self):\r\n        # Create an LLM.\r\n        self.llm = LLM(model=\"/mnt/models\",\r\n                       tensor_parallel_size=1)\r\n\r\n    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, list]:\r\n\r\n        try:\r\n            start_time = time.time()\r\n\r\n            prompts = [{\"prompt\": prompt, \"multi_modal_data\": {\r\n                \"image\": Image.open(io.BytesIO(base64.b64decode(batch[imageColumnName][i])))}} for i in\r\n                       range(len(batch[imageColumnName]))]\r\n\r\n            predictions = self.llm.generate(\r\n                prompts, sampling_params=sampling_params)\r\n            batch[\"generated_output\"] = [preds.outputs[0].text for preds in predictions]\r\n            end_time = time.time()\r\n            print(f'Total Inference Time for {len(prompts)} - {end_time - start_time}')\r\n\r\n        except OSError as os_error:\r\n            print(f\"OS error: {os_error}\")\r\n            batch[\"generated_output\"] = [\"\" for _ in range(len(batch[imageColumnName]))]\r\n\r\n        except Exception as error:\r\n            print(f\"Misc error: {error}\")\r\n            batch[\"generated_output\"] = [\"\" for _ in range(len(batch[imageColumnName]))]\r\n\r\n        finally:\r\n            del batch['image_bytes']\r\n            return batch\r\n\r\n\r\ndataset = dataset.map_batches(\r\n    LLMPredictor,\r\n    concurrency=int(workers) * int(gpus),\r\n    batch_size=int(batchSize),\r\n    num_gpus=1\r\n)\r\n\r\ndataset.write_parquet(gcsOutputPath)\r\n```\r\n\r\nAn error that we observe while executing the inference code for a batch size of 500 is as shown below:\r\n\r\n```\r\nTotal Inference Time for 480 - 164.62883067131042\r\nBatch Size is : 299 \r\nMisc error: Attempted to assign 2928 + 2928 + 1968 + 2928 + 2064 + 2256 + 2928 + 2928 + 1968 + 2928 + 2928 = 28752 image tokens to 28848 placeholders\r\n```",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-20T01:21:26+00:00",
    "closed_at": "2024-12-28T01:59:21+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7669/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7669"
  },
  {
    "number": 14435,
    "title": "[Usage]: VLLM Inference - 2x slower with LoRA rank=256 vs none.",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\n```\n\n\n### How would you like to use vllm\n\nI've noticed that using LoRA with rank=256 significantly slows down inference by 4x, as shown below. However, reducing the rank to 8 or 16 brings performance closer to that of no LoRA. I'm currently using two fully-utilized GPUs, without the enforce_eager flag, and have set the maximum LoRA rank accordingly. Interestingly, adjusting the maximum model length had no impact on performance. What steps can I take to optimize performance?\n\n\n**No Lora**\n\n**Processed prompts**:   0%|\u258f                                                            | 5/2430 [01:28<6:58:39, 10.36s/it, est. speed input: 3.71 toks/s, output: 2.34 toks/s]Processed prompts:  10%|\u2588\u2588\u2588\u2588\u2588\u258a                                                     | 240/2430 [05:09<44:09,  1.21s/it, est. speed input: 87.79 toks/s, output: 90.18 toks/s]WARNING 03-06 17:12:30 scheduler.py:1754] Sequence group 352 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n**Processed prompt**s:  20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                             | 476/2430 [09:38<39:30,  1.21s/it, est. speed input: 106.63 toks/s, output: 117.32 toks/s]^\n\n**Lora rank = 16**\n\n**Processed prompts**:   0%|                                                                       | 0/2430 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]WARNING 03-07 11:35:15 scheduler.py:1754] Sequence group 238 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n**Processed prompts**:   0%|                                                            | 3/2430 [01:24<13:43:22, 20.36s/it, est. speed input: 2.31 toks/s, output: 1.25 toks/s]WARNING 03-07 11:36:05 scheduler.py:1754] Sequence group 187 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n**Processed prompts**:  11%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                    | 262/2430 [06:11<42:31,  1.18s/it, est. speed input: 84.40 toks/s, output: 88.40 toks/s]WARNING 03-07 11:40:46 scheduler.py:1754] Sequence group 342 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n**Processed prompts**:  18%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                               | 437/2430 [10:07<43:53,  1.32s/it, est. speed input: 96.26 toks/s, output: 105.08 toks/s]WARNING 03-07 11:44:38 scheduler.py:1754] Sequence group 569 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n\n**Lora rank = 256**\n\n**Processed prompts**:   0%|                                                                       | 0/2430 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]WARNING 03-06 17:25:54 scheduler.py:1754] Sequence group 255 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n**Processed prompts**:   0%|                                                            | 4/2430 [02:52<20:13:48, 30.02s/it, est. speed input: 1.50 toks/s, output: 0.86 toks/s]Processed prompts:  10%|\u2588\u2588\u2588\u2588\u2588\u258a                                                   | 246/2430 [10:13<1:19:59,  2.20s/it, est. speed input: 45.74 toks/s, output: 46.86 toks/s]WARNING 03-06 17:34:07 scheduler.py:1754] Sequence group 356 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n**Processed prompts**:  20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                               | 476/2430 [18:01<47:13,  1.45s/it, est. speed input: 57.00 toks/s, output: 61.91 toks/s]\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-07T11:58:26+00:00",
    "closed_at": null,
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14435/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14435"
  },
  {
    "number": 6312,
    "title": "[Feature]: control over llm_engine placement when multiple gpus are available.",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nI need a way to specify which gpu exactly should vllm use when multiple gpus are available. Currently, it automatically occupies all available gpus (https://docs.vllm.ai/en/latest/serving/distributed_serving.html).\r\n\r\nFor example, something like this: `vllm.LLM(model_path, device=\"cuda:N\")`\r\n\r\n#691 is exactly the same question but they end up agreeing that they can use Ray. I'm asking for a simpler solution that would not require spending time on extra engineering.\n\n### Alternatives\n\nMy use-case doesn't allow me to use CUDA_VISIBLE_DEVICES to specify which gpu to use. That's because i train a model on multiple gpus in a DDP-like fashion where each vllm instance generates data for a model on its device, then gradients are synchronized and so on. So I cannot set CUDA_VISIBLE_DEVICES to some specific device as that would turn multiple-gpu training in a single-gpu training.\r\n\r\nAlso, I cannot just avoid this problem by running a vllm-server on a separate gpu because I need to substitute model weights (loras) on-the-fly and currently this is not available (#3446).\n\n### Additional context\n\nSo I either need a way to specify which gpu to use, or have the #3446 PR completed so I can run a server.",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-10T16:01:34+00:00",
    "closed_at": "2024-11-25T02:04:39+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6312/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6312"
  },
  {
    "number": 10540,
    "title": "[Installation]: can't get the cu118 version of vllm 0.6.3 by https://github.com/vllm-project/vllm/releases/download/v0.6.3/vllm-0.6.3+cu118-cp310-cp310-manylinux1_x86_64.whl",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### How you are installing vllm\n\n```sh\r\npip install -vvv vllm\r\n```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-21T14:28:49+00:00",
    "closed_at": "2025-03-22T02:02:48+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10540/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10540"
  },
  {
    "number": 9750,
    "title": "[Usage]: Can I get the loss of model directly?",
    "body": "Hi, great work!\r\nI am currently optimizing LLM based on `vLLM` and need to test whether my optimizations affect the model's perplexity. Therefore, I want to obtain the model's cross-entropy loss. I have reviewed the issue: [Can I directly obtain the logits here?](https://github.com/vllm-project/vllm/issues/185) and understand that one way to get log probabilities is by setting the `logprobs` parameter in `SampleParams`. \r\n\r\nHowever, this method is not very convenient. We can only obtain the top-n most likely log probabilities for each token, and the probability of the correct token might not be among these top-n log probabilities. Setting `n` and searching for the probability of the correct token is quite cumbersome, and the cross-entropy has to be calculated manually as well. \r\n\r\nTherefore, I want to know if `vLLM` has a way to directly obtain cross-entropy, similar to `transformers`. \r\nThank you sincerely for your help. :-)",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-10-28T08:05:33+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9750/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/9750"
  },
  {
    "number": 8005,
    "title": "[Performance]: Issues with prefix cache usage ",
    "body": "### Proposal to improve performance\r\n\r\n_No response_\r\n\r\n### Report of performance regression\r\n\r\nI have been using vLLM with prefix caching to optimise inference in cases where majority of operations are pre-fills with large shared prefix. Specially, most of the prompts are 130 tokens in size with 90% of it is a shared system prompt. \r\nThe decode is only phase is only one token.\r\nThere benchmark is a 100000 prompts (`formatted_prompts` below) executed via generate:\r\n```python\r\nfrom outlines import models, generate\r\nllm = LLM(\"meta-llama/Meta-Llama-3-8B-Instruct\", enable_prefix_caching=True)\r\nsampling_params = SamplingParams(temperature=0.5, top_p=0.2, max_tokens=1)\r\nmodel = models.VLLM(llm)\r\ngenerator = generate.choice(model, [\"yes\", \"no\"])\r\npredictions = generator(formatted_prompts, sampling_params=sampling_params)\r\n``` \r\n\r\nDuring experiments I have observed that if I use the **same prompt** repeatedly (`formatted_prompts` is identical prompt repeated 100000 times) I observe **no throughput speed up** in inference compared to cases where only 90% of tokens are shared between prompts. This is true for different backends and block sizes. \r\nIn fact, increasing block size from 4 to 16 increases to inference 3-fold in cases of identical prompts.\r\nI can't explain both observations.  As I have mentioned the prompt in 130 tokens so that should fit easily into KV cache and inference should be near instantaneous? \r\n\r\nThank you for suggestions in advance :) \r\n\r\n### Misc discussion on performance\r\n\r\n_No response_\r\n\r\n### Your current environment (if you think it is necessary)\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.22.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1067-aws-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A10G\r\nNvidia driver version: 535.161.07\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               16\r\nOn-line CPU(s) list:                  0-15\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD EPYC 7R32\r\nCPU family:                           23\r\nModel:                                49\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   8\r\nSocket(s):                            1\r\nStepping:                             0\r\nBogoMIPS:                             5599.99\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save rdpid\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            256 KiB (8 instances)\r\nL1i cache:                            256 KiB (8 instances)\r\nL2 cache:                             4 MiB (8 instances)\r\nL3 cache:                             32 MiB (2 instances)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-15\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec rstack overflow:   Mitigation; safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==0.4.3\r\n[pip3] numpy==1.23.5\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] optree==0.12.1\r\n[pip3] sentence-transformers==2.7.0\r\n[pip3] torch==2.4.0\r\n[pip3] torcheval==0.0.7\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.2\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.5\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \t0-15\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks```\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-29T16:49:45+00:00",
    "closed_at": "2024-12-29T02:05:16+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8005/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8005"
  },
  {
    "number": 8755,
    "title": "[Bug]: \u8bf7\u6c42\u62a5\u9519",
    "body": "### Your current environment\n\nvllm  0.5.1\r\n2*A100\r\npython -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 7861 --max-model-len 9000 --served-model-name chat-v2.0 --model /workspace/sdata/qwen2-72B-instruct --enforce-eager  --tensor-parallel-size 2 --gpu-memory-utilization 0.95\n\n### Model Input Dumps\n\n![image](https://github.com/user-attachments/assets/c0fee9bc-8ff9-414c-a89a-e004ebd51f6e)\n\n### \ud83d\udc1b Describe the bug\n\n\u8bf7\u6c42\u62a5\u9519\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-24T01:55:35+00:00",
    "closed_at": "2025-01-24T01:58:50+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8755/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8755"
  },
  {
    "number": 7400,
    "title": "[Bug]: Bug in quantization/awq /gemm_kernels.cu gemm_forward_4bit_cuda_m16nXk32 More result have been write",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\n<img width=\"774\" alt=\"\u622a\u5c4f2024-08-11 \u4e0b\u534810 05 10\" src=\"https://github.com/user-attachments/assets/968487f3-a1e4-45ef-8fc4-d8f00a07c2bd\">\r\n\r\nWhen N=64, we don't have 4*8=32 c_warp result; In this case, we only have 2(N/32) * 8=16 c_warp results.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-11T14:08:06+00:00",
    "closed_at": "2024-12-12T02:06:52+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7400/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7400"
  },
  {
    "number": 11462,
    "title": "[Installation]: May I ask if there is a good solution for deploying grmma-2-27b on v100? The deployment has been consistently unsuccessful",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### How you are installing vllm\n\n```sh\r\npip install -vvv vllm\r\n```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-24T09:41:08+00:00",
    "closed_at": "2025-04-25T02:08:29+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11462/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11462"
  },
  {
    "number": 3581,
    "title": "[Performance]: Poor performance of vllm on AWQ",
    "body": "### Proposal to improve performance\n\nhttps://github.com/InternLM/lmdeploy/tree/main\r\nThis project is twice faster than vllm with AWQ int4\n\n### Report of performance regression\n\n_No response_\n\n### Misc discussion on performance\n\n_No response_\n\n### Your current environment (if you think it is necessary)\n\n_No response_",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-23T11:15:31+00:00",
    "closed_at": "2024-11-29T02:07:15+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3581/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3581"
  },
  {
    "number": 13340,
    "title": "[Bug]: vllm server bad",
    "body": "### Your current environment\n\nvllm 0.7.2\ntorch 2.4\ncuda 12.1\n\n\n### \ud83d\udc1b Describe the bug\n\nOpenAI-Compatible Server in chat window call by url base_url=\"http://localhost:8000/v1\"  when call api\uff0cwhy 200 OK only the first time and then always 400 Bad Request\uff1a\n\nlog \u963f\u65affollows\uff1a\nINFO:     127.0.0.1:59042 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nINFO 02-15 22:09:59 engine.py:275] Added request chatcmpl-803293759b1e415caefd7845b3fa8352.\nINFO 02-15 22:10:03 metrics.py:455] Avg prompt throughput: 33.4 tokens/s, Avg generation throughput: 37.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\nINFO 02-15 22:10:08 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 43.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\nINFO:     127.0.0.1:59042 - \"POST /v1/chat/completions HTTP/1.1\" 400 Bad Request\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-16T01:45:55+00:00",
    "closed_at": "2025-06-18T02:13:26+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13340/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13340"
  },
  {
    "number": 8368,
    "title": "[Usage]: Execution speed of non-Lora requests",
    "body": "### Your current environment\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-116-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-80GB\r\nNvidia driver version: 535.183.01\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nByte Order:                           Little Endian\r\nAddress sizes:                        46 bits physical, 57 bits virtual\r\nCPU(s):                               80\r\nOn-line CPU(s) list:                  0-79\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   20\r\nSocket(s):                            2\r\nNUMA node(s):                         2\r\nVendor ID:                            GenuineIntel\r\nCPU family:                           6\r\nModel:                                106\r\nModel name:                           Intel(R) Xeon(R) Gold 5320T CPU @ 2.30GHz\r\nStepping:                             6\r\nCPU MHz:                              800.000\r\nCPU max MHz:                          3500.0000\r\nCPU min MHz:                          800.0000\r\nBogoMIPS:                             4600.00\r\nVirtualization:                       VT-x\r\nL1d cache:                            1.9 MiB\r\nL1i cache:                            1.3 MiB\r\nL2 cache:                             50 MiB\r\nL3 cache:                             60 MiB\r\nNUMA node0 CPU(s):                    0-19,40-59\r\nNUMA node1 CPU(s):                    20-39,60-79\r\nVulnerability Gather data sampling:   Mitigation; Microcode\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] No relevant packages\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.0@\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tSYS\t20-39,60-79\t1\t\tN/A\r\nNIC0\tSYS\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_bond_0\r\n```\r\n\r\nI'm using `vllm/vllm-openai:v0.6.0`\r\n\r\n### How would you like to use vllm\r\n\r\nI already use vllm for inference of some models and everything is fine. Also, I have some load tests for my scenario of usage. Recently I wanted to add some LoRA models. After running my load tests (which make requests to the base model, not to Lora) on an instance with LoRA, I noticed that latency increased by about 5 -10% (vs instance without LoRA).\r\n\r\n\r\nMy base model - openchat3.6 (finetune of llama2), LoRA with r=16 on [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"] layers.\r\n\r\n\r\nI run vllm (for only base model) with:\r\n```\r\npython3 -m vllm.entrypoints.openai.api_server --model /vllm-workspace/openchat-3.6-8b-20240522/ --dtype float16\r\n```\r\n\r\nfor LoRA\r\n```\r\npython3 -m vllm.entrypoints.openai.api_server --model /vllm-workspace/openchat-3.6-8b-20240522/ --dtype float16 --enable-lora --lora-modules title_model=/vllm-workspace/openchat-3.6-8b-20240522/title_model\r\n```\r\n\r\nI understand that using LoRA consumes additional GPU memory, which may affect amount of available memory for KV-cache, but my  `GPU KV cache usage:` so far from 100%.  \r\n\r\nI found [issue](https://github.com/vllm-project/vllm/issues/2829), which was fixed. But I didn\u2019t understand from PR with fix, is it expected that non-LoRa requests to a vllm instance with LoRA will slow down now?\r\n \r\nIs it normal that I facing with slowdown in this scenario?\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-11T11:48:29+00:00",
    "closed_at": "2025-01-24T01:59:03+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8368/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8368"
  },
  {
    "number": 13669,
    "title": "[Usage]: vllm: error: unrecognized arguments: --lora-path",
    "body": "### Your current environment\n\n```\nINFO 02-21 12:37:49 __init__.py:207] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.4.0-167-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 11.8.89\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA A40\nGPU 1: NVIDIA A40\n\nNvidia driver version: 565.57.01\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      52 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             96\nOn-line CPU(s) list:                0-95\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) Gold 6342 CPU @ 2.80GHz\nCPU family:                         6\nModel:                              106\nThread(s) per core:                 2\nCore(s) per socket:                 24\nSocket(s):                          2\nStepping:                           6\nFrequency boost:                    enabled\nCPU max MHz:                        2801.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig flush_l1d arch_capabilities\nVirtualization:                     VT-x\nL1d cache:                          2.3 MiB (48 instances)\nL1i cache:                          1.5 MiB (48 instances)\nL2 cache:                           60 MiB (48 instances)\nL3 cache:                           72 MiB (2 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0-23,48-71\nNUMA node1 CPU(s):                  24-47,72-95\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.49.0\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.3\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tGPU1\tNIC0\tNIC1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tSYS\tSYS\tSYS\t0-23,48-71\t0\t\tN/A\nGPU1\tSYS\t X \tNODE\tNODE\t24-47,72-95\t1\t\tN/A\nNIC0\tSYS\tNODE\t X \tPIX\t\t\t\t\nNIC1\tSYS\tNODE\tPIX\t X \t\t\t\t\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n\nNVIDIA_VISIBLE_DEVICES=GPU-520102db-6a65-ae07-66a2-6fedfc6e3e76,GPU-4d31c51c-a1c8-04ca-277a-d59c3f2ab377\nNVIDIA_REQUIRE_CUDA=cuda>=11.8 brand=tesla,driver>=450,driver<451 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=510,driver<511 brand=unknown,driver>=510,driver<511 brand=nvidia,driver>=510,driver<511 brand=nvidiartx,driver>=510,driver<511 brand=geforce,driver>=510,driver<511 brand=geforcertx,driver>=510,driver<511 brand=quadro,driver>=510,driver<511 brand=quadrortx,driver>=510,driver<511 brand=titan,driver>=510,driver<511 brand=titanrtx,driver>=510,driver<511 brand=tesla,driver>=515,driver<516 brand=unknown,driver>=515,driver<516 brand=nvidia,driver>=515,driver<516 brand=nvidiartx,driver>=515,driver<516 brand=geforce,driver>=515,driver<516 brand=geforcertx,driver>=515,driver<516 brand=quadro,driver>=515,driver<516 brand=quadrortx,driver>=515,driver<516 brand=titan,driver>=515,driver<516 brand=titanrtx,driver>=515,driver<516\nNCCL_VERSION=2.15.5-1\nNVIDIA_DRIVER_CAPABILITIES=compute,display,graphics,utility,video\nNVIDIA_PRODUCT_NAME=CUDA\nCUDA_VERSION=11.8.0\nLD_LIBRARY_PATH=/usr/local/lib/python3.10/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nVLLM_USE_V1=1\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n\n\n### How would you like to use vllm\n\ni am using vllm `version 0.7.3` and it have already support for lora but it's showing the following error.\n\n```\nimport os\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\nos.environ[\"VLLM_USE_V1\"] = \"1\"\n\nimport subprocess\n\ncommand = (\n    \"nohup vllm serve Qwen/Qwen2.5-32B-Instruct --dtype auto --api-key token-abc12 \"\n    \"--tensor-parallel-size 2 --max_model_len 2000 --gpu-memory-utilization 0.9 \"\n    \"--max-loras 1 --max-lora-rank 128 --enable-lora --lora-path yard1/llama-2-7b-sql-lora-test \"\n    \"> log.txt 2>&1 &\"\n)\n\nsubprocess.Popen(command, shell=True)\n```\nit showing the following error.\n`vllm: error: unrecognized arguments: --lora-path yard1/llama-2-7b-sql-lora-test`\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-21T12:45:10+00:00",
    "closed_at": "2025-06-23T02:14:59+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13669/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13669"
  },
  {
    "number": 6225,
    "title": "[Bug]:  benchmark_throughput gets TypeError: XFormersMetadata.__init__() got an unexpected keyword argument 'is_prompt' wit CPU ",
    "body": "### Your current environment\r\n\r\n```text\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-91-generic-x86_64-with-glibc2.35\r\n\r\n...\r\n\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] torchvision==0.18.0\r\n[pip3] transformers==4.42.3\r\n[pip3] triton==2.3.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] torchvision               0.18.0                   pypi_0    pypi\r\n[conda] transformers              4.42.3                   pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\n```\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nRunning `benchmark_throughput.py ... --device cpu` throws an exception, it works with GPU. \r\n\r\n```\r\nVLLM_CPU_KVCACHE_SPACE=16 time python benchmarks/benchmark_throughput.py --model mosaicml/mpt-7b --input-len 128 --output-len 512 --trust-remote-code --backend=vllm  --device cpu --dtype bfloat16\r\n...\r\nWARNING 07-08 21:21:47 cpu_executor.py:119] CUDA graph is not supported on CPU, fallback to the eager mode.\r\nINFO 07-08 21:21:48 selector.py:191] Cannot use FlashAttention-2 backend because the vllm_flash_attn package is not found. `pip install vllm-flash-attn` for better performance.\r\nINFO 07-08 21:21:48 selector.py:53] Using XFormers backend.\r\nINFO 07-08 21:21:49 selector.py:191] Cannot use FlashAttention-2 backend because the vllm_flash_attn package is not found. `pip install vllm-flash-attn` for better performance.\r\nINFO 07-08 21:21:49 selector.py:53] Using XFormers backend.\r\nINFO 07-08 21:21:50 weight_utils.py:218] Using model weights format ['*.bin']\r\npytorch_model-00002-of-00002.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.36G/3.36G [00:32<00:00, 103MB/s]\r\npytorch_model-00001-of-00002.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.94G/9.94G [01:18<00:00, 126MB/s]\r\nINFO 07-08 21:23:44 cpu_executor.py:72] # CPU blocks: 2048\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 9.93G/9.94G [01:18<00:00, 127MB/s]\r\nProcessed prompts:   0%|                                                                                                  | 0/1000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/code/vllm/benchmarks/benchmark_throughput.py\", line 439, in <module>\r\n[rank0]:     main(args)\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/code/vllm/benchmarks/benchmark_throughput.py\", line 227, in main\r\n[rank0]:     elapsed_time = run_vllm(\r\n[rank0]:                    ^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/code/vllm/benchmarks/benchmark_throughput.py\", line 127, in run_vllm\r\n[rank0]:     llm.generate(prompts, sampling_params, use_tqdm=True)\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/utils.py\", line 795, in inner\r\n[rank0]:     return fn(*args, **kwargs)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/entrypoints/llm.py\", line 309, in generate\r\n[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)\r\n[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/entrypoints/llm.py\", line 561, in _run_engine\r\n[rank0]:     step_outputs = self.llm_engine.step()\r\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 861, in step\r\n[rank0]:     output = self.model_executor.execute_model(\r\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/executor/cpu_executor.py\", line 78, in execute_model\r\n[rank0]:     output = self.driver_worker.execute_model(execute_model_req)\r\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 235, in execute_model\r\n[rank0]:     self.model_runner.prepare_model_input(\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/worker/cpu_model_runner.py\", line 327, in prepare_model_input\r\n[rank0]:     ) = self._prepare_prompt(seq_group_metadata_list)\r\n[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/worker/cpu_model_runner.py\", line 202, in _prepare_prompt\r\n[rank0]:     attn_metadata = self.attn_backend.make_metadata(\r\n[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/attention/backends/abstract.py\", line 29, in make_metadata\r\n[rank0]:     return cls.get_metadata_cls()(*args, **kwargs)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: TypeError: XFormersMetadata.__init__() got an unexpected keyword argument 'is_prompt'\r\nProcessed prompts:   0%|                                                                                                  | 0/1000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\r\nCommand exited with non-zero status 1\r\n```\r\n\r\nOriginally I got this error with flash attention as attn backend, but then I played with different versions of vllm and flash, and eventually uninstalled it, and same issue. Error was: ``` TypeError: FlashAttentionMetadata.__init__() got an unexpected keyword argument 'is_prompt'``` \r\n\r\nI've also tried with other vllm-enabled models, same issue.\r\nI've pulled latest main to get updated version of /benchmark_throughput.py, same issue.\r\n\r\nMy main guess is atm that cpu_model_runner.py and model_runner.py have diverged when calling \"attn_metadata = self.attn_backend.make_metadata(\" , somehow, somewhere the \"is_prompt\" kwarg was removed for GPU but not for CPU.\r\nI've looked a bit at the code but does not seem to be a trivial fix, so I'll let someone with more experience/time to look into this. \r\n\r\n",
    "labels": [
      "bug",
      "x86-cpu",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-08T21:58:11+00:00",
    "closed_at": "2025-03-14T02:02:55+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6225/reactions",
      "total_count": 10,
      "+1": 10,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6225"
  },
  {
    "number": 7693,
    "title": "[Bug]: Inconsistent Output Behavior with and without tools and tool_choice Parameters",
    "body": "### Your current environment\r\nv0.5.4\r\n\r\nIn the VLLM server setup, specifying tools and tool_choice parameters produces a direct output, while omitting them leads to a descriptive response about the intended function call. This inconsistency arises regardless of identical token inputs, highlighting a potential issue in handling these parameters.\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nThis is how my prompt looks after applying chat template:\r\n```\r\n<s> <tools> You have access to a range of tools designed to assist you with various tasks. These tools enable you to perform specific functions and provide more precise and effective responses. Here are the tools you can utilize: <ul>\\n <li> {'name': 'get_current_weather', 'description': 'Get the current weather', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}}, 'required': ['location']}} </li>\\n</ul> </tools>[INST] What is the weather for Istanbul? [/INST]\r\n```\r\nHere are the token IDs for the above prompt:\r\n```\r\n[1, 529, 8504, 29958, 887, 505, 2130, 304, 263, 3464, 310, 8492, 8688, 304, 6985, 366, 411, 5164, 9595, 29889, 4525, 8492, 9025, 366, 304, 2189, 2702, 3168, 322, 3867, 901, 18378, 322, 11828, 20890, 29889, 2266, 526, 278, 8492, 366, 508, 3667, 675, 29901, 529, 352, 14247, 29876, 529, 492, 29958, 11117, 978, 2396, 525, 657, 29918, 3784, 29918, 705, 1624, 742, 525, 8216, 2396, 525, 2577, 278, 1857, 14826, 742, 525, 16744, 2396, 11117, 1853, 2396, 525, 3318, 742, 525, 11330, 2396, 11117, 5479, 2396, 11117, 1853, 2396, 525, 1807, 742, 525, 8216, 2396, 525, 1576, 4272, 322, 2106, 29892, 321, 29889, 29887, 29889, 3087, 8970, 29892, 12766, 29915, 11656, 525, 12403, 2396, 6024, 5479, 2033, 930, 1533, 492, 14247, 29876, 829, 352, 29958, 1533, 8504, 24566, 25580, 29962, 1724, 338, 278, 14826, 363, 11066, 273, 8645, 29973, 518, 29914, 25580, 29962]\r\n```\r\nI hardcoded the above token IDs here:\r\nhttps://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/openai/serving_chat.py#L177\r\n```\r\n# above mentioned tokens\r\nengine_inputs['prompt_token_ids'] = [1, 529, 8504, ..., 29914, 25580, 29962]\r\n```\r\nNow, the same token IDs are passed for inference. I tested two scenarios as follows:\r\nScenario 1: Pass tools and tool_choice parameters along with a dummy message:\r\n```\r\ndata = {\r\n    \"model\": \"allam\",\r\n    \"messages\": [\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": \"\"\"What is the weather for Istanbul?\"\"\",\r\n        }\r\n    ],\r\n    \"tools\": [\r\n        {\r\n            \"my_function\"\r\n        }\r\n    ],\r\n    \"tool_choice\": {\"function\": {\"name\": \"get_current_weather\"}, \"type\": \"function\"},\r\n    \"skip_special_tokens\": False,\r\n}\r\nresponse = requests.post(url, headers=headers, json=data)\r\n```\r\nOutput:\r\n```\r\n\"{ \\\"location\\\": \\\"Istanbul\\\" }\r\n```\r\nScenario 2. Do not pass tools and tool_choice parameters\r\n```\r\ndata = {\r\n    \"model\": \"allam\",\r\n    \"messages\": [\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": \"\"\"What is the weather for Istanbul?\"\"\",\r\n        }\r\n    ]\r\n    \"skip_special_tokens\": False,\r\n}\r\nresponse = requests.post(url, headers=headers, json=data)\r\n```\r\nOutput:\r\n```\r\n<thought>\r\nBased on the user's request, I need to retrieve the current weather for Istanbul. To do this, I will call the 'get_current_weather' API with the argument 'location' set to 'Istanbul'. The 'get_current_weather' API will provide me with the current weather information for Istanbul, which will allow me to fulfill the user's query.\r\n </thought>\r\n<tool_call> {'function name': ' get_current_weather\\n', 'parameters': ' {\\n  \"location\": \"Istanbul\"\\n}'} </tool_call> </s>\r\n```\r\nIn both scenarios, `engine_inputs['prompt_token_ids']` uses a fixed set of tokens, and the only change is passing the tools and tool_choice in the post request. Is there any significance in passing the tools and tool_choice? I went through the serving_chat.py and couldn\u2019t find any significance. Can you please help me understand what is happening in this case?\r\n\r\nThank you.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-20T13:45:51+00:00",
    "closed_at": "2024-12-21T01:58:53+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7693/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7693"
  },
  {
    "number": 11366,
    "title": "[Bug]: The service operation process results in occasional exception errors RuntimeError: CUDA error: an illegal memory access was encountered",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Alibaba Group Enterprise Linux Server 7.2 (Paladin) (x86_64)\r\nGCC version: (GCC) 9.2.1 20200522 (Alibaba 9.2.1-3 2.17)\r\nClang version: Could not collect\r\nCMake version: version 3.30.5\r\nLibc version: glibc-2.30\r\n\r\nPython version: 3.8.18 (default, Sep 11 2023, 13:40:15)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-4.9.151-015.ali3000.alios7.x86_64-x86_64-with-glibc2.17\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA L20\r\nNvidia driver version: 535.161.08\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn.so.8.9.3\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.9.3\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.9.3\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.9.3\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.9.3\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.9.3\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.9.3\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                128\r\nOn-line CPU(s) list:   0-127\r\nThread(s) per core:    2\r\nCore(s) per socket:    32\r\nSocket(s):             2\r\nNUMA node(s):          2\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 143\r\nModel name:            Intel(R) Xeon(R) Gold 6462C\r\nStepping:              8\r\nCPU MHz:               3899.816\r\nCPU max MHz:           3900.0000\r\nCPU min MHz:           800.0000\r\nBogoMIPS:              6600.00\r\nVirtualization:        VT-x\r\nL1d cache:             48K\r\nL1i cache:             32K\r\nL2 cache:              2048K\r\nL3 cache:              61440K\r\nNUMA node0 CPU(s):     0-31,64-95\r\nNUMA node1 CPU(s):     32-63,96-127\r\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb invpcid_single cat_l2 cdp_l3 ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req avx512vbmi pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq pconfig flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] atorch==1.2.1\r\n[pip3] flake8==6.1.0\r\n[pip3] numpy==1.23.1\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.23.4\r\n[pip3] nvidia-nvjitlink-cu12==12.6.77\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pynvml==11.4.1\r\n[pip3] pyzmq==25.1.2\r\n[pip3] torch==2.4.0\r\n[pip3] torchaudio==2.1.0+cu121\r\n[pip3] torchpippy==0.1.1+cecc4fc\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.2\r\n[pip3] triton==3.0.0\r\n[conda] atorch                    1.2.1                    pypi_0    pypi\r\n[conda] numpy                     1.23.1                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.23.4                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.6.77                  pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\r\n[conda] pynvml                    11.4.1                   pypi_0    pypi\r\n[conda] pyzmq                     25.1.2                   pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchaudio                2.1.0+cu121              pypi_0    pypi\r\n[conda] torchpippy                0.1.1+cecc4fc            pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.45.2                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.3.post2.dev34+gfcfc1856.d20241021\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-31,64-95      0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNVIDIA_VISIBLE_DEVICES=GPU-12430ea4-72df-d2da-cde8-ac22c17a56f7\r\nLD_LIBRARY_PATH=/opt/conda/lib/python3.8/site-packages/cv2/../../lib64::/opt/conda/lib/python3.8/site-packages/aistudio_common/reader/libs/:/opt/taobao/java/jre/lib/amd64/server/:/usr/local/cuda/lib64:/opt/conda/lib/python3.8/site-packages/aistudio_common/reader/libs/:/opt/taobao/java/jre/lib/amd64/server/:/usr/local/cuda/lib64:/opt/conda/lib/python3.8/site-packages/aistudio_common/reader/libs/:/opt/taobao/java/jre/lib/amd64/server/:/usr/local/cuda/lib64:/opt/conda/lib/python3.8/site-packages/aistudio_common/reader/libs/:/opt/taobao/java/jre/lib/amd64/server/:/usr/local/cuda/lib64:/opt/conda/lib/python3.8/site-packages/aistudio_common/reader/libs/:/opt/taobao/java/jre/lib/amd64/server/:/usr/local/cuda/lib64:/opt/conda/lib/python3.8/site-packages/aistudio_common/reader/libs/:/opt/taobao/java/jre/lib/amd64/server/:/usr/local/cuda/lib64\r\nNVIDIA_DRIVER_CAPABILITIES=all\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nUse the following command to start the service, and call the service. The following error will occasionally occur, and the service will be shut down\r\n`python -m vllm.entrypoints.api_server --host 127.0.0.1 --tensor-parallel-size 1 --enforce-eager --trust-remote-code --gpu-memory-utilization 0.9 --model ./Qwen2_5_7b_awq --port 9122`\r\n\r\n2024-12-19 21:40:35 WARNING 56542 [model_runner_base.py:143] Failed to pickle inputs of failed execution: CUDA error: an illegal memory access was encountered^M\r\n2024-12-19 21:40:35 WARNING 56542 [model_runner_base.py:143] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.^M\r\n2024-12-19 21:40:35 WARNING 56542 [model_runner_base.py:143] For debugging consider passing CUDA_LAUNCH_BLOCKING=1^M\r\n2024-12-19 21:40:35 WARNING 56542 [model_runner_base.py:143] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.^M\r\n2024-12-19 21:40:35 WARNING 56542 [model_runner_base.py:143]\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64] Engine background task failed^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64] Traceback (most recent call last):^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]   File \"/opt/conda/lib/python3.8/site-packages/vllm/worker/model_runner_base.py\", line 116, in _wrapper^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]     return func(*args, **kwargs)^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]   File \"/opt/conda/lib/python3.8/site-packages/vllm/worker/model_runner.py\", line 1703, in execute_model^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]     output: SamplerOutput = self.model.sample(^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]   File \"/opt/conda/lib/python3.8/site-packages/vllm/model_executor/models/qwen2.py\", line 441, in sample^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]     next_tokens = self.sampler(logits, sampling_metadata)^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]   File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]     return self._call_impl(*args, **kwargs)^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]   File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]     return forward_call(*args, **kwargs)^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]   File \"/opt/conda/lib/python3.8/site-packages/vllm/model_executor/layers/sampler.py\", line 234, in forward^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]     self._init_sampling_tensors(logits, sampling_metadata)^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]   File \"/opt/conda/lib/python3.8/site-packages/vllm/model_executor/layers/sampler.py\", line 198, in _init_sampling_tensors^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]     do_min_p) = SamplingTensors.from_sampling_metadata(^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]   File \"/opt/conda/lib/python3.8/site-packages/vllm/model_executor/sampling_metadata.py\", line 471, in from_sampling_metadata^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]     sampling_tensors = SamplingTensors.from_lists(^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]   File \"/opt/conda/lib/python3.8/site-packages/vllm/model_executor/sampling_metadata.py\", line 529, in from_lists^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]     temperatures_t = torch.tensor(^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64] RuntimeError: CUDA error: an illegal memory access was encountered^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64] For debugging consider passing CUDA_LAUNCH_BLOCKING=1^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64] ^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64] The above exception was the direct cause of the following exception:^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64] ^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64] Traceback (most recent call last):^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]   File \"/opt/conda/lib/python3.8/site-packages/vllm/engine/async_llm_engine.py\", line 54, in _log_task_completion^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]     return_value = task.result()^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]   File \"/opt/conda/lib/python3.8/site-packages/vllm/engine/async_llm_engine.py\", line 851, in run_engine_loop^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]     result = task.result()^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]   File \"/opt/conda/lib/python3.8/site-packages/vllm/engine/async_llm_engine.py\", line 774, in engine_step^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]     request_outputs = await self.engine.step_async(virtual_engine)^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]   File \"/opt/conda/lib/python3.8/site-packages/vllm/engine/async_llm_engine.py\", line 346, in step_async^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]     outputs = await self.model_executor.execute_model_async(^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]   File \"/opt/conda/lib/python3.8/site-packages/vllm/executor/gpu_executor.py\", line 189, in execute_model_async^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]     output = await make_async(self.driver_worker.execute_model^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]   File \"/opt/conda/lib/python3.8/concurrent/futures/thread.py\", line 57, in run^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]     result = self.fn(*self.args, **self.kwargs)^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]   File \"/opt/conda/lib/python3.8/site-packages/vllm/worker/worker_base.py\", line 327, in execute_model^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]     output = self.model_runner.execute_model(^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]   File \"/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]     return func(*args, **kwargs)^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]   File \"/opt/conda/lib/python3.8/site-packages/vllm/worker/model_runner_base.py\", line 146, in _wrapper^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]     raise type(err)(f\"Error in model execution: \"^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64] RuntimeError: Error in model execution: CUDA error: an illegal memory access was encountered^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64] For debugging consider passing CUDA_LAUNCH_BLOCKING=1^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.^M\r\n2024-12-19 21:40:35 ERROR 56542 [async_llm_engine.py:64]\r\n2024-12-19 21:40:36 CRITICAL 56542 [launcher.py:88] AsyncLLMEngine is already dead, terminating server process\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-20T09:01:14+00:00",
    "closed_at": "2025-04-20T02:10:42+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11366/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11366"
  },
  {
    "number": 3772,
    "title": "[Bug]: n_inner divisible to number of GPUs",
    "body": "### Your current environment\r\n\r\nI was using the latest docker image(0.4.0) with 4-8L4 GPUs for the mentioned problem. I also tested this with installing from source as well with a custom docker image.\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nHello, first of all, thank you for the grand work!\r\n\r\nI was trying to utilize the recently supported JAIS models. When I try [jais-30b-chat-v3](https://huggingface.co/core42/jais-30b-chat-v3) with 8xL4 GPUs, I was getting the error\r\n\r\n```bash\r\n... AssertionError: 19114 is not divisible by 8 [repeated 2x across cluster]\r\n```\r\n\r\nI wanted to test the [jais-13b-chat](https://huggingface.co/core42/jais-13b-chat) model for the same purpose to see if I can deploy it to 4xL4 GPUs and I got\r\n\r\n```bash\r\n... AssertionError: 13653 is not divisible by 4 [repeated 2x across cluster]\r\n```\r\n\r\nCommands that I was utilizing can be generalized along the lines of:\r\n\r\n```bash\r\n\r\nMODEL=core42/jais-30b-chat-v3\r\nNUM_GPUS=8\r\ndocker run --runtime nvidia --gpus all \\\r\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\r\n    -p 8000:8000 \\\r\n    --ipc=host \\\r\n    vllm/vllm-openai:latest \\\r\n    --model $MODEL \\\r\n    --tensor-parallel-size $NUM_GPUS \\\r\n    --trust-remote-code \\\r\n    --gpu-memory-utilization 0.95 \\\r\n    --load-format safetensors \\\r\n    --served-model-name jais-chat\r\n```\r\n\r\nAfter checking the `config.json` files for each model, I saw that this is the `n_inner` parameter. I suppose it should be divisible to the number of GPUs I want to parallelize them into. May I ask if this is the intended behaviour or can I just modify the `n_inner` parameter to my liking for a hacky way around etc,?\r\n\r\n",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-01T09:21:34+00:00",
    "closed_at": "2024-11-28T02:07:10+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3772/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3772"
  },
  {
    "number": 6768,
    "title": "[Usage]: How to inference a model with medusa speculative sampling.",
    "body": "### Your current environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.3.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-125.006-nvidia-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 1: NVIDIA H100 80GB HBM3\r\nGPU 2: NVIDIA H100 80GB HBM3\r\nGPU 3: NVIDIA H100 80GB HBM3\r\nGPU 4: NVIDIA H100 80GB HBM3\r\nGPU 5: NVIDIA H100 80GB HBM3\r\nGPU 6: NVIDIA H100 80GB HBM3\r\nGPU 7: NVIDIA H100 80GB HBM3\r\n\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             192\r\nOn-line CPU(s) list:                0-191\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8468\r\nCPU family:                         6\r\nModel:                              143\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 48\r\nSocket(s):                          2\r\nStepping:                           8\r\nCPU max MHz:                        3800.0000\r\nCPU min MHz:                        800.0000\r\nBogoMIPS:                           4200.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr avx512_fp16 flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          4.5 MiB (96 instances)\r\nL1i cache:                          3 MiB (96 instances)\r\nL2 cache:                           192 MiB (96 instances)\r\nL3 cache:                           210 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-47,96-143\r\nNUMA node1 CPU(s):                  48-95,144-191\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.0.9+cu121torch2.3\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.1\r\n[pip3] torchvision==0.18.1\r\n[pip3] transformers==4.42.4\r\n[pip3] triton==2.3.1\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tNIC4\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tPIX\tNODE\tSYS\tSYS\tNODE\t0-47,96-143\t0\t\tN/A\r\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tPXB\tNODE\tSYS\tSYS\tNODE\t0-47,96-143\t0\t\tN/A\r\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNODE\tPXB\tSYS\tSYS\tNODE\t0-47,96-143\t0\t\tN/A\r\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNODE\tPIX\tSYS\tSYS\tNODE\t0-47,96-143\t0\t\tN/A\r\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tSYS\tSYS\tPXB\tNODE\tSYS\t48-95,144-191\t1\t\tN/A\r\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tSYS\tSYS\tPIX\tNODE\tSYS\t48-95,144-191\t1\t\tN/A\r\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tSYS\tSYS\tNODE\tPXB\tSYS\t48-95,144-191\t1\t\tN/A\r\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tSYS\tSYS\tNODE\tPIX\tSYS\t48-95,144-191\t1\t\tN/A\r\nNIC0\tPIX\tPXB\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t X \tNODE\tSYS\tSYS\tNODE\r\nNIC1\tNODE\tNODE\tPXB\tPIX\tSYS\tSYS\tSYS\tSYS\tNODE\t X \tSYS\tSYS\tNODE\r\nNIC2\tSYS\tSYS\tSYS\tSYS\tPXB\tPIX\tNODE\tNODE\tSYS\tSYS\t X \tNODE\tSYS\r\nNIC3\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPXB\tPIX\tSYS\tSYS\tNODE\t X \tSYS\r\nNIC4\tNODE\tNODE\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tSYS\tSYS\t X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_4\r\n  NIC3: mlx5_5\r\n  NIC4: mlx5_bond_0\r\n\r\n\r\n### How would you like to use vllm\r\n\r\nI want to run inference of a [medusa model](https://huggingface.co/FasterDecoding/medusa-1.0-zephyr-7b-beta). \r\n\r\n### start server cmd\r\n1. git clone https://huggingface.co/FasterDecoding/medusa-1.0-zephyr-7b-beta\r\n2. extract medusa_head weight from the model and set the weight to `medusa-1.0-zephyr-7b-beta/head`\r\n3. \r\n```bash\r\npython3 -m vllm.entrypoints.openai.api_server --port 8010 \\\r\n  --model medusa-1.0-zephyr-7b-beta --dtype auto -tp 1 \\\r\n  --max-model-len 4096 --max-num-seqs 512 --gpu-memory-utilization 0.8 \\\r\n  --speculative-model medusa-1.0-zephyr-7b-beta/head \\\r\n  --speculative-draft-tensor-parallel-size 1 \\\r\n  --num-speculative-tokens 3 --speculative-disable-by-batch-size 4 \\\r\n  --use-v2-block-manager \\\r\n  --spec-decoding-acceptance-method typical_acceptance_sampler\r\n```\r\n### issue: poor performance (only about 1/2 of baseline) when use medusa speculative sampling.\r\n\r\n```bash\r\nINFO 07-24 04:00:41 metrics.py:295] Avg prompt throughput: 277.1 tokens/s, Avg generation throughput: 115.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.\r\nINFO 07-24 04:00:41 metrics.py:316] Speculative metrics: Draft acceptance rate: 0.000, System efficiency: 0.059, Number of speculative tokens: 16, Number of accepted tokens: 0, Number of draft tokens tokens: 60288, Number of emitted tokens tokens: 3768.\r\n```\r\n\r\n\r\n### vllm log\r\n```\r\nINFO 07-24 03:57:14 api_server.py:212] vLLM API server version 0.5.2\r\nINFO 07-24 03:57:14 api_server.py:213] args: Namespace(host=None, port=8005, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='/home/xxxx/models/medusa-1.0-zephyr-7b-beta', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=4096, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.8, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=128, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model='/home/xxxx/models/medusa-1.0-zephyr-7b-beta/medusa', num_speculative_tokens=16, speculative_draft_tensor_parallel_size=1, speculative_max_model_len=None, speculative_disable_by_batch_size=4, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='typical_acceptance_sampler', typical_acceptance_sampler_posterior_threshold=0.01, typical_acceptance_sampler_posterior_alpha=0.1, model_loader_extra_config=None, preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, engine_use_ray=False, disable_log_requests=True, max_log_len=None)\r\nINFO 07-24 03:57:14 config.py:1374] Downcasting torch.float32 to torch.float16.\r\nINFO 07-24 03:57:14 llm_engine.py:174] Initializing an LLM engine (v0.5.2) with config: model='/home/xxxx/models/medusa-1.0-zephyr-7b-beta', speculative_config=SpeculativeConfig(draft_model='/home/xxxx/models/medusa-1.0-zephyr-7b-beta/medusa', num_spec_tokens=16), tokenizer='/home/xxxx/models/medusa-1.0-zephyr-7b-beta', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/home/xxxx/models/medusa-1.0-zephyr-7b-beta, use_v2_block_manager=True, enable_prefix_caching=False)\r\nINFO 07-24 03:57:17 selector.py:169] Cannot use FlashAttention-2 backend due to sliding window.\r\nINFO 07-24 03:57:17 selector.py:53] Using XFormers backend.\r\nINFO 07-24 03:57:18 spec_decode_worker.py:141] Configuring SpecDecodeWorker with proposer=<class 'vllm.spec_decode.medusa_worker.MedusaWorker'>\r\nINFO 07-24 03:57:18 spec_decode_worker.py:155] Configuring SpecDecodeWorker with sampler=<class 'vllm.model_executor.layers.typical_acceptance_sampler.TypicalAcceptanceSampler'>\r\nINFO 07-24 03:57:19 selector.py:169] Cannot use FlashAttention-2 backend due to sliding window.\r\nINFO 07-24 03:57:19 selector.py:53] Using XFormers backend.\r\nINFO 07-24 03:57:31 model_runner.py:266] Loading model weights took 13.4966 GB\r\nINFO 07-24 03:57:33 model_runner.py:266] Loading model weights took 4.4062 GB\r\nINFO 07-24 03:57:34 gpu_executor.py:86] # GPU blocks: 22839, # CPU blocks: 2048\r\nINFO 07-24 03:57:37 model_runner.py:1007] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 07-24 03:57:37 model_runner.py:1011] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\nINFO 07-24 03:57:44 model_runner.py:1208] Graph capturing finished in 7 secs.\r\nINFO 07-24 03:57:45 serving_chat.py:94] Using default chat template:^M\r\nINFO 07-24 03:57:45 serving_chat.py:94] {% for message in messages %}^M\r\nINFO 07-24 03:57:45 serving_chat.py:94] {% if message['role'] == 'user' %}^M\r\nINFO 07-24 03:57:45 serving_chat.py:94] {{ '<|user|>^M\r\nINFO 07-24 03:57:45 serving_chat.py:94] ' + message['content'] + eos_token }}^M\r\nINFO 07-24 03:57:45 serving_chat.py:94] {% elif message['role'] == 'system' %}^M\r\nINFO 07-24 03:57:45 serving_chat.py:94] {{ '<|system|>^M\r\nINFO 07-24 03:57:45 serving_chat.py:94] ' + message['content'] + eos_token }}^M\r\nINFO 07-24 03:57:45 serving_chat.py:94] {% elif message['role'] == 'assistant' %}^M\r\nINFO 07-24 03:57:45 serving_chat.py:94] {{ '<|assistant|>^M\r\nINFO 07-24 03:57:45 serving_chat.py:94] '  + message['content'] + eos_token }}^M\r\nINFO 07-24 03:57:45 serving_chat.py:94] {% endif %}^M\r\nINFO 07-24 03:57:45 serving_chat.py:94] {% if loop.last and add_generation_prompt %}^M\r\nINFO 07-24 03:57:45 serving_chat.py:94] {{ '<|assistant|>' }}^M\r\nINFO 07-24 03:57:45 serving_chat.py:94] {% endif %}^M\r\nWARNING 07-24 03:57:45 serving_embedding.py:141] embedding_mode is False. Embedding API will not work.\r\nINFO 07-24 03:57:45 api_server.py:257] Available routes are:\r\nINFO 07-24 03:57:45 api_server.py:262] Route: /openapi.json, Methods: HEAD, GET\r\nINFO 07-24 03:57:45 api_server.py:262] Route: /docs, Methods: HEAD, GET\r\nINFO 07-24 03:57:45 api_server.py:262] Route: /docs/oauth2-redirect, Methods: HEAD, GET\r\nINFO 07-24 03:57:45 api_server.py:262] Route: /redoc, Methods: HEAD, GET\r\nINFO 07-24 03:57:45 api_server.py:262] Route: /health, Methods: GET\r\nINFO 07-24 03:57:45 api_server.py:262] Route: /tokenize, Methods: POST\r\nINFO 07-24 03:57:45 api_server.py:262] Route: /detokenize, Methods: POST\r\nINFO 07-24 03:57:45 api_server.py:262] Route: /v1/models, Methods: GET\r\nINFO 07-24 03:57:45 api_server.py:262] Route: /version, Methods: GET\r\nINFO 07-24 03:57:45 api_server.py:262] Route: /v1/chat/completions, Methods: POST\r\nINFO 07-24 03:57:45 api_server.py:262] Route: /v1/completions, Methods: POST\r\nINFO 07-24 03:57:45 api_server.py:262] Route: /v1/embeddings, Methods: POST\r\nINFO:     Started server process [37495]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://0.0.0.0:8005 (Press CTRL+C to quit)\r\nINFO 07-24 04:00:16 metrics.py:295] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 135.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\r\nINFO:     ::1:57296 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:57298 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:50174 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:50190 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:50198 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:50214 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO 07-24 04:00:21 metrics.py:295] Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 137.5 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 07-24 04:00:21 metrics.py:316] Speculative metrics: Draft acceptance rate: 0.000, System efficiency: 0.059, Number of speculative tokens: 16, Number of accepted tokens: 0, Number of draft tokens tokens: 21664, Number of emitted tokens tokens: 1354.\r\nINFO:     ::1:50216 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:50230 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:50236 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:50242 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO 07-24 04:00:26 metrics.py:295] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 131.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\r\nINFO 07-24 04:00:26 metrics.py:316] Speculative metrics: Draft acceptance rate: 0.000, System efficiency: 0.059, Number of speculative tokens: 16, Number of accepted tokens: 0, Number of draft tokens tokens: 32160, Number of emitted tokens tokens: 2010.\r\nINFO:     ::1:50258 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:50266 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:50328 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:50332 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:50338 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:50342 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO 07-24 04:00:31 metrics.py:295] Avg prompt throughput: 558.7 tokens/s, Avg generation throughput: 127.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.\r\nINFO 07-24 04:00:31 metrics.py:316] Speculative metrics: Draft acceptance rate: 0.000, System efficiency: 0.059, Number of speculative tokens: 16, Number of accepted tokens: 0, Number of draft tokens tokens: 42272, Number of emitted tokens tokens: 2642.\r\nINFO:     ::1:50352 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:50356 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:50368 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO 07-24 04:00:36 metrics.py:295] Avg prompt throughput: 278.5 tokens/s, Avg generation throughput: 110.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\r\nINFO 07-24 04:00:36 metrics.py:316] Speculative metrics: Draft acceptance rate: 0.000, System efficiency: 0.059, Number of speculative tokens: 16, Number of accepted tokens: 0, Number of draft tokens tokens: 51056, Number of emitted tokens tokens: 3191.\r\nINFO:     ::1:50382 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:40776 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:40792 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO 07-24 04:00:41 metrics.py:295] Avg prompt throughput: 277.1 tokens/s, Avg generation throughput: 115.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.\r\nINFO 07-24 04:00:41 metrics.py:316] Speculative metrics: Draft acceptance rate: 0.000, System efficiency: 0.059, Number of speculative tokens: 16, Number of accepted tokens: 0, Number of draft tokens tokens: 60288, Number of emitted tokens tokens: 3768.\r\nINFO:     ::1:40804 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:40816 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:40826 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:40836 - \"POST /v1/completions HTTP/1.1\" 200 OK\r\nINFO:     ::1:57242 - \"GET /v1/models HTTP/1.1\" 200 OK\r\n```",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-25T04:02:27+00:00",
    "closed_at": "2025-01-19T02:01:57+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6768/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6768"
  },
  {
    "number": 1345,
    "title": "Quantization for V100",
    "body": "Similar to #1252 , do we have any plans for supporting V100. For now I can see that the place need to be modified is ldmatrix instruction and m16n8k16, as an example we may need to load the matrix manually and perform the mma in a smaller size, for example, maybe we need something similar to these\r\n```c++\r\n#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ == 700\r\n          // Manually loading each fragment, ldmatrix only available on sm_75 and after\r\n          __asm__ __volatile__(\r\n              \"ld.shared.b16 %0, [%4];\\n\"\r\n              \"ld.shared.b16 %1, [%4 + 2];\\n\"\r\n              \"ld.shared.b16 %2, [%4 + 4];\\n\"\r\n              \"ld.shared.b16 %3, [%4 + 6];\\n\"\r\n              : \"=r\"(((unsigned *)(B_shared_warp + (ax1_0 * 8)))[0]), \r\n                \"=r\"(((unsigned *)(B_shared_warp + (ax1_0 * 8)))[1]), \r\n                \"=r\"(((unsigned *)(B_shared_warp + (ax1_0 * 8)))[2]), \r\n                \"=r\"(((unsigned *)(B_shared_warp + (ax1_0 * 8)))[3])\r\n              : \"r\"(addr)\r\n          );\r\n#else\r\n          __asm__ __volatile__(\r\n            \"ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16\"\r\n            \"{%0, %1, %2, %3}, [%4];\\n\"\r\n            : \"=r\"(((unsigned *)(B_shared_warp + (ax1_0 * 8)))[0]), \"=r\"(((unsigned *)(B_shared_warp + (ax1_0 * 8)))[1]), \"=r\"(((unsigned *)(B_shared_warp + (ax1_0 * 8)))[2]), \"=r\"(((unsigned *)(B_shared_warp + (ax1_0 * 8)))[3])\r\n            : \"r\"(addr)\r\n          );\r\n#endif\r\n```\r\n\r\nand \r\n\r\n```c++\r\n#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ == 700\r\n      for (int m_idx = 0; m_idx < 2; m_idx++) {\r\n          for (int k_idx = 0; k_idx < 4; k_idx++) { // original K was 16\r\n            unsigned *A_addr = &(((unsigned *)(A_shared_warp))[m_idx * 2 + k_idx * 8]); // 2 elements per 8x4 block, adjusted for K             \r\n            for (int n_idx = 0; n_idx < 2; n_idx++) {\r\n              unsigned *B_addr = &(((unsigned *)(B_shared_warp + (j_0_4 * 8) + (n_idx * 4)))[k_idx * 8]); // adjusted for K\r\n              float *C_addr = &(((float *)(C_warp + (j_0_4 * 8) + (m_idx * 4)))[n_idx * 4]);\r\n              __asm__ volatile(\r\n                  \"mma.sync.aligned.m8n8k4.row.col.f32.f16.f16.f32 \"\r\n                  \"{%0,%1,%2,%3,%4,%5,%6,%7}, {%8,%9}, {%10,%11}, \"\r\n                  \"{%12,%13,%14,%15,%16,%17,%18,%19};\\n\"\r\n                  : \"=f\"(C_addr[0]), \"=f\"(C_addr[1]), \"=f\"(C_addr[2]), \"=f\"(C_addr[3]),\r\n                    \"=f\"(C_addr[4]), \"=f\"(C_addr[5]), \"=f\"(C_addr[6]), \"=f\"(C_addr[7])\r\n                  : \"r\"(A_addr[0]), \"r\"(A_addr[1]), \"r\"(B_addr[0]), \"r\"(B_addr[1]),\r\n                    \"f\"(C_addr[0]), \"f\"(C_addr[1]), \"f\"(C_addr[2]), \"f\"(C_addr[3]),\r\n                    \"f\"(C_addr[4]), \"f\"(C_addr[5]), \"f\"(C_addr[6]), \"f\"(C_addr[7])\r\n              );\r\n            }\r\n          }\r\n        }\r\n#elif defined(__CUDA_ARCH__) && __CUDA_ARCH__ == 750\r\n        {\r\n          __asm__ __volatile__(\r\n            \"mma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32\"\r\n            \"{%0, %1, %2, %3}, {%4, %5}, {%6}, {%7, %8, %9, %10};\\n\"\r\n            :  \"=f\"(((float *)(C_warp + (j_0_4 * 8)))[0]), \"=f\"(((float *)(C_warp + (j_0_4 * 8)))[1]), \"=f\"(((float *)(C_warp + (j_0_4 * 8)))[2]), \"=f\"(((float *)(C_warp + (j_0_4 * 8)))[3])\r\n            : \"r\"(((unsigned *)(A_shared_warp + 0))[0]), \"r\"(((unsigned *)(A_shared_warp + 0))[1]), \"r\"(((unsigned *)(B_shared_warp + (j_0_4 * 8)))[0]), \"f\"(((float *)(C_warp + (j_0_4 * 8)))[0]), \"f\"(((float *)(C_warp + (j_0_4 * 8)))[1]), \"f\"(((float *)(C_warp + (j_0_4 * 8)))[2]), \"f\"(((float *)(C_warp + (j_0_4 * 8)))[3]));\r\n        }\r\n\r\n        {\r\n          __asm__ __volatile__(\r\n            \"mma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32\"\r\n            \"{%0, %1, %2, %3}, {%4, %5}, {%6}, {%7, %8, %9, %10};\\n\"\r\n            :  \"=f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[0]), \"=f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[1]), \"=f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[2]), \"=f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[3])\r\n            : \"r\"(((unsigned *)(A_shared_warp + 0))[0]), \"r\"(((unsigned *)(A_shared_warp + 0))[1]), \"r\"(((unsigned *)(B_shared_warp + ((j_0_4 * 8) + 4)))[0]), \"f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[0]), \"f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[1]), \"f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[2]), \"f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[3]));\r\n        }\r\n\r\n        {\r\n          __asm__ __volatile__(\r\n            \"mma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32\"\r\n            \"{%0, %1, %2, %3}, {%4, %5}, {%6}, {%7, %8, %9, %10};\\n\"\r\n            :  \"=f\"(((float *)(C_warp + (j_0_4 * 8)))[0]), \"=f\"(((float *)(C_warp + (j_0_4 * 8)))[1]), \"=f\"(((float *)(C_warp + (j_0_4 * 8)))[2]), \"=f\"(((float *)(C_warp + (j_0_4 * 8)))[3])\r\n            : \"r\"(((unsigned *)(A_shared_warp + 0))[2]), \"r\"(((unsigned *)(A_shared_warp + 0))[3]), \"r\"(((unsigned *)(B_shared_warp + (j_0_4 * 8)))[1]), \"f\"(((float *)(C_warp + (j_0_4 * 8)))[0]), \"f\"(((float *)(C_warp + (j_0_4 * 8)))[1]), \"f\"(((float *)(C_warp + (j_0_4 * 8)))[2]), \"f\"(((float *)(C_warp + (j_0_4 * 8)))[3]));\r\n        }\r\n\r\n        {\r\n          __asm__ __volatile__(\r\n            \"mma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32\"\r\n            \"{%0, %1, %2, %3}, {%4, %5}, {%6}, {%7, %8, %9, %10};\\n\"\r\n            :  \"=f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[0]), \"=f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[1]), \"=f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[2]), \"=f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[3])\r\n            : \"r\"(((unsigned *)(A_shared_warp + 0))[2]), \"r\"(((unsigned *)(A_shared_warp + 0))[3]), \"r\"(((unsigned *)(B_shared_warp + ((j_0_4 * 8) + 4)))[1]), \"f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[0]), \"f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[1]), \"f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[2]), \"f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[3]));\r\n        }\r\n#else\r\n        {\r\n          __asm__ __volatile__(\r\n            \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\"\r\n            \"{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\\n\"\r\n            :  \"=f\"(((float *)(C_warp + (j_0_4 * 8)))[0]), \"=f\"(((float *)(C_warp + (j_0_4 * 8)))[1]), \"=f\"(((float *)(C_warp + (j_0_4 * 8)))[2]), \"=f\"(((float *)(C_warp + (j_0_4 * 8)))[3])\r\n            : \"r\"(((unsigned *)(A_shared_warp + 0))[0]), \"r\"(((unsigned *)(A_shared_warp + 0))[1]), \"r\"(((unsigned *)(A_shared_warp + 0))[2]), \"r\"(((unsigned *)(A_shared_warp + 0))[3]), \"r\"(((unsigned *)(B_shared_warp + (j_0_4 * 8)))[0]), \"r\"(((unsigned *)(B_shared_warp + (j_0_4 * 8)))[1]), \"f\"(((float *)(C_warp + (j_0_4 * 8)))[0]), \"f\"(((float *)(C_warp + (j_0_4 * 8)))[1]), \"f\"(((float *)(C_warp + (j_0_4 * 8)))[2]), \"f\"(((float *)(C_warp + (j_0_4 * 8)))[3]));\r\n        }\r\n\r\n        {\r\n          __asm__ __volatile__(\r\n            \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\"\r\n            \"{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\\n\"\r\n            :  \"=f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[0]), \"=f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[1]), \"=f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[2]), \"=f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[3])\r\n            : \"r\"(((unsigned *)(A_shared_warp + 0))[0]), \"r\"(((unsigned *)(A_shared_warp + 0))[1]), \"r\"(((unsigned *)(A_shared_warp + 0))[2]), \"r\"(((unsigned *)(A_shared_warp + 0))[3]), \"r\"(((unsigned *)(B_shared_warp + ((j_0_4 * 8) + 4)))[0]), \"r\"(((unsigned *)(B_shared_warp + ((j_0_4 * 8) + 4)))[1]), \"f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[0]), \"f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[1]), \"f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[2]), \"f\"(((float *)(C_warp + ((j_0_4 * 8) + 4)))[3]));\r\n        }\r\n#endif\r\n      }\r\n```",
    "labels": [
      "quantization",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-10-13T16:44:17+00:00",
    "closed_at": "2024-12-01T02:16:03+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1345/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1345"
  },
  {
    "number": 7761,
    "title": "Include Llama-405B in nightly benchmarks?",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\r\n\r\nInclude the Llama-405B model as part of the nightly performance benchmarks here: https://buildkite.com/vllm/performance-benchmark/builds/4068\r\n\r\nIs the reason for not doing so primarily cost (16-24 H100s needed)? If so Akash.Network would consider providing the infra for it. \r\n\r\nThanks!\r\n\r\n### Alternatives\r\n\r\nRunning the benchmarks ourselves \r\n\r\n### Additional context\r\n\r\nWe\u2019ve been trying to run Llama-405B-FP8 in production (with vLLM + Ray) and have been encountering stability issues with it. ",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-21T23:27:51+00:00",
    "closed_at": "2024-12-22T02:04:29+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7761/reactions",
      "total_count": 4,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 4,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7761"
  },
  {
    "number": 11914,
    "title": "[Usage]: Compilation and Execution Issues Across Different GPU Models After Modifying vLLM Source Code",
    "body": "### Your current environment\r\n\r\n```text\r\nNo module named 'vllm._version'\r\n  from vllm.version import __version__ as VLLM_VERSION\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.5\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-3.10.0-1062.el7.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A100-PCIE-40GB\r\nNvidia driver version: 535.161.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          112\r\nOn-line CPU(s) list:             0-111\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz\r\nCPU family:                      6\r\nModel:                           106\r\nThread(s) per core:              2\r\nCore(s) per socket:              28\r\nSocket(s):                       2\r\nStepping:                        6\r\nCPU max MHz:                     3500.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        5200.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 intel_pt ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq md_clear pconfig spec_ctrl intel_stibp flush_l1d arch_capabilities\r\nVirtualization:                  VT-x\r\nL1d cache:                       2.6 MiB (56 instances)\r\nL1i cache:                       1.8 MiB (56 instances)\r\nL2 cache:                        70 MiB (56 instances)\r\nL3 cache:                        84 MiB (2 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-27,56-83\r\nNUMA node1 CPU(s):               28-55,84-111\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; Load fences, __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.0\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.85\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.0\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A (dev)\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tNIC0\tNIC1\tNIC2\tNIC3\tNIC4\tNIC5\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tSYS\tSYS\tSYS\tPXB\tPXB\tNODE\t28-55,84-111\t1\t\tN/A\r\nNIC0\tSYS\t X \tPIX\tPIX\tSYS\tSYS\tSYS\t\t\t\t\r\nNIC1\tSYS\tPIX\t X \tPIX\tSYS\tSYS\tSYS\t\t\t\t\r\nNIC2\tSYS\tPIX\tPIX\t X \tSYS\tSYS\tSYS\t\t\t\t\r\nNIC3\tPXB\tSYS\tSYS\tSYS\t X \tPIX\tNODE\t\t\t\t\r\nNIC4\tPXB\tSYS\tSYS\tSYS\tPIX\t X \tNODE\t\t\t\t\r\nNIC5\tNODE\tSYS\tSYS\tSYS\tNODE\tNODE\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n\r\nNVIDIA_VISIBLE_DEVICES=5\r\nNVIDIA_REQUIRE_CUDA=cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\r\nNCCL_VERSION=2.17.1-1\r\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\r\nNVIDIA_PRODUCT_NAME=CUDA\r\nNVIDIA_CUDA_END_OF_LIFE=1\r\nCUDA_VERSION=12.1.0\r\nLD_LIBRARY_PATH=/usr/local/lib/python3.10/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nCUDA_MODULE_LOADING=LAZY\r\n\r\n```\r\n\r\n\r\n### How would you like to use vllm\r\n\r\nI have modified parts of the vLLM source code, which has led to compilation and execution issues across different NVIDIA GPU models. Specifically:\r\n\r\n- Compiling on A10, does not run on A100/A800.\r\n\r\n- Compiling on A800, runs on A10 and A100 but fails on H800.\r\n\r\nI am seeking guidance on how to compile the code so that it can run successfully on any of the GPUs mentioned above.\r\n\r\nError Message:\r\n\r\n`[rank0]:   File \"vllm/model_executor/layers/linear.py\", line 135, in apply<br />\r\n[rank0]:     return F.linear(x, layer.weight, bias)<br />\r\n[rank0]: RuntimeError: CUDA error: no kernel image is available for execution on the device<br />\r\n[rank0]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.`\r\n\r\nQuestion:\r\nHow should I compile the code to ensure compatibility across all specified GPU models? Are there specific flags or configurations that need to be used during compilation?\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-10T02:49:36+00:00",
    "closed_at": "2025-05-17T02:09:03+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11914/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11914"
  },
  {
    "number": 16632,
    "title": "[Usage]: [swift with vllm and only using vllm serve]  leads to different result\uff0810% diff\uff09",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\n```\n **~ $ python collect_env.py**\nINFO 04-15 11:43:18 __init__.py:190] Automatically detected platform cuda.\nCollecting environment information...\n/home/work/py39/lib/python3.9/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\nPyTorch version: 2.5.1+cu118\nIs debug build: False\nCUDA used to build PyTorch: 11.8\nROCM used to build PyTorch: N/A\n\nOS: CentOS Linux 7 (Core) (x86_64)\nGCC version: (GCC) 12.1.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.17\n\nPython version: 3.9.2 (default, Mar  3 2021, 20:02:32)  [GCC 7.3.0] (64-bit runtime)\nPython platform: Linux-5.10.0-1.0.0.34-x86_64-with-glibc2.17\nIs CUDA available: True\nCUDA runtime version: 10.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: NVIDIA A10\nGPU 1: NVIDIA A10\n\nNvidia driver version: 535.154.05\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:          x86_64\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                256\nOn-line CPU(s) list:   0-255\nThread(s) per core:    2\nCore(s) per socket:    64\nSocket(s):             2\nNUMA node(s):          2\nVendor ID:             AuthenticAMD\nCPU family:            25\nModel:                 1\nModel name:            AMD EPYC 7W83 64-Core Processor\nStepping:              1\nCPU MHz:               3200.910\nCPU max MHz:           3673.0950\nCPU min MHz:           1500.0000\nBogoMIPS:              5090.40\nVirtualization:        AMD-V\nL1d cache:             32K\nL1i cache:             32K\nL2 cache:              512K\nL3 cache:              32768K\nNUMA node0 CPU(s):     0-63,128-191\nNUMA node1 CPU(s):     64-127,192-255\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif umip pku vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu11==11.11.3.6\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu11==11.8.87\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu11==11.8.89\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu11==11.8.89\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu11==9.1.0.70\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu11==10.9.0.58\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu11==10.3.0.86\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu11==11.4.1.48\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu11==11.7.5.86\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu11==2.21.5\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu11==11.8.86\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] onnx==1.16.2\n[pip3] onnxruntime==1.19.0\n[pip3] pynvml==12.0.0\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.5.1+cu118\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1+cu118\n[pip3] transformers==4.49.0.dev0\n[pip3] transformers-stream-generator==0.0.5\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n\tGPU0\tGPU1\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tSYS\tSYS\t0-63,128-191\t0\t\tN/A\nGPU1\tSYS\t X \tSYS\t0-63,128-191\t0\t\tN/A\nNIC0\tSYS\tSYS\t X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_bond_0\n\nCUDA_HOME=/home/work/cuda-11.8\nCUDA_HOME=/home/work/cuda-11.8\nLD_LIBRARY_PATH=/home/work/py39/lib/python3.9/site-packages/cv2/../../lib64:/home/work/py39/lib/python3.9/site-packages/nvidia/nvjitlink/lib:\nMKL_DEBUG_CPU_TYPE=5\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n\n### How would you like to use vllm\n\nmy model base is qwen2.5vl-7b and we trained it with swift, and when i deploy with swift (vllm accelerate) and use vllm serve\uff0c it lead to different result\n**The task is to determine whether the two signboards are the signboards of the same store.**\n**swift deploy script\uff1a**\nsame Accuracy \uff1a 0.9695121951219512 =159/164\nsame Recall 0.7871287128712872 =159/202\nclient\uff1a\n`\n/root/paddlejob/workspace/env_run/yinzilong/miniconda3_vllm/bin/swift deploy \\\n    --adapters /root/paddlejob/workspace/env_run/yinzilong/panchong/v3-20250312-205937/checkpoint-20000 \\\n    --infer_backend vllm \\\n    --temperature 0 \\\n    --gpu_memory_utilization 0.9 \\\n    --max_new_tokens 2048 \\\n    --tensor_parallel_size 1 \\\n    --logprobs true \\\n    --limit_mm_per_prompt '{\"image\": 2}' \\\n    --served_model_name 'qwen2_5_vl' \\\n    --host '10.96.206.107' \\\n    --port 8601\n`\nquery\uff1a\n`\ndef infer_batch(engine: 'InferEngine', infer_requests: List['InferRequest']):\n    request_config = RequestConfig(max_tokens=512, temperature=0, logprobs=True, top_logprobs=2)\n    metric = InferStats()\n    resp_list = engine.infer(infer_requests, request_config, metrics=[metric])\n    query0 = infer_requests[0].messages[0]['content']\n    # print(f'query0: {query0}')\n    # print(f'response0: {resp_list[0].choices[0].message.content}')\n    # print(f'metric: {metric.compute()}')\n    metric.reset()  # reuse\n    return resp_list\nif pic1_url == \"\" and pic2_url == \"\":\n    infer_requests = [InferRequest(**get_data('text', query, []))]\nelif pic1_url != \"\" and pic2_url == \"\":\n    bid_image = get_pic_whole(pic1_url, pic1_ori_pos)\n    infer_requests = [InferRequest(**get_data('image', query, [bid_image]))]\nelif pic1_url == \"\" and pic2_url != \"\":\n    bid_image = get_pic_whole(pic2_url, pic2_ori_pos)\n    infer_requests = [InferRequest(**get_data('image', query, [bid_image]))]\nelse:\n    bid_image1 = get_pic_whole(pic1_url, pic1_ori_pos)\n    bid_image2 = get_pic_whole(pic2_url, pic2_ori_pos)\n    infer_requests = [InferRequest(**get_data('image', query, [bid_image1, bid_image2]))]\nresp_list = infer_batch(self.engine, infer_requests)\nresponse_content = resp_list[0].choices[0].message.content\nlogprobs = resp_list[0].choices[0].logprobs\n`\n**when i use vllm**\nsame Accuracy\uff1a 0.8835978835978836 =167/189\nsame Recall: 0.8267326732673267 =167/202\n\nclient\n`\nVLLM_USE_V1=0 \\\nVLLM_WORKER_MULTIPROC_METHOD=spawn \\\n/root/paddlejob/workspace/env_run/yinzilong/miniconda3_vllm/bin/vllm serve \\\n/root/paddlejob/workspace/env_run/yinzilong/panchong/output/v3-20250312-205937/checkpoint-20000-merged \\\n--gpu-memory-utilization 0.9 \\\n--swap-space 8 \\\n--max-model-len 2048 \\\n--tensor-parallel-size 1 \\\n--host 10.95.192.17 \\\n--port 8601 \\\n--limit-mm-per-prompt \"image=2\" \n`\nquery with openai:\n`\nresp = self.client.chat.completions.create(model=self.model, \n                                                       messages=messages, \n                                                       max_tokens=10240, \n                                                       temperature=0,\n                                                        logprobs=True,\n                                                        top_logprobs=1)\n`\nit also happens\uff1aCompare whit swift request and swift deploy\uff0c**Requests are much slower**  when we use vllm serve and openai request.\n\nLooking forward to your reply \ud83e\udd7a thanks\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-15T03:59:34+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16632/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16632"
  },
  {
    "number": 7004,
    "title": "[Bug]: Mistral Nemo Instruct almost never returns JSON when using `guided_json`",
    "body": "### Your current environment\r\n\r\n```text\r\nThe output of `python collect_env.py`\r\nCollecting environment information...\r\nPyTorch version: 2.3.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Amazon Linux 2 (x86_64)\r\nGCC version: (GCC) 7.3.1 20180712 (Red Hat 7.3.1-17)\r\nClang version: Could not collect\r\nCMake version: version 3.27.7\r\nLibc version: glibc-2.26\r\n\r\nPython version: 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:20:04) [GCC 11.3.0] (64-bit runtime)\r\nPython platform: Linux-5.10.220-209.869.amzn2.x86_64-x86_64-with-glibc2.26\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA L4\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              8\r\nOn-line CPU(s) list: 0-7\r\nThread(s) per core:  2\r\nCore(s) per socket:  4\r\nSocket(s):           1\r\nNUMA node(s):        1\r\nVendor ID:           AuthenticAMD\r\nCPU family:          25\r\nModel:               1\r\nModel name:          AMD EPYC 7R13 Processor\r\nStepping:            1\r\nCPU MHz:             3634.954\r\nBogoMIPS:            5300.00\r\nHypervisor vendor:   KVM\r\nVirtualization type: full\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            512K\r\nL3 cache:            16384K\r\nNUMA node0 CPU(s):   0-7\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.9.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.2\r\n[pip3] nvidia-nccl-cu11==2.14.3\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.1\r\n[pip3] torchvision==0.18.1\r\n[pip3] triton==2.3.1\r\n[pip3] vllm-nccl-cu12==2.18.1.0.4.0\r\n[conda] numpy                     1.26.2                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu11          2.14.3                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.1                    pypi_0    pypi\r\n[conda] torchvision               0.18.1                   pypi_0    pypi\r\n[conda] triton                    2.3.1                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.4.0             pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.3.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \t0-7\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI am using `vLLM` and `Nemo Instruct` and playing around with the new model (fp8) and even with vLLMs' `guided_json` and including the schema in prompt, nemo almost never returns a output if JSON is involved however it does finish generating a output that is usually a letter or a symbol at the most.\r\n\r\nDoes Mistral Nemo not support JSON output with vLLM?\r\nThis works as expected with instruct-v0.3\r\n\r\nThis is what I get using the inference API on the hugging face model card\r\n\r\nwrite a sample JSON response for a user info query, \r\n\r\n```\r\n{\r\n\"status\": \"success\",\r\n\"data\": {\r\n\"user_id\": 1,\r\n\"name\": \"Julien\",\r\n\"email\": \"julien@example.com\",\r\n\"created_at\": \"2022-01-01T00:00:00Z\",\r\n\"updated_at\": \"2022-01-02T00:00:00Z\"\r\n}\r\n}\r\n```\r\n\r\n\r\n",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-31T20:31:11+00:00",
    "closed_at": "2024-12-01T02:14:18+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7004/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7004"
  },
  {
    "number": 15131,
    "title": "[Usage]: relationship between embedding size and vocab_size",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\n```\n\n\n### How would you like to use vllm\n\nI\u2019ve noticed that the embedding size is always smaller than the vocab_size. Additionally, sometimes the `prompt_token_ids` are larger than the embedding size. \u200bIs there a way to map the embedding vector to each of the prompt tokens so that I can retrieve the logit of a prompt token like this:\n`embeds[i, labels[i]]`?\n\n```python\noutputs = llm.encode(prompts)\nprint(f'vocab_size: {llm.get_tokenizer().vocab_size}')\nfor i in range(len(outputs)):\n    labels = outputs[i].prompt_token_ids[1:]\n    embeds = outputs[i].outputs.data\n    print(f'{i}-th prompt_token_ids: {labels}')\n    print(f'{i}-th embeddings: {embeds.shape}')\n```\n\n```log\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 55.18it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nvocab_size: 50254\n0-th prompt_token_ids: [4007, 273, 253, 1986, 2077, 310]\n0-th embeddings: torch.Size([7, 2560])\n1-th prompt_token_ids: [13, 619, 1416, 310]\n1-th embeddings: torch.Size([5, 2560])\n2-th prompt_token_ids: [5347, 273, 6181, 310]\n2-th embeddings: torch.Size([5, 2560])\n3-th prompt_token_ids: [2852, 273, 14980, 310]\n3-th embeddings: torch.Size([5, 2560])\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-19T14:02:44+00:00",
    "closed_at": null,
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15131/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15131"
  },
  {
    "number": 11191,
    "title": "[Installation]: Installing vllm in GH200 machine (aarch64) causes problems with cusparse.h missing",
    "body": "### Your current environment\n\nI cannot run collect_env.py since that would require vllm\r\n<img width=\"414\" alt=\"Screenshot 2024-12-13 at 9 18 01\u202fPM\" src=\"https://github.com/user-attachments/assets/94ae6cdf-7a0b-4727-9650-d9f4d9599e4e\" />\r\n\r\n\n\n### How you are installing vllm\n\nI am following the instructions from here:\r\n[https://docs.vllm.ai/en/stable/getting_started/installation.html#use-an-existing-pytorch-installation](https://docs.vllm.ai/en/stable/getting_started/installation.html?fbclid=IwZXh0bgNhZW0CMTAAAR0rKk7-u-dGjP9zdYYSFVpbj0REfhwjOhFgzrLC2DWeQDb5D1KbQFy-xLQ_aem_aEMTM-Po9v5WOAzcqzVmlg#use-an-existing-pytorch-installation)\r\n\r\nProblem I am facing:\r\n\r\n```\r\npip install . --verbose --no-build-isolation\r\nUsing pip 24.3.1 from /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/pip (python 3.10)\r\nProcessing /work/nvme/bcfp/ftajwar/vllm\r\n  Running command Preparing metadata (pyproject.toml)\r\n  running dist_info\r\n  creating /tmp/pip-modern-metadata-nus2ddwg/vllm.egg-info\r\n  writing /tmp/pip-modern-metadata-nus2ddwg/vllm.egg-info/PKG-INFO\r\n  writing dependency_links to /tmp/pip-modern-metadata-nus2ddwg/vllm.egg-info/dependency_links.txt\r\n  writing entry points to /tmp/pip-modern-metadata-nus2ddwg/vllm.egg-info/entry_points.txt\r\n  writing requirements to /tmp/pip-modern-metadata-nus2ddwg/vllm.egg-info/requires.txt\r\n  writing top-level names to /tmp/pip-modern-metadata-nus2ddwg/vllm.egg-info/top_level.txt\r\n  writing manifest file '/tmp/pip-modern-metadata-nus2ddwg/vllm.egg-info/SOURCES.txt'\r\n  reading manifest template 'MANIFEST.in'\r\n  adding license file 'LICENSE'\r\n  writing manifest file '/tmp/pip-modern-metadata-nus2ddwg/vllm.egg-info/SOURCES.txt'\r\n  creating '/tmp/pip-modern-metadata-nus2ddwg/vllm-0.6.4.post2.dev359+g4863e5fb.d20241214.cu123.dist-info'\r\n  Preparing metadata (pyproject.toml) ... done\r\nRequirement already satisfied: psutil in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (6.1.0)\r\nRequirement already satisfied: sentencepiece in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.2.0)\r\nRequirement already satisfied: numpy<2.0.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (1.26.3)\r\nRequirement already satisfied: requests>=2.26.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (2.32.3)\r\nRequirement already satisfied: tqdm in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (4.67.1)\r\nCollecting blake3 (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123)\r\n  Obtaining dependency information for blake3 from https://files.pythonhosted.org/packages/1a/9c/bf926c668066ab3875d2ac588d1c29424324303aad9138f6476370753348/blake3-1.0.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata\r\n  Using cached blake3-1.0.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (4.1 kB)\r\nRequirement already satisfied: py-cpuinfo in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (9.0.0)\r\nRequirement already satisfied: transformers>=4.45.2 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (4.46.3)\r\nRequirement already satisfied: tokenizers>=0.19.1 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.20.3)\r\nRequirement already satisfied: protobuf in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (5.29.1)\r\nRequirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.115.6)\r\nRequirement already satisfied: aiohttp in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (3.11.10)\r\nRequirement already satisfied: openai>=1.45.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (1.57.2)\r\nRequirement already satisfied: uvicorn[standard] in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.32.1)\r\nRequirement already satisfied: pydantic>=2.9 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (2.10.3)\r\nRequirement already satisfied: pillow in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (10.4.0)\r\nRequirement already satisfied: prometheus_client>=0.18.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.21.1)\r\nRequirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (7.0.0)\r\nRequirement already satisfied: tiktoken>=0.6.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.7.0)\r\nRequirement already satisfied: lm-format-enforcer<0.11,>=0.10.9 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.10.9)\r\nCollecting outlines==0.1.9 (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123)\r\n  Obtaining dependency information for outlines==0.1.9 from https://files.pythonhosted.org/packages/9f/24/ffa97943aee9e7f1cdad76f57ab7679df283c593be0fbdd5eda711a4d705/outlines-0.1.9-py3-none-any.whl.metadata\r\n  Using cached outlines-0.1.9-py3-none-any.whl.metadata (17 kB)\r\nRequirement already satisfied: typing_extensions>=4.10 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (4.12.2)\r\nRequirement already satisfied: filelock>=3.16.1 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (3.16.1)\r\nRequirement already satisfied: partial-json-parser in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.2.1.1.post4)\r\nRequirement already satisfied: pyzmq in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (26.2.0)\r\nRequirement already satisfied: msgspec in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.18.6)\r\nRequirement already satisfied: gguf==0.10.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.10.0)\r\nRequirement already satisfied: importlib_metadata in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (8.5.0)\r\nRequirement already satisfied: mistral_common>=1.5.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from mistral_common[opencv]>=1.5.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (1.5.1)\r\nRequirement already satisfied: pyyaml in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (6.0.2)\r\nRequirement already satisfied: einops in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.8.0)\r\nRequirement already satisfied: compressed-tensors==0.8.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.8.0)\r\nRequirement already satisfied: ray>=2.9 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (2.40.0)\r\nRequirement already satisfied: nvidia-ml-py>=12.560.30 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (12.560.30)\r\nRequirement already satisfied: torch>=1.7.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from compressed-tensors==0.8.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (2.6.0.dev20241210+cu124)\r\nRequirement already satisfied: interegular in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from outlines==0.1.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.3.3)\r\nRequirement already satisfied: jinja2 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from outlines==0.1.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (3.1.3)\r\nRequirement already satisfied: lark in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from outlines==0.1.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (1.2.2)\r\nRequirement already satisfied: nest_asyncio in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from outlines==0.1.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (1.6.0)\r\nRequirement already satisfied: cloudpickle in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from outlines==0.1.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (3.1.0)\r\nRequirement already satisfied: diskcache in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from outlines==0.1.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (5.6.3)\r\nRequirement already satisfied: referencing in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from outlines==0.1.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.35.1)\r\nRequirement already satisfied: jsonschema in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from outlines==0.1.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (4.23.0)\r\nRequirement already satisfied: pycountry in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from outlines==0.1.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (24.6.1)\r\nRequirement already satisfied: airportsdata in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from outlines==0.1.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (20241001)\r\nCollecting outlines_core==0.1.24 (from outlines==0.1.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123)\r\n  Using cached outlines_core-0.1.24-cp310-cp310-linux_aarch64.whl\r\nRequirement already satisfied: starlette<0.42.0,>=0.40.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.41.3)\r\nRequirement already satisfied: packaging in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from lm-format-enforcer<0.11,>=0.10.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (24.2)\r\nRequirement already satisfied: opencv-python-headless<5.0.0,>=4.0.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from mistral_common[opencv]>=1.5.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (4.10.0.84)\r\nRequirement already satisfied: anyio<5,>=3.5.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from openai>=1.45.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (4.7.0)\r\nRequirement already satisfied: distro<2,>=1.7.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from openai>=1.45.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (1.9.0)\r\nRequirement already satisfied: httpx<1,>=0.23.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from openai>=1.45.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.28.1)\r\nRequirement already satisfied: jiter<1,>=0.4.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from openai>=1.45.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.8.2)\r\nRequirement already satisfied: sniffio in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from openai>=1.45.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (1.3.1)\r\nRequirement already satisfied: annotated-types>=0.6.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from pydantic>=2.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.7.0)\r\nRequirement already satisfied: pydantic-core==2.27.1 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from pydantic>=2.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (2.27.1)\r\nRequirement already satisfied: click>=7.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from ray>=2.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (8.1.7)\r\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from ray>=2.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (1.1.0)\r\nRequirement already satisfied: aiosignal in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from ray>=2.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (1.3.1)\r\nRequirement already satisfied: frozenlist in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from ray>=2.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (1.5.0)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from requests>=2.26.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (2.1.1)\r\nRequirement already satisfied: idna<4,>=2.5 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from requests>=2.26.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (3.4)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from requests>=2.26.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (1.26.13)\r\nRequirement already satisfied: certifi>=2017.4.17 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from requests>=2.26.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (2022.12.7)\r\nRequirement already satisfied: regex>=2022.1.18 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from tiktoken>=0.6.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (2024.11.6)\r\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from tokenizers>=0.19.1->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.26.5)\r\nRequirement already satisfied: safetensors>=0.4.1 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from transformers>=4.45.2->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.4.5)\r\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from aiohttp->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (2.4.4)\r\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from aiohttp->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (5.0.1)\r\nRequirement already satisfied: attrs>=17.3.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from aiohttp->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (24.2.0)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from aiohttp->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (6.1.0)\r\nRequirement already satisfied: propcache>=0.2.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from aiohttp->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.2.1)\r\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from aiohttp->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (1.18.3)\r\nRequirement already satisfied: zipp>=3.20 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from importlib_metadata->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (3.21.0)\r\nRequirement already satisfied: h11>=0.8 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from uvicorn[standard]->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.14.0)\r\nRequirement already satisfied: httptools>=0.6.3 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from uvicorn[standard]->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.6.4)\r\nRequirement already satisfied: python-dotenv>=0.13 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from uvicorn[standard]->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (1.0.1)\r\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from uvicorn[standard]->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.21.0)\r\nRequirement already satisfied: watchfiles>=0.13 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from uvicorn[standard]->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (1.0.3)\r\nRequirement already satisfied: websockets>=10.4 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from uvicorn[standard]->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (14.1)\r\nRequirement already satisfied: exceptiongroup>=1.0.2 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.45.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (1.2.2)\r\nRequirement already satisfied: httpcore==1.* in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.45.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (1.0.7)\r\nRequirement already satisfied: fsspec>=2023.5.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.1->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (2024.9.0)\r\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from jsonschema->outlines==0.1.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (2024.10.1)\r\nRequirement already satisfied: rpds-py>=0.7.1 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from jsonschema->outlines==0.1.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (0.22.3)\r\nRequirement already satisfied: sympy==1.13.1 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from torch>=1.7.0->compressed-tensors==0.8.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (1.13.1)\r\nRequirement already satisfied: networkx in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from torch>=1.7.0->compressed-tensors==0.8.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (3.2.1)\r\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.7.0->compressed-tensors==0.8.0->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (1.3.0)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages (from jinja2->outlines==0.1.9->vllm==0.6.4.post2.dev359+g4863e5fb.d20241214.cu123) (2.1.5)\r\nUsing cached outlines-0.1.9-py3-none-any.whl (87 kB)\r\nUsing cached blake3-1.0.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (354 kB)\r\nBuilding wheels for collected packages: vllm\r\n  Running command Building wheel for vllm (pyproject.toml)\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  copying vllm/_version.py -> build/lib.linux-aarch64-cpython-310/vllm\r\n  running build_ext\r\n  Using MAX_JOBS=1 as the number of jobs.\r\n  -- Build type: RelWithDebInfo\r\n  -- Target device: cuda\r\n  -- Found python matching: /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/bin/python3.10.\r\n  -- PyTorch: CUDA detected: 12.3\r\n  -- PyTorch: CUDA nvcc is: /opt/nvidia/hpc_sdk/Linux_aarch64/24.3/cuda/12.3/bin/nvcc\r\n  -- PyTorch: CUDA toolkit directory: /opt/nvidia/hpc_sdk/Linux_aarch64/24.3/cuda/12.3\r\n  -- PyTorch: Header version is: 12.3\r\n  -- Found Python: /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/bin/python3.10 (found version \"3.10.16\") found components: Interpreter\r\n  CMake Warning at /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/torch/share/cmake/Caffe2/public/cuda.cmake:140 (message):\r\n    Failed to compute shorthash for libnvrtc.so\r\n  Call Stack (most recent call first):\r\n    /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\r\n    /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\r\n    CMakeLists.txt:84 (find_package)\r\n\r\n\r\n  CMake Warning (dev) at /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/cmake/data/share/cmake-3.31/Modules/FindPackageHandleStandardArgs.cmake:441 (message):\r\n    The package name passed to `find_package_handle_standard_args` (nvtx3) does\r\n    not match the name of the calling package (Caffe2).  This can lead to\r\n    problems in calling code that expects `find_package` result variables\r\n    (e.g., `_FOUND`) to follow a certain pattern.\r\n  Call Stack (most recent call first):\r\n    /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/torch/share/cmake/Caffe2/public/cuda.cmake:178 (find_package_handle_standard_args)\r\n    /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\r\n    /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\r\n    CMakeLists.txt:84 (find_package)\r\n  This warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n  -- Could NOT find nvtx3 (missing: nvtx3_dir)\r\n  CMake Warning at /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/torch/share/cmake/Caffe2/public/cuda.cmake:184 (message):\r\n    Cannot find NVTX3, find old NVTX instead\r\n  Call Stack (most recent call first):\r\n    /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\r\n    /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\r\n    CMakeLists.txt:84 (find_package)\r\n\r\n\r\n  -- USE_CUDNN is set to 0. Compiling without cuDNN support\r\n  -- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support\r\n  -- USE_CUDSS is set to 0. Compiling without cuDSS support\r\n  -- USE_CUFILE is set to 0. Compiling without cuFile support\r\n  -- Autodetected CUDA architecture(s):  9.0\r\n  -- Added CUDA NVCC flags for: -gencode;arch=compute_90,code=sm_90\r\n  CMake Warning at /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\r\n    static library kineto_LIBRARY-NOTFOUND not found.\r\n  Call Stack (most recent call first):\r\n    /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:121 (append_torchlib_if_found)\r\n    CMakeLists.txt:84 (find_package)\r\n\r\n\r\n  CMake Warning at CMakeLists.txt:107 (message):\r\n    Pytorch version 2.5.1 expected for CUDA build, saw 2.6.0 instead.\r\n\r\n\r\n  -- CUDA target architectures: 9.0\r\n  -- CUDA supported target architectures: 9.0\r\n  -- FetchContent base directory: /work/nvme/bcfp/ftajwar/vllm/.deps\r\n  -- CMake Version: 3.31.1\r\n  -- CUTLASS 3.5.1\r\n  -- CUDART: /opt/nvidia/hpc_sdk/Linux_aarch64/24.3/cuda/12.3/lib64/libcudart.so\r\n  -- CUDA Driver: /opt/nvidia/hpc_sdk/Linux_aarch64/24.3/cuda/12.3/lib64/stubs/libcuda.so\r\n  -- NVRTC: /opt/nvidia/hpc_sdk/Linux_aarch64/24.3/cuda/12.3/lib64/libnvrtc.so\r\n  -- Default Install Location: install\r\n  -- Make cute::tuple be the new standard-layout tuple type\r\n  -- CUDA Compilation Architectures: 70;72;75;80;86;87;89;90;90a\r\n  -- Enable caching of reference results in conv unit tests\r\n  -- Enable rigorous conv problem sizes in conv unit tests\r\n  -- Using NVCC flags: --expt-relaxed-constexpr;-DCUTE_USE_PACKED_TUPLE=1;-DCUTLASS_TEST_LEVEL=0;-DCUTLASS_TEST_ENABLE_CACHED_RESULTS=1;-DCUTLASS_CONV_UNIT_TEST_RIGOROUS_SIZE_ENABLED=1;-DCUTLASS_DEBUG_TRACE_LEVEL=0;-Xcompiler=-Wconversion;-Xcompiler=-fno-strict-aliasing;-lineinfo\r\n  -- Configuring cublas ...\r\n  -- cuBLAS Disabled.\r\n  -- Configuring cuBLAS ... done.\r\n  -- Building Marlin kernels for archs: 9.0\r\n  -- Building scaled_mm_c3x for archs: 9.0a;9.0\r\n  -- Not building scaled_mm_c2x as all archs are already built for and covered by scaled_mm_c3x\r\n  -- Machete generation script hash: c5413e168eabdf729344fe0f89eb8aa2\r\n  -- Last run machete generate script hash: c5413e168eabdf729344fe0f89eb8aa2\r\n  -- Machete generation script has not changed, skipping generation.\r\n  -- Building Machete kernels for archs: 9.0a\r\n  -- Enabling C extension.\r\n  -- Building Marlin MOE kernels for archs: 9.0\r\n  -- Enabling moe extension.\r\n  -- Build type: RelWithDebInfo\r\n  -- Target device: cuda\r\n  -- Found Python: /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/bin/python3.10 (found version \"3.10.16\") found components: Interpreter Development.Module Development.SABIModule\r\n  -- Building vllm-flash-attn inside vLLM. Skipping flag detection and relying on parent build.\r\n  -- vllm-flash-attn is available at /work/nvme/bcfp/ftajwar/vllm/.deps/vllm-flash-attn-src\r\n  -- Configuring done (6.3s)\r\n  -- Generating done (0.1s)\r\n  -- Build files have been written to: /work/nvme/bcfp/ftajwar/vllm/build/temp.linux-aarch64-cpython-310\r\n  Using MAX_JOBS=1 as the number of jobs.\r\n  [1/115] Building CUDA object CMakeFiles/_moe_C.dir/csrc/moe/moe_align_sum_kernels.cu.o\r\n  FAILED: CMakeFiles/_moe_C.dir/csrc/moe/moe_align_sum_kernels.cu.o\r\n  /opt/nvidia/hpc_sdk/Linux_aarch64/24.3/cuda/12.3/bin/nvcc -forward-unknown-to-host-compiler -DPy_LIMITED_API=3 -DTORCH_EXTENSION_NAME=_moe_C -DUSE_C10D_GLOO -DUSE_C10D_NCCL -DUSE_DISTRIBUTED -DUSE_RPC -DUSE_TENSORPIPE -D_moe_C_EXPORTS -I/work/nvme/bcfp/ftajwar/vllm/csrc -isystem /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/include/python3.10 -isystem /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/torch/include -isystem /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/nvidia/hpc_sdk/Linux_aarch64/24.3/cuda/12.3/include -DONNX_NAMESPACE=onnx_c2 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda -O2 -g -DNDEBUG -std=c++17 -Xcompiler=-fPIC --expt-relaxed-constexpr -DENABLE_FP8 --threads=1 -DENABLE_SCALED_MM_C3X=1 -D_GLIBCXX_USE_CXX11_ABI=1 -gencode arch=compute_90,code=sm_90 -MD -MT CMakeFiles/_moe_C.dir/csrc/moe/moe_align_sum_kernels.cu.o -MF CMakeFiles/_moe_C.dir/csrc/moe/moe_align_sum_kernels.cu.o.d -x cu -c /work/nvme/bcfp/ftajwar/vllm/csrc/moe/moe_align_sum_kernels.cu -o CMakeFiles/_moe_C.dir/csrc/moe/moe_align_sum_kernels.cu.o\r\n  In file included from /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/torch/include/ATen/cuda/CUDAContext.h:3,\r\n                   from /work/nvme/bcfp/ftajwar/vllm/csrc/moe/moe_align_sum_kernels.cu:2:\r\n  /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/torch/include/ATen/cuda/CUDAContextLight.h:7:10: fatal error: cusparse.h: No such file or directory\r\n      7 | #include <cusparse.h>\r\n        |          ^~~~~~~~~~~~\r\n  compilation terminated.\r\n  ninja: build stopped: subcommand failed.\r\n  Traceback (most recent call last):\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\r\n      main()\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\r\n      json_out['return_val'] = hook(**hook_input['kwargs'])\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 251, in build_wheel\r\n      return _build_backend().build_wheel(wheel_directory, config_settings,\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/build_meta.py\", line 438, in build_wheel\r\n      return _build(['bdist_wheel', '--dist-info-dir', str(metadata_directory)])\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/build_meta.py\", line 426, in _build\r\n      return self._build_with_temp_dir(\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/build_meta.py\", line 407, in _build_with_temp_dir\r\n      self.run_setup()\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/build_meta.py\", line 320, in run_setup\r\n      exec(code, locals())\r\n    File \"<string>\", line 600, in <module>\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/__init__.py\", line 117, in setup\r\n      return distutils.core.setup(**attrs)\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 183, in setup\r\n      return run_commands(dist)\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 199, in run_commands\r\n      dist.run_commands()\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 954, in run_commands\r\n      self.run_command(cmd)\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/dist.py\", line 995, in run_command\r\n      super().run_command(command)\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\r\n      cmd_obj.run()\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/command/bdist_wheel.py\", line 381, in run\r\n      self.run_command(\"build\")\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\r\n      self.distribution.run_command(command)\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/dist.py\", line 995, in run_command\r\n      super().run_command(command)\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\r\n      cmd_obj.run()\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/_distutils/command/build.py\", line 135, in run\r\n      self.run_command(cmd_name)\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\r\n      self.distribution.run_command(command)\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/dist.py\", line 995, in run_command\r\n      super().run_command(command)\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\r\n      cmd_obj.run()\r\n    File \"<string>\", line 238, in run\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/command/build_ext.py\", line 99, in run\r\n      _build_ext.run(self)\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\r\n      self.build_extensions()\r\n    File \"<string>\", line 212, in build_extensions\r\n    File \"/work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/subprocess.py\", line 369, in check_call\r\n      raise CalledProcessError(retcode, cmd)\r\n  subprocess.CalledProcessError: Command '['cmake', '--build', '.', '-j=1', '--target=_moe_C', '--target=vllm_flash_attn_c', '--target=_C']' returned non-zero exit status 1.\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Building wheel for vllm (pyproject.toml) did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> See above for output.\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  full command: /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/bin/python3.10 /work/nvme/bcfp/ftajwar/anaconda3/envs/exploration/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py build_wheel /tmp/tmph8nwqrko\r\n  cwd: /work/nvme/bcfp/ftajwar/vllm\r\n  Building wheel for vllm (pyproject.toml) ... error\r\n  ERROR: Failed building wheel for vllm\r\nFailed to build vllm\r\nERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (vllm)\r\n```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-14T02:18:26+00:00",
    "closed_at": "2025-04-18T02:06:38+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11191/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11191"
  }
]
[
  {
    "number": 6199,
    "title": "[Bug]: safetensor format support for Mistral-7B-v0.3",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.3\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-176-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 11.8.89\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA A100-SXM4-40GB\r\nGPU 1: NVIDIA A100-SXM4-40GB\r\nGPU 2: NVIDIA A100-SXM4-40GB\r\nGPU 3: NVIDIA A100-SXM4-40GB\r\nGPU 4: NVIDIA A100-SXM4-40GB\r\nGPU 5: NVIDIA A100-SXM4-40GB\r\nGPU 6: NVIDIA A100-SXM4-40GB\r\nGPU 7: NVIDIA A100-SXM4-40GB\r\n\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      43 bits physical, 48 bits virtual\r\nCPU(s):                             256\r\nOn-line CPU(s) list:                0-255\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 64\r\nSocket(s):                          2\r\nNUMA node(s):                       8\r\nVendor ID:                          AuthenticAMD\r\nCPU family:                         23\r\nModel:                              49\r\nModel name:                         AMD EPYC 7742 64-Core Processor\r\nStepping:                           0\r\nFrequency boost:                    enabled\r\nCPU MHz:                            3402.618\r\nCPU max MHz:                        2250.0000\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           4491.57\r\nVirtualization:                     AMD-V\r\nL1d cache:                          4 MiB\r\nL1i cache:                          4 MiB\r\nL2 cache:                           64 MiB\r\nL3 cache:                           512 MiB\r\nNUMA node0 CPU(s):                  0-15,128-143\r\nNUMA node1 CPU(s):                  16-31,144-159\r\nNUMA node2 CPU(s):                  32-47,160-175\r\nNUMA node3 CPU(s):                  48-63,176-191\r\nNUMA node4 CPU(s):                  64-79,192-207\r\nNUMA node5 CPU(s):                  80-95,208-223\r\nNUMA node6 CPU(s):                  96-111,224-239\r\nNUMA node7 CPU(s):                  112-127,240-255\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Vulnerable\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip rdpid overflow_recov succor smca sme sev sev_es\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] torchmetrics==1.4.0.post0\r\n[pip3] torchvision==0.18.0\r\n[pip3] transformers==4.42.3\r\n[pip3] transformers-stream-generator==0.0.5\r\n[pip3] triton==2.3.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] torchmetrics              1.4.0.post0              pypi_0    pypi\r\n[conda] torchvision               0.18.0                   pypi_0    pypi\r\n[conda] transformers              4.42.3                   pypi_0    pypi\r\n[conda] transformers-stream-generator 0.0.5                    pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tNIC4\tNIC5\tNIC6\tNIC7\tNIC8\tNIC9\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t48-63,176-191\t3\t\tN/A\r\nGPU1\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t48-63,176-191\t3\t\tN/A\r\nGPU2\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t16-31,144-159\t1\t\tN/A\r\nGPU3\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t16-31,144-159\t1\t\tN/A\r\nGPU4\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\t112-127,240-255\t7\t\tN/A\r\nGPU5\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\t112-127,240-255\t7\t\tN/A\r\nGPU6\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\t80-95,208-223\t5\t\tN/A\r\nGPU7\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\t80-95,208-223\t5\t\tN/A\r\nNIC0\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\r\nNIC1\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\t X \tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\r\nNIC2\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\r\nNIC3\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\t X \tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\r\nNIC4\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPXB\tSYS\tSYS\tSYS\tSYS\r\nNIC5\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\t X \tSYS\tSYS\tSYS\tSYS\r\nNIC6\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPXB\tSYS\tSYS\r\nNIC7\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\t X \tSYS\tSYS\r\nNIC8\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPIX\r\nNIC9\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPIX\t X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  NIC8: mlx5_8\r\n  NIC9: mlx5_9\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nI've downloaded `model-0000{1,2,3}-of-00003.safetensors`.\r\nBut when I was loading the model with vllm, I still need to download `consolidated.safetensors`. The case didn't show on other model series, so I think it's a bug.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-08T02:40:38+00:00",
    "closed_at": "2024-11-25T02:05:13+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6199/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6199"
  },
  {
    "number": 4247,
    "title": "[Bug]: KeyError: 'model.layers.45.block_sparse_moe.gate.g_idx'",
    "body": "### Your current environment\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.2.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-25-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.99\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA GeForce RTX 2080 Ti\r\nGPU 1: NVIDIA GeForce RTX 2080 Ti\r\nGPU 2: NVIDIA GeForce RTX 2080 Ti\r\nGPU 3: NVIDIA GeForce RTX 2080 Ti\r\nGPU 4: NVIDIA GeForce RTX 2080 Ti\r\nGPU 5: NVIDIA GeForce RTX 2080 Ti\r\nGPU 6: NVIDIA GeForce RTX 2080 Ti\r\nGPU 7: NVIDIA GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 535.161.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\n\u67b6\u6784\uff1a                              x86_64\r\nCPU \u8fd0\u884c\u6a21\u5f0f\uff1a                      32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\n\u5b57\u8282\u5e8f\uff1a                            Little Endian\r\nCPU:                                48\r\n\u5728\u7ebf CPU \u5217\u8868\uff1a                     0-47\r\n\u5382\u5546 ID\uff1a                           GenuineIntel\r\n\u578b\u53f7\u540d\u79f0\uff1a                          Intel(R) Xeon(R) Gold 6136 CPU @ 3.00GHz\r\nCPU \u7cfb\u5217\uff1a                          6\r\n\u578b\u53f7\uff1a                              85\r\n\u6bcf\u4e2a\u6838\u7684\u7ebf\u7a0b\u6570\uff1a                    2\r\n\u6bcf\u4e2a\u5ea7\u7684\u6838\u6570\uff1a                      12\r\n\u5ea7\uff1a                                2\r\n\u6b65\u8fdb\uff1a                              4\r\nCPU \u6700\u5927 MHz\uff1a                      3700.0000\r\nCPU \u6700\u5c0f MHz\uff1a                      1200.0000\r\nBogoMIPS\uff1a                          6000.00\r\n\u6807\u8bb0\uff1a                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear flush_l1d arch_capabilities\r\nL1d \u7f13\u5b58\uff1a                          768 KiB (24 instances)\r\nL1i \u7f13\u5b58\uff1a                          768 KiB (24 instances)\r\nL2 \u7f13\u5b58\uff1a                           24 MiB (24 instances)\r\nL3 \u7f13\u5b58\uff1a                           49.5 MiB (2 instances)\r\nNUMA \u8282\u70b9\uff1a                         2\r\nNUMA \u8282\u70b90 CPU\uff1a                    0-11,24-35\r\nNUMA \u8282\u70b91 CPU\uff1a                    12-23,36-47\r\nVulnerability Gather data sampling: Mitigation; Microcode\r\nVulnerability Itlb multihit:        KVM: Mitigation: VMX unsupported\r\nVulnerability L1tf:                 Mitigation; PTE Inversion\r\nVulnerability Mds:                  Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Meltdown:             Mitigation; PTI\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:             Mitigation; IBRS\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; IBRS, IBPB conditional, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Mitigation; Clear CPU buffers; SMT vulnerable\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.19.3\r\n[pip3] torch==2.2.1\r\n[pip3] torchaudio==2.2.2\r\n[pip3] torchvision==0.17.2\r\n[pip3] triton==2.2.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.3.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.19.3                   pypi_0    pypi\r\n[conda] torch                     2.2.1                    pypi_0    pypi\r\n[conda] torchaudio                2.2.2                    pypi_0    pypi\r\n[conda] torchvision               0.17.2                   pypi_0    pypi\r\n[conda] triton                    2.2.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.3.0             pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV2     NODE    NODE    SYS     SYS     SYS     SYS     0-11,24-35      0               N/A\r\nGPU1    NV2      X      NODE    NODE    SYS     SYS     SYS     SYS     0-11,24-35      0               N/A\r\nGPU2    NODE    NODE     X      NV2     SYS     SYS     SYS     SYS     0-11,24-35      0               N/A\r\nGPU3    NODE    NODE    NV2      X      SYS     SYS     SYS     SYS     0-11,24-35      0               N/A\r\nGPU4    SYS     SYS     SYS     SYS      X      NV2     NODE    NODE    12-23,36-47     1               N/A\r\nGPU5    SYS     SYS     SYS     SYS     NV2      X      NODE    NODE    12-23,36-47     1               N/A\r\nGPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      NV2     12-23,36-47     1               N/A\r\nGPU7    SYS     SYS     SYS     SYS     NODE    NODE    NV2      X      12-23,36-47     1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\r\npython -m vllm.entrypoints.openai.api_server --served-model-name=8x22b --model=/home/jarrelscy/Mixtral-8x22B-Instruct-v0.1-GPTQ-4bit --gpu-memory-utilizatio=0.95 --max-model-len=60000 --max-num-seqs=2 --tensor-parallel-size=8 --trust-remote-code --host=0.0.0.0 --port=8001 --max-log-len=1000\r\n\r\nKeyError: 'model.layers.45.block_sparse_moe.gate.g_idx'",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-21T23:20:50+00:00",
    "closed_at": "2024-11-29T02:06:39+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4247/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4247"
  },
  {
    "number": 11184,
    "title": "[Bug]: Bert tokenizer is tokenizing some tokens as `UNK`",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nWith some Bert and Roberta models like `sentence-transformers/all-MiniLM-L12-v2` I found that the output is not similar to the one generated by `sentence-transformers`. If I place the following prints in `_normalize_prompt_text_to_input()` in `serving_engine.py`\r\n```\r\n        print(f\"{input_ids=}\")\r\n```\r\nI get `[101, 100, 3007, 1997, 100, 2003, 100, 1012, 102]` for the sentence \"The capital of France is Paris.\". 100 is the `UNK` token.  When I run with sentence-transformers, I get `[ 101, 1996, 3007, 1997, 2605, 2003, 3000, 1012,  102]` . This problem happens both with `--tokenizer-mode auto` and `--tokenizer-mode slow`.\r\n\r\ncc: @DarkLight1337 \n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-12-13T21:33:18+00:00",
    "closed_at": "2025-01-09T03:05:45+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11184/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11184"
  },
  {
    "number": 13054,
    "title": "[Bug]: RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead. on Gaudi2",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nAutomatically detected platform hpu.\nCollecting environment information...\nPyTorch version: 2.5.1a0+git6fc067b\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-122-generic-x86_64-with-glibc2.35\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               160\nOn-line CPU(s) list:                  0-159\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8380 CPU @ 2.30GHz\nCPU family:                           6\nModel:                                106\nThread(s) per core:                   2\nCore(s) per socket:                   40\nSocket(s):                            2\nStepping:                             6\nCPU max MHz:                          3400.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4600.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            3.8 MiB (80 instances)\nL1i cache:                            2.5 MiB (80 instances)\nL2 cache:                             100 MiB (80 instances)\nL3 cache:                             120 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-39,80-119\nNUMA node1 CPU(s):                    40-79,120-159\nVulnerability Gather data sampling:   Mitigation; Microcode\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] habana-torch-dataloader==1.19.1.26\n[pip3] habana-torch-plugin==1.19.1.26\n[pip3] numpy==1.26.4\n[pip3] pynvml==8.0.4\n[pip3] pytorch-lightning==2.5.0.post0\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1a0+git6fc067b\n[pip3] torch_tb_profiler==0.4.0\n[pip3] torchaudio==2.5.1a0+1661daf\n[pip3] torchdata==0.9.0+d4bb3e6\n[pip3] torchmetrics==1.6.1\n[pip3] torchtext==0.18.0a0+9bed85d\n[pip3] torchvision==0.20.1a0+3ac97aa\n[pip3] transformers==4.48.2\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nLD_LIBRARY_PATH=/opt/habanalabs/libfabric-1.22.0/lib:/opt/amazon/openmpi/lib:/usr/lib/habanalabs:\nVLLM_NO_USAGE_STATS=1\nVLLM_TARGET_DEVICE=hpu\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nBuild vLLM using: \n```\ndocker build -t vllm-hpu:v0.7.2-mainline -f Dockerfile.hpu .\n```\n\nThen run it using:\n\n```\n\"cd /workspace/vllm && pip install git+https://github.com/huggingface/transformers accelerate && pip install qwen-vl-utils[decord]==0.0.8 && vllm serve Qwen/Qwen2.5-VL-7B-Instruct --disable_log_requests --port 8080 --tensor-parallel-size 2\"\n```\n\nThe hardware is a node with 8 Gaudi 2 HL-225H mezzanine cards with 2x Xeon 8380 Processors. The container has 2 Gaudi 2 cards, 70Gi memory and 25Gi of hugepages-2Mi assigned to it.\n\n```text\nview size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/engine/multiprocessing/engine.py\", line 380, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/engine/multiprocessing/engine.py\", line 123, in from_engine_args\n    return cls(ipc_path=ipc_path,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/engine/multiprocessing/engine.py\", line 75, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/engine/llm_engine.py\", line 276, in __init__\n    self._initialize_kv_caches()\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/engine/llm_engine.py\", line 416, in _initialize_kv_caches\n    self.model_executor.determine_num_available_blocks())\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/executor/executor_base.py\", line 101, in determine_num_available_blocks\n    results = self.collective_rpc(\"determine_num_available_blocks\")\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/executor/executor_base.py\", line 307, in collective_rpc\n    return self._run_workers(method, *args, **(kwargs or {}))\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/executor/mp_distributed_executor.py\", line 185, in _run_workers\n    driver_worker_output = run_method(self.driver_worker, sent_method,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/utils.py\", line 2220, in run_method\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/worker/hpu_worker.py\", line 214, in determine_num_available_blocks\n    self.model_runner.profile_run()\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/worker/hpu_model_runner.py\", line 1311, in profile_run\n    self.warmup_scenario(max_batch_size, max_seq_len, True, kv_caches,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/worker/hpu_model_runner.py\", line 1383, in warmup_scenario\n    self.execute_model(inputs, kv_caches, warmup_mode=True)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/worker/hpu_model_runner.py\", line 1967, in execute_model\n    hidden_states = self.model.forward(\n  File \"/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/graphs.py\", line 736, in forward\n    return wrapped_hpugraph_forward(\n  File \"/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/graphs.py\", line 586, in wrapped_hpugraph_forward\n    return orig_fwd(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/worker/hpu_model_runner.py\", line 371, in forward\n    hidden_states = self.model(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/model_executor/models/qwen2_5_vl.py\", line 1095, in forward\n    hidden_states = self.language_model.model(\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/compilation/decorators.py\", line 172, in __call__\n    return self.forward(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/model_executor/models/qwen2.py\", line 348, in forward\n    hidden_states, residual = layer(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1847, in _call_impl\n    return inner()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1793, in inner\n    result = forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/model_executor/models/qwen2.py\", line 247, in forward\n    hidden_states = self.self_attn(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1847, in _call_impl\n    return inner()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1793, in inner\n    result = forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/model_executor/models/qwen2.py\", line 178, in forward\n    q, k = self.rotary_emb(positions, q, k)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1847, in _call_impl\n    return inner()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1793, in inner\n    result = forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/model_executor/layers/rotary_embedding.py\", line 822, in forward\n    query = query.view(num_tokens, -1, self.head_size)\nRuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\nTraceback (most recent call last):\n  File \"/usr/local/bin/vllm\", line 33, in <module>\n    sys.exit(load_entry_point('vllm==0.7.2+gaudi000', 'console_scripts', 'vllm')())\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/scripts.py\", line 204, in main\n    args.dispatch_function(args)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/scripts.py\", line 44, in serve\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.10/dist-packages/uvloop/__init__.py\", line 82, in run\n    return loop.run_until_complete(wrapper())\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.10/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/entrypoints/openai/api_server.py\", line 875, in run_server\n    async with build_async_engine_client(args) as engine_client:\n  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/entrypoints/openai/api_server.py\", line 136, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/entrypoints/openai/api_server.py\", line 230, in build_async_engine_client_from_engine_args\n    raise RuntimeError(\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-02-10T20:28:41+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13054/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13054"
  },
  {
    "number": 12692,
    "title": "[Bug]: V1 engine ignores guided json",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 02-03 05:55:13 __init__.py:183] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.31.4\nLibc version: glibc-2.35\n\nPython version: 3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-6.8.0-52-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3\nNvidia driver version: 560.35.05\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               128\nOn-line CPU(s) list:                  0-127\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Gold 6448Y\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   32\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          4100.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4200.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hfi vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            3 MiB (64 instances)\nL1i cache:                            2 MiB (64 instances)\nL2 cache:                             128 MiB (64 instances)\nL3 cache:                             120 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126\nNUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.2\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0,2,4,6,8,10    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNVIDIA_VISIBLE_DEVICES=GPU-6cf6dad3-da49-c3af-bb85-7275c18ce3d6\nNVIDIA_REQUIRE_CUDA=cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\nNCCL_VERSION=2.17.1-1\nNVIDIA_DRIVER_CAPABILITIES=all\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nNVIDIA_CUDA_END_OF_LIFE=1\nCUDA_VERSION=12.1.0\nLD_LIBRARY_PATH=/usr/local/lib/python3.12/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nWhen making a request with to the OpenAi compatible Api with the extra fields for guided_json generation like so:\n\n```\n{\n  \"model\": \"Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"what is the height of the eiffel tower\"\n    }\n  ],\n  \"guided_json\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"height\": {\n        \"type\": \"number\"\n      }\n    },\n    \"required\": [\n      \"height\"\n    ]\n  }\n}\n```\n\nThe output simply ignores the guided decoding paramter. When switching back to V0 it works fine.\n\nHere are the logs from the vllm server:\n\n```\n`INFO 02-03 05:59:31 logger.py:37] Received request chatcmpl-178de64763e84ddd81a9f6f62ee0f4c3: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nwhat is the height of the eiffel tower<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32739, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json={'type': 'object', 'properties': {'height': {'type': 'number'}}, 'required': ['height']}, regex=None, choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None)), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\nDEBUG 02-03 05:59:31 async_llm_engine.py:546] Building guided decoding logits processor. Params: GuidedDecodingParams(json={'type': 'object', 'properties': {'height': {'type': 'number'}}, 'required': ['height']}, regex=None, choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None)\nINFO 02-03 05:59:32 engine.py:273] Added request chatcmpl-178de64763e84ddd81a9f6f62ee0f4c3.\nINFO 02-03 05:59:32 metrics.py:453] Avg prompt throughput: 4.7 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nINFO:     **.***.**.**:***** - \"POST /v1/chat/completions HTTP/1.1\" 200 OK`\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-02-03T14:01:44+00:00",
    "closed_at": "2025-06-06T02:18:22+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12692/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12692"
  },
  {
    "number": 19867,
    "title": "[Bug]: AiterFlashAttentionImpl.__init__() got multiple values for argument 'use_irope' for llama4 model",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWe hit an exception on running llama4 models with latest code on ROCm V1:\n\n```\n(VllmWorker rank=2 pid=267) ERROR 06-19 01:00:39 [multiproc_executor.py:488] TypeError: AiterFlashAttentionImpl.__init__() got multiple values for argument 'use_irope'\n```\nCurrent work-around:\nTo turn off AITER_MHA, with VLLM_ROCM_USE_AITER_MHA=0\n\n\nProposal:\n\n- [ ] Fix the bug (the team is working on it)\n- [ ] Add a end-to-end test for one of the small llama4 models\n- [ ] \n\nThe motivation for adding an end to end test for a small version of llama4 models, is that we have seen issues of breaking llama4 models in the past because of lacking such tests.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2025-06-19T14:36:59+00:00",
    "closed_at": "2025-07-14T17:39:11+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19867/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19867"
  },
  {
    "number": 20716,
    "title": "[Bug]: LLM.classify() fails on second call with ModernBERT due to missing torch.SymInt shape symbols",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nINFO 07-09 15:38:22 [__init__.py:244] Automatically detected platform cuda.\nCollecting environment information...\nuv is set\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : Could not collect\nCMake version                : version 3.22.1\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.1+cu126\nIs debug build               : False\nCUDA used to build PyTorch   : 12.6\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0] (64-bit runtime)\nPython platform              : Linux-5.15.0-133-generic-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.4.131\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : \nGPU 0: NVIDIA H200\nGPU 1: NVIDIA H200\nGPU 2: NVIDIA H200\nGPU 3: NVIDIA H200\nGPU 4: NVIDIA H200\nGPU 5: NVIDIA H200\nGPU 6: NVIDIA H200\nGPU 7: NVIDIA H200\n\nNvidia driver version        : 550.127.05\ncuDNN version                : Could not collect\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               224\nOn-line CPU(s) list:                  0-223\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8480+\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   56\nSocket(s):                            2\nStepping:                             8\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            5.3 MiB (112 instances)\nL1i cache:                            3.5 MiB (112 instances)\nL2 cache:                             224 MiB (112 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190,192,194,196,198,200,202,204,206,208,210,212,214,216,218,220,222\nNUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159,161,163,165,167,169,171,173,175,177,179,181,183,185,187,189,191,193,195,197,199,201,203,205,207,209,211,213,215,217,219,221,223\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-cufile-cu12==1.11.1.6\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-ml-py==12.575.51\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pynvml==12.0.0\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.1\n[pip3] torchaudio==2.7.0\n[pip3] torchvision==0.22.0\n[pip3] transformers==4.52.4\n[pip3] triton==3.3.1\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.1\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10NIC11    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     PIX     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS  SYS      0,2,4,6,8,10    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS  SYS      0,2,4,6,8,10    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     SYS  SYS      0,2,4,6,8,10    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS  SYS      0,2,4,6,8,10    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX     PIX     NODE    NODE NODE     1,3,5,7,9,11    1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     NODE NODE     1,3,5,7,9,11    1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX  NODE     1,3,5,7,9,11    1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE PIX      1,3,5,7,9,11    1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      PIX     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS  SYS\nNIC1    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX      X      PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS  SYS\nNIC2    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     PIX      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS  SYS\nNIC3    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      NODE    NODE    SYS     SYS     SYS     SYS     SYS  SYS\nNIC4    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE     X      NODE    SYS     SYS     SYS     SYS     SYS  SYS\nNIC5    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE     X      SYS     SYS     SYS     SYS     SYS  SYS\nNIC6    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     PIX     NODE    NODE NODE\nNIC7    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      PIX     NODE    NODE NODE\nNIC8    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX      X      NODE    NODE NODE\nNIC9    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      NODE NODE\nNIC10   SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE     X   NODE\nNIC11   SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE  X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n  NIC10: mlx5_10\n  NIC11: mlx5_11\n\n==============================\n     Environment Variables\n==============================\nLD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda-12.4/lib64\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n```python\nfrom vllm import LLM\nimport time\n\nllm = LLM(model=\"NousResearch/Minos-v1\",\n          max_model_len=8192,\n          task=\"classify\",\n          compilation_config={\"level\": 3})\n\nstart = time.time()\nresult = llm.classify([\"Hello, how are you?\"], use_tqdm=False)\nprint(result[0].outputs.probs)\nprint(f\"First inference took: {time.time() - start:.2f}s\")\n\n# Second call fails\nllm.classify([\"Hello, how are you?\"], use_tqdm=False)\n```\n\nI'm encountering an issue when using `vLLM==0.9.1` with a custom model (`ModernBERTForSequenceClassification`). The first call to `LLM.classify()` works as expected. However, a second call to `classify()` on the same instance crashes with an error originating from `vllm/compilation/backends.py`, specifically due to this code:\n\n```python\nsym_shape_indices = [i for i, x in enumerate(args) if isinstance(x, torch.SymInt)]\n```\n\nThe list ends up empty because the input shape arguments are plain integers (e.g., `7`) instead of symbolic shapes like `s0`, which is expected by the compiler backend.\n\n**Error Message:**\n```python\nFile ~/llmenv/lib/python3.12/site-packages/vllm/compilation/cuda_piecewise_backend.py:112, in CUDAPiecewiseBackend.__call__(self, *args)\n    109     self.check_for_ending_compilation()\n    110     return self.compiled_graph_for_general_shape(*args)\n--> 112 runtime_shape = args[self.sym_shape_indices[0]]\n    113 if runtime_shape not in self.concrete_size_entries:\n    114     # we don't need to do anything for this shape\n    115     return self.compiled_graph_for_general_shape(*args)\n\nIndexError: list index out of range\n```\n\nRoot Cause (Suspected):\n\nAfter debugging, we believe the issue is linked to the torch._dynamo.config. The assume_static_by_default setting being True by default for some models (like ModernBERT). This causes TorchDynamo to treat shape arguments as static integers, which breaks subsequent compilation passes that expect torch.SymInt.\n\nNotably:\n\n`qwen2.5-0.5b` works fine and emits symbolic shapes (`s0`, etc.)\n\nModernBERT models emit plain integers like `7`, leading to errors during shape checks or dynamic compilation.\n\n\n**Important, I can compile the ModernBert with the raw transformers package**\n**Important2, mannully set self.sym_shape_indices = [1] make the code successfully running with batch_size = 1**\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-07-09T22:46:17+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20716/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20716"
  },
  {
    "number": 5961,
    "title": "[Bug]: vLLM crash when running Phi-3-small-8k-instruct with enable-chunked-prefill",
    "body": "### Your current environment\n\n\r\n```\r\nimage\": \"vllm/vllm-openai:latest\",\r\n--model=microsoft/Phi-3-small-8k-instruct \r\n--tensor-parallel-size=1\r\n--disable-log-requests\r\n--trust-remote-code\r\n--enable-chunked-prefill\r\n--max-num-batched-tokens=2048\r\n--max-model-len=4096\r\n--gpu-memory-utilization=0.9\",\r\n```\r\nAccelerator: 1x Nvidia L4\n\n### \ud83d\udc1b Describe the bug\n\n```\r\nERROR 06-28 13:26:18 async_llm_engine.py:52] Engine background task failed\r\nERROR 06-28 13:26:18 async_llm_engine.py:52] Traceback (most recent call last):\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 42, in _log_task_completion\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return_value = task.result()\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 532, in run_engine_loop\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     has_requests_in_progress = await asyncio.wait_for(\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return fut.result()\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 506, in engine_step\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     request_outputs = await self.engine.step_async()\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 235, in step_async\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     output = await self.model_executor.execute_model_async(\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 117, in execute_model_async\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     output = await make_async(self.driver_worker.execute_model\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     result = self.fn(*self.args, **self.kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return func(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 280, in execute_model\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     output = self.model_runner.execute_model(seq_group_metadata_list,\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return func(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 749, in execute_model\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     hidden_states = model_executable(\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3_small.py\", line 416, in forward\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     output_hidden_states = self.model(\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3_small.py\", line 338, in forward\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     hidden_states = layer(\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3_small.py\", line 282, in forward\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     hidden_states = self.self_attn(\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3_small.py\", line 244, in forward\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     attn_output = self.attn(q, k, v, kv_cache, attn_metadata=attn_metadata)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/layer.py\", line 89, in forward\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return self.impl.forward(query, key, value, kv_cache, attn_metadata,\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/backends/blocksparse_attn.py\", line 376, in forward\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     or prefill_meta.block_tables.numel() == 0, \\\r\nERROR 06-28 13:26:18 async_llm_engine.py:52] AssertionError: Does not support prefix-enabled attention.\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-06-28T13:34:09+00:00",
    "closed_at": "2024-06-28T22:41:15+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5961/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5961"
  },
  {
    "number": 14979,
    "title": "[Bug]: Model weights in GiB ",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.12.9 (main, Feb 12 2025, 14:50:50) [Clang 19.1.6 ] (64-bit runtime)\nPython platform: Linux-5.15.0-134-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA L40S\nNvidia driver version: 570.86.10\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               48\nOn-line CPU(s) list:                  0-47\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Gold 5412U\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   24\nSocket(s):                            1\nStepping:                             8\nCPU max MHz:                          3900.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4200.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            1.1 MiB (24 instances)\nL1i cache:                            768 KiB (24 instances)\nL2 cache:                             48 MiB (24 instances)\nL3 cache:                             45 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-47\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pynvml==12.0.0\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.49.0\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: N/A\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-47    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n\n\n</details>\n\n\n\n### \ud83d\udc1b Describe the bug\n\nHello,\nI think there's a small error in the unit displayed by model_runner.py concerning the weight of a model.\nGiven the calculation, I suggest replacing GB with GiB.\nWhat do you think? \n\n self.model_memory_usage = m.consumed_memory\n        logger.info(\"Loading model weights took %.4f GB\",\n                    self.model_memory_usage / **float(2**30))**\n\nIncorrect output : \nmodel_runner.py:1115] Loading model weights took 12.5523 **GB**\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-17T18:28:30+00:00",
    "closed_at": "2025-03-31T17:00:51+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14979/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14979"
  },
  {
    "number": 354,
    "title": "Loading Models that require execution of third party code (trust_remote_code=True)",
    "body": "I am trying to load MPT using the AsyncLLMEngine:\r\n\r\n```\r\n\r\nengine_args = AsyncEngineArgs(\"mosaicml/mpt-7b-chat\", engine_use_ray=True)\r\nengine = AsyncLLMEngine.from_engine_args(engine_args)\r\n```\r\n\r\nBut I am getting this error:\r\n`ValueError: Loading mosaicml/mpt-7b-chat-local requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.`\r\n\r\nIs there any workaround for this or could it be possible to add the option to trust remote code to EngineArgs?",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-07-04T08:05:46+00:00",
    "closed_at": "2024-03-08T10:22:14+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/354/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/354"
  },
  {
    "number": 15004,
    "title": "[Bug]:  Failed to Run Qwen2.5-7B with RTX 3070 & CPU Offload (14GB) Despite Sufficient Theoretical Memory",
    "body": "### Your current environment\n\nThe output of `python collect_env.py`\n(vllm) roy@Roy-L:~/projects$ python collect_env.py\nINFO 03-12 13:15:42 __init__.py:207] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.8.61\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 3070 Laptop GPU\nNvidia driver version: 572.70\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               16\nOn-line CPU(s) list:                  0-15\nVendor ID:                            AuthenticAMD\nModel name:                           AMD Ryzen 7 5800H with Radeon Graphics\nCPU family:                           25\nModel:                                80\nThread(s) per core:                   2\nCore(s) per socket:                   8\nSocket(s):                            1\nStepping:                             0\nBogoMIPS:                             6387.84\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr arat npt nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload umip vaes vpclmulqdq rdpid fsrm\nVirtualization:                       AMD-V\nHypervisor vendor:                    Microsoft\nVirtualization type:                  full\nL1d cache:                            256 KiB (8 instances)\nL1i cache:                            256 KiB (8 instances)\nL2 cache:                             4 MiB (8 instances)\nL3 cache:                             16 MiB (1 instance)\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP conditional; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.49.0\n[pip3] triton==3.1.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.2.1                   pypi_0    pypi\n[conda] torch                     2.5.1                    pypi_0    pypi\n[conda] torchaudio                2.5.1                    pypi_0    pypi\n[conda] torchvision               0.20.1                   pypi_0    pypi\n[conda] transformers              4.49.0                   pypi_0    pypi\n[conda] triton                    3.1.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.3\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X                              N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=/home/roy/miniconda3/envs/vllm/lib/python3.12/site-packages/cv2/../../lib64:/usr/local/cuda-12.8/lib64:/usr/local/cuda-12.8/lib64:/usr/local/cuda-12.8/lib64:\nVLLM_USE_V1=1\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n### \ud83d\udc1b Describe the bug\n\nI'm using WSL2. When attempting to run \u200bQwen2.5-7B using vLLM 0.7.3 with CPU offloading, the initialization fails with ValueError: No available memory for the cache blocks, despite having:\n\n32GB system RAM (14GB allocated via cpu_offload_gb) (\"I have tried the cpu_offload_gb parameter in the range of 10 to 22, but none of these settings allowed the model to run successfully.\")\n8GB NVIDIA RTX 3070 GPU\nTheoretical memory requirements (14GB offloaded to RAM + ~ 8*0.9(gpu_memory_utilization)=7.2GB VRAM ) appear compatible with hardware specs, but practical allocation fails.\n\nThe code is simple:\n`\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n\ntokenizer = AutoTokenizer.from_pretrained(\"/home/roy/models/Qwen2.5-7B\")\n\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=256)\n\nllm = LLM(model=\"/home/roy/models/Qwen2.5-7B\", max_model_len = 512, cpu_offload_gb= 12,gpu_memory_utilization=0.95)\n\nprompt = \"Tell me something about large language models.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\n\noutputs = llm.generate([text], sampling_params)\n\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n`\n\nError log here:\n(vllm) (base) roy@Roy-L:~/projects$ /home/roy/miniconda3/envs/vllm/bin/python /home/roy/projects/test/test_qwen7b\nINFO 03-18 13:04:44 init.py:207] Automatically detected platform cuda.\nWARNING 03-18 13:04:46 arg_utils.py:1385] Setting max_num_batched_tokens to 8192 for LLM_CLASS usage context.\nINFO 03-18 13:04:55 config.py:549] This model supports multiple tasks: {'generate', 'classify', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\nINFO 03-18 13:04:55 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=8192.\nWARNING 03-18 13:04:55 config.py:3314] CPU offload is not supported with torch.compile yet. Disabling torch.compile.\nINFO 03-18 13:04:56 core.py:50] Initializing a V1 LLM engine (v0.7.3) with config: model='/home/roy/models/Qwen2.5-7B', speculative_config=None, tokenizer='/home/roy/models/Qwen2.5-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=512, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/roy/models/Qwen2.5-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\nWARNING 03-18 13:04:56 utils.py:2262] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,list_loras,load_config,pin_lora,remove_lora,scheduler_config not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ff7f7abe660>\nWARNING 03-18 13:04:57 interface.py:304] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\nINFO 03-18 13:04:57 gpu_model_runner.py:1049] Starting to load model /home/roy/models/Qwen2.5-7B...\nINFO 03-18 13:04:57 cuda.py:157] Using Flash Attention backend on V1 engine.\nWARNING 03-18 13:04:58 topk_topp_sampler.py:46] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\nWARNING 03-18 13:04:58 rejection_sampler.py:47] FlashInfer is not available. Falling back to the PyTorch-native implementation of rejection sampling. For the best performance, please install FlashInfer.\nLoading safetensors checkpoint shards: 0% Completed | 0/4 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards: 25% Completed | 1/4 [00:07<00:22, 7.48s/it]\nLoading safetensors checkpoint shards: 50% Completed | 2/4 [00:13<00:13, 6.68s/it]\nLoading safetensors checkpoint shards: 75% Completed | 3/4 [00:18<00:05, 5.69s/it]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:22<00:00, 4.98s/it]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:22<00:00, 5.50s/it]\n\nINFO 03-18 13:05:21 gpu_model_runner.py:1060] Loading model weights took 14.2487 GB\nERROR 03-18 13:09:10 core.py:291] EngineCore hit an exception: Traceback (most recent call last):\nERROR 03-18 13:09:10 core.py:291] File \"/home/roy/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 283, in run_engine_core\nERROR 03-18 13:09:10 core.py:291] engine_core = EngineCoreProc(*args, **kwargs)\nERROR 03-18 13:09:10 core.py:291] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-18 13:09:10 core.py:291] File \"/home/roy/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 238, in init\nERROR 03-18 13:09:10 core.py:291] super().init(vllm_config, executor_class, log_stats)\nERROR 03-18 13:09:10 core.py:291] File \"/home/roy/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 59, in init\nERROR 03-18 13:09:10 core.py:291] num_gpu_blocks, num_cpu_blocks = self._initialize_kv_caches(\nERROR 03-18 13:09:10 core.py:291] ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-18 13:09:10 core.py:291] File \"/home/roy/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 102, in _initialize_kv_caches\nERROR 03-18 13:09:10 core.py:291] kv_cache_configs = get_kv_cache_configs(vllm_config, kv_cache_specs,\nERROR 03-18 13:09:10 core.py:291] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-18 13:09:10 core.py:291] File \"/home/roy/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py\", line 561, in get_kv_cache_configs\nERROR 03-18 13:09:10 core.py:291] check_enough_kv_cache_memory(vllm_config, kv_cache_spec,\nERROR 03-18 13:09:10 core.py:291] File \"/home/roy/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py\", line 455, in check_enough_kv_cache_memory\nERROR 03-18 13:09:10 core.py:291] raise ValueError(\"No available memory for the cache blocks. \"\nERROR 03-18 13:09:10 core.py:291] ValueError: No available memory for the cache blocks. Try increasing gpu_memory_utilization when initializing the engine.\nERROR 03-18 13:09:10 core.py:291]\nCRITICAL 03-18 13:09:10 core_client.py:191] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\nKilled\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-18T05:34:12+00:00",
    "closed_at": "2025-07-18T02:28:25+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15004/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15004"
  },
  {
    "number": 5311,
    "title": "[Bug]: RuntimeError: CUDA error: no kernel image is available for execution on the device CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\nPyTorch version: 2.1.2+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: 14.0.0-1ubuntu1.1\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-35-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.3.107\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA GeForce GTX 1080 Ti\r\nGPU 1: NVIDIA GeForce GTX 1080 Ti\r\nGPU 2: NVIDIA GeForce GTX 1080 Ti\r\nGPU 3: NVIDIA GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 535.171.04\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             32\r\nOn-line CPU(s) list:                0-31\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz\r\nCPU family:                         6\r\nModel:                              79\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 8\r\nSocket(s):                          2\r\nStepping:                           1\r\nCPU max MHz:                        3000.0000\r\nCPU min MHz:                        1200.0000\r\nBogoMIPS:                           4195.10\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts vnmi md_clear flush_l1d\r\nVirtualization:                     VT-x\r\nL1d cache:                          512 KiB (16 instances)\r\nL1i cache:                          512 KiB (16 instances)\r\nL2 cache:                           4 MiB (16 instances)\r\nL3 cache:                           40 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30\r\nNUMA node1 CPU(s):                  1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        KVM: Mitigation: VMX disabled\r\nVulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable\r\nVulnerability Mds:                  Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Meltdown:             Mitigation; PTI\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP conditional; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Mitigation; Clear CPU buffers; SMT vulnerable\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.3\r\n[pip3] nvidia-nccl-cu12==2.18.1\r\n[pip3] torch==2.1.2\r\n[pip3] transformers==4.36.2\r\n[pip3] triton==2.1.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.2.3\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tPHB\tSYS\tSYS\t0,2,4,6,8,10\t0\t\tN/A\r\nGPU1\tPHB\t X \tSYS\tSYS\t0,2,4,6,8,10\t0\t\tN/A\r\nGPU2\tSYS\tSYS\t X \tPHB\t1,3,5,7,9,11\t1\t\tN/A\r\nGPU3\tSYS\tSYS\tPHB\t X \t1,3,5,7,9,11\t1\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n### \ud83d\udc1b Describe the bug\n\nuse triton + vllm backend \r\nImage: nvcr.io/nvidia/tritonserver:24.01-vllm-python-py3   \r\nmodel: opt-125m\r\n\r\ncurl output: {\"error\":\"Error generating stream: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for the actual cause.\"}\r\n\r\ntriton stdout:\r\n\r\nI0606 07:44:10.297581 8157 model.py:275] [vllm] Error generating stream: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for the actual cause.\r\n(_AsyncLLMEngine pid=10245) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::RayWorkerVllm.execute_method() (pid=10360, ip=10.42.0.27, actor_id=f9f1b30972eea279464b50a801000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7dd004723760>)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/ray_utils.py\", line 31, in execute_method\r\n(_AsyncLLMEngine pid=10245)     return executor(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(_AsyncLLMEngine pid=10245)     return func(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 147, in execute_model\r\n(_AsyncLLMEngine pid=10245)     output = self.model_runner.execute_model(seq_group_metadata_list,\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(_AsyncLLMEngine pid=10245)     return func(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 279, in execute_model\r\n(_AsyncLLMEngine pid=10245)     hidden_states = self.model(\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(_AsyncLLMEngine pid=10245)     return self._call_impl(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(_AsyncLLMEngine pid=10245)     return forward_call(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/opt.py\", line 313, in forward\r\n(_AsyncLLMEngine pid=10245)     hidden_states = self.model(input_ids, positions, kv_caches,\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(_AsyncLLMEngine pid=10245)     return self._call_impl(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(_AsyncLLMEngine pid=10245)     return forward_call(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/opt.py\", line 287, in forward\r\n(_AsyncLLMEngine pid=10245)     return self.decoder(input_ids, positions, kv_caches, input_metadata,\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(_AsyncLLMEngine pid=10245)     return self._call_impl(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(_AsyncLLMEngine pid=10245)     return forward_call(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/opt.py\", line 259, in forward\r\n(_AsyncLLMEngine pid=10245)     hidden_states = layer(hidden_states, kv_caches[i], input_metadata,\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(_AsyncLLMEngine pid=10245)     return self._call_impl(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(_AsyncLLMEngine pid=10245)     return forward_call(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/opt.py\", line 164, in forward\r\n(_AsyncLLMEngine pid=10245)     hidden_states = self.self_attn(hidden_states=hidden_states,\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(_AsyncLLMEngine pid=10245)     return self._call_impl(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(_AsyncLLMEngine pid=10245)     return forward_call(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/opt.py\", line 106, in forward\r\n(_AsyncLLMEngine pid=10245)     attn_output = self.attn(q, k, v, key_cache, value_cache,\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(_AsyncLLMEngine pid=10245)     return self._call_impl(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(_AsyncLLMEngine pid=10245)     return forward_call(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/attention.py\", line 156, in forward\r\n(_AsyncLLMEngine pid=10245)     out = xops.memory_efficient_attention_forward(\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/__init__.py\", line 244, in memory_efficient_attention_forward\r\n(_AsyncLLMEngine pid=10245)     return _memory_efficient_attention_forward(\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/__init__.py\", line 341, in _memory_efficient_attention_forward\r\n(_AsyncLLMEngine pid=10245)     out, *_ = op.apply(inp, needs_gradient=False)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/cutlass.py\", line 202, in apply\r\n(_AsyncLLMEngine pid=10245)     return cls.apply_bmhk(inp, needs_gradient=needs_gradient)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/cutlass.py\", line 266, in apply_bmhk\r\n(_AsyncLLMEngine pid=10245)     out, lse, rng_seed, rng_offset = cls.OPERATOR(\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 692, in __call__\r\n(_AsyncLLMEngine pid=10245)     return self._op(*args, **kwargs or {})\r\n(_AsyncLLMEngine pid=10245) RuntimeError: CUDA error: no kernel image is available for execution on the device\r\n(_AsyncLLMEngine pid=10245) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n(_AsyncLLMEngine pid=10245) For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n(_AsyncLLMEngine pid=10245) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n(_AsyncLLMEngine pid=10245) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::RayWorkerVllm.execute_method() (pid=10358, ip=10.42.0.27, actor_id=bb92b604606d38c3607c4f7401000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x741ff1dd3790>)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/ray_utils.py\", line 31, in execute_method\r\n(_AsyncLLMEngine pid=10245)     return executor(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(_AsyncLLMEngine pid=10245)     return func(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 147, in execute_model\r\n(_AsyncLLMEngine pid=10245)     output = self.model_runner.execute_model(seq_group_metadata_list,\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(_AsyncLLMEngine pid=10245)     return func(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 279, in execute_model\r\n(_AsyncLLMEngine pid=10245)     hidden_states = self.model(\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(_AsyncLLMEngine pid=10245)     return self._call_impl(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(_AsyncLLMEngine pid=10245)     return forward_call(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/opt.py\", line 313, in forward\r\n(_AsyncLLMEngine pid=10245)     hidden_states = self.model(input_ids, positions, kv_caches,\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(_AsyncLLMEngine pid=10245)     return self._call_impl(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(_AsyncLLMEngine pid=10245)     return forward_call(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/opt.py\", line 287, in forward\r\n(_AsyncLLMEngine pid=10245)     return self.decoder(input_ids, positions, kv_caches, input_metadata,\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(_AsyncLLMEngine pid=10245)     return self._call_impl(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(_AsyncLLMEngine pid=10245)     return forward_call(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/opt.py\", line 259, in forward\r\n(_AsyncLLMEngine pid=10245)     hidden_states = layer(hidden_states, kv_caches[i], input_metadata,\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(_AsyncLLMEngine pid=10245)     return self._call_impl(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(_AsyncLLMEngine pid=10245)     return forward_call(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/opt.py\", line 164, in forward\r\n(_AsyncLLMEngine pid=10245)     hidden_states = self.self_attn(hidden_states=hidden_states,\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(_AsyncLLMEngine pid=10245)     return self._call_impl(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(_AsyncLLMEngine pid=10245)     return forward_call(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/opt.py\", line 106, in forward\r\n(_AsyncLLMEngine pid=10245)     attn_output = self.attn(q, k, v, key_cache, value_cache,\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(_AsyncLLMEngine pid=10245)     return self._call_impl(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(_AsyncLLMEngine pid=10245)     return forward_call(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/attention.py\", line 156, in forward\r\n(_AsyncLLMEngine pid=10245)     out = xops.memory_efficient_attention_forward(\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/__init__.py\", line 244, in memory_efficient_attention_forward\r\n(_AsyncLLMEngine pid=10245)     return _memory_efficient_attention_forward(\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/__init__.py\", line 341, in _memory_efficient_attention_forward\r\n(_AsyncLLMEngine pid=10245)     out, *_ = op.apply(inp, needs_gradient=False)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/cutlass.py\", line 202, in apply\r\n(_AsyncLLMEngine pid=10245)     return cls.apply_bmhk(inp, needs_gradient=needs_gradient)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/cutlass.py\", line 266, in apply_bmhk\r\n(_AsyncLLMEngine pid=10245)     out, lse, rng_seed, rng_offset = cls.OPERATOR(\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 692, in __call__\r\n(_AsyncLLMEngine pid=10245)     return self._op(*args, **kwargs or {})\r\n(_AsyncLLMEngine pid=10245) RuntimeError: CUDA error: no kernel image is available for execution on the device\r\n(_AsyncLLMEngine pid=10245) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n(_AsyncLLMEngine pid=10245) For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n(_AsyncLLMEngine pid=10245) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n(_AsyncLLMEngine pid=10245) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::RayWorkerVllm.execute_method() (pid=10361, ip=10.42.0.27, actor_id=9294639f58f94e6c005fb99f01000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x79cc90adf760>)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/ray_utils.py\", line 31, in execute_method\r\n(_AsyncLLMEngine pid=10245)     return executor(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(_AsyncLLMEngine pid=10245)     return func(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 147, in execute_model\r\n(_AsyncLLMEngine pid=10245)     output = self.model_runner.execute_model(seq_group_metadata_list,\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(_AsyncLLMEngine pid=10245)     return func(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 279, in execute_model\r\n(_AsyncLLMEngine pid=10245)     hidden_states = self.model(\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(_AsyncLLMEngine pid=10245)     return self._call_impl(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(_AsyncLLMEngine pid=10245)     return forward_call(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/opt.py\", line 313, in forward\r\n(_AsyncLLMEngine pid=10245)     hidden_states = self.model(input_ids, positions, kv_caches,\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(_AsyncLLMEngine pid=10245)     return self._call_impl(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(_AsyncLLMEngine pid=10245)     return forward_call(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/opt.py\", line 287, in forward\r\n(_AsyncLLMEngine pid=10245)     return self.decoder(input_ids, positions, kv_caches, input_metadata,\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(_AsyncLLMEngine pid=10245)     return self._call_impl(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(_AsyncLLMEngine pid=10245)     return forward_call(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/opt.py\", line 259, in forward\r\n(_AsyncLLMEngine pid=10245)     hidden_states = layer(hidden_states, kv_caches[i], input_metadata,\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(_AsyncLLMEngine pid=10245)     return self._call_impl(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(_AsyncLLMEngine pid=10245)     return forward_call(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/opt.py\", line 164, in forward\r\n(_AsyncLLMEngine pid=10245)     hidden_states = self.self_attn(hidden_states=hidden_states,\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(_AsyncLLMEngine pid=10245)     return self._call_impl(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(_AsyncLLMEngine pid=10245)     return forward_call(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/opt.py\", line 106, in forward\r\n(_AsyncLLMEngine pid=10245)     attn_output = self.attn(q, k, v, key_cache, value_cache,\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(_AsyncLLMEngine pid=10245)     return self._call_impl(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(_AsyncLLMEngine pid=10245)     return forward_call(*args, **kwargs)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/attention.py\", line 156, in forward\r\n(_AsyncLLMEngine pid=10245)     out = xops.memory_efficient_attention_forward(\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/__init__.py\", line 244, in memory_efficient_attention_forward\r\n(_AsyncLLMEngine pid=10245)     return _memory_efficient_attention_forward(\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/__init__.py\", line 341, in _memory_efficient_attention_forward\r\n(_AsyncLLMEngine pid=10245)     out, *_ = op.apply(inp, needs_gradient=False)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/cutlass.py\", line 202, in apply\r\n(_AsyncLLMEngine pid=10245)     return cls.apply_bmhk(inp, needs_gradient=needs_gradient)\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/cutlass.py\", line 266, in apply_bmhk\r\n(_AsyncLLMEngine pid=10245)     out, lse, rng_seed, rng_offset = cls.OPERATOR(\r\n(_AsyncLLMEngine pid=10245)   File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 692, in __call__\r\n(_AsyncLLMEngine pid=10245)     return self._op(*args, **kwargs or {})\r\n(_AsyncLLMEngine pid=10245) RuntimeError: CUDA error: no kernel image is available for execution on the device\r\n(_AsyncLLMEngine pid=10245) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n(_AsyncLLMEngine pid=10245) For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n(_AsyncLLMEngine pid=10245) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-06-06T07:50:01+00:00",
    "closed_at": "2025-02-11T16:43:28+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5311/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5311"
  },
  {
    "number": 5885,
    "title": "[Bug]: Concurrently captioning images with phi3 Vision can cause the backend to crash",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.6\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-112-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA H100 PCIe\r\nNvidia driver version: 550.67\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             128\r\nOn-line CPU(s) list:                0-127\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 9354 32-Core Processor\r\nCPU family:                         25\r\nModel:                              17\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 32\r\nSocket(s):                          2\r\nStepping:                           1\r\nFrequency boost:                    enabled\r\nCPU max MHz:                        3799.0720\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           6500.17\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov succor smca fsrm flush_l1d\r\nVirtualization:                     AMD-V\r\nL1d cache:                          2 MiB (64 instances)\r\nL1i cache:                          2 MiB (64 instances)\r\nL2 cache:                           64 MiB (64 instances)\r\nL3 cache:                           512 MiB (16 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-31,64-95\r\nNUMA node1 CPU(s):                  32-63,96-127\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; safe RET\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] onnxruntime-gpu==1.18.0\r\n[pip3] torch==2.3.0\r\n[pip3] torchaudio==2.1.1\r\n[pip3] torchvision==0.18.0\r\n[pip3] transformers==4.41.2\r\n[pip3] triton==2.3.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      32-63,96-127    1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen running multiple requests to phi-3 vision, I sometimes get this error. Causing the app to refuse further requests.\r\n\r\nStacktrace on VLLM side:\r\n```\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 278, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 132, in create_chat_completion\r\n    generator = await openai_serving_chat.create_chat_completion(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 293, in create_chat_completion\r\n    return await self.chat_completion_full_generator(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 493, in chat_completion_full_generator\r\n    async for res in result_generator:\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 684, in generate\r\n    async for output in self._process_request(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 797, in _process_request\r\n    raise e\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 793, in _process_request\r\n    async for request_output in stream:\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 90, in __anext__\r\n    raise result\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 43, in _log_task_completion\r\n    return_value = task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 552, in run_engine_loop\r\n    has_requests_in_progress = await self.engine_step()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 525, in engine_step\r\n    request_outputs = await self.engine.step_async()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 236, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 121, in execute_model_async\r\n    output = await make_async(self.driver_worker.execute_model\r\n  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 255, in execute_model\r\n    output = self.model_runner.execute_model(model_input, self.kv_cache)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 999, in execute_model\r\n    hidden_states = model_executable(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3v.py\", line 376, in forward\r\n    inputs_embeds = self.vision_embed_tokens(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3v.py\", line 220, in forward\r\n    sub_img = sub_img[:B_]\r\nTypeError: only integer tensors of a single element can be converted to an index\r\n```\r\n\r\nThe workaround I used was to send 1 request at a time with a multiprocessing lock from my client process.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-06-27T05:17:59+00:00",
    "closed_at": "2024-06-27T08:29:26+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5885/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5885"
  },
  {
    "number": 8477,
    "title": "[Bug]: : CPU silently doesn't support multi-step (--num-scheduler-steps)",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nINFO 09-13 19:13:45 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\r\nPyTorch version: 2.4.0+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: Could not collect\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-4.18.0-372.46.1.el8_6.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-80GB\r\n  MIG 3g.40gb     Device  0:\r\n\r\nNvidia driver version: 535.104.05\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          80\r\nOn-line CPU(s) list:             0-79\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel Xeon Processor (Icelake)\r\nCPU family:                      6\r\nModel:                           134\r\nThread(s) per core:              2\r\nCore(s) per socket:              20\r\nSocket(s):                       2\r\nStepping:                        0\r\nBogoMIPS:                        5600.03\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd arat avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm md_clear arch_capabilities\r\nVirtualization:                  VT-x\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       2.5 MiB (80 instances)\r\nL1i cache:                       2.5 MiB (80 instances)\r\nL2 cache:                        160 MiB (40 instances)\r\nL3 cache:                        32 MiB (2 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-39\r\nNUMA node1 CPU(s):               40-79\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] intel_extension_for_pytorch==2.4.0+gitfbaa4bc\r\n[pip3] numpy==1.26.4\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0+cpu\r\n[pip3] torchvision==0.19.0+cpu\r\n[pip3] transformers==4.44.2\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.0@36bf8150cc3a048d69d9d2196128462014b9599d\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\n\u001b[4mGPU0\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\r\nGPU0\t X \tSYS\t40-79\t1\t\tN/A\r\nNIC0\tSYS\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nI tested the following script running on a CPU backend which set num_scheduler_steps > 1 to force use mult-step:\r\n\r\n```python\r\nfrom vllm import LLM, SamplingParams\r\n\r\nllm = LLM(model=\"facebook/opt-125M\", \r\n          gpu_memory_utilization=0.4, \r\n          max_model_len=1024,  \r\n          num_scheduler_steps=8\r\n          )\r\n\r\n\r\nparams = SamplingParams(seed=123,  prompt_logprobs=5, temperature=1)\r\n\r\nprompts = [\"How to make pizza?\"]\r\noutputs = llm.generate(prompts, sampling_params=params )\r\n\r\nfor o in outputs:\r\n    print('_________')\r\n    print('### Text')\r\n    print('_________')\r\n    for o2 in o.outputs:\r\n        print(o2.text)\r\n\r\n```\r\n\r\nGot the following output:\r\n```\r\nINFO 09-13 19:33:10 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\r\nWARNING 09-13 19:33:13 arg_utils.py:902] Enabled BlockSpaceManagerV2 because it is required for multi-step (--num-scheduler-steps > 1)\r\nWARNING 09-13 19:33:13 config.py:370] Async output processing is only supported for CUDA or TPU. Disabling it for other platforms.\r\nINFO 09-13 19:33:13 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='facebook/opt-125M', speculative_config=None, tokenizer='facebook/opt-125M', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=facebook/opt-125M, use_v2_block_manager=True, num_scheduler_steps=8, enable_prefix_caching=False, use_async_output_proc=False)\r\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\r\n  warnings.warn(\r\nWARNING 09-13 19:33:14 cpu_executor.py:321] float16 is not supported on CPU, casting to bfloat16.\r\nWARNING 09-13 19:33:14 cpu_executor.py:324] CUDA graph is not supported on CPU, fallback to the eager mode.\r\nWARNING 09-13 19:33:14 cpu_executor.py:350] Environment variable VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by default.\r\nINFO 09-13 19:33:14 selector.py:183] Cannot use _Backend.FLASH_ATTN backend on CPU.\r\nINFO 09-13 19:33:14 selector.py:128] Using Torch SDPA backend.\r\nINFO 09-13 19:33:15 selector.py:183] Cannot use _Backend.FLASH_ATTN backend on CPU.\r\nINFO 09-13 19:33:15 selector.py:128] Using Torch SDPA backend.\r\nINFO 09-13 19:33:15 weight_utils.py:235] Using model weights format ['*.bin']\r\nLoading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/weight_utils.py:417: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  state = torch.load(bin_file, map_location=\"cpu\")\r\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.83it/s]\r\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.82it/s]\r\n\r\nINFO 09-13 19:33:16 cpu_executor.py:208] # CPU blocks: 7281\r\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:03<00:00,  3.86s/it, est. speed input: 1.55 toks/s, output: 4.14 toks/s]\r\n_________\r\n### Text\r\n_________\r\n\r\n\r\nMethod\r\n\r\n```\r\n\r\nHowever accordingly to #8198 prompt logs causes vllm to crash when using multi-step. Which does not happen in the above log. This was actually a dummy way to check if the feature is actually active. Moreover checking the code, CPU has several specific classes that make a parallel implementation for CPU backend and it looks like it is not using the parameters of multi-step scheduling. There is also no warning in the log that inform the feature is not workig.\r\n\r\n## Expectation\r\n\r\nAdd a checking in the code to raise an exception or warning to inform the user that the feature is not supported.\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-13T19:55:13+00:00",
    "closed_at": "2025-01-13T02:03:03+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8477/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8477"
  },
  {
    "number": 13848,
    "title": "[Bug]: vLLM 0.7.3 TypeError in vllm.entrypoints.api_server Argument Parsing",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 02-25 14:13:01 __init__.py:190] Automatically detected platform cpu.\nCollecting environment information...\nPyTorch version: 2.5.1\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: macOS 14.7.3 (arm64)\nGCC version: Could not collect\nClang version: 16.0.0 (clang-1600.0.26.6)\nCMake version: Could not collect\nLibc version: N/A\n\nPython version: 3.11.6 (main, Feb 25 2025, 12:41:54) [Clang 16.0.0 (clang-1600.0.26.6)] (64-bit runtime)\nPython platform: macOS-14.7.3-arm64-arm-64bit\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nApple M2 Pro\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-ml-py==12.570.86\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.49.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nLD_LIBRARY_PATH=/Users/mdobbali@mastercontrol.com/Library/Caches/pypoetry/virtualenvs/models-gt-QYEJX-py3.11/lib/python3.11/site-packages/cv2/../../lib:\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\n\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen running vllm.entrypoints.api_server, I encounter a TypeError related to argparse argument parsing.\n\n```bash\npoetry run python -m vllm.entrypoints.api_server \\\n    --model SomeOrg/SomeModel \\\n    --host 0.0.0.0 \\\n    --port 8000\n```\n\n**Error Message**\n\n```bash\nTraceback (most recent call last):\n  File \"/path/to/python3.11/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/path/to/python3.11/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/path/to/vllm/entrypoints/api_server.py\", line 148, in <module>\n    parser.add_argument(\"--port\", type=int, default=8000, ge=1024, le=65535)\n  File \"/path/to/python3.11/argparse.py\", line 1430, in add_argument\n    action = action_class(**kwargs)\nTypeError: _StoreAction.__init__() got an unexpected keyword argument 'ge'\n```\n\n**Steps to Reproduce**\n\n1. Install vLLM from source using:\n```bash\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\npip install .\n\n```\n2. Run the vLLM API Server command as shown above.\n3. Observe the TypeError due to the `ge` argument\n\n**Expected Behavior**\nThe API server should start normally, listening on the specified port.\n\n**Additional Debugging Attempts**\n\n- Tried using a different Python version (3.12) but encountered the same issue.\n- Installed the latest vLLM from the main branch.\n- Downgraded vLLM to 0.7.2, and the error disappeared\n\n**Proposed Solution**\n\n- It seems like argparse in Python 3.11+ does not support `ge` and `le` constraints in `parser.add_argument()`.\n\nPotential fixes:\n\n- Remove or Replace ge and le constraints with manual validation inside api_server.py.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-02-25T21:27:29+00:00",
    "closed_at": null,
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13848/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13848"
  },
  {
    "number": 19367,
    "title": "[Bug]: Sliding Window Attention not supported in V1 for ROCm",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nINFO 06-09 15:59:09 [__init__.py:248] Automatically detected platform rocm.\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : 18.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.3.1 24491 1e0fda770a2079fbd71e4b70974d74f62fd3af10)\nCMake version                : version 3.31.4\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0a0+git6c0e746\nIs debug build               : False\nCUDA used to build PyTorch   : N/A\nROCM used to build PyTorch   : 6.3.42133-1b9c17779\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)\nPython platform              : Linux-6.8.0-52-generic-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : Could not collect\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : AMD Instinct MI250X/MI250 (gfx90a:sramecc+:xnack-)\nNvidia driver version        : Could not collect\ncuDNN version                : Could not collect\nHIP runtime version          : 6.3.42133\nMIOpen runtime version       : 3.3.0\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               128\nOn-line CPU(s) list:                  0-127\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 7713 64-Core Processor\nCPU family:                           25\nModel:                                1\nThread(s) per core:                   1\nCore(s) per socket:                   64\nSocket(s):                            2\nStepping:                             1\nFrequency boost:                      enabled\nCPU max MHz:                          3720.7029\nCPU min MHz:                          1500.0000\nBogoMIPS:                             3992.52\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm debug_swap\nVirtualization:                       AMD-V\nL1d cache:                            4 MiB (128 instances)\nL1i cache:                            4 MiB (128 instances)\nL2 cache:                             64 MiB (128 instances)\nL3 cache:                             512 MiB (16 instances)\nNUMA node(s):                         8\nNUMA node0 CPU(s):                    0-15\nNUMA node1 CPU(s):                    16-31\nNUMA node2 CPU(s):                    32-47\nNUMA node3 CPU(s):                    48-63\nNUMA node4 CPU(s):                    64-79\nNUMA node5 CPU(s):                    80-95\nNUMA node6 CPU(s):                    96-111\nNUMA node7 CPU(s):                    112-127\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==1.26.4\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.7.0a0+git6c0e746\n[pip3] torchvision==0.21.0+7af6987\n[pip3] transformers==4.52.3\n[pip3] triton==3.2.0+gite5be006a\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : 6.3.42133-1b9c17779\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.8.5.dev681+g964472b96.d20250528 (git sha: 964472b96, date: 20250528)\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n  ============================ ROCm System Management Interface ============================\n================================ Weight between two GPUs =================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            15           15           30           30           30           15           30\nGPU1   15           0            30           15           30           15           30           45\nGPU2   15           30           0            15           15           30           30           30\nGPU3   30           15           15           0            30           45           30           15\nGPU4   30           30           15           30           0            15           15           30\nGPU5   30           15           30           45           15           0            30           15\nGPU6   15           30           30           30           15           30           0            15\nGPU7   30           45           30           15           30           15           15           0\n\n================================= Hops between two GPUs ==================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            1            1            1            1            1            1            1\nGPU1   1            0            1            1            1            1            1            1\nGPU2   1            1            0            1            1            1            1            1\nGPU3   1            1            1            0            1            1            1            1\nGPU4   1            1            1            1            0            1            1            1\nGPU5   1            1            1            1            1            0            1            1\nGPU6   1            1            1            1            1            1            0            1\nGPU7   1            1            1            1            1            1            1            0\n\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI\nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI\nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI\nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI\nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI\nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0\n\n======================================= Numa Nodes =======================================\nGPU[0]          : (Topology) Numa Node: 3\nGPU[0]          : (Topology) Numa Affinity: 3\nGPU[1]          : (Topology) Numa Node: 3\nGPU[1]          : (Topology) Numa Affinity: 3\nGPU[2]          : (Topology) Numa Node: 2\nGPU[2]          : (Topology) Numa Affinity: 2\nGPU[3]          : (Topology) Numa Node: 2\nGPU[3]          : (Topology) Numa Affinity: 2\nGPU[4]          : (Topology) Numa Node: 7\nGPU[4]          : (Topology) Numa Affinity: 7\nGPU[5]          : (Topology) Numa Node: 7\nGPU[5]          : (Topology) Numa Affinity: 7\nGPU[6]          : (Topology) Numa Node: 6\nGPU[6]          : (Topology) Numa Affinity: 6\nGPU[7]          : (Topology) Numa Node: 6\nGPU[7]          : (Topology) Numa Affinity: 6\n================================== End of ROCm SMI Log ===================================\n\n==============================\n     Environment Variables\n==============================\nPYTORCH_ROCM_ARCH=gfx90a;gfx942\nVLLM_ROCM_CUSTOM_PAGED_ATTN=1\nVLLM_TARGET_DEVICE=rocm\nLD_LIBRARY_PATH=/opt/rocm/lib:/usr/local/lib:\nVLLM_USE_V1=1\nVERBOSE=1\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nTORCHINDUCTOR_CACHE_DIR=/tmp/torchinductor_root\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nMultiple architectures, such as Qwen2, use Sliding Window Attention. However, there is no option in V1 to run Sliding Window Attention on ROCm. Sending a request to the server crashes, for:\n```bash\nMODEL_NAME=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\nexport VLLM_USE_V1=1\npython3 -m vllm.entrypoints.openai.api_server   --port 8080   --model $MODEL_NAME   --served-model-name $MODEL_NAME   --gpu-memory-utilization 0.95   --disable-custom-all-reduce   --tensor-parallel-size 1   --enable-chunked-prefill   --disable-log-requests   --enable-reasoning   --reasoning-parser deepseek_r1\n```\nThis is because it uses Triton Flash Attention, which not support Sliding Window Attention. As a result, sending a request to the server crashes vLLM:\n\n**Request:**\n```bash\ncurl -iX POST \"http://localhost:8080/v1/chat/completions\"         -H \"Content-Type: application/json\"         -d '{\n    \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n    \"messages\": [{ \"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n    \"stream\": false\n  }'\n```\n\n\n**Resulting logs from crash:**\n``` \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/triton/language/core.py\", line 34, in wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/triton/language/core.py\", line 1281, in arange\n    return semantic.arange(start, end, _builder)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/triton/language/semantic.py\", line 610, in arange\n    raise ValueError(\"arange's range must be a power of 2\")\nValueError: arange's range must be a power of 2\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/vllm/vllm/v1/engine/core.py\", line 493, in run_engine_core\n    raise e\n  File \"/vllm/vllm/v1/engine/core.py\", line 482, in run_engine_core\n    engine_core.run_busy_loop()\n  File \"/vllm/vllm/v1/engine/core.py\", line 509, in run_busy_loop\n    self._process_engine_step()\n  File \"/vllm/vllm/v1/engine/core.py\", line 534, in _process_engine_step\n    outputs = self.step_fn()\n              ^^^^^^^^^^^^^^\n  File \"/vllm/vllm/v1/engine/core.py\", line 222, in step\n    model_output = self.execute_model(scheduler_output)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/v1/engine/core.py\", line 209, in execute_model\n    raise err\n  File \"/vllm/vllm/v1/engine/core.py\", line 203, in execute_model\n    return self.model_executor.execute_model(scheduler_output)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/v1/executor/abstract.py\", line 86, in execute_model\n    output = self.collective_rpc(\"execute_model\",\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/utils.py\", line 2534, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/v1/worker/gpu_worker.py\", line 276, in execute_model\n    output = self.model_runner.execute_model(scheduler_output,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/v1/worker/gpu_model_runner.py\", line 1156, in execute_model\n    model_output = self.model(\n                   ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/model_executor/models/qwen2.py\", line 480, in forward\n    hidden_states = self.model(input_ids, positions, intermediate_tensors,\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/compilation/decorators.py\", line 245, in __call__\n    model_output = self.forward(*args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/model_executor/models/qwen2.py\", line 339, in forward\n    def forward(\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\", line 764, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\", line 830, in call_wrapped\n    return self._wrapped_call(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\", line 406, in __call__\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\", line 393, in __call__\n    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<eval_with_key>.58\", line 212, in forward\n    submod_1 = self.submod_1(getitem, s0, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\", line 830, in call_wrapped\n    return self._wrapped_call(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\", line 406, in __call__\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py\", line 393, in __call__\n    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<eval_with_key>.2\", line 5, in forward\n    unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_1, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_1 = unified_attention_with_output = None\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_ops.py\", line 1156, in __call__\n    return self._op(*args, **(kwargs or {}))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/vllm/vllm/attention/layer.py\", line 425, in unified_attention_with_output\n    self.impl.forward(self,\n  File \"/vllm/vllm/v1/attention/backends/triton_attn.py\", line 201, in forward\n    unified_attention(\n  File \"/vllm/vllm/attention/ops/triton_unified_attention.py\", line 294, in unified_attention\n    kernel_unified_attention_2d[(\n  File \"/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py\", line 330, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py\", line 623, in run\n    kernel = self.compile(\n             ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py\", line 280, in compile\n    module = src.make_ir(options, codegen_fns, module_map, context)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py\", line 85, in make_ir\n    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntriton.compiler.errors.CompilationError: at 67:13:\n    q_block_local_idx = q_block_global_idx - q_block_start_idx\n\n    cur_batch_in_all_start_index = tl.load(query_start_len_ptr + seq_idx)\n    cur_batch_in_all_stop_index = tl.load(query_start_len_ptr + seq_idx + 1)\n\n    cur_batch_query_len = cur_batch_in_all_stop_index \\\n        - cur_batch_in_all_start_index\n\n    if q_block_local_idx * BLOCK_Q >= cur_batch_query_len:\n        return\n\n    offs_m = tl.arange(0, BLOCK_Q * num_queries_per_kv)\n             ^\n\n```\n\nIn V0, one can use ROCm's Custom Paged Attention, however this is not supported on V1, apparently due to [numerical instabilities on V1](https://github.com/vllm-project/vllm/blob/c1c7dbbeeb6d4f0155d25b673f2063bfb14b37b9/vllm/platforms/rocm.py#L135):\n```python \n   # custom paged attn always supported on V0. On V1, requires sliding window\n   # disabled due to observed numerical discrepancy.\n    if ON_GFX9:\n        return ((not envs.VLLM_USE_V1 or sliding_window == 0\n                 or sliding_window == (-1, -1))\n                and (qtype == torch.half or qtype == torch.bfloat16)\n                and (head_size == 64 or head_size == 128)\n                and (block_size == 16 or block_size == 32)\n                and (gqa_ratio >= 1 and gqa_ratio <= 16)\n                and max_seq_len <= 32768 and (envs.VLLM_ROCM_CUSTOM_PAGED_ATTN)\n                and not (envs.VLLM_ROCM_USE_AITER_PAGED_ATTN\n                         and envs.VLLM_ROCM_USE_AITER))\n\n    else:\n        return (ON_GFX11_GFX12 and (not envs.VLLM_USE_V1 or sliding_window == 0\n                                    or sliding_window == (-1, -1))\n                and (qtype == torch.half or qtype == torch.bfloat16)\n                and head_size == 128 and block_size == 16\n                and (gqa_ratio >= 3 and gqa_ratio <= 16)\n                and max_seq_len <= 32768 and alibi_slopes is None\n                and kv_cache_dtype == \"auto\"\n                and envs.VLLM_ROCM_CUSTOM_PAGED_ATTN)\n\n```\n\nSetting `VLLM_USE_TRITON_FLASH_ATTN=0` does not work on V1, as it will still use Triton Flash Attention despite the flag, and suffer from the resulting crash if one sends a request. E.g. from the logs:\n``` \nINFO 06-09 16:08:58 [rocm.py:184] Using Triton Attention backend on V1 engine.\n```\n\nAs such, there is no way to run Qwen2 architectures, or any architectures that use Sliding Window Attention, on ROCm in V1. [Given the plans to deprecate V0](https://github.com/vllm-project/vllm/issues/18571), this is going to be quite concerning for ROCm.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2025-06-09T16:12:29+00:00",
    "closed_at": "2025-06-10T20:28:04+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19367/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19367"
  },
  {
    "number": 12178,
    "title": "[Bug]: AMD GPU docker image build No matching distribution found for torch==2.6.0.dev20241113+rocm6.2",
    "body": "### Your current environment\n\nArchlinux 13th Gen Intel(R) Core(TM) i9-13900HX environment to build the docker image\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nTrying to build the AMD GPU docker image:\n```\ngit checkout v0.6.6.post1\nDOCKER_BUILDKIT=1 docker build -f Dockerfile.rocm -t substratusai/vllm-rocm:v0.6.6.post1 .\n```\n\nResults in following error:\n\n```\n1.147 Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/rocm6.2\n1.717 ERROR: Could not find a version that satisfies the requirement torch==2.6.0.dev20241113+rocm6.2 (from versions: 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0.dev20241119+rocm6.2, 2.6.0.dev20241120+rocm6.2, 2.6.0.dev20241121+rocm6.2, 2.6.0.dev20241122+rocm6.2)\n2.135 ERROR: No matching distribution found for torch==2.6.0.dev20241113+rocm6.2\n------\nDockerfile.rocm:49\n--------------------\n  48 |     # Install torch == 2.6.0 on ROCm\n  49 | >>> RUN --mount=type=cache,target=/root/.cache/pip \\\n  50 | >>>     case \"$(ls /opt | grep -Po 'rocm-[0-9]\\.[0-9]')\" in \\\n  51 | >>>         *\"rocm-6.2\"*) \\\n  52 | >>>             python3 -m pip uninstall -y torch torchvision \\\n  53 | >>>             && python3 -m pip install --pre \\\n  54 | >>>                 torch==2.6.0.dev20241113+rocm6.2 \\\n  55 | >>>                 'setuptools-scm>=8' \\\n  56 | >>>                 torchvision==0.20.0.dev20241113+rocm6.2 \\\n  57 | >>>                 --extra-index-url https://download.pytorch.org/whl/nightly/rocm6.2;; \\\n  58 | >>>         *) ;; esac\n  59 |     \n--------------------\nERROR: failed to solve: process \"/bin/sh -c case \\\"$(ls /opt | grep -Po 'rocm-[0-9]\\\\.[0-9]')\\\" in         *\\\"rocm-6.2\\\"*)             python3 -m pip uninstall -y torch torchvision             && python3 -m pip install --pre                 torch==2.6.0.dev20241113+rocm6.2                 'setuptools-scm>=8'                 torchvision==0.20.0.dev20241113+rocm6.2                 --extra-index-url https://download.pytorch.org/whl/nightly/rocm6.2;;         *) ;; esac\" did not complete successfully: exit code: 1\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2025-01-17T23:36:10+00:00",
    "closed_at": "2025-03-12T05:50:14+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12178/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/12178"
  },
  {
    "number": 4432,
    "title": "[Bug]:  all_reduce assert result == 0, File \"torch/cuda/graphs.py\", line 88, in capture_end    super().capture_end(), RuntimeError: CUDA error: operation failed due to a previous error during capture",
    "body": "### Your current environment\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.1.2+cu118\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.8\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Centos 7 (Final) (x86_64)\r\nGCC version: (GCC) 7.3.0\r\nClang version: Could not collect\r\nCMake version: version 3.26.1\r\nLibc version: glibc-2.17\r\n\r\nPython version: 3.8.12 (default, Nov 11 2021, 20:11:20)  [GCC 7.3.0] (64-bit runtime)\r\nPython platform: Linux-4.14.105-1-tlinux3-0013-x86_64-with-glibc2.2.5\r\nIs CUDA available: True\r\nCUDA runtime version: 11.8.89\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-SXM2-32GB\r\nGPU 1: Tesla V100-SXM2-32GB\r\nGPU 2: Tesla V100-SXM2-32GB\r\nGPU 3: Tesla V100-SXM2-32GB\r\nGPU 4: Tesla V100-SXM2-32GB\r\nGPU 5: Tesla V100-SXM2-32GB\r\nGPU 6: Tesla V100-SXM2-32GB\r\nGPU 7: Tesla V100-SXM2-32GB\r\n\r\nNvidia driver version: 450.156.00\r\ncuDNN version: Probably one of the following:\r\n/usr/lib64/libcudnn.so.8.0.5\r\n/usr/lib64/libcudnn_adv_infer.so.8.0.5\r\n/usr/lib64/libcudnn_adv_train.so.8.0.5\r\n/usr/lib64/libcudnn_cnn_infer.so.8.0.5\r\n/usr/lib64/libcudnn_cnn_train.so.8.0.5\r\n/usr/lib64/libcudnn_ops_infer.so.8.0.5\r\n/usr/lib64/libcudnn_ops_train.so.8.0.5\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                96\r\nOn-line CPU(s) list:   0-95\r\nThread(s) per core:    2\r\nCore(s) per socket:    24\r\nSocket(s):             2\r\nNUMA node(s):          2\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 85\r\nModel name:            Intel(R) Xeon(R) Platinum 8255C CPU @ 2.50GHz\r\nStepping:              7\r\nCPU MHz:               3099.587\r\nCPU max MHz:           2501.0000\r\nCPU min MHz:           1000.0000\r\nBogoMIPS:              5000.00\r\nVirtualization:        VT-x\r\nL1d cache:             32K\r\nL1i cache:             32K\r\nL2 cache:              1024K\r\nL3 cache:              36608K\r\nNUMA node0 CPU(s):     0-23,48-71\r\nNUMA node1 CPU(s):     24-47,72-95\r\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke avx512_vnni flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.23.5\r\n[pip3] nvidia-nccl-cu11==2.19.3\r\n[pip3] pytorchvideo==0.1.5\r\n[pip3] torch==2.1.2+cu118\r\n[pip3] torchaudio==0.9.0\r\n[pip3] torchdata==0.6.0\r\n[pip3] torchvision==0.15.2\r\n[pip3] triton==2.1.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    mlx5_0  mlx5_1  mlx5_2  mlx5_3  mlx5_4  mlx5_5  mlx5_6  mlx5_7  mlx5_8  mlx5_9  mlx5_10 mlx5_11 mlx5_12 mlx5_13 mlx5_14 mlx5_15 mlx5_16   mlx5_17 CPU Affinity    NUMA Affinity\r\nGPU0     X      NV1     NV2     NV1     SYS     SYS     SYS     NV2     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE      NODE    NODE    0-23,48-71      0\r\nGPU1    NV1      X      NV1     NV2     SYS     SYS     NV2     SYS     NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE    NODE      NODE    NODE    0-23,48-71      0\r\nGPU2    NV2     NV1      X      NV2     SYS     NV1     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     0-23,48-71      0\r\nGPU3    NV1     NV2     NV2      X      NV1     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX     0-23,48-71      0\r\nGPU4    SYS     SYS     SYS     NV1      X      NV2     NV2     NV1     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     24-47,72-95     1\r\nGPU5    SYS     SYS     NV1     SYS     NV2      X      NV1     NV2     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     24-47,72-95     1\r\nGPU6    SYS     NV2     SYS     SYS     NV2     NV1      X      NV1     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     24-47,72-95     1\r\nGPU7    NV2     SYS     SYS     SYS     NV1     NV2     NV1      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS       SYS     SYS     24-47,72-95     1\r\nmlx5_0  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_1  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_2  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_3  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_4  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_5  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_6  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_7  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_8  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_9  NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_10 NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_11 NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX     PIX       PIX     PIX\r\nmlx5_12 NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX     PIX       PIX     PIX\r\nmlx5_13 NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX     PIX       PIX     PIX\r\nmlx5_14 NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X      PIX       PIX     PIX\r\nmlx5_15 NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX      X        PIX     PIX\r\nmlx5_16 NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX        X      PIX\r\nmlx5_17 NODE    NODE    PIX     PIX     SYS     SYS     SYS     SYS     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX     PIX       PIX      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\n## when i run starcoder2, error come out:\r\n\r\n2024-04-28 20:49:41,941 INFO worker.py:1752 -- Started a local Ray instance.\r\nINFO 04-28 20:49:43 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='/apdcephfs_cq10/share_1567347/share_info/llm_models/starcoder2-15b', tokenizer='/apdcephfs_cq10/share_1567347/share_info/llm_models/starcoder2-15b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\r\n/usr/local/python/lib/python3.8/site-packages/vllm/executor/ray_gpu_executor.py:87: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable HOST_IP.\r\n  driver_ip = get_ip()\r\n(RayWorkerVllm pid=62031) /usr/local/python/lib/python3.8/site-packages/vllm/engine/ray_utils.py:48: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable HOST_IP.\r\n(RayWorkerVllm pid=62031)   return get_ip()\r\nINFO 04-28 20:49:50 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\r\nINFO 04-28 20:49:50 selector.py:25] Using XFormers backend.\r\n(RayWorkerVllm pid=62111) INFO 04-28 20:49:52 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\r\n(RayWorkerVllm pid=62111) INFO 04-28 20:49:52 selector.py:25] Using XFormers backend.\r\nINFO 04-28 20:49:52 pynccl_utils.py:45] vLLM is using nccl==2.10.3\r\n(RayWorkerVllm pid=62111) INFO 04-28 20:49:52 pynccl_utils.py:45] vLLM is using nccl==2.10.3\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Bootstrap : Using eth1:9.91.2.209<0>\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO NET/Socket : Using [0]eth1:9.91.2.209<0>\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Using network Socket\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Bootstrap : Using eth1:9.91.2.209<0>\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO cudaDriverVersion 11080\r\nNCCL version 2.18.6+cuda11.8\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO NET/Socket : Using [0]eth1:9.91.2.209<0>\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO Using network Socket\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO comm 0x53f905b0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1a000 commId 0x605ed9e94f174b - Init START\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO Channel 00/02 :    0   1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO Channel 01/02 :    0   1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO P2P Chunksize set to 131072\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO Connected all rings\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO Connected all trees\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62307 [0] NCCL INFO comm 0x53f905b0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1a000 commId 0x605ed9e94f174b - Init COMPLETE\r\nNCCL version 2.10.3+cuda11.0\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Channel 00/02 :    0   1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Channel 01/02 :    0   1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Channel 00 : 0[1a000] -> 1[1b000] via direct shared memory\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Channel 01 : 0[1a000] -> 1[1b000] via direct shared memory\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Connected all rings\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Connected all trees\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO comm 0x5483be20 rank 0 nranks 2 cudaDev 0 busId 1a000 - Init COMPLETE\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO Bootstrap : Using eth1:9.91.2.209<0>\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO NET/Socket : Using [0]eth1:9.91.2.209<0>\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO Using network Socket\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to LOC\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO Channel 00 : 1[1b000] -> 0[1a000] via direct shared memory\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO Channel 01 : 1[1b000] -> 0[1a000] via direct shared memory\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO Connected all rings\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO Connected all trees\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO Launch mode Parallel\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO comm 0xbd04190 rank 1 nranks 2 cudaDev 1 busId 1b000 - Init COMPLETE\r\nINFO 04-28 20:50:02 model_runner.py:104] Loading model weights took 14.8672 GB\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO Using network Socket\r\n(RayWorkerVllm pid=62111) INFO 04-28 20:50:12 model_runner.py:104] Loading model weights took 14.8672 GB\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO comm 0x98440b0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1a000 commId 0x59afa392e79b7504 - Init START\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO Channel 00/02 :    0   1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO Channel 01/02 :    0   1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO P2P Chunksize set to 131072\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO Connected all rings\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO Connected all trees\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:62379 [0] NCCL INFO comm 0x98440b0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1a000 commId 0x59afa392e79b7504 - Init COMPLETE\r\nINFO 04-28 20:50:20 ray_gpu_executor.py:240] # GPU blocks: 15176, # CPU blocks: 6553\r\nINFO 04-28 20:50:23 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 04-28 20:50:23 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(RayWorkerVllm pid=62111) INFO 04-28 20:50:23 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n(RayWorkerVllm pid=62111) INFO 04-28 20:50:23 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] enqueue.cc:267 NCCL WARN Cuda failure 'dependency created on uncaptured work in another stream'\r\nts-1580615599d3449d98cf56a265c10977-worker-6:56890:56890 [0] NCCL INFO enqueue.cc:1045 -> 1\r\nTraceback (most recent call last):\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/worker/model_runner.py\", line 921, in capture\r\n    hidden_states = self.model(\r\n  File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/models/starcoder2.py\", line 260, in forward\r\n    hidden_states = self.model(input_ids, positions, kv_caches,\r\n  File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/models/starcoder2.py\", line 219, in forward\r\n    hidden_states = self.embed_tokens(input_ids)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 107, in forward\r\n    output = tensor_model_parallel_all_reduce(output_parallel)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/parallel_utils/communication_op.py\", line 35, in tensor_model_parallel_all_reduce\r\n    pynccl_utils.all_reduce(input_)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/parallel_utils/pynccl_utils.py\", line 55, in all_reduce\r\n    comm.all_reduce(input_, op)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/parallel_utils/pynccl.py\", line 258, in all_reduce\r\n    assert result == 0\r\nAssertionError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"lua_test_file_gen_vllm.py\", line 221, in <module>\r\n    main()\r\n  File \"lua_test_file_gen_vllm.py\", line 105, in main\r\n    llm = LLM(model=args.model, tensor_parallel_size=args.num_gpus, dtype=\"float16\", gpu_memory_utilization=0.9)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/entrypoints/llm.py\", line 112, in __init__\r\n    self.llm_engine = LLMEngine.from_engine_args(\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/engine/llm_engine.py\", line 196, in from_engine_args\r\n    engine = cls(\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/engine/llm_engine.py\", line 110, in __init__\r\n    self.model_executor = executor_class(model_config, cache_config,\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/executor/ray_gpu_executor.py\", line 65, in __init__\r\n    self._init_cache()\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/executor/ray_gpu_executor.py\", line 253, in _init_cache\r\n    self._run_workers(\"warm_up_model\")\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/executor/ray_gpu_executor.py\", line 324, in _run_workers\r\n    driver_worker_output = getattr(self.driver_worker,\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/worker/worker.py\", line 167, in warm_up_model\r\n    self.model_runner.capture_model(self.gpu_cache)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/worker/model_runner.py\", line 854, in capture_model\r\n    graph_runner.capture(\r\n  File \"/usr/local/python/lib/python3.8/site-packages/vllm/worker/model_runner.py\", line 921, in capture\r\n    hidden_states = self.model(\r\n  File \"/usr/local/python/lib/python3.8/site-packages/torch/cuda/graphs.py\", line 197, in __exit__\r\n    self.cuda_graph.capture_end()\r\n  File \"/usr/local/python/lib/python3.8/site-packages/torch/cuda/graphs.py\", line 88, in capture_end\r\n    super().capture_end()\r\nRuntimeError: CUDA error: operation failed due to a previous error during capture\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n(RayWorkerVllm pid=62111) \r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] enqueue.cc:267 NCCL WARN Cuda failure 'dependency created on uncaptured work in another stream'\r\n(RayWorkerVllm pid=62111) ts-1580615599d3449d98cf56a265c10977-worker-6:62111:62111 [1] NCCL INFO enqueue.cc:1045 -> 1\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] Error executing method warm_up_model. This might cause deadlock in distributed execution.\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] Traceback (most recent call last):\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/worker/model_runner.py\", line 921, in capture\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     hidden_states = self.model(\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     return self._call_impl(*args, **kwargs)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     return forward_call(*args, **kwargs)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/models/starcoder2.py\", line 260, in forward\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     hidden_states = self.model(input_ids, positions, kv_caches,\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     return self._call_impl(*args, **kwargs)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     return forward_call(*args, **kwargs)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/models/starcoder2.py\", line 219, in forward\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     hidden_states = self.embed_tokens(input_ids)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     return self._call_impl(*args, **kwargs)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     return forward_call(*args, **kwargs)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 107, in forward\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     output = tensor_model_parallel_all_reduce(output_parallel)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/parallel_utils/communication_op.py\", line 35, in tensor_model_parallel_all_reduce\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     pynccl_utils.all_reduce(input_)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/parallel_utils/pynccl_utils.py\", line 55, in all_reduce\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     comm.all_reduce(input_, op)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/model_executor/parallel_utils/pynccl.py\", line 258, in all_reduce\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     assert result == 0\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] AssertionError\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] \r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] During handling of the above exception, another exception occurred:\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] \r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] Traceback (most recent call last):\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/engine/ray_utils.py\", line 37, in execute_method\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     return executor(*args, **kwargs)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/worker/worker.py\", line 167, in warm_up_model\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     self.model_runner.capture_model(self.gpu_cache)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     return func(*args, **kwargs)\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/worker/model_runner.py\", line 854, in capture_model\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     graph_runner.capture(\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/vllm/worker/model_runner.py\", line 921, in capture\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     hidden_states = self.model(\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/torch/cuda/graphs.py\", line 197, in __exit__\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     self.cuda_graph.capture_end()\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]   File \"/usr/local/python/lib/python3.8/site-packages/torch/cuda/graphs.py\", line 88, in capture_end\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44]     super().capture_end()\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] RuntimeError: CUDA error: operation failed due to a previous error during capture\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n(RayWorkerVllm pid=62111) ERROR 04-28 20:50:23 ray_utils.py:44] \r\n(RayWorkerVllm pid=62111) /usr/local/python/lib/python3.8/site-packages/vllm/engine/ray_utils.py:48: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable HOST_IP.\r\n(RayWorkerVllm pid=62111)   return get_ip()",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-28T13:01:49+00:00",
    "closed_at": "2024-11-28T02:05:53+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4432/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4432"
  },
  {
    "number": 3627,
    "title": "[Bug]: System error: Can't get attribute 'TokenizerGroup' on <module 'vllm.transformers_utils.tokenizer'",
    "body": "### Your current environment\r\n\r\n```text\r\ncuda 12.1  simple pip install vllm\r\n```\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\n`python benchmarks/benchmark_throughput.py --backend vllm --input-len 1024 --output-len 512 --model /share/datasets/public_models/Qwen_Qwen-72B-Chat --tensor-parallel-size 4 --trust-remote-code`\r\n\r\nThis will result in the following errors:\r\n\r\n![image](https://github.com/vllm-project/vllm/assets/79788571/f2bb89b0-bc19-4e3b-859f-0b63ffe76dd7)",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-26T04:36:28+00:00",
    "closed_at": "2024-11-29T02:07:04+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3627/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3627"
  },
  {
    "number": 7550,
    "title": "[Bug]:  online fp8 quantization with jais model got assert error due to cutlass_scaled_mm()",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n\r\nNvidia driver version: 555.42.06\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nCPU(s):                             384\r\nOn-line CPU(s) list:                0-383\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 96\r\nSocket(s):                          2\r\nNUMA node(s):                       2\r\nVendor ID:                          AuthenticAMD\r\nCPU family:                         25\r\nModel:                              17\r\nModel name:                         AMD EPYC 9654 96-Core Processor\r\nStepping:                           1\r\nFrequency boost:                    enabled\r\nCPU MHz:                            1500.000\r\nCPU max MHz:                        3707.8120\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           4799.99\r\nVirtualization:                     AMD-V\r\nL1d cache:                          6 MiB\r\nL1i cache:                          6 MiB\r\nL2 cache:                           192 MiB\r\nL3 cache:                           768 MiB\r\nNUMA node0 CPU(s):                  0-95,192-287\r\nNUMA node1 CPU(s):                  96-191,288-383\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; Safe RET\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov succor smca fsrm flush_l1d\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.2+cu121torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] pyzmq==26.1.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.0\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.4@\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\n\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nrun `throughput benchmark.py`  with jais-13B/30B models with following command:\r\n\r\n```sh\r\n python3 /vllm/benchmarks/benchmark_throughput.py --model core42/jais-13b-chat  --num-prompts $req -tp $tp --distributed-executor-backend mp --input-len $inp --output-len $out --trust-remote-code --dtype auto --enforce-eager   --quantization fp8\r\n```\r\n\r\nerror logs as:\r\n\r\n```yml\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/jais.py\", line 206, in forward\r\n[rank0]:     feed_forward_hidden_states = self.mlp(hidden_states)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/jais.py\", line 162, in forward\r\n[rank0]:     hidden_states2, _ = self.c_fc2(hidden_states)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py\", line 355, in forward\r\n[rank0]:     output_parallel = self.quant_method.apply(self, input_, bias)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/fp8.py\", line 240, in apply\r\n[rank0]:     return apply_fp8_linear(\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/utils/w8a8_utils.py\", line 126, in apply_fp8_linear\r\n[rank0]:     return ops.cutlass_scaled_mm(qinput,\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/_custom_ops.py\", line 32, in wrapper\r\n[rank0]:     return fn(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/_custom_ops.py\", line 242, in cutlass_scaled_mm\r\n[rank0]:     assert (b.shape[0] % 16 == 0 and b.shape[1] % 16 == 0)\r\n``` \r\n",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-15T09:28:39+00:00",
    "closed_at": "2025-04-16T02:20:10+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7550/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7550"
  },
  {
    "number": 8531,
    "title": "[Bug]: benchmark_serving.py generates different numbers of tokens at different runs",
    "body": "### Your current environment\n\n4xH100.\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nWhen benchmarking the performance of vllm with `benchmark_serving.py`, it will generate different number of tokens at different runs.\r\n\r\nCode to launch vllm server\r\n```\r\nvllm serve meta-llama/Meta-Llama-3.1-70B-Instruct \\\r\n    --disable-log-requests \\\r\n    --tensor-parallel-size 4\r\n```\r\n\r\nCode to run the benchmark\r\n```\r\npython benchmarks/benchmark_serving.py \\\r\n    --backend vllm \\\r\n    --model meta-llama/Meta-Llama-3.1-70B-Instruct\\\r\n    --dataset-name sharegpt \\\r\n    --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \\\r\n    --request-rate 1 \\\r\n    --num-prompts 200 \\\r\n    --save-result\r\n```\r\n\r\nIf I run the benchmark_serving.py script twice, the number of generated tokens is different for the two runs.\r\nThe output of the first run:\r\n```\r\n============ Serving Benchmark Result ============                                                                                                                                                                     \r\nSuccessful requests:                     200                                                                                                                                                                           \r\nBenchmark duration (s):                  203.41                                                                                                                                                                        \r\nTotal input tokens:                      42659                                                                                                                                                                         \r\nTotal generated tokens:                  **38614**                                                                                                                                                                         \r\nRequest throughput (req/s):              0.98                                                                                                                                                                          \r\nOutput token throughput (tok/s):         189.84                                                                                                                                                                        \r\nTotal Token throughput (tok/s):          399.56                                                                                                                                                                        \r\n---------------Time to First Token----------------                                                                                                                                                                     \r\nMean TTFT (ms):                          62.95                                                                                                                                                                         \r\nMedian TTFT (ms):                        64.68                                                                                                                                                                         \r\nP99 TTFT (ms):                           141.49                                                                                                                                                                        \r\n-----Time per Output Token (excl. 1st token)------                                                                                                                                                                     \r\nMean TPOT (ms):                          20.10                                                                                                                                                                         \r\nMedian TPOT (ms):                        19.93                                                                                                                                                                         \r\nP99 TPOT (ms):                           24.28                                                                                                                                                                         \r\n---------------Inter-token Latency----------------                                                                                                                                                                     \r\nMean ITL (ms):                           19.98                                                                                                                                                                         \r\nMedian ITL (ms):                         19.60                                                                                                                                                                         \r\nP99 ITL (ms):                            44.31                                                                                                                                                                         \r\n================================================== \r\n```\r\nTotal generated tokens:                  **38614**         \r\nThe output of the second run\r\n```\r\n============ Serving Benchmark Result ============                                                                                                                                                              [3/452]\r\nSuccessful requests:                     200                                                                                                                                                                           \r\nBenchmark duration (s):                  203.40                                                                                                                                                                        \r\nTotal input tokens:                      42659                                                                                                                                                                         \r\nTotal generated tokens:                  **38536**     \r\nRequest throughput (req/s):              0.98      \r\nOutput token throughput (tok/s):         189.46    \r\nTotal Token throughput (tok/s):          399.20    \r\n---------------Time to First Token----------------\r\nMean TTFT (ms):                          60.23     \r\nMedian TTFT (ms):                        64.19     \r\nP99 TTFT (ms):                           127.43    \r\n-----Time per Output Token (excl. 1st token)------\r\nMean TPOT (ms):                          20.01     \r\nMedian TPOT (ms):                        19.87     \r\nP99 TPOT (ms):                           22.67     \r\n---------------Inter-token Latency----------------\r\nMean ITL (ms):                           19.93     \r\nMedian ITL (ms):                         19.57     \r\nP99 ITL (ms):                            43.91     \r\n==================================================\r\n```\r\nTotal generated tokens:                  **38536**  .\r\nEven if I relaunch the server for the second run, the randomness still exists.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-17T06:09:27+00:00",
    "closed_at": "2025-03-01T02:05:45+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8531/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8531"
  },
  {
    "number": 16141,
    "title": "[Bug]: V1 engine peak memory usage calculations incorrect",
    "body": "### Your current environment\n\nThe `collect_env.py` script doesn't work because I don't have vllm installed in my environment. This bug is reproducible using the docker image, so I don't think this matters.\n\nAffected VLLM version is `v0.8.3`.\n\n### \ud83d\udc1b Describe the bug\n\nThe peak memory usage calculations for VLLM is buggy. It seems to think that the memory usage of the other processes on the GPU contribute to the minimum required. This happens with `v0.8.3`.\n\nThis is a problem when running multiple instances of VLLM on the same GPU.\n\n## Repro steps\n\nThis is easy to reproduce with the docker image. Here is the `nvidia-smi` output before running VLLM. No memory usage.\n\n```text\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA RTX A6000               Off |   00000000:81:00.0 Off |                    0 |\n| 30%   39C    P2             N/A /  300W |       1MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n```\n\nThen I run one instance\n\n```bash\ndocker run --rm --gpus=all --runtime nvidia --ipc=host \\\n    -v ~/.cache:/root/.cache \\\n    vllm/vllm-openai:v0.8.3 \\\n    --model Qwen/Qwen2.5-0.5B-Instruct-AWQ \\\n    --gpu-memory-utilization 0.1 \\\n    --max-num-seqs 1 \\\n    --max-model-len 512\n```\n\n<details>\n<summary>Complete logs</summary>\n\n```text\nINFO 04-06 18:42:42 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-06 18:42:45 [api_server.py:1034] vLLM API server version 0.8.3\nINFO 04-06 18:42:45 [api_server.py:1035] args: Namespace(host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2.5-0.5B-Instruct-AWQ', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=512, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.1, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=1, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)\nINFO 04-06 18:42:53 [config.py:600] This model supports multiple tasks: {'embed', 'score', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\nINFO 04-06 18:42:54 [awq_marlin.py:114] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\nINFO 04-06 18:42:54 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=2048.\nINFO 04-06 18:42:59 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-06 18:43:01 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='Qwen/Qwen2.5-0.5B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=512, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-0.5B-Instruct-AWQ, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\nWARNING 04-06 18:43:02 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f6d3b5030e0>\nINFO 04-06 18:43:02 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 04-06 18:43:02 [cuda.py:221] Using Flash Attention backend on V1 engine.\nINFO 04-06 18:43:02 [gpu_model_runner.py:1258] Starting to load model Qwen/Qwen2.5-0.5B-Instruct-AWQ...\nINFO 04-06 18:43:03 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.\nINFO 04-06 18:43:03 [weight_utils.py:265] Using model weights format ['*.safetensors']\nINFO 04-06 18:43:03 [weight_utils.py:315] No model.safetensors.index.json found in remote.\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.51it/s]\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.50it/s]\n\nINFO 04-06 18:43:03 [loader.py:447] Loading weights took 0.18 seconds\nINFO 04-06 18:43:04 [gpu_model_runner.py:1273] Model loading took 0.4315 GiB and 0.935828 seconds\nINFO 04-06 18:43:10 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/e05df7ec0c/rank_0_0 for vLLM's torch.compile\nINFO 04-06 18:43:10 [backends.py:426] Dynamo bytecode transform time: 6.97 s\nINFO 04-06 18:43:11 [backends.py:115] Directly load the compiled graph for shape None from the cache\nINFO 04-06 18:43:17 [monitor.py:33] torch.compile takes 6.97 s in total\nINFO 04-06 18:43:17 [kv_cache_utils.py:578] GPU KV cache size: 314,272 tokens\nINFO 04-06 18:43:17 [kv_cache_utils.py:581] Maximum concurrency for 512 tokens per request: 613.81x\nINFO 04-06 18:43:35 [gpu_model_runner.py:1608] Graph capturing finished in 18 secs, took 0.41 GiB\nINFO 04-06 18:43:35 [core.py:162] init engine (profile, create kv cache, warmup model) took 31.70 seconds\nWARNING 04-06 18:43:35 [config.py:1088] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\nINFO 04-06 18:43:35 [serving_chat.py:117] Using default chat sampling params from model: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\nINFO 04-06 18:43:35 [serving_completion.py:61] Using default completion sampling params from model: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\nINFO 04-06 18:43:35 [api_server.py:1081] Starting vLLM API server on http://0.0.0.0:8000\nINFO 04-06 18:43:35 [launcher.py:26] Available routes are:\nINFO 04-06 18:43:35 [launcher.py:34] Route: /openapi.json, Methods: GET, HEAD\nINFO 04-06 18:43:35 [launcher.py:34] Route: /docs, Methods: GET, HEAD\nINFO 04-06 18:43:35 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: GET, HEAD\nINFO 04-06 18:43:35 [launcher.py:34] Route: /redoc, Methods: GET, HEAD\nINFO 04-06 18:43:35 [launcher.py:34] Route: /health, Methods: GET\nINFO 04-06 18:43:35 [launcher.py:34] Route: /load, Methods: GET\nINFO 04-06 18:43:35 [launcher.py:34] Route: /ping, Methods: GET, POST\nINFO 04-06 18:43:35 [launcher.py:34] Route: /tokenize, Methods: POST\nINFO 04-06 18:43:35 [launcher.py:34] Route: /detokenize, Methods: POST\nINFO 04-06 18:43:35 [launcher.py:34] Route: /v1/models, Methods: GET\nINFO 04-06 18:43:35 [launcher.py:34] Route: /version, Methods: GET\nINFO 04-06 18:43:35 [launcher.py:34] Route: /v1/chat/completions, Methods: POST\nINFO 04-06 18:43:35 [launcher.py:34] Route: /v1/completions, Methods: POST\nINFO 04-06 18:43:35 [launcher.py:34] Route: /v1/embeddings, Methods: POST\nINFO 04-06 18:43:35 [launcher.py:34] Route: /pooling, Methods: POST\nINFO 04-06 18:43:35 [launcher.py:34] Route: /score, Methods: POST\nINFO 04-06 18:43:35 [launcher.py:34] Route: /v1/score, Methods: POST\nINFO 04-06 18:43:35 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST\nINFO 04-06 18:43:35 [launcher.py:34] Route: /rerank, Methods: POST\nINFO 04-06 18:43:35 [launcher.py:34] Route: /v1/rerank, Methods: POST\nINFO 04-06 18:43:35 [launcher.py:34] Route: /v2/rerank, Methods: POST\nINFO 04-06 18:43:35 [launcher.py:34] Route: /invocations, Methods: POST\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\n```\n</details>\n\nIt starts successfully and consumes 10% of GPU memory. As expected.\n\n```text\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA RTX A6000               Off |   00000000:81:00.0 Off |                    0 |\n| 30%   52C    P2             84W /  300W |    4910MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n```\n\nIf I start a second instance (same docker command as before) while the first is still running, it fails with the error ``No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.``, despite the GPU having plenty of memory left.\n\n<details>\n<summary>Complete logs</summary>\n\n```text\nINFO 04-06 18:43:47 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-06 18:43:50 [api_server.py:1034] vLLM API server version 0.8.3\nINFO 04-06 18:43:50 [api_server.py:1035] args: Namespace(host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2.5-0.5B-Instruct-AWQ', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=512, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.1, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=1, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)\nINFO 04-06 18:43:58 [config.py:600] This model supports multiple tasks: {'classify', 'reward', 'generate', 'score', 'embed'}. Defaulting to 'generate'.\nINFO 04-06 18:43:59 [awq_marlin.py:114] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\nINFO 04-06 18:43:59 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=2048.\nINFO 04-06 18:44:04 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-06 18:44:06 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='Qwen/Qwen2.5-0.5B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=512, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-0.5B-Instruct-AWQ, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\nWARNING 04-06 18:44:07 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7faf5a91ff20>\nINFO 04-06 18:44:07 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 04-06 18:44:07 [cuda.py:221] Using Flash Attention backend on V1 engine.\nINFO 04-06 18:44:07 [gpu_model_runner.py:1258] Starting to load model Qwen/Qwen2.5-0.5B-Instruct-AWQ...\nINFO 04-06 18:44:07 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.\nINFO 04-06 18:44:08 [weight_utils.py:265] Using model weights format ['*.safetensors']\nINFO 04-06 18:44:08 [weight_utils.py:315] No model.safetensors.index.json found in remote.\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.43it/s]\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.42it/s]\n\nINFO 04-06 18:44:08 [loader.py:447] Loading weights took 0.18 seconds\nINFO 04-06 18:44:08 [gpu_model_runner.py:1273] Model loading took 0.4315 GiB and 0.825929 seconds\nINFO 04-06 18:44:15 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/e05df7ec0c/rank_0_0 for vLLM's torch.compile\nINFO 04-06 18:44:15 [backends.py:426] Dynamo bytecode transform time: 6.93 s\nINFO 04-06 18:44:16 [backends.py:115] Directly load the compiled graph for shape None from the cache\nINFO 04-06 18:44:21 [monitor.py:33] torch.compile takes 6.93 s in total\nERROR 04-06 18:44:22 [core.py:390] EngineCore hit an exception: Traceback (most recent call last):\nERROR 04-06 18:44:22 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 378, in run_engine_core\nERROR 04-06 18:44:22 [core.py:390]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 04-06 18:44:22 [core.py:390]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-06 18:44:22 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 319, in __init__\nERROR 04-06 18:44:22 [core.py:390]     super().__init__(vllm_config, executor_class, log_stats)\nERROR 04-06 18:44:22 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 71, in __init__\nERROR 04-06 18:44:22 [core.py:390]     self._initialize_kv_caches(vllm_config)\nERROR 04-06 18:44:22 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 137, in _initialize_kv_caches\nERROR 04-06 18:44:22 [core.py:390]     get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,\nERROR 04-06 18:44:22 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/core/kv_cache_utils.py\", line 643, in get_kv_cache_config\nERROR 04-06 18:44:22 [core.py:390]     check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)\nERROR 04-06 18:44:22 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/core/kv_cache_utils.py\", line 480, in check_enough_kv_cache_memory\nERROR 04-06 18:44:22 [core.py:390]     raise ValueError(\"No available memory for the cache blocks. \"\nERROR 04-06 18:44:22 [core.py:390] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.\nERROR 04-06 18:44:22 [core.py:390]\nCRITICAL 04-06 18:44:22 [core_client.py:361] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1121, in <module>\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1069, in run_server\n    async with build_async_engine_client(args) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 146, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 178, in build_async_engine_client_from_engine_args\n    async_llm = AsyncLLM.from_vllm_config(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 136, in from_vllm_config\n    return cls(\n           ^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 102, in __init__\n    self.engine_core = EngineCoreClient.make_client(\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 69, in make_client\n    return AsyncMPClient(vllm_config, executor_class, log_stats)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 570, in __init__\n    super().__init__(\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 401, in __init__\n    engine.proc_handle.wait_for_startup()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/utils.py\", line 127, in wait_for_startup\n    if self.reader.recv()[\"status\"] != \"READY\":\n       ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n    buf = self._recv_bytes()\n          ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n    buf = self._recv(4)\n          ^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 399, in _recv\n    raise EOFError\nEOFError\n```\n\n</details>\n\nThis error doesn't happen with the V0 engine.\n\n```bash\ndocker run --rm --gpus=all --runtime nvidia --ipc=host \\\n    -e VLLM_USE_V1=0 \\\n    -v ~/.cache:/root/.cache \\\n    vllm/vllm-openai:v0.8.3 \\\n    --model Qwen/Qwen2.5-0.5B-Instruct-AWQ \\\n    --gpu-memory-utilization 0.1 \\\n    --max-num-seqs 1 \\\n    --max-model-len 512\n```\n\n\n<details>\n<summary>Complete logs</summary>\n\n```text\nINFO 04-06 18:44:54 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-06 18:44:57 [api_server.py:1034] vLLM API server version 0.8.3\nINFO 04-06 18:44:57 [api_server.py:1035] args: Namespace(host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2.5-0.5B-Instruct-AWQ', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=512, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.1, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=1, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)\nINFO 04-06 18:45:05 [config.py:600] This model supports multiple tasks: {'classify', 'embed', 'score', 'generate', 'reward'}. Defaulting to 'generate'.\nINFO 04-06 18:45:07 [awq_marlin.py:114] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\nINFO 04-06 18:45:07 [api_server.py:246] Started engine process with PID 93\nINFO 04-06 18:45:10 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-06 18:45:12 [llm_engine.py:242] Initializing a V0 LLM engine (v0.8.3) with config: model='Qwen/Qwen2.5-0.5B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=512, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-0.5B-Instruct-AWQ, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[1],\"max_capture_size\":1}, use_cached_outputs=True,\nINFO 04-06 18:45:13 [cuda.py:292] Using Flash Attention backend.\nINFO 04-06 18:45:13 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 04-06 18:45:13 [model_runner.py:1110] Starting to load model Qwen/Qwen2.5-0.5B-Instruct-AWQ...\nINFO 04-06 18:45:14 [weight_utils.py:265] Using model weights format ['*.safetensors']\nINFO 04-06 18:45:14 [weight_utils.py:315] No model.safetensors.index.json found in remote.\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.44it/s]\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.43it/s]\n\nINFO 04-06 18:45:14 [loader.py:447] Loading weights took 0.18 seconds\nINFO 04-06 18:45:14 [model_runner.py:1146] Model loading took 0.4315 GiB and 0.745171 seconds\nINFO 04-06 18:45:15 [worker.py:267] Memory profiling takes 0.47 seconds\nINFO 04-06 18:45:15 [worker.py:267] the current vLLM instance can use total_gpu_memory (44.55GiB) x gpu_memory_utilization (0.10) = 4.45GiB\nINFO 04-06 18:45:15 [worker.py:267] model weights take 0.43GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 0.09GiB; the rest of the memory reserved for KV Cache is 3.88GiB.\nINFO 04-06 18:45:15 [executor_base.py:112] # cuda blocks: 21186, # CPU blocks: 21845\nINFO 04-06 18:45:15 [executor_base.py:117] Maximum concurrency for 512 tokens per request: 662.06x\nINFO 04-06 18:45:20 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nCapturing CUDA graph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.47it/s]\nINFO 04-06 18:45:20 [model_runner.py:1598] Graph capturing finished in 0 secs, took 0.03 GiB\nINFO 04-06 18:45:20 [llm_engine.py:448] init engine (profile, create kv cache, warmup model) took 5.76 seconds\nWARNING 04-06 18:45:21 [config.py:1088] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\nINFO 04-06 18:45:21 [serving_chat.py:117] Using default chat sampling params from model: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\nINFO 04-06 18:45:21 [serving_completion.py:61] Using default completion sampling params from model: {'repetition_penalty': 1.1, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\nINFO 04-06 18:45:21 [api_server.py:1081] Starting vLLM API server on http://0.0.0.0:8000\nINFO 04-06 18:45:21 [launcher.py:26] Available routes are:\nINFO 04-06 18:45:21 [launcher.py:34] Route: /openapi.json, Methods: HEAD, GET\nINFO 04-06 18:45:21 [launcher.py:34] Route: /docs, Methods: HEAD, GET\nINFO 04-06 18:45:21 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: HEAD, GET\nINFO 04-06 18:45:21 [launcher.py:34] Route: /redoc, Methods: HEAD, GET\nINFO 04-06 18:45:21 [launcher.py:34] Route: /health, Methods: GET\nINFO 04-06 18:45:21 [launcher.py:34] Route: /load, Methods: GET\nINFO 04-06 18:45:21 [launcher.py:34] Route: /ping, Methods: POST, GET\nINFO 04-06 18:45:21 [launcher.py:34] Route: /tokenize, Methods: POST\nINFO 04-06 18:45:21 [launcher.py:34] Route: /detokenize, Methods: POST\nINFO 04-06 18:45:21 [launcher.py:34] Route: /v1/models, Methods: GET\nINFO 04-06 18:45:21 [launcher.py:34] Route: /version, Methods: GET\nINFO 04-06 18:45:21 [launcher.py:34] Route: /v1/chat/completions, Methods: POST\nINFO 04-06 18:45:21 [launcher.py:34] Route: /v1/completions, Methods: POST\nINFO 04-06 18:45:21 [launcher.py:34] Route: /v1/embeddings, Methods: POST\nINFO 04-06 18:45:21 [launcher.py:34] Route: /pooling, Methods: POST\nINFO 04-06 18:45:21 [launcher.py:34] Route: /score, Methods: POST\nINFO 04-06 18:45:21 [launcher.py:34] Route: /v1/score, Methods: POST\nINFO 04-06 18:45:21 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST\nINFO 04-06 18:45:21 [launcher.py:34] Route: /rerank, Methods: POST\nINFO 04-06 18:45:21 [launcher.py:34] Route: /v1/rerank, Methods: POST\nINFO 04-06 18:45:21 [launcher.py:34] Route: /v2/rerank, Methods: POST\nINFO 04-06 18:45:21 [launcher.py:34] Route: /invocations, Methods: POST\nINFO 04-06 18:45:21 [launcher.py:34] Route: /metrics, Methods: GET\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\n```\n</details>\n\nLooking at the [`determine_available_memory` function](https://github.com/vllm-project/vllm/blob/main/vllm/v1/worker/gpu_worker.py#L179) in the V1 `gpu_worker`, my guess is that the culprit is the `non_torch_allocations ` calculations. It doesn't understand that some of the GPU memory outside of torch is used by other processes and not related to this instance of VLLM.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-04-07T01:47:50+00:00",
    "closed_at": null,
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16141/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16141"
  },
  {
    "number": 7878,
    "title": "[Bug]:  Requests larger than 75k input tokens cause `Input prompt (512 tokens) is too long and exceeds the capacity of block_manager` error",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Amazon Linux 2 (x86_64)\r\nGCC version: (GCC) 7.3.1 20180712 (Red Hat 7.3.1-17)\r\nClang version: Could not collect\r\nCMake version: version 3.27.7\r\nLibc version: glibc-2.26\r\n\r\nPython version: 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:20:04) [GCC 11.3.0] (64-bit runtime)\r\nPython platform: Linux-5.10.220-209.869.amzn2.x86_64-x86_64-with-glibc2.26\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA L4\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              8\r\nOn-line CPU(s) list: 0-7\r\nThread(s) per core:  2\r\nCore(s) per socket:  4\r\nSocket(s):           1\r\nNUMA node(s):        1\r\nVendor ID:           AuthenticAMD\r\nCPU family:          25\r\nModel:               1\r\nModel name:          AMD EPYC 7R13 Processor\r\nStepping:            1\r\nCPU MHz:             3364.353\r\nBogoMIPS:            5299.99\r\nHypervisor vendor:   KVM\r\nVirtualization type: full\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            512K\r\nL3 cache:            16384K\r\nNUMA node0 CPU(s):   0-7\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.9.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.2\r\n[pip3] nvidia-nccl-cu11==2.14.3\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] triton==3.0.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.4.0\r\n[conda] numpy                     1.26.2                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu11          2.14.3                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.4.0             pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.5\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \t0-7\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI have a server up and running using this\r\n```\r\nvllm serve Mistral-Nemo-Instruct-2407/ --port 8006 --gpu-memory-utilization 0.9 --max-model-len 128000 --tensor-parallel-size 1 --pipeline-parallel-size 2 --quantization fp8 --uvicorn-log-level debug \r\n```\r\n\r\non two separate `NVidia GPUs`.\r\nHowever recently I have started noticing this error that I do not recall seeing before.\r\nI am using `documents` that are up to `125k tokens` in size.\r\n\r\n```\r\nInput prompt (512 tokens) is too long and exceeds the capacity of block_manager\r\n```\r\n\r\ni have tried looking around the issues list and going through what I think would be the solutions. I have tried the v2 block manager. setting `max_num_batched_tokens` to all possible values (outrageously even the context window of the model) but I keep seeing that error (replace number of tokens with the tokens set at `max_num_batched_tokens`).\r\n\r\nI have also tried `enabling/disabling chunked prefill` and that didn't help either. I am not sure what's more left to do and looking for help around this problem.\r\n\r\n<details>\r\n<summary>The output of vllm once serve command is executed`</summary>\r\n\r\n```text\r\n\r\nINFO 08-26 19:18:49 api_server.py:440] vLLM API server version 0.5.5\r\nINFO 08-26 19:18:49 api_server.py:441] args: Namespace(model_tag='Mistral-Nemo-Instruct-2407/', host='langmodel2', port=8006, uvicorn_log_level='debug', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, model='Mistral-Nemo-Instruct-2407/', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=128000, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=2, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization='fp8', rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None, dispatch_function=<function serve at 0x7fe663255ea0>)\r\nINFO 08-26 19:18:49 api_server.py:144] Multiprocessing frontend to use ipc:///tmp/db61d9b2-4054-4ebb-92a8-3a5b1d8a81ed for RPC Path.\r\nINFO 08-26 19:18:49 api_server.py:161] Started engine process with PID 11035\r\nINFO 08-26 19:18:54 config.py:813] Defaulting to use ray for distributed inference\r\nWARNING 08-26 19:18:54 arg_utils.py:839] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\r\nINFO 08-26 19:18:54 config.py:911] Chunked prefill is enabled with max_num_batched_tokens=512.\r\n2024-08-26 19:18:54,825\tINFO worker.py:1603 -- Connecting to existing Ray cluster at address: 10.0.4.226:6379...\r\n2024-08-26 19:18:54,830\tINFO worker.py:1779 -- Connected to Ray cluster. View the dashboard at 127.0.0.1:8265\r\nINFO 08-26 19:18:54 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='Mistral-Nemo-Instruct-2407/', speculative_config=None, tokenizer='Mistral-Nemo-Instruct-2407/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Mistral-Nemo-Instruct-2407/, use_v2_block_manager=False, enable_prefix_caching=False)\r\nINFO 08-26 19:18:55 ray_gpu_executor.py:133] use_ray_spmd_worker: False\r\nINFO 08-26 19:19:04 utils.py:975] Found nccl from library libnccl.so.2\r\nINFO 08-26 19:19:04 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(RayWorkerWrapper pid=2169, ip=10.0.4.135) INFO 08-26 19:19:04 utils.py:975] Found nccl from library libnccl.so.2\r\n(RayWorkerWrapper pid=2169, ip=10.0.4.135) INFO 08-26 19:19:04 pynccl.py:63] vLLM is using nccl==2.20.5\r\nINFO 08-26 19:19:04 model_runner.py:879] Starting to load model Mistral-Nemo-Instruct-2407/...\r\nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n(RayWorkerWrapper pid=2169, ip=10.0.4.135) INFO 08-26 19:19:04 model_runner.py:879] Starting to load model Mistral-Nemo-Instruct-2407/...\r\nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:01,  2.15it/s]\r\nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:01,  1.91it/s]\r\nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:01<00:00,  3.00it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:01<00:00,  4.69it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:01<00:00,  3.62it/s]\r\n\r\nINFO 08-26 19:19:06 model_runner.py:890] Loading model weights took 6.5727 GB\r\n(RayWorkerWrapper pid=2169, ip=10.0.4.135) INFO 08-26 19:19:06 model_runner.py:890] Loading model weights took 6.5727 GB\r\nINFO 08-26 19:19:06 distributed_gpu_executor.py:56] # GPU blocks: 9650, # CPU blocks: 3276\r\nINFO 08-26 19:19:09 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 08-26 19:19:09 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(RayWorkerWrapper pid=2169, ip=10.0.4.135) INFO 08-26 19:19:09 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n(RayWorkerWrapper pid=2169, ip=10.0.4.135) INFO 08-26 19:19:09 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(RayWorkerWrapper pid=2169, ip=10.0.4.135) INFO 08-26 19:19:33 model_runner.py:1300] Graph capturing finished in 24 secs.\r\nINFO 08-26 19:19:34 model_runner.py:1300] Graph capturing finished in 26 secs.\r\nINFO 08-26 19:19:35 api_server.py:209] vLLM to use /tmp/tmp841d197_ as PROMETHEUS_MULTIPROC_DIR\r\nWARNING 08-26 19:19:35 serving_embedding.py:188] embedding_mode is False. Embedding API will not work.\r\nINFO 08-26 19:19:35 launcher.py:20] Available routes are:\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /openapi.json, Methods: HEAD, GET\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /docs, Methods: HEAD, GET\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /docs/oauth2-redirect, Methods: HEAD, GET\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /redoc, Methods: HEAD, GET\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /health, Methods: GET\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /tokenize, Methods: POST\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /detokenize, Methods: POST\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /v1/models, Methods: GET\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /version, Methods: GET\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /v1/chat/completions, Methods: POST\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /v1/completions, Methods: POST\r\nINFO 08-26 19:19:35 launcher.py:28] Route: /v1/embeddings, Methods: POST\r\nINFO 08-26 19:19:35 launcher.py:33] Launching Uvicorn with --limit_concurrency 32765. To avoid this limit at the expense of performance run with --disable-frontend-multiprocessing\r\nINFO:     Started server process [10979]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://langmodel2:8006 (Press CTRL+C to quit)\r\n```\r\n</details>\r\n\r\n\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-26T19:16:25+00:00",
    "closed_at": "2025-02-21T02:00:23+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7878/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7878"
  },
  {
    "number": 5062,
    "title": "[Bug]: When load model weights, there are infinite loading",
    "body": "### Your current environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.1.2+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-88-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100 80GB PCIe\r\nGPU 1: NVIDIA A100 80GB PCIe\r\nGPU 2: NVIDIA A100 80GB PCIe\r\nGPU 3: NVIDIA A100 80GB PCIe\r\n\r\nNvidia driver version: 525.147.05\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nCPU(s):                             128\r\nOn-line CPU(s) list:                0-127\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 32\r\nSocket(s):                          2\r\nNUMA node(s):                       2\r\nVendor ID:                          GenuineIntel\r\nCPU family:                         6\r\nModel:                              106\r\nModel name:                         Intel(R) Xeon(R) Gold 6338 CPU @ 2.00GHz\r\nStepping:                           6\r\nCPU MHz:                            800.000\r\nCPU max MHz:                        3200.0000\r\nCPU min MHz:                        800.0000\r\nBogoMIPS:                           4000.00\r\nVirtualization:                     VT-x\r\nL1d cache:                          3 MiB\r\nL1i cache:                          2 MiB\r\nL2 cache:                           80 MiB\r\nL3 cache:                           96 MiB\r\nNUMA node0 CPU(s):                  0-31,64-95\r\nNUMA node1 CPU(s):                  32-63,96-127\r\nVulnerability Gather data sampling: Mitigation; Microcode\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==7.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.18.1\r\n[pip3] torch==2.1.2\r\n[pip3] triton==2.1.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.18.1                   pypi_0    pypi\r\n[conda] torch                     2.1.2                    pypi_0    pypi\r\n[conda] triton                    2.1.0                    pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity\r\nGPU0     X      PXB     PXB     PXB     0-31,64-95      0\r\nGPU1    PXB      X      PXB     PXB     0-31,64-95      0\r\nGPU2    PXB     PXB      X      PIX     0-31,64-95      0\r\nGPU3    PXB     PXB     PIX      X      0-31,64-95      0\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nMy code is simple. just call the model:\r\nsampling_params = SamplingParams(\r\n            n=10,\r\n            temperature=0.6,\r\n            top_p=0.95,\r\n        )\r\nmodel = LLM(\r\n          model='deepseek-ai/deepseek-coder-33b-base'\r\n          tensor_parallel_size=2,\r\n          dtype=\"bfloat16\",\r\n          max_model_len=8192,\r\n        )\r\n\r\n**When \"tensor_parallel_size\" is equal to 1, there are no problems to implement, inference etc.\r\nBut \"tensor_parallel_size\" is more than 1, cannot load the model weights and it is loading infinitely.\r\nThere are no errors in log.**\r\n\r\nThe log is following:\r\n```2024-05-27 10:41:01,135\tINFO worker.py:1749 -- Started a local Ray instance.\r\nINFO 05-27 10:41:02 llm_engine.py:75] Initializing an LLM engine (v0.4.0) with config: model='deepseek-ai/deepseek-coder-33b-base', tokenizer='deepseek-ai/deepseek-coder-33b-base', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nINFO 05-27 10:41:11 selector.py:45] Cannot use FlashAttention because the package is not found. Please install it for better performance.\r\nINFO 05-27 10:41:11 selector.py:21] Using XFormers backend.\r\n\u001b[36m(RayWorkerVllm pid=307831)\u001b[0m INFO 05-27 10:41:11 selector.py:45] Cannot use FlashAttention because the package is not found. Please install it for better performance.\r\n\u001b[36m(RayWorkerVllm pid=307831)\u001b[0m INFO 05-27 10:41:11 selector.py:21] Using XFormers backend.\r\nINFO 05-27 10:41:12 pynccl_utils.py:45] vLLM is using nccl==2.18.1```\r\n\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-05-27T02:15:22+00:00",
    "closed_at": "2024-06-13T09:00:12+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5062/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5062"
  },
  {
    "number": 4083,
    "title": "[Bug]: vllm_C is missing. ",
    "body": "### Your current environment\n\nPrevious fix from https://github.com/vllm-project/vllm/pull/3913 did not seem to work. Same issue still encountered. \r\n\r\n```text\r\nCollecting environment information...\r\nINFO 04-15 07:13:37 pynccl.py:58] Loading nccl from library /home/me/.config/vllm/nccl/cu12/libnccl.so.2.18.1\r\nPyTorch version: 2.1.2+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\r\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\r\nClang version: 11.0.1-2\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.2 (default, Feb 28 2021, 17:03:44)  [GCC 10.2.1 20210110] (64-bit runtime)\r\nPython platform: Linux-5.16.0-0.bpo.4-amd64-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nGPU 1: NVIDIA A100-SXM4-80GB\r\nGPU 2: NVIDIA A100-SXM4-80GB\r\nGPU 3: NVIDIA A100-SXM4-80GB\r\nGPU 4: NVIDIA A100-SXM4-80GB\r\nGPU 5: NVIDIA A100-SXM4-80GB\r\nGPU 6: NVIDIA A100-SXM4-80GB\r\nGPU 7: NVIDIA A100-SXM4-80GB\r\n\r\nNvidia driver version: 535.54.03\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   48 bits physical, 48 bits virtual\r\nCPU(s):                          256\r\nOn-line CPU(s) list:             0-255\r\nThread(s) per core:              2\r\nCore(s) per socket:              64\r\nSocket(s):                       2\r\nNUMA node(s):                    8\r\nVendor ID:                       AuthenticAMD\r\nCPU family:                      25\r\nModel:                           1\r\nModel name:                      AMD EPYC 7763 64-Core Processor\r\nStepping:                        1\r\nCPU MHz:                         2381.263\r\nBogoMIPS:                        4890.70\r\nVirtualization:                  AMD-V\r\nL1d cache:                       4 MiB\r\nL1i cache:                       4 MiB\r\nL2 cache:                        64 MiB\r\nL3 cache:                        512 MiB\r\nNUMA node0 CPU(s):               0-15,128-143\r\nNUMA node1 CPU(s):               16-31,144-159\r\nNUMA node2 CPU(s):               32-47,160-175\r\nNUMA node3 CPU(s):               48-63,176-191\r\nNUMA node4 CPU(s):               64-79,192-207\r\nNUMA node5 CPU(s):               80-95,208-223\r\nNUMA node6 CPU(s):               96-111,224-239\r\nNUMA node7 CPU(s):               112-127,240-255\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.1.2\r\n[pip3] triton==2.1.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tSYS\tPXB\tSYS\tSYS\t48-63,176-191\t3\t\tN/A\r\nGPU1\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tSYS\tPXB\tSYS\tSYS\t48-63,176-191\t3\t\tN/A\r\nGPU2\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tPXB\tSYS\tSYS\tSYS\t16-31,144-159\t1\t\tN/A\r\nGPU3\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tPXB\tSYS\tSYS\tSYS\t16-31,144-159\t1\t\tN/A\r\nGPU4\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tSYS\tSYS\tSYS\tPXB\t112-127,240-255\t7\t\tN/A\r\nGPU5\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tSYS\tSYS\tSYS\tPXB\t112-127,240-255\t7\t\tN/A\r\nGPU6\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tSYS\tSYS\tPXB\tSYS\t80-95,208-223\t5\t\tN/A\r\nGPU7\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tSYS\tSYS\tPXB\tSYS\t80-95,208-223\t5\t\tN/A\r\nNIC0\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\t X \tSYS\tSYS\tSYS\t\t\t\t\r\nNIC1\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tSYS\tSYS\t\t\t\t\r\nNIC2\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\t X \tSYS\t\t\t\t\r\nNIC3\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\n```(vllm-venv) me@mycomputer:~/calvinn/vllm$ python -m vllm.entrypoints.openai.api_server \\\r\n--model facebook/opt-125m\r\nINFO 04-15 07:11:52 pynccl.py:58] Loading nccl from library /home/team/.config/vllm/nccl/cu12/libnccl.so.2.18.1\r\nINFO 04-15 07:11:53 api_server.py:149] vLLM API server version 0.4.0.post1\r\nINFO 04-15 07:11:53 api_server.py:150] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='facebook/opt-125m', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, num_speculative_tokens=None, tensorizer_uri=None, verify_hash=False, encryption_keyfile=None, num_readers=1, s3_access_key_id=None, s3_secret_access_key=None, s3_endpoint=None, vllm_tensorized=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 651/651 [00:00<00:00, 212kB/s]\r\nINFO 04-15 07:11:53 llm_engine.py:82] Initializing an LLM engine (v0.4.0.post1) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, seed=0)\r\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 685/685 [00:00<00:00, 269kB/s]\r\nvocab.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 899k/899k [00:00<00:00, 3.76MB/s]\r\nmerges.txt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 456k/456k [00:00<00:00, 954kB/s]\r\nspecial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 441/441 [00:00<00:00, 564kB/s]\r\nINFO 04-15 07:11:59 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.\r\nINFO 04-15 07:11:59 selector.py:33] Using XFormers backend.\r\nINFO 04-15 07:12:00 weight_utils.py:197] Using model weights format ['*.bin']\r\npytorch_model.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 251M/251M [00:00<00:00, 385MB/s]\r\nINFO 04-15 07:12:02 model_runner.py:169] Loading model weights took 0.2389 GB\r\nINFO 04-15 07:12:02 gpu_executor.py:80] # GPU blocks: 127977, # CPU blocks: 7281\r\nINFO 04-15 07:12:04 model_runner.py:967] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 04-15 07:12:04 model_runner.py:971] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/team/calvinn/vllm/vllm/entrypoints/openai/api_server.py\", line 157, in <module>\r\n    engine = AsyncLLMEngine.from_engine_args(\r\n  File \"/home/team/calvinn/vllm/vllm/engine/async_llm_engine.py\", line 347, in from_engine_args\r\n    engine = cls(\r\n  File \"/home/team/calvinn/vllm/vllm/engine/async_llm_engine.py\", line 311, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm/engine/async_llm_engine.py\", line 421, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm/engine/llm_engine.py\", line 133, in __init__\r\n    self._initialize_kv_caches()\r\n  File \"/home/team/calvinn/vllm/vllm/engine/llm_engine.py\", line 204, in _initialize_kv_caches\r\n    self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\r\n  File \"/home/team/calvinn/vllm/vllm/executor/gpu_executor.py\", line 83, in initialize_cache\r\n    self.driver_worker.initialize_cache(num_gpu_blocks, num_cpu_blocks)\r\n  File \"/home/team/calvinn/vllm/vllm/worker/worker.py\", line 175, in initialize_cache\r\n    self._warm_up_model()\r\n  File \"/home/team/calvinn/vllm/vllm/worker/worker.py\", line 186, in _warm_up_model\r\n    self.model_runner.capture_model(self.gpu_cache)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm/worker/model_runner.py\", line 1035, in capture_model\r\n    graph_runner.capture(\r\n  File \"/home/team/calvinn/vllm/vllm/worker/model_runner.py\", line 1087, in capture\r\n    self.model(\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm/model_executor/models/opt.py\", line 300, in forward\r\n    hidden_states = self.model(input_ids, positions, kv_caches,\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm/model_executor/models/opt.py\", line 275, in forward\r\n    return self.decoder(input_ids, positions, kv_caches, attn_metadata)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm/model_executor/models/opt.py\", line 249, in forward\r\n    hidden_states = layer(hidden_states, kv_caches[i], attn_metadata)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm/model_executor/models/opt.py\", line 157, in forward\r\n    hidden_states = self.self_attn(hidden_states=hidden_states,\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm/model_executor/models/opt.py\", line 101, in forward\r\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/team/calvinn/vllm/vllm/attention/layer.py\", line 48, in forward\r\n    return self.impl.forward(query, key, value, kv_cache, attn_metadata,\r\n  File \"/home/team/calvinn/vllm/vllm/attention/backends/xformers.py\", line 200, in forward\r\n    PagedAttention.write_to_paged_cache(key, value, key_cache,\r\n  File \"/home/team/calvinn/vllm/vllm/attention/ops/paged_attn.py\", line 72, in write_to_paged_cache\r\n    ops.reshape_and_cache(\r\n  File \"/home/team/calvinn/vllm/vllm/_custom_ops.py\", line 175, in reshape_and_cache\r\n    vllm_cache_ops.reshape_and_cache(key, value, key_cache, value_cache,\r\nNameError: name 'vllm_cache_ops' is not defined```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-04-15T07:15:06+00:00",
    "closed_at": "2024-06-13T09:16:35+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4083/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 2
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4083"
  },
  {
    "number": 13535,
    "title": "[Bug]: Ray+vllm run, then crash",
    "body": "### Your current environment\n\n<details>\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-94-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: Tesla T4\nGPU 1: Tesla T4\n\nNvidia driver version: 560.35.03\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      40 bits physical, 48 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             16\nOn-line CPU(s) list:                0-15\nVendor ID:                          GenuineIntel\nModel name:                         Intel Xeon Processor (Skylake, IBRS)\nCPU family:                         6\nModel:                              85\nThread(s) per core:                 1\nCore(s) per socket:                 1\nSocket(s):                          16\nStepping:                           4\nBogoMIPS:                           4000.02\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ibrs ibpb fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 arat avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq\nHypervisor vendor:                  KVM\nVirtualization type:                full\nL1d cache:                          512 KiB (16 instances)\nL1i cache:                          512 KiB (16 instances)\nL2 cache:                           64 MiB (16 instances)\nL3 cache:                           256 MiB (16 instances)\nNUMA node(s):                       1\nNUMA node0 CPU(s):                  0-15\nVulnerability Gather data sampling: Unknown: Dependent on hypervisor status\nVulnerability Itlb multihit:        KVM: Mitigation: VMX unsupported\nVulnerability L1tf:                 Mitigation; PTE Inversion\nVulnerability Mds:                  Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Meltdown:             Mitigation; PTI\nVulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Retbleed:             Mitigation; IBRS\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Vulnerable\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; IBRS, IBPB conditional, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.0.post2+cu124torch2.5\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.5.1\n[pip3] torchao==0.8.0\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.46.1\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.6.4.post1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PHB     0-15    0               N/A\nGPU1    PHB      X      0-15    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNCCL_SOCKET_IFNAME=ens3\nNCCL_NVLS_ENABLE=0\nNCCL_DEBUG=error\nNCCL_NET=Socket\nLD_LIBRARY_PATH=/root/project/sglang/venv/lib/python3.11/site-packages/cv2/../../lib64:/usr/local/cuda-12.4/lib64:/usr/local/cuda-12.4/lib64:\nNCCL_IB_DISABLE=0\nCUDA_HOME=/usr/local/cuda-12.4\nCUDA_HOME=/usr/local/cuda-12.4\nCUDA_MODULE_LOADING=LAZY\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n### \ud83d\udc1b Describe the bug\n\nrun `python -m vllm.entrypoints.openai.api_server --disable-custom-all-reduce --gpu-memory-utilization 0.8 --dtype float16 --trust-remote-code --host 0.0.0.0 --served-model-name qwen_coder --tensor-parallel-size 4 --distributed-executor-backend ray --model /root/model/Qwen/Qwen2.5-7B-Instruct/`\n\ncrash:\n```\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) [rank3]:[E219 08:58:32.606233339 ProcessGroupNCCL.cpp:616] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=31, OpType=ALLREDUCE, NumelIn=117440512, NumelOut=117440512, Timeout(ms)=600000) ran for 600031 milliseconds before timing out.\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) [rank3]:[E219 08:58:32.606667496 ProcessGroupNCCL.cpp:1785] [PG ID 2 PG GUID 3 Rank 3] Exception (either an error or timeout) detected by watchdog at work: 31, last enqueued NCCL work: 58, last completed NCCL work: 30.\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) [rank3]:[E219 08:58:32.606693321 ProcessGroupNCCL.cpp:1834] [PG ID 2 PG GUID 3 Rank 3] Timeout at NCCL work: 31, last enqueued NCCL work: 58, last completed NCCL work: 30.\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) [rank3]:[E219 08:58:32.606716422 ProcessGroupNCCL.cpp:630] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) [rank3]:[E219 08:58:32.606733115 ProcessGroupNCCL.cpp:636] [Rank 3] To avoid data inconsistency, we are taking the entire process down.\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) [rank3]:[E219 08:58:32.611865696 ProcessGroupNCCL.cpp:1595] [PG ID 2 PG GUID 3 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=31, OpType=ALLREDUCE, NumelIn=117440512, NumelOut=117440512, Timeout(ms)=600000) ran for 600031 milliseconds before timing out.\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f247c16c446 in /root/project/sglang/venv/lib/python3.11/site-packages/torch/lib/libc10.so)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f1a3ac39772 in /root/project/sglang/venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f1a3ac40bb3 in /root/project/sglang/venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f1a3ac4261d in /root/project/sglang/venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #4: <unknown function> + 0xdc253 (0x7f24993a3253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #5: <unknown function> + 0x94ac3 (0x7f249b71cac3 in /lib/x86_64-linux-gnu/libc.so.6)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #6: <unknown function> + 0x126850 (0x7f249b7ae850 in /lib/x86_64-linux-gnu/libc.so.6)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) \n(RayWorkerWrapper pid=270291, ip=10.175.94.190) [2025-02-19 08:58:32,022 E 270291 270373] logging.cc:108: Unhandled exception: N3c1016DistBackendErrorE. what(): [PG ID 2 PG GUID 3 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=31, OpType=ALLREDUCE, NumelIn=117440512, NumelOut=117440512, Timeout(ms)=600000) ran for 600031 milliseconds before timing out.\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f247c16c446 in /root/project/sglang/venv/lib/python3.11/site-packages/torch/lib/libc10.so)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f1a3ac39772 in /root/project/sglang/venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f1a3ac40bb3 in /root/project/sglang/venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f1a3ac4261d in /root/project/sglang/venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #4: <unknown function> + 0xdc253 (0x7f24993a3253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #5: <unknown function> + 0x94ac3 (0x7f249b71cac3 in /lib/x86_64-linux-gnu/libc.so.6)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #6: <unknown function> + 0x126850 (0x7f249b7ae850 in /lib/x86_64-linux-gnu/libc.so.6)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) \n(RayWorkerWrapper pid=270291, ip=10.175.94.190) Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f247c16c446 in /root/project/sglang/venv/lib/python3.11/site-packages/torch/lib/libc10.so)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #1: <unknown function> + 0xe4271b (0x7f1a3a8af71b in /root/project/sglang/venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #2: <unknown function> + 0xdc253 (0x7f24993a3253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #3: <unknown function> + 0x94ac3 (0x7f249b71cac3 in /lib/x86_64-linux-gnu/libc.so.6)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) frame #4: <unknown function> + 0x126850 (0x7f249b7ae850 in /lib/x86_64-linux-gnu/libc.so.6)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) \n(RayWorkerWrapper pid=2667395) \n(RayWorkerWrapper pid=2667395) \n(RayWorkerWrapper pid=2667395) \n(RayWorkerWrapper pid=270237, ip=10.175.94.190) \n(RayWorkerWrapper pid=270237, ip=10.175.94.190) \n(RayWorkerWrapper pid=270237, ip=10.175.94.190) \n(RayWorkerWrapper pid=270237, ip=10.175.94.190) [2025-02-19 08:58:32,122 E 270237 270370] logging.cc:115: Stack trace: \n(RayWorkerWrapper pid=270237, ip=10.175.94.190)  /root/project/sglang/venv/lib/python3.11/site-packages/ray/_raylet.so(+0x1141c3a) [0x7f4072e63c3a] ray::operator<<()\n(RayWorkerWrapper pid=270237, ip=10.175.94.190) /root/project/sglang/venv/lib/python3.11/site-packages/ray/_raylet.so(+0x1144ec2) [0x7f4072e66ec2] ray::TerminateHandler()\n(RayWorkerWrapper pid=270237, ip=10.175.94.190) /lib/x86_64-linux-gnu/libstdc++.so.6(+0xae20c) [0x7f4071ba420c]\n(RayWorkerWrapper pid=270237, ip=10.175.94.190) /lib/x86_64-linux-gnu/libstdc++.so.6(+0xae277) [0x7f4071ba4277]\n(RayWorkerWrapper pid=270237, ip=10.175.94.190) /lib/x86_64-linux-gnu/libstdc++.so.6(+0xae1fe) [0x7f4071ba41fe]\n(RayWorkerWrapper pid=270237, ip=10.175.94.190) /root/project/sglang/venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so(+0xe427c9) [0x7f360fd207c9] c10d::ProcessGroupNCCL::ncclCommWatchdog()\n(RayWorkerWrapper pid=270237, ip=10.175.94.190) /lib/x86_64-linux-gnu/libstdc++.so.6(+0xdc253) [0x7f4071bd2253]\n(RayWorkerWrapper pid=270237, ip=10.175.94.190) /lib/x86_64-linux-gnu/libc.so.6(+0x94ac3) [0x7f4073f4bac3]\n(RayWorkerWrapper pid=270237, ip=10.175.94.190) /lib/x86_64-linux-gnu/libc.so.6(+0x126850) [0x7f4073fdd850]\n(RayWorkerWrapper pid=270237, ip=10.175.94.190) \n(RayWorkerWrapper pid=270237, ip=10.175.94.190) *** SIGABRT received at time=1739955512 on cpu 15 ***\n(RayWorkerWrapper pid=270237, ip=10.175.94.190) PC: @     0x7f4073f4d9fc  (unknown)  pthread_kill\n(RayWorkerWrapper pid=270237, ip=10.175.94.190)     @     0x7f4073ef9520  (unknown)  (unknown)\n(RayWorkerWrapper pid=270237, ip=10.175.94.190) [2025-02-19 08:58:32,123 E 270237 270370] logging.cc:440: *** SIGABRT received at time=1739955512 on cpu 15 ***\n(RayWorkerWrapper pid=270237, ip=10.175.94.190) [2025-02-19 08:58:32,123 E 270237 270370] logging.cc:440: PC: @     0x7f4073f4d9fc  (unknown)  pthread_kill\n(RayWorkerWrapper pid=270237, ip=10.175.94.190) [2025-02-19 08:58:32,123 E 270237 270370] logging.cc:440:     @     0x7f4073ef9520  (unknown)  (unknown)\n(RayWorkerWrapper pid=270237, ip=10.175.94.190) Fatal Python error: Aborted\n(RayWorkerWrapper pid=270237, ip=10.175.94.190) \n(RayWorkerWrapper pid=270237, ip=10.175.94.190) \n(RayWorkerWrapper pid=270237, ip=10.175.94.190) Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, uvloop.loop, ray._raylet, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, markupsafe._speedups, PIL._imaging, msgspec._core, sentencepiece._sentencepiece, regex._regex, PIL._imagingft, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._helpers, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket, frozenlist._frozenlist, pyarrow.lib, pyarrow._json, zmq.backend.cython._zmq (total: 52)\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) /lib/x86_64-linux-gnu/libstdc++.so.6(+0xae20c) [0x7f249937520c]\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) /lib/x86_64-linux-gnu/libstdc++.so.6(+0xae277) [0x7f2499375277]\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) /lib/x86_64-linux-gnu/libstdc++.so.6(+0xae1fe) [0x7f24993751fe]\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) /root/project/sglang/venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so(+0xe427c9) [0x7f1a3a8af7c9] c10d::ProcessGroupNCCL::ncclCommWatchdog()\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) /lib/x86_64-linux-gnu/libstdc++.so.6(+0xdc253) [0x7f24993a3253]\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) /lib/x86_64-linux-gnu/libc.so.6(+0x94ac3) [0x7f249b71cac3]\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) /lib/x86_64-linux-gnu/libc.so.6(+0x126850) [0x7f249b7ae850]\n(RayWorkerWrapper pid=270291, ip=10.175.94.190) \n(RayWorkerWrapper pid=270291, ip=10.175.94.190) \n(RayWorkerWrapper pid=270291, ip=10.175.94.190) \n(RayWorkerWrapper pid=2667395) /lib/x86_64-linux-gnu/libstdc++.so.6(+0xae20c) [0x7fdb8762020c]\n(RayWorkerWrapper pid=2667395) /lib/x86_64-linux-gnu/libstdc++.so.6(+0xae277) [0x7fdb87620277]\n(RayWorkerWrapper pid=2667395) /lib/x86_64-linux-gnu/libstdc++.so.6(+0xae1fe) [0x7fdb876201fe]\n(RayWorkerWrapper pid=2667395) /root/project/sglang/venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so(+0xe427c9) [0x7fd15b3007c9] c10d::ProcessGroupNCCL::ncclCommWatchdog()\n(RayWorkerWrapper pid=2667395) /lib/x86_64-linux-gnu/libstdc++.so.6(+0xdc253) [0x7fdb8764e253]\n(RayWorkerWrapper pid=2667395) /lib/x86_64-linux-gnu/libc.so.6(+0x94ac3) [0x7fdb899c4ac3]\n(RayWorkerWrapper pid=2667395) /lib/x86_64-linux-gnu/libc.so.6(+0x126850) [0x7fdb89a56850]\n(RayWorkerWrapper pid=2667395) \n(RayWorkerWrapper pid=2667395) \n(RayWorkerWrapper pid=2667395) \n(raylet) A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffa55573c318fcb45089df416702000000 Worker ID: 40060cc4698e6b41b7073182fde862a6a5ea32e8f6ac18bc2f4e1c15 Node ID: 1a6050c7a4f77ea40a9aa1d8f08ba6cfbec96c0138c62b2190c918cb Worker IP address: 10.175.94.190 Worker port: 10005 Worker PID: 270291 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n(RayWorkerWrapper pid=2667395) INFO 02-19 16:47:57 model_runner.py:1077] Loading model weights took 3.5546 GB [repeated 2x across cluster]\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "closed",
    "created_at": "2025-02-19T09:39:51+00:00",
    "closed_at": "2025-03-24T22:37:25+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13535/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13535"
  },
  {
    "number": 16911,
    "title": "[Bug]: guided_grammar example syntax does not work",
    "body": "### Your current environment\n\nI'm using vllm hosted on a K8s instance and was not able to execute the environment collection python file there. But this is the error I get:\n\n<details>\n<summary>Error message</summary>\n\n```\nINFO 04-21 00:33:45 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 66.7%                                                                                  \nINFO 04-21 00:33:55 [logger.py:39] Received request chatcmpl-5a7a5fbaada34f3a88b577a238ddd279: prompt: \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nGenerate an SQL query to show the 'username' and 'email'from the 'users' table.<|im_end|>\\n<|im_start|>assistant\\ \nINFO 04-21 00:33:55 [async_llm.py:228] Added request chatcmpl-5a7a5fbaada34f3a88b577a238ddd279.                                                                                                                                                                                                         \nERROR 04-21 00:33:55 [core.py:387] EngineCore hit an exception: Traceback (most recent call last):                                                                                                                                                                                                      \nERROR 04-21 00:33:55 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 380, in run_engine_core                                                                                                                                                                \nERROR 04-21 00:33:55 [core.py:387]     engine_core.run_busy_loop()                                                                                                                                                                                                                                      \nERROR 04-21 00:33:55 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 402, in run_busy_loop                                                                                                                                                                  \nERROR 04-21 00:33:55 [core.py:387]     self._process_engine_step()                                                                                                                                                                                                                                      \nERROR 04-21 00:33:55 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 431, in _process_engine_step                                                                                                                                                           \nERROR 04-21 00:33:55 [core.py:387]     outputs = self.step_fn()                                                                                                                                                                                                                                         \nERROR 04-21 00:33:55 [core.py:387]               ^^^^^^^^^^^^^^                                                                                                                                                                                                                                         \nERROR 04-21 00:33:55 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 206, in step                                                                                                                                                                           \nERROR 04-21 00:33:55 [core.py:387]     scheduler_output = self.scheduler.schedule()                                                                                                                                                                                                                     \nERROR 04-21 00:33:55 [core.py:387]                        ^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                     \nERROR 04-21 00:33:55 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/core/sched/scheduler.py\", line 286, in schedule                                                                                                                                                              \nERROR 04-21 00:33:55 [core.py:387]     if structured_output_req and structured_output_req.grammar:                                                                                                                                                                                                      \nERROR 04-21 00:33:55 [core.py:387]                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                       \nERROR 04-21 00:33:55 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/structured_output/request.py\", line 43, in grammar                                                                                                                                                           \nERROR 04-21 00:33:55 [core.py:387]     completed = self._check_grammar_completion()                                                                                                                                                                                                                     \nERROR 04-21 00:33:55 [core.py:387]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                     \nERROR 04-21 00:33:55 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/structured_output/request.py\", line 31, in _check_grammar_completion                                                                                                                                         \nERROR 04-21 00:33:55 [core.py:387]     self._grammar = self._grammar.result(timeout=0.0001)                                                                                                                                                                                                             \nERROR 04-21 00:33:55 [core.py:387]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                             \nERROR 04-21 00:33:55 [core.py:387]   File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 456, in result                                                                                                                                                                                        \nERROR 04-21 00:33:55 [core.py:387]     return self.__get_result()                                                                                                                                                                                                                                       \nERROR 04-21 00:33:55 [core.py:387]            ^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                       \nERROR 04-21 00:33:55 [core.py:387]   File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result                                                                                                                                                                                  \nERROR 04-21 00:33:55 [core.py:387]     raise self._exception                                                                                                                                                                                                                                            \nERROR 04-21 00:33:55 [core.py:387]   File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 59, in run                                                                                                                                                                                           \nERROR 04-21 00:33:55 [core.py:387]     result = self.fn(*self.args, **self.kwargs)                                                                                                                                                                                                                      \nERROR 04-21 00:33:55 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                      \nERROR 04-21 00:33:55 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/structured_output/__init__.py\", line 77, in _async_create_grammar                                                                                                                                            \nERROR 04-21 00:33:55 [core.py:387]     return self.backend.compile_grammar(request_type, grammar_spec)                                                                                                                                                                                                  \nERROR 04-21 00:33:55 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                  \nERROR 04-21 00:33:55 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/structured_output/backend_xgrammar.py\", line 97, in compile_grammar                                                                                                                                          \nERROR 04-21 00:33:55 [core.py:387]     ctx = self.compiler.compile_grammar(grammar_spec)                                                                                                                                                                                                                \nERROR 04-21 00:33:55 [core.py:387]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                \nERROR 04-21 00:33:55 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/xgrammar/compiler.py\", line 211, in compile_grammar                                                                                                                                                                  \nERROR 04-21 00:33:55 [core.py:387]     grammar = Grammar.from_ebnf(grammar, root_rule_name=root_rule_name)                                                                                                                                                                                              \nERROR 04-21 00:33:55 [core.py:387]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                              \nERROR 04-21 00:33:55 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/xgrammar/grammar.py\", line 110, in from_ebnf                                                                                                                                                                         \nERROR 04-21 00:33:55 [core.py:387]     return Grammar._create_from_handle(_core.Grammar.from_ebnf(ebnf_string, root_rule_name))                                                                                                                                                                         \nERROR 04-21 00:33:55 [core.py:387]                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                          \nERROR 04-21 00:33:55 [core.py:387] RuntimeError: [00:33:55] /project/cpp/grammar_parser.cc:78: EBNF parse error at line 2, column 5: Expect rule name                                                                                                                                                   \nERROR 04-21 00:33:55 [core.py:387]                                                                                                                                                                                                                                                                      \nERROR 04-21 00:33:55 [core.py:387]                                                                                                                                                                                                                                                                      \nCRITICAL 04-21 00:33:55 [core_client.py:359] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.                                                                                                                                                         \nKilled                                                                                                                                                                                                                                                                                                  \nstream closed EOF for s92818/vllm-deployment-openai-768966b6df-fmrwx (vllm-deployment-openai)  \n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n\nI'm trying to use guided grammar as found in [documentation](https://docs.vllm.ai/en/latest/features/structured_outputs.html). All examples about structured outputs worked as presented but the one with EBNF. I tried to remove the tabstops in front of the lines, to rename the start node to root but it did not help.\n\n```\nwrong_simplified_sql_grammar = \"\"\"\n?start: select_statement\n?select_statement: \"SELECT \" column_list \" FROM \" table_name\n?column_list: column_name (\",\" column_name)*\n?table_name: identifier\n?column_name: identifier\n?identifier: /[a-zA-Z_][a-zA-Z0-9_]*/\n\"\"\"\n\nprompt = (\"Generate an SQL query to show the 'username' and 'email'\"\n          \"from the 'users' table.\")\ncompletion = client.chat.completions.create(\n    model=\"Qwen/Qwen2.5-7B\",\n    messages=[{\n        \"role\": \"user\",\n        \"content\": prompt,\n    }],\n    extra_body={\n        \"guided_grammar\": wrong_simplified_sql_grammar\n        },\n)\nprint(completion.choices[0].message.content)\n```\n\nBecause I got some messages about wrong escape sequences in other examples I added an r in front of the string. But then the `?` at the line start get formatted. Looking at the [EBNF documentation of xgrammar](https://xgrammar.mlc.ai/docs/how_to/ebnf_guided_generation.html) made me reformulate the grammar as following and it worked:\n\n```\nsimplified_sql_grammar = r\"\"\"\nroot ::= select_statement\nselect_statement ::= \"SELECT \" column_list \" FROM \" table_name\ncolumn_list ::= column_name (\", \" column_name)*\ntable_name ::= identifier\ncolumn_name ::= identifier\nidentifier ::= [a-zA-Z_][a-zA-Z_]*\n\"\"\"\n```\n\nNaming the first node `root` seems to be obligatory. The `/` in `/[a-zA-Z_][a-zA-Z_]*/` will kill the deployment as well.\n\nI was not able to try it with the transformers locally yet. But even though it would not work their either I think the documentation for vllm might have to be adjusted.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "open",
    "created_at": "2025-04-21T08:13:48+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16911/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16911"
  },
  {
    "number": 13678,
    "title": "[Bug]: Mamba2 models (Bamba and Codestral Mamba) fail on RoCM",
    "body": "### Your current environment\n\nVia @hackey:\n>I am using:\nROCM (Dual AMD 7900 xtx)\nUbuntu 24.04\n\n### \ud83d\udc1b Describe the bug\n\nSee https://github.com/vllm-project/vllm/issues/6479#issuecomment-2674292711\n\nSpecifically this part:\n```\nregistry.py:321]     from vllm.attention.backends.flash_attn import FlashAttentionMetadata ERROR 02-21 11:17:10 registry.py:321]   File \"/usr/local/lib/python3.12/dist-packages/vllm/attention/backends/flash_attn.py\", line 25, in <module> ERROR 02-21 11:17:10 registry.py:321]     from vllm.vllm_flash_attn import (flash_attn_varlen_func, ERROR 02-21 11:17:10 registry.py:321] ImportError: cannot import name 'flash_attn_varlen_func' from 'vllm.vllm_flash_attn' (unknown location) ERROR 02-21 11:17:10 registry.py:321]  Traceback (most recent call last): File \"/usr/local/bin/vllm\", line 8, in <module> sys.exit(main()) ^^^^^^ File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/main.py\", line 73, in main \n```\n\nIt looks like the problem is caused by importing FlashAttentionMetadata in MambaMixer2, which pulls in vllm_flash_attn, which is unsupported on RoCM.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-21T17:00:25+00:00",
    "closed_at": "2025-06-26T02:26:03+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13678/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/13678"
  },
  {
    "number": 5906,
    "title": "[Bug]: Internal Server Error when hosting Salesforce/SFR-Embedding-Mistral",
    "body": "### Your current environment\r\n\r\nUsing latest docker image vllm/vllm-openai:v0.5.0.post1\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nWhen trying to send a request to the /v1/embeddings endpoint of the deployed model, I get the response \"Internal Server Error\". For reference, there is the log from the according vllm container:\r\n```\r\n/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\r\n  warnings.warn(\r\nINFO 06-27 13:41:32 api_server.py:177] vLLM API server version 0.5.0.post1\r\nINFO 06-27 13:41:32 api_server.py:178] args: Namespace(host=None, port=8080, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='/mnt/models/', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir='/models-cache', load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=8000, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, image_processor=None, image_processor_revision=None, disable_image_processor=False, scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, num_speculative_tokens=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, model_loader_extra_config=None, preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\nINFO 06-27 13:41:32 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='/mnt/models/', speculative_config=None, tokenizer='/mnt/models/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8000, download_dir='/models-cache', load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/mnt/models/)\r\nINFO 06-27 13:41:33 selector.py:150] Cannot use FlashAttention-2 backend due to sliding window.\r\nINFO 06-27 13:41:33 selector.py:51] Using XFormers backend.\r\nINFO 06-27 13:41:35 selector.py:150] Cannot use FlashAttention-2 backend due to sliding window.\r\nINFO 06-27 13:41:35 selector.py:51] Using XFormers backend.\r\nINFO 06-27 13:41:37 model_runner.py:160] Loading model weights took 13.2524 GB\r\nWARNING 06-27 13:41:37 serving_chat.py:95] No chat template provided. Chat API will not work.\r\nINFO 06-27 13:41:37 serving_embedding.py:144] Activating the server engine with embedding enabled.\r\nINFO:     Started server process [1]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\r\nINFO 06-27 13:41:47 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 13:41:57 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\n...\r\nINFO 06-27 14:06:07 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:06:17 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:06:27 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:06:37 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:06:47 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:06:57 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:07:07 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:07:17 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:07:27 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:07:37 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:07:47 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:07:57 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO:     127.0.0.6:0 - \"GET / HTTP/1.1\" 404 Not Found\r\nINFO:     127.0.0.6:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\r\nINFO:     127.0.0.6:0 - \"GET /docs HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.6:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\r\nINFO 06-27 14:08:07 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:08:17 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:08:27 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:08:37 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:08:47 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:08:57 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:09:07 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:09:17 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:09:27 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO:     127.0.0.6:0 - \"GET /docs HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.6:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\r\nINFO 06-27 14:09:37 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO:     127.0.0.6:0 - \"POST /v1/embeddings HTTP/1.1\" 400 Bad Request\r\nINFO 06-27 14:09:47 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO:     127.0.0.6:0 - \"POST /v1/embeddings HTTP/1.1\" 400 Bad Request\r\nINFO 06-27 14:09:57 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO:     127.0.0.6:0 - \"POST /v1/embeddings HTTP/1.1\" 400 Bad Request\r\nINFO:     127.0.0.6:0 - \"POST /v1/embeddings HTTP/1.1\" 404 Not Found\r\nINFO 06-27 14:10:07 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO:     127.0.0.6:0 - \"POST /v1/embeddings HTTP/1.1\" 400 Bad Request\r\nINFO 06-27 14:10:17 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:10:22 async_llm_engine.py:564] Received request cmpl-3967bc8c26d44e53aae54866bc6ba40f-0: prompt: '<unk>', params: PoolingParams(additional_metadata=string), prompt_token_ids: [0], lora_request: None.\r\nINFO 06-27 14:10:22 metrics.py:341] Avg prompt throughput: 0.2 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 06-27 14:10:22 async_llm_engine.py:133] Finished request cmpl-3967bc8c26d44e53aae54866bc6ba40f-0.\r\nINFO:     127.0.0.6:0 - \"POST /v1/embeddings HTTP/1.1\" 500 Internal Server Error\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 399, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 123, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 93, in __call__\r\n    await self.simple_response(scope, receive, send, request_headers=headers)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 148, in simple_response\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 756, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 776, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 297, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 77, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 72, in app\r\n    response = await func(request)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 278, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 138, in create_embedding\r\n    return JSONResponse(content=generator.model_dump())\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 184, in __init__\r\n    super().__init__(content, status_code, headers, media_type, background)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 41, in __init__\r\n    self.body = self.render(content)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 187, in render\r\n    return json.dumps(\r\n  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\r\n    **kw).encode(obj)\r\n  File \"/usr/lib/python3.10/json/encoder.py\", line 199, in encode\r\n    chunks = self.iterencode(o, _one_shot=True)\r\n  File \"/usr/lib/python3.10/json/encoder.py\", line 257, in iterencode\r\n    return _iterencode(o, 0)\r\nValueError: Out of range float values are not JSON compliant\r\nINFO 06-27 14:10:37 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\n```\r\n\r\nDo you have any idea what can be wrong here? Thanks in advance for your help!",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-06-27T14:14:58+00:00",
    "closed_at": "2024-12-12T02:07:14+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5906/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5906"
  },
  {
    "number": 4081,
    "title": "[Bug]: Error when running pytest: TypeError: 'ABCMeta' object is not subscriptable",
    "body": "### Your current environment\n\nWhen calling `python collect_env.py`, running into the same error as below.\r\n\n\n### \ud83d\udc1b Describe the bug\n\nEncountering the below issue when running tests. Suspected to be related to mypy changes\r\n![image](https://github.com/vllm-project/vllm/assets/88394319/fd9057d8-6f9c-4f51-b60d-88b7b88b70d2)\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-04-15T06:43:33+00:00",
    "closed_at": "2024-04-15T21:47:32+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4081/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4081"
  }
]
[
  {
    "number": 19631,
    "title": "[Bug]: Illegal memory access on llama4 maverick",
    "body": "### Your current environment\n\nPyTorch 2.7.0, vLLM main branch built from source.\n\n### \ud83d\udc1b Describe the bug\n\nRepro:\n```py\nvllm serve meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 --tensor-parallel-size 8 --max-num-batched-tokens 40000 --max-model-len 8192 --max-num-seqs 128 --gpu-memory-utilization 0.8\n```\ngives a CUDA Illegal Memory Access, as well as some errors:\n```\nERROR 06-13 15:32:09 [core.py:515] EngineCore failed to start.\nERROR 06-13 15:32:09 [core.py:515] Traceback (most recent call last):\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/engine/core.py\", line 506, in run_engine_core\nERROR 06-13 15:32:09 [core.py:515]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 06-13 15:32:09 [core.py:515]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/engine/core.py\", line 390, in __init__\nERROR 06-13 15:32:09 [core.py:515]     super().__init__(vllm_config, executor_class, log_stats,\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/engine/core.py\", line 83, in __init__\nERROR 06-13 15:32:09 [core.py:515]     self._initialize_kv_caches(vllm_config)\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/engine/core.py\", line 168, in _initialize_kv_caches\nERROR 06-13 15:32:09 [core.py:515]     self.model_executor.initialize_from_config(kv_cache_configs)\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/executor/abstract.py\", line 66, in initialize_from_config\nERROR 06-13 15:32:09 [core.py:515]     self.collective_rpc(\"compile_or_warm_up_model\")\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/executor/multiproc_executor.py\", line 220, in collective_rpc\nERROR 06-13 15:32:09 [core.py:515]     result = get_response(w, dequeue_timeout)\nERROR 06-13 15:32:09 [core.py:515]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/executor/multiproc_executor.py\", line 207, in get_response\nERROR 06-13 15:32:09 [core.py:515]     raise RuntimeError(\nERROR 06-13 15:32:09 [core.py:515] RuntimeError: Worker failed with error 'Expected result >= 0 to be true, but got false.  (Could this error message be\n improved?  If so, please report an enhancement request to PyTorch.)', please check the stack trace above for the root cause\n(VllmWorker rank=1 pid=3350867) ERROR 06-13 15:32:09 [multiproc_executor.py:527]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/compilation/cuda_piece\nwise_backend.py\", line 156, in __call__\n(VllmWorker rank=1 pid=3350867) ERROR 06-13 15:32:09 [multiproc_executor.py:527]     return entry.runnable(*args)\n(VllmWorker rank=2 pid=3350868) ERROR 06-13 15:32:09 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=3350867) ERROR 06-13 15:32:09 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=2 pid=3350868) ERROR 06-13 15:32:09 [multiproc_executor.py:527]   File \"/home/rzou/.cache/vllm/torch_compile_cache/d98525c527/rank_2_0/\ninductor_cache/rl/crl3f6qy7nm5k2qs65o6f44vppuehyqkkmjhxy6q5mty7zgba2kx.py\", line 1282, in call\n(VllmWorker rank=7 pid=3350875) ERROR 06-13 15:32:09 [multiproc_executor.py:527]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/compilation/cuda_piece\nwise_backend.py\", line 156, in __call__\n(VllmWorker rank=1 pid=3350867) ERROR 06-13 15:32:09 [multiproc_executor.py:527]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/compilation/compiler_i\nnterface.py\", line 510, in compiled_graph\n(VllmWorker rank=2 pid=3350868) ERROR 06-13 15:32:09 [multiproc_executor.py:527]     buf52 = empty_strided_cuda(((-32768) + s0, ), (1, ), torch.int32)\n(VllmWorker rank=5 pid=3350871) ERROR 06-13 15:32:09 [multiproc_executor.py:527]     return self.current_callable(inputs)\n(VllmWorker rank=7 pid=3350875) ERROR 06-13 15:32:09 [multiproc_executor.py:527]     return entry.runnable(*args)\n(VllmWorker rank=6 pid=3350873) ERROR 06-13 15:32:09 [multiproc_executor.py:527]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n...\n\n(VllmWorker rank=7 pid=3350875) Exception ignored in: <function CustomAllreduce.__del__ at 0x7efceedfe2a0>\n(VllmWorker rank=7 pid=3350875) Traceback (most recent call last):\n(VllmWorker rank=7 pid=3350875)   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/distributed/device_communicators/custom_all_reduce.py\", line 276, in __\ndel__\n(VllmWorker rank=7 pid=3350875)     self.close()\n(VllmWorker rank=7 pid=3350875)   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/distributed/device_communicators/custom_all_reduce.py\", line 272, in cl\nose\n(VllmWorker rank=7 pid=3350875)     self.free_shared_buffer(self.meta_ptrs, rank=self.rank)\n(VllmWorker rank=7 pid=3350875)   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/distributed/device_communicators/custom_all_reduce.py\", line 304, in fr\nee_shared_buffer\n(VllmWorker rank=7 pid=3350875)     ops.free_shared_buffer(pointers[rank])\n(VllmWorker rank=7 pid=3350875)   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/_custom_ops.py\", line 1758, in free_shared_buffer\n(VllmWorker rank=7 pid=3350875)     torch.ops._C_custom_ar.free_shared_buffer(ptr)\n(VllmWorker rank=7 pid=3350875)   File \"/home/rzou/dev/stable0/env/lib/python3.12/site-packages/torch/_ops.py\", line 1158, in __call__\n(VllmWorker rank=7 pid=3350875)     return self._op(*args, **(kwargs or {}))\n(VllmWorker rank=7 pid=3350875)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=7 pid=3350875) RuntimeError: CUDA error: an illegal memory access was encountered\n(VllmWorker rank=7 pid=3350875) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n(VllmWorker rank=7 pid=3350875) For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n(VllmWorker rank=7 pid=3350875) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n(VllmWorker rank=7 pid=3350875)\n(VllmWorker rank=1 pid=3350867) ERROR 06-13 15:32:09 [multiproc_executor.py:527]     graph_output = inductor_compiled_graph(list_args)\n(VllmWorker rank=5 pid=3350871) ERROR 06-13 15:32:09 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=2 pid=3350868) ERROR 06-13 15:32:09 [multiproc_executor.py:527]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=7 pid=3350875) ERROR 06-13 15:32:09 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^\n```\n\nI think this started from https://github.com/vllm-project/vllm/pull/19168. After turning off the chunking optimization, the errors go away.\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile",
      "llama"
    ],
    "state": "closed",
    "created_at": "2025-06-13T22:33:29+00:00",
    "closed_at": "2025-07-07T17:10:56+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19631/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19631"
  },
  {
    "number": 18022,
    "title": "[Bug]: vLLM does not serve text-only version of Llama4",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nNot related\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nHi all! \nI am trying to serve a text-only version of Llama 4 Scout (17B-16E) using vLLM. This model requires the Llama4ForCausalLM architecture. However, it seems that vLLM currently expects only the multimodal Llama 4.\n\nAlthough the Llama4ForCausalLM class is implemented in vllm/model_executor/models/llama4.py, it is not registered in the _TEXT_GENERATION_MODELS dictionary in vllm/model_executor/models/registry.py. After manually adding an entry for Llama4ForCausalLM, I was able to serve the model successfully.\n\nThis looks like an oversight or a missing feature, and might be considered a bug.\n\nFor the reference, the text-only version of Llama4 was loaded and saved with AutoModelForCausalLM with the model config updated accordingly. \n```\nmodel_config = AutoConfig.from_pretrained(config[\"model\"][\"path\"], trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    config[\"model\"][\"path\"],\n    attn_implementation=config[\"model\"][\"attn_implementation\"],\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n    trust_remote_code=True,\n    token=token,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(config[\"model\"][\"path\"])\n\n```\n\n```\nos.makedirs(path, exist_ok=True)\nmodel.save_pretrained(path)\ntokenizer.save_pretrained(path)\nmodel_config.save_pretrained(path)\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "llama"
    ],
    "state": "open",
    "created_at": "2025-05-12T20:23:48+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18022/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/18022"
  }
]
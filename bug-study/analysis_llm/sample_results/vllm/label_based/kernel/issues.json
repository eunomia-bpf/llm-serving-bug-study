[
  {
    "number": 18866,
    "title": "[Feature]: Vectorize `scaled_int8_quant`",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nSimilar to the recent discoveries in https://github.com/vllm-project/vllm/pull/18844, vectorizing our quantization methods can have a huge impact on e2e performance.\n\nCurrently we only use `vectorization.h` in `csrc/quantization/fp8/common.cuh` and `csrc/quantization/fused_kernels/layernorm_utils.cuh`, so we should expand this to more implementations like `csrc/quantization/compressed_tensors/int8_quant_kernels.cu` for faster INT8 activation quantization.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "feature request",
      "kernel"
    ],
    "state": "closed",
    "created_at": "2025-05-28T23:47:33+00:00",
    "closed_at": "2025-06-15T11:08:02+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18866/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18866"
  }
]
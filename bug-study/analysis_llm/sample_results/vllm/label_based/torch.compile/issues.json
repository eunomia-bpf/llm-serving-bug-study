[
  {
    "number": 19656,
    "title": "[Bug]: Inductor codegen: fatal error: stddef.h: No such file or directory",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : CentOS Stream 9 (x86_64)\nGCC version                  : (GCC) 11.5.0 20240719 (Red Hat 11.5.0-5)\nClang version                : 20.1.1 (CentOS 20.1.1-3.el9)\nCMake version                : version 3.26.5\nLibc version                 : glibc-2.34\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu128\nIs debug build               : False\nCUDA used to build PyTorch   : 12.8\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.10 (main, Apr  9 2025, 00:00:00) [GCC 11.5.0 20240719 (Red Hat 11.5.0-5)] (64-bit runtime)\nPython platform              : Linux-6.4.3-0_fbk20_zion_2830_g3e5ab162667d-x86_64-with-glibc2.34\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.8.93\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : \nGPU 0: NVIDIA H100\nGPU 1: NVIDIA H100\nGPU 2: NVIDIA H100\nGPU 3: NVIDIA H100\nGPU 4: NVIDIA H100\nGPU 5: NVIDIA H100\nGPU 6: NVIDIA H100\nGPU 7: NVIDIA H100\n\nNvidia driver version        : 535.154.05\ncuDNN version                : Probably one of the following:\n/usr/lib64/libcudnn.so.8.9.2\n/usr/lib64/libcudnn.so.9.2.0\n/usr/lib64/libcudnn_adv.so.9.2.0\n/usr/lib64/libcudnn_adv_infer.so.8.9.2\n/usr/lib64/libcudnn_adv_train.so.8.9.2\n/usr/lib64/libcudnn_cnn.so.9.2.0\n/usr/lib64/libcudnn_cnn_infer.so.8.9.2\n/usr/lib64/libcudnn_cnn_train.so.8.9.2\n/usr/lib64/libcudnn_engines_precompiled.so.9.2.0\n/usr/lib64/libcudnn_engines_runtime_compiled.so.9.2.0\n/usr/lib64/libcudnn_graph.so.9.2.0\n/usr/lib64/libcudnn_heuristic.so.9.2.0\n/usr/lib64/libcudnn_ops.so.9.2.0\n/usr/lib64/libcudnn_ops_infer.so.8.9.2\n/usr/lib64/libcudnn_ops_train.so.8.9.2\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      52 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             384\nOn-line CPU(s) list:                0-383\nVendor ID:                          AuthenticAMD\nModel name:                         AMD EPYC 9654 96-Core Processor\nCPU family:                         25\nModel:                              17\nThread(s) per core:                 2\nCore(s) per socket:                 96\nSocket(s):                          2\nStepping:                           1\nFrequency boost:                    enabled\nCPU(s) scaling MHz:                 80%\nCPU max MHz:                        3707.8120\nCPU min MHz:                        1500.0000\nBogoMIPS:                           4792.82\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid overflow_recov succor smca fsrm flush_l1d\nVirtualization:                     AMD-V\nL1d cache:                          6 MiB (192 instances)\nL1i cache:                          6 MiB (192 instances)\nL2 cache:                           192 MiB (192 instances)\nL3 cache:                           768 MiB (24 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0-95,192-287\nNUMA node1 CPU(s):                  96-191,288-383\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Vulnerable: eIBRS with unprivileged eBPF\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.8.3.14\n[pip3] nvidia-cuda-cupti-cu12==12.8.57\n[pip3] nvidia-cuda-nvrtc-cu12==12.8.61\n[pip3] nvidia-cuda-runtime-cu12==12.8.57\n[pip3] nvidia-cudnn-cu12==9.7.1.26\n[pip3] nvidia-cufft-cu12==11.3.3.41\n[pip3] nvidia-cufile-cu12==1.13.0.11\n[pip3] nvidia-curand-cu12==10.3.9.55\n[pip3] nvidia-cusolver-cu12==11.7.2.55\n[pip3] nvidia-cusparse-cu12==12.5.7.53\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.8.61\n[pip3] nvidia-nvtx-cu12==12.8.55\n[pip3] pyzmq==26.4.0\n[pip3] sentence-transformers==3.2.1\n[pip3] torch==2.7.0+cu128\n[pip3] torchaudio==2.7.0+cu128\n[pip3] torchvision==0.22.0+cu128\n[pip3] transformers==4.51.3\n[pip3] transformers-stream-generator==0.0.5\n[pip3] triton==3.3.0\n[pip3] tritonclient==2.51.0\n[pip3] vector-quantize-pytorch==1.21.2\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.1.dev7046+g467bef1.d20250615 (git sha: 467bef1, date: 20250615)\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    CPU Affinity      NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     0-95,192-287      0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    PHB     PHB     SYS     SYS     0-95,192-287      0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     0-95,192-287      0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     0-95,192-287      0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     96-191,288-383    1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     96-191,288-383    1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     PHB     PHB     96-191,288-383    1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     96-191,288-383    1               N/A\nNIC0    SYS     PHB     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     SYS     SYS\nNIC1    SYS     PHB     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      SYS     SYS\nNIC2    SYS     SYS     SYS     SYS     SYS     SYS     PHB     SYS     SYS     SYS      X      PIX\nNIC3    SYS     SYS     SYS     SYS     SYS     SYS     PHB     SYS     SYS     SYS     PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n\n==============================\n     Environment Variables\n==============================\nCUDA_CACHE_PATH=/data/users/quinnzhu/.nv/ComputeCache\nMAX_JOBS=48\nCUDA_NVCC_EXECUTABLE=/usr/local/cuda-12.8/lib64/\nLD_LIBRARY_PATH=/usr/local/cuda-12.8/lib64/:/usr/local/cuda/lib64:/usr/local/cuda-12.8/lib64/:/usr/local/cuda/lib64:\nCUDA_HOME=/usr/local/cuda-12.8\nCUDA_HOME=/usr/local/cuda-12.8\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nRun a simple code with FA3. torch inductor seems to fail with a Triton compilation error.\nTried to upgrade gcc but still facing same error.\n\nStack trace:\n```text\nINFO 06-14 21:52:14 [gpu_model_runner.py:1656] Model loading took 0.2389 GiB and 0.340157 seconds\nINFO 06-14 21:52:16 [backends.py:508] Using cache directory: /home/quinnzhu/.cache/vllm/torch_compile_cache/9eb7e1ffe1/rank_0_0/backbone for vLLM's torch.compile\nINFO 06-14 21:52:16 [backends.py:519] Dynamo bytecode transform time: 1.30 s\nIn file included from /data/users/quinnzhu/venv/p312env/lib/python3.12/site-packages/triton/backends/nvidia/include/cuda.h:56,\n                 from /tmp/tmp23j1qrsv/main.c:1:\n/usr/include/stdlib.h:31:10: fatal error: stddef.h: No such file or directory\n   31 | #include <stddef.h>\n      |          ^~~~~~~~~~\ncompilation terminated.\nERROR 06-14 21:52:16 [core.py:515] EngineCore failed to start.\nERROR 06-14 21:52:16 [core.py:515] Traceback (most recent call last):\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/engine/core.py\", line 506, in run_engine_core\nERROR 06-14 21:52:16 [core.py:515]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 06-14 21:52:16 [core.py:515]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/engine/core.py\", line 390, in __init__\nERROR 06-14 21:52:16 [core.py:515]     super().__init__(vllm_config, executor_class, log_stats,\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/engine/core.py\", line 83, in __init__\nERROR 06-14 21:52:16 [core.py:515]     self._initialize_kv_caches(vllm_config)\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/engine/core.py\", line 141, in _initialize_kv_caches\nERROR 06-14 21:52:16 [core.py:515]     available_gpu_memory = self.model_executor.determine_available_memory()\nERROR 06-14 21:52:16 [core.py:515]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/executor/abstract.py\", line 76, in determine_available_memory\nERROR 06-14 21:52:16 [core.py:515]     output = self.collective_rpc(\"determine_available_memory\")\nERROR 06-14 21:52:16 [core.py:515]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/gitrepos/vllm/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\nERROR 06-14 21:52:16 [core.py:515]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 06-14 21:52:16 [core.py:515]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/gitrepos/vllm/vllm/utils.py\", line 2690, in run_method\nERROR 06-14 21:52:16 [core.py:515]     return func(*args, **kwargs)\nERROR 06-14 21:52:16 [core.py:515]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 06-14 21:52:16 [core.py:515]     return func(*args, **kwargs)\nERROR 06-14 21:52:16 [core.py:515]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/worker/gpu_worker.py\", line 205, in determine_available_memory\nERROR 06-14 21:52:16 [core.py:515]     self.model_runner.profile_run()\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/worker/gpu_model_runner.py\", line 2045, in profile_run\nERROR 06-14 21:52:16 [core.py:515]     hidden_states = self._dummy_run(self.max_num_tokens)\nERROR 06-14 21:52:16 [core.py:515]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 06-14 21:52:16 [core.py:515]     return func(*args, **kwargs)\nERROR 06-14 21:52:16 [core.py:515]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/worker/gpu_model_runner.py\", line 1880, in _dummy_run\nERROR 06-14 21:52:16 [core.py:515]     outputs = model(\nERROR 06-14 21:52:16 [core.py:515]               ^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\nERROR 06-14 21:52:16 [core.py:515]     return self._call_impl(*args, **kwargs)\nERROR 06-14 21:52:16 [core.py:515]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\nERROR 06-14 21:52:16 [core.py:515]     return forward_call(*args, **kwargs)\nERROR 06-14 21:52:16 [core.py:515]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/gitrepos/vllm/vllm/model_executor/models/opt.py\", line 392, in forward\nERROR 06-14 21:52:16 [core.py:515]     hidden_states = self.model(input_ids, positions, intermediate_tensors,\nERROR 06-14 21:52:16 [core.py:515]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/gitrepos/vllm/vllm/compilation/decorators.py\", line 239, in __call__\nERROR 06-14 21:52:16 [core.py:515]     output = self.compiled_callable(*args, **kwargs)\nERROR 06-14 21:52:16 [core.py:515]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 663, in _fn\nERROR 06-14 21:52:16 [core.py:515]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1\nERROR 06-14 21:52:16 [core.py:515]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 760, in _compile_fx_inner\nERROR 06-14 21:52:16 [core.py:515]     raise InductorError(e, currentframe()).with_traceback(\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 745, in _compile_fx_inner\nERROR 06-14 21:52:16 [core.py:515]     mb_compiled_graph = fx_codegen_and_compile(\nERROR 06-14 21:52:16 [core.py:515]                         ^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1295, in fx_codegen_and_compile\nERROR 06-14 21:52:16 [core.py:515]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)\nERROR 06-14 21:52:16 [core.py:515]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1197, in codegen_and_compile\nERROR 06-14 21:52:16 [core.py:515]     compiled_fn = graph.compile_to_module().call\nERROR 06-14 21:52:16 [core.py:515]                   ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/graph.py\", line 2083, in compile_to_module\nERROR 06-14 21:52:16 [core.py:515]     return self._compile_to_module()\nERROR 06-14 21:52:16 [core.py:515]            ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/graph.py\", line 2091, in _compile_to_module\nERROR 06-14 21:52:16 [core.py:515]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\nERROR 06-14 21:52:16 [core.py:515]                                                              ^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/graph.py\", line 2002, in codegen\nERROR 06-14 21:52:16 [core.py:515]     self.scheduler.codegen()\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/scheduler.py\", line 4135, in codegen\nERROR 06-14 21:52:16 [core.py:515]     else self._codegen(self.nodes)\nERROR 06-14 21:52:16 [core.py:515]          ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/scheduler.py\", line 4264, in _codegen\nERROR 06-14 21:52:16 [core.py:515]     self.get_backend(device).codegen_node(node)\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/codegen/cuda_combined_scheduling.py\", line 104, in codegen_node\nERROR 06-14 21:52:16 [core.py:515]     return self._triton_scheduling.codegen_node(node)\nERROR 06-14 21:52:16 [core.py:515]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/codegen/simd.py\", line 1320, in codegen_node\nERROR 06-14 21:52:16 [core.py:515]     return self.codegen_node_schedule(\nERROR 06-14 21:52:16 [core.py:515]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/codegen/simd.py\", line 1365, in codegen_node_schedule\nERROR 06-14 21:52:16 [core.py:515]     src_code = kernel.codegen_kernel()\nERROR 06-14 21:52:16 [core.py:515]                ^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/codegen/triton.py\", line 3623, in codegen_kernel\nERROR 06-14 21:52:16 [core.py:515]     **self.inductor_meta_common(),\nERROR 06-14 21:52:16 [core.py:515]       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/codegen/triton.py\", line 3447, in inductor_meta_common\nERROR 06-14 21:52:16 [core.py:515]     \"backend_hash\": torch.utils._triton.triton_hash_with_backend(),\nERROR 06-14 21:52:16 [core.py:515]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/utils/_triton.py\", line 111, in triton_hash_with_backend\nERROR 06-14 21:52:16 [core.py:515]     backend = triton_backend()\nERROR 06-14 21:52:16 [core.py:515]               ^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/utils/_triton.py\", line 103, in triton_backend\nERROR 06-14 21:52:16 [core.py:515]     target = driver.active.get_current_target()\nERROR 06-14 21:52:16 [core.py:515]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/triton/runtime/driver.py\", line 23, in __getattr__\nERROR 06-14 21:52:16 [core.py:515]     self._initialize_obj()\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/triton/runtime/driver.py\", line 20, in _initialize_obj\nERROR 06-14 21:52:16 [core.py:515]     self._obj = self._init_fn()\nERROR 06-14 21:52:16 [core.py:515]                 ^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/triton/runtime/driver.py\", line 9, in _create_driver\nERROR 06-14 21:52:16 [core.py:515]     return actives[0]()\nERROR 06-14 21:52:16 [core.py:515]            ^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/triton/backends/nvidia/driver.py\", line 535, in __init__\nERROR 06-14 21:52:16 [core.py:515]     self.utils = CudaUtils()  # TODO: make static\nERROR 06-14 21:52:16 [core.py:515]                  ^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/triton/backends/nvidia/driver.py\", line 89, in __init__\nERROR 06-14 21:52:16 [core.py:515]     mod = compile_module_from_src(Path(os.path.join(dirname, \"driver.c\")).read_text(), \"cuda_utils\")\nERROR 06-14 21:52:16 [core.py:515]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/triton/backends/nvidia/driver.py\", line 66, in compile_module_from_src\nERROR 06-14 21:52:16 [core.py:515]     so = _build(name, src_path, tmpdir, library_dirs(), include_dir, libraries)\nERROR 06-14 21:52:16 [core.py:515]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-14 21:52:16 [core.py:515]   File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/triton/runtime/build.py\", line 36, in _build\nERROR 06-14 21:52:16 [core.py:515]     subprocess.check_call(cc_cmd, stdout=subprocess.DEVNULL)\nERROR 06-14 21:52:16 [core.py:515]   File \"/usr/lib64/python3.12/subprocess.py\", line 413, in check_call\nERROR 06-14 21:52:16 [core.py:515]     raise CalledProcessError(retcode, cmd)\nERROR 06-14 21:52:16 [core.py:515] torch._inductor.exc.InductorError: CalledProcessError: Command '['/usr/lib64/ccache/gcc', '/tmp/tmp23j1qrsv/main.c', '-O3', '-shared', '-fPIC', '-Wno-psabi', '-o', '/tmp/tmp23j1qrsv/cuda_utils.cpython-312-x86_64-linux-gnu.so', '-lcuda', '-L/data/users/quinnzhu/venv/p312env/lib/python3.12/site-packages/triton/backends/nvidia/lib', '-L/lib64', '-I/data/users/quinnzhu/venv/p312env/lib/python3.12/site-packages/triton/backends/nvidia/include', '-I/tmp/tmp23j1qrsv', '-I/usr/include/python3.12']' returned non-zero exit status 1.\nERROR 06-14 21:52:16 [core.py:515] \nERROR 06-14 21:52:16 [core.py:515] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\nERROR 06-14 21:52:16 [core.py:515] \nProcess EngineCore_0:\nTraceback (most recent call last):\n  File \"/usr/lib64/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib64/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/engine/core.py\", line 519, in run_engine_core\n    raise e\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/engine/core.py\", line 506, in run_engine_core\n    engine_core = EngineCoreProc(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/engine/core.py\", line 390, in __init__\n    super().__init__(vllm_config, executor_class, log_stats,\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/engine/core.py\", line 83, in __init__\n    self._initialize_kv_caches(vllm_config)\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/engine/core.py\", line 141, in _initialize_kv_caches\n    available_gpu_memory = self.model_executor.determine_available_memory()\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/executor/abstract.py\", line 76, in determine_available_memory\n    output = self.collective_rpc(\"determine_available_memory\")\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/utils.py\", line 2690, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/worker/gpu_worker.py\", line 205, in determine_available_memory\n    self.model_runner.profile_run()\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/worker/gpu_model_runner.py\", line 2045, in profile_run\n    hidden_states = self._dummy_run(self.max_num_tokens)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/worker/gpu_model_runner.py\", line 1880, in _dummy_run\n    outputs = model(\n              ^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/model_executor/models/opt.py\", line 392, in forward\n    hidden_states = self.model(input_ids, positions, intermediate_tensors,\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/compilation/decorators.py\", line 239, in __call__\n    output = self.compiled_callable(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 663, in _fn\n    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 760, in _compile_fx_inner\n    raise InductorError(e, currentframe()).with_traceback(\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 745, in _compile_fx_inner\n    mb_compiled_graph = fx_codegen_and_compile(\n                        ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1295, in fx_codegen_and_compile\n    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1197, in codegen_and_compile\n    compiled_fn = graph.compile_to_module().call\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/graph.py\", line 2083, in compile_to_module\n    return self._compile_to_module()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/graph.py\", line 2091, in _compile_to_module\n    self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n                                                             ^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/graph.py\", line 2002, in codegen\n    self.scheduler.codegen()\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/scheduler.py\", line 4135, in codegen\n    else self._codegen(self.nodes)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/scheduler.py\", line 4264, in _codegen\n    self.get_backend(device).codegen_node(node)\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/codegen/cuda_combined_scheduling.py\", line 104, in codegen_node\n    return self._triton_scheduling.codegen_node(node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/codegen/simd.py\", line 1320, in codegen_node\n    return self.codegen_node_schedule(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/codegen/simd.py\", line 1365, in codegen_node_schedule\n    src_code = kernel.codegen_kernel()\n               ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/codegen/triton.py\", line 3623, in codegen_kernel\n    **self.inductor_meta_common(),\n      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/_inductor/codegen/triton.py\", line 3447, in inductor_meta_common\n    \"backend_hash\": torch.utils._triton.triton_hash_with_backend(),\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/utils/_triton.py\", line 111, in triton_hash_with_backend\n    backend = triton_backend()\n              ^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/torch/utils/_triton.py\", line 103, in triton_backend\n    target = driver.active.get_current_target()\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/triton/runtime/driver.py\", line 23, in __getattr__\n    self._initialize_obj()\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/triton/runtime/driver.py\", line 20, in _initialize_obj\n    self._obj = self._init_fn()\n                ^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/triton/runtime/driver.py\", line 9, in _create_driver\n    return actives[0]()\n           ^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/triton/backends/nvidia/driver.py\", line 535, in __init__\n    self.utils = CudaUtils()  # TODO: make static\n                 ^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/triton/backends/nvidia/driver.py\", line 89, in __init__\n    mod = compile_module_from_src(Path(os.path.join(dirname, \"driver.c\")).read_text(), \"cuda_utils\")\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/triton/backends/nvidia/driver.py\", line 66, in compile_module_from_src\n    so = _build(name, src_path, tmpdir, library_dirs(), include_dir, libraries)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/venv/p312env/lib64/python3.12/site-packages/triton/runtime/build.py\", line 36, in _build\n    subprocess.check_call(cc_cmd, stdout=subprocess.DEVNULL)\n  File \"/usr/lib64/python3.12/subprocess.py\", line 413, in check_call\n    raise CalledProcessError(retcode, cmd)\ntorch._inductor.exc.InductorError: CalledProcessError: Command '['/usr/lib64/ccache/gcc', '/tmp/tmp23j1qrsv/main.c', '-O3', '-shared', '-fPIC', '-Wno-psabi', '-o', '/tmp/tmp23j1qrsv/cuda_utils.cpython-312-x86_64-linux-gnu.so', '-lcuda', '-L/data/users/quinnzhu/venv/p312env/lib/python3.12/site-packages/triton/backends/nvidia/lib', '-L/lib64', '-I/data/users/quinnzhu/venv/p312env/lib/python3.12/site-packages/triton/backends/nvidia/include', '-I/tmp/tmp23j1qrsv', '-I/usr/include/python3.12']' returned non-zero exit status 1.\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\nTraceback (most recent call last):\n  File \"/data/users/quinnzhu/gitrepos/vllm/run_fa3.py\", line 15, in <module>\n    llm = LLM(model=\"facebook/opt-125m\")\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/entrypoints/llm.py\", line 262, in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/engine/llm_engine.py\", line 501, in from_engine_args\n    return engine_cls.from_vllm_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/engine/llm_engine.py\", line 124, in from_vllm_config\n    return cls(vllm_config=vllm_config,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/engine/llm_engine.py\", line 101, in __init__\n    self.engine_core = EngineCoreClient.make_client(\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/engine/core_client.py\", line 75, in make_client\n    return SyncMPClient(vllm_config, executor_class, log_stats)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/engine/core_client.py\", line 558, in __init__\n    super().__init__(\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/engine/core_client.py\", line 422, in __init__\n    self._init_engines_direct(vllm_config, local_only,\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/engine/core_client.py\", line 491, in _init_engines_direct\n    self._wait_for_engine_startup(handshake_socket, input_address,\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/engine/core_client.py\", line 511, in _wait_for_engine_startup\n    wait_for_engine_startup(\n  File \"/data/users/quinnzhu/gitrepos/vllm/vllm/v1/utils.py\", line 494, in wait_for_engine_startup\n    raise RuntimeError(\"Engine core initialization failed. \"\n```\n\nCode:\n```python\nimport os\nos.environ[\"VLLM_FLASH_ATTN_VERSION\"] = \"3\"\nos.environ[\"VLLM_ATTENTION_BACKEND\"] = \"FLASH_ATTN\"\n\nfrom vllm import LLM, SamplingParams\n\nprompts = [\n    \"Hello, my name is\",\n    \"The president of the United States is\",\n    \"The capital of France is\",\n    \"The future of AI is\",\n]\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\nllm = LLM(model=\"facebook/opt-125m\")\noutputs = llm.generate(prompts, sampling_params)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "closed",
    "created_at": "2025-06-15T05:05:13+00:00",
    "closed_at": "2025-06-20T05:00:54+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19656/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19656"
  },
  {
    "number": 19631,
    "title": "[Bug]: Illegal memory access on llama4 maverick",
    "body": "### Your current environment\n\nPyTorch 2.7.0, vLLM main branch built from source.\n\n### \ud83d\udc1b Describe the bug\n\nRepro:\n```py\nvllm serve meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 --tensor-parallel-size 8 --max-num-batched-tokens 40000 --max-model-len 8192 --max-num-seqs 128 --gpu-memory-utilization 0.8\n```\ngives a CUDA Illegal Memory Access, as well as some errors:\n```\nERROR 06-13 15:32:09 [core.py:515] EngineCore failed to start.\nERROR 06-13 15:32:09 [core.py:515] Traceback (most recent call last):\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/engine/core.py\", line 506, in run_engine_core\nERROR 06-13 15:32:09 [core.py:515]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 06-13 15:32:09 [core.py:515]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/engine/core.py\", line 390, in __init__\nERROR 06-13 15:32:09 [core.py:515]     super().__init__(vllm_config, executor_class, log_stats,\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/engine/core.py\", line 83, in __init__\nERROR 06-13 15:32:09 [core.py:515]     self._initialize_kv_caches(vllm_config)\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/engine/core.py\", line 168, in _initialize_kv_caches\nERROR 06-13 15:32:09 [core.py:515]     self.model_executor.initialize_from_config(kv_cache_configs)\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/executor/abstract.py\", line 66, in initialize_from_config\nERROR 06-13 15:32:09 [core.py:515]     self.collective_rpc(\"compile_or_warm_up_model\")\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/executor/multiproc_executor.py\", line 220, in collective_rpc\nERROR 06-13 15:32:09 [core.py:515]     result = get_response(w, dequeue_timeout)\nERROR 06-13 15:32:09 [core.py:515]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/executor/multiproc_executor.py\", line 207, in get_response\nERROR 06-13 15:32:09 [core.py:515]     raise RuntimeError(\nERROR 06-13 15:32:09 [core.py:515] RuntimeError: Worker failed with error 'Expected result >= 0 to be true, but got false.  (Could this error message be\n improved?  If so, please report an enhancement request to PyTorch.)', please check the stack trace above for the root cause\n(VllmWorker rank=1 pid=3350867) ERROR 06-13 15:32:09 [multiproc_executor.py:527]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/compilation/cuda_piece\nwise_backend.py\", line 156, in __call__\n(VllmWorker rank=1 pid=3350867) ERROR 06-13 15:32:09 [multiproc_executor.py:527]     return entry.runnable(*args)\n(VllmWorker rank=2 pid=3350868) ERROR 06-13 15:32:09 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=3350867) ERROR 06-13 15:32:09 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=2 pid=3350868) ERROR 06-13 15:32:09 [multiproc_executor.py:527]   File \"/home/rzou/.cache/vllm/torch_compile_cache/d98525c527/rank_2_0/\ninductor_cache/rl/crl3f6qy7nm5k2qs65o6f44vppuehyqkkmjhxy6q5mty7zgba2kx.py\", line 1282, in call\n(VllmWorker rank=7 pid=3350875) ERROR 06-13 15:32:09 [multiproc_executor.py:527]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/compilation/cuda_piece\nwise_backend.py\", line 156, in __call__\n(VllmWorker rank=1 pid=3350867) ERROR 06-13 15:32:09 [multiproc_executor.py:527]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/compilation/compiler_i\nnterface.py\", line 510, in compiled_graph\n(VllmWorker rank=2 pid=3350868) ERROR 06-13 15:32:09 [multiproc_executor.py:527]     buf52 = empty_strided_cuda(((-32768) + s0, ), (1, ), torch.int32)\n(VllmWorker rank=5 pid=3350871) ERROR 06-13 15:32:09 [multiproc_executor.py:527]     return self.current_callable(inputs)\n(VllmWorker rank=7 pid=3350875) ERROR 06-13 15:32:09 [multiproc_executor.py:527]     return entry.runnable(*args)\n(VllmWorker rank=6 pid=3350873) ERROR 06-13 15:32:09 [multiproc_executor.py:527]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n...\n\n(VllmWorker rank=7 pid=3350875) Exception ignored in: <function CustomAllreduce.__del__ at 0x7efceedfe2a0>\n(VllmWorker rank=7 pid=3350875) Traceback (most recent call last):\n(VllmWorker rank=7 pid=3350875)   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/distributed/device_communicators/custom_all_reduce.py\", line 276, in __\ndel__\n(VllmWorker rank=7 pid=3350875)     self.close()\n(VllmWorker rank=7 pid=3350875)   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/distributed/device_communicators/custom_all_reduce.py\", line 272, in cl\nose\n(VllmWorker rank=7 pid=3350875)     self.free_shared_buffer(self.meta_ptrs, rank=self.rank)\n(VllmWorker rank=7 pid=3350875)   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/distributed/device_communicators/custom_all_reduce.py\", line 304, in fr\nee_shared_buffer\n(VllmWorker rank=7 pid=3350875)     ops.free_shared_buffer(pointers[rank])\n(VllmWorker rank=7 pid=3350875)   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/_custom_ops.py\", line 1758, in free_shared_buffer\n(VllmWorker rank=7 pid=3350875)     torch.ops._C_custom_ar.free_shared_buffer(ptr)\n(VllmWorker rank=7 pid=3350875)   File \"/home/rzou/dev/stable0/env/lib/python3.12/site-packages/torch/_ops.py\", line 1158, in __call__\n(VllmWorker rank=7 pid=3350875)     return self._op(*args, **(kwargs or {}))\n(VllmWorker rank=7 pid=3350875)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=7 pid=3350875) RuntimeError: CUDA error: an illegal memory access was encountered\n(VllmWorker rank=7 pid=3350875) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n(VllmWorker rank=7 pid=3350875) For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n(VllmWorker rank=7 pid=3350875) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n(VllmWorker rank=7 pid=3350875)\n(VllmWorker rank=1 pid=3350867) ERROR 06-13 15:32:09 [multiproc_executor.py:527]     graph_output = inductor_compiled_graph(list_args)\n(VllmWorker rank=5 pid=3350871) ERROR 06-13 15:32:09 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=2 pid=3350868) ERROR 06-13 15:32:09 [multiproc_executor.py:527]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=7 pid=3350875) ERROR 06-13 15:32:09 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^\n```\n\nI think this started from https://github.com/vllm-project/vllm/pull/19168. After turning off the chunking optimization, the errors go away.\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile",
      "llama"
    ],
    "state": "closed",
    "created_at": "2025-06-13T22:33:29+00:00",
    "closed_at": "2025-07-07T17:10:56+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19631/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19631"
  },
  {
    "number": 17348,
    "title": "[Bug]:  h unknown: block: [487,0,0], thread: [31,0,0] Assertion `index out of bounds: 0 <= tl.broadcast_to(tmp34, [XBLOCK]) < 131072` failed.",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: CentOS Linux 8 (x86_64)\nGCC version: (GCC) 10.5.0\nClang version: Could not collect\nCMake version: version 3.20.2\nLibc version: glibc-2.29\n\nPython version: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0] (64-bit runtime)\nPython platform: Linux-4.18.0-348.7.1.el8_5.x86_64-x86_64-with-glibc2.29\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 550.135\ncuDNN version: Probably one of the following:\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn.so.8.9.7\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.9.7\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.9.7\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.9.7\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.9.7\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.9.7\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.9.7\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\n\u67b6\u6784\uff1a           x86_64\nCPU \u8fd0\u884c\u6a21\u5f0f\uff1a   32-bit, 64-bit\n\u5b57\u8282\u5e8f\uff1a         Little Endian\nCPU:             144\n\u5728\u7ebf CPU \u5217\u8868\uff1a  0-143\n\u6bcf\u4e2a\u6838\u7684\u7ebf\u7a0b\u6570\uff1a 2\n\u6bcf\u4e2a\u5ea7\u7684\u6838\u6570\uff1a   36\n\u5ea7\uff1a             2\nNUMA \u8282\u70b9\uff1a      2\n\u5382\u5546 ID\uff1a        GenuineIntel\nCPU \u7cfb\u5217\uff1a       6\n\u578b\u53f7\uff1a           106\n\u578b\u53f7\u540d\u79f0\uff1a       Intel(R) Xeon(R) Platinum 8352V CPU @ 2.10GHz\n\u6b65\u8fdb\uff1a           6\nCPU MHz\uff1a        1973.831\nCPU \u6700\u5927 MHz\uff1a   2101.0000\nCPU \u6700\u5c0f MHz\uff1a   800.0000\nBogoMIPS\uff1a       4200.00\n\u865a\u62df\u5316\uff1a         VT-x\nL1d \u7f13\u5b58\uff1a       48K\nL1i \u7f13\u5b58\uff1a       32K\nL2 \u7f13\u5b58\uff1a        1280K\nL3 \u7f13\u5b58\uff1a        55296K\nNUMA \u8282\u70b90 CPU\uff1a 0-35,72-107\nNUMA \u8282\u70b91 CPU\uff1a 36-71,108-143\n\u6807\u8bb0\uff1a           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdqdtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust sgx bmi1 hle avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cdsha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid sgx_lc fsrm md_clear pconfig flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] gptqmodel==1.9.0+cu124torch2.5\n[pip3] numpy==1.25.0\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] triton==3.1.0\n[conda] gptqmodel                 1.9.0+cu124torch2.5          pypi_0    pypi\n[conda] numpy                     1.25.0                   pypi_0    pypi\n[conda] torch                     2.5.1                    pypi_0    pypi\n[conda] torchaudio                2.5.1                    pypi_0    pypi\n[conda] torchvision               0.20.1                   pypi_0    pypi\n[conda] triton                    3.1.0                    pypi_0    pypi\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI run in termine and when model start the error occurs\n\n(VllmWorker rank=3 pid=24287) INFO 04-29 11:48:08 [backends.py:430] Dynamo bytecode transform time: 14.77 s\n(VllmWorker rank=1 pid=24285) INFO 04-29 11:48:11 [backends.py:136] Cache the graph of shape None for later use\n(VllmWorker rank=0 pid=24284) INFO 04-29 11:48:11 [backends.py:136] Cache the graph of shape None for later use\n(VllmWorker rank=5 pid=24289) INFO 04-29 11:48:11 [backends.py:136] Cache the graph of shape None for later use\n(VllmWorker rank=2 pid=24286) INFO 04-29 11:48:12 [backends.py:136] Cache the graph of shape None for later use\n(VllmWorker rank=4 pid=24288) INFO 04-29 11:48:12 [backends.py:136] Cache the graph of shape None for later use\n(VllmWorker rank=7 pid=24291) INFO 04-29 11:48:12 [backends.py:136] Cache the graph of shape None for later use\n(VllmWorker rank=6 pid=24290) INFO 04-29 11:48:12 [backends.py:136] Cache the graph of shape None for later use\n(VllmWorker rank=3 pid=24287) INFO 04-29 11:48:12 [backends.py:136] Cache the graph of shape None for later use\n(VllmWorker rank=1 pid=24285) INFO 04-29 11:49:01 [backends.py:148] Compiling a graph for general shape takes 52.85 s\n(VllmWorker rank=4 pid=24288) INFO 04-29 11:49:02 [backends.py:148] Compiling a graph for general shape takes 53.08 s\n(VllmWorker rank=2 pid=24286) INFO 04-29 11:49:02 [backends.py:148] Compiling a graph for general shape takes 53.17 s\n(VllmWorker rank=0 pid=24284) INFO 04-29 11:49:02 [backends.py:148] Compiling a graph for general shape takes 53.29 s\n(VllmWorker rank=6 pid=24290) INFO 04-29 11:49:02 [backends.py:148] Compiling a graph for general shape takes 53.20 s\n(VllmWorker rank=5 pid=24289) INFO 04-29 11:49:02 [backends.py:148] Compiling a graph for general shape takes 53.45 s\n(VllmWorker rank=7 pid=24291) INFO 04-29 11:49:02 [backends.py:148] Compiling a graph for general shape takes 53.65 s\n(VllmWorker rank=3 pid=24287) INFO 04-29 11:49:04 [backends.py:148] Compiling a graph for general shape takes 54.75 s\n/home/server/.cache/vllm/torch_compile_cache/ed10166bd0/rank_7_0/inductor_cache/se/csevw3yquhsktn7rdvuevsglpc4rb3qbdlu4a5lnuzofdrq54f2o.py:62: unknown: block: [486,0,0], thread: [8,0,0] Assertion `index out of bounds: 0 <= tl.broadcast_to(tmp34, [XBLOCK]) < 131072` failed.\n\nI can't find any solutions. I check the nccl and the output shows passed.\n\n# Using devices\n#  Rank  0 Group  0 Pid  20836 on    6000gpu device  0 [0000:4f:00] NVIDIA RTX A6000\n#  Rank  1 Group  0 Pid  20836 on    6000gpu device  1 [0000:52:00] NVIDIA RTX A6000\n#  Rank  2 Group  0 Pid  20836 on    6000gpu device  2 [0000:56:00] NVIDIA RTX A6000\n#  Rank  3 Group  0 Pid  20836 on    6000gpu device  3 [0000:57:00] NVIDIA RTX A6000\n#  Rank  4 Group  0 Pid  20836 on    6000gpu device  4 [0000:ce:00] NVIDIA RTX A6000\n#  Rank  5 Group  0 Pid  20836 on    6000gpu device  5 [0000:d1:00] NVIDIA RTX A6000\n#  Rank  6 Group  0 Pid  20836 on    6000gpu device  6 [0000:d5:00] NVIDIA RTX A6000\n#  Rank  7 Group  0 Pid  20836 on    6000gpu device  7 [0000:d6:00] NVIDIA RTX A6000\n#\n#                                                              out-of-place                       in-place\n#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong\n#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)\n           8             2     float     sum      -1    32.29    0.00    0.00      0    31.62    0.00    0.00      0\n          16             4     float     sum      -1    31.84    0.00    0.00      0    31.70    0.00    0.00      0\n          32             8     float     sum      -1    31.82    0.00    0.00      0    31.89    0.00    0.00      0\n          64            16     float     sum      -1    31.98    0.00    0.00      0    32.13    0.00    0.00      0\n         128            32     float     sum      -1    32.70    0.00    0.01      0    32.71    0.00    0.01      0\n         256            64     float     sum      -1    32.65    0.01    0.01      0    33.01    0.01    0.01      0\n         512           128     float     sum      -1    32.53    0.02    0.03      0    32.54    0.02    0.03      0\n        1024           256     float     sum      -1    32.62    0.03    0.05      0    32.68    0.03    0.05      0\n        2048           512     float     sum      -1    32.71    0.06    0.11      0    32.47    0.06    0.11      0\n        4096          1024     float     sum      -1    32.63    0.13    0.22      0    32.56    0.13    0.22      0\n        8192          2048     float     sum      -1    32.62    0.25    0.44      0    32.84    0.25    0.44      0\n       16384          4096     float     sum      -1    32.99    0.50    0.87      0    32.70    0.50    0.88      0\n       32768          8192     float     sum      -1    32.91    1.00    1.74      0    32.87    1.00    1.74      0\n       65536         16384     float     sum      -1    35.10    1.87    3.27      0    34.37    1.91    3.34      0\n      131072         32768     float     sum      -1    53.54    2.45    4.28      0    53.01    2.47    4.33      0\n      262144         65536     float     sum      -1    90.53    2.90    5.07      0    92.21    2.84    4.98      0\n      524288        131072     float     sum      -1    147.3    3.56    6.23      0    145.9    3.59    6.29      0\n     1048576        262144     float     sum      -1    206.2    5.09    8.90      0    206.0    5.09    8.91      0\n     2097152        524288     float     sum      -1    298.0    7.04   12.32      0    297.6    7.05   12.33      0\n     4194304       1048576     float     sum      -1    503.1    8.34   14.59      0    501.8    8.36   14.63      0\n     8388608       2097152     float     sum      -1    965.6    8.69   15.20      0    967.6    8.67   15.17      0\n    16777216       4194304     float     sum      -1   1890.5    8.87   15.53      0   1890.6    8.87   15.53      0\n    33554432       8388608     float     sum      -1   3761.6    8.92   15.61      0   3757.6    8.93   15.63      0\n    67108864      16777216     float     sum      -1   7477.6    8.97   15.71      0   7474.7    8.98   15.71      0\n   134217728      33554432     float     sum      -1    14920    9.00   15.74      0    14874    9.02   15.79      0\n# Out of bounds values : 0 OK\n# Avg bus bandwidth    : 5.44125 \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "closed",
    "created_at": "2025-04-29T03:54:46+00:00",
    "closed_at": "2025-04-29T05:45:53+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17348/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17348"
  },
  {
    "number": 17344,
    "title": "[Bug]: Running `vllm serve Qwen2.5-VL-72B-Instruct-AWQ` results in an error when upgrading the vLLM version to 0.8.5.",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.1 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-91-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-80GB\nNvidia driver version: 535.161.08\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      46 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             14\nOn-line CPU(s) list:                0-13\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) Platinum 8336C CPU @ 2.30GHz\nCPU family:                         6\nModel:                              106\nThread(s) per core:                 2\nCore(s) per socket:                 7\nSocket(s):                          1\nStepping:                           6\nBogoMIPS:                           4589.21\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd arat avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid md_clear arch_capabilities\nHypervisor vendor:                  KVM\nVirtualization type:                full\nL1d cache:                          336 KiB (7 instances)\nL1i cache:                          224 KiB (7 instances)\nL2 cache:                           8.8 MiB (7 instances)\nL3 cache:                           54 MiB (1 instance)\nNUMA node(s):                       1\nNUMA node0 CPU(s):                  0-13\nVulnerability Gather data sampling: Unknown: Dependent on hypervisor status\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.2\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.5\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-13    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nReproduce command: vllm serve /data/modelscope/Qwen2.5-VL-72B-Instruct-AWQ --max-model-len=32768 --limit-mm-per-prompt '{\"image\"=16}' --gpu-memory-utilization=0.9\n\nError output like below:\n```\nERROR 04-29 10:42:00 [core.py:396] EngineCore failed to start.\nERROR 04-29 10:42:00 [core.py:396] Traceback (most recent call last):\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\nERROR 04-29 10:42:00 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\nERROR 04-29 10:42:00 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 71, in __init__\nERROR 04-29 10:42:00 [core.py:396]     self._initialize_kv_caches(vllm_config)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 129, in _initialize_kv_caches\nERROR 04-29 10:42:00 [core.py:396]     available_gpu_memory = self.model_executor.determine_available_memory()\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/executor/abstract.py\", line 75, in determine_available_memory\nERROR 04-29 10:42:00 [core.py:396]     output = self.collective_rpc(\"determine_available_memory\")\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\nERROR 04-29 10:42:00 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/utils.py\", line 2456, in run_method\nERROR 04-29 10:42:00 [core.py:396]     return func(*args, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 04-29 10:42:00 [core.py:396]     return func(*args, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 183, in determine_available_memory\nERROR 04-29 10:42:00 [core.py:396]     self.model_runner.profile_run()\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1651, in profile_run\nERROR 04-29 10:42:00 [core.py:396]     hidden_states = self._dummy_run(self.max_num_tokens)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 04-29 10:42:00 [core.py:396]     return func(*args, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1497, in _dummy_run\nERROR 04-29 10:42:00 [core.py:396]     outputs = model(\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nERROR 04-29 10:42:00 [core.py:396]     return self._call_impl(*args, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\nERROR 04-29 10:42:00 [core.py:396]     return forward_call(*args, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2_5_vl.py\", line 1106, in forward\nERROR 04-29 10:42:00 [core.py:396]     hidden_states = self.language_model.model(\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 238, in __call__\nERROR 04-29 10:42:00 [core.py:396]     output = self.compiled_callable(*args, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 574, in _fn\nERROR 04-29 10:42:00 [core.py:396]     return fn(*args, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 325, in forward\nERROR 04-29 10:42:00 [core.py:396]     def forward(\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nERROR 04-29 10:42:00 [core.py:396]     return self._call_impl(*args, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\nERROR 04-29 10:42:00 [core.py:396]     return forward_call(*args, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 745, in _fn\nERROR 04-29 10:42:00 [core.py:396]     return fn(*args, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/fx/graph_module.py\", line 822, in call_wrapped\nERROR 04-29 10:42:00 [core.py:396]     return self._wrapped_call(self, *args, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/fx/graph_module.py\", line 400, in __call__\nERROR 04-29 10:42:00 [core.py:396]     raise e\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/fx/graph_module.py\", line 387, in __call__\nERROR 04-29 10:42:00 [core.py:396]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nERROR 04-29 10:42:00 [core.py:396]     return self._call_impl(*args, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\nERROR 04-29 10:42:00 [core.py:396]     return forward_call(*args, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"<eval_with_key>.162\", line 2169, in forward\nERROR 04-29 10:42:00 [core.py:396]     submod_0 = self.submod_0(l_inputs_embeds_, s0, l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_qweight_, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_scales_, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_qzeros_, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_g_idx_, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_g_idx_sort_indices_, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_workspace, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_bias_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_, l_positions_, s2);  l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_qweight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_scales_ = l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_qzeros_ = l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_g_idx_ = l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_g_idx_sort_indices_ = l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_workspace = l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_bias_ = None\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/compilation/backends.py\", line 612, in __call__\nERROR 04-29 10:42:00 [core.py:396]     return self.compiled_graph_for_general_shape(*args)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 745, in _fn\nERROR 04-29 10:42:00 [core.py:396]     return fn(*args, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 1184, in forward\nERROR 04-29 10:42:00 [core.py:396]     return compiled_fn(full_args)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 323, in runtime_wrapper\nERROR 04-29 10:42:00 [core.py:396]     all_outs = call_func_at_runtime_with_args(\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\nERROR 04-29 10:42:00 [core.py:396]     out = normalize_as_list(f(args))\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 672, in inner_fn\nERROR 04-29 10:42:00 [core.py:396]     outs = compiled_fn(args)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 490, in wrapper\nERROR 04-29 10:42:00 [core.py:396]     return compiled_fn(runtime_args)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_inductor/output_code.py\", line 466, in __call__\nERROR 04-29 10:42:00 [core.py:396]     return self.current_callable(inputs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_inductor/utils.py\", line 2128, in run\nERROR 04-29 10:42:00 [core.py:396]     return model(new_inputs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/.cache/vllm/torch_compile_cache/ad24c14253/rank_0_0/inductor_cache/qo/cqofcthiyqgq2qgfhymaumxrr5ycw2u6xxcpo74ztvqslexfiejb.py\", line 671, in call\nERROR 04-29 10:42:00 [core.py:396]     triton_poi_fused_add_4.run(buf12, arg9_1, triton_poi_fused_add_4_xnumel, grid=grid(triton_poi_fused_add_4_xnumel), stream=stream0)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 1034, in run\nERROR 04-29 10:42:00 [core.py:396]     self.autotune_to_one_config(*args, grid=grid, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 911, in autotune_to_one_config\nERROR 04-29 10:42:00 [core.py:396]     timings = self.benchmark_all_configs(*args, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 885, in benchmark_all_configs\nERROR 04-29 10:42:00 [core.py:396]     timings = {\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 886, in <dictcomp>\nERROR 04-29 10:42:00 [core.py:396]     launcher: self.bench(launcher, *args, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 787, in bench\nERROR 04-29 10:42:00 [core.py:396]     return benchmarker.benchmark_gpu(kernel_call, rep=40)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_inductor/runtime/benchmarking.py\", line 66, in wrapper\nERROR 04-29 10:42:00 [core.py:396]     return fn(self, *args, **kwargs)\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_inductor/runtime/benchmarking.py\", line 202, in benchmark_gpu\nERROR 04-29 10:42:00 [core.py:396]     return self.triton_do_bench(_callable, **kwargs, return_mode=\"median\")\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/triton/testing.py\", line 118, in do_bench\nERROR 04-29 10:42:00 [core.py:396]     di.synchronize()\nERROR 04-29 10:42:00 [core.py:396]   File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 985, in synchronize\nERROR 04-29 10:42:00 [core.py:396]     return torch._C._cuda_synchronize()\nERROR 04-29 10:42:00 [core.py:396] RuntimeError: CUDA error: an illegal memory access was encountered\nERROR 04-29 10:42:00 [core.py:396] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nERROR 04-29 10:42:00 [core.py:396] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\nERROR 04-29 10:42:00 [core.py:396] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\nERROR 04-29 10:42:00 [core.py:396]\nProcess EngineCore_0:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 400, in run_engine_core\n    raise e\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n    engine_core = EngineCoreProc(*args, **kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n    super().__init__(vllm_config, executor_class, log_stats,\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 71, in __init__\n    self._initialize_kv_caches(vllm_config)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 129, in _initialize_kv_caches\n    available_gpu_memory = self.model_executor.determine_available_memory()\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/executor/abstract.py\", line 75, in determine_available_memory\n    output = self.collective_rpc(\"determine_available_memory\")\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/utils.py\", line 2456, in run_method\n    return func(*args, **kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 183, in determine_available_memory\n    self.model_runner.profile_run()\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1651, in profile_run\n    hidden_states = self._dummy_run(self.max_num_tokens)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1497, in _dummy_run\n    outputs = model(\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2_5_vl.py\", line 1106, in forward\n    hidden_states = self.language_model.model(\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 238, in __call__\n    output = self.compiled_callable(*args, **kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 574, in _fn\n    return fn(*args, **kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 325, in forward\n    def forward(\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 745, in _fn\n    return fn(*args, **kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/fx/graph_module.py\", line 822, in call_wrapped\n    return self._wrapped_call(self, *args, **kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n    raise e\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/fx/graph_module.py\", line 387, in __call__\n    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"<eval_with_key>.162\", line 2169, in forward\n    submod_0 = self.submod_0(l_inputs_embeds_, s0, l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_qweight_, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_scales_, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_qzeros_, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_g_idx_, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_g_idx_sort_indices_, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_workspace, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_bias_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_, l_positions_, s2);  l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_qweight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_scales_ = l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_qzeros_ = l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_g_idx_ = l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_g_idx_sort_indices_ = l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_workspace = l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_bias_ = None\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/compilation/backends.py\", line 612, in __call__\n    return self.compiled_graph_for_general_shape(*args)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 745, in _fn\n    return fn(*args, **kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 1184, in forward\n    return compiled_fn(full_args)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 323, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 672, in inner_fn\n    outs = compiled_fn(args)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 490, in wrapper\n    return compiled_fn(runtime_args)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_inductor/output_code.py\", line 466, in __call__\n    return self.current_callable(inputs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_inductor/utils.py\", line 2128, in run\n    return model(new_inputs)\n  File \"/home/lucas/.cache/vllm/torch_compile_cache/ad24c14253/rank_0_0/inductor_cache/qo/cqofcthiyqgq2qgfhymaumxrr5ycw2u6xxcpo74ztvqslexfiejb.py\", line 671, in call\n    triton_poi_fused_add_4.run(buf12, arg9_1, triton_poi_fused_add_4_xnumel, grid=grid(triton_poi_fused_add_4_xnumel), stream=stream0)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 1034, in run\n    self.autotune_to_one_config(*args, grid=grid, **kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 911, in autotune_to_one_config\n    timings = self.benchmark_all_configs(*args, **kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 885, in benchmark_all_configs\n    timings = {\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 886, in <dictcomp>\n    launcher: self.bench(launcher, *args, **kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 787, in bench\n    return benchmarker.benchmark_gpu(kernel_call, rep=40)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_inductor/runtime/benchmarking.py\", line 66, in wrapper\n    return fn(self, *args, **kwargs)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/_inductor/runtime/benchmarking.py\", line 202, in benchmark_gpu\n    return self.triton_do_bench(_callable, **kwargs, return_mode=\"median\")\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/triton/testing.py\", line 118, in do_bench\n    di.synchronize()\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 985, in synchronize\n    return torch._C._cuda_synchronize()\nRuntimeError: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n[rank0]:[W429 10:42:01.769171143 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nTraceback (most recent call last):\n  File \"/home/lucas/envs/nlp-vllm/bin/vllm\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/entrypoints/cli/main.py\", line 53, in main\n    args.dispatch_function(args)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/entrypoints/cli/serve.py\", line 27, in cmd\n    uvloop.run(run_server(args))\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/uvloop/__init__.py\", line 82, in run\n    return loop.run_until_complete(wrapper())\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 1078, in run_server\n    async with build_async_engine_client(args) as engine_client:\n  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 146, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 178, in build_async_engine_client_from_engine_args\n    async_llm = AsyncLLM.from_vllm_config(\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py\", line 150, in from_vllm_config\n    return cls(\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py\", line 118, in __init__\n    self.engine_core = core_client_class(\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\", line 642, in __init__\n    super().__init__(\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\", line 398, in __init__\n    self._wait_for_engine_startup()\n  File \"/home/lucas/envs/nlp-vllm/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\", line 430, in _wait_for_engine_startup\n    raise RuntimeError(\"Engine core initialization failed. \"\nRuntimeError: Engine core initialization failed. See root cause above.\n```\n\nTips: Everything works fine with vLLM version 0.8.4.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "closed",
    "created_at": "2025-04-29T02:51:02+00:00",
    "closed_at": "2025-05-13T04:18:28+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17344/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17344"
  },
  {
    "number": 17309,
    "title": "[Bug]: triton placeholder is conflicting with pytorch's triton checks",
    "body": "### Your current environment\n\nAddition of a PlaceholderModule for triton [PR:15099](https://github.com/vllm-project/vllm/pull/15099) has broken pytorch's internal checks for triton. This is breaking vllm's model serving (tested for arch: ppc64le).\n\nPytorch has  conditional checks for triton [_is_triton_available()](https://github.com/pytorch/pytorch/blob/v2.6.0/torch/_inductor/runtime/hints.py#L34)\nOnce vllm is imported, the above referenced function returns `True` and the control wrongly flows to importing triton functions which causes `ModuleNotFoundError` [here](https://github.com/pytorch/pytorch/blob/v2.6.0/torch/_inductor/runtime/hints.py#L67)\n\n\nSuggestions:\n\n1. We can try bumping up torch version to 2.7.0\n    v2.7.0 slightly different imports to check for triton - [has_triton_package()](https://github.com/pytorch/pytorch/blob/v2.7.0/torch/_inductor/runtime/hints.py#L38)\n    Implementation details for has_triton_package [here](https://github.com/pytorch/pytorch/blob/v2.7.0/torch/utils/_triton.py#L7)\n\n2. We can patch existing pytorch installation in vllm Dockerfile.ppc64le [[patch]](https://github.com/pytorch/pytorch/pull/147442/commits/386234e168ceec96108a48ece3cdbe6ff3d532c6)\n\ncc: @Isotr0py @youkaichao \n\n### \ud83d\udc1b Describe the bug\n\nStacktrace on container:\n\n```\nERROR 04-28 07:56:28 [registry.py:354] Error in inspecting model architecture 'GraniteForCausalLM'\nERROR 04-28 07:56:28 [registry.py:354] Traceback (most recent call last):\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/vllm/model_executor/models/registry.py\", line 586, in _run_in_subprocess\nERROR 04-28 07:56:28 [registry.py:354]     returned.check_returncode()\nERROR 04-28 07:56:28 [registry.py:354]   File \"/usr/lib64/python3.12/subprocess.py\", line 502, in check_returncode\nERROR 04-28 07:56:28 [registry.py:354]     raise CalledProcessError(self.returncode, self.args, self.stdout,\nERROR 04-28 07:56:28 [registry.py:354] subprocess.CalledProcessError: Command '['/opt/vllm/bin/python', '-m', 'vllm.model_executor.models.registry']' returned non-zero exit status 1.\nERROR 04-28 07:56:28 [registry.py:354]\nERROR 04-28 07:56:28 [registry.py:354] The above exception was the direct cause of the following exception:\nERROR 04-28 07:56:28 [registry.py:354]\nERROR 04-28 07:56:28 [registry.py:354] Traceback (most recent call last):\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/vllm/model_executor/models/registry.py\", line 352, in _try_inspect_model_cls\nERROR 04-28 07:56:28 [registry.py:354]     return model.inspect_model_cls()\nERROR 04-28 07:56:28 [registry.py:354]            ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/vllm/model_executor/models/registry.py\", line 323, in inspect_model_cls\nERROR 04-28 07:56:28 [registry.py:354]     return _run_in_subprocess(\nERROR 04-28 07:56:28 [registry.py:354]            ^^^^^^^^^^^^^^^^^^^\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/vllm/model_executor/models/registry.py\", line 589, in _run_in_subprocess\nERROR 04-28 07:56:28 [registry.py:354]     raise RuntimeError(f\"Error raised in subprocess:\\n\"\nERROR 04-28 07:56:28 [registry.py:354] RuntimeError: Error raised in subprocess:\nERROR 04-28 07:56:28 [registry.py:354] <frozen runpy>:128: RuntimeWarning: 'vllm.model_executor.models.registry' found in sys.modules after import of package 'vllm.model_executor.models', but prior to execution of 'vllm.model_executor.models.registry'; this may result in unpredictable behaviour\nERROR 04-28 07:56:28 [registry.py:354] Traceback (most recent call last):\nERROR 04-28 07:56:28 [registry.py:354]   File \"<frozen runpy>\", line 198, in _run_module_as_main\nERROR 04-28 07:56:28 [registry.py:354]   File \"<frozen runpy>\", line 88, in _run_code\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/vllm/model_executor/models/registry.py\", line 610, in <module>\nERROR 04-28 07:56:28 [registry.py:354]     _run()\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/vllm/model_executor/models/registry.py\", line 603, in _run\nERROR 04-28 07:56:28 [registry.py:354]     result = fn()\nERROR 04-28 07:56:28 [registry.py:354]              ^^^^\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/vllm/model_executor/models/registry.py\", line 324, in <lambda>\nERROR 04-28 07:56:28 [registry.py:354]     lambda: _ModelInfo.from_model_cls(self.load_model_cls()))\nERROR 04-28 07:56:28 [registry.py:354]                                       ^^^^^^^^^^^^^^^^^^^^^\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/vllm/model_executor/models/registry.py\", line 327, in load_model_cls\nERROR 04-28 07:56:28 [registry.py:354]     mod = importlib.import_module(self.module_name)\nERROR 04-28 07:56:28 [registry.py:354]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-28 07:56:28 [registry.py:354]   File \"/usr/lib64/python3.12/importlib/__init__.py\", line 90, in import_module\nERROR 04-28 07:56:28 [registry.py:354]     return _bootstrap._gcd_import(name[level:], package, level)\nERROR 04-28 07:56:28 [registry.py:354]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-28 07:56:28 [registry.py:354]   File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\nERROR 04-28 07:56:28 [registry.py:354]   File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\nERROR 04-28 07:56:28 [registry.py:354]   File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\nERROR 04-28 07:56:28 [registry.py:354]   File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\nERROR 04-28 07:56:28 [registry.py:354]   File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\nERROR 04-28 07:56:28 [registry.py:354]   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/vllm/model_executor/models/granite.py\", line 40, in <module>\nERROR 04-28 07:56:28 [registry.py:354]     from vllm.model_executor.layers.logits_processor import LogitsProcessor\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/vllm/model_executor/layers/logits_processor.py\", line 13, in <module>\nERROR 04-28 07:56:28 [registry.py:354]     from vllm.model_executor.layers.vocab_parallel_embedding import (\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 139, in <module>\nERROR 04-28 07:56:28 [registry.py:354]     @torch.compile(dynamic=True, backend=current_platform.simple_compile_backend)\nERROR 04-28 07:56:28 [registry.py:354]      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/torch/__init__.py\", line 2536, in fn\nERROR 04-28 07:56:28 [registry.py:354]     return compile(\nERROR 04-28 07:56:28 [registry.py:354]            ^^^^^^^^\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/torch/__init__.py\", line 2565, in compile\nERROR 04-28 07:56:28 [registry.py:354]     return torch._dynamo.optimize(\nERROR 04-28 07:56:28 [registry.py:354]            ^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 842, in optimize\nERROR 04-28 07:56:28 [registry.py:354]     return _optimize(rebuild_ctx, *args, **kwargs)\nERROR 04-28 07:56:28 [registry.py:354]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 896, in _optimize\nERROR 04-28 07:56:28 [registry.py:354]     backend = get_compiler_fn(backend)\nERROR 04-28 07:56:28 [registry.py:354]               ^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 783, in get_compiler_fn\nERROR 04-28 07:56:28 [registry.py:354]     from .repro.after_dynamo import wrap_backend_debug\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 16, in <module>\nERROR 04-28 07:56:28 [registry.py:354]     from torch._dynamo.debug_utils import (\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/torch/_dynamo/debug_utils.py\", line 25, in <module>\nERROR 04-28 07:56:28 [registry.py:354]     from torch._dynamo.testing import rand_strided\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/torch/_dynamo/testing.py\", line 27, in <module>\nERROR 04-28 07:56:28 [registry.py:354]     from torch._dynamo.backends.debugging import aot_eager\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/torch/_dynamo/backends/debugging.py\", line 10, in <module>\nERROR 04-28 07:56:28 [registry.py:354]     from functorch.compile import min_cut_rematerialization_partition\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/functorch/compile/__init__.py\", line 2, in <module>\nERROR 04-28 07:56:28 [registry.py:354]     from torch._functorch.aot_autograd import (\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 36, in <module>\nERROR 04-28 07:56:28 [registry.py:354]     from torch._inductor.output_code import OutputCode\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/torch/_inductor/output_code.py\", line 47, in <module>\nERROR 04-28 07:56:28 [registry.py:354]     from torch._inductor.cudagraph_utils import (\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/torch/_inductor/cudagraph_utils.py\", line 10, in <module>\nERROR 04-28 07:56:28 [registry.py:354]     from torch._inductor.utils import InputType\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/torch/_inductor/utils.py\", line 50, in <module>\nERROR 04-28 07:56:28 [registry.py:354]     from torch._inductor.runtime.hints import DeviceProperties\nERROR 04-28 07:56:28 [registry.py:354]   File \"/opt/vllm/lib64/python3.12/site-packages/torch/_inductor/runtime/hints.py\", line 67, in <module>\nERROR 04-28 07:56:28 [registry.py:354]     from triton.compiler.compiler import AttrsDescriptor\nERROR 04-28 07:56:28 [registry.py:354] ModuleNotFoundError: No module named 'triton.compiler'; 'triton' is not a package\nERROR 04-28 07:56:28 [registry.py:354]\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 1130, in <module>\n    uvloop.run(run_server(args))\n  File \"/opt/vllm/lib64/python3.12/site-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib64/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib64/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/opt/vllm/lib64/python3.12/site-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 1078, in run_server\n    async with build_async_engine_client(args) as engine_client:\n  File \"/usr/lib64/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 146, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/usr/lib64/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 166, in build_async_engine_client_from_engine_args\n    vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/engine/arg_utils.py\", line 1112, in create_engine_config\n    model_config = self.create_model_config()\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/engine/arg_utils.py\", line 1000, in create_model_config\n    return ModelConfig(\n           ^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/config.py\", line 516, in __init__\n    self.multimodal_config = self._init_multimodal_config(\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/config.py\", line 585, in _init_multimodal_config\n    if self.registry.is_multimodal_model(self.architectures):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/model_executor/models/registry.py\", line 504, in is_multimodal_model\n    model_cls, _ = self.inspect_model_cls(architectures)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/model_executor/models/registry.py\", line 464, in inspect_model_cls\n    return self._raise_for_unsupported(architectures)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/lib64/python3.12/site-packages/vllm/model_executor/models/registry.py\", line 414, in _raise_for_unsupported\n    raise ValueError(\nValueError: Model architectures ['GraniteForCausalLM'] failed to be inspected. Please check the logs for more details\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "closed",
    "created_at": "2025-04-28T14:18:25+00:00",
    "closed_at": "2025-05-02T07:45:02+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17309/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/17309"
  },
  {
    "number": 16150,
    "title": "[Bug]: Error When Launching Llama-4-Scout-17B-16E-Instruct Without `--kv-cache-dtype fp8`",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 04-07 11:13:31 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\n/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.28.1\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-94-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.3.107\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA H100 80GB HBM3\nGPU 1: NVIDIA H100 80GB HBM3\nGPU 2: NVIDIA H100 80GB HBM3\nGPU 3: NVIDIA H100 80GB HBM3\nGPU 4: NVIDIA H100 80GB HBM3\nGPU 5: NVIDIA H100 80GB HBM3\nGPU 6: NVIDIA H100 80GB HBM3\nGPU 7: NVIDIA H100 80GB HBM3\n\nNvidia driver version: 535.129.03\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      52 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             192\nOn-line CPU(s) list:                0-191\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) Platinum 8468V\nCPU family:                         6\nModel:                              143\nThread(s) per core:                 2\nCore(s) per socket:                 48\nSocket(s):                          2\nStepping:                           8\nCPU max MHz:                        3800.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           4800.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                     VT-x\nL1d cache:                          4.5 MiB (96 instances)\nL1i cache:                          3 MiB (96 instances)\nL2 cache:                           192 MiB (96 instances)\nL3 cache:                           195 MiB (2 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0-47,96-143\nNUMA node1 CPU(s):                  48-95,144-191\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-dali-cuda120==1.34.0\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] nvidia-pyindex==1.0.9\n[pip3] onnx==1.15.0rc2\n[pip3] optree==0.15.0\n[pip3] pynvml==11.4.1\n[pip3] pytorch-quantization==2.1.2\n[pip3] pyzmq==25.1.2\n[pip3] torch==2.6.0\n[pip3] torch-tensorrt==2.3.0a0\n[pip3] torchaudio==2.6.0\n[pip3] torchdata==0.7.1a0\n[pip3] torchtext==0.17.0a0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.0\n[pip3] triton==3.2.0\n[conda] No relevant packages\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3\nvLLM Build Flags:\nCUDA Archs: 5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     PIX     SYS     SYS     0-47,96-143     0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     PIX     SYS     48-95,144-191   1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     48-95,144-191   1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     PIX     48-95,144-191   1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     48-95,144-191   1               N/A\nNIC0    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS\nNIC1    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS\nNIC2    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS      X      SYS\nNIC3    SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n\nNVIDIA_VISIBLE_DEVICES=GPU-3a6d9eec-13d8-bfca-3668-204ab09379ef,GPU-ab44c55b-99b1-5821-d382-150b233fb39f,GPU-080dc61b-a2d8-4a88-e73f-c9d1f7d30a95,GPU-06cb9a7c-4cc5-3d1f-66a8-7e6c621ae217,GPU-5071dd6b-acd5-ed2a-009b-141e51d820eb,GPU-4cd47a10-6313-7393-37e7-7367f08bc018,GPU-6a1fb276-f409-db79-f944-53dfb0f7ddc6,GPU-e4ed4b90-5883-a84c-e913-8f4bf558b85a\nCUBLAS_VERSION=12.3.4.1\nNVIDIA_REQUIRE_CUDA=cuda>=9.0\nCUDA_CACHE_DISABLE=1\nTORCH_CUDA_ARCH_LIST=5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX\nNCCL_VERSION=2.19.stable.20231214+cuda12.3\nNVIDIA_DRIVER_CAPABILITIES=compute,utility,video\nVLLM_WORKER_MULTIPROC_METHOD=spawn\nNVIDIA_PRODUCT_NAME=PyTorch\nCUDA_VERSION=\nPYTORCH_VERSION=2.3.0a0+ebedce2\nPYTORCH_BUILD_NUMBER=0\nMAX_JOBS=64\nCUDNN_VERSION=9.0.0.306\nPYTORCH_HOME=/opt/pytorch/pytorch\nLD_LIBRARY_PATH=/usr/local/cuda/compat/lib.real:/usr/local/lib/python3.10/dist-packages/torch/lib:/usr/local/lib/python3.10/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNVIDIA_BUILD_ID=82611821\nCUDA_DRIVER_VERSION=545.23.08\nPYTORCH_BUILD_VERSION=2.3.0a0+ebedce2\nCUDA_HOME=/usr/local/cuda\nCUDA_HOME=/usr/local/cuda\nCUDA_MODULE_LOADING=LAZY\nNVIDIA_REQUIRE_JETPACK_HOST_MOUNTS=\nNVIDIA_PYTORCH_VERSION=24.02\nTORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen attempting to launch the vLLM server using the following command from the documentation, I encountered an error:\n```bash\nvllm serve meta-llama/Llama-4-Scout-17B-16E-Instruct -tp 8 --max-model-len 128000 --override-generation-config='{\"attn_temperature_tuning\": true}' --load-format runai_streamer\n```\n\nError Logs:\n```\nERROR 04-07 10:56:15 [core.py:390] EngineCore hit an exception: Traceback (most recent call last):\nERROR 04-07 10:56:15 [core.py:390]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py\", line 378, in run_engine_core\nERROR 04-07 10:56:15 [core.py:390]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 04-07 10:56:15 [core.py:390]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py\", line 319, in __init__\nERROR 04-07 10:56:15 [core.py:390]     super().__init__(vllm_config, executor_class, log_stats)\nERROR 04-07 10:56:15 [core.py:390]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py\", line 71, in __init__\nERROR 04-07 10:56:15 [core.py:390]     self._initialize_kv_caches(vllm_config)\nERROR 04-07 10:56:15 [core.py:390]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py\", line 132, in _initialize_kv_caches\nERROR 04-07 10:56:15 [core.py:390]     available_gpu_memory = self.model_executor.determine_available_memory()\nERROR 04-07 10:56:15 [core.py:390]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/executor/abstract.py\", line 66, in determine_available_memory\nERROR 04-07 10:56:15 [core.py:390]     output = self.collective_rpc(\"determine_available_memory\")\nERROR 04-07 10:56:15 [core.py:390]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 134, in collective_rpc\nERROR 04-07 10:56:15 [core.py:390]     raise e\nERROR 04-07 10:56:15 [core.py:390]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 123, in collective_rpc\nERROR 04-07 10:56:15 [core.py:390]     raise result\nERROR 04-07 10:56:15 [core.py:390] ValueError: too many values to unpack (expected 12)\nERROR 04-07 10:56:15 [core.py:390] \n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py\", line 387, in __call__\n(VllmWorker rank=1 pid=9198) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     return self.current_callable(inputs)\n(VllmWorker rank=3 pid=9324) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/output_code.py\", line 466, in __call__\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n(VllmWorker rank=1 pid=9198) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/root/.cache/vllm/torch_compile_cache/ea2d23402a/rank_1_0/inductor_cache/7a/c7anr47kmmdiy3qutmy34yk4g6ninrz4eu4dl2k46yjetwpxxcyh.py\", line 370, in call\n(VllmWorker rank=3 pid=9324) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     return self.current_callable(inputs)\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n(VllmWorker rank=1 pid=9198) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1 = args\n(VllmWorker rank=3 pid=9324) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/root/.cache/vllm/torch_compile_cache/ea2d23402a/rank_3_0/inductor_cache/js/cjs2yfrbs7n6dixeibsukc7xrpinqgfbc76l4ckdkz77iy4glynb.py\", line 370, in call\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     return self._call_impl(*args, **kwargs)\n(VllmWorker rank=1 pid=9198) ERROR 04-07 10:56:15 [multiproc_executor.py:383] ValueError: too many values to unpack (expected 12)\n(VllmWorker rank=3 pid=9324) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1 = args\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n(VllmWorker rank=3 pid=9324) ERROR 04-07 10:56:15 [multiproc_executor.py:383] ValueError: too many values to unpack (expected 12)\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     submod_6 = self.submod_6(getitem_12, s0, \n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     return forward_call(*args, **kwargs)\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"<eval_with_key>.98\", line 461, in forward\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     submod_6 = self.submod_6(getitem_12, s0, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_, getitem_13, l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_feed_forward_modules_router_parameters_weight_, l_self_modules_layers_modules_2_modules_feed_forward_modules_shared_expert_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_feed_forward_modules_shared_expert_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_feed_forward_modules_experts_parameters_w13_weight_, l_self_modules_layers_modules_2_modules_feed_forward_modules_experts_parameters_w2_weight_, l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_, l_positions_);  getitem_12 = l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_ = getitem_13 = l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_feed_forward_modules_router_parameters_weight_ = l_self_modules_layers_modules_2_modules_feed_forward_modules_shared_expert_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_feed_forward_modules_shared_expert_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_feed_forward_modules_experts_parameters_w13_weight_ = l_self_modules_layers_modules_2_modules_feed_forward_modules_experts_parameters_w2_weight_ = l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_ = None\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/usr/local/lib/python3.10/dist-packages/vllm/compilation/backends.py\", line 608, in __call__\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     return self.compiled_graph_for_general_shape(*args)\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/usr/local/lib/python3.10/dist-packages/vllm/compilation/compiler_interface.py\", line 357, in compiled_graph\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     graph_output = inductor_compiled_graph(list_args)\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/output_code.py\", line 466, in __call__\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     return self.current_callable(inputs)\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]   File \"/root/.cache/vllm/torch_compile_cache/ea2d23402a/rank_5_0/inductor_cache/bx/cbxnsxbxmoiuzf32l5mxinvqc2wu2ex6u5o76dkfslsyn6wabunx.py\", line 370, in call\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383]     arg0_1, arg1_1, arg2_1, arg3_1, arg4_1, arg5_1, arg6_1, arg7_1, arg8_1, arg9_1, arg10_1, arg11_1 = args\n(VllmWorker rank=5 pid=9469) ERROR 04-07 10:56:15 [multiproc_executor.py:383] ValueError: too many values to unpack (expected 12)\n(VllmWorker rank=2 pid=9252) ERROR 04-07 10:56:15 [multiproc_executor.py:383] WorkerProc hit an exception: %s\nCRITICAL 04-07 10:56:15 [core_client.py:361] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\nKilled\n```\n\nBut after I add `--kv-cache-dtype fp8`, it seems to return to normal.\n```bash\nvllm serve meta-llama/Llama-4-Scout-17B-16E-Instruct -tp 8 --max-model-len 128000 --override-generation-config='{\"attn_temperature_tuning\": true}' --kv-cache-dtype fp8 --load-format runai_streamer\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "closed",
    "created_at": "2025-04-07T03:33:28+00:00",
    "closed_at": "2025-04-15T06:11:13+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16150/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16150"
  },
  {
    "number": 15896,
    "title": "[Feature]: Enable CUDA Graph without turn on torch.compile / Inductor for V1",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nFor simple models, we may not need fusion from torch.compile. And piecewise approach may be slow. So we would like to enable this feature.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "torch.compile"
    ],
    "state": "closed",
    "created_at": "2025-04-01T17:19:26+00:00",
    "closed_at": "2025-05-29T02:16:53+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15896/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15896"
  },
  {
    "number": 15592,
    "title": "[Bug]:ModuleNotFoundError: No module named 'vllm._C' ",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\n(vllm3) [root@hygpu-002 envs]# python vllm/collect_env.py \n/mnt/qy-test/envs/vllm/vllm/__init__.py:5: RuntimeWarning: Failed to read commit hash:\nNo module named 'vllm._version'\n  from .version import __version__, __version_tuple__  # isort:skip\nINFO 03-27 10:36:47 [__init__.py:239] Automatically detected platform cuda.\nTraceback (most recent call last):\n  File \"/mnt/qy-test/envs/vllm/collect_env.py\", line 17, in <module>\n    from vllm.envs import environment_variables\n  File \"/mnt/qy-test/envs/vllm/vllm/__init__.py\", line 11, in <module>\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\n  File \"/mnt/qy-test/envs/vllm/vllm/engine/arg_utils.py\", line 22, in <module>\n    from vllm.executor.executor_base import ExecutorBase\n  File \"/mnt/qy-test/envs/vllm/vllm/executor/executor_base.py\", line 16, in <module>\n    from vllm.model_executor.layers.sampler import SamplerOutput\n  File \"/mnt/qy-test/envs/vllm/vllm/model_executor/layers/sampler.py\", line 23, in <module>\n    from vllm.spec_decode.metrics import SpecDecodeWorkerMetrics\n  File \"/mnt/qy-test/envs/vllm/vllm/spec_decode/metrics.py\", line 9, in <module>\n    from vllm.model_executor.layers.spec_decode_base_sampler import (\n  File \"/mnt/qy-test/envs/vllm/vllm/model_executor/layers/spec_decode_base_sampler.py\", line 10, in <module>\n    from vllm.platforms import current_platform\n  File \"/mnt/qy-test/envs/vllm/vllm/platforms/__init__.py\", line 271, in __getattr__\n    _current_platform = resolve_obj_by_qualname(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/qy-test/envs/vllm/vllm/utils.py\", line 1910, in resolve_obj_by_qualname\n    module = importlib.import_module(module_name)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/qy-test/envs/conda/envs/vllm3/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/qy-test/envs/vllm/vllm/platforms/cuda.py\", line 15, in <module>\n    import vllm._C  # noqa\n    ^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'vllm._C'\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen I tried to run the collect_env.py file, I encountered the error:\n\nModuleNotFoundError: No module named 'vllm._C'  \n\nAdditionally, I downloaded a .whl file to install the environment using the command:\n\npip install vllm-0.8.3.dev166%2Bg29930428e.cu128-cp312-cp312-linux_x86_64.whl  \n\nThe installation completed without any errors, but when I attempted to deploy a local model, the following command:\n\npython -m vllm.entrypoints.openai.api_server --model /mnt/qy-test/pvc-cd37befa-daf6-42c1-80f5-70b54cbbf302/qwen-QwQ --trust-remote-code --host 0.0.0.0 --port 8080 \n\nresulted in an error\uff1a\n\nINFO 03-27 10:47:18 [__init__.py:239] Automatically detected platform cuda.\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 189, in _run_module_as_main\n  File \"<frozen runpy>\", line 112, in _get_module_details\n  File \"/mnt/qy-test/envs/conda/envs/vllm3/lib/python3.12/site-packages/vllm/__init__.py\", line 11, in <module>\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\n  File \"/mnt/qy-test/envs/conda/envs/vllm3/lib/python3.12/site-packages/vllm/engine/arg_utils.py\", line 22, in <module>\n    from vllm.executor.executor_base import ExecutorBase\n  File \"/mnt/qy-test/envs/conda/envs/vllm3/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 16, in <module>\n    from vllm.model_executor.layers.sampler import SamplerOutput\n  File \"/mnt/qy-test/envs/conda/envs/vllm3/lib/python3.12/site-packages/vllm/model_executor/layers/sampler.py\", line 23, in <module>\n    from vllm.spec_decode.metrics import SpecDecodeWorkerMetrics\n  File \"/mnt/qy-test/envs/conda/envs/vllm3/lib/python3.12/site-packages/vllm/spec_decode/metrics.py\", line 9, in <module>\n    from vllm.model_executor.layers.spec_decode_base_sampler import (\n  File \"/mnt/qy-test/envs/conda/envs/vllm3/lib/python3.12/site-packages/vllm/model_executor/layers/spec_decode_base_sampler.py\", line 10, in <module>\n    from vllm.platforms import current_platform\n  File \"/mnt/qy-test/envs/conda/envs/vllm3/lib/python3.12/site-packages/vllm/platforms/__init__.py\", line 271, in __getattr__\n    _current_platform = resolve_obj_by_qualname(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/qy-test/envs/conda/envs/vllm3/lib/python3.12/site-packages/vllm/utils.py\", line 1905, in resolve_obj_by_qualname\n    module = importlib.import_module(module_name)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/qy-test/envs/conda/envs/vllm3/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/qy-test/envs/conda/envs/vllm3/lib/python3.12/site-packages/vllm/platforms/cuda.py\", line 15, in <module>\n    import vllm._C  # noqa\n    ^^^^^^^^^^^^^^\nImportError: /lib64/libc.so.6: version `GLIBC_2.32' not found (required by /mnt/qy-test/envs/conda/envs/vllm3/lib/python3.12/site-packages/vllm/_C.abi3.so)\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "closed",
    "created_at": "2025-03-27T02:52:34+00:00",
    "closed_at": "2025-05-28T17:19:27+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15592/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15592"
  },
  {
    "number": 10945,
    "title": "[Performance]: V1 CudaGrpah",
    "body": "### Proposal to improve performance\n\n_No response_\n\n### Report of performance regression\n\n_No response_\n\n### Misc discussion on performance\n\nI have ported the vllm code to my TTS model, using llama for autoregressive token generation, and I am using version v0.2.7. I noticed that during the decode step, using the `torch.cuda.CUDAGraph().replay()` method, my inference speed has increased to 6 times the original. I observed that version V1 does not use `torch.cuda.CUDAGraph()`, and upon testing, I found that setting `VLLM_TORCH_COMPILE_LEVEL=3` not only fails to achieve a 6-fold increase in inference speed but also slows down the process, with a significant increase in the time taken by RMSNorm. Are there any suggestions to help me modify the code?\r\n\n\n### Your current environment (if you think it is necessary)\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance",
      "torch.compile",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-06T06:33:54+00:00",
    "closed_at": "2025-04-15T02:08:08+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10945/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10945"
  },
  {
    "number": 7315,
    "title": "[Feature]: Support attention backend with FlexAttention",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\r\n\r\nFlexAttention was proposed as a performant attention implementation leveraging `torch.compile` with easy APIs for adding support for complex attention variants such as Causal, [Relative Positional Embeddings](https://paperswithcode.com/method/relative-position-encodings), [Alibi](https://paperswithcode.com/method/alibi), [Sliding Window Attention](https://mistral.ai/news/announcing-mistral-7b/), [PrefixLM](https://twitter.com/andersonbcdefg/status/1800907703688339569), [Document Masking/Sample Packing/Jagged Tensors](https://github.com/pytorch/torchtune/pull/875), [Tanh Soft-Capping](https://twitter.com/LysandreJik/status/1807779471891538199), [PagedAttention](https://arxiv.org/abs/2309.06180), etc.\r\n\r\nhttps://pytorch.org/blog/flexattention/\r\n\r\nWhile it is not the fastest attention backend (yet!) it is clearly performant enough while enabling much more flexibility than current compiled backends to easily implement attention features we need for crucial models, like Soft-capping in Gemma 2 which we currently rely on FlashInfer for. Not to mention it is a first-class citizen for `torch.compile`.\r\n\r\n**The current blocker is it will not be available until PyTorch 2.5.0.**\r\n\r\n![image](https://github.com/user-attachments/assets/4e508c4c-1b80-4b97-b8b6-89f3db6b1639)\r\n\r\n<img src=\"https://github.com/user-attachments/assets/f1558be9-e0dc-4beb-899b-4eddc29a8b03\" width=\"400\" />\r\n\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\n_No response_",
    "labels": [
      "feature request",
      "torch.compile",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-08T19:52:26+00:00",
    "closed_at": "2025-02-14T01:59:26+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7315/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/7315"
  },
  {
    "number": 20451,
    "title": "[RFC]: vLLM-compile low-hanging fruit cold start improvements",
    "body": "### Motivation.\n\nThis issue tracks potential low-hanging fruit for improving vLLM-compile cold start time. @anijain2305, @BoyuanFeng, and I sat down to look at some traces and noticed some things we can improve.\n\nThere are more longer-term projects for improving torch.compile cold start time, but those will probably take a bit to hit.\n\n### Proposed Change.\n\n- [ ] vLLM's [custom bytecode hook](https://github.com/vllm-project/vllm/blob/536fd330036b0406786c847f68e4f67cba06f421/vllm/compilation/wrapper.py#L77-L121) seems to take a long time (~7 seconds on llama-3.1-70b model). I'm not sure how much of this is actually needed for runtime execution. We should guard the decompilation step behind an envvar. If VLLM_COMPILE_DEPYF=0 (default), we write out a `transformed_code.py` that has a comment that says \"Please set VLLM_COMPILE_DEPYF=1 to populate this file\".\n- [ ] In llama-3.1-70b, with piecewise cudagraphs, we split a module into 80 different subgraphs. A lot of these subgraphs are literally the same. However, subgraphs 2-79 (approx) are cache-hitting in fx_graph_cache, but they are cache missing in AOTAutogradCache. This needs some more investigation as to why they are cache missing there.\n\n### Feedback Period.\n\n7/2-7/11, but really, anytime until these things are fixed.\n\n### CC List.\n\ncc @ProExpertProg @youkaichao @WoosukKwon @jamesjwu @zhxchen17\n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "torch.compile",
      "startup-ux"
    ],
    "state": "open",
    "created_at": "2025-07-03T19:22:18+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20451/reactions",
      "total_count": 4,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 2,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20451"
  },
  {
    "number": 20402,
    "title": "[RFC]: vLLM-compile (minus cudagraphs) warm-start time should be close to zero",
    "body": "### Motivation.\n\n@BoyuanFeng did some benchmarks of vLLM cold vs warm start of a 70B model. In the warm start, compilation (ignoring cudagraphs) took 25 out of 132 seconds, almost 20% of the time. On warm start, all of the hard work (compiling artifacts) should have been already done.\n\nThe theoretical minimum amount of time that vLLM-compile needs to spend in warm start is the amount of time it takes to load all the compiled code.\n\n![Image](https://github.com/user-attachments/assets/b34204f8-5ad5-49d4-bdc6-6805610ac6be)\n\n### Proposed Change.\n\nThe following categories correspond to what is in the chart above.\n\nDynamo:\n- On warm start, vLLM always re-runs Dynamo. We don't need to do this: instead, we can directly serialize the bytecode that Dynamo produces and re-load it.\n- Originally I was planning on waiting until torch.compile implemented \"precompilation\", which will skip Dynamo on warm start. It might be worth figuring out how to get a simpler version of this into vLLM, especially because \"precompilation\" in torch is still a bit away. vLLM just needs to serialize the Dynamo-produced bytecode; we don't care about graph breaks or guards.\n\nInductor:\n- TL;DR: vLLM is doing some compute on loading the compiled artifact. It shouldn't need to do this compute. We should be able to fix this in vLLM\n- Details: With piecewise cudagraphs, there are N compiled artifacts. The way vLLM loads the compiled artifacts is that we do a full forward-pass through the model, using FakeTensors. When the forward pass hits one of these \"missing compiled artifacts\", then it loads it from disk.\n- We don't need to run the full forward pass. The full forward pass on FakeTensors is slow. it should be possible to record all of the compiled artifacts we need to load and just load them all together and construct the right objects for runtime.\n\nOther: this needs some more investigation.\n\n### Feedback Period.\n\n7/2 - 7/11\n\n### CC List.\n\n@ProExpertProg @youkaichao @WoosukKwon @robertgshaw2-redhat @jamesjwu @zhxchen17\n\n### Any Other Things.\n\nthank you\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "torch.compile",
      "startup-ux"
    ],
    "state": "open",
    "created_at": "2025-07-02T22:04:20+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20402/reactions",
      "total_count": 5,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20402"
  },
  {
    "number": 20394,
    "title": "[RFC][UX]: debug mode for vLLM-compile",
    "body": "### Motivation.\n\nvLLM-compile (CompilationLevel.PIECEWISE) makes a lot of assumptions about the models that allow it to make them run really fast. There are two main assumptions that commonly lead to silent incorrectness if the models violate them. I've spent countless hours debugging user issues for it to turn out to be one of these assumptions. We should add a debug mode option for vLLM-compile that, when turned on, adds some safety checks for these assumptions at the tradeoff of some additional overhead. This will let users self-diagnose the issues without me in the loop.\n\nThis is one of the items mentioned in https://github.com/vllm-project/vllm/issues/20283, I'm expanding it to include some more details.\n\n### Proposed Change.\n\nThe two assumptions that bite us are:\n1) the [vLLM Dynamic Shapes Issue](https://docs.google.com/document/d/1R3XvVEpJeVi3whyxf4xpyZufGplbrfw628oXLZ6fqG0/edit?tab=t.0#heading=h.59xosv6nz9lg). vLLM performs one single graph capture with dynamic batch size and expects the graph to work for all batch sizes. However, the graph may not actually be valid for all batch sizes. \n2) CUDAGraphs assume that the input addresses of Tensors do not change. Changing the input addresses (e.g. doing model.weight1 = new_weight1) violates the assumption.\n\nWe should add an option to have some additional \"safety checks\" in CompilationConfig. Here are the safety checks that we would add.\n\n#### For the vLLM Dynamic Shapes Issue\n\ntorch.compile produces a symbolic expression about what batch sizes the captured graph is valid for. These might look like \"size < 60000 && size % 128 == 0\". At runtime, when we execute the vLLM model, we should validate that the batch size actually passes the symbolic expression.\n\n#### For CUDAGraphs input addresses\n\nWe know all of the inputs being cudagraph'ed. We can record their input addresses, and, at runtime, always check that the input addresses did not change.\n\n### Feedback Period.\n\n7/2 - 7/11\n\n### CC List.\n\n@ProExpertProg @youkaichao @mgoin @robertgshaw2-redhat @WoosukKwon @drisspg @houseroad \n\n### Any Other Things.\n\nthank you\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "torch.compile"
    ],
    "state": "open",
    "created_at": "2025-07-02T17:56:56+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20394/reactions",
      "total_count": 3,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20394"
  },
  {
    "number": 20342,
    "title": "[Bug]: V1 pre-compiled graph loading much slower than V0",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : 19.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.4.1 25184 c87081df219c42dc27c5b6d86c0525bc7d01f727)\nCMake version                : version 3.31.6\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+gitf717b2a\nIs debug build               : False\nCUDA used to build PyTorch   : N/A\nROCM used to build PyTorch   : 6.4.43483-a187df25c\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0] (64-bit runtime)\nPython platform              : Linux-6.8.0-52-generic-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : Could not collect\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : AMD Instinct MI250X/MI250 (gfx90a:sramecc+:xnack-)\nNvidia driver version        : Could not collect\ncuDNN version                : Could not collect\nHIP runtime version          : 6.4.43483\nMIOpen runtime version       : 3.4.0\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               128\nOn-line CPU(s) list:                  0-127\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 7713 64-Core Processor\nCPU family:                           25\nModel:                                1\nThread(s) per core:                   1\nCore(s) per socket:                   64\nSocket(s):                            2\nStepping:                             1\nFrequency boost:                      enabled\nCPU max MHz:                          3720.7029\nCPU min MHz:                          1500.0000\nBogoMIPS:                             3992.52\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm debug_swap\nVirtualization:                       AMD-V\nL1d cache:                            4 MiB (128 instances)\nL1i cache:                            4 MiB (128 instances)\nL2 cache:                             64 MiB (128 instances)\nL3 cache:                             512 MiB (16 instances)\nNUMA node(s):                         8\nNUMA node0 CPU(s):                    0-15\nNUMA node1 CPU(s):                    16-31\nNUMA node2 CPU(s):                    32-47\nNUMA node3 CPU(s):                    48-63\nNUMA node4 CPU(s):                    64-79\nNUMA node5 CPU(s):                    80-95\nNUMA node6 CPU(s):                    96-111\nNUMA node7 CPU(s):                    112-127\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.2.6\n[pip3] pyzmq==27.0.0\n[pip3] torch==2.7.0+gitf717b2a\n[pip3] torchvision==0.21.0+7af6987\n[pip3] transformers==4.53.0\n[pip3] triton==3.2.0+gite5be006a\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : 6.4.43483-a187df25c\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.1.dev7354+g16f89f4.d20250628 (git sha: 16f89f4, date: 20250628)\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n  ============================ ROCm System Management Interface ============================\n================================ Weight between two GPUs =================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            15           15           30           30           30           15           30\nGPU1   15           0            30           15           30           15           30           45\nGPU2   15           30           0            15           15           30           30           30\nGPU3   30           15           15           0            30           45           30           15\nGPU4   30           30           15           30           0            15           15           30\nGPU5   30           15           30           45           15           0            30           15\nGPU6   15           30           30           30           15           30           0            15\nGPU7   30           45           30           15           30           15           15           0\n\n================================= Hops between two GPUs ==================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            1            1            1            1            1            1            1\nGPU1   1            0            1            1            1            1            1            1\nGPU2   1            1            0            1            1            1            1            1\nGPU3   1            1            1            0            1            1            1            1\nGPU4   1            1            1            1            0            1            1            1\nGPU5   1            1            1            1            1            0            1            1\nGPU6   1            1            1            1            1            1            0            1\nGPU7   1            1            1            1            1            1            1            0\n\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI\nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI\nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI\nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI\nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI\nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0\n\n======================================= Numa Nodes =======================================\nGPU[0]          : (Topology) Numa Node: 3\nGPU[0]          : (Topology) Numa Affinity: 3\nGPU[1]          : (Topology) Numa Node: 3\nGPU[1]          : (Topology) Numa Affinity: 3\nGPU[2]          : (Topology) Numa Node: 2\nGPU[2]          : (Topology) Numa Affinity: 2\nGPU[3]          : (Topology) Numa Node: 2\nGPU[3]          : (Topology) Numa Affinity: 2\nGPU[4]          : (Topology) Numa Node: 7\nGPU[4]          : (Topology) Numa Affinity: 7\nGPU[5]          : (Topology) Numa Node: 7\nGPU[5]          : (Topology) Numa Affinity: 7\nGPU[6]          : (Topology) Numa Node: 6\nGPU[6]          : (Topology) Numa Affinity: 6\nGPU[7]          : (Topology) Numa Node: 6\nGPU[7]          : (Topology) Numa Affinity: 6\n================================== End of ROCm SMI Log ===================================\n\n==============================\n     Environment Variables\n==============================\nPYTORCH_ROCM_ARCH=gfx90a;gfx942\nLD_LIBRARY_PATH=/opt/rocm/lib:/usr/local/lib:\nPYTORCH_TUNABLEOP_FILENAME=/app/afo_tune_device_%d_full.csv\nPYTORCH_TUNABLEOP_TUNING=0\nPYTORCH_TUNABLEOP_ENABLED=1\nVLLM_TARGET_DEVICE=rocm\nVERBOSE=1\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen using V0, loading compilation artifacts typically takes single-digit seconds. However, with V1 it takes around 30x longer, e.g. 70s for the graph for a single shape for a larger model. This can be tested by observing the compilation graph load times when running the same command a 2nd time (allowing vLLM to load the pre-compiled graphs from the cache rather than compiling from scratch):\n```bash\nexport VLLM_USE_V1=1\nMODEL_NAME=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\necho \"Running with compilation config \"\npython3 -m vllm.entrypoints.openai.api_server \\\n  --port 8080 \\\n  --model $MODEL_NAME \\\n  --served-model-name $MODEL_NAME \\\n  --gpu-memory-utilization 0.95 \\\n  --disable-custom-all-reduce \\\n  --tensor-parallel-size 1 \\\n  --enable-chunked-prefill \\\n  --disable-log-requests \\\n  --enable-reasoning \\\n  --compilation-config '{\"compile_sizes\": [1], \"level\": 3, \"cudagraph_capture_sizes\": [256, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 144, 136, 16, 152, 24, 128, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248, 248]}' \\\n  --reasoning-parser deepseek_r1\n```\n\nIf one sets `export VLLM_USE_V1=0` and re-runs the above twice, it will load the compilation artifacts much faster. From turning debugging logs on, one can see that V1 loads 28x times more graphs compared to V0, which explains why it's so slow to initialize. \n\nV0:\n```bash\nDEBUG 07-01 21:11:24 [backends.py:123] Directly load the 0-th graph for shape 1 from inductor via handle ('fn7nwql5utlkjsob3xnghodbg4wpmfszfp4t5rbntqa4tjau3wp5', '/root/.cache/vllm/torch_compile_cache/469366c8cf/rank_0_0/inductor_cache/x3/cx37k6oulv3pi5qbdgh4xvj5h2emyzprlhb36dgljhlhfckglrsp.py')\nINFO 07-01 21:11:24 [backends.py:167] Directly load the compiled graph(s) for shape 1 from the cache, took 4.428 s\n```\n\nV1:\n```bash\nDEBUG 07-01 21:20:33 [backends.py:151] Runtime shape: 1\nDEBUG 07-01 21:20:33 [backends.py:123] Directly load the 28-th graph for shape 1 from inductor via handle ('fhu67rrnvc35buqntqu3cckyijpmigkuhnw2f4czeby4womoyu74', '/root/.cache/vllm/torch_compile_cache/250015fb60/rank_0_0/inductor_cache/cd/ccdqxkqebrqnhlnyzlmblhaolao245amjndrrhp2akftnnmo6nki.py')\nINFO 07-01 21:20:33 [backends.py:155] Successfully loaded graph 28\n```\n\nCompared to V0, compilation gives similar speedups compared to vanilla V1 for a given batch size, which suggests to me that the extra loaded graphs are not necessary for performance, although this has not been tested.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "open",
    "created_at": "2025-07-01T23:47:53+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20342/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20342"
  },
  {
    "number": 19579,
    "title": "[Bug]: V1 piecewise cudagraph capture size on ROCm is much higher than on cuda",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nROCM Version                 : 6.3.42133-1b9c17779\nvLLM Version                 : 0.9.1.dev325+g9d880f594 (git sha: 9d880f594)\nPYTORCH_TUNABLEOP_TUNING=0\nPYTORCH_TUNABLEOP_ENABLED=1\nPYTORCH_ROCM_ARCH=gfx942\nLD_LIBRARY_PATH=/opt/rocm/lib:/usr/local/lib:\nPYTORCH_TUNABLEOP_FILENAME=/app/afo_tune_device_%d_full.csv\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n</details>\n\n\n\n\n\n### \ud83d\udc1b Describe the bug\n\nThe size of piecewise cudagraph is much higher on rocm (mi300) than on cuda (h100). See table below. Also, this issue seems to be specific to piecewise capture; when doing a fullgraph capture on rocm, the graph size is fine.\n\n**Note**: The issue is Not related to rccl/all_reduce etc. because the captured sizes below use TP=1\n\n#### Instructions to reproduce the issue:\nEngine init logs contain the graph captured size. e.g: \n`VLLM_USE_V1=1 python examples/offline_inference/basic/generate.py`\n```\nINFO 06-12 19:49:27 [gpu_model_runner.py:2051] Graph capturing finished in 38 secs, took 6.32 GiB\n```\n\n| Model (V1 engine)   | rocm      | cuda     |\n|---------------------|-----------|----------|\n| Llama-2-7b-hf       | 2.97 GiB  | 0.61 GiB | \n| Llama-2-70b-hf      | 6.32 GiB  | 1.35 GiB |  \n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "open",
    "created_at": "2025-06-12T20:55:01+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19579/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19579"
  },
  {
    "number": 19504,
    "title": "[Bug]: Qwen3-GPTQ | Error in inspecting model architecture 'Qwen3MoeForCausalLM'",
    "body": "### Your current environment\n\n **VLLM v 0.9.0.1**\n\n\n### \ud83d\udc1b Describe the bug\n\nI am using docker image with **VLLM v 0.9.0.1**\nI have download the model [`Qwen/Qwen3-235B-A22B-GPTQ-Int4`] at this directory `qwen3-gptq`: \n\nI have a node with 8 H100 GPUs\n`VLLM_USE_V1=0 vllm serve qwen3-gptq   --tensor-parallel-size 8  --max-model-len 32000   --gpu-memory-utilization 0.9   --distributed-executor-backend mp `\n\n\nI have this error \n```\nINFO 06-01 11:19:03 [__init__.py:243] Automatically detected platform cuda.\nINFO 06-01 11:19:24 [__init__.py:31] Available plugins for group vllm.general_plugins:\nINFO 06-01 11:19:24 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\nINFO 06-01 11:19:24 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n\ninit_-py:36] All plugins in this group will be loaded. Set \"VLLM_PLUGINS' to control which plugins to load.\n[registry-py: 363] Error in inspecting model architecture 'Qwen3MoeForCausalLM'\n[registry-py: 363] Traceback (most recent call last):\n[registry-py: 363] File */usr/local/lib/python3,10/dist-packages/vl]m/model_executor/models/registry-py\", line 594, in _run\n[registry-py: 363] returned. check_returncode(\n[registry-py: 363] File \"/usr/lib/python3.10/subprocess.py\", line 457, in check_ returncode    \n[registry-py: 363] raise CalledProcessError(self.returncode, self.args, self. stdout,\nsubprocess.CalledProcessError: Command '['/usr/bin/python3', '-m', *vllm.model_executor.models.registry')'\n\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\nFile \"/usr/local/1ib/python3.10/dist-packages/v11m/model_executor/models/registry-py\", line 361, in _try_i\nreturn model. inspect_model_cls()\nFile \"/usr/local/1ib/python3.10/dist-packages/v11m/model_executor/models/registry-py*, line 332, in inspec\nreturn -run_in_subprocess(\nFile,\"/usr/local/lib/python3.10/dist-packages/v11m/model_executor/models/registry-py\", line 597, in run in _subprocess\nraise RuntimeError (f\"Error raised in subprocess: \\n\"\nRuntimeError: Error raised in subprocess:\n387 [registry-py:3631 /usr/lib/python3.10/runpy-py:126: RuntimeWarning: 'Vilm-model_executor.models registry' found in sys.modules after import of packag\nir.models.registry'; this may result in unpredictable behaviour\nwarn (RuntimeWarning(msg))\n[registry-py:363] Traceback (most recent call last):\n[registry-py:363] File \"/usr/lib/python3,10/runpy-py\", line 196, in _run_module_as_main\n[registry-py: 363] return _run_ code(code, main_globals, None,\n[registry-py: 363] File \"/usr/lib/python3.10/runpy-py\", line 86, in _run_code\nregistry-py:363] exec (code, run globals)\n[registry-py:363] sr/loca1/1ib/python3.10/dist-packages/v11m/model_executor/models/registry-py*, Line 618, in \u2039module\n[registry-py:363] File /us/Local/13b/python3.10/dist-packages/v11m/model_ executor/models/registry-py*. Line 612, in _rum\nFile \"/usr/lib/python3.10/importlib/.\n_init_-- py\"\", line 126, in import module return\nbootstrap. _god_import (name [level:], package, level)\nFile \u2039 frozen importlib._bootstrap>\", line 1050, in _ged_ import importlib. _bootstrap>\", line 1027, in _find_and_load importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\nFile importlib._bootstrap>\", line 688, in _load_unlocked\nFile \u2039frozen importlib._bootstrap_external>\" , line 883, in exec_ module\nFile \u2039 frozen importlib. _bootstrap>\", line 241, in _call with_frames_ removed\nFile /usr/local/lib/python3.10/dist-packages/v11m/model_executor/models/qwen3_moe-py\", line 37, in \u2039module>\nfrom vIlm. model _executon. layers.fused_moe import FusedMoE\nFile \"/usr/1ocal/Iib/python3.10/dist-packages/v11m/mode1_executor/layers/fused_moe/_init_-py*, line 6, in \u2039module>\nfrom vllm.model_executor.layers.fused_moe.layer import (\nFile \"/usr/local/lib/python3.10/dist-packages/v1lm/model\nexecutor/layers/fused_moe/layer-py\", line 34, in \u2039module\u203a\nfrom .fused_batched moe import (BatchedPrepareAndFinalize,\nFile \"/usr/local/1ib/python3.10/dist-packages/v1lm/model_executor/layers/fused_moe/fused_batched_moe.py\", line 10, in \u2039module> from v1lm.model_executor.layers.fused_moe.fused_moe import (\nFile */usr/local/1ib/python3.10/dist-packages/v11m/model_executor/layers/fused_moe/fused_moe-py\", line 986, in \u2039module> def grouped_topk(\nFile \"/usr/local/lib/python3.10/dist-packages/torch/_init_\u2022py\", line 2543, in fn return compile(\nFile */usr/local/lib/python3,10/dist-packages/torch/_init_\u2022py\", line 2562, in compile from torch. inductor. compiler bisector import CompilerBisector\nFile /usr/local/1ib/python3.10/dist-packages/torch/_inductor/compiler_bisector.py\", line 628, in \u2039module\u203a\nCompilerBisector bisection_enabled = get is_bisection_enabled)\nFile */usr/local/lib/python3.10/dist-packages/torch/_inductor/compiler_bisector.py\", line 623, in get is_bisection enabled\nCompilerBisector get_ subsystem is not None\nFile */usr/local/lib/python3.10/dist-packages/torch/_inductor/compiler_bisector.py\", line 207, in get subsystem\nfile_path = os.path.join(cls.get_dir(),\n\"bisect_status.txt\")\nFile \"/usr/local/1ib/python3.10/dist-packages/torch/_inductor/compiler_bisector-py\", line 125, in get dir return f\"(cache_dir() if not cls.in process_cache else cls.in_process_cache)/(SUBDIR_NAME)* File */us/local/lib/python3.10/dist-packages/torch/_inductor/runtime/cache_dir_utils-py\", line 13, in cache dir\nos. environ (TORCHINDUCTOR_CACHE DIR cache_din = default cache_dire)\nFile */us/local/lib/python3.10/dist-packages/torch/_inductor/runtime/cache_dir_utils-py\", line 19, in default_ cache_dir sanitized username = re. sub(r' [\\V/:**\"\u00bb>|1'\n*_*, getpass getuser())\n\nsanitized username = re. sub(r'[M/:*?\"<>|]' , getpass -getuser))\nFile \"/usr/lib/python3.10/getpass.py\", line 169, in getuser\n    return pwd. getpwuid (os.getuid()) [e]\n\nKeyError: 'getpwuid(): uid not found: 1001'\n\n```\n\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "open",
    "created_at": "2025-06-11T18:11:20+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19504/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19504"
  },
  {
    "number": 19491,
    "title": "[Bug]: vLLM outputs are not reproducible",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : Could not collect\nCMake version                : version 3.22.1\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu126\nIs debug build               : False\nCUDA used to build PyTorch   : 12.6\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0] (64-bit runtime)\nPython platform              : Linux-5.15.0-135-generic-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : Could not collect\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration :\nGPU 0: NVIDIA H100 80GB HBM3\nGPU 1: NVIDIA H100 80GB HBM3\nGPU 2: NVIDIA H100 80GB HBM3\nGPU 3: NVIDIA H100 80GB HBM3\nGPU 4: NVIDIA H100 80GB HBM3\nGPU 5: NVIDIA H100 80GB HBM3\nGPU 6: NVIDIA H100 80GB HBM3\nGPU 7: NVIDIA H100 80GB HBM3\n\nNvidia driver version        : 550.127.05\ncuDNN version                : Could not collect\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               224\nOn-line CPU(s) list:                  0-223\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8480+\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   56\nSocket(s):                            2\nStepping:                             8\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            5.3 MiB (112 instances)\nL1i cache:                            3.5 MiB (112 instances)\nL2 cache:                             224 MiB (112 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190,192,194,196,198,200,202,204,206,208,210,212,214,216,218,220,222\nNUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159,161,163,165,167,169,171,173,175,177,179,181,183,185,187,189,191,193,195,197,199,201,203,205,207,209,211,213,215,217,219,221,223\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-cufile-cu12==1.11.1.6\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-htop==1.2.0\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pytorch-lightning==2.5.1.post0\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.0\n[pip3] torchaudio==2.7.0\n[pip3] torchmetrics==0.10.3\n[pip3] torchvision==0.22.0\n[pip3] transformers==4.52.4\n[pip3] triton==3.3.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.6.4.1                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.6.80                  pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.6.77                  pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.6.77                  pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.5.1.17                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.3.0.4                 pypi_0    pypi\n[conda] nvidia-cufile-cu12        1.11.1.6                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.7.77                pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.7.1.2                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.5.4.2                 pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.3                    pypi_0    pypi\n[conda] nvidia-htop               1.2.0                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.26.2                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.6.85                  pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.6.77                  pypi_0    pypi\n[conda] pytorch-lightning         2.5.1.post0              pypi_0    pypi\n[conda] pyzmq                     26.4.0                   pypi_0    pypi\n[conda] torch                     2.7.0                    pypi_0    pypi\n[conda] torchaudio                2.7.0                    pypi_0    pypi\n[conda] torchmetrics              0.10.3                   pypi_0    pypi\n[conda] torchvision               0.22.0                   pypi_0    pypi\n[conda] transformers              4.52.4                   pypi_0    pypi\n[conda] triton                    3.3.0                    pypi_0    pypi\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.1\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    0,2,4,6,8,10    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    0,2,4,6,8,10    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    0,2,4,6,8,10    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    0,2,4,6,8,10    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    1,3,5,7,9,11    1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    1,3,5,7,9,11    1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    1,3,5,7,9,11    1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      1,3,5,7,9,11    1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n==============================\n     Environment Variables\n==============================\nCUDA_VISIBLE_DEVICES=3\nCUDA_VISIBLE_DEVICES=3\nVLLM_CONFIGURE_LOGGING=0\nVLLM_ENABLE_V1_MULTIPROCESSING=0\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen using `AsyncLLM`, the outputs are not reproducible, even when following the instructions in the documentation (https://docs.vllm.ai/en/stable/usage/reproducibility.html).\n\nFor example, running the below script multiple times and diff-ing the outputs reveals differences in the logprobs even for outputs that are identical (which shouldn't be the case) and in some cases changed outputs. It seems like it's necessary to make a larger number of requests at the same time to observe the effect (possibly it's related to the scheduling, so a certain pressure is required).\n\nWhen setting `temperature=0`, I expect totally deterministic output from the model, and this is extremely important to our use cases, where we might need to debug problematic model outputs for particular inputs. For this small Qwen2 model, the effect is relatively minor, but for larger models I observed much larger differences.\n\n```python\nimport asyncio\nimport itertools\nfrom hashlib import sha1\n\nimport datasets\nimport vllm\nfrom vllm.v1.engine.async_llm import AsyncLLM\n\n\nasync def main():\n    # 198 prompts, actual contents don't matter\n    prompts = datasets.load_dataset(\"cais/mmlu\", \"high_school_geography\")[\"test\"][\"question\"]\n    llm = AsyncLLM.from_engine_args(vllm.AsyncEngineArgs(model=\"Qwen/Qwen2-0.5B-Instruct\", disable_log_requests=True))\n\n    results = await asyncio.gather(*(request(llm, prompt) for prompt in prompts))\n    for result in results:\n        print(\n            f\"{result.request_id}: {result.outputs[0].cumulative_logprob} {result.outputs[0].text[:40]} {sha1(result.outputs[0].text.encode('utf-8')).hexdigest()}\"\n        )\n\n\nrequest_counter = itertools.count()\n\n\nasync def request(llm, prompt: str):\n    request_id = next(request_counter)\n    chat_prompt = [{\"role\": \"user\", \"content\": prompt}]\n    tokens = llm.tokenizer.tokenizer.apply_chat_template(chat_prompt, add_generation_prompt=True, tokenize=True)\n    results_generator = llm.generate(\n        vllm.TokensPrompt(prompt_token_ids=tokens),\n        sampling_params=vllm.SamplingParams(temperature=0.0, max_tokens=1024, logprobs=0),\n        request_id=str(request_id),\n    )\n    final_output = None\n    async for request_output in results_generator:\n        final_output = request_output\n    return final_output\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "open",
    "created_at": "2025-06-11T14:40:27+00:00",
    "closed_at": null,
    "comments": 16,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19491/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19491"
  },
  {
    "number": 19480,
    "title": "[Bug]: Compile inductor / CUDA Graph build before the memory profiling",
    "body": "### Your current environment\n\nRunning Llama4 Maverick on H100x8\n\n### \ud83d\udc1b Describe the bug\n\nOtherwise, it's easy to get OOM. Inductor and CUDA graph themselves may consume a lot of memory, especially, inductor may leverage some profiling to search the best config for the kernels.\n\n```\nexport LLAMA_DIR=meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8; export PORT=8081 VLLM_LOGGING_LEVEL=DEBUG VLLM_DISABLE_COMPILE_CACHE=1 SAFETENSORS_FAST_GPU=1 vllm serve $LLAMA_DIR --disable-log-requests -tp 8 --host :: --port $PORT --served-model-name default --no-enable-prefix-caching --max-model-len 4096 --gpu-memory-utilization 0.8 2>&1 | tee marverik_fp8_no_compile.log\n```\n\nIf we use 0.9 or 0.95, it's easy to reproduce the issue on H100x8 machines.\n0.8 may be okay.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "open",
    "created_at": "2025-06-11T08:42:44+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19480/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19480"
  },
  {
    "number": 19403,
    "title": "[Bug]: Issue of Unstable Output for Identical Queries",
    "body": "### Your current environment\n\nINFO 06-10 00:07:35 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.31.4\nLibc version: glibc-2.35\n\nPython version: 3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.4.54-1.0.0.std7c.el7.2.x86_64-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA GeForce RTX 4090\nGPU 1: NVIDIA GeForce RTX 4090\nGPU 2: NVIDIA GeForce RTX 4090\nGPU 3: NVIDIA GeForce RTX 4090\nGPU 4: NVIDIA GeForce RTX 4090\nGPU 5: NVIDIA GeForce RTX 4090\nGPU 6: NVIDIA GeForce RTX 4090\nGPU 7: NVIDIA GeForce RTX 4090\n\nNvidia driver version: 535.104.05\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   52 bits physical, 57 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          176\nOn-line CPU(s) list:             0-175\nVendor ID:                       GenuineIntel\nBIOS Vendor ID:                  Intel(R) Corporation\nModel name:                      Intel(R) Xeon(R) Platinum 8458P\nBIOS Model name:                 Intel(R) Xeon(R) Platinum 8458P\nCPU family:                      6\nModel:                           143\nThread(s) per core:              2\nCore(s) per socket:              44\nSocket(s):                       2\nStepping:                        8\nFrequency boost:                 enabled\nCPU max MHz:                     2701.0000\nCPU min MHz:                     800.0000\nBogoMIPS:                        5400.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid cldemote movdiri movdir64b md_clear pconfig flush_l1d arch_capabilities\nVirtualization:                  VT-x\nL1d cache:                       4.1 MiB (88 instances)\nL1i cache:                       2.8 MiB (88 instances)\nL2 cache:                        176 MiB (88 instances)\nL3 cache:                        165 MiB (2 instances)\nNUMA node(s):                    2\nNUMA node0 CPU(s):               0-43,88-131\nNUMA node1 CPU(s):               44-87,132-175\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.52.4\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.5\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NODE    NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    0-43,88-131     0               N/A\nGPU1    NODE     X      NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    0-43,88-131     0               N/A\nGPU2    NODE    NODE     X      NODE    SYS     SYS     SYS     SYS     NODE    NODE    0-43,88-131     0               N/A\nGPU3    NODE    NODE    NODE     X      SYS     SYS     SYS     SYS     NODE    NODE    0-43,88-131     0               N/A\nGPU4    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     44-87,132-175   1               N/A\nGPU5    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     SYS     44-87,132-175   1               N/A\nGPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS     SYS     44-87,132-175   1               N/A\nGPU7    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     44-87,132-175   1               N/A\nNIC0    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      PIX\nNIC1    NODE    NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n\nNVIDIA_VISIBLE_DEVICES=all\nNVIDIA_REQUIRE_CUDA=cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\nNCCL_VERSION=2.17.1-1\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nNVIDIA_CUDA_END_OF_LIFE=1\nCUDA_VERSION=12.1.0\nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n### \ud83d\udc1b Describe the bug\n\nWhen the enforce_eager parameter is set to False, an anomalous behavior was observed where the service intermittently produced incorrect results for the initial query in certain operational contexts.\n\n**Problem Description**\n- In serial request scenarios involving single-data inputs, regardless of the total number of requests issued, the first request consistently returned erroneous outputs (refer to Figure for visual representation).\n- When executing concurrent requests with single-data payloads, instances of incorrect outputs occurred randomly, with a significantly elevated frequency compared to serial execution.\n- Restoration of normal functionality was observed for both serial and concurrent request scenarios when the enforce_eager parameter was set to True.\n- This phenomenon was reproducibly observed across both online service deployments and offline engine testing environments.\n\n**Problem Analysis**\nA working hypothesis suggests that the instability may be attributed to the CUDA Graph's computational behavior during the initial capture of tensors with specific geometric configurations.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "open",
    "created_at": "2025-06-10T07:07:59+00:00",
    "closed_at": null,
    "comments": 23,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19403/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19403"
  },
  {
    "number": 18851,
    "title": "[Bug]: Strange error `AssertionError: failed to get the hash of the compiled graph` when running `Qwen/Qwen3-8B` via `LLM` class",
    "body": "### Your current environment\n\n```\n>>> import vllm; vllm.__version__\nINFO 05-28 19:02:30 [__init__.py:248] Automatically detected platform cuda.\n'0.9.1.dev59+gb6a6e7a52'\n>>>\n>>> import torch; torch.__version__\n'2.7.0+cu126'\n>>> import transformers; transformers.__version__\n'4.52.2'\n```\n\n### \ud83d\udc1b Describe the bug\n\n``` \n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522] Traceback (most recent call last):                                                                                                                                                                                                      (VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 517, in worker_busy_loop                                                                                                    (VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     output = func(*args, **kwargs)                                                                                                                                                                                                      (VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]              ^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                      (VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     return func(*args, **kwargs)\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 185, in determine_available_memory\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     self.model_runner.profile_run()\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1854, in profile_run\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     hidden_states = self._dummy_run(self.max_num_tokens)\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     return func(*args, **kwargs)\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1695, in _dummy_run\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     outputs = model(\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]               ^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     return self._call_impl(*args, **kwargs)\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     return forward_call(*args, **kwargs)\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py\", line 300, in forward\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     hidden_states = self.model(input_ids, positions, intermediate_tensors,\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 238, in __call__\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     output = self.compiled_callable(*args, **kwargs)\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 663, in _fn\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1544, in _call_user_compiler\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     raise BackendCompilerFailed(\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1519, in _call_user_compiler\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     compiled_fn = compiler_fn(gm, self.example_inputs())\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 150, in __call__\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     compiled_gm = compiler_fn(gm, example_inputs)\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 150, in __call__\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     compiled_gm = compiler_fn(gm, example_inputs)\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/torch/__init__.py\", line 2392, in __call__\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     return self.compiler_fn(model_, inputs_, **self.kwargs)\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/compilation/backends.py\", line 498, in __call__\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     PiecewiseCompileInterpreter(self.split_gm, submod_names_to_compile,\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/compilation/backends.py\", line 275, in run\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     return super().run(*fake_args)\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/torch/fx/interpreter.py\", line 171, in run\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     self.env[node] = self.run_node(node)\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]                      ^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/torch/fx/interpreter.py\", line 240, in run_node\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     return getattr(self, n.op)(n.target, args, kwargs)\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/compilation/backends.py\", line 291, in call_module\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     compiler_manager.compile(\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]                      ^^^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/compilation/backends.py\", line 147, in compile\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     compiled_graph, handle = self.compiler.compile(\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]                              ^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]   File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py\", line 414, in compile\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]     assert hash_str is not None, (\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7fc6002fbe90>' raised:\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522] AssertionError: failed to get the hash of the compiled graph\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522]\n(VllmWorker rank=1 pid=191128) ERROR 05-28 18:58:32 [multiproc_executor.py:522] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/fs/ml/validation.py\", line 43, in infer\n    llm = vllm.LLM(\n          ^^^^^^^^^\n  File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/utils.py\", line 1177, in inner\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/entrypoints/llm.py\", line 250, in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/engine/llm_engine.py\", line 511, in from_engine_args\n    return engine_cls.from_vllm_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py\", line 115, in from_vllm_config\n    return cls(vllm_config=vllm_config,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py\", line 92, in __init__\n    self.engine_core = EngineCoreClient.make_client(\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 75, in make_client\n    return SyncMPClient(vllm_config, executor_class, log_stats)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 580, in __init__\n    super().__init__(\n  File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 418, in __init__\n    self._wait_for_engine_startup(output_address, parallel_config)\n  File \"/mnt/fs/venv_cu126_py312/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 484, in _wait_for_engine_startup\n    raise RuntimeError(\"Engine core initialization failed. \"\nRuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_0': 1}\n\"\"\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "open",
    "created_at": "2025-05-28T19:04:20+00:00",
    "closed_at": null,
    "comments": 20,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18851/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/18851"
  },
  {
    "number": 17593,
    "title": "[Bug]: torch._inductor.exc.InductorError: TypeError: cannot pickle 'torch._C.DispatchKeySet' object",
    "body": "### Your current environment\n\nvLLM main branch, PyTorch main branch\n\n### \ud83d\udc1b Describe the bug\n\nRepro:\n`pytest -v -s tests/compile/piecewise/test_toy_llama.py`\n\nGives:\n```\n>                           rv = reductor(4)\nE                           torch._inductor.exc.InductorError: TypeError: cannot pickle 'torch._C.DispatchKeySet' object\nE\nE                           Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even m\nore developer context, set TORCH_LOGS=\"+dynamo\"\n\n../env/lib/python3.12/copy.py:151: InductorError\n====================================================================== warnings summary =======================================================================\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "open",
    "created_at": "2025-05-02T15:20:59+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17593/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/17593"
  },
  {
    "number": 17513,
    "title": "[Bug]: [V1][Spec Dec] EAGLE TP > 1 leads to errors when using --enforce_eager",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.6 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: version 3.16.3\nLibc version: glibc-2.31\n\nPython version: 3.12.9 | packaged by conda-forge | (main, Mar  4 2025, 22:48:41) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-5.15.0-1064-aws-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA A100-SXM4-40GB\nGPU 1: NVIDIA A100-SXM4-40GB\nGPU 2: NVIDIA A100-SXM4-40GB\nGPU 3: NVIDIA A100-SXM4-40GB\nGPU 4: NVIDIA A100-SXM4-40GB\nGPU 5: NVIDIA A100-SXM4-40GB\nGPU 6: NVIDIA A100-SXM4-40GB\nGPU 7: NVIDIA A100-SXM4-40GB\n\nNvidia driver version: 535.183.01\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      46 bits physical, 48 bits virtual\nCPU(s):                             96\nOn-line CPU(s) list:                0-95\nThread(s) per core:                 2\nCore(s) per socket:                 24\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              85\nModel name:                         Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz\nStepping:                           7\nCPU MHz:                            3000.006\nBogoMIPS:                           6000.01\nHypervisor vendor:                  KVM\nVirtualization type:                full\nL1d cache:                          1.5 MiB\nL1i cache:                          1.5 MiB\nL2 cache:                           48 MiB\nL3 cache:                           71.5 MiB\nNUMA node0 CPU(s):                  0-23,48-71\nNUMA node1 CPU(s):                  24-47,72-95\nVulnerability Gather data sampling: Unknown: Dependent on hypervisor status\nVulnerability Itlb multihit:        KVM: Mitigation: VMX unsupported\nVulnerability L1tf:                 Mitigation; PTE Inversion\nVulnerability Mds:                  Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Meltdown:             Mitigation; PTI\nVulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Retbleed:             Vulnerable\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Vulnerable\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Retpolines; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Retpoline\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke\n\nVersions of relevant libraries:\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.4.0\n[pip3] sentence-transformers==3.2.1\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.1\n[pip3] transformers-stream-generator==0.0.5\n[pip3] triton==3.2.0\n[pip3] tritonclient==2.51.0\n[pip3] vector-quantize-pytorch==1.21.2\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.4.0                   pypi_0    pypi\n[conda] sentence-transformers     3.2.1                    pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] transformers              4.51.1                   pypi_0    pypi\n[conda] transformers-stream-generator 0.0.5                    pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\n[conda] tritonclient              2.51.0                   pypi_0    pypi\n[conda] vector-quantize-pytorch   1.21.2                   pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.5.dev352+gd299b92a0 (git sha: d299b92a0)\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV12    NV12    NV12    NV12    NV12    NV12    NV12    0-23,48-71      0               N/A\nGPU1    NV12     X      NV12    NV12    NV12    NV12    NV12    NV12    0-23,48-71      0               N/A\nGPU2    NV12    NV12     X      NV12    NV12    NV12    NV12    NV12    0-23,48-71      0               N/A\nGPU3    NV12    NV12    NV12     X      NV12    NV12    NV12    NV12    0-23,48-71      0               N/A\nGPU4    NV12    NV12    NV12    NV12     X      NV12    NV12    NV12    24-47,72-95     1               N/A\nGPU5    NV12    NV12    NV12    NV12    NV12     X      NV12    NV12    24-47,72-95     1               N/A\nGPU6    NV12    NV12    NV12    NV12    NV12    NV12     X      NV12    24-47,72-95     1               N/A\nGPU7    NV12    NV12    NV12    NV12    NV12    NV12    NV12     X      24-47,72-95     1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen I specify TP > 1 and `--enforce_eager` at the same time, vLLM raises some torch dynamo error. IIUC, under eager mode, torch.compile should not be involved. To reproduce the error, run \n\n`VLLM_USE_V1=1 python examples/offline_inference/eagle.py --enforce_eager --draft_tp 8 --tp 8`\n\nI was suspecting my [PR](https://github.com/vllm-project/vllm/pull/17211) that applies torch.compile & cuda graph caused this error but it seems to be not the case. vLLM already suffers from the error before the PR was merged. One can verify this by checking out the parent commit [here](https://github.com/vllm-project/vllm/commit/c9c1b59e59a35d5004e3914e23015617fc330b31). The command seems to work fine with this [PR](https://github.com/vllm-project/vllm/pull/16035). So basically something in-between messed things up. \n\ncc @WoosukKwon @LiuXiaoxuanPKU \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "open",
    "created_at": "2025-05-01T01:42:30+00:00",
    "closed_at": null,
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17513/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/17513"
  },
  {
    "number": 16501,
    "title": "[RFC]: vLLM x torch.compile caching should be opt-out by default",
    "body": "### Motivation.\n\nHow vLLM decides to cache torch.compile compilations is brittle. There's [a list of configs](https://github.com/vllm-project/vllm/blob/70de35a8816e224663aede45b7f54eef250a5cfe/vllm/compilation/backends.py#L360-L394) that it takes into account and hashes, if any of these configs change then vLLM decides that it needs to do a fresh torch.compile run.\n\nAs we saw in https://github.com/vllm-project/vllm/pull/16491, it's very easy to add a new feature to one of the configs and forget to update the hash function. In that PR, the problem was that ModelConfig's hash function did not take into account [everything that could change the compilation](https://github.com/vllm-project/vllm/blob/70de35a8816e224663aede45b7f54eef250a5cfe/vllm/config.py#L279-L303).\n\n\n\n\n\n### Proposed Change.\n\nThe hash functions are currently opt-in: when someone adds a new feature or does a refactor they may need to add something to the hash functions. After discussion with the PyTorch Compiler team (cc @oulgen), we instead propose changing the hash functions to be opt-out.\n\nWhat that means is that ModelConfig's compute_hash function instead contains a list of fields that it should not include in the hash:\n```py\ndef compute_hash(self):\n   factors = list(self.__dict__.values())\n   factors.remove(\"enforce_eager\")\n   factors.remove(\"tokenizer_config\")\n   ...\n```\n\nOpt-out seems safer. The risk of incorrect caching is (1) model errors unexpectedly and (2) silent incorrectness, so we think it's better to be more conservative.\n\n### Feedback Period.\n\nEOD Friday, April 18th, 2025.\n\n### CC List.\n\n@youkaichao @tlrmchlsmth @mgoin @ProExpertProg @houseroad \n\n### Any Other Things.\n\nthank you\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "torch.compile"
    ],
    "state": "open",
    "created_at": "2025-04-11T16:58:19+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16501/reactions",
      "total_count": 8,
      "+1": 8,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16501"
  },
  {
    "number": 16337,
    "title": "[Bug]: FP8 Quantization with enforce_eager=False Causes Gibberish Output on Llama-4-Scout Model (VLLM_USE_V1=1)",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nPyTorch version: 2.7.0a0+git295f2ed\nIs debug build: False\nCUDA used to build PyTorch: N/A\nROCM used to build PyTorch: 6.3.42133-1b9c17779\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: 18.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.3.1 24491 1e0fda770a2079fbd71e4b70974d74f62fd3af10)\nCMake version: version 3.31.6\nLibc version: glibc-2.35\n\nPython version: 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-116-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: AMD Instinct MI300X (gfx942:sramecc+:xnack-)\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\nHIP runtime version: 6.3.42133\nMIOpen runtime version: 3.3.0\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               192\nOn-line CPU(s) list:                  0-191\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 9654 96-Core Processor\nCPU family:                           25\nModel:                                17\nThread(s) per core:                   1\nCore(s) per socket:                   96\nSocket(s):                            2\nStepping:                             1\nFrequency boost:                      enabled\nCPU max MHz:                          3707.8120\nCPU min MHz:                          1500.0000\nBogoMIPS:                             4792.60\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov succor smca fsrm flush_l1d\nVirtualization:                       AMD-V\nL1d cache:                            6 MiB (192 instances)\nL1i cache:                            6 MiB (192 instances)\nL2 cache:                             192 MiB (192 instances)\nL3 cache:                             768 MiB (24 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-95\nNUMA node1 CPU(s):                    96-191\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.0a0+git295f2ed\n[pip3] torchvision==0.21.0+7af6987\n[pip3] transformers==4.51.0\n[pip3] triton==3.2.0+gite5be006a\n[conda] Could not collect\nROCM Version: 6.3.42133-1b9c17779\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n============================ ROCm System Management Interface ============================\n================================ Weight between two GPUs =================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \nGPU0   0            15           15           15           15           15           15           15           \nGPU1   15           0            15           15           15           15           15           15           \nGPU2   15           15           0            15           15           15           15           15           \nGPU3   15           15           15           0            15           15           15           15           \nGPU4   15           15           15           15           0            15           15           15           \nGPU5   15           15           15           15           15           0            15           15           \nGPU6   15           15           15           15           15           15           0            15           \nGPU7   15           15           15           15           15           15           15           0            \n\n================================= Hops between two GPUs ==================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \nGPU0   0            1            1            1            1            1            1            1            \nGPU1   1            0            1            1            1            1            1            1            \nGPU2   1            1            0            1            1            1            1            1            \nGPU3   1            1            1            0            1            1            1            1            \nGPU4   1            1            1            1            0            1            1            1            \nGPU5   1            1            1            1            1            0            1            1            \nGPU6   1            1            1            1            1            1            0            1            \nGPU7   1            1            1            1            1            1            1            0            \n\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         \nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         \nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         \nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         \nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         \nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            \n\n======================================= Numa Nodes =======================================\nGPU[0]\t\t: (Topology) Numa Node: 0\nGPU[0]\t\t: (Topology) Numa Affinity: 0\nGPU[1]\t\t: (Topology) Numa Node: 0\nGPU[1]\t\t: (Topology) Numa Affinity: 0\nGPU[2]\t\t: (Topology) Numa Node: 0\nGPU[2]\t\t: (Topology) Numa Affinity: 0\nGPU[3]\t\t: (Topology) Numa Node: 0\nGPU[3]\t\t: (Topology) Numa Affinity: 0\nGPU[4]\t\t: (Topology) Numa Node: 1\nGPU[4]\t\t: (Topology) Numa Affinity: 1\nGPU[5]\t\t: (Topology) Numa Node: 1\nGPU[5]\t\t: (Topology) Numa Affinity: 1\nGPU[6]\t\t: (Topology) Numa Node: 1\nGPU[6]\t\t: (Topology) Numa Affinity: 1\nGPU[7]\t\t: (Topology) Numa Node: 1\nGPU[7]\t\t: (Topology) Numa Affinity: 1\n================================== End of ROCm SMI Log ===================================\n\nPYTORCH_TUNABLEOP_TUNING=0\nPYTORCH_TUNABLEOP_ENABLED=1\nPYTORCH_ROCM_ARCH=gfx942\nLD_LIBRARY_PATH=/opt/rocm/lib:/usr/local/lib:\nPYTORCH_TUNABLEOP_FILENAME=/app/afo_tune_device_%d_full.csv\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen running Llama-4-Scout-17B-16E-Instruct with the following combination of vars/args:\n-  VLLM_USE_V1=1\n-  quantization=\"fp8\"\n-  enforce_eager=False\n\nThe model produces gibberish. To reproduce the issue, run the following script:\n\n```bash\nVLLM_WORKER_MULTIPROC_METHOD=spawn SAFETENSORS_FAST_GPU=1 VLLM_USE_V1=1 python example.py\n```\n\n```python\n\n#example.py\n\nfrom vllm import LLM, SamplingParams\n\ndef test():\n\n    prompts = [\n        \"The color of the sky is blue but sometimes it can also be\",\n        \"The capital of France is\",\n    ]\n    sampling_params = SamplingParams(temperature=0.8,\n                                     top_p=0.95,\n                                     max_tokens=256)\n    llm = LLM(\n        model=\"/app/model/Llama-4-Scout-17B-16E-Instruct/\",\n        tensor_parallel_size=4,\n        quantization=\"fp8\",  \n        max_model_len=8192,\n        enforce_eager=False,\n    )\n\n    outputs = llm.generate(prompts, sampling_params)\n    # Print the outputs.\n    for output in outputs:\n        prompt = output.prompt\n        generated_text = output.outputs[0].text\n        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\n\nif __name__ == \"__main__\":\n    test()\n\n```\nThis issue occurs only when `enforce_eager=False`. Setting `enforce_eager` to true will generate reasonable output. Furthermore, removing cuda graph padding from `num_input_tokens` in  `vllm/v1/worker/gpu_model_runner.py` seems to resolve this issue:\n\n```python\n        # Remove cuda padding\n        # if (self.use_cuda_graph\n        #         and num_scheduled_tokens <= self.cudagraph_batch_sizes[-1]):\n        #     # Use piecewise CUDA graphs.\n        #     # Add padding to the batch size.\n        #     num_input_tokens = self.vllm_config.pad_for_cudagraph(\n        #         num_scheduled_tokens)\n        # else:\n        #     # Eager mode.\n        #     num_input_tokens = num_scheduled_tokens\n        num_input_tokens = num_scheduled_tokens\n        attn_metadata.num_input_tokens = num_input_tokens\n```\n\n### Sample outputs\n\n#### With CUDA padding\n\n```text\nGenerated text: ' seen as sky blue if I have seen sunlight.\\n\\n\u2022 counters old, counters...'\nGenerated text: \" given as well as capital France. \\n\\nThe port, int. The enclosure is a big...\"\n```\n\n#### Without CUDA padding\n\n```text\nGenerated text: 'red, orange, or other colors depending on the time of day and atmospheric conditions. ...'\nGenerated text: \"known as the City of Light. It is famous for its art, fashion, and culture. If you are planning to visit Paris,...\"\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "open",
    "created_at": "2025-04-09T11:22:10+00:00",
    "closed_at": null,
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16337/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16337"
  },
  {
    "number": 16009,
    "title": "[Bug]: TypeError: __init__() missing 1 required positional argument: 'inner_exception'",
    "body": "### Your current environment\n\n<details>\n\n```text\nPyTorch version: 2.6.0+cu118\nIs debug build: False\nCUDA used to build PyTorch: 11.8\nROCM used to build PyTorch: N/A\n\nOS: CentOS Linux 7 (Core) (x86_64)\nGCC version: (GCC) 9.3.1 20200408 (Red Hat 9.3.1-2)\nClang version: Could not collect\nCMake version: version 3.27.6\nLibc version: glibc-2.17\n\nPython version: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21)  [GCC 12.3.0] (64-bit runtime)\nPython platform: Linux-4.18.0-147.mt20200626.413.el8_1.x86_64-x86_64-with-glibc2.17\nIs CUDA available: True\nCUDA runtime version: 11.7.99\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA A100-SXM4-80GB\nGPU 1: NVIDIA A100-SXM4-80GB\n\nNvidia driver version: Could not collect\ncuDNN version: Probably one of the following:\n/usr/lib64/libcudnn.so.8.5.0\n/usr/lib64/libcudnn_adv_infer.so.8.5.0\n/usr/lib64/libcudnn_adv_train.so.8.5.0\n/usr/lib64/libcudnn_cnn_infer.so.8.5.0\n/usr/lib64/libcudnn_cnn_train.so.8.5.0\n/usr/lib64/libcudnn_ops_infer.so.8.5.0\n/usr/lib64/libcudnn_ops_train.so.8.5.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:          x86_64\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                192\nOn-line CPU(s) list:   0-35\nOff-line CPU(s) list:  36-191\nThread(s) per core:    0\nCore(s) per socket:    48\nSocket(s):             2\nNUMA node(s):          2\nVendor ID:             AuthenticAMD\nCPU family:            23\nModel:                 49\nModel name:            AMD EPYC 7642 48-Core Processor\nStepping:              0\nCPU MHz:               3231.852\nCPU max MHz:           2300.0000\nCPU min MHz:           1500.0000\nBogoMIPS:              4591.47\nVirtualization:        AMD-V\nL1d cache:             32K\nL1i cache:             32K\nL2 cache:              512K\nL3 cache:              16384K\nNUMA node0 CPU(s):     0-47,96-143\nNUMA node1 CPU(s):     48-95,144-191\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate sme ssbd mba sev ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip rdpid overflow_recov succor smca\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu11==11.11.3.6\n[pip3] nvidia-cuda-cupti-cu11==11.8.87\n[pip3] nvidia-cuda-nvrtc-cu11==11.8.89\n[pip3] nvidia-cuda-runtime-cu11==11.8.89\n[pip3] nvidia-cudnn-cu11==9.1.0.70\n[pip3] nvidia-cufft-cu11==10.9.0.58\n[pip3] nvidia-curand-cu11==10.3.0.86\n[pip3] nvidia-cusolver-cu11==11.4.1.48\n[pip3] nvidia-cusparse-cu11==11.7.5.86\n[pip3] nvidia-nccl-cu11==2.21.5\n[pip3] nvidia-nvtx-cu11==11.8.86\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0+cu118\n[pip3] torchaudio==2.6.0+cu118\n[pip3] torchvision==0.21.0+cu118\n[pip3] transformers==4.50.3\n[pip3] triton==3.2.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu11        11.11.3.6                pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu11    11.8.87                  pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu11    11.8.89                  pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu11  11.8.89                  pypi_0    pypi\n[conda] nvidia-cudnn-cu11         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu11         10.9.0.58                pypi_0    pypi\n[conda] nvidia-curand-cu11        10.3.0.86                pypi_0    pypi\n[conda] nvidia-cusolver-cu11      11.4.1.48                pypi_0    pypi\n[conda] nvidia-cusparse-cu11      11.7.5.86                pypi_0    pypi\n[conda] nvidia-nccl-cu11          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvtx-cu11          11.8.86                  pypi_0    pypi\n[conda] pyzmq                     26.3.0                   pypi_0    pypi\n[conda] torch                     2.6.0+cu118              pypi_0    pypi\n[conda] torchaudio                2.6.0+cu118              pypi_0    pypi\n[conda] torchvision               0.21.0+cu118             pypi_0    pypi\n[conda] transformers              4.50.3                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tGPU1\tmlx5_0\tmlx5_1\tmlx5_2\tmlx5_3\tmlx5_4\tmlx5_5\tmlx5_6\tmlx5_7\tCPU Affinity\tNUMA Affinity\nGPU0\t X \tNV12\tNODE\tNODE\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\t0-47,96-143\t0\nGPU1\tNV12\t X \tNODE\tNODE\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\t0-47,96-143\t0\nmlx5_0\tNODE\tNODE\t X \tPIX\tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t\t\nmlx5_1\tNODE\tNODE\tPIX\t X \tNODE\tNODE\tSYS\tSYS\tSYS\tSYS\t\t\nmlx5_2\tPXB\tPXB\tNODE\tNODE\t X \tPIX\tSYS\tSYS\tSYS\tSYS\t\t\nmlx5_3\tPXB\tPXB\tNODE\tNODE\tPIX\t X \tSYS\tSYS\tSYS\tSYS\t\t\nmlx5_4\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPIX\tNODE\tNODE\t\t\nmlx5_5\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPIX\t X \tNODE\tNODE\t\t\nmlx5_6\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\t X \tPIX\t\t\nmlx5_7\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tNODE\tNODE\tPIX\t X \t\t\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nCUDNN_VERSION=8.5.0.96\nNVIDIA_REQUIRE_CUDA=cuda>=11.4 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441 driver>=450\nNCCL_IB_GID_INDEX=7\nLD_LIBRARY_PATH=/lib64:/usr/lib64::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/java/jre/lib/amd64/server:/opt/meituan/hadoop/lib/native\nNVIDIA_VISIBLE_DEVICES=GPU-cdea27d7-2349-e4d2-94e9-c3eacdf2fbd1,GPU-fbe0b47c-1999-1849-72da-932cb548807f\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nOMP_NUM_THREADS=36\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n``` python\nexport CUDA_VISIBLE_DEVICES=0,1\n\nvllm serve \"QwQ-32B\" \\\n --host 0.0.0.0 \\\n --port 44390 \\\n --tensor-parallel-size 2\n```\n\n```\nERROR 04-03 18:30:13 [core.py:343] EngineCore hit an exception: Traceback (most recent call last):\nERROR 04-03 18:30:13 [core.py:343]   File \"/envs/vllm/lib/python3.9/site-packages/vllm/v1/engine/core.py\", line 335, in run_engine_core\nERROR 04-03 18:30:13 [core.py:343]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 04-03 18:30:13 [core.py:343]   File \"/envs/vllm/lib/python3.9/site-packages/vllm/v1/engine/core.py\", line 290, in __init__\nERROR 04-03 18:30:13 [core.py:343]     super().__init__(vllm_config, executor_class, log_stats)\nERROR 04-03 18:30:13 [core.py:343]   File \"/envs/vllm/lib/python3.9/site-packages/vllm/v1/engine/core.py\", line 63, in __init__\nERROR 04-03 18:30:13 [core.py:343]     num_gpu_blocks, num_cpu_blocks = self._initialize_kv_caches(\nERROR 04-03 18:30:13 [core.py:343]   File \"/envs/vllm/lib/python3.9/site-packages/vllm/v1/engine/core.py\", line 122, in _initialize_kv_caches\nERROR 04-03 18:30:13 [core.py:343]     available_gpu_memory = self.model_executor.determine_available_memory()\nERROR 04-03 18:30:13 [core.py:343]   File \"/envs/vllm/lib/python3.9/site-packages/vllm/v1/executor/abstract.py\", line 66, in determine_available_memory\nERROR 04-03 18:30:13 [core.py:343]     output = self.collective_rpc(\"determine_available_memory\")\nERROR 04-03 18:30:13 [core.py:343]   File \"/envs/vllm/lib/python3.9/site-packages/vllm/v1/executor/multiproc_executor.py\", line 134, in collective_rpc\nERROR 04-03 18:30:13 [core.py:343]     raise e\nERROR 04-03 18:30:13 [core.py:343]   File \"/envs/vllm/lib/python3.9/site-packages/vllm/v1/executor/multiproc_executor.py\", line 118, in collective_rpc\nERROR 04-03 18:30:13 [core.py:343]     status, result = w.worker_response_mq.dequeue(\nERROR 04-03 18:30:13 [core.py:343]   File \"/envs/vllm/lib/python3.9/site-packages/vllm/distributed/device_communicators/shm_broadcast.py\", line 471, in dequeue\nERROR 04-03 18:30:13 [core.py:343]     obj = pickle.loads(buf[1:])\nERROR 04-03 18:30:13 [core.py:343] TypeError: __init__() missing 1 required positional argument: 'inner_exception'\nERROR 04-03 18:30:13 [core.py:343] \nCRITICAL 04-03 18:30:13 [core_client.py:269] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n```\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-03T10:58:22+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16009/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16009"
  },
  {
    "number": 16006,
    "title": "[Bug]: crash during debug, works ok running cli",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nCollecting environment information...\n\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: 14.0.0-1ubuntu1.1\nCMake version: version 3.31.4\nLibc version: glibc-2.35\n\nPython version: 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-6.8.0-50-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090\nNvidia driver version: 570.86.15\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        39 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               12\nOn-line CPU(s) list:                  0-11\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz\nCPU family:                           6\nModel:                                158\nThread(s) per core:                   2\nCore(s) per socket:                   6\nSocket(s):                            1\nStepping:                             10\nCPU max MHz:                          4600.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             6399.96\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb pti ssbd ibrs ibpb stibp tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp vnmi md_clear flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            192 KiB (6 instances)\nL1i cache:                            192 KiB (6 instances)\nL2 cache:                             1.5 MiB (6 instances)\nL3 cache:                             12 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-11\nVulnerability Gather data sampling:   Mitigation; Microcode\nVulnerability Itlb multihit:          KVM: Mitigation: VMX disabled\nVulnerability L1tf:                   Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable\nVulnerability Mds:                    Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Meltdown:               Mitigation; PTI\nVulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; IBRS\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; IBRS; IBPB conditional; STIBP conditional; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Mitigation; Microcode\nVulnerability Tsx async abort:        Mitigation; TSX disabled\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.3\n[pip3] triton==3.1.0\n[conda] blas                      1.0                         mkl  \n[conda] cuda-cudart               11.7.99                       0    nvidia\n[conda] cuda-cupti                11.7.101                      0    nvidia\n[conda] cuda-libraries            11.7.1                        0    nvidia\n[conda] cuda-nvrtc                11.7.99                       0    nvidia\n[conda] cuda-nvtx                 11.7.91                       0    nvidia\n[conda] cuda-runtime              11.7.1                        0    nvidia\n[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\n[conda] libcublas                 11.10.3.66                    0    nvidia\n[conda] libcufft                  10.7.2.124           h4fbf590_0    nvidia\n[conda] libcufile                 1.8.0.34                      0    nvidia\n[conda] libcurand                 10.3.4.52                     0    nvidia\n[conda] libcusolver               11.4.0.1                      0    nvidia\n[conda] libcusparse               11.7.4.91                     0    nvidia\n[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\n[conda] libnpp                    11.7.4.75                     0    nvidia\n[conda] libnvjpeg                 11.8.0.2                      0    nvidia\n[conda] mkl                       2023.1.0         h213fc3f_46344  \n[conda] mkl-service               2.4.0           py311h5eee18b_1  \n[conda] mkl_fft                   1.3.8           py311h5eee18b_0  \n[conda] mkl_random                1.2.4           py311hdb19cb5_0  \n[conda] numpy                     1.26.0          py311h08b1b3b_0  \n[conda] numpy-base                1.26.0          py311hf175353_0  \n[conda] pytorch                   2.1.0              py3.11_cpu_0    pytorch\n[conda] pytorch-cuda              11.7                 h778d358_5    pytorch\n[conda] pytorch-mutex             1.0                         cpu    pytorch\n[conda] torchaudio                2.1.0                 py311_cpu    pytorch\n[conda] torchvision               0.16.0                py311_cpu    pytorch\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.3.dev62+gcb080f32.d20250213\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \t0-11\t0\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=:/home/cju/.mujoco/mujoco210/bin:/usr/lib/nvidia\nCUDA_HOME=/usr/local/cuda\nCUDA_HOME=/usr/local/cuda\nVLLM_USE_V1=1\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nrunning the cli  `vllm serve facebook/opt-125m` works out ok. The server starts up.\nhowever, it crashes when debugging it in vscode with the following launch.json:\n```\n{\n    // Use IntelliSense to learn about possible attributes.\n    // Hover to view descriptions of existing attributes.\n    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"vllm serve\",\n            \"type\": \"debugpy\",\n            \"request\": \"launch\",\n            \"program\": \"${workspaceFolder}/vllm/scripts.py\",\n            \"console\": \"integratedTerminal\",\n            \"justMyCode\": false,\n            \"args\": [\n                \"serve\", \"facebook/opt-125m\"\n            ]\n        }\n    ]\n}\n```\n\nThe error is:\n\n> INFO 04-03 17:56:20 weight_utils.py:254] Using model weights format ['*.safetensors']\n> INFO 04-03 17:56:21 weight_utils.py:306] No model.safetensors.index.json found in remote.\n> Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n> Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.66s/it]\n> Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.66s/it]\n> \n> INFO 04-03 17:56:28 gpu_model_runner.py:918] Loading model weights took 2.8875 GB\n> INFO 04-03 17:57:17 backends.py:408] Using cache directory: /home/cju/.cache/vllm/torch_compile_cache/ca60c2a0fd/rank_0 for vLLM's torch.compile\n> INFO 04-03 17:57:17 backends.py:418] Dynamo bytecode transform time: 26.71 s\n> INFO 04-03 17:57:19 backends.py:115] Directly load the compiled graph for shape None from the cache\n> ERROR 04-03 17:57:33 core.py:208] EngineCore hit an exception: Traceback (most recent call last):\n> ERROR 04-03 17:57:33 core.py:208]   File \"_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx\", line 499, in _pydevd_sys_monitoring_cython._get_code_line_info\n> ERROR 04-03 17:57:33 core.py:208] KeyError: <code object forward at 0x4d452b40, file \"/home/cju/aigc/vllm/vllm/model_executor/models/qwen2.py\", line 327>\n> ERROR 04-03 17:57:33 core.py:208] \n> ERROR 04-03 17:57:33 core.py:208] During handling of the above exception, another exception occurred:\n> ERROR 04-03 17:57:33 core.py:208] \n> ERROR 04-03 17:57:33 core.py:208] Traceback (most recent call last):\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/vllm/v1/engine/core.py\", line 200, in run_engine_core\n> ERROR 04-03 17:57:33 core.py:208]     engine_core = EngineCoreProc(*args, **kwargs)\n> ERROR 04-03 17:57:33 core.py:208]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/vllm/v1/engine/core.py\", line 153, in __init__\n> ERROR 04-03 17:57:33 core.py:208]     super().__init__(vllm_config, executor_class)\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/vllm/v1/engine/core.py\", line 51, in __init__\n> ERROR 04-03 17:57:33 core.py:208]     num_gpu_blocks, num_cpu_blocks = self._initialize_kv_caches(\n> ERROR 04-03 17:57:33 core.py:208]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/vllm/v1/engine/core.py\", line 76, in _initialize_kv_caches\n> ERROR 04-03 17:57:33 core.py:208]     availble_gpu_memory = self.model_executor.determine_available_memory()\n> ERROR 04-03 17:57:33 core.py:208]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/vllm/v1/executor/abstract.py\", line 60, in determine_available_memory\n> ERROR 04-03 17:57:33 core.py:208]     output = self.collective_rpc(\"determine_available_memory\")\n> ERROR 04-03 17:57:33 core.py:208]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/vllm/executor/uniproc_executor.py\", line 51, in collective_rpc\n> ERROR 04-03 17:57:33 core.py:208]     answer = run_method(self.driver_worker, method, args, kwargs)\n> ERROR 04-03 17:57:33 core.py:208]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/vllm/utils.py\", line 2220, in run_method\n> ERROR 04-03 17:57:33 core.py:208]     return func(*args, **kwargs)\n> ERROR 04-03 17:57:33 core.py:208]            ^^^^^^^^^^^^^^^^^^^^^\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n> ERROR 04-03 17:57:33 core.py:208]     return func(*args, **kwargs)\n> ERROR 04-03 17:57:33 core.py:208]            ^^^^^^^^^^^^^^^^^^^^^\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/vllm/v1/worker/gpu_worker.py\", line 163, in determine_available_memory\n> ERROR 04-03 17:57:33 core.py:208]     self.model_runner.profile_run()\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/vllm/v1/worker/gpu_model_runner.py\", line 1143, in profile_run\n> ERROR 04-03 17:57:33 core.py:208]     hidden_states = self._dummy_run(self.max_num_tokens,\n> ERROR 04-03 17:57:33 core.py:208]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n> ERROR 04-03 17:57:33 core.py:208]     return func(*args, **kwargs)\n> ERROR 04-03 17:57:33 core.py:208]            ^^^^^^^^^^^^^^^^^^^^^\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/vllm/v1/worker/gpu_model_runner.py\", line 1011, in _dummy_run\n> ERROR 04-03 17:57:33 core.py:208]     hidden_states = model(\n> ERROR 04-03 17:57:33 core.py:208]                     ^^^^^^\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n> ERROR 04-03 17:57:33 core.py:208]     return self._call_impl(*args, **kwargs)\n> ERROR 04-03 17:57:33 core.py:208]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n> ERROR 04-03 17:57:33 core.py:208]     return forward_call(*args, **kwargs)\n> ERROR 04-03 17:57:33 core.py:208]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/vllm/model_executor/models/qwen2.py\", line 486, in forward\n> ERROR 04-03 17:57:33 core.py:208]     hidden_states = self.model(input_ids, positions, kv_caches,\n> ERROR 04-03 17:57:33 core.py:208]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/vllm/compilation/decorators.py\", line 238, in __call__\n> ERROR 04-03 17:57:33 core.py:208]     output = self.compiled_callable(*args, **kwargs)\n> ERROR 04-03 17:57:33 core.py:208]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 465, in _fn\n> ERROR 04-03 17:57:33 core.py:208]     return fn(*args, **kwargs)\n> ERROR 04-03 17:57:33 core.py:208]            ^^^^^^^^^^^^^^^^^^^\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/vllm/model_executor/models/qwen2.py\", line 327, in forward\n> ERROR 04-03 17:57:33 core.py:208]     def forward(\n> ERROR 04-03 17:57:33 core.py:208]     \n> ERROR 04-03 17:57:33 core.py:208]   File \"<stringsource>\", line 69, in cfunc.to_py.__Pyx_CFunc_893235__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_18instruction_offset.wrap\n> ERROR 04-03 17:57:33 core.py:208]   File \"_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx\", line 1702, in _pydevd_sys_monitoring_cython._start_method_event\n> ERROR 04-03 17:57:33 core.py:208]   File \"_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx\", line 570, in _pydevd_sys_monitoring_cython._get_func_code_info\n> ERROR 04-03 17:57:33 core.py:208]   File \"_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx\", line 505, in _pydevd_sys_monitoring_cython._get_code_line_info\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1269, in __call__\n> ERROR 04-03 17:57:33 core.py:208]     return self._torchdynamo_orig_callable(\n> ERROR 04-03 17:57:33 core.py:208]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 492, in __call__\n> ERROR 04-03 17:57:33 core.py:208]     unimplemented(\"generator\")\n> ERROR 04-03 17:57:33 core.py:208]   File \"/home/cju/aigc/vllm/.venv/lib/python3.12/site-packages/torch/_dynamo/exc.py\", line 297, in unimplemented\n> ERROR 04-03 17:57:33 core.py:208]     raise Unsupported(msg, case_name=case_name)\n> ERROR 04-03 17:57:33 core.py:208] torch._dynamo.exc.Unsupported: generator\n> ERROR 04-03 17:57:33 core.py:208] \n> CRITICAL 04-03 17:57:33 core_client.py:156] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "open",
    "created_at": "2025-04-03T10:13:52+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16006/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16006"
  },
  {
    "number": 15360,
    "title": "[Bug]: vLLM v1 hanging during Torch compilation",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-1077-aws-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA A10G\nNvidia driver version: 535.161.07\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               8\nOn-line CPU(s) list:                  0-7\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 7R32\nCPU family:                           23\nModel:                                49\nThread(s) per core:                   2\nCore(s) per socket:                   4\nSocket(s):                            1\nStepping:                             0\nBogoMIPS:                             5599.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save rdpid\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            128 KiB (4 instances)\nL1i cache:                            128 KiB (4 instances)\nL2 cache:                             2 MiB (4 instances)\nL3 cache:                             16 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-7\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\nVulnerability Spec rstack overflow:   Mitigation; safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.555.43\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] nvidia-pytriton==0.5.14\n[pip3] optree==0.12.1\n[pip3] pyzmq==26.3.0\n[pip3] sentence-transformers==2.7.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torcheval==0.0.7\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.50.0\n[pip3] triton==3.2.0\n[pip3] tritonclient==2.55.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \t0-7\t0\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNCCL_SOCKET_IFNAME=eth\nLOCAL_RANK=0\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nExperiencing a hanging issue when running the offline LLM API. Using Qwen2.5-7b previously saved to disk.\n(Per the [docs](https://docs.vllm.ai/en/latest/design/multiprocessing.html), I set the multiproc method to spawn to avoid CUDA reinitialization error).\n\n```python\nimport os\nos.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n\nfrom vllm import LLM\n\nllm = LLM(model=\"Qwen/Qwen2.5-7B-Instruct\", gpu_memory_utilization=0.9, max_model_len=6600)\n```\n\nThe above hangs after `Dynamo bytecode transform time`, when I believe Torch compilation should occur:\n\n```text\n/databricks/python/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n  warnings.warn(\n2025-03-23 16:29:28.424785: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nINFO 03-23 16:29:35 [__init__.py:256] Automatically detected platform cuda.\nINFO 03-23 16:29:50 [config.py:583] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting to 'generate'.\nINFO 03-23 16:29:50 [config.py:1693] Chunked prefill is enabled with max_num_batched_tokens=8192.\n/databricks/python/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n  warnings.warn(\n2025-03-23 16:29:57.856557: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nINFO 03-23 16:29:59 [__init__.py:256] Automatically detected platform cuda.\nINFO 03-23 16:30:02 [core.py:53] Initializing a V1 LLM engine (v0.8.1) with config: model='/dbfs/FileStore/spark-dl-models/qwen-2.5-7b', speculative_config=None, tokenizer='/dbfs/FileStore/spark-dl-models/qwen-2.5-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=6600, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/dbfs/FileStore/spark-dl-models/qwen-2.5-7b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\nWARNING 03-23 16:30:03 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ff7addf3150>\nINFO 03-23 16:30:04 [parallel_state.py:967] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 03-23 16:30:04 [cuda.py:215] Using Flash Attention backend on V1 engine.\nINFO 03-23 16:30:04 [gpu_model_runner.py:1164] Starting to load model /dbfs/FileStore/spark-dl-models/qwen-2.5-7b...\nWARNING 03-23 16:30:04 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:06<00:18,  6.14s/it]\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:14<00:14,  7.33s/it]\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:24<00:08,  8.65s/it]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:35<00:00,  9.55s/it]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:35<00:00,  8.86s/it]\n\nINFO 03-23 16:30:40 [loader.py:429] Loading weights took 35.65 seconds\nINFO 03-23 16:30:40 [gpu_model_runner.py:1176] Model loading took 14.2487 GB and 36.225706 seconds\nINFO 03-23 16:30:52 [backends.py:409] Using cache directory: /root/.cache/vllm/torch_compile_cache/cdb7480a5d/rank_0_0 for vLLM's torch.compile\nINFO 03-23 16:30:52 [backends.py:419] Dynamo bytecode transform time: 11.24 s\n(CELL HANGS HERE)\n```\n\nSetting VLLM_DISABLE_COMPILE_CACHE=1 does not resolve the issue.\n\nHowever, **disabling the v1 engine** or **setting enforce_eager=True** will resolve the issue:\n\n```python\nimport os\nos.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\nos.environ[\"VLLM_USE_V1\"] = \"0\"\n\nfrom vllm import LLM\n\nllm = LLM(model=\"Qwen/Qwen2.5-7B-Instruct\", gpu_memory_utilization=0.9, max_model_len=6600)\n```\n\nThe model loads successfully:\n```\n/databricks/python/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n  warnings.warn(\n2025-03-23 16:33:43.628045: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nINFO 03-23 16:33:50 [__init__.py:256] Automatically detected platform cuda.\nINFO 03-23 16:34:05 [config.py:583] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting to 'generate'.\nINFO 03-23 16:34:05 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.1) with config: model='/dbfs/FileStore/spark-dl-models/qwen-2.5-7b', speculative_config=None, tokenizer='/dbfs/FileStore/spark-dl-models/qwen-2.5-7b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=6600, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/dbfs/FileStore/spark-dl-models/qwen-2.5-7b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \nINFO 03-23 16:34:07 [cuda.py:285] Using Flash Attention backend.\nINFO 03-23 16:34:07 [parallel_state.py:967] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 03-23 16:34:07 [model_runner.py:1110] Starting to load model /dbfs/FileStore/spark-dl-models/qwen-2.5-7b...\nINFO 03-23 16:34:40 [loader.py:429] Loading weights took 32.38 seconds\nINFO 03-23 16:34:40 [model_runner.py:1146] Model loading took 14.2487 GB and 32.737843 seconds\nINFO 03-23 16:34:44 [worker.py:267] Memory profiling takes 3.04 seconds\nINFO 03-23 16:34:44 [worker.py:267] the current vLLM instance can use total_gpu_memory (21.99GiB) x gpu_memory_utilization (0.90) = 19.79GiB\nINFO 03-23 16:34:44 [worker.py:267] model weights take 14.25GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.43GiB; the rest of the memory reserved for KV Cache is 4.05GiB.\nINFO 03-23 16:34:44 [executor_base.py:111] # cuda blocks: 4741, # CPU blocks: 4681\nINFO 03-23 16:34:44 [executor_base.py:116] Maximum concurrency for 6600 tokens per request: 11.49x\nINFO 03-23 16:34:54 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nCapturing CUDA graph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:35<00:00,  1.01s/it]INFO 03-23 16:35:29 [model_runner.py:1570] Graph capturing finished in 36 secs, took 0.76 GiB\nINFO 03-23 16:35:29 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 48.96 seconds\n```\n\nNote that i encountered this issue running on a Databricks instance\u2014when running locally or on Dataproc with the same torch/vllm versions, the v1 engine compiles the model successfully. Any insight on this would be greatly appreciated!\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile"
    ],
    "state": "open",
    "created_at": "2025-03-23T16:41:41+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15360/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15360"
  },
  {
    "number": 14703,
    "title": "[WIP][RFC]: Use auto-functionalization V2 in PyTorch 2.7+",
    "body": "### Motivation\n\nIn PyTorch 2.6, `auto_functionalized_v2` was introduced as a replacement for the `auto_functionalized` higher-order, partially to address the issues with redundant tensor copies in vLLM. However, certain custom fusion passes rely on pattern matching and don't currently work with `auto_functionalized_v2`.\n\nDue to this as well as a separate issue with V2 ([PyTorch#147924](https://github.com/pytorch/pytorch/issues/147924)), we are currently disabling V2 in PyTorch 2.6+. We have also circumvented the copy issues using a `FixFunctionalizationPass`, reducing the urgency for enabling V2.\n\nI am creating this RFC to centralize the discussion about when to upgrade to V2 and how to mitigate it in custom fusion passes.\n\n#### Motivation for custom passes\n\nOur graph-level optimization system performs graph transformations that would break abstractions or be intrusive to model code in some other way. For example, `RMSNormFusionPass` performs manual fusion of RMSNorm and quantization custom ops. A simplified model definition looks like this, whether quantization is enabled or not:\n```\nx2 = RMSNorm(x1)\nx3 = Linear(x2)\n```\nIf quantization is on, `Linear` consists of a `quant` followed by a `mm` operation. With `RMSNormFusionPass` we fuse the `quant` onto the `rms_norm` during Inductor compilation, resulting in performance gains without breaking the `Linear` abstraction.\n\nBecause `rms_norm` is in-place, we have to pattern-match the `auto_functionalized` op in the FX graph.\n\n#### Why upgrading is not simple\n\n`auto_functionalized_v2` contains additional arguments ([more info](https://dev-discuss.pytorch.org/t/a-new-strategy-for-automatic-custom-operators-functionalization/2733)) to better track tensors and their views to correctly re-inplace the functionalized ops and avoid redundant copies. (They're planning on [adding even more arguments](https://github.com/pytorch/pytorch/issues/138220) soon.)\n\nThis makes pattern matching against it more complex (if not impossible) than the current version. If we want to enable V2, we need to choose a strategy for pattern matching and replacement.\n\n### Proposed Change\n\nAfter discussing this with @zou3519, I see a few possible approaches to pattern matching in-place ops. Please feel free to suggest any others.\n#### 1. Match patterns before functionalization.\n\nWe ran into problems with this in the past, but @zou3519 says it should work\u2122. We should certainly give it a try, as it would make our patterns simpler as well. With custom replacement infrastructure, we could also manually edit the graph instead of using the pattern matcher, and the current code is basically already able to do this.\n\nWe might have to do noop-elimination on a non-functional graph (redundant slices & reshapes) to avoid imposing restriction on model code (whole point of custom fusion passes). Because the graph is also [not normalized or stable](https://github.com/pytorch/pytorch/blob/42aeb5d2596f3bbb05facdbfc5f66d149a0bc191/torch/_inductor/config.py#L206-L209), this might be hard.\n\n#### 2. Match patterns after Inductor re-inplaces functionalized ops. \n\nThis would require a new custom hook point from the PyTorch team. Otherwise it has similar pros/cons to option 1, except the noop elimination will have happened already.\n\n#### 3. Introduce functional custom ops.\nInstead of wrapping RMSNorm inside auto_functionalized_v2, define a custom rms_norm_functional higher-order operator. Pattern matching would occur on rms_norm_functional + quant instead. \n\nRight now, it seems like approach 2 is best. If we agree on this, it might be worth asking PyTorch for a custom post-functionalization hook (unless I'm missing something and this already exists).\n\n### Feedback Period.\n\nThis is not urgent yet, we can set a timeline when we establish this to be a priority.\n\n### CC List.\n\n@youkaichao @zou3519 @SageMoore @bnellnm @tms @robertgshaw2-redhat \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "torch.compile",
      "unstale"
    ],
    "state": "open",
    "created_at": "2025-03-12T21:19:52+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14703/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14703"
  }
]
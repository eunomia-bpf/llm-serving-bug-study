[
  {
    "number": 4163,
    "title": "[Installation]: import llm meet error",
    "body": "### Your current environment\n\n```text\r\nTraceback (most recent call last):\r\n  File \"inference.py\", line 355, in <module>\r\n    data_all_with_response = get_pred_func(data=data_all, task_prompt=task_prompt,\\\r\n  File \"inference.py\", line 24, in get_pred_vllm\r\n    from vllm import LLM, SamplingParams\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/__init__.py\", line 3, in <module>\r\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/engine/arg_utils.py\", line 6, in <module>\r\n    from vllm.config import (CacheConfig, ModelConfig, ParallelConfig,\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/config.py\", line 9, in <module>\r\n    from vllm.utils import get_cpu_memory, is_hip\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/utils.py\", line 8, in <module>\r\n    from vllm._C import cuda_utils\r\nImportError: /usr/local/lib/python3.8/dist-packages/vllm/_C.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN2at4_ops15to_dtype_layout4callERKNS_6TensorEN3c108optionalINS5_10ScalarTypeEEENS6_INS5_6LayoutEEENS6_INS5_6DeviceEEENS6_IbEEbbNS6_INS5_12MemoryFormatEEE```\r\n\n\n### How you are installing vllm\n\n```sh\r\nexport VLLM_VERSION=0.2.4\r\nexport PYTHON_VERSION=38\r\npip install https://github.com/vllm-project/vllm/releases/download/v${VLLM_VERSION}/vllm-${VLLM_VERSION}+cu118-cp${PYTHON_VERSION}-cp${PYTHON_VERSION}-manylinux1_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu118\r\n```\r\n",
    "labels": [
      "installation",
      "unstale"
    ],
    "state": "closed",
    "created_at": "2024-04-18T07:06:57+00:00",
    "closed_at": "2025-01-14T13:57:43+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4163/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4163"
  },
  {
    "number": 9960,
    "title": "[Installation]: I was never able to install it, which cuda version is required?",
    "body": "### Your current environment\r\n\r\nI use ubunt 22.04\r\n\r\nInstalling this is almost impossible, what are actually requirements lets say for cuda. I spend many hours trying to install and never worked, there way always an error, something related to cuda version.\r\n\r\n\r\n### How you are installing vllm\r\n\r\n```sh\r\npip install -vvv vllm\r\n```\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-02T22:53:06+00:00",
    "closed_at": "2025-03-03T02:03:22+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9960/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9960"
  },
  {
    "number": 10036,
    "title": "[Installation]: Missing v0.6.3.post1-cu118-cp310.whl. Can share it? Thanks so much",
    "body": "### Your current environment\n\nMissing v0.6.3.post1-cu118-cp310.whl. Can share it? Thanks so much\n\n### How you are installing vllm\n\nMissing v0.6.3.post1-cu118-cp310.whl. Can share it? Thanks so much\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "unstale"
    ],
    "state": "closed",
    "created_at": "2024-11-05T12:46:28+00:00",
    "closed_at": "2025-04-15T03:15:12+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10036/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10036"
  },
  {
    "number": 14033,
    "title": "[Installation]: Dockerfile.cpu installation problem vLLM",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\n```\nDockerfile.cpu installation I can't complete the build somehow, I want to use vLLM over CPU since I don't have a graphics card on my own server, but the installation gives an error as follows.\nOS= rockylinux 9.4 \nram 16gb\nvCPU=> 24\nHypervisor= proxmox 8.2.7\ndocker version => Docker version 28.0.1, build 068a01e\n\nerrors messages;\n\ndocker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .\n\nDockerfile.cpu:54\n--------------------\n  53 |     \n  54 | >>> RUN --mount=type=cache,target=/root/.cache/pip \\\n  55 | >>>     --mount=type=cache,target=/root/.cache/ccache \\\n  56 | >>>     --mount=type=bind,source=.git,target=.git \\\n  57 | >>>     VLLM_TARGET_DEVICE=cpu python3 setup.py bdist_wheel && \\\n  58 | >>>     pip install dist/*.whl && \\\n  59 | >>>     rm -rf dist\n  60 |     \n--------------------\nERROR: failed to solve: process \"/bin/sh -c VLLM_TARGET_DEVICE=cpu python3 setup.py bdist_wheel &&     pip install dist/*.whl &&     rm -rf dist\" did not complete successfully: exit code: 1\n\n### How you are installing vllm\n\ndocker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-28T10:02:14+00:00",
    "closed_at": "2025-07-02T02:14:03+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14033/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14033"
  },
  {
    "number": 9385,
    "title": "[Installation]: Installation instructions for ROCm can be mainlined",
    "body": "### Your current environment\n\nN/A\r\n\n\n### How you are installing vllm\n\nhttps://docs.vllm.ai/en/stable/getting_started/amd-installation.html option 2\r\n\r\nThe problem is that it says to checkout a very specific commit of triton. Triton just published a new version of 3.1 that has AMD support mainlined but the dependency in the vllm pip package still tries to install 3.0. If someone tells me how I can update the dependency on 3.1 we can simplify the AMD instructions I think.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-15T18:21:50+00:00",
    "closed_at": "2025-02-13T01:59:19+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9385/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9385"
  },
  {
    "number": 436,
    "title": "Running to (Installing build dependencies ) this step is stuck",
    "body": "![image](https://github.com/vllm-project/vllm/assets/54533917/99997cfa-f6f6-4de1-9641-0e4c90884256)\r\n\r\n\r\nsystem\uff1a ubuntu 20.04\r\nNvidia driver 515\r\ncuda 11.7\r\nrtx3090\r\npython 3.8\r\nvllm 0.1.2",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2023-07-12T03:08:05+00:00",
    "closed_at": "2024-03-08T12:27:03+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/436/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/436"
  },
  {
    "number": 6769,
    "title": "[Installation]: Unable to build docker image using Dockerfile.openvino",
    "body": "### Your current environment\n\n```text\r\n(base) user@zahid:~/vllm$ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.2 LTS (x86_64)\r\nGCC version: Could not collect\r\nClang version: Could not collect\r\nCMake version: version 3.30.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.12.1 | packaged by Anaconda, Inc. | (main, Jan 19 2024, 15:51:05) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1026-intel-iotg-x86_64-with-glibc2.35\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: N/A\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Gold 6338N CPU @ 2.20GHz\r\nCPU family:                      6\r\nModel:                           106\r\nThread(s) per core:              2\r\nCore(s) per socket:              32\r\nSocket(s):                       2\r\nStepping:                        6\r\nCPU max MHz:                     3500.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        4400.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\r\nVirtualization:                  VT-x\r\nL1d cache:                       3 MiB (64 instances)\r\nL1i cache:                       2 MiB (64 instances)\r\nL2 cache:                        80 MiB (64 instances)\r\nL3 cache:                        96 MiB (2 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126\r\nNUMA node1 CPU(s):               1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n\r\n```\r\n\n\n### How you are installing vllm\n\nTried building vllm:openvino image using [Dockerfile.openvino](https://github.com/vllm-project/vllm/blob/main/Dockerfile.openvino). However, the docker image build is failing with below error.\r\n\r\n```sh\r\ndocker build -t vllm:openvino -f Dockerfile.openvino .\r\n```\r\n\r\n\r\nBelow is the docker build error:\r\n\r\n--------------------\r\n\r\n[ 3/12] WORKDIR /workspace                                                                                           0.0s\r\n[ 4/12] COPY requirements-build.txt /workspace/vllm/                                                                 0.0s\r\n[ 5/12] COPY requirements-common.txt /workspace/vllm/                                                                0.0s\r\n[ 6/12] COPY requirements-openvino.txt /workspace/vllm/                                                              0.0s\r\n[ 7/12] COPY vllm/ /workspace/vllm/vllm                                                                              0.1s\r\n[ 8/12] COPY setup.py /workspace/vllm/                                                                               0.0s\r\nERROR [ 9/12] RUN PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cpu\" python3 -m pip install -r /workspace/  21.9s\r\n > [ 9/12] RUN PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cpu\" python3 -m pip install -r /workspace/vllm/requirements-build.txt:\r\n1.698 Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\r\n3.670 Collecting cmake>=3.21\r\n3.779   Downloading cmake-3.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.9 MB)\r\n6.573 Collecting ninja\r\n6.589   Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\r\n7.599 Collecting packaging\r\n7.612   Downloading packaging-24.1-py3-none-any.whl (53 kB)\r\n9.064 Collecting setuptools>=49.4.0\r\n9.077   Downloading setuptools-71.1.0-py3-none-any.whl (2.3 MB)\r\n11.10 Collecting torch==2.3.1\r\n11.11   Downloading https://download.pytorch.org/whl/cpu/torch-2.3.1%2Bcpu-cp38-cp38-linux_x86_64.whl (190.4 MB)\r\n20.24 Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from -r /workspace/vllm/requirements-build.txt (line 7)) (0.34.2)\r\n21.21 Collecting networkx\r\n21.22   Downloading https://download.pytorch.org/whl/networkx-3.2.1-py3-none-any.whl (1.6 MB)\r\n21.78 ERROR: Package 'networkx' requires a different Python: 3.8.10 not in '>=3.9'\r\n\r\nDockerfile.openvino:19\r\n\r\n  17 |\r\n  18 |     # install build requirements\r\n  19 | >>> RUN PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cpu\" python3 -m pip install -r /workspace/vllm/requirements-build.txt\r\n  20 |     # build vLLM with OpenVINO backend\r\n  21 |     RUN PIP_PRE=1 PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cpu https://storage.openvinotoolkit.org/simple/wheels/nightly/\" VLLM_TARGET_DEVICE=\"openvino\" python3 -m pip install /workspace/vllm/\r\n\r\nERROR: failed to solve: process \"/bin/sh -c PIP_EXTRA_INDEX_URL=\\\"https://download.pytorch.org/whl/cpu\\\" python3 -m pip install -r /workspace/vllm/requirements-build.txt\" did not complete successfully: exit code: 1\r\n\r\n\r\n--------------------\r\n\r\nThe build was working properly until last week, but issues began arising starting from this commit: 1689219ebf01c750de492271832e27c39df38648.\r\n \r\n[[CI/Build] Build on Ubuntu 20.04 instead of 22.04 (](https://github.com/vllm-project/vllm/commit/1689219ebf01c750de492271832e27c39df38648)[#6517](https://github.com/vllm-project/vllm/pull/6517)[)](https://github.com/vllm-project/vllm/commit/1689219ebf01c750de492271832e27c39df38648)",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2024-07-25T04:05:59+00:00",
    "closed_at": "2024-07-30T18:33:02+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6769/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6769"
  },
  {
    "number": 7025,
    "title": "[Installation]: Could not install packages due to an OSError: [Errno 28] No space left on device but disk still have space",
    "body": "### Your current environment\n\n![image](https://github.com/user-attachments/assets/b25198d8-8530-49a1-b116-9882b5fb5977)\r\ni install vllm in /mnt , i found is still have space but it has a wrong like:\r\n\r\nInstalling build dependencies ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 pip subprocess to install build dependencies did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [47 lines of output]\r\n      Looking in indexes: http://mirrors.cloud.aliyuncs.com/pypi/simple/\r\n      Collecting cmake>=3.21\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/78/5e/c274ffd124b8d4d95734af94c1080f0421c89dabdea2475651a7bd1e02ca/cmake-3.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.9 MB)\r\n      Collecting ninja\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/6d/92/8d7aebd4430ab5ff65df2bfee6d5745f95c004284db2d8ca76dcbfd9de47/ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\r\n      Collecting packaging\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/08/aa/cc0199a5f0ad350994d660967a8efb233fe0416e4639146c089643407ce6/packaging-24.1-py3-none-any.whl (53 kB)\r\n      Collecting setuptools>=49.4.0\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/e1/58/e0ef3b9974a04ce9cde2a7a33881ddcb2d68450803745804545cdd8d258f/setuptools-72.1.0-py3-none-any.whl (2.3 MB)\r\n      Collecting torch==2.3.0\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/51/03/1abad10990c76bee3703857b1617563b241f87d297ee466dbad922b0c308/torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl (779.1 MB)\r\n      Collecting wheel\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/7d/cd/d7460c9a869b16c3dd4e1e403cce337df165368c71d6af229a74699622ce/wheel-0.43.0-py3-none-any.whl (65 kB)\r\n      Collecting filelock (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/ae/f0/48285f0262fe47103a4a45972ed2f9b93e4c80b8fd609fa98da78b2a5706/filelock-3.15.4-py3-none-any.whl (16 kB)\r\n      Collecting typing-extensions>=4.8.0 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl (37 kB)\r\n      Collecting sympy (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/b2/fe/81695a1aa331a842b582453b605175f419fe8540355886031328089d840a/sympy-1.13.1-py3-none-any.whl (6.2 MB)\r\n      Collecting networkx (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/38/e9/5f72929373e1a0e8d142a130f3f97e6ff920070f87f91c4e13e40e0fba5a/networkx-3.3-py3-none-any.whl (1.7 MB)\r\n      Collecting jinja2 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/31/80/3a54838c3fb461f6fec263ebf3a3a41771bd05190238de3486aae8540c36/jinja2-3.1.4-py3-none-any.whl (133 kB)\r\n      Collecting fsspec (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/5e/44/73bea497ac69bafde2ee4269292fa3b41f1198f4bb7bbaaabde30ad29d4a/fsspec-2024.6.1-py3-none-any.whl (177 kB)\r\n      Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/b6/9f/c64c03f49d6fbc56196664d05dba14e3a561038a81a638eeb47f4d4cfd48/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\r\n      Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/eb/d5/c68b1d2cdfcc59e72e8a5949a37ddb22ae6cade80cd4a57a84d4c8b55472/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\r\n      Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/7e/00/6b218edd739ecfc60524e585ba8e6b00554dd908de2c9c66c1af3e44e18d/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\r\n      Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\r\n      Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/37/6d/121efd7382d5b0284239f4ab1fc1590d86d34ed4a4a2fdb13b30ca8e5740/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\r\n      Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/86/94/eb540db023ce1d162e7bea9f8f5aa781d57c65aed513c33ee9a5123ead4d/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\r\n      Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/44/31/4890b1c9abc496303412947fc7dcea3d14861720642b49e8ceed89636705/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\r\n      Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/bc/1d/8de1e5c67099015c834315e333911273a8c6aaba78923dd1d1e25fc5f217/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\r\n      Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/65/5b/cfaeebf25cd9fdec14338ccb16f6b2c4c7fa9163aefcf057d86b9cc248bb/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\r\n      Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/4b/2a/0a131f572aa09f741c30ccd45a8e56316e8be8dfc7bc19bf0ab7cfef7b19/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\r\n      ERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\r\n      \r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: subprocess-exited-with-error\r\n\r\n\u00d7 pip subprocess to install build dependencies did not run successfully.\r\n\u2502 exit code: 1\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This error originates from a subprocess, and is likely not a problem with pip.\n\n### How you are installing vllm\n\npip install vllm\r\n",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-01T09:24:12+00:00",
    "closed_at": "2024-12-01T02:14:15+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7025/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7025"
  },
  {
    "number": 11037,
    "title": "[Installation]: no version of pip install vllm works - Failed to initialize NumPy: No Module named 'numpy'",
    "body": "### Your current environment\n\n```text\r\nTraceback (most recent call last):\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/collect_env.py\", line 15, in <module>\r\n    from vllm.envs import environment_variables\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/__init__.py\", line 3, in <module>\r\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/engine/arg_utils.py\", line 11, in <module>\r\n    from vllm.config import (CacheConfig, CompilationConfig, ConfigFormat,\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/config.py\", line 21, in <module>\r\n    from vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/model_executor/__init__.py\", line 1, in <module>\r\n    from vllm.model_executor.parameter import (BasevLLMParameter,\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/model_executor/parameter.py\", line 7, in <module>\r\n    from vllm.distributed import get_tensor_model_parallel_rank\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/distributed/__init__.py\", line 1, in <module>\r\n    from .communication_op import *\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/distributed/communication_op.py\", line 6, in <module>\r\n    from .parallel_state import get_tp_group\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/distributed/parallel_state.py\", line 38, in <module>\r\n    import vllm.distributed.kv_transfer.kv_transfer_agent as kv_transfer\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/distributed/kv_transfer/kv_transfer_agent.py\", line 15, in <module>\r\n    from vllm.distributed.kv_transfer.kv_connector.factory import (\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/distributed/kv_transfer/kv_connector/factory.py\", line 3, in <module>\r\n    from .base import KVConnectorBase\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/distributed/kv_transfer/kv_connector/base.py\", line 14, in <module>\r\n    from vllm.sequence import IntermediateTensors\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/sequence.py\", line 16, in <module>\r\n    from vllm.inputs import SingletonInputs, SingletonInputsAdapter\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/inputs/__init__.py\", line 7, in <module>\r\n    from .registry import (DummyData, InputContext, InputProcessingContext,\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/inputs/registry.py\", line 13, in <module>\r\n    from vllm.transformers_utils.tokenizer import AnyTokenizer\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/transformers_utils/tokenizer.py\", line 16, in <module>\r\n    from vllm.utils import make_async\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/utils.py\", line 44, in <module>\r\n    from vllm.platforms import current_platform\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/platforms/__init__.py\", line 100, in <module>\r\n    from .cuda import CudaPlatform\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/platforms/cuda.py\", line 14, in <module>\r\n    import vllm._C  # noqa\r\nModuleNotFoundError: No module named 'vllm._C'\r\n\r\n```\r\n\n\n### How you are installing vllm\n\n```\r\npip install -vvv vllm\r\npip install vllm==0.6.4\r\npip install -e .\r\n```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "open",
    "created_at": "2024-12-09T22:11:26+00:00",
    "closed_at": null,
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11037/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11037"
  },
  {
    "number": 15550,
    "title": "[Installation]: Transformer installation requires uv venv --system now",
    "body": "Hi all, a small one I can take care of is a breaking change introduced in \n\nhttps://github.com/vllm-project/vllm/commit/7ffcccfa5ca3ef6b56c292ad2489e077a5cdd6f5#diff-dd2c0eb6ea5cfc6c4bd4eac30934e2d5746747af48fef6da689e85b752f39557R62\n\nThe installation instructions in [here](https://docs.vllm.ai/en/latest/deployment/docker.html#deployment-docker-pre-built-image) should probably include `--system` as in:\n\n`RUN uv pip install --system git+https://github.com/huggingface/transformers.git`\n\nThanks for your hard work!\n\n\n### How you are installing vllm\n\n```sh\npip install -vvv vllm\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2025-03-26T13:56:48+00:00",
    "closed_at": "2025-03-27T12:38:47+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15550/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15550"
  },
  {
    "number": 9420,
    "title": "[Installation]:  issue with docker setup",
    "body": "### Your current environment\n\ni am using unbuntu 22.04  having gpu of 16GB and ram of 34GB \nmodel is already download and reside at /home/ids/llm_models/zephyr\nbut after using below command I am getting error :\ndocker run --runtime nvidia --gpus all \\\n    -v /home/ids/llm_models/zephyr:/root/.cache/huggingface \\\n    --env \"HUGGING_FACE_HUB_TOKEN=<key>\" \\\n    -p 8080:8080 \\\n    --ipc=host \\\n    vllm/vllm-openai:latest \\\n    --model \"/root/.cache/huggingface/zephyr-7b-alpha.Q8_0.gguf\" \\\n    --load-format \"gguf\" \\\n    --dtype \"float16\" \\\n    --quantization \"gguf\" \\\n    --cpu-offload-gb 10 \\\n    --gpu-memory-utilization 0.5 \\\n    --max_seq_len_to_capture 4096 \\\n    --max-num-batched-tokens 4096 \\\n    --enable-chunked-prefill \\\n    --max-num-seqs 256 \\\n    --served-model-name \"zephyr-7b-alpha\"\n\n\n\"\"\"\nNo module named 'vllm._version'\n  from vllm.version import __version__ as VLLM_VERSION\nINFO 10-16 05:09:17 api_server.py:528] vLLM API server version dev\nINFO 10-16 05:09:17 api_server.py:529] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/root/.cache/huggingface/zephyr-7b-alpha.Q8_0.gguf', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='gguf', config_format='auto', dtype='float16', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=10.0, gpu_memory_utilization=0.5, num_gpu_blocks_override=None, max_num_batched_tokens=4096, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization='gguf', rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=4096, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=True, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['zephyr-7b-alpha'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False)\nINFO 10-16 05:09:17 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/fc60c1e8-f6d2-4de6-a797-276510d5f310 for IPC Path.\nINFO 10-16 05:09:17 api_server.py:179] Started engine process with PID 33\n/usr/local/lib/python3.12/dist-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\nNo module named 'vllm._version'\n  from vllm.version import __version__ as VLLM_VERSION\nINFO 10-16 05:09:22 config.py:1670] Downcasting torch.float32 to torch.float16.\nINFO 10-16 05:09:26 config.py:1670] Downcasting torch.float32 to torch.float16.\nWARNING 10-16 05:09:26 config.py:306] gguf quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nINFO 10-16 05:09:26 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=4096.\nYou are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\nWARNING 10-16 05:09:30 config.py:306] gguf quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nINFO 10-16 05:09:30 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=4096.\nINFO 10-16 05:09:30 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/root/.cache/huggingface/zephyr-7b-alpha.Q8_0.gguf', speculative_config=None, tokenizer='/root/.cache/huggingface/zephyr-7b-alpha.Q8_0.gguf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.GGUF, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gguf, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=zephyr-7b-alpha, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, mm_processor_kwargs=None)\nMerges were not in checkpoint, building merges on the fly.\n  6%|\u258b         | 2045/32000 [00:01<00:17, 1668.14it/s]You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n 15%|\u2588\u258c        | 4891/32000 [00:03<00:23, 1161.38it/s]Merges were not in checkpoint, building merges on the fly.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 32000/32000 [00:31<00:00, 1019.16it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 32000/32000 [00:31<00:00, 1015.99it/s]\nINFO 10-16 05:10:11 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 10-16 05:10:11 selector.py:115] Using XFormers backend.\n/usr/local/lib/python3.12/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n/usr/local/lib/python3.12/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\nINFO 10-16 05:10:11 model_runner.py:1060] Starting to load model /root/.cache/huggingface/zephyr-7b-alpha.Q8_0.gguf...\nINFO 10-16 05:10:13 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 10-16 05:10:13 selector.py:115] Using XFormers backend.\n/usr/local/lib/python3.12/dist-packages/torch/nested/__init__.py:220: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n  return _nested.nested_tensor(\nINFO 10-16 05:10:19 model_runner.py:1071] Loading model weights took 0.2672 GB\nINFO 10-16 05:10:19 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20241016-051019.pkl...\nINFO 10-16 05:10:19 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20241016-051019.pkl.\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner_base.py\", line 116, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1665, in execute_model\n    hidden_or_intermediate_states = model_executable(\n                                    ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 556, in forward\n    model_output = self.model(input_ids, positions, kv_caches,\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 345, in forward\n    hidden_states, residual = layer(positions, hidden_states,\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 378, in forward\n    k: v.to(device, non_blocking=True)\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 392, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 141, in from_engine_args\n    return cls(\n           ^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 78, in __init__\n    self.engine = LLMEngine(*args,\n                  ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 349, in __init__\n    self._initialize_kv_caches()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 484, in _initialize_kv_caches\n    self.model_executor.determine_num_available_blocks())\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/gpu_executor.py\", line 114, in determine_num_available_blocks\n    return self.driver_worker.determine_num_available_blocks()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 223, in determine_num_available_blocks\n    self.model_runner.profile_run()\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1309, in profile_run\n    self.execute_model(model_input, kv_caches, intermediate_tensors)\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner_base.py\", line 152, in _wrapper\n    raise type(err)(\nRuntimeError: Error in model execution (input dumped to /tmp/err_execute_model_input_20241016-051019.pkl): CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 585, in <module>\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 552, in run_server\n    async with build_async_engine_client(args) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 107, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 194, in build_async_engine_client_from_engine_args\n    raise RuntimeError(\nRuntimeError: Engine process failed to start\n\"\"\"\n\n### How you are installing vllm\n\n\ndocker run --runtime nvidia --gpus all \\\n    -v /home/ids/llm_models/zephyr:/root/.cache/huggingface \\\n    --env \"HUGGING_FACE_HUB_TOKEN=\" \\\n    -p 8080:8080 \\\n    --ipc=host \\\n    vllm/vllm-openai:latest \\\n    --model \"/root/.cache/huggingface/zephyr-7b-alpha.Q8_0.gguf\" \\\n    --load-format \"gguf\" \\\n    --dtype \"float16\" \\\n    --quantization \"gguf\" \\\n    --cpu-offload-gb 10 \\\n    --gpu-memory-utilization 0.5 \\\n    --max_seq_len_to_capture 4096 \\\n    --max-num-batched-tokens 4096 \\\n    --enable-chunked-prefill \\\n    --max-num-seqs 256 \\\n    --served-model-name \"zephyr-7b-alpha\"\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-16T12:11:28+00:00",
    "closed_at": "2025-02-14T01:59:02+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9420/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9420"
  },
  {
    "number": 4913,
    "title": "[Installation]: Failed building editable for vllm",
    "body": "### Your current environment\n\n```\r\nThe output of `python collect_env.py`\r\n\r\n\r\nCollecting environment information...\r\n\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_ops_train.so.8\r\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn.so.8\r\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8\r\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8\r\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8\r\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8\r\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8\r\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn.so.8\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_adv_train.so.8\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_ops_train.so.8\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn.so.8\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_adv_train.so.8\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_ops_train.so.8\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: N/A\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             128\r\nOn-line CPU(s) list:                0-127\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz\r\nCPU family:                         6\r\nModel:                              106\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 32\r\nSocket(s):                          2\r\nStepping:                           6\r\nCPU max MHz:                        3400.0000\r\nCPU min MHz:                        800.0000\r\nBogoMIPS:                           5200.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr \r\nL1d cache:                          3 MiB (64 instances)\r\nL1i cache:                          2 MiB (64 instances)\r\nL2 cache:                           80 MiB (64 instances)\r\nL3 cache:                           96 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-31,64-95\r\nNUMA node1 CPU(s):                  32-63,96-127\r\nVulnerability Gather data sampling: Vulnerable: No microcode\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] No relevant packages\r\n[conda] No relevant packagesROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PXB     PXB     PXB     SYS     SYS     0-31,64-95      0               N/A\r\nGPU1    PXB      X      PXB     PXB     SYS     SYS     0-31,64-95      0               N/A\r\nGPU2    PXB     PXB      X      PIX     SYS     SYS     0-31,64-95      0               N/A\r\nGPU3    PXB     PXB     PIX      X      SYS     SYS     0-31,64-95      0               N/A\r\nGPU4    SYS     SYS     SYS     SYS      X      PIX     32-63,96-127    1               N/A\r\nGPU5    SYS     SYS     SYS     SYS     PIX      X      32-63,96-127    1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\n```\r\n\n\n### How you are installing vllm\n\n```sh\r\ngit clone https://github.com/vllm-project/vllm.git\r\ncd vllm\r\npip install -e .  \r\n```\r\n\r\n'''\r\nBuilding wheels for collected packages: vllm\r\n  Building editable for vllm (pyproject.toml) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Building editable for vllm (pyproject.toml) did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500\r\n      CMake Error at /tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/cmake/data/share/cmake-3.29/Modules/CMakeDetermineCompilerId.cmake:814 (message):\r\n        Compiling the CUDA compiler identification source file\r\n        \"CMakeCUDACompilerId.cu\" failed.\r\n      \r\n        Compiler: /usr/bin/nvcc\r\n      \r\n        Build flags:\r\n      \r\n        Id flags: --keep;--keep-dir;tmp -v\r\n      \r\n      \r\n      \r\n        The output was:\r\n      \r\n        255\r\n      \r\n        #$ _NVVM_BRANCH_=nvvm\r\n      \r\n        #$ _SPACE_=\r\n      \r\n        #$ _CUDART_=cudart\r\n      \r\n        #$ _HERE_=/usr/lib/nvidia-cuda-toolkit/bin\r\n      \r\n        #$ _THERE_=/usr/lib/nvidia-cuda-toolkit/bin\r\n      \r\n        #$ _TARGET_SIZE_=\r\n      \r\n        #$ _TARGET_DIR_=\r\n      \r\n        #$ _TARGET_SIZE_=64\r\n      \r\n        #$ NVVMIR_LIBRARY_DIR=/usr/lib/nvidia-cuda-toolkit/libdevice\r\n      \r\n        #$\r\n        PATH=/usr/lib/nvidia-cuda-toolkit/bin:/tmp/pip-build-env-l6d1bzk0/overlay/bin:/tmp/pip-build-env-l6d1bzk0/normal/bin:/usr/local/cuda/bin:/data2/fanbingbing/.conda/envs/vllm-embedding/bin:/usr/local/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\r\n      \r\n      \r\n        #$ LIBRARIES= -L/usr/lib/x86_64-linux-gnu/stubs -L/usr/lib/x86_64-linux-gnu\r\n      \r\n        #$ rm tmp/a_dlink.reg.c\r\n      \r\n        #$ gcc -D__CUDA_ARCH__=520 -D__CUDA_ARCH_LIST__=520 -E -x c++\r\n     \r\n        \"tmp/CMakeCUDACompilerId.cudafe1.gpu\" \"tmp/CMakeCUDACompilerId.cpp1.ii\" -o\r\n        \"tmp/CMakeCUDACompilerId.ptx\"\r\n      \r\n        #$ ptxas -arch=sm_52 -m64 \"tmp/CMakeCUDACompilerId.ptx\" -o\r\n        \"tmp/CMakeCUDACompilerId.sm_52.cubin\"\r\n      \r\n        ptxas tmp/CMakeCUDACompilerId.ptx, line 9; fatal : Unsupported .version\r\n        7.5; current version is '6.3'\r\n      \r\n        ptxas fatal : Ptx assembly aborted due to errors\r\n      \r\n        # --error 0xff --\r\n      \r\n      \r\n      \r\n      \r\n      \r\n      Call Stack (most recent call first):\r\n        /tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/cmake/data/share/cmake-3.29/Modules/CMakeDetermineCompilerId.cmake:8 (CMAKE_DETERMINE_COMPILER_ID_BUILD)\r\n        /tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/cmake/data/share/cmake-3.29/Modules/CMakeDetermineCompilerId.cmake:53 (__determine_compiler_id_test)\r\n        /tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/cmake/data/share/cmake-3.29/Modules/CMakeDetermineCUDACompiler.cmake:131 (CMAKE_DETERMINE_COMPILER_ID)\r\n        /tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/torch/share/cmake/Caffe2/public/cuda.cmake:47 (enable_language)\r\n        /tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:87 (include)\r\n        /tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\r\n        CMakeLists.txt:67 (find_package)\r\n      \r\n      \r\n      -- Configuring incomplete, errors occurred!\r\n      Traceback (most recent call last):\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/command/editable_wheel.py\", line 155, in run\r\n          self._create_wheel_file(bdist_wheel)\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/command/editable_wheel.py\", line 357, in _create_wheel_file\r\n          files, mapping = self._run_build_commands(dist_name, unpacked, lib, tmp)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/command/editable_wheel.py\", line 280, in _run_build_commands\r\n          self._run_build_subcommands()\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/command/editable_wheel.py\", line 307, in _run_build_subcommands\r\n          self.run_command(name)\r\n        File \"/tmp/pip-build-env-l6d1bzk0/over\r\n          _build_ext.run(self)\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\r\n          self.build_extensions()\r\n        File \"<string>\", line 192, in build_extensions\r\n        File \"<string>\", line 175, in configure\r\n        File \"/data2/fanbingbing/.conda/envs/vllm-embedding/lib/python3.11/subprocess.py\", line 413, in check_call\r\n          raise CalledProcessError(retcode, cmd)\r\n      subprocess.CalledProcessError: Command '['cmake', '/data2/fanbingbing/Segregation/LLaMA-embedding/vllm', '-G', 'Ninja', '-DCMAKE_BUILD_TYPE=RelWithDebInfo', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=/tmp/tmp6odosk48.build-lib/vllm', '-DCMAKE_ARCHIVE_OUTPUT_DIRECTORY=/tmp/tmpy8shcns0.build-temp', '-DVLLM_TARGET_DEVICE=cuda', '-DVLLM_PYTHON_EXECUTABLE=/data2/fanbingbing/.conda/envs/vllm-embedding/bin/python', '-DCMAKE_JOB_POOL_COMPILE:STRING=compile', '-DCMAKE_JOB_POOLS:STRING=compile=128']' returned non-zero exit status 1.\r\n      /tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py:988: _DebuggingTips: Problem in editable installation.\r\n      !!\r\n      \r\n              ********************************************************************************\r\n              An error happened while installing `vllm` in editable mode.\r\n      \r\n              The following steps are recommended to help debug this problem:\r\n      \r\n              - Try to install the project normally, without using the editable mode.\r\n                Does the error still persist?\r\n                (If it does, try fixing the problem before attempting the editable mode).\r\n              - If you are using binary extensions, make sure you have all OS-level\r\n                dependencies installed (e.g. compilers, toolchains, binary libraries, ...).\r\n              - Try the latest version of setuptools (maybe the error was already fixed).\r\n              - If you (or your project dependencies) are using any setuptools extension\r\n                or customization, make sure they support the editable mode.\r\n      \r\n              After following the steps above, if the problem still persists and\r\n              you think this is related to how setuptools handles editable installations,\r\n              please submit a reproducible example\r\n              (see https://stackoverflow.com/help/minimal-reproducible-example) to:\r\n      \r\n                  https://github.com/pypa/setuptools/issues\r\n      \r\n              See https://setuptools.pypa.io/en/latest/userguide/development_mode.html for details.\r\n              ********************************************************************************\r\n      \r\n      !!\r\n        cmd_obj.run()\r\n      Traceback (most recent call last):\r\n        File \"/data2/fanbingbing/.conda/envs/vllm-embedding/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\r\n          main()\r\n        File \"/data2/fanbingbing/.conda/envs/vllm-embedding/lib/python3.11/site-packages/pip/_vendor/py\r\n          return self._build_with_temp_dir(\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 395, in _build_with_temp_dir\r\n          self.run_setup()\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 311, in run_setup\r\n          exec(code, locals())\r\n        File \"<string>\", line 401, in <module>\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/__init__.py\", line 104, in setup\r\n          return distutils.core.setup(**attrs)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 184, in setup\r\n          return run_commands(dist)\r\n                 ^^^^^^^^^^^^^^^^^^\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 200, in run_commands\r\n          dist.run_commands()\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\r\n          self.run_command(cmd)\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 967, in run_command\r\n          super().run_command(command)\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\r\n          cmd_obj.run()\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/command/editable_wheel.py\", line 155, in run\r\n          self._create_wheel_file(bdist_wheel)\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/command/editable_wheel.py\", line 357, in _create_wheel_file\r\n          files, mapping = self._run_build_commands(dist_name, unpacked, lib, tmp)\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/command/editable_wheel.py\", line 280, in _run_build_commands\r\n          self._run_build_subcommands()\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/command/editable_wheel.py\", line 307, in _run_build_subcommands\r\n          self.run_command(name)\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\r\n          self.distribution.run_command(command)\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 967, in run_command\r\n          super().run_command(command)\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\r\n          cmd_obj.run()\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 91, in run\r\n          _build_ext.run(self)\r\n        File \"/tmp/pip-build-env-l6d1bzk0/overlay/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\r\n          self.build_extensions()\r\n        File \"<string>\", line 192, in build_extensions\r\n        File \"<string>\", line 175, in configure\r\n        File \"/data2/fanbingbing/.conda/envs/vllm-embedding/lib/python3.11/subprocess.py\", line 413, in check_call\r\n          raise CalledProcessError(retcode, cmd)\r\n      subprocess.CalledProcessError: Command '['cmake', '/data2/fanbingbing/Segregation/LLaMA-embedding/vllm', '-G', 'Ninja', '-DCMAKE_BUILD_TYPE=RelWithDebInfo', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=/tmp/tmp6odosk48.build-lib/vllm', '-DCMAKE_ARCHIVE_OUTPUT_DIRECTORY=/tmp/tmpy8shcns0.build-temp', '-DVLLM_TARGET_DEVICE=cuda', '-DVLLM_PYTHON_EXECUTABLE=/data2/fanbingbing/.conda/envs/vllm-embedding/bin/python', '-DCMAKE_JOB_POOL_COMPILE:STRING=compile', '-DCMAKE_JOB_POOLS:STRING=compile=128']' returned non-zero exit status 1.\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building editable for vllm\r\nFailed to build vllm\r\nERROR: Could not build wheels for vllm, which is required to install pyproject.toml-based projects\r\n'''",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2024-05-20T07:14:29+00:00",
    "closed_at": "2024-09-19T12:18:37+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4913/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4913"
  },
  {
    "number": 20483,
    "title": "[Installation]: How to install v0.9.2rc1 through Docker?",
    "body": "### Your current environment\n\nHow to install v0.9.2rc1 through Docker?\n\n\n### How you are installing vllm\n\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "open",
    "created_at": "2025-07-04T09:20:22+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20483/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20483"
  },
  {
    "number": 552,
    "title": "Building wheel for vllm (pyproject.toml) did not run successfully.",
    "body": "I'm trying to build a vLLM image on docker, but I keep getting an error while installing vllm module. I've tried with the given Dockerfile and I also try to build it from source but I keep getting this error. I'm not sure to understand what is the cause. \r\n\r\nAlso, the installation of vllm module takes an hour, while it says it should take around 5/10min. It might come from my slow internet connexion.\r\n\r\nI had to remove the beggining of the build log as it exceed max github length, but no error and no warning until building wheel.\r\n\r\n<details>\r\n<summary>Dockerfile</summary>\r\n\r\n```\r\nFROM nvcr.io/nvidia/pytorch:22.12-py3 AS base\r\n\r\nEXPOSE 22/tcp\r\nEXPOSE 8000/tcp\r\n\r\nRUN python -m pip install --upgrade pip && \\\r\n    pip uninstall -y torch && \\\r\n    pip install vllm\r\n```\r\n\r\n</details>\r\n<details>\r\n  <summary>Docker build log</summary>\r\n  \r\n```\r\n[+] Building 3580.7s (5/5) FINISHED\r\n => [internal] load build definition from Dockerfile                                                                                                 0.0s \r\n => => transferring dockerfile: 1.78kB                                                                                                               0.0s \r\n => [internal] load .dockerignore                                                                                                                    0.0s \r\n => => transferring context: 2B                                                                                                                      0.0s \r\n => [internal] load metadata for nvcr.io/nvidia/pytorch:22.12-py3                                                                                    2.5s \r\n => CACHED [1/2] FROM nvcr.io/nvidia/pytorch:22.12-py3@sha256:09a80f272dd173c9d8f28c23a1985aebe2bd3edd41a184ee9634f6e3f8a1f63d                       0.0s \r\n => ERROR [2/2] RUN python -m pip install --upgrade pip &&     pip uninstall -y torch &&     pip install vllm                                     3577.9s \r\n------\r\n > [2/2] RUN python -m pip install --upgrade pip &&     pip uninstall -y torch &&     pip install vllm:\r\n...\r\n\r\n#4 3049.3 Downloading sentry_sdk-1.28.1-py2.py3-none-any.whl (214 kB)\r\n#4 3049.4    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 214.7/214.7 kB 2.1 MB/s eta 0:00:00\r\n#4 3049.5 Downloading markdown2-2.4.9-py2.py3-none-any.whl (39 kB)\r\n#4 3049.5 Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\r\n#4 3049.7 Building wheels for collected packages: vllm, ffmpy, lit, pathtools, wavedrom\r\n#4 3049.7   Building wheel for vllm (pyproject.toml): started\r\n#4 3565.0   Building wheel for vllm (pyproject.toml): still running...\r\n#4 3570.6   Building wheel for vllm (pyproject.toml): finished with status 'error'\r\n#4 3570.9   error: subprocess-exited-with-error\r\n#4 3570.9\r\n#4 3570.9   \u00d7 Building wheel for vllm (pyproject.toml) did not run successfully.\r\n#4 3570.9   \u2502 exit code: 1\r\n#4 3570.9   \u2570\u2500> [270 lines of output]\r\n#4 3570.9       running bdist_wheel\r\n#4 3570.9       running build\r\n#4 3570.9       running build_py\r\n#4 3570.9       creating build\r\n#4 3570.9       creating build/lib.linux-x86_64-cpython-38\r\n#4 3570.9       creating build/lib.linux-x86_64-cpython-38/vllm\r\n#4 3570.9       copying vllm/utils.py -> build/lib.linux-x86_64-cpython-38/vllm\r\n#4 3570.9       copying vllm/sampling_params.py -> build/lib.linux-x86_64-cpython-38/vllm\r\n#4 3570.9       copying vllm/outputs.py -> build/lib.linux-x86_64-cpython-38/vllm\r\n#4 3570.9       copying vllm/logger.py -> build/lib.linux-x86_64-cpython-38/vllm\r\n#4 3570.9       copying vllm/block.py -> build/lib.linux-x86_64-cpython-38/vllm\r\n#4 3570.9       copying vllm/config.py -> build/lib.linux-x86_64-cpython-38/vllm\r\n#4 3570.9       copying vllm/sequence.py -> build/lib.linux-x86_64-cpython-38/vllm\r\n#4 3570.9       copying vllm/__init__.py -> build/lib.linux-x86_64-cpython-38/vllm\r\n#4 3570.9       creating build/lib.linux-x86_64-cpython-38/vllm/model_executor\r\n#4 3570.9       copying vllm/model_executor/weight_utils.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor\r\n#4 3570.9       copying vllm/model_executor/utils.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor\r\n#4 3570.9       copying vllm/model_executor/model_loader.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor\r\n#4 3570.9       copying vllm/model_executor/input_metadata.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor\r\n#4 3570.9       copying vllm/model_executor/__init__.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor\r\n#4 3570.9       creating build/lib.linux-x86_64-cpython-38/vllm/worker\r\n#4 3570.9       copying vllm/worker/worker.py -> build/lib.linux-x86_64-cpython-38/vllm/worker\r\n#4 3570.9       copying vllm/worker/cache_engine.py -> build/lib.linux-x86_64-cpython-38/vllm/worker\r\n#4 3570.9       copying vllm/worker/__init__.py -> build/lib.linux-x86_64-cpython-38/vllm/worker\r\n#4 3570.9       creating build/lib.linux-x86_64-cpython-38/vllm/transformers_utils\r\n#4 3570.9       copying vllm/transformers_utils/config.py -> build/lib.linux-x86_64-cpython-38/vllm/transformers_utils\r\n#4 3570.9       copying vllm/transformers_utils/tokenizer.py -> build/lib.linux-x86_64-cpython-38/vllm/transformers_utils\r\n#4 3570.9       copying vllm/transformers_utils/__init__.py -> build/lib.linux-x86_64-cpython-38/vllm/transformers_utils\r\n#4 3570.9       creating build/lib.linux-x86_64-cpython-38/vllm/entrypoints\r\n#4 3570.9       copying vllm/entrypoints/llm.py -> build/lib.linux-x86_64-cpython-38/vllm/entrypoints\r\n#4 3570.9       copying vllm/entrypoints/api_server.py -> build/lib.linux-x86_64-cpython-38/vllm/entrypoints\r\n#4 3570.9       copying vllm/entrypoints/__init__.py -> build/lib.linux-x86_64-cpython-38/vllm/entrypoints\r\n#4 3570.9       creating build/lib.linux-x86_64-cpython-38/vllm/engine\r\n#4 3570.9       copying vllm/engine/ray_utils.py -> build/lib.linux-x86_64-cpython-38/vllm/engine\r\n#4 3570.9       copying vllm/engine/llm_engine.py -> build/lib.linux-x86_64-cpython-38/vllm/engine\r\n#4 3570.9       copying vllm/engine/async_llm_engine.py -> build/lib.linux-x86_64-cpython-38/vllm/engine\r\n#4 3570.9       copying vllm/engine/__init__.py -> build/lib.linux-x86_64-cpython-38/vllm/engine\r\n#4 3570.9       copying vllm/engine/arg_utils.py -> build/lib.linux-x86_64-cpython-38/vllm/engine\r\n#4 3570.9       creating build/lib.linux-x86_64-cpython-38/vllm/core\r\n#4 3570.9       copying vllm/core/policy.py -> build/lib.linux-x86_64-cpython-38/vllm/core\r\n#4 3570.9       copying vllm/core/block_manager.py -> build/lib.linux-x86_64-cpython-38/vllm/core\r\n#4 3570.9       copying vllm/core/scheduler.py -> build/lib.linux-x86_64-cpython-38/vllm/core\r\n#4 3570.9       copying vllm/core/__init__.py -> build/lib.linux-x86_64-cpython-38/vllm/core\r\n#4 3570.9       creating build/lib.linux-x86_64-cpython-38/vllm/model_executor/layers\r\n#4 3570.9       copying vllm/model_executor/layers/sampler.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/layers\r\n#4 3570.9       copying vllm/model_executor/layers/attention.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/layers\r\n#4 3570.9       copying vllm/model_executor/layers/layernorm.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/layers\r\n#4 3570.9       copying vllm/model_executor/layers/__init__.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/layers\r\n#4 3570.9       copying vllm/model_executor/layers/activation.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/layers\r\n#4 3570.9       creating build/lib.linux-x86_64-cpython-38/vllm/model_executor/parallel_utils\r\n#4 3570.9       copying vllm/model_executor/parallel_utils/parallel_state.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/parallel_utils      \r\n#4 3570.9       copying vllm/model_executor/parallel_utils/__init__.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/parallel_utils\r\n#4 3570.9       creating build/lib.linux-x86_64-cpython-38/vllm/model_executor/models\r\n#4 3570.9       copying vllm/model_executor/models/bloom.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/models\r\n#4 3570.9       copying vllm/model_executor/models/gpt2.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/models\r\n#4 3570.9       copying vllm/model_executor/models/opt.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/models\r\n#4 3570.9       copying vllm/model_executor/models/gpt_bigcode.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/models\r\n#4 3570.9       copying vllm/model_executor/models/mpt.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/models\r\n#4 3570.9       copying vllm/model_executor/models/gpt_neox.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/models\r\n#4 3570.9       copying vllm/model_executor/models/__init__.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/models\r\n#4 3570.9       copying vllm/model_executor/models/llama.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/models\r\n#4 3570.9       creating build/lib.linux-x86_64-cpython-38/vllm/model_executor/parallel_utils/tensor_parallel\r\n#4 3570.9       copying vllm/model_executor/parallel_utils/tensor_parallel/utils.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/parallel_utils/tensor_parallel\r\n#4 3570.9       copying vllm/model_executor/parallel_utils/tensor_parallel/layers.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/parallel_utils/tensor_parallel\r\n#4 3570.9       copying vllm/model_executor/parallel_utils/tensor_parallel/random.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/parallel_utils/tensor_parallel\r\n#4 3570.9       copying vllm/model_executor/parallel_utils/tensor_parallel/mappings.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/parallel_utils/tensor_parallel\r\n#4 3570.9       copying vllm/model_executor/parallel_utils/tensor_parallel/__init__.py -> build/lib.linux-x86_64-cpython-38/vllm/model_executor/parallel_utils/tensor_parallel\r\n#4 3570.9       creating build/lib.linux-x86_64-cpython-38/vllm/transformers_utils/configs\r\n#4 3570.9       copying vllm/transformers_utils/configs/mpt.py -> build/lib.linux-x86_64-cpython-38/vllm/transformers_utils/configs\r\n#4 3570.9       copying vllm/transformers_utils/configs/__init__.py -> build/lib.linux-x86_64-cpython-38/vllm/transformers_utils/configs\r\n#4 3570.9       creating build/lib.linux-x86_64-cpython-38/vllm/entrypoints/openai\r\n#4 3570.9       copying vllm/entrypoints/openai/api_server.py -> build/lib.linux-x86_64-cpython-38/vllm/entrypoints/openai\r\n#4 3570.9       copying vllm/entrypoints/openai/protocol.py -> build/lib.linux-x86_64-cpython-38/vllm/entrypoints/openai\r\n#4 3570.9       copying vllm/entrypoints/openai/__init__.py -> build/lib.linux-x86_64-cpython-38/vllm/entrypoints/openai\r\n#4 3570.9       running build_ext\r\n#4 3570.9       building 'vllm.cache_ops' extension\r\n#4 3570.9       creating /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/build/temp.linux-x86_64-cpython-38\r\n#4 3570.9       creating /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/build/temp.linux-x86_64-cpython-38/csrc\r\n#4 3570.9       No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/utils/cpp_extension.py:388: UserWarning: The detected CUDA version (11.8) has a minor version mismatch with the version that was used to compile PyTorch (11.7). Most likely this shouldn't be a problem.\r\n#4 3570.9         warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/utils/cpp_extension.py:398: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 11.8\r\n#4 3570.9         warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\r\n#4 3570.9       Emitting ninja build file /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/build/temp.linux-x86_64-cpython-38/build.ninja...\r\n#4 3570.9       Compiling objects...\r\n#4 3570.9       Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\n#4 3570.9       [1/2] c++ -MMD -MF /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/build/temp.linux-x86_64-cpython-38/csrc/cache.o.d -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include -I/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/TH -I/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c -c /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache.cpp -o /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/build/temp.linux-x86_64-cpython-38/csrc/cache.o -g -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=cache_ops -D_GLIBCXX_USE_CXX11_ABI=0\r\n#4 3570.9       [2/2] /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include -I/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/TH -I/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c -c /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu -o /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/build/temp.linux-x86_64-cpython-38/csrc/cache_kernels.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 --threads 8 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=cache_ops -D_GLIBCXX_USE_CXX11_ABI=0\r\n#4 3570.9       FAILED: /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/build/temp.linux-x86_64-cpython-38/csrc/cache_kernels.o\r\n#4 3570.9       /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include -I/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/TH \r\n-I/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c -c /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu -o /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/build/temp.linux-x86_64-cpython-38/csrc/cache_kernels.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 --threads 8 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=cache_ops -D_GLIBCXX_USE_CXX11_ABI=0\r\n#4 3570.9       /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu(41): warning #1143-D: arithmetic on pointer to void \r\nor function type\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu(42): warning #1143-D: arithmetic on pointer to void \r\nor function type\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison \r\nof unsigned integer with zero\r\n#4 3570.9                 detected during:\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\r\n#4 3570.9       (61): here\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/c10/core/TensorImpl.h(77): here\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison \r\nof unsigned integer with zero\r\n#4 3570.9                 detected during:\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\r\n#4 3570.9       (61): here\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/ATen/core/qualified_name.h(73): here\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu(300): warning #550-D: variable \"src_key_indices\" was set but never used\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu(301): warning #550-D: variable \"src_value_indices\" was set but never used\r\n#4 3570.9\r\n#4 3570.9       /bin/bash: line 1:   367 Killed                  cicc --c++17 --gnu_version=90400 --display_error_number --orig_src_file_name \"/tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu\" --orig_src_path_name \"/tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu\" --allow_managed --relaxed_constexpr -arch compute_70 -m64 --no-version-ident -ftz=0 -prec_div=1 -prec_sqrt=1 -fmad=1 --include_file_name \"tmpxft_000000f1_00000000-3_cache_kernels.fatbin.c\" -tused --module_id_file_name \"/tmp/tmpxft_000000f1_00000000-4_cache_kernels.module_id\" --gen_c_file_name \"/tmp/tmpxft_000000f1_00000000-10_cache_kernels.compute_70.cudafe1.c\" --stub_file_name \"/tmp/tmpxft_000000f1_00000000-10_cache_kernels.compute_70.cudafe1.stub.c\" --gen_device_file_name \"/tmp/tmpxft_000000f1_00000000-10_cache_kernels.compute_70.cudafe1.gpu\" \"/tmp/tmpxft_000000f1_00000000-12_cache_kernels.compute_70.cpp1.ii\" -o \"/tmp/tmpxft_000000f1_00000000-10_cache_kernels.compute_70.ptx\" > /tmp/tmpxft_000000f1_00000000-22_1e62a10_stdout 2> \r\n/tmp/tmpxft_000000f1_00000000-22_1e62a10_stderr\r\n#4 3570.9       /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu(41): warning #1143-D: arithmetic on pointer to void \r\nor function type\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu(42): warning #1143-D: arithmetic on pointer to void \r\nor function type\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison \r\nof unsigned integer with zero\r\n#4 3570.9                 detected during:\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\r\n#4 3570.9       (61): here\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/c10/core/TensorImpl.h(77): here\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison \r\nof unsigned integer with zero\r\n#4 3570.9                 detected during:\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\r\n#4 3570.9       (61): here\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/ATen/core/qualified_name.h(73): here\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu(41): warning #1143-D: arithmetic on pointer to void \r\nor function type\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu(42): warning #1143-D: arithmetic on pointer to void \r\nor function type\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison \r\nof unsigned integer with zero\r\n#4 3570.9                 detected during:\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\r\n#4 3570.9       (61): here\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/c10/core/TensorImpl.h(77): here\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison \r\nof unsigned integer with zero\r\n#4 3570.9                 detected during:\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\r\n#4 3570.9       (61): here\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/ATen/core/qualified_name.h(73): here\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu(300): warning #550-D: variable \"src_key_indices\" was set but never used\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu(301): warning #550-D: variable \"src_value_indices\" was set but never used\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu(41): warning #1143-D: arithmetic on pointer to void \r\nor function type\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu(42): warning #1143-D: arithmetic on pointer to void \r\nor function type\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison \r\nof unsigned integer with zero\r\n#4 3570.9                 detected during:\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\r\n#4 3570.9       (61): here\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/c10/core/TensorImpl.h(77): here\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison \r\nof unsigned integer with zero\r\n#4 3570.9                 detected during:\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\r\n#4 3570.9       (61): here\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/ATen/core/qualified_name.h(73): here\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu(300): warning #550-D: variable \"src_key_indices\" was set but never used\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu(301): warning #550-D: variable \"src_value_indices\" was set but never used\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu(41): warning #1143-D: arithmetic on pointer to void \r\nor function type\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu(42): warning #1143-D: arithmetic on pointer to void \r\nor function type\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison \r\nof unsigned integer with zero\r\n#4 3570.9                 detected during:\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\r\n#4 3570.9       (61): here\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/c10/core/TensorImpl.h(77): here\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/c10/util/irange.h(54): warning #186-D: pointless comparison \r\nof unsigned integer with zero\r\n#4 3570.9                 detected during:\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\r\n#4 3570.9       (61): here\r\n#4 3570.9                   instantiation of \"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\r\n#4 3570.9       /tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/include/ATen/core/qualified_name.h(73): here\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu(300): warning #550-D: variable \"src_key_indices\" was set but never used\r\n#4 3570.9\r\n#4 3570.9       /tmp/pip-install-84soz_ys/vllm_d8938fe9fc4a483f86388d0c975b69f4/csrc/cache_kernels.cu(301): warning #550-D: variable \"src_value_indices\" was set but never used\r\n#4 3570.9\r\n#4 3570.9       ninja: build stopped: subcommand failed.\r\n#4 3570.9       Traceback (most recent call last):\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 1893, in _run_ninja_build     \r\n#4 3570.9           subprocess.run(\r\n#4 3570.9         File \"/usr/lib/python3.8/subprocess.py\", line 516, in run\r\n#4 3570.9           raise CalledProcessError(retcode, process.args,\r\n#4 3570.9       subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\r\n#4 3570.9\r\n#4 3570.9       The above exception was the direct cause of the following exception:\r\n#4 3570.9\r\n#4 3570.9       Traceback (most recent call last):\r\n#4 3570.9         File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\r\n#4 3570.9           main()\r\n#4 3570.9         File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\r\n#4 3570.9           json_out['return_val'] = hook(**hook_input['kwargs'])\r\n#4 3570.9         File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 251, in build_wheel\r\n#4 3570.9           return _build_backend().build_wheel(wheel_directory, config_settings,\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/build_meta.py\", line 416, in build_wheel\r\n#4 3570.9           return self._build_with_temp_dir(['bdist_wheel'], '.whl',\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/build_meta.py\", line 401, in _build_with_temp_dir      \r\n#4 3570.9           self.run_setup()\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/build_meta.py\", line 338, in run_setup\r\n#4 3570.9           exec(code, locals())\r\n#4 3570.9         File \"<string>\", line 145, in <module>\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/__init__.py\", line 107, in setup\r\n#4 3570.9           return distutils.core.setup(**attrs)\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/_distutils/core.py\", line 185, in setup\r\n#4 3570.9           return run_commands(dist)\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\r\n#4 3570.9           dist.run_commands()\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\r\n#4 3570.9           self.run_command(cmd)\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/dist.py\", line 1234, in run_command\r\n#4 3570.9           super().run_command(command)\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\r\n#4 3570.9           cmd_obj.run()\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/wheel/bdist_wheel.py\", line 346, in run\r\n#4 3570.9           self.run_command(\"build\")\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\r\n#4 3570.9           self.distribution.run_command(command)\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/dist.py\", line 1234, in run_command\r\n#4 3570.9           super().run_command(command)\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\r\n#4 3570.9           cmd_obj.run()\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/_distutils/command/build.py\", line 131, in run\r\n#4 3570.9           self.run_command(cmd_name)\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\r\n#4 3570.9           self.distribution.run_command(command)\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/dist.py\", line 1234, in run_command\r\n#4 3570.9           super().run_command(command)\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\r\n#4 3570.9           cmd_obj.run()\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/command/build_ext.py\", line 84, in run\r\n#4 3570.9           _build_ext.run(self)\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/_distutils/command/build_ext.py\", line 345, in run     \r\n#4 3570.9           self.build_extensions()\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 843, in build_extensions      \r\n#4 3570.9           build_ext.build_extensions(self)\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/_distutils/command/build_ext.py\", line 467, in build_extensions\r\n#4 3570.9           self._build_extensions_serial()\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/_distutils/command/build_ext.py\", line 493, in _build_extensions_serial\r\n#4 3570.9           self.build_extension(ext)\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/command/build_ext.py\", line 246, in build_extension    \r\n#4 3570.9           _build_ext.build_extension(self, ext)\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/setuptools/_distutils/command/build_ext.py\", line 548, in build_extension\r\n#4 3570.9           objects = self.compiler.compile(\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 658, in unix_wrap_ninja_compile\r\n#4 3570.9           _write_ninja_file_and_compile_objects(\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 1574, in _write_ninja_file_and_compile_objects\r\n#4 3570.9           _run_ninja_build(\r\n#4 3570.9         File \"/tmp/pip-build-env-envtk_ac/overlay/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 1909, in _run_ninja_build     \r\n#4 3570.9           raise RuntimeError(message) from e\r\n#4 3570.9       RuntimeError: Error compiling objects for extension\r\n#4 3570.9       [end of output]\r\n#4 3570.9\r\n#4 3570.9   note: This error originates from a subprocess, and is likely not a problem with pip.\r\n#4 3570.9   ERROR: Failed building wheel for vllm\r\n#4 3571.0   Building wheel for ffmpy (setup.py): started\r\n#4 3571.7   Building wheel for ffmpy (setup.py): finished with status 'done'\r\n#4 3571.7   Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5595 sha256=0d8a0b1c30fff91552aef5da8c6b5b9b3d3ef25eb2756d171eae0d94a959d783\r\n#4 3571.7   Stored in directory: /tmp/pip-ephem-wheel-cache-pdhhjch9/wheels/75/a3/1a/2f3f90b9a4eb0408109ae1b5bae01efbdf8ab4ef98797433e4\r\n#4 3571.8   Building wheel for lit (pyproject.toml): started\r\n#4 3572.1   Building wheel for lit (pyproject.toml): finished with status 'done'\r\n#4 3572.1   Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93584 sha256=4a73c3d05fc512e02aeac2bdf5e33e9fc47e54dec31eedd069324c842b1c558d#4 3572.1   Stored in directory: /tmp/pip-ephem-wheel-cache-pdhhjch9/wheels/05/ab/f1/0102fea49a41c753f0e79a1a4012417d5d7ef0f93224694472\r\n#4 3572.1   Building wheel for pathtools (setup.py): started\r\n#4 3572.5   Building wheel for pathtools (setup.py): finished with status 'done'\r\n#4 3572.5   Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=5bed2b14d1c9b63a1dd5e5e74a60756baf84dd221c8b8d8c072f4c0bda167832\r\n#4 3572.5   Stored in directory: /tmp/pip-ephem-wheel-cache-pdhhjch9/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\r\n#4 3572.5   Building wheel for wavedrom (setup.py): started\r\n#4 3573.0   Building wheel for wavedrom (setup.py): finished with status 'done'\r\n#4 3573.0   Created wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=29952 sha256=d5dc1d3dba178bc1e127dd303ac17834d6e30ad06d66f4caa975002eadf151e8\r\n#4 3573.0   Stored in directory: /tmp/pip-ephem-wheel-cache-pdhhjch9/wheels/ce/b2/21/5266400b4e65ab57862edfd078d2382696cec13ffe8fb171cb\r\n#4 3573.0 Successfully built ffmpy lit pathtools wavedrom\r\n#4 3573.0 Failed to build vllm\r\n#4 3573.0 ERROR: Could not build wheels for vllm, which is required to install pyproject.toml-based projects\r\n------\r\nprocess \"/bin/sh -c python -m pip install --upgrade pip &&     pip uninstall -y torch &&     pip install vllm\" did not complete successfully: exit code: 1\r\n```\r\n</details>",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2023-07-23T14:46:47+00:00",
    "closed_at": "2024-03-08T10:40:32+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/552/reactions",
      "total_count": 12,
      "+1": 12,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/552"
  },
  {
    "number": 10244,
    "title": "[Installation]: Install Gpu vllm got no module named triton",
    "body": "### Your current environment\n\n```text\r\nInstall Gpu vllm 0.6.3 post6 with tesla v100and cuda 12.4,when import vllm got no module named triton\r\n```\r\n\n\n### How you are installing vllm\n\n```sh\r\ninstall with source code\r\n```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-12T06:26:47+00:00",
    "closed_at": "2025-03-20T02:03:49+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10244/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10244"
  },
  {
    "number": 8502,
    "title": "[Installation]: Container image do not build Dockerfile.cpu",
    "body": "### Your current environment\r\n\r\n```text\r\n$: podman -v\r\npodman version 5.2.2\r\n```\r\n\r\n\r\n### How you are installing vllm\r\n\r\n```sh\r\n$: podman build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .\r\n...\r\nError: building at STEP \"RUN --mount=type=cache,target=/root/.cache/pip --mount=type=bind,src=requirements-build.txt,target=requirements-build.txt pip install --upgrade pip &&     pip install -r requirements-build.txt\": resolving mountpoints for container \"3a97f46183fa64e10c96f20f9a38a5ed46d2e9e7c4e7bbfbce6fa1adfdacd66e\": invalid container path \"requirements-build.txt\", must be an absolute path\r\n```\r\n\r\nWe can see that in the `Dockerfile.cpu` we are mounting the requirement-*.txt files\r\n\r\nhttps://github.com/vllm-project/vllm/blob/fc990f97958636ce25e4471acfd5651b096b0311/Dockerfile.cpu#L29\r\n\r\nhttps://github.com/vllm-project/vllm/blob/fc990f97958636ce25e4471acfd5651b096b0311/Dockerfile.cpu#L51\r\n\r\nhttps://github.com/vllm-project/vllm/blob/fc990f97958636ce25e4471acfd5651b096b0311/Dockerfile.cpu#L52\r\n\r\nAnd while they are mounted, no absolute path is provided, leading to the error above.\r\n\r\n## Solution proposal\r\n\r\n```diff\r\n+ COPY requirements-build.txt requirements-build.txt\r\nRUN --mount=type=cache,target=/root/.cache/pip \\\r\n-    --mount=type=bind,src=requirements-build.txt,target=requirements-build.txt \\\r\n    pip install --upgrade pip && \\\r\n    pip install -r requirements-build.txt\r\n```\r\n\r\nOR\r\n\r\n```diff\r\n+ WORKDIR /\r\nRUN --mount=type=cache,target=/root/.cache/pip \\\r\n    --mount=type=bind,src=requirements-build.txt,target=requirements-build.txt \\\r\n    pip install --upgrade pip && \\\r\n    pip install -r requirements-build.txt\r\n```\r\n\r\nAccording to https://github.com/containers/buildah/issues/4309 a WORKDIR must be defined to use a relative path for target. I opened an issue on the buildah side to have more information https://github.com/containers/buildah/issues/5738\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2024-09-16T13:58:50+00:00",
    "closed_at": "2024-09-18T02:49:54+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8502/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8502"
  },
  {
    "number": 9809,
    "title": "[Installation]: `Dockerfile.rocm` requires a torch nightly build that's no longer hosted: requires 2024-07-26, earliest is 2024-08-31.",
    "body": "### Your current environment\n\n(current environment is irrelevant because this is a replacement for the nightly build reference)\n\n### How you are installing vllm\n\n```sh\r\ngit clone <vllm https URL>\r\ncd vllm\r\ngit checkout -b 0.6.1 v0.6.1\r\npodman build -f Dockerfile.rocm -t vllm-rocm .\r\n```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "rocm",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-29T19:38:48+00:00",
    "closed_at": "2025-03-11T02:03:46+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9809/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9809"
  },
  {
    "number": 8745,
    "title": "Error loading models since versions 0.6.1xxx",
    "body": "### Your current environment\r\n\r\n```\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-187-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.3.107\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A40\r\nGPU 1: NVIDIA A40\r\nGPU 2: NVIDIA A40\r\nGPU 3: NVIDIA A40\r\n\r\nNvidia driver version: 535.183.01\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      46 bits physical, 57 bits virtual\r\nCPU(s):                             96\r\nOn-line CPU(s) list:                0-95\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 24\r\nSocket(s):                          2\r\nNUMA node(s):                       2\r\nVendor ID:                          GenuineIntel\r\nCPU family:                         6\r\nModel:                              106\r\nModel name:                         Intel(R) Xeon(R) Gold 6336Y CPU @ 2.40GHz\r\nStepping:                           6\r\nCPU MHz:                            800.012\r\nBogoMIPS:                           4800.00\r\nVirtualization:                     VT-x\r\n.\r\n.\r\n.\r\n```\r\n\r\n\r\n### How you are installing vllm\r\n\r\n```\r\npip install -U vllm\r\n```\r\n\r\nIt seems like in the recent few weeks a lot of crucial updates has been made to properly use vllm, which exist in version 0.6.1 but lacks in version 0.6.1.post2. However, the available version through pip is the old 0.6.1.post2.\r\n\r\nFor example, [#8157](https://github.com/vllm-project/vllm/pull/8157) possibly fix issue [#8553](https://github.com/vllm-project/vllm/issues/8553), which I am also having.\r\n\r\n---------------------------------------\r\n\r\nAn update - After installing version 0.6.1 via pip, I am still having the error in issue #8553 when I try to initiate the model (which I downloaded already using Huggingface interface)\r\n\r\n```\r\nTraceback (most recent call last): \r\n  File \"<string>\", line 1, in <module> \r\n  File \"/home/ido.amit/miniconda3/envs/benchmark/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main \r\n    exitcode = _main(fd, parent_sentinel)                                                            \r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                      \r\n  File \"/home/ido.amit/miniconda3/envs/benchmark/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main                  \r\n    self = reduction.pickle.load(from_parent)                                                                                  \r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                 \r\nModuleNotFoundError: No module named 'transformers_modules.microsoft.Phi-3'                                                   \r\nERROR 09-24 00:33:22 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 3840118 died, exit code: 1                \r\nINFO 09-24 00:33:22 multiproc_worker_utils.py:123] Killing local vLLM worker processes   \r\n```\r\n\r\nThanks in advance for the help!\r\n",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2024-09-23T21:17:29+00:00",
    "closed_at": "2024-10-18T11:11:35+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8745/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8745"
  },
  {
    "number": 8243,
    "title": "[Installation]: NotImplementedError get_device_capability",
    "body": "### Your current environment\n\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: AlmaLinux release 8.10 (Cerulean Leopard) (x86_64)\r\nGCC version: (GCC) 8.5.0 20210514 (Red Hat 8.5.0-22)\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.28\r\n\r\nPython version: 3.11.9 (main, Jul  2 2024, 16:32:17) [GCC 8.5.0 20210514 (Red Hat 8.5.0-22)] (64-bit runtime)\r\nPython platform: Linux-4.18.0-553.8.1.el8_10.x86_64-x86_64-with-glibc2.28\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 1: NVIDIA H100 80GB HBM3\r\nGPU 2: NVIDIA H100 80GB HBM3\r\nGPU 3: NVIDIA H100 80GB HBM3\r\nGPU 4: NVIDIA H100 80GB HBM3\r\nGPU 5: NVIDIA H100 80GB HBM3\r\nGPU 6: NVIDIA H100 80GB HBM3\r\nGPU 7: NVIDIA H100 80GB HBM3\r\n\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              96\r\nOn-line CPU(s) list: 0-95\r\nThread(s) per core:  1\r\nCore(s) per socket:  48\r\nSocket(s):           2\r\nNUMA node(s):        8\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               143\r\nModel name:          Intel(R) Xeon(R) Platinum 8468\r\nStepping:            8\r\nCPU MHz:             3800.000\r\nCPU max MHz:         3800.0000\r\nCPU min MHz:         800.0000\r\nBogoMIPS:            4200.00\r\nL1d cache:           48K\r\nL1i cache:           32K\r\nL2 cache:            2048K\r\nL3 cache:            107520K\r\nNUMA node0 CPU(s):   0-11\r\nNUMA node1 CPU(s):   12-23\r\nNUMA node2 CPU(s):   24-35\r\nNUMA node3 CPU(s):   36-47\r\nNUMA node4 CPU(s):   48-59\r\nNUMA node5 CPU(s):   60-71\r\nNUMA node6 CPU(s):   72-83\r\nNUMA node7 CPU(s):   84-95\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.68\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.2\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.0@32e7db25365415841ebc7c4215851743fbb1bad1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\t0-11\t0\t\tN/A\r\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\t24-35\t2\t\tN/A\r\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\t36-47\t3\t\tN/A\r\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tSYS\t12-23\t1\t\tN/A\r\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tSYS\t48-59\t4\t\tN/A\r\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tPIX\t72-83\t6\t\tN/A\r\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tSYS\t84-95\t7\t\tN/A\r\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tSYS\t60-71\t5\t\tN/A\r\nNIC0\tSYS\tSYS\tSYS\tSYS\tSYS\tPIX\tSYS\tSYS\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_bond_0\n\n### How you are installing vllm\n\nWe are running on kubernetes (which works for test cuda containers) using the vllm 0.6.0 container, tried also on 0.5.4 and same issue\r\n\r\nthe full error is in the container is\r\n```\r\nProcess SpawnProcess-1:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/server.py\", line 230, in run_rpc_server\r\n    server = AsyncEngineRPCServer(async_engine_args, usage_context, rpc_path)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/server.py\", line 31, in __init__\r\n    self.engine = AsyncLLMEngine.from_engine_args(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 740, in from_engine_args\r\n    engine = cls(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 636, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 840, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 272, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 270, in __init__\r\n    self.model_executor = executor_class(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\", line 46, in __init__\r\n    self._init_executor()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 37, in _init_executor\r\n    self.driver_worker = self._create_worker()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 104, in _create_worker\r\n    return create_worker(**self._get_create_worker_kwargs(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 23, in create_worker\r\n    wrapper.init_worker(**kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 444, in init_worker\r\n    self.worker = worker_class(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 99, in __init__\r\n    self.model_runner: GPUModelRunnerBase = ModelRunnerClass(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 842, in __init__\r\n    self.attn_backend = get_attn_backend(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/selector.py\", line 108, in get_attn_backend\r\n    backend = which_attn_to_use(num_heads, head_size, num_kv_heads,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/selector.py\", line 215, in which_attn_to_use\r\n    if current_platform.get_device_capability()[0] < 8:\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/platforms/interface.py\", line 28, in get_device_capability\r\n    raise NotImplementedError\r\nNotImplementedError\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2024-09-06T17:42:09+00:00",
    "closed_at": "2024-09-10T14:02:21+00:00",
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8243/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8243"
  },
  {
    "number": 18328,
    "title": "[Installation]: https://hub.docker.com/u/vllm went wrong!",
    "body": "### Your current environment\n\nThe vllm repo of docker hub went wrong.\n<img width=\"1342\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/841b974a-3825-4af1-843c-80ececfdced8\" />\n\nBut other repos are good.\n<img width=\"604\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/946716ca-e8e3-4348-95ee-15d25e129cc9\" />\n\n### How you are installing vllm\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2025-05-19T01:43:13+00:00",
    "closed_at": "2025-05-19T06:20:13+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18328/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/18328"
  },
  {
    "number": 5690,
    "title": "[Installation]: poetry add vllm not working on my Mac -- xformers (0.0.26.post1) not supporting PEP 517 builds.",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### How you are installing vllm\n\n```\r\npoetry add vllm\r\n```\r\nCan someone tell me what versions of ray and torch are compatible to add vlllm via poetry?\r\n```\r\n- Installing vllm-flash-attn (2.5.9): Failed\r\n-  Unable to find installation candidates for ray (2.24.0)\r\nNote: This error originates from the build backend, and is likely not a problem with poetry but with xformers (0.0.26.post1) not supporting PEP 517 builds. You can verify this by running 'pip wheel --no-cache-dir --use-pep517 \"xformers (==0.0.26.post1)\"'\r\n```\r\n",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-06-19T17:41:35+00:00",
    "closed_at": "2025-03-30T02:10:11+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5690/reactions",
      "total_count": 12,
      "+1": 12,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5690"
  },
  {
    "number": 13427,
    "title": "[Installation]:",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\n```\n\n\n### How you are installing vllm\n\n```sh\npip install -vvv vllm\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2025-02-17T21:54:28+00:00",
    "closed_at": "2025-02-18T05:54:37+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13427/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13427"
  },
  {
    "number": 18673,
    "title": "[Installation]: Hard to find right wheel files to build the release version",
    "body": "@youkaichao @DarkLight1337 \nHello, I'm seeking help with building `v0.8.5.post1` and installing `other released versions`.\n\n### Your current environment\n\ntrying to install v0.8.5.post1\n\n### How you are installing vllm\n\nhttps://github.com/vllm-project/vllm/issues/15347\nFirstly, I had trouble identifying the correct commit IDs for the prebuilt wheel files from https://wheels.vllm.ai/\n<img width=\"849\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f6a5966b-401d-4c3b-accf-0ed13e617558\" />\n\nhttps://github.com/vllm-project/vllm/issues/16217\nSecondly, It would be very helpful if wheels were also published for the exact commits of the latest releases.\nIn my case, I\u2019m trying to build v0.8.5.post1 using the prebuilt wheels. However, there is no wheel corresponding to the last commit ID of that release. (first try)\nI then attempted to find the last commit of v0.8.5.post1 from the main branch and used the wheel for that commit, but encountered an import error. (second try)\nI suspect this is due to differences between the commit histories of the main and release branches, which may be causing incompatibilities.\n\nBelow is the script I used to try building v0.8.5.post1 and error messages.\n\n### 1. first try\n\n<img width=\"1331\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/16a420f0-9d22-4e35-b29f-87a7ff7153c5\" />\n\n```\npython -m pip install --upgrade pip\npip install -r ./requirements/build.txt\npip install -r ./requirements/common.txt\npip install -r ./requirements/cuda.txt\npip install transformer-engine==2.3.0\npip install flash_attn==2.7.4.post1\nexport VLLM_COMMIT=3015d5634e74d59704e2b39bab0dbe2e6f86a38a\nexport VLLM_PRECOMPILED_WHEEL_LOCATION=https://wheels.vllm.ai/${VLLM_COMMIT}/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl\npip install -e .\npip install -U pynvml\n```\n\n```\n      Downloading wheel from https://wheels.vllm.ai/3015d5634e74d59704e2b39bab0dbe2e6f86a38a/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl to /tmp/vllm-wheels3snl5y0_/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl\n      error: Failed to get vLLM wheel from https://wheels.vllm.ai/3015d5634e74d59704e2b39bab0dbe2e6f86a38a/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building editable for vllm\nFailed to build vllm\nERROR: Failed to build installable wheels for some pyproject.toml based projects (vllm)\n```\n\n### 2. second try\n\n<img width=\"1331\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a29ed5ab-0e84-4440-8c59-009995a2f763\" />\n\n```\npython -m pip install --upgrade pip\npip install -r ./requirements/build.txt\npip install -r ./requirements/common.txt\npip install -r ./requirements/cuda.txt\npip install transformer-engine==2.3.0\npip install flash_attn==2.7.4.post1\nexport VLLM_COMMIT=0f87d8f7b26d2f71117211d337952396b75dac50\nexport VLLM_PRECOMPILED_WHEEL_LOCATION=https://wheels.vllm.ai/${VLLM_COMMIT}/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl\npip install -e .\npip install -U pynvml\n```\nbuild success but gives me an following import error.\n```\n  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"/vllm-v0.8.5/vllm/platforms/cuda.py\", line 15, in <module>\n    import vllm._C  # noqa\nImportError: /vllm-v0.8.5/vllm/_C.abi3.so: undefined symbol: _ZN3c106ivalue14ConstantString6createENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "open",
    "created_at": "2025-05-25T03:17:17+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18673/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/18673"
  },
  {
    "number": 9701,
    "title": "[Installation] pip install vllm (0.6.3) will force a reinstallation of the CPU version torch and replace cuda torch on windows",
    "body": "pip install vllm (0.6.3) will force a reinstallation of the CPU version torch and replace cuda torch on windows. pip install vllm\uff080.6.3\uff09\u5c06\u5f3a\u5236\u91cd\u65b0\u5b89\u88c5CPU\u7248\u672c\u7684torch\u5e76\u5728Windows\u4e0a\u66ff\u6362cuda torch\u3002\r\n> > \r\n> > \r\n> > I don't quite get what you mean, how can you have different versions of torch for CPU and GPU at the same time?\u6211\u4e0d\u592a\u660e\u767d\u4f60\u7684\u610f\u601d\uff0c\u4f60\u600e\u4e48\u80fd\u6709\u4e0d\u540c\u7248\u672c\u7684\u706b\u70acCPU\u548cGPU\u5728\u540c\u4e00\u65f6\u95f4\uff1f\r\n> \r\n> only cuda torch\r\n> \r\n> ```\r\n>  pip install vllm --no-deps\r\n> Collecting vllm\r\n>   Using cached vllm-0.6.3.post1.tar.gz (2.7 MB)\r\n>   Installing build dependencies ... error\r\n>   error: subprocess-exited-with-error\r\n> \r\n>   \u00d7 pip subprocess to install build dependencies did not run successfully.\r\n>   \u2502 exit code: 2\r\n>   \u2570\u2500> [86 lines of output]\r\n>       Collecting cmake>=3.26\r\n>         Using cached cmake-3.30.5-py3-none-win_amd64.whl.metadata (6.4 kB)\r\n>       Collecting ninja\r\n>         Using cached ninja-1.11.1.1-py2.py3-none-win_amd64.whl.metadata (5.4 kB)\r\n> \r\n>       Collecting packaging\r\n>         Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\r\n>       Collecting setuptools>=61\r\n>         Using cached setuptools-75.2.0-py3-none-any.whl.metadata (6.9 kB)\r\n>       Collecting setuptools-scm>=8.0\r\n>         Using cached setuptools_scm-8.1.0-py3-none-any.whl.metadata (6.6 kB)\r\n>       Collecting torch==2.4.0\r\n>         Using cached torch-2.4.0-cp310-cp310-win_amd64.whl.metadata (27 kB)\r\n>       Collecting wheel\r\n>         Using cached wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\r\n>       Collecting jinja2\r\n>         Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\r\n>       Collecting filelock (from torch==2.4.0)\r\n>         Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\r\n>       Collecting typing-extensions>=4.8.0 (from torch==2.4.0)\r\n>         Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\r\n> \r\n>       Collecting sympy (from torch==2.4.0)\r\n>         Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\r\n>       Collecting networkx (from torch==2.4.0)\r\n>         Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\r\n>       Collecting fsspec (from torch==2.4.0)\r\n>         Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\r\n>       Collecting tomli>=1 (from setuptools-scm>=8.0)\r\n>         Using cached tomli-2.0.2-py3-none-any.whl.metadata (10.0 kB)\r\n>       Collecting MarkupSafe>=2.0 (from jinja2)\r\n>         Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl.metadata (4.1 kB\r\n> )\r\n>       Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.4.0)\r\n>         Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\r\n>       Downloading torch-2.4.0-cp310-cp310-win_amd64.whl (197.9 MB)\r\n>                                                   3.9/197.9 MB 21.3 kB/s eta 2:3\r\n> 1:31\r\n>       ERROR: Exception:\r\n>       Traceback (most recent call last):\r\n>         File \"D:\\my\\env\\python3.10.10\\Lib\\site-packages\\pip\\_vendor\\urllib3\\resp\r\n> onse.py\", line 438, in _error_catcher\r\n>           yield\r\n>         File \"D:\\my\\env\\python3.10.10\\Lib\\site-packages\\pip\\_vendor\\urllib3\\resp\r\n> onse.py\", line 561, in read\r\n>           data = self._fp_read(amt) if not fp_closed else b\"\"\r\n>         File \"D:\\my\\env\\python3.10.10\\Lib\\site-packages\\pip\\_vendor\\urllib3\\resp\r\n> onse.py\", line 527, in _fp_read\r\n>           return self._fp.read(amt) if amt is not None else self._fp.read()\r\n>         File \"D:\\my\\env\\python3.10.10\\Lib\\site-packages\\pip\\_vendor\\cachecontrol\r\n> \\filewrapper.py\", line 98, in read\r\n>           data: bytes = self.__fp.read(amt)\r\n>         File \"D:\\my\\env\\python3.10.10\\lib\\http\\client.py\", line 465, in read\r\n>           s = self.fp.read(amt)\r\n>         File \"D:\\my\\env\\python3.10.10\\lib\\socket.py\", line 705, in readinto\r\n>           return self._sock.recv_into(b)\r\n>         File \"D:\\my\\env\\python3.10.10\\lib\\ssl.py\", line 1274, in recv_into\r\n>           return self.read(nbytes, buffer)\r\n>         File \"D:\\my\\env\\python3.10.10\\lib\\ssl.py\", line 1130, in read\r\n>           return self._sslobj.read(len, buffer)\r\n>       TimeoutError: The read operation timed out\r\n>      \r\n>       During handling of the above exception, another exception occurred:\r\n>      \r\n>       Traceback (most recent call last):\r\n>         File \"D:\\my\\env\\python3.10.10\\Lib\\site-packages\\pip\\_internal\\cli\\base_c\r\n> ommand.py\", line 105, in _run_wrapper\r\n>           status = _inner_run()\r\n>         File \"D:\\my\\env\\python3.10.10\\Lib\\site-packages\\pip\\_internal\\cli\\base_c\r\n> ommand.py\", line 96, in _inner_run\r\n>           return self.run(options, args)\r\n>         File \"D:\\my\\env\\python3.10.10\\Lib\\site-packages\\pip\\_internal\\cli\\req_co\r\n> mmand.py\", line 67, in wrapper\r\n>           return func(self, options, args)\r\n>         File \"D:\\my\\env\\python3.10.10\\Lib\\site-packages\\pip\\_internal\\commands\\i\r\n> nstall.py\", line 379, in run\r\n>           requirement_set = resolver.resolve(\r\n>         File \"D:\\my\\env\\python3.10.10\\Lib\\site-packages\\pip\\_internal\\resolution\r\n> \\resolvelib\\resolver.py\", line 179, in resolve\r\n>           self.factory.preparer.prepare_linked_requirements_more(reqs)\r\n>         File \"D:\\my\\env\\python3.10.10\\Lib\\site-packages\\pip\\_internal\\operations\r\n> \\prepare.py\", line 554, in prepare_linked_requirements_more\r\n>           self._complete_partial_requirements(\r\n>         File \"D:\\my\\env\\python3.10.10\\Lib\\site-packages\\pip\\_internal\\operations\r\n> \\prepare.py\", line 469, in _complete_partial_requirements\r\n>           for link, (filepath, _) in batch_download:\r\n>         File \"D:\\my\\env\\python3.10.10\\Lib\\site-packages\\pip\\_internal\\network\\do\r\n> wnload.py\", line 184, in __call__\r\n>           for chunk in chunks:\r\n>         File \"D:\\my\\env\\python3.10.10\\Lib\\site-packages\\pip\\_internal\\cli\\progre\r\n> ss_bars.py\", line 55, in _rich_progress_bar\r\n>           for chunk in iterable:\r\n>         File \"D:\\my\\env\\python3.10.10\\Lib\\site-packages\\pip\\_internal\\network\\ut\r\n> ils.py\", line 65, in response_chunks\r\n>           for chunk in response.raw.stream(\r\n>         File \"D:\\my\\env\\python3.10.10\\Lib\\site-packages\\pip\\_vendor\\urllib3\\resp\r\n> onse.py\", line 622, in stream\r\n>           data = self.read(amt=amt, decode_content=decode_content)\r\n>         File \"D:\\my\\env\\python3.10.10\\Lib\\site-packages\\pip\\_vendor\\urllib3\\resp\r\n> onse.py\", line 560, in read\r\n>           with self._error_catcher():\r\n>         File \"D:\\my\\env\\python3.10.10\\lib\\contextlib.py\", line 153, in __exit__\r\n>           self.gen.throw(typ, value, traceback)\r\n>         File \"D:\\my\\env\\python3.10.10\\Lib\\site-packages\\pip\\_vendor\\urllib3\\resp\r\n> onse.py\", line 443, in _error_catcher\r\n>           raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\r\n>       pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host=\r\n> 'files.pythonhosted.org', port=443): Read timed out.\r\n>       [end of output]\r\n> \r\n>   note: This error originates from a subprocess, and is likely not a problem wit\r\n> h pip.\r\n> error: subprocess-exited-with-error\r\n> \r\n> \u00d7 pip subprocess to install build dependencies did not run successfully.\r\n> \u2502 exit code: 2\r\n> \u2570\u2500> See above for output.\r\n> ```\r\n> \r\n> If you internet is not good. You are so lucky. Because it will fail during the process of forcibly replacing CUDA torch with CPU. If you have a good internet connection. So things will become very bad. Your torch will transition from CUDA to a lower version CPU. And pip install vllm --no-deps or pip install vllm has same issue\r\n\r\nWhat is your original version of pytorch?\r\n\r\n_Originally posted by @DarkLight1337 in https://github.com/vllm-project/vllm/issues/4194#issuecomment-2435665167_\r\n\r\npip show torch\r\nName: torch\r\nVersion: 2.5.0+cu124\r\nSummary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\r\nHome-page: https://pytorch.org/\r\nAuthor: PyTorch Team\r\nAuthor-email: packages@pytorch.org\r\nLicense: BSD-3-Clause\r\nLocation: d:\\my\\env\\python3.10.10\\lib\\site-packages\r\nRequires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\r\nRequired-by: accelerate, auto_gptq, bitsandbytes, compressed-tensors, encodec, flash_attn, optimum, peft, stable-baselines3, timm, t\r\norchaudio, torchvision, trl, vector-quantize-pytorch, vocos\r\n            ",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-25T17:08:40+00:00",
    "closed_at": "2025-04-01T02:13:00+00:00",
    "comments": 50,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9701/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9701"
  },
  {
    "number": 8851,
    "title": "[Installation]: Cannot install with Poetry",
    "body": "### Your current environment\n\n5.47   /tmp/tmp4enbmtnv/.venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:258: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\r\n45.47     cpu = _conversion_method_template(device=torch.device(\"cpu\"))\r\n45.47   Traceback (most recent call last):\r\n45.47     File \"/usr/local/lib/python3.12/site-packages/pyproject_hooks/_in_process/_in_process.py\", line 373, in <module>\r\n45.47       main()\r\n45.47     File \"/usr/local/lib/python3.12/site-packages/pyproject_hooks/_in_process/_in_process.py\", line 357, in main\r\n45.47       json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\r\n45.47                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n45.47     File \"/usr/local/lib/python3.12/site-packages/pyproject_hooks/_in_process/_in_process.py\", line 134, in get_requires_for_build_wheel\r\n45.47       return hook(config_settings)\r\n45.47              ^^^^^^^^^^^^^^^^^^^^^\r\n45.47     File \"/tmp/tmp4enbmtnv/.venv/lib/python3.12/site-packages/setuptools/build_meta.py\", line 332, in get_requires_for_build_wheel\r\n45.47       return self._get_build_requires(config_settings, requirements=[])\r\n45.47              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n45.47     File \"/tmp/tmp4enbmtnv/.venv/lib/python3.12/site-packages/setuptools/build_meta.py\", line 302, in _get_build_requires\r\n45.47       self.run_setup()\r\n45.47     File \"/tmp/tmp4enbmtnv/.venv/lib/python3.12/site-packages/setuptools/build_meta.py\", line 318, in run_setup\r\n45.47       exec(code, locals())\r\n45.47     File \"<string>\", line 510, in <module>\r\n45.47     File \"<string>\", line 450, in get_requirements\r\n45.47   ValueError: Unsupported platform, please use CUDA, ROCm, Neuron, OpenVINO, or CPU.\r\n45.47   \r\n45.47 \r\n45.47   at /usr/local/lib/python3.12/site-packages/poetry/installation/chef.py:164 in _prepare\r\n45.48       160\u2502 \r\n45.48       161\u2502                 error = ChefBuildError(\"\\n\\n\".join(message_parts))\r\n45.48       162\u2502 \r\n45.48       163\u2502             if error is not None:\r\n45.48     \u2192 164\u2502                 raise error from None\r\n45.48       165\u2502 \r\n45.48       166\u2502             return path\r\n45.48       167\u2502 \r\n45.48       168\u2502     def _prepare_sdist(self, archive: Path, destination: Path | None = None) -> Path:\r\n45.48 \r\n45.48 Note: This error originates from the build backend, and is likely not a problem with poetry but with vllm (0.6.2) not supporting PEP 517 builds. You can verify this by running 'pip wheel --no-cache-dir --use-pep517 \"vllm (==0.6.2)\"'.\r\n45.48 \r\n------\r\n\n\n### How would you like to use vllm\n\nI'm using MacOS and want to install vllm with poerty.\r\nmy actions in docker:\r\npoetry add \"numpy=1.26.4\"\r\npoerty add vllm\r\nand i catch this problem.\r\nHow i can install vllm lib on my mac ?\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-26T13:02:53+00:00",
    "closed_at": "2025-02-07T01:59:47+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8851/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8851"
  },
  {
    "number": 11615,
    "title": "[Installation]: Hitting issues while trying to build vllm image using Dockerfile.rocm (v0.6.2)",
    "body": "### Your current environment\n\nRunning docker build on ubuntu server.\r\n\r\nFacing following error:\r\n![image](https://github.com/user-attachments/assets/12e75246-2c48-4751-9484-d0182cceff50)\r\n\r\nI checked https://download.pytorch.org/whl/nightly/torch/\r\nBut required wheel file: torch==2.6.0.dev20240918 and torchvision==0.20.0.dev20240918 is not present.\r\n\n\n### How you are installing vllm\n\ndocker build -f Dockerfile.rocm -t vllm-rocm:0.6.2 .\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2024-12-30T06:04:03+00:00",
    "closed_at": "2025-01-07T06:20:09+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11615/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11615"
  },
  {
    "number": 10251,
    "title": "[Installation]: offline chat gpt",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### How you are installing vllm\n\n```sh\r\npip install -vvv vllm\r\n```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-12T08:10:40+00:00",
    "closed_at": "2025-03-13T02:04:09+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10251/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10251"
  },
  {
    "number": 7498,
    "title": "[Installation]: build docker images: Failed to build mamba-ssm",
    "body": "### Your current environment\r\n\r\n```\r\n=> ERROR [mamba-builder 3/3] RUN pip --verbose wheel -r requirements-mamba.txt     --no-build-isolation --no-deps --no-cache-dir\r\n909.8s\r\n------\r\n > [mamba-builder 3/3] RUN pip --verbose wheel -r requirements-mamba.txt     --no-build-isolation --no-deps --no-cache-dir:\r\n1.518 Collecting mamba-ssm>=1.2.2 (from -r requirements-mamba.txt (line 2))\r\n1.776   Downloading mamba_ssm-2.2.2.tar.gz (85 kB)\r\n1.904   Preparing metadata (setup.py): started\r\n1.904   Running command python setup.py egg_info\r\n4.070   No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\r\n4.146\r\n4.146\r\n4.147   torch.__version__  = 2.4.0+cu121\r\n4.147\r\n4.148\r\n4.149   running egg_info\r\n4.149   creating /tmp/pip-pip-egg-info-d2_c2trw/mamba_ssm.egg-info\r\n4.154   writing /tmp/pip-pip-egg-info-d2_c2trw/mamba_ssm.egg-info/PKG-INFO\r\n4.154   writing dependency_links to /tmp/pip-pip-egg-info-d2_c2trw/mamba_ssm.egg-info/dependency_links.txt\r\n4.155   writing requirements to /tmp/pip-pip-egg-info-d2_c2trw/mamba_ssm.egg-info/requires.txt\r\n4.156   writing top-level names to /tmp/pip-pip-egg-info-d2_c2trw/mamba_ssm.egg-info/top_level.txt\r\n4.156   writing manifest file '/tmp/pip-pip-egg-info-d2_c2trw/mamba_ssm.egg-info/SOURCES.txt'\r\n4.250   reading manifest file '/tmp/pip-pip-egg-info-d2_c2trw/mamba_ssm.egg-info/SOURCES.txt'\r\n4.250   adding license file 'LICENSE'\r\n4.251   adding license file 'AUTHORS'\r\n4.252   writing manifest file '/tmp/pip-pip-egg-info-d2_c2trw/mamba_ssm.egg-info/SOURCES.txt'\r\n4.670   Preparing metadata (setup.py): finished with status 'done'\r\n4.964 Collecting causal-conv1d>=1.2.0 (from -r requirements-mamba.txt (line 3))\r\n5.029   Downloading causal_conv1d-1.4.0.tar.gz (9.3 kB)\r\n5.048   Preparing metadata (setup.py): started\r\n5.049   Running command python setup.py egg_info\r\n7.259   No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\r\n7.351\r\n7.352\r\n7.352   torch.__version__  = 2.4.0+cu121\r\n7.353\r\n7.353\r\n7.354   running egg_info\r\n7.355   creating /tmp/pip-pip-egg-info-uye1h_am/causal_conv1d.egg-info\r\n7.363   writing /tmp/pip-pip-egg-info-uye1h_am/causal_conv1d.egg-info/PKG-INFO\r\n7.364   writing dependency_links to /tmp/pip-pip-egg-info-uye1h_am/causal_conv1d.egg-info/dependency_links.txt\r\n7.364   writing requirements to /tmp/pip-pip-egg-info-uye1h_am/causal_conv1d.egg-info/requires.txt\r\n7.365   writing top-level names to /tmp/pip-pip-egg-info-uye1h_am/causal_conv1d.egg-info/top_level.txt\r\n7.366   writing manifest file '/tmp/pip-pip-egg-info-uye1h_am/causal_conv1d.egg-info/SOURCES.txt'\r\n7.458   reading manifest file '/tmp/pip-pip-egg-info-uye1h_am/causal_conv1d.egg-info/SOURCES.txt'\r\n7.459   adding license file 'LICENSE'\r\n7.460   adding license file 'AUTHORS'\r\n7.460   writing manifest file '/tmp/pip-pip-egg-info-uye1h_am/causal_conv1d.egg-info/SOURCES.txt'\r\n7.917   Preparing metadata (setup.py): finished with status 'done'\r\n7.928 Building wheels for collected packages: mamba-ssm, causal-conv1d\r\n7.929   Building wheel for mamba-ssm (setup.py): started\r\n7.930   Running command python setup.py bdist_wheel\r\n10.18   No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\r\n10.18\r\n10.18\r\n10.18   torch.__version__  = 2.4.0+cu121\r\n10.18\r\n10.18\r\n10.23   running bdist_wheel\r\n10.26   Guessing wheel URL:  https://github.com/state-spaces/mamba/releases/download/v2.2.2/mamba_ssm-2.2.2+cu122torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\r\n623.7   error: <urlopen error retrieval incomplete: got only 178257920 out of 323988104 bytes>\r\n624.1   error: subprocess-exited-with-error\r\n624.1\r\n624.1   \u00d7 python setup.py bdist_wheel did not run successfully.\r\n624.1   \u2502 exit code: 1\r\n624.1   \u2570\u2500> See above for output.\r\n624.1\r\n624.1   note: This error originates from a subprocess, and is likely not a problem with pip.\r\n\r\n624.1   # - It imports setuptools before invoking setup.py, to enable projects that directly\r\n624.1   #   import from `distutils.core` to work with newer packaging standards.\r\n624.1   # - It provides a clear error message when setuptools is not installed.\r\n624.1   # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so\r\n624.1   #   setuptools doesn'\"'\"'t think the script is `-c`. This avoids the following warning:\r\n624.1   #     manifest_maker: standard file '\"'\"'-c'\"'\"' not found\".\r\n624.1   # - It generates a shim setup.py, for handling setup.cfg-only projects.\r\n624.1   import os, sys, tokenize\r\n624.1\r\n624.1   try:\r\n624.1       import setuptools\r\n624.1   except ImportError as error:\r\n624.1       print(\r\n624.1           \"ERROR: Can not execute `setup.py` since setuptools is not available in \"\r\n624.1           \"the build environment.\",\r\n624.1           file=sys.stderr,\r\n624.1       )\r\n624.1       sys.exit(1)\r\n624.1\r\n624.1   __file__ = %r\r\n624.1   sys.argv[0] = __file__\r\n624.1\r\n624.1   if os.path.exists(__file__):\r\n624.1       filename = __file__\r\n624.1       with tokenize.open(__file__) as f:\r\n624.1           setup_py_code = f.read()\r\n624.1   else:\r\n624.1       filename = \"<auto-generated setuptools caller>\"\r\n624.1       setup_py_code = \"from setuptools import setup; setup()\"\r\n624.1\r\n624.1   exec(compile(setup_py_code, filename, \"exec\"))\r\n624.1   '\"'\"''\"'\"''\"'\"' % ('\"'\"'/tmp/pip-wheel-uaoqe42l/mamba-ssm_d3adad62e7824b8383a22aabf455e8de/setup.py'\"'\"',), \"<pip-setuptools-caller>\", \"exec\"))' bdist_wheel -d /tmp/pip-wheel-0tek0r5o\r\n624.1   cwd: /tmp/pip-wheel-uaoqe42l/mamba-ssm_d3adad62e7824b8383a22aabf455e8de/\r\n624.1   Building wheel for mamba-ssm (setup.py): finished with status 'error'\r\n624.1   ERROR: Failed building wheel for mamba-ssm\r\n624.1   Running setup.py clean for mamba-ssm\r\n624.1   Running command python setup.py clean\r\n626.1   No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\r\n626.2\r\n626.2\r\n626.2   torch.__version__  = 2.4.0+cu121\r\n626.2\r\n626.2\r\n626.2   running clean\r\n626.2   'build/lib.linux-x86_64-cpython-310' does not exist -- can't clean it\r\n626.2   'build/bdist.linux-x86_64' does not exist -- can't clean it\r\n626.2   'build/scripts-3.10' does not exist -- can't clean it\r\n626.6   Building wheel for causal-conv1d (setup.py): started\r\n626.6   Running command python setup.py bdist_wheel\r\n628.6   No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\r\n628.6\r\n628.6\r\n628.6   torch.__version__  = 2.4.0+cu121\r\n628.6\r\n628.6\r\n628.7   running bdist_wheel\r\n628.7   Guessing wheel URL:  https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.4.0/causal_conv1d-1.4.0+cu122torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\r\n908.5   Raw wheel path /tmp/pip-wheel-mfpcfgry/causal_conv1d-1.4.0-cp310-cp310-linux_x86_64.whl\r\n908.8   Building wheel for causal-conv1d (setup.py): finished with status 'done'\r\n909.0   Created wheel for causal-conv1d: filename=causal_conv1d-1.4.0-cp310-cp310-linux_x86_64.whl size=104867883 sha256=b5e7cf7e964b5e99275d97ba1e1b0ee4e3073f4593743ba1f1c6aa394a3008cc\r\n909.0   Stored in directory: /tmp/pip-ephem-wheel-cache-62nr0jbv/wheels/e3/dd/4c/205f24e151736bd22f5980738dd10a19af6f093b6f4dcab006\r\n909.0 Successfully built causal-conv1d\r\n909.0 Failed to build mamba-ssm\r\n909.6 ERROR: Failed to build one or more wheels\r\n------\r\nDockerfile.longshine:152\r\n--------------------\r\n 151 |     # Download the wheel or build it if a pre-compiled release doesn't exist\r\n 152 | >>> RUN pip --verbose wheel -r requirements-mamba.txt \\\r\n 153 | >>>     --no-build-isolation --no-deps --no-cache-dir\r\n 154 |\r\n--------------------\r\nERROR: failed to solve: process \"/bin/sh -c pip --verbose wheel -r requirements-mamba.txt     --no-build-isolation --no-deps --no-cache-dir\" did not complete successfully: exit code: 1\r\n```\r\n\r\n### How you are installing vllm\r\n\r\nvllm version: v0.5.2, v0.5.3, v0.5.4 (versions that have mamba)\r\ncuda tool kit:  cuda 12.1\r\ncuda driver:  535.183.01\r\ndevice: RTX 4090\r\nCPU: intel \r\ndocker build -t xxxx -f Dockerfile .\r\n",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2024-08-14T02:48:22+00:00",
    "closed_at": "2024-10-11T15:56:30+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7498/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7498"
  },
  {
    "number": 8996,
    "title": "[Installation]: RISC-V support?",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Fedora Linux 38 (Thirty Eight) (riscv64)\r\nGCC version: (GCC) 13.2.1 20230728 (Red Hat 13.2.1-1)\r\nClang version: Could not collect\r\nCMake version: version 3.27.4\r\nLibc version: glibc-2.37\r\n\r\nPython version: 3.11.5 (main, Sep 15 2023, 00:00:00) [GCC 13.2.1 20230728 (Red Hat 13.2.1-1)] (64-bit runtime)\r\nPython platform: Linux-6.1.80-riscv64-with-glibc2.37\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: N/A\r\n\r\nCPU:\r\nArchitecture:        riscv64\r\nByte Order:          Little Endian\r\nCPU(s):              128\r\nOn-line CPU(s) list: 0-127\r\nNUMA node(s):        8\r\nNUMA node0 CPU(s):   0-7,16-23\r\nNUMA node1 CPU(s):   8-15,24-31\r\nNUMA node2 CPU(s):   32-39,48-55\r\nNUMA node3 CPU(s):   40-47,56-63\r\nNUMA node4 CPU(s):   64-71,80-87\r\nNUMA node5 CPU(s):   72-79,88-95\r\nNUMA node6 CPU(s):   96-103,112-119\r\nNUMA node7 CPU(s):   104-111,120-127\r\n\r\nVersions of relevant libraries:\r\n[pip3] No relevant packages\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\r\n\n\n### How you are installing vllm\n\n```sh\r\npip install -v -r requirements-cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu\r\n```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-01T09:51:30+00:00",
    "closed_at": "2025-01-31T01:58:15+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8996/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8996"
  },
  {
    "number": 17015,
    "title": "[Installation]:  Cannot install vllm due to xformers:   ERROR: Failed building wheel for xformers  fatal: Not a git repository (or any parent up to mount point /scratch)  assert len(sources) > 0   AssertionError",
    "body": "\nHi all, \n\nI am pip installing the latest vllm, 0.8.4.\n\nCUDA: 12.4\ntorch: 2.6.0\nPython: 3.12.1\n\n```text\nThe output of `python collect_env.py`\n```\n\npython collect_env.py\nTraceback (most recent call last):\n  File \"/program/ms-swift/collect_env.py\", line 17, in <module>\n    from vllm.envs import environment_variables\nModuleNotFoundError: No module named 'vllm'\n\n\n\nI got several errors:\n\nERROR: Failed building wheel for xformers \nfatal: Not a git repository (or any parent up to mount point /scratch) \nassert len(sources) > 0 AssertionError \n\n\nError logs:\n----------------------------------------\n\n(myvenv_msswift) [data@sh1 /program//ms-swift] $ pip install xformers\nCollecting xformers\n  Using cached xformers-0.0.29.post3.tar.gz (8.5 MB)\n\n\n  Preparing metadata (setup.py) ... done\nBuilding wheels for collected packages: xformers\n  Building wheel for xformers (setup.py) ... error\n  error: subprocess-exited-with-error\n  \n  \u00d7 python setup.py bdist_wheel did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [257 lines of output]\n      fatal: Not a git repository (or any parent up to mount point /scratch)\n      Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n      /program//myvenv_msswift/lib/python3.12/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n      !!\n      \n              ********************************************************************************\n              Please consider removing the following classifiers in favor of a SPDX license expression:\n      \n              License :: OSI Approved :: BSD License\n      \n              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n              ********************************************************************************\n      \n      !!\n        self._finalize_license_expression()\n      running bdist_wheel\n      running build\n      running build_py\n      creating build/lib.linux-x86_64-cpython-312/xformers\n      copying xformers/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers\n      copying xformers/test.py -> build/lib.linux-x86_64-cpython-312/xformers\n      copying xformers/attn_bias_utils.py -> build/lib.linux-x86_64-cpython-312/xformers\n      copying xformers/checkpoint.py -> build/lib.linux-x86_64-cpython-312/xformers\n      copying xformers/info.py -> build/lib.linux-x86_64-cpython-312/xformers\n      copying xformers/_deprecation_warning.py -> build/lib.linux-x86_64-cpython-312/xformers\n      copying xformers/utils.py -> build/lib.linux-x86_64-cpython-312/xformers\n      copying xformers/_cpp_lib.py -> build/lib.linux-x86_64-cpython-312/xformers\n      creating build/lib.linux-x86_64-cpython-312/xformers/ops\n      copying xformers/ops/sp24.py -> build/lib.linux-x86_64-cpython-312/xformers/ops\n      copying xformers/ops/differentiable_collectives.py -> build/lib.linux-x86_64-cpython-312/xformers/ops\n      copying xformers/ops/indexing.py -> build/lib.linux-x86_64-cpython-312/xformers/ops\n      copying xformers/ops/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/ops\n      copying xformers/ops/swiglu_op.py -> build/lib.linux-x86_64-cpython-312/xformers/ops\n      copying xformers/ops/rmsnorm.py -> build/lib.linux-x86_64-cpython-312/xformers/ops\n      copying xformers/ops/tiled_matmul.py -> build/lib.linux-x86_64-cpython-312/xformers/ops\n      copying xformers/ops/seqpar.py -> build/lib.linux-x86_64-cpython-312/xformers/ops\n      copying xformers/ops/common.py -> build/lib.linux-x86_64-cpython-312/xformers/ops\n      copying xformers/ops/unbind.py -> build/lib.linux-x86_64-cpython-312/xformers/ops\n      copying xformers/ops/modpar_layers.py -> build/lib.linux-x86_64-cpython-312/xformers/ops\n      copying xformers/ops/sequence_parallel_fused_ops.py -> build/lib.linux-x86_64-cpython-312/xformers/ops\n      copying xformers/ops/rope_padded.py -> build/lib.linux-x86_64-cpython-312/xformers/ops\n      creating build/lib.linux-x86_64-cpython-312/xformers/benchmarks\n      copying xformers/benchmarks/benchmark_swiglu.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks\n      copying xformers/benchmarks/benchmark_indexing.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks\n      copying xformers/benchmarks/benchmark_sp24.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks\n      copying xformers/benchmarks/benchmark_revnet.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks\n      copying xformers/benchmarks/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks\n      copying xformers/benchmarks/benchmark_attn_decoding.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks\n      copying xformers/benchmarks/benchmark_sddmm.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks\n      copying xformers/benchmarks/benchmark_tiled_matmul.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks\n      copying xformers/benchmarks/benchmark_merge_attentions.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks\n      copying xformers/benchmarks/benchmark_core.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks\n      copying xformers/benchmarks/benchmark_mem_eff_attention.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks\n      copying xformers/benchmarks/benchmark_sequence_parallel_fused.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks\n      copying xformers/benchmarks/utils.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks\n      copying xformers/benchmarks/benchmark_nystrom_utils.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks\n      creating build/lib.linux-x86_64-cpython-312/xformers/triton\n      copying xformers/triton/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/triton\n      copying xformers/triton/importing.py -> build/lib.linux-x86_64-cpython-312/xformers/triton\n      copying xformers/triton/vararg_kernel.py -> build/lib.linux-x86_64-cpython-312/xformers/triton\n      creating build/lib.linux-x86_64-cpython-312/xformers/_flash_attn\n      copying xformers/_flash_attn/fused_softmax.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn\n      copying xformers/_flash_attn/flash_attn_interface.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn\n      copying xformers/_flash_attn/flash_attn_triton.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn\n      copying xformers/_flash_attn/flash_attn_triton_og.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn\n      copying xformers/_flash_attn/flash_blocksparse_attention.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn\n      copying xformers/_flash_attn/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn\n      copying xformers/_flash_attn/flash_blocksparse_attn_interface.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn\n      copying xformers/_flash_attn/bert_padding.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn\n      creating build/lib.linux-x86_64-cpython-312/xformers/components\n      copying xformers/components/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/components\n      copying xformers/components/input_projection.py -> build/lib.linux-x86_64-cpython-312/xformers/components\n      copying xformers/components/residual.py -> build/lib.linux-x86_64-cpython-312/xformers/components\n      creating build/lib.linux-x86_64-cpython-312/xformers/sparse\n      copying xformers/sparse/_csr_ops.py -> build/lib.linux-x86_64-cpython-312/xformers/sparse\n      copying xformers/sparse/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/sparse\n      copying xformers/sparse/blocksparse_tensor.py -> build/lib.linux-x86_64-cpython-312/xformers/sparse\n      copying xformers/sparse/csr_tensor.py -> build/lib.linux-x86_64-cpython-312/xformers/sparse\n      copying xformers/sparse/utils.py -> build/lib.linux-x86_64-cpython-312/xformers/sparse\n      creating build/lib.linux-x86_64-cpython-312/xformers/profiler\n      copying xformers/profiler/find_slowest.py -> build/lib.linux-x86_64-cpython-312/xformers/profiler\n      copying xformers/profiler/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/profiler\n      copying xformers/profiler/device_limits.py -> build/lib.linux-x86_64-cpython-312/xformers/profiler\n      copying xformers/profiler/profiler_dcgm_impl.py -> build/lib.linux-x86_64-cpython-312/xformers/profiler\n      copying xformers/profiler/profiler_dcgm.py -> build/lib.linux-x86_64-cpython-312/xformers/profiler\n      copying xformers/profiler/profiler.py -> build/lib.linux-x86_64-cpython-312/xformers/profiler\n      copying xformers/profiler/api.py -> build/lib.linux-x86_64-cpython-312/xformers/profiler\n      copying xformers/profiler/profile_analyzer.py -> build/lib.linux-x86_64-cpython-312/xformers/profiler\n      creating build/lib.linux-x86_64-cpython-312/xformers/ops/fmha\n      copying xformers/ops/fmha/dispatch.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/fmha\n      copying xformers/ops/fmha/ck_splitk.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/fmha\n      copying xformers/ops/fmha/ck.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/fmha\n      copying xformers/ops/fmha/attn_bias.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/fmha\n      copying xformers/ops/fmha/ck_decoder.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/fmha\n      copying xformers/ops/fmha/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/fmha\n      copying xformers/ops/fmha/cutlass.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/fmha\n      copying xformers/ops/fmha/triton_splitk.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/fmha\n      copying xformers/ops/fmha/flash.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/fmha\n      copying xformers/ops/fmha/common.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/fmha\n      copying xformers/ops/fmha/torch_attention_compat.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/fmha\n      copying xformers/ops/fmha/flash3.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/fmha\n      creating build/lib.linux-x86_64-cpython-312/xformers/ops/_triton\n      copying xformers/ops/_triton/k_index_select_cat.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/_triton\n      copying xformers/ops/_triton/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/_triton\n      copying xformers/ops/_triton/rope_padded_kernels.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/_triton\n      copying xformers/ops/_triton/tiled_matmul_kernels.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/_triton\n      copying xformers/ops/_triton/k_scaled_index_add.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/_triton\n      copying xformers/ops/_triton/rmsnorm_kernels.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/_triton\n      copying xformers/ops/_triton/matmul_perf_model.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/_triton\n      creating build/lib.linux-x86_64-cpython-312/xformers/ops/fmha/_triton\n      copying xformers/ops/fmha/_triton/splitk_kernels.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/fmha/_triton\n      copying xformers/ops/fmha/_triton/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/ops/fmha/_triton\n      creating build/lib.linux-x86_64-cpython-312/xformers/benchmarks/LRA\n      copying xformers/benchmarks/LRA/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks/LRA\n      copying xformers/benchmarks/LRA/run_with_submitit.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks/LRA\n      copying xformers/benchmarks/LRA/run_grid_search.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks/LRA\n      copying xformers/benchmarks/LRA/batch_fetch_results.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks/LRA\n      copying xformers/benchmarks/LRA/batch_submit.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks/LRA\n      copying xformers/benchmarks/LRA/run_tasks.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks/LRA\n      creating build/lib.linux-x86_64-cpython-312/xformers/benchmarks/LRA/code\n      copying xformers/benchmarks/LRA/code/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks/LRA/code\n      copying xformers/benchmarks/LRA/code/dataset.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks/LRA/code\n      copying xformers/benchmarks/LRA/code/model_wrapper.py -> build/lib.linux-x86_64-cpython-312/xformers/benchmarks/LRA/code\n      creating build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/models\n      copying xformers/_flash_attn/models/gpt_neox.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/models\n      copying xformers/_flash_attn/models/bigcode.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/models\n      copying xformers/_flash_attn/models/falcon.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/models\n      copying xformers/_flash_attn/models/llama.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/models\n      copying xformers/_flash_attn/models/opt.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/models\n      copying xformers/_flash_attn/models/vit.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/models\n      copying xformers/_flash_attn/models/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/models\n      copying xformers/_flash_attn/models/gptj.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/models\n      copying xformers/_flash_attn/models/baichuan.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/models\n      copying xformers/_flash_attn/models/bert.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/models\n      copying xformers/_flash_attn/models/btlm.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/models\n      copying xformers/_flash_attn/models/gpt.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/models\n      creating build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/ops\n      copying xformers/_flash_attn/ops/rms_norm.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/ops\n      copying xformers/_flash_attn/ops/layer_norm.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/ops\n      copying xformers/_flash_attn/ops/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/ops\n      copying xformers/_flash_attn/ops/fused_dense.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/ops\n      copying xformers/_flash_attn/ops/activations.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/ops\n      creating build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/layers\n      copying xformers/_flash_attn/layers/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/layers\n      copying xformers/_flash_attn/layers/patch_embed.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/layers\n      copying xformers/_flash_attn/layers/rotary.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/layers\n      creating build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/losses\n      copying xformers/_flash_attn/losses/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/losses\n      copying xformers/_flash_attn/losses/cross_entropy.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/losses\n      creating build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/modules\n      copying xformers/_flash_attn/modules/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/modules\n      copying xformers/_flash_attn/modules/mlp.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/modules\n      copying xformers/_flash_attn/modules/mha.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/modules\n      copying xformers/_flash_attn/modules/embedding.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/modules\n      copying xformers/_flash_attn/modules/block.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/modules\n      creating build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/utils\n      copying xformers/_flash_attn/utils/pretrained.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/utils\n      copying xformers/_flash_attn/utils/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/utils\n      copying xformers/_flash_attn/utils/generation.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/utils\n      copying xformers/_flash_attn/utils/benchmark.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/utils\n      copying xformers/_flash_attn/utils/distributed.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/utils\n      creating build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/flash_attn_triton_amd\n      copying xformers/_flash_attn/flash_attn_triton_amd/fwd_ref.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/flash_attn_triton_amd\n      copying xformers/_flash_attn/flash_attn_triton_amd/bwd_prefill.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/flash_attn_triton_amd\n      copying xformers/_flash_attn/flash_attn_triton_amd/bwd_ref.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/flash_attn_triton_amd\n      copying xformers/_flash_attn/flash_attn_triton_amd/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/flash_attn_triton_amd\n      copying xformers/_flash_attn/flash_attn_triton_amd/bench.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/flash_attn_triton_amd\n      copying xformers/_flash_attn/flash_attn_triton_amd/interface_fa.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/flash_attn_triton_amd\n      copying xformers/_flash_attn/flash_attn_triton_amd/test.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/flash_attn_triton_amd\n      copying xformers/_flash_attn/flash_attn_triton_amd/fwd_prefill.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/flash_attn_triton_amd\n      copying xformers/_flash_attn/flash_attn_triton_amd/interface_torch.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/flash_attn_triton_amd\n      copying xformers/_flash_attn/flash_attn_triton_amd/fwd_decode.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/flash_attn_triton_amd\n      copying xformers/_flash_attn/flash_attn_triton_amd/utils.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/flash_attn_triton_amd\n      creating build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/ops/triton\n      copying xformers/_flash_attn/ops/triton/layer_norm.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/ops/triton\n      copying xformers/_flash_attn/ops/triton/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/ops/triton\n      copying xformers/_flash_attn/ops/triton/k_activations.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/ops/triton\n      copying xformers/_flash_attn/ops/triton/cross_entropy.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/ops/triton\n      copying xformers/_flash_attn/ops/triton/mlp.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/ops/triton\n      copying xformers/_flash_attn/ops/triton/linear.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/ops/triton\n      copying xformers/_flash_attn/ops/triton/rotary.py -> build/lib.linux-x86_64-cpython-312/xformers/_flash_attn/ops/triton\n      creating build/lib.linux-x86_64-cpython-312/xformers/components/attention\n      copying xformers/components/attention/sparsity_config.py -> build/lib.linux-x86_64-cpython-312/xformers/components/attention\n      copying xformers/components/attention/__init__.py -> build/lib.linux-x86_64-cpython-312/xformers/components/attention\n      copying xformers/components/attention/attention_mask.py -> build/lib.linux-x86_64-cpython-312/xformers/components/attention\n      copying xformers/components/attention/base.py -> build/lib.linux-x86_64-cpython-312/xformers/components/attention\n      copying xformers/components/attention/core.py -> build/lib.linux-x86_64-cpython-312/xformers/components/attention\n      copying xformers/components/attention/scaled_dot_product.py -> build/lib.linux-x86_64-cpython-312/xformers/components/attention\n      copying xformers/components/attention/_sputnik_sparse.py -> build/lib.linux-x86_64-cpython-312/xformers/components/attention\n      copying xformers/components/attention/attention_patterns.py -> build/lib.linux-x86_64-cpython-312/xformers/components/attention\n      copying xformers/components/attention/fourier_mix.py -> build/lib.linux-x86_64-cpython-312/xformers/components/attention\n      copying xformers/components/attention/utils.py -> build/lib.linux-x86_64-cpython-312/xformers/components/attention\n      running build_ext\n      /program//myvenv_msswift/lib/python3.12/site-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no c++ version bounds defined for CUDA version 12.4\n        warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n      building 'xformers._C_flashattention3' extension\n      Emitting ninja build file /program//tmp/pip-install-gt0wu8az/xformers_ffc4a3a3171944fa93faf9b117af7b8c/build/temp.linux-x86_64-cpython-312/build.ninja...\n      Creating directory /program//tmp/pip-install-gt0wu8az/xformers_ffc4a3a3171944fa93faf9b117af7b8c/build/temp.linux-x86_64-cpython-312...\n      Traceback (most recent call last):\n        File \"<string>\", line 2, in <module>\n        File \"<pip-setuptools-caller>\", line 34, in <module>\n        File \"/program//tmp/pip-install-gt0wu8az/xformers_ffc4a3a3171944fa93faf9b117af7b8c/setup.py\", line 700, in <module>\n          setuptools.setup(\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/__init__.py\", line 117, in setup\n          return distutils.core.setup(**attrs)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 186, in setup\n          return run_commands(dist)\n                 ^^^^^^^^^^^^^^^^^^\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 202, in run_commands\n          dist.run_commands()\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1002, in run_commands\n          self.run_command(cmd)\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/dist.py\", line 1104, in run_command\n          super().run_command(command)\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1021, in run_command\n          cmd_obj.run()\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/command/bdist_wheel.py\", line 370, in run\n          self.run_command(\"build\")\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 357, in run_command\n          self.distribution.run_command(command)\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/dist.py\", line 1104, in run_command\n          super().run_command(command)\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1021, in run_command\n          cmd_obj.run()\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/_distutils/command/build.py\", line 135, in run\n          self.run_command(cmd_name)\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 357, in run_command\n          self.distribution.run_command(command)\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/dist.py\", line 1104, in run_command\n          super().run_command(command)\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1021, in run_command\n          cmd_obj.run()\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/command/build_ext.py\", line 99, in run\n          _build_ext.run(self)\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 368, in run\n          self.build_extensions()\n        File \"/program//tmp/pip-install-gt0wu8az/xformers_ffc4a3a3171944fa93faf9b117af7b8c/setup.py\", line 657, in build_extensions\n          super().build_extensions()\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 900, in build_extensions\n          build_ext.build_extensions(self)\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 484, in build_extensions\n          self._build_extensions_serial()\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 510, in _build_extensions_serial\n          self.build_extension(ext)\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/command/build_ext.py\", line 264, in build_extension\n          _build_ext.build_extension(self, ext)\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 565, in build_extension\n          objects = self.compiler.compile(\n                    ^^^^^^^^^^^^^^^^^^^^^^\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 713, in unix_wrap_ninja_compile\n          _write_ninja_file_and_compile_objects(\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1855, in _write_ninja_file_and_compile_objects\n          _write_ninja_file(\n        File \"/program//myvenv_msswift/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 2409, in _write_ninja_file\n          assert len(sources) > 0\n      AssertionError\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for xformers\n  Running setup.py clean for xformers\nFailed to build xformers\nERROR: Failed to build installable wheels for some pyproject.toml based projects (xformers)\n\n### How you are installing vllm\n\nI am running pip install vllm.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "open",
    "created_at": "2025-04-23T00:09:22+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17015/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17015"
  }
]
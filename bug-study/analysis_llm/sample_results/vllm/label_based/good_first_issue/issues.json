[
  {
    "number": 8947,
    "title": "[Bug]: vllm serve --config.yaml - Order of arguments matters?",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Rocky Linux release 8.10 (Green Obsidian) (x86_64)\r\nGCC version: (GCC) 11.3.0\r\nClang version: Could not collect\r\nCMake version: version 3.26.5\r\nLibc version: glibc-2.28\r\n\r\nPython version: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-4.18.0-553.16.1.el8_10.x86_64-x86_64-with-glibc2.28\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H100\r\nGPU 1: NVIDIA H100\r\nGPU 2: NVIDIA H100\r\n  MIG 1g.12gb     Device  0:\r\n  MIG 1g.12gb     Device  1:\r\n  MIG 1g.12gb     Device  2:\r\n  MIG 1g.12gb     Device  3:\r\n  MIG 1g.12gb     Device  4:\r\n  MIG 1g.12gb     Device  5:\r\n  MIG 1g.12gb     Device  6:\r\nGPU 3: NVIDIA H100\r\n\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              96\r\nOn-line CPU(s) list: 0-95\r\nThread(s) per core:  1\r\nCore(s) per socket:  48\r\nSocket(s):           2\r\nNUMA node(s):        8\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               143\r\nModel name:          Intel(R) Xeon(R) Platinum 8468\r\nStepping:            8\r\nCPU MHz:             3701.598\r\nCPU max MHz:         3800.0000\r\nCPU min MHz:         800.0000\r\nBogoMIPS:            4200.00\r\nL1d cache:           48K\r\nL1i cache:           32K\r\nL2 cache:            2048K\r\nL3 cache:            107520K\r\nNUMA node0 CPU(s):   0-11\r\nNUMA node1 CPU(s):   12-23\r\nNUMA node2 CPU(s):   24-35\r\nNUMA node3 CPU(s):   36-47\r\nNUMA node4 CPU(s):   48-59\r\nNUMA node5 CPU(s):   60-71\r\nNUMA node6 CPU(s):   72-83\r\nNUMA node7 CPU(s):   84-95\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.68\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] sentence-transformers==3.0.1\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.1\r\n[pip3] triton==3.0.0\r\n[conda] No relevant packages\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.1.dev238+ge2c6e0a82\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV6     NV6     NV6     SYS     SYS     0-11    0               N/A\r\nGPU1    NV6      X      NV6     SYS     PIX     SYS     24-35   2               N/A\r\nGPU2    SYS     SYS      X      SYS     SYS     SYS     48-59   4               N/A\r\nGPU3    NV6     SYS     NV6      X      SYS     PIX     72-83   6               N/A\r\nNIC0    SYS     PIX     SYS     SYS      X      SYS\r\nNIC1    SYS     SYS     SYS     PIX     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nWhen serving a vllm server with ``vllm serve path/to/model --config path/to/config.yaml`` the position of the argument ``served-model-name`` seems to be cruical to successfully run the server.\r\n\r\nP.s. \r\nWhy is `collect_env.py` showing `vLLM Version: 0.6.1.dev238+ge2c6e0a82`\r\nI definitely used vllm=0.6.2 and my pip shows the same.\r\n\r\nConfig that works flawlessly:\r\n```\r\nserved-model-name: \"MyModel\"\r\nhost: \"127.0.0.1\"\r\nport: 6379\r\nuvicorn-log-level: \"info\"\r\n```\r\nHere, the server runs and I can call the model using the name \"MyModel\".\r\n\r\nConfig that does not work:\r\n```\r\nhost: \"127.0.0.1\"\r\nport: 6379\r\nuvicorn-log-level: \"info\"\r\nserved-model-name: \"MyModel\"\r\n```\r\n\r\nWith the latter config, I get the following error:\r\n``vllm serve: error: the following arguments are required: model_tag``\r\n\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-09-29T15:06:36+00:00",
    "closed_at": "2024-10-05T17:35:13+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8947/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8947"
  },
  {
    "number": 3666,
    "title": "[Misc]: Implement CPU/GPU swapping in BlockManagerV2",
    "body": "Recently, we refactored the block manager subsystem to improve testability by separating concerns of each layer. See https://github.com/vllm-project/vllm/pull/3492 for more information.\r\n\r\nThe V2 implementation does not have support for CPU-GPU swapping. It can be added in the [CpuGpuBlockAllocator](https://github.com/vllm-project/vllm/blob/321dc1619ad60b6df74fa86ac6299bc83c223996/vllm/core/block/cpu_gpu_block_allocator.py). My first take on the design is that it should simply keep track of the requested swap requests and have the scheduler `get_and_clear` them after each scheduling step.\r\n\r\n![image](https://github.com/vllm-project/vllm/assets/950914/55cf0db2-2614-463b-a053-eb3f182c01bb)\r\n",
    "labels": [
      "good first issue",
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-03-27T21:40:21+00:00",
    "closed_at": "2024-06-03T20:41:11+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3666/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3666"
  },
  {
    "number": 18166,
    "title": "[HELP WANTED] Fix Failing Spec Decoding Test",
    "body": "### Issue\n\nWe are seeing a test failure related to EAGLE on V0. We would appreciate anyone who can help addressing it. \n\n```bash\npytest -s -v tests/spec_decode/e2e/test_eagle_correctness.py::test_eagle_e2e_greedy_correctness_with_preemption\n```\n\nPR which disables the test: https://github.com/vllm-project/vllm/pull/18165\n\nIf anyone has capacity to help out with re-enabling this, we would greatly appreciate it!",
    "labels": [
      "bug",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2025-05-14T20:14:33+00:00",
    "closed_at": "2025-05-19T02:49:47+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18166/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/18166"
  },
  {
    "number": 7307,
    "title": "[Feature]: Testing - Use `torch.testing.assert_close` instead of `torch.allclose` as a Recommended Practice",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\r\n\r\nSee https://pytorch.org/docs/stable/testing.html\r\n\r\n`assert_close` will print the values which violate the allclose condition. `assert torch.allclose` will not\r\n\r\nThis leads to better diagnosability of failed tests\r\n\r\n\r\n",
    "labels": [
      "good first issue",
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-08-08T16:13:20+00:00",
    "closed_at": "2024-08-16T04:24:05+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7307/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7307"
  },
  {
    "number": 8481,
    "title": "[Feature]: Batch inference for `llm.chat()` API",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\r\n\r\nCurrently `llm.chat()` API only supports one conversation per inference. This means we cannot use this API to fully leverage vLLM for efficient offline processing.\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\n\r\nImplementation should be rather straightforward:\r\n\r\n1. at API level, `llm.chat()` should also accept a list of conversations.\r\n2. When `llm.chat()` is invoked, the list of conversations will be parsed into list of prompts, and all multimodal data items will be retrieved and loaded into their corresponding format that `llm.generate()` accepts.\r\n3. Send the list of `{prompt: xxx, multi_modal_data: xxx}` to the `llm.generate()`\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-09-14T03:17:43+00:00",
    "closed_at": "2024-09-24T16:44:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8481/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/8481"
  },
  {
    "number": 3148,
    "title": "Support `response_format: json_object` in OpenAI server",
    "body": "We just merged the support for structured generation support with Outlines. The next step is to integreate with Grammar based finite state machine https://github.com/outlines-dev/outlines/pull/541 into vLLM to support arbitrary JSON format. ",
    "labels": [
      "help wanted",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-03-01T19:12:00+00:00",
    "closed_at": "2024-03-16T20:35:28+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3148/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3148"
  },
  {
    "number": 13351,
    "title": "[Feature]: Consolidate performance benchmark datasets",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nOn vLLM we have two main benchmark scripts ([benchmark_throughput.py](https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_throughput.py) and [benchmark_serving.py](https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_serving.py)) to measure the performance of vLLM. \n\nHowever, the dataset sampling functions are defined within each script itself and over time it'll be hard to maintain these and to add new datasets to both scripts as we want to have the flexibility to run benchmark on different datasets.\n\n### Alternatives\n\nIdeally the dataset sampling should be defined in a separate file (e.g, `benchmark_dataset.py`) where we define the sampling functions for different datasets (sharegpt, sonnet, random, vision arena, etc), and the benchmark scripts themselves can simply import from benchmark_dataset depending on which dataset is specified at command line. \n\nThis modularization brings us a number of benefits:\n- Ensure the alignment of dataset sampling between the two benchmarks in case we want to compare the performance between online serving and offline inference.\n- Ease the process of adding new types of benchmark datasets.\n- Open up the opportunity to support user-defined custom datasets as long as they conform to a format that we pre-define.\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "help wanted",
      "good first issue",
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-02-16T09:04:02+00:00",
    "closed_at": "2025-03-15T05:49:45+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13351/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/13351"
  },
  {
    "number": 16162,
    "title": "[Bug]: --enable-prompt-tokens-details not working in V1",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 04-07 06:47:48 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\nClang version: Could not collect\nCMake version: version 3.18.4\nLibc version: glibc-2.31\n\nPython version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.10.0-34-cloud-amd64-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA L4\nNvidia driver version: 550.90.07\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nByte Order:                           Little Endian\nAddress sizes:                        46 bits physical, 48 bits virtual\nCPU(s):                               4\nOn-line CPU(s) list:                  0-3\nThread(s) per core:                   2\nCore(s) per socket:                   2\nSocket(s):                            1\nNUMA node(s):                         1\nVendor ID:                            GenuineIntel\nCPU family:                           6\nModel:                                85\nModel name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\nStepping:                             7\nCPU MHz:                              2200.172\nBogoMIPS:                             4400.34\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            64 KiB\nL1i cache:                            64 KiB\nL2 cache:                             2 MiB\nL3 cache:                             38.5 MiB\nNUMA node0 CPU(s):                    0-3\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; Enhanced IBRS\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.49.0\n[pip3] triton==3.2.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.3.0                   pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] transformers              4.49.0                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.1.dev5290+g7c5a2f8 (git sha: 7c5a2f8\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \t0-3\t0\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=/home/marko/miniconda3/envs/vllm/lib/python3.12/site-packages/cv2/../../lib64:\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nV1 does not report prompt token details if enabled. Seems implementation is missing.\n\n```\npython -m vllm.entrypoints.openai.api_server --model \"facebook/opt-125m\" \\\n    --enable-prompt-tokens-details \\\n    --enable-prefix-caching \\\n    --served-model-name latest \\\n    --chat-template=\"{% for message in messages %} {% if message[\\\"role\\\"] == \\\"user\\\" %} {{ \\\"Question:\\n\\\" + message[\\\"content\\\"] + \\\"\\n\\\" }} {% elif message[\\\"role\\\"] == \\\"system\\\" %} {{ \\\"System:\\n\\\" >\n```\n\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"dummy\", base_url=\"http://localhost:8000/v1\")\n\n# prompt with more than 16 tokens\nmessages = [{\"role\": \"user\", \"content\": \"Hello! What's your name? What do you do?\"}]\n\n# first request to cache the prompt\nresponse = client.chat.completions.create(model=\"latest\",messages=messages,max_tokens=256)\n# second request to fetch from prefix cache\nresponse = client.chat.completions.create(model=\"latest\",messages=messages,max_tokens=256)\n\nprint(response.usage)\n```\n\nV0 output\n```\nCompletionUsage(completion_tokens=73, prompt_tokens=24, total_tokens=97, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=16))\n```\n\nV1 output\n```\nCompletionUsage(completion_tokens=256, prompt_tokens=24, total_tokens=280, completion_tokens_details=None, prompt_tokens_details=None)\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2025-04-07T07:00:55+00:00",
    "closed_at": "2025-05-26T10:14:34+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16162/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16162"
  },
  {
    "number": 17036,
    "title": "[Bug]: Multiple openai endpoint Missing Content-Type Header",
    "body": "### Your current environment\n\nNot reletaed\n\n### \ud83d\udc1b Describe the bug\n\nDuring property-based testing of the vLLM API, defined by an OpenAPI 3.1 schema, we observed a recurring issue with missing Content-Type headers across several endpoints.\n\n```python\n1. GET /health:\nIssue: Missing Content-Type header\n\nDetails: The test failed because the server expects a Content-Type: application/json header, but it was not included in the request.\n\n\n2. GET /ping:\nIssue: Missing Content-Type header\n\nDetails: Similar to the previous failure, the Content-Type: application/json header was missing for the request to /ping.\n\n\n\n3. POST /ping:\nIssue: Missing Content-Type header\n\nDetails: The failure is due to the absence of the Content-Type: application/json header in the POST request to /ping.\n\nlogs:\n\ntest_new.py u,uuuu,,uu,,,,u,,,u.                                                                                                                                                                                                       [100%]\n\n================================================================================================================== FAILURES ==================================================================================================================\n_______________________________________________________________________________________ test_openapi_schema_with_fixture (verbose_name='GET /health') ________________________________________________________________________________________\n\n    @wraps(test)\n>   def test_function(*args: Any, **kwargs: Any) -> Any:\n\ndsd/lib64/python3.12/site-packages/schemathesis/_hypothesis.py:83: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncase = Case(headers={'Content-Type': 'application/json'})\n\n    @schema.parametrize()\n    def test_openapi_schema_with_fixture(case):\n        case.headers = {\n            \"Content-Type\": \"application/json\",\n        }\n>       case.call_and_validate(verify=False)\nE       schemathesis.exceptions.CheckFailed: \nE       \nE       1. Missing Content-Type header\nE       \nE           The following media types are documented in the schema:\nE           - `application/json`\nE       \nE       [200] OK:\nE       \nE           <EMPTY>\nE       \nE       Reproduce with: \nE       \nE           curl -X GET -H 'Content-Type: application/json' --insecure http://127.0.0.1:8080/health\nE       \nE       Falsifying example: test_openapi_schema_with_fixture(\nE           case=,\nE       )\n\ntest_new.py:46: CheckFailed\n----------------------------------------------------------------------------------------------------------- Captured stdout setup ------------------------------------------------------------------------------------------------------------\nhttp://127.0.0.1:8080\n------------------------------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------------------------------\n\n________________________________________________________________________________________ test_openapi_schema_with_fixture (verbose_name='GET /ping') _________________________________________________________________________________________\n\n    @wraps(test)\n>   def test_function(*args: Any, **kwargs: Any) -> Any:\n\ndsd/lib64/python3.12/site-packages/schemathesis/_hypothesis.py:83: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncase = Case(headers={'Content-Type': 'application/json'})\n\n    @schema.parametrize()\n    def test_openapi_schema_with_fixture(case):\n        case.headers = {\n            \"Content-Type\": \"application/json\",\n        }\n>       case.call_and_validate(verify=False)\nE       schemathesis.exceptions.CheckFailed: \nE       \nE       1. Missing Content-Type header\nE       \nE           The following media types are documented in the schema:\nE           - `application/json`\nE       \nE       [200] OK:\nE       \nE           <EMPTY>\nE       \nE       Reproduce with: \nE       \nE           curl -X GET -H 'Content-Type: application/json' --insecure http://127.0.0.1:8080/ping\nE       \nE       Falsifying example: test_openapi_schema_with_fixture(\nE           case=,\nE       )\n\ntest_new.py:46: CheckFailed\n----------------------------------------------------------------------------------------------------------- Captured stdout setup ------------------------------------------------------------------------------------------------------------\nhttp://127.0.0.1:8080\n------------------------------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------------------------------\n\n________________________________________________________________________________________ test_openapi_schema_with_fixture (verbose_name='POST /ping') ________________________________________________________________________________________\n\n    @wraps(test)\n>   def test_function(*args: Any, **kwargs: Any) -> Any:\n\ndsd/lib64/python3.12/site-packages/schemathesis/_hypothesis.py:83: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncase = Case(headers={'Content-Type': 'application/json'})\n\n    @schema.parametrize()\n    def test_openapi_schema_with_fixture(case):\n        case.headers = {\n            \"Content-Type\": \"application/json\",\n        }\n>       case.call_and_validate(verify=False)\nE       schemathesis.exceptions.CheckFailed: \nE       \nE       1. Missing Content-Type header\nE       \nE           The following media types are documented in the schema:\nE           - `application/json`\nE       \nE       [200] OK:\nE       \nE           <EMPTY>\nE       \nE       Reproduce with: \nE       \nE           curl -X POST -H 'Content-Type: application/json' --insecure http://127.0.0.1:8080/ping\nE       \nE       Falsifying example: test_openapi_schema_with_fixture(\nE           case=,\nE       )\n\ntest_new.py:46: CheckFailed\n----------------------------------------------------------------------------------------------------------- Captured stdout setup ------------------------------------------------------------------------------------------------------------\nhttp://127.0.0.1:8080\n------------------------------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------------------------------\n\n______________________________________________________________________________________ test_openapi_schema_with_fixture (verbose_name='POST /tokenize') ______________________________________________________________________________________\n\n    @wraps(test)\n>   def test_function(*args: Any, **kwargs: Any) -> Any:\n\ndsd/lib64/python3.12/site-packages/schemathesis/_hypothesis.py:83: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncase = Case(headers={'Content-Type': 'application/json'}, body={'messages': []})\n\n    @schema.parametrize()\n    def test_openapi_schema_with_fixture(case):\n        case.headers = {\n            \"Content-Type\": \"application/json\",\n        }\n>       case.call_and_validate(verify=False)\nE       schemathesis.exceptions.CheckFailed: \nE       \nE       1. Server error\nE       \nE       2. Undocumented HTTP status code\nE       \nE           Received: 500\nE           Documented: 200, 422\nE       \nE       [500] Internal Server Error:\nE       \nE           `Internal Server Error`\nE       \nE       Reproduce with: \nE       \nE           curl -X POST -H 'Content-Type: application/json' -d '{\"messages\": []}' --insecure http://127.0.0.1:8080/tokenize\nE       \nE       Falsifying example: test_openapi_schema_with_fixture(\nE           case=,\nE       )\n\ntest_new.py:46: CheckFailed\n----------------------------------------------------------------------------------------------------------- Captured stdout setup ------------------------------------------------------------------------------------------------------------\nhttp://127.0.0.1:8080\n------------------------------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------------------------------\n\n_____________________________________________________________________________________ test_openapi_schema_with_fixture (verbose_name='POST /detokenize') _____________________________________________________________________________________\n\n    @wraps(test)\n>   def test_function(*args: Any, **kwargs: Any) -> Any:\n\ndsd/lib64/python3.12/site-packages/schemathesis/_hypothesis.py:83: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncase = Case(headers={'Content-Type': 'application/json'}, body={'tokens': [-1]})\n\n    @schema.parametrize()\n    def test_openapi_schema_with_fixture(case):\n        case.headers = {\n            \"Content-Type\": \"application/json\",\n        }\n>       case.call_and_validate(verify=False)\nE       schemathesis.exceptions.CheckFailed: \nE       \nE       1. Server error\nE       \nE       2. Undocumented HTTP status code\nE       \nE           Received: 500\nE           Documented: 200, 422\nE       \nE       [500] Internal Server Error:\nE       \nE           `Internal Server Error`\nE       \nE       Reproduce with: \nE       \nE           curl -X POST -H 'Content-Type: application/json' -d '{\"tokens\": [-1]}' --insecure http://127.0.0.1:8080/detokenize\nE       \nE       Falsifying example: test_openapi_schema_with_fixture(\nE           case=,\nE       )\n\ntest_new.py:46: CheckFailed\n----------------------------------------------------------------------------------------------------------- Captured stdout setup ------------------------------------------------------------------------------------------------------------\nhttp://127.0.0.1:8080\n------------------------------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------------------------------\n\n________________________________________________________________________________ test_openapi_schema_with_fixture (verbose_name='POST /v1/chat/completions') _________________________________________________________________________________\n\n    @wraps(test)\n>   def test_function(*args: Any, **kwargs: Any) -> Any:\nE   schemathesis.exceptions.CheckFailed: Schemathesis found 2 distinct sets of failures.\nE   ==================== \nE   \nE   1. Server error\nE   \nE   2. Undocumented HTTP status code\nE   \nE       Received: 500\nE       Documented: 200, 422\nE   \nE   [500] Internal Server Error:\nE   \nE       `Internal Server Error`\nE   \nE   Reproduce with: \nE   \nE       curl -X POST -H 'Content-Type: application/json' -d '{\"messages\": []}' --insecure http://127.0.0.1:8080/v1/chat/completions\nE   \nE   ==================== \nE   \nE   1. Undocumented HTTP status code\nE   \nE       Received: 400\nE       Documented: 200, 422\nE   \nE   [400] Bad Request:\nE   \nE       `{\"object\":\"error\",\"message\":\"list object has no element 0\",\"type\":\"BadRequestError\",\"param\":null,\"code\":400}`\nE   \nE   Reproduce with: \nE   \nE       curl -X POST -H 'Content-Type: application/json' -d '{\"messages\": [{\"role\": \"assistant\"}]}' --insecure http://127.0.0.1:8080/v1/chat/completions\n\ndsd/lib64/python3.12/site-packages/schemathesis/_hypothesis.py:83: CheckFailed\n----------------------------------------------------------------------------------------------------------- Captured stdout setup ------------------------------------------------------------------------------------------------------------\nhttp://127.0.0.1:8080\n------------------------------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------------------------------\n\n___________________________________________________________________________________ test_openapi_schema_with_fixture (verbose_name='POST /v1/completions') ___________________________________________________________________________________\n\n    @wraps(test)\n>   def test_function(*args: Any, **kwargs: Any) -> Any:\nE   schemathesis.exceptions.CheckFailed: Schemathesis found 2 distinct sets of failures.\nE   ==================== \nE   \nE   1. Undocumented HTTP status code\nE   \nE       Received: 400\nE       Documented: 200, 422\nE   \nE   [400] Bad Request:\nE   \nE       `{\"object\":\"error\",\"message\":\"please provide at least one prompt\",\"type\":\"BadRequestError\",\"param\":null,\"code\":400}`\nE   \nE   Reproduce with: \nE   \nE       curl -X POST -H 'Content-Type: application/json' -d '{\"prompt\": []}' --insecure http://127.0.0.1:8080/v1/completions\nE   \nE   ==================== \nE   \nE   1. Server error\nE   \nE   2. Undocumented HTTP status code\nE   \nE       Received: 500\nE       Documented: 200, 422\nE   \nE   [500] Internal Server Error:\nE   \nE       `Internal Server Error`\nE   \nE   Reproduce with: \nE   \nE       curl -X POST -H 'Content-Type: application/json' -d '{\"prompt\": [-1]}' --insecure http://127.0.0.1:8080/v1/completions\n\ndsd/lib64/python3.12/site-packages/schemathesis/_hypothesis.py:83: CheckFailed\n----------------------------------------------------------------------------------------------------------- Captured stdout setup ------------------------------------------------------------------------------------------------------------\nhttp://127.0.0.1:8080\n------------------------------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------------------------------\n\n______________________________________________________________________________ test_openapi_schema_with_fixture (verbose_name='POST /v1/audio/transcriptions') _______________________________________________________________________________\n\n    @wraps(test)\n>   def test_function(*args: Any, **kwargs: Any) -> Any:\n\ndsd/lib64/python3.12/site-packages/schemathesis/_hypothesis.py:83: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncase = Case(headers={'Content-Type': 'application/json'}, body={'file': Binary(data=b'')})\n\n    @schema.parametrize()\n    def test_openapi_schema_with_fixture(case):\n        case.headers = {\n            \"Content-Type\": \"application/json\",\n        }\n>       case.call_and_validate(verify=False)\nE       schemathesis.exceptions.CheckFailed: \nE       \nE       1. Undocumented HTTP status code\nE       \nE           Received: 400\nE           Documented: 200, 422\nE       \nE       [400] Bad Request:\nE       \nE           `{\"object\":\"error\",\"message\":\"[{'type': 'value_error', 'loc': ('body', 'file'), 'msg': \\\"Value error, Expected UploadFile, received: <class 'str'>\\\", 'input': \\\"b''\\\", 'ctx': {'error': ValueError(\\\"Expected UploadFile, received: <class 'str'>\\\")}}]\",\"type\":\"BadRequestError\",\"param\":null,\"code\":400}`\nE       \nE       Reproduce with: \nE       \nE           curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' -d file=b%27%27 --insecure http://127.0.0.1:8080/v1/audio/transcriptions\nE       \nE       Falsifying example: test_openapi_schema_with_fixture(\nE           case=,\nE       )\n\ntest_new.py:46: CheckFailed\n----------------------------------------------------------------------------------------------------------- Captured stdout setup ------------------------------------------------------------------------------------------------------------\nhttp://127.0.0.1:8080\n------------------------------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------------------------------\n\n____________________________________________________________________________________ test_openapi_schema_with_fixture (verbose_name='POST /invocations') _____________________________________________________________________________________\n\n    @wraps(test)\n>   def test_function(*args: Any, **kwargs: Any) -> Any:\n\ndsd/lib64/python3.12/site-packages/schemathesis/_hypothesis.py:83: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncase = Case(headers={'Content-Type': 'application/json'})\n\n    @schema.parametrize()\n    def test_openapi_schema_with_fixture(case):\n        case.headers = {\n            \"Content-Type\": \"application/json\",\n        }\n>       case.call_and_validate(verify=False)\nE       schemathesis.exceptions.CheckFailed: \nE       \nE       1. Server error\nE       \nE       2. Undocumented HTTP status code\nE       \nE           Received: 500\nE           Documented: 200\nE       \nE       [500] Internal Server Error:\nE       \nE           `Internal Server Error`\nE       \nE       Reproduce with: \nE       \nE           curl -X POST -H 'Content-Type: application/json' --insecure http://127.0.0.1:8080/invocations\nE       \nE       Falsifying example: test_openapi_schema_with_fixture(\nE           case=,\nE       )\n\ntest_new.py:46: CheckFailed\n----------------------------------------------------------------------------------------------------------- Captured stdout setup ------------------------------------------------------------------------------------------------------------\nhttp://127.0.0.1:8080\n------------------------------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------------------------------\n\n============================================================================================================== warnings summary ==============================================================================================================\ndsd/lib64/python3.12/site-packages/schemathesis/generation/coverage.py:305\n  /home/takumar/Downloads/dsd/lib64/python3.12/site-packages/schemathesis/generation/coverage.py:305: DeprecationWarning: jsonschema.exceptions.RefResolutionError is deprecated as of version 4.18.0. If you wish to catch potential reference resolution errors, directly catch referencing.exceptions.Unresolvable.\n    ref_error: type[Exception] = jsonschema.RefResolutionError,\n\ndsd/lib64/python3.12/site-packages/schemathesis/internal/deprecation.py:6\n  /home/takumar/Downloads/dsd/lib64/python3.12/site-packages/schemathesis/internal/deprecation.py:6: DeprecationWarning: Argument `method` is deprecated and will be removed in Schemathesis 4.0. Use `include` and `exclude` methods instead.\n    warnings.warn(\n\n``` \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2025-04-23T08:06:01+00:00",
    "closed_at": "2025-05-10T06:13:33+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17036/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/17036"
  },
  {
    "number": 7993,
    "title": "[Bug]: gguf file without .gguf extension fails to run, even with \"--quantization gguf --load-format gguf\" flags",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\n$ python collect_env.py\r\nCollecting environment information...\r\nWARNING 08-29 11:55:28 _custom_ops.py:17] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Fedora release 40 (Forty) (x86_64)\r\nGCC version: (GCC) 14.2.1 20240801 (Red Hat 14.2.1-1)\r\nClang version: 18.1.6 (Fedora 18.1.6-3.fc40)\r\nCMake version: version 3.28.2\r\nLibc version: glibc-2.39\r\n\r\nPython version: 3.12.5 (main, Aug  7 2024, 00:00:00) [GCC 14.2.1 20240801 (Red Hat 14.2.1-1)] (64-bit runtime)\r\nPython platform: Linux-6.10.6-200.fc40.x86_64-x86_64-with-glibc2.39\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        39 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               12\r\nOn-line CPU(s) list:                  0-11\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Core(TM) i7-10850H CPU @ 2.70GHz\r\nCPU family:                           6\r\nModel:                                165\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   6\r\nSocket(s):                            1\r\nStepping:                             2\r\nCPU(s) scaling MHz:                   82%\r\nCPU max MHz:                          5100.0000\r\nCPU min MHz:                          800.0000\r\nBogoMIPS:                             5399.81\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp vnmi pku ospke md_clear flush_l1d arch_capabilities\r\nVirtualization:                       VT-x\r\nL1d cache:                            192 KiB (6 instances)\r\nL1i cache:                            192 KiB (6 instances)\r\nL2 cache:                             1.5 MiB (6 instances)\r\nL3 cache:                             12 MiB (1 instance)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-11\r\nVulnerability Gather data sampling:   Mitigation; Microcode\r\nVulnerability Itlb multihit:          KVM: Mitigation: VMX disabled\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Mitigation; Enhanced IBRS\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\r\nVulnerability Srbds:                  Mitigation; Microcode\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==7.1.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.20\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.2\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.5@09c7792610ada9f88bbf87d32b472dd44bf23cc2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nRunning .gguf files without .gguf extension does not work... renaming these files with .gguf extension and running without any flags (just using \"vllm serve granite-code:latest.gguf\") works.\r\n\r\nExpected behaviour is that we don't require .gguf extension to run .gguf file.\r\n\r\n```\r\n$ vllm serve ~/.local/share/ramalama/models/ollama/granite-code:latest --quantization gguf --load-format gguf\r\nWARNING 08-29 11:56:16 _custom_ops.py:17] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\r\nINFO 08-29 11:56:19 api_server.py:440] vLLM API server version 0.5.5\r\nINFO 08-29 11:56:19 api_server.py:441] args: Namespace(model_tag='/home/curtine/.local/share/ramalama/models/ollama/granite-code:latest', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, model='/home/curtine/.local/share/ramalama/models/ollama/granite-code:latest', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='gguf', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization='gguf', rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None, dispatch_function=<function serve at 0x7f15a2e81f80>)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.12/site-packages/transformers/configuration_utils.py\", line 722, in _get_config_dict\r\n    config_dict = cls._dict_from_json_file(resolved_config_file)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/transformers/configuration_utils.py\", line 825, in _dict_from_json_file\r\n    text = reader.read()\r\n           ^^^^^^^^^^^^^\r\n  File \"<frozen codecs>\", line 322, in decode\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xac in position 446: invalid start byte\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/vllm\", line 8, in <module>\r\n    sys.exit(main())\r\n             ^^^^^^\r\n  File \"/usr/local/lib64/python3.12/site-packages/vllm/scripts.py\", line 156, in main\r\n    args.dispatch_function(args)\r\n  File \"/usr/local/lib64/python3.12/site-packages/vllm/scripts.py\", line 37, in serve\r\n    asyncio.run(run_server(args))\r\n  File \"/usr/lib64/python3.12/asyncio/runners.py\", line 194, in run\r\n    return runner.run(main)\r\n           ^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib64/python3.12/asyncio/runners.py\", line 118, in run\r\n    return self._loop.run_until_complete(task)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib64/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\r\n    return future.result()\r\n           ^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib64/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 443, in run_server\r\n    async with build_async_engine_client(args) as async_engine_client:\r\n  File \"/usr/lib64/python3.12/contextlib.py\", line 210, in __aenter__\r\n    return await anext(self.gen)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib64/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 117, in build_async_engine_client\r\n    if (model_is_embedding(args.model, args.trust_remote_code,\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib64/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 71, in model_is_embedding\r\n    return ModelConfig(model=model_name,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib64/python3.12/site-packages/vllm/config.py\", line 169, in __init__\r\n    self.hf_config = get_config(self.model, trust_remote_code, revision,\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib64/python3.12/site-packages/vllm/transformers_utils/config.py\", line 64, in get_config\r\n    config = AutoConfig.from_pretrained(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py\", line 976, in from_pretrained\r\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/transformers/configuration_utils.py\", line 632, in get_config_dict\r\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/site-packages/transformers/configuration_utils.py\", line 726, in _get_config_dict\r\n    raise EnvironmentError(\r\nOSError: It looks like the config file at '/home/curtine/.local/share/ramalama/models/ollama/granite-code:latest' is not a valid JSON file.\r\n```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-08-29T10:58:28+00:00",
    "closed_at": "2024-09-02T12:43:27+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7993/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7993"
  },
  {
    "number": 2538,
    "title": "outputs includes eos token",
    "body": "### code\n\n```\ngen_kwargs = {\"top_p\": top_p, \"temperature\": temperature, \"max_tokens\": max_length, \"include_stop_str_in_output\": False}\neos_token_id = [tokenizer.eos_token_id, tokenizer.get_command(\"<|user|>\"), tokenizer.get_command(\"<|observation|>\")]\nsampling_params = SamplingParams(stop_token_ids=eos_token_id, **gen_kwargs)\noutputs = self.model.generate(sampling_params=sampling_params, prompt_token_ids=inputs[\"input_ids\"].tolist())\n```\n\n### outputs string\n\n```\nI need to use the insauto_quote_tool tool to get the user's car insurance quote.<|assistant|> insauto_quote_tool\n ```python\ntool_call(type='object', properties={'quote_biz_id': '20220906000831000002005700226000'})\n```<|observation|>\n```\n\n### question\n\nlwhy is '<|observation|>' still at the end?\n\n### environment\n\nModel\uff1aChatGLM3-6b",
    "labels": [
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-01-22T03:14:12+00:00",
    "closed_at": "2024-02-27T22:05:41+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2538/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2538"
  },
  {
    "number": 5205,
    "title": "[Feature]: Option to override HuggingFace's configurations",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\r\n\r\nThe configuration files on HuggingFace may have missing information (e.g. #2051) or contain bugs (e.g. #4008). In such cases, it may be necessary to provide/override the configuration files to enable the model to be loaded correctly. However, apart from chat templates, there is currently no method of doing so; we have to update the source HuggingFace repository directly. It may take time for the authors of those repositories to respond, especially if they are unofficial ones which are not as well-maintained.\r\n\r\nIt would be great if we could provide our own `config.json`, `tokenizer_config.json`, etc., through the vLLM CLI to apply patches as necessary.\r\n\r\n### Related work\r\n\r\n#1756 lets us specify alternative chat templates or provide a chat template when it is missing from `tokenizer_config.json`. However, it currently only applies to the OpenAI API-compatible server. #5049 will add chat method to the main LLM entrypoint, but does not provide a built-in way to load the chat template automatically like in #1756.\r\n\r\nSome vLLM models have already hardcoded patches to HuggingFace `config.json`; these can be found under `vllm/transformers_utils/configs`.\r\n",
    "labels": [
      "good first issue",
      "feature request",
      "unstale"
    ],
    "state": "closed",
    "created_at": "2024-06-03T05:45:53+00:00",
    "closed_at": "2024-11-09T16:19:28+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5205/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/5205"
  },
  {
    "number": 3283,
    "title": "Order of keys for guided JSON",
    "body": "Hi\r\n\r\nTrying to use a json template with a mixtral 7x8b model.\r\nHowever the model generates the json with keys in alphabetic order which has a significant impact on generation quality (for my task at least).\r\n\r\n```python\r\n\r\njson_template= {\r\n    \"type\": \"object\",\r\n    \"properties\": {\r\n        \"first_key\": {\r\n            \"type\": \"array\",\r\n            \"items\": {\"type\": \"string\"},\r\n            \"minItems\": 1\r\n            },\r\n        \"another_key\": {\r\n            \"type\": \"array\",\r\n            \"items\": {\"type\": \"string\"},\r\n            \"minItems\": 1\r\n            },\r\n    \"required\": [\"first_key\", \"another_key\"]\r\n    }\r\n\r\nchat_response = client.chat.completions.create(\r\n    model=model,\r\n    messages=[\r\n        {\"role\": \"user\", \"content\": prompt},\r\n    ],\r\n    max_tokens=256,\r\n    temperature=0.7,\r\n    top_p=1,\r\n    extra_body=dict(guided_json=json_template)\r\n)\r\n\r\n# The returned JSON is in alphabetic order\r\n> \"{'another_key': ['some_output'], 'first_key': ['another_output']}\"\r\n```\r\nIs there a way to force the generation to follow the initial pattern (without alphabetic order)?",
    "labels": [
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-03-08T13:45:25+00:00",
    "closed_at": "2024-06-10T18:35:46+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3283/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3283"
  },
  {
    "number": 15509,
    "title": "[Bug]: Embed model has additional dense module(dim=1792, but only 1024)",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.31.4\nLibc version: glibc-2.35\n\nPython version: 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-3.10.0-957.el7.x86_64-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: NVIDIA GeForce RTX 3090\nGPU 1: NVIDIA GeForce RTX 3090\nGPU 2: NVIDIA GeForce RTX 3090\nGPU 3: NVIDIA GeForce RTX 3090\nGPU 4: NVIDIA GeForce RTX 3090\nGPU 5: NVIDIA GeForce RTX 3090\nGPU 6: NVIDIA GeForce RTX 3090\nGPU 7: NVIDIA GeForce RTX 3090\n\nNvidia driver version: 535.183.06\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   46 bits physical, 48 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          64\nOn-line CPU(s) list:             0-63\nVendor ID:                       GenuineIntel\nModel name:                      Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz\nCPU family:                      6\nModel:                           85\nThread(s) per core:              2\nCore(s) per socket:              16\nSocket(s):                       2\nStepping:                        7\nBogoMIPS:                        4200.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_ppin intel_pt ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm arat pln pts pku ospke avx512_vnni spec_ctrl intel_stibp flush_l1d arch_capabilities\nVirtualization:                  VT-x\nL1d cache:                       1 MiB (32 instances)\nL1i cache:                       1 MiB (32 instances)\nL2 cache:                        32 MiB (32 instances)\nL3 cache:                        44 MiB (2 instances)\nNUMA node(s):                    2\nNUMA node0 CPU(s):               0-15,32-47\nNUMA node1 CPU(s):               16-31,48-63\nVulnerability L1tf:              Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; Load fences, __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.1.post1+cu124torch2.5\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.49.0\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.3\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\nGPU1    PIX      X      NODE    NODE    SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\nGPU2    NODE    NODE     X      PIX     SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\nGPU3    NODE    NODE    PIX      X      SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\nGPU4    SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    16-31,48-63     1               N/A\nGPU5    SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    16-31,48-63     1               N/A\nGPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      PIX     16-31,48-63     1               N/A\nGPU7    SYS     SYS     SYS     SYS     NODE    NODE    PIX      X      16-31,48-63     1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNVIDIA_VISIBLE_DEVICES=all\nNVIDIA_REQUIRE_CUDA=cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\nNCCL_VERSION=2.17.1-1\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nNVIDIA_CUDA_END_OF_LIFE=1\nCUDA_VERSION=12.1.0\nCUDA_VISIBLE_DEVICES=1\nCUDA_VISIBLE_DEVICES=1\nLD_LIBRARY_PATH=/usr/local/lib/python3.12/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nThe embed model has an additional dense module with an output dimension of 1792, but the actual output dimension of vllm is still 1024.\n\nsame issue in TEI: [https://github.com/huggingface/text-embeddings-inference/issues/423](https://github.com/huggingface/text-embeddings-inference/issues/423)\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "help wanted",
      "good first issue"
    ],
    "state": "open",
    "created_at": "2025-03-26T01:15:06+00:00",
    "closed_at": null,
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15509/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15509"
  },
  {
    "number": 19012,
    "title": "[Feature]: Microbatch Tokenization",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nTokenization is pretty slow (for concurrency=1, DGX H100's CPU can do about only about 1k tokens/ms), under high load, this becomes the performance bottleneck. \n\nIn vLLM's API server today, we process each request's tokenization sequentialy:\n\nhttps://github.com/vllm-project/vllm/blob/b9f61e13875e1682d3982829006bec26981fde4d/vllm/entrypoints/openai/serving_engine.py#L222-L228\n\nHowever, just by calling `tokenizer.__call__` with a list of string, significant speed up can be achieved. \n\n```\nIn [22]: inp = \"hi \"*10000\n\nIn [23]: inp_batch = [inp]*16\n\nIn [24]: %timeit tokenizer(inp)\n10.7 ms \u00b1 135 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [25]: %timeit tokenizer(inp_batch)\n31.6 ms \u00b1 407 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n```\n\nThere are several ways to speed this up. One approach is to set the thread pool to `N=number_of_cores`, however, @njhill pointed out that transformers's tokenizer actually don't release the GIL and it is not thread safe. \n\nThe other alternative is to use process pool tokenizer. This has been explored in RayTokenizerGroup which is essentially an optimized version of this approach using Ray. However, the serialization cost is still high. \n\nIn this feature request, I propose refactoring the file's tokenization path to leverage microbatching in asyncio.\n* We still main a single thread for the tokenizer to run. \n* All tokenizer call request are sent to a threading/asyncio queue\n* The tokenizer thread pull from the queue and execute all elements in the queue in one call. \n\nHere's a production level microbatching wrapper implementation for reference: https://github.com/ray-project/ray/blob/master/python/ray/serve/batching.py We can probably do something simpler \n\n\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\nThis can be reproduced by running 1B models with long prompts as requests. \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-06-02T05:28:07+00:00",
    "closed_at": "2025-07-07T16:54:12+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19012/reactions",
      "total_count": 7,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19012"
  },
  {
    "number": 14927,
    "title": "[Feature]: Consolidate `LRUCache` implementations",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\n#14805 introduced `cachetools.LRUCache` to support different size for each item and prepare for a thread-safe implementation. On the other hand, the code under `vllm/adapter_commons` uses the existing `vllm.utils.LRUCache`. To clean up the code, we should consolidate these implementations inside `vllm.utils.LRUCache`. This cache should support the following features:\n\n- Pinning specific items in the cache (the existing `vllm.utils.LRUCache`)\n- Custom function to compute the size for each item (`cachetools.LRUCache`)\n- Custom callback functions when an item is removed (`vllm.adapter_commons.AdapterLRUCache`)\n- The cache should remain compatible with `collections.abc.MutableMapping` interface so it can be passed to `cachetools.cached` to make it thread-safe.\n\n### Alternatives\n\nKeep the two implementations separate. However, this may cause confusion since the two classes share the same name.\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-03-17T05:44:45+00:00",
    "closed_at": "2025-03-27T06:43:44+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14927/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/14927"
  },
  {
    "number": 5250,
    "title": "[Misc]: Why prometheus metric vllm:request_success_total doubles the value?",
    "body": "### Anything you want to discuss about vllm.\n\nI am using the following script to display the vllm metric:request_success_total: `sum(increase(vllm:request_success_total{model_name=\"$MODEL_NAME\"}[$__rate_interval])) by (finished_reason)`\r\n\r\n\r\nBut each of my queries in the model is displayed on the graph in the amount of \"2\". It seems that the value is incremented twice by mistake with a single request.\r\n\r\n![image](https://github.com/vllm-project/vllm/assets/21113432/b5b686de-02c8-416d-8797-4d368d00790b)\r\n",
    "labels": [
      "bug",
      "good first issue",
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-06-04T13:21:02+00:00",
    "closed_at": "2024-06-04T19:55:46+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5250/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5250"
  },
  {
    "number": 288,
    "title": "Support for Constrained decoding",
    "body": "For getting structured outputs from custom-finetuned LLMs, extensive use of [constrained decoding](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.DisjunctiveConstraint) is standard. \r\n\r\nIs there a plan to add support for DisjunctiveConstraint (and others) to vLLM in the near future? \r\nHow would one go about implementing this in vLLM (if I were to add a PR)?",
    "labels": [
      "good first issue",
      "feature request"
    ],
    "state": "closed",
    "created_at": "2023-06-28T09:23:14+00:00",
    "closed_at": "2024-03-19T22:46:11+00:00",
    "comments": 32,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/288/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/288"
  },
  {
    "number": 14532,
    "title": "[Feature]: [V1] Validate / Fix Load Formats on V1",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nWe are not sure if `--load-format sharded_state` of `--load-format tensorizer` work with V1\n\nThis issue asks to look into it and fix any issues that occur, including for TP>1\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-03-10T02:48:53+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14532/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14532"
  },
  {
    "number": 19772,
    "title": "[Feature]: Evaluate prompt presence on subsequent audio chunks",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nStarting with #19597 , vllm now supports chunking audios longer than 30s when serving Whisper.\nThe logic is pretty simple right now, as the audio is chunked at semi-fixed intervals, looking for \"silence\" in a small window around the chunk limit.\nThe request is then executed in a \"concurrent mode\", batching the audio chunks. \nhttps://github.com/vllm-project/vllm/blob/cda92307c145e7722cdc33e6d26e105eeb22b882/vllm/entrypoints/openai/serving_transcription.py#L215-L226\n\nHence there's no sequential dependency at the moment, in particular the transcription of chunk_i is not piped as prompt to chunk_i+1 (optimal strategy, as per the Whisper paper).\nIn this regard,  it would be nice to asses with longer audio samples whether feeding the original prompt to subsequent chunks after the first one is actually beneficial to the quality of the generated output.\nMy understanding is that the prompt will condition the model on the text that appeared in the past 30s, and hence it may actually be harmful to the final quality of the transcription, given a long enough input.\nThis task requires evaluating the precision/error rate on longer sequences similarly to what it was done here https://github.com/vllm-project/vllm/blob/main/tests/entrypoints/openai/correctness/test_transcription_api_correctness.py, with and without original prompt for all chunks but the first one.  \n\n\nHere's the detail of the line I am referring to:\nhttps://github.com/vllm-project/vllm/blob/cda92307c145e7722cdc33e6d26e105eeb22b882/vllm/entrypoints/openai/serving_transcription.py#L224\n\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-06-17T21:08:13+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19772/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19772"
  },
  {
    "number": 10377,
    "title": "[Feature]: NVIDIA Triton GenAI Perf Benchmark",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nThe GenAI perf toolkit from NVIDIA can be used as an alternative benchmark tools for vLLM. While we already have benchmark scripts and framework in `benchmarks` directory, we should test out different load generators to compare the performance and accuracy of the benchmark clients. \r\n\r\nIn this issues, I described some tasks that we need help with to try out the new benchmark harness:\r\n* Compare the output of the genai perf with the `benchmark_serving`, on the coverage of the result metrics and the accuracy. \r\n* Vary the workloads ShareGPT/Sonnet/synthetics\r\n* Implement it as an alternative harness through the script. \r\n\r\nHappy to elaborate as well. \r\n\r\nhttps://pypi.org/project/genai-perf/ \n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "help wanted",
      "good first issue",
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-15T22:03:06+00:00",
    "closed_at": "2025-02-27T07:25:08+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10377/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/10377"
  },
  {
    "number": 16118,
    "title": "[Feature]: Estimate max-model-len when the KV cache memory is not enough",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nWhen the KV cache is not enough for holding one request, vLLM v1 will raise an error like this\n> ERROR 04-05 01:12:55 [core.py:390] ValueError: To serve at least one request with the models's max seq len (1048576), (24.00 GiB KV cache is needed, which is larger than the available KV cache memory (9.97 GiB). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.\n\nIt would be more convenient if we can provide an estimated `max_model_len` to the users in this error log.\n\nThe estimation is more complex than `max_model_len = block_size * num_gpu_blocks` after the introduction of different types of KV cache like sliding window, and help wanted on implementing with binary search of `max_model_len` based on the `KVCacheSpec.max_memory_usage_bytes`.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "help wanted",
      "good first issue",
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-04-06T06:02:27+00:00",
    "closed_at": "2025-04-09T02:12:52+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16118/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16118"
  },
  {
    "number": 11965,
    "title": "[Doc]: Invalid JSON examples in Engine Args Document",
    "body": "### \ud83d\udcda The doc issue\n\nOn page https://docs.vllm.ai/en/latest/serving/engine_args.html#engine-args\r\n\r\nRegarding the flag `--override-pooler-config`. The documentation provides the following example:\r\n\r\n> Override or set the pooling method for pooling models. e.g. {\u201cpooling_type\u201d: \u201cmean\u201d, \u201cnormalize\u201d: false}.\u2019\r\n\r\nHowever this example does not work if copy-pasted into a UTF-8 aware text editor as it is not a valid JSON document. (The quotation marks are not ascii quotation marks, they are left-quote and right-quote.) This is an insidious error as it is nearly invisible to the naked eye.\r\n\r\nIn addition to `--override-pooler-config`, this issue affects `--override-neuron-config`, `--rope-scaling`, and `--mm-processor-kwargs`.\n\n### Suggest a potential alternative/fix\n\nChange\r\n\r\n> Override or set the pooling method for pooling models. e.g. {\u201cpooling_type\u201d: \u201cmean\u201d, \u201cnormalize\u201d: false}.\u2019\r\n\r\nto\r\n\r\n> Override or set the pooling method for pooling models. e.g. `{\"pooling_type\": \"mean\", \"normalize\": false}`.\u2019\r\n\r\n(Replace non-ascii quotes with ascii quotes and surround with a code block to ensure verbatim inclusion.)\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation",
      "help wanted",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2025-01-12T05:16:06+00:00",
    "closed_at": "2025-01-14T17:03:06+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11965/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11965"
  },
  {
    "number": 3665,
    "title": "[Misc]: Implement SlidingWindowBlockTable in BlockManagerV2",
    "body": "Recently, we refactored the block manager subsystem to improve testability by separating concerns of each layer. See https://github.com/vllm-project/vllm/pull/3492 for more information.\r\n\r\nThe V2 implementation does not yet have sliding window support. This issue tracks adding sliding window to the V2 block table so that we can support models that use this feature.\r\n\r\nMy initial take on the design is to implement a `SlidingWindowBlockTable` that composes within it a `BlockTable`. The `SlidingWindowBlockTable` will then drop blocks that are outside of the context window (potentially mapping them to a devnull block). This will preserve the semantics of the v1 block manager sliding window while fitting into the new design.",
    "labels": [
      "good first issue",
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-03-27T21:37:17+00:00",
    "closed_at": "2024-05-28T02:07:08+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3665/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3665"
  },
  {
    "number": 19881,
    "title": "[Feature]: Implement `check_health` for V1",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently `check_health` is a no-op in V1. We should have an explicit way to check if the engine is still alive and all the subprocesses are healthy. This will enable better functionality in an operational system\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-06-19T20:54:34+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19881/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19881"
  },
  {
    "number": 3873,
    "title": "[Doc/Feature]: Llava 1.5 in OpenAI compatible server",
    "body": "### \ud83d\udcda The doc issue\n\nHey vLLM team it looks like there is added support for llava 1.5 but there are no docs or examples on how to use it via the api server. Are there any reference examples? For using llava via the OpenAI sdk? \n\n### Suggest a potential alternative/fix\n\n_No response_",
    "labels": [
      "documentation",
      "help wanted",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-04-05T19:07:32+00:00",
    "closed_at": "2024-06-07T18:25:15+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3873/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3873"
  },
  {
    "number": 14677,
    "title": "[Bug]: Unit test `tests/models/embedding/vision_language/test_phi3v.py` failing",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\nCollecting environment information...                                                                                            \nPyTorch version: 2.5.1+cu124                                                                                                     \nIs debug build: False                                                                                                            \nCUDA used to build PyTorch: 12.4                                                                                                 \nROCM used to build PyTorch: N/A                                                                                                  \n                                                                                                                                 \nOS: Ubuntu 24.04.1 LTS (x86_64)                                                                                                  \nGCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0                                                                               \nClang version: 18.1.3 (1ubuntu1)                                                                                                 \nCMake version: version 3.28.3                                                                                                    \nLibc version: glibc-2.39                                                                                                         \n                                                                                                                                 \nPython version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0] (64-bit runtime)                \nPython platform: Linux-6.8.0-47-generic-x86_64-with-glibc2.39                                                                    \nIs CUDA available: True                                                                                                          \nCUDA runtime version: 12.6.77                                                                                                    \nCUDA_MODULE_LOADING set to: LAZY                                                                      \nNvidia driver version: 550.127.05                                                                                                \ncuDNN version: Could not collect                                                                                                 \nHIP runtime version: N/A                                                                                                         \nMIOpen runtime version: N/A                                                                                                      \nIs XNNPACK available: True    \n\nVersions of relevant libraries:                                                                                                  \n[pip3] mypy-extensions==1.0.0                                                                                                    \n[pip3] numpy==1.26.4                                                                                                             \n[pip3] nvidia-cublas-cu12==12.4.5.8                                                                                              \n[pip3] nvidia-cuda-cupti-cu12==12.4.127                                                                                          \n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127                                                                                          \n[pip3] nvidia-cuda-runtime-cu12==12.4.127                                                                                        \n[pip3] nvidia-cudnn-cu12==9.1.0.70                                                                                               \n[pip3] nvidia-cufft-cu12==11.2.1.3                                                                                               \n[pip3] nvidia-curand-cu12==10.3.5.147                                                                                            \n[pip3] nvidia-cusolver-cu12==11.6.1.9                                                                                            \n[pip3] nvidia-cusparse-cu12==12.3.1.170                                                                                          \n[pip3] nvidia-nccl-cu12==2.21.5                                                                                                  \n[pip3] nvidia-nvjitlink-cu12==12.4.127                                                                                           \n[pip3] nvidia-nvtx-cu12==12.4.127                                                                                                \n[pip3] pyzmq==26.2.1                                                                                                             \n[pip3] sentence-transformers==3.2.1                                                                                              \n[pip3] torch==2.5.1                                                                                                              \n[pip3] torchaudio==2.5.1                                                                                                         \n[pip3] torchvision==0.20.1                                                                                                       \n[pip3] transformers==4.49.0                                                                                                      \n[pip3] transformers-stream-generator==0.0.5                                                                                      \n[pip3] triton==3.1.0                                                                                                             \n[pip3] tritonclient==2.51.0                                                                                                      \n[pip3] vector-quantize-pytorch==1.21.2 \n[conda] numpy                     1.26.4                   pypi_0    pypi                                                        \n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi                                                        \n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi                                                        \n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi                                                        \n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi                                                        \n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi                                                        \n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi                                                        \n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi                                                        \n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi                                                        \n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi                                                        \n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi                                                        \n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi                                                        \n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi                                                        \n[conda] pyzmq                     26.2.1                   pypi_0    pypi                                                        \n[conda] sentence-transformers     3.2.1                    pypi_0    pypi                                                        \n[conda] torch                     2.5.1                    pypi_0    pypi                                                        \n[conda] torchaudio                2.5.1                    pypi_0    pypi                                                        \n[conda] torchvision               0.20.1                   pypi_0    pypi                                                        \n[conda] transformers              4.49.0                   pypi_0    pypi                                                        \n[conda] transformers-stream-generator 0.0.5                    pypi_0    pypi                                                    \n[conda] triton                    3.1.0                    pypi_0    pypi\n[conda] tritonclient              2.51.0                   pypi_0    pypi\n[conda] vector-quantize-pytorch   1.21.2                   pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.1.dev5072+g63d635d (git sha: 63d635d\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nLD_LIBRARY_PATH=miniconda3/envs/vllm073/lib/python3.12/site-packages/cv2/../../lib64:/usr/lib/x86_64-linux-gnu/openmpi/lib/:/usr/local/cuda-12.6/lib64:\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nBUG: unittest `tests/models/embedding/vision_language/test_phi3v.py` is failiing\n\nAfter installing vllm, run\n\n`pytest -sv tests/models/embedding/vision_language/test_phi3v.py::test_models_image[half-TIGER-Lab/VLM2Vec-Full]`\n\nA snippet of unittest failure log\n```\ntests/models/embedding/vision_language/test_phi3v.py:119:                                                                        \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests/models/embedding/vision_language/test_phi3v.py:69: in _run_test                                                            \n    check_embeddings_close(                                                                                                      \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n                                                                                                                                 \n    def check_embeddings_close(                                                                                                  \n        *,                                                                                                                       \n        embeddings_0_lst: Sequence[list[float]],                                                                                 \n        embeddings_1_lst: Sequence[list[float]],                                                                                 \n        name_0: str,                                                                                                             \n        name_1: str,                                                                                                             \n        tol: float = 1e-3,                                                                                                       \n    ) -> None:                                                                                                                   \n        assert len(embeddings_0_lst) == len(embeddings_1_lst)                                                                    \n                                                                                                                                 \n        for prompt_idx, (embeddings_0, embeddings_1) in enumerate(                                                               \n                zip(embeddings_0_lst, embeddings_1_lst)):                                                                        \n            assert len(embeddings_0) == len(embeddings_1), (                                                                     \n                f\"Length mismatch: {len(embeddings_0)} vs. {len(embeddings_1)}\")                                                 \n                                                                                                                                 \n            sim = F.cosine_similarity(torch.tensor(embeddings_0),                                                                \n                                      torch.tensor(embeddings_1),                                                                \n                                      dim=0)                                                                                     \n                                                                                                                                 \n            fail_msg = (f\"Test{prompt_idx}:\"                                                                                     \n                        f\"\\n{name_0}:\\t{embeddings_0[:16]!r}\"                                                                    \n                        f\"\\n{name_1}:\\t{embeddings_1[:16]!r}\")   \n>           assert sim >= 1 - tol, fail_msg\nE           AssertionError: Test0:\nE           hf: [0.0005359649658203125, 0.0028438568115234375, -0.033050537109375, 0.009765625, 0.003566741943359375, 0.004444122314453125, -0.0088958740234375, -0.032867431640625, -0.006267547607421875, -0.021881103515625, 0.021087646484375, 0.0010557174682617188, 0.0147247314453125, -0.004642486572265625, 0.0032863616943359375, 0.006305694580078125]\nE           vllm:       [0.0029010772705078125, 0.005504608154296875, -0.030792236328125, 0.00962066650390625, 0.0016222000122070312, 0.0036258697509765625, -0.00925445556640625, -0.03411865234375, -0.005634307861328125, -0.0216522216796875, 0.022430419921875, 0.0002880096435546875, 0.016082763671875, -0.00467681884765625, 0.004161834716796875, 0.004535675048828125]\n\ntests/models/embedding/utils.py:32: AssertionError\n------------------------------------------------------- Captured log call -------------------------------------------------------\nWARNING  transformers_modules.microsoft.Phi-3.5-vision-instruct.4a0d683eba9f1d0cbfb6151705d1ee73c25a80ca.modeling_phi3_v:logging.py:329 You are not running the flash-attention implementation, expect numerical differences.\n======================================================= warnings summary ========================================================\ntests/models/embedding/vision_language/test_phi3v.py::test_models_image[half-TIGER-Lab/VLM2Vec-Full]\n  /home/tan/miniconda3/envs/vllm073/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:594: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n    warnings.warn(\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n==================================================== short test summary info ====================================================\nFAILED tests/models/embedding/vision_language/test_phi3v.py::test_models_image[half-TIGER-Lab/VLM2Vec-Full] - AssertionError: Test0:\n=========================================== 1 failed, 1 warning in 311.72s (0:05:11) ============================================\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "help wanted",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2025-03-12T11:49:15+00:00",
    "closed_at": "2025-03-30T09:01:36+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14677/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14677"
  },
  {
    "number": 21062,
    "title": "[Feature]: Remove xformers requirement for Mistral-format Pixtral and Mistral3",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nI implemented this a while ago for the HF-format of Pixtral in https://github.com/vllm-project/vllm/pull/9597 by using the torch SDPA implementation. Xformers is not available on all architectures and most other vision encoders have multiple backends for attention. Pixtral is maybe the only that uses xformers strictly.\n\nWe should be able to replace the `xops` usage in the `pixtral.py` classes `VisionTransformer` and `Attention` by following the same substitution as in the HF modules.\nSuch as \nhttps://github.com/vllm-project/vllm/blob/a0f8a7964694a6077689b242b5eca95de392d4bb/vllm/model_executor/models/pixtral.py#L1274-L1282\nand\nhttps://github.com/vllm-project/vllm/blob/a0f8a7964694a6077689b242b5eca95de392d4bb/vllm/model_executor/models/pixtral.py#L1087-L1099\n\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "feature request",
      "multi-modality"
    ],
    "state": "open",
    "created_at": "2025-07-16T16:13:25+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/21062/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/21062"
  },
  {
    "number": 279,
    "title": "Possibility of Passing Prompts as List[str] to AsyncEngine.generate()",
    "body": "Hi! Thank you for your amazing framework! I have tried serving a GPT BigCode model using vllm together with ray following the example: https://github.com/ray-project/ray/blob/3d3183d944424a960a2c6ce048abd1316c901c1e/doc/source/serve/doc_code/vllm_example.py And in my use case the response is in \"non-streaming\" format. I directly passed the request to the vllm async engine to use the continuous batching ability. However, when I tested it with stress testing tool, I found the improvement of the latency and throughput is not that good. \r\n\r\nOne reason behind might be that the average length of testing input prompt is quite long (around 1000 tokens) which uses almost all space of KV cache in GPU and some of them are duplicated. Therefore I may need to do a preprocessing of the request in the batch level first to filter out some duplicate requests then pass the request to vllm engine. Currently, if I want to use async engine, I can only pass one prompt to the pool at one time. May I know if you have the plan to add a function like allowing adding multi prompts into the pool at one time (each with a different request_id) and retrieve them based on request_id?",
    "labels": [
      "good first issue",
      "feature request"
    ],
    "state": "closed",
    "created_at": "2023-06-27T14:18:28+00:00",
    "closed_at": "2024-03-08T10:29:13+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/279/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/279"
  },
  {
    "number": 986,
    "title": "It seems that SamplingParams doesnt support the bad_words_ids parameter when generating",
    "body": "`bad_words_ids` described [here](https://github.com/huggingface/transformers/blob/main/src/transformers/generation/configuration_utils.py#L145C11-L145C11) is useful for production applications.\r\nHowever It seems that vlllm doesnt support the bad_words_ids parameter when generating.\r\nIs there a plan to support it?\r\n```\r\n  bad_words_ids(`List[List[int]]`, *optional*):\r\n  List of list of token ids that are not allowed to be generated. Check\r\n  [`~generation.NoBadWordsLogitsProcessor`] for further documentation and examples.\r\n```",
    "labels": [
      "good first issue",
      "feature request"
    ],
    "state": "closed",
    "created_at": "2023-09-08T01:53:15+00:00",
    "closed_at": "2024-10-26T16:29:40+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/986/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/986"
  }
]
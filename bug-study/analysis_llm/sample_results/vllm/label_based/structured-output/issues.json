[
  {
    "number": 19493,
    "title": "[Bug]: Corrupted output when using JSON structured response (v0.9.1)",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.4 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : Could not collect\nCMake version                : version 3.22.1\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu126\nIs debug build               : False\nCUDA used to build PyTorch   : 12.6\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0] (64-bit runtime)\nPython platform              : Linux-5.15.0-141-generic-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : Could not collect\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : \nGPU 0: NVIDIA GeForce RTX 3090\nGPU 1: NVIDIA GeForce RTX 3090\n\nNvidia driver version        : 575.57.08\ncuDNN version                : Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               24\nOn-line CPU(s) list:                  0-23\nVendor ID:                            AuthenticAMD\nModel name:                           AMD Ryzen 9 5900X 12-Core Processor\nCPU family:                           25\nModel:                                33\nThread(s) per core:                   2\nCore(s) per socket:                   12\nSocket(s):                            1\nStepping:                             0\nFrequency boost:                      enabled\nCPU max MHz:                          3700.0000\nCPU min MHz:                          2200.0000\nBogoMIPS:                             7386.03\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\nVirtualization:                       AMD-V\nL1d cache:                            384 KiB (12 instances)\nL1i cache:                            384 KiB (12 instances)\nL2 cache:                             6 MiB (12 instances)\nL3 cache:                             64 MiB (2 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-23\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; safe RET, no microcode\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-cufile-cu12==1.11.1.6\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.0\n[pip3] torchaudio==2.7.0\n[pip3] torchvision==0.22.0\n[pip3] transformers==4.52.4\n[pip3] triton==3.3.0\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.1\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n  \tGPU0\tGPU1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tPHB\t0-23\t0\t\tN/A\nGPU1\tPHB\t X \t0-23\t0\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n==============================\n     Environment Variables\n==============================\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI'm seeing a lot of the following errors:\n\n```\n[backend_xgrammar.py:160] Failed to advance FSM for request chatcmpl-XXX for tokens 500. Please file an issue.\n```\n\nThis happens when specifying the `response_format` to be of `{'type': 'json_object'}`, but only when multiple requests are received. See bellow to reproduce.\n\n#### Reproduce\n\n```\npip install vllm==0.9.1\n\nvllm serve mediainbox/cogito-14b-gptq-q4 \\\n  --port=8098 \\\n  --host=0.0.0.0 \\\n  --max-model-len 4K \\\n  --disable-fastapi-docs\n```\n\nNow run the following script:\n\n```py\nimport concurrent.futures\n\nimport requests\n\nOPTIONS = {\"temperature\": 0.0, \"max_tokens\": 256, \"response_format\": {\"type\": \"json_object\"}}\nPROMPT = [\n    {\"role\": \"system\", \"content\": \"You are a Named Entity Recognition system. Output should be JSON in a single line.\"},\n    {\"role\": \"user\", \"content\": \"Extract entities from this sentence: 'Hi Robert did you go to TacoBox?'\"},\n]\n\n\ndef call_vllm():\n    resp = requests.post(\n        \"http://localhost:8098/v1/chat/completions\",\n        json={\n            \"model\": \"mediainbox/cogito-14b-gptq-q4\",\n            \"messages\": PROMPT,\n            **OPTIONS,\n        },\n        timeout=10,\n    )\n    return resp.json()[\"choices\"][0][\"message\"][\"content\"]\n\n\nif __name__ == \"__main__\":\n    for concurrency in [1, 2]:\n        print(f\"\\nConcurrency level: {concurrency}\")\n        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:\n            futures = [executor.submit(call_vllm) for _ in range(10)]\n            for f in concurrent.futures.as_completed(futures):\n                print(f.result())\n\n```\n\nreturns the following:\n\n```shell\nConcurrency level: 1\n{\"entities\": {\"Robert\": \"PERSON\", \"TacoBox\": \"ORGANIZATION\"}}\n{\"entities\": {\"Robert\": \"PERSON\", \"TacoBox\": \"ORGANIZATION\"}}\n{\"entities\": {\"Robert\": \"PERSON\", \"TacoBox\": \"ORGANIZATION\"}}\n{\"entities\": {\"Robert\": \"PERSON\", \"TacoBox\": \"ORGANIZATION\"}}\n{\"entities\": {\"Robert\": \"PERSON\", \"TacoBox\": \"ORGANIZATION\"}}\n{\"entities\": {\"Robert\": \"PERSON\", \"TacoBox\": \"ORGANIZATION\"}}\n{\"entities\": {\"Robert\": \"PERSON\", \"TacoBox\": \"ORGANIZATION\"}}\n{\"entities\": {\"Robert\": \"PERSON\", \"TacoBox\": \"ORGANIZATION\"}}\n{\"entities\": {\"Robert\": \"PERSON\", \"TacoBox\": \"ORGANIZATION\"}}\n{\"entities\": {\"Robert\": \"PERSON\", \"TacoBox\": \"ORGANIZATION\"}}\n\nConcurrency level: 2\n{\"entities\": {\"Robert\": \"PERSON\", \"TacoBox\": \"ORGANIZATION\"}}\nlplplplplplplplplplplplplplplplplplpl{\"entities\": [{\"text\": \"Robert\", \"type\": \"PERSON\"}]}\nlplplpl{\"entities\": {\"Robert\": \"PERSON\", \"TacoBox\": \"ORGANIZATION\"}}\nlplplplplpl{\"entities\": {\"Robert\": \"PERSON\", \"TacoBox\": \"ORGANIZATION\"}}\nlplplplplpl{\"entities\": {\"Robert\": \"PERSON\", \"TacoBox\": \"ORGANIZATION\"}}\nlplplpl{\"entities\": {\"Robert\": \"PERSON\", \"TacoBox\": \"ORGANIZATION\"}}\nlplplplplpl{\"entities\": {\"Robert\": \"PERSON\", \"TacoBox\": \"ORGANIZATION\"}}\nlplplplplpl{\"entities\": {\"Robert\": \"PERSON\", \"TacoBox\": \"ORGANIZATION\"}}\nlplplplplplpl{\"entities\": {\"PER\": [\"Robert\"], \"ORG\": [\"TacoBox\"]}}\nlplplpl{\"entities\": {\"Robert\": \"PERSON\", \"TacoBox\": \"ORGANIZATION\"}}\n```\n\nWhich totally corrupts the output.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "closed",
    "created_at": "2025-06-11T15:23:27+00:00",
    "closed_at": "2025-06-12T23:30:10+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19493/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19493"
  },
  {
    "number": 15073,
    "title": "[Bug]: guided_json \u8bf7\u6c42\u62a5\u9519 \u5728 v0.7.2",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\u6700\u8fd1\u628avllm\u4ece0.6.3\u5347\u7ea7\u5230\u4e860.7.2 \u53d1\u73b0\u4e4b\u524d\u7684guided_json\u8c03\u7528\u62a5\u9519\u4e86\u3002\n\u4e4b\u524d\u5728vllm0.6.3\u7248\u672c\u662f\u6ca1\u95ee\u9898\u7684\n\n\u4ee3\u7801\u5982\u4e0b\n\nfrom pydantic import BaseModel\nfrom enum import Enum\n\nfrom openai import OpenAI\n\nopenai_api_key = \"none\"\nopenai_api_base = \"http://10.12.167.20:8888/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nclass CarType(str, Enum):\n    sedan = \"sedan\"\n    suv = \"SUV\"\n    truck = \"Truck\"\n    coupe = \"Coupe\"\n\n\nclass CarDescription(BaseModel):\n    brand: str\n    model: str\n    car_type: CarType\n\n\njson_schema = CarDescription.model_json_schema()\n\ncompletion = client.chat.completions.create(\n    model=\"QWen\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Generate a JSON with the brand, model and car_type of the most iconic car from the 90's\",\n        }\n    ],\n    extra_body={\"guided_json\": json_schema},\n)\nprint(completion.choices[0].message.content)\n\n\n\n\u8bf7\u6c42\u7aef\u7684\u62a5\u9519\u5185\u5bb9\uff1a\n\nD:\\code\\code_data_process\\venv\\Scripts\\python.exe \"C:/Program Files/JetBrains/PyCharm 2023.3.6/plugins/python/helpers/pycharm/_jb_pytest_runner.py\" --path D:\\code\\code_data_process\\test_guided_json.py \nTesting started at 10:11 ...\nLaunching pytest with arguments D:\\code\\code_data_process\\test_guided_json.py --no-header --no-summary -q in D:\\code\\code_data_process\n\n============================= test session starts =============================\ncollecting ... \ntest_guided_json.py:None (test_guided_json.py)\ntest_guided_json.py:29: in <module>\n    completion = client.chat.completions.create(\nvenv\\lib\\site-packages\\openai\\_utils\\_utils.py:279: in wrapper\n    return func(*args, **kwargs)\nvenv\\lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:914: in create\n    return self._post(\nvenv\\lib\\site-packages\\openai\\_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\nvenv\\lib\\site-packages\\openai\\_base_client.py:919: in request\n    return self._request(\nvenv\\lib\\site-packages\\openai\\_base_client.py:1008: in _request\n    return self._retry_request(\nvenv\\lib\\site-packages\\openai\\_base_client.py:1057: in _retry_request\n    return self._request(\nvenv\\lib\\site-packages\\openai\\_base_client.py:1008: in _request\n    return self._retry_request(\nvenv\\lib\\site-packages\\openai\\_base_client.py:1057: in _retry_request\n    return self._request(\nvenv\\lib\\site-packages\\openai\\_base_client.py:1023: in _request\n    raise self._make_status_error_from_response(err.response) from None\nE   openai.InternalServerError: Internal Server Error\ncollected 0 items / 1 error\n\n!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n============================== 1 error in 4.30s ===============================\n\nProcess finished with exit code 2\n\n\u6a21\u578b\u7aef\u7684\u62a5\u9519\u5185\u5bb9\uff1a\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    return await self.app(scope, receive, send)\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/starlette/applications.py\", line 112, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    raise exc\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n    await self.app(scope, receive, send)\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/starlette/routing.py\", line 714, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/starlette/routing.py\", line 734, in app\n    await route.handle(scope, receive, send)\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/vllm/entrypoints/utils.py\", line 56, in wrapper\n    return handler_task.result()\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 392, in create_chat_completion\n    generator = await handler.create_chat_completion(request, raw_request)\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_chat.py\", line 263, in create_chat_completion\n    return await self.chat_completion_full_generator(\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_chat.py\", line 682, in chat_completion_full_generator\n    async for res in result_generator:\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/vllm/engine/multiprocessing/client.py\", line 606, in _process_request\n    params = await \\\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 553, in build_guided_decoding_logits_processor_async\n    processor = await get_guided_decoding_logits_processor(\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/vllm/model_executor/guided_decoding/__init__.py\", line 107, in get_guided_decoding_logits_processor\n    from vllm.model_executor.guided_decoding.xgrammar_decoding import (  # noqa\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/vllm/model_executor/guided_decoding/xgrammar_decoding.py\", line 15, in <module>\n    import xgrammar as xgr\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/xgrammar/__init__.py\", line 1, in <module>\n    from . import testing\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/xgrammar/testing.py\", line 11, in <module>\n    from .matcher import GrammarMatcher, bitmask_dtype\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/xgrammar/matcher.py\", line 13, in <module>\n    from .kernels import apply_token_bitmask_inplace_kernels\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/xgrammar/kernels/__init__.py\", line 12, in <module>\n    from .apply_token_bitmask_inplace_cuda import apply_token_bitmask_inplace_cuda\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_cuda.py\", line 54, in <module>\n    _load_torch_ops()\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_cuda.py\", line 42, in _load_torch_ops\n    torch.utils.cpp_extension.load_inline(\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1646, in load_inline\n    return _jit_compile(\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1746, in _jit_compile\n    return _import_module_from_library(name, build_directory, is_python_module)\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2145, in _import_module_from_library\n    torch.ops.load_library(filepath)\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/site-packages/torch/_ops.py\", line 1350, in load_library\n    ctypes.CDLL(path)\n  File \"/root/wangjianqiang/miniforge3/envs/QwQ/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n    self._handle = _dlopen(self._name, mode)\nOSError: /root/.cache/torch_extensions/py310_cu124/xgrammar/xgrammar.so: cannot open shared object file: No such file or directory\n\n\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n1\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "closed",
    "created_at": "2025-03-19T02:24:13+00:00",
    "closed_at": "2025-04-24T17:39:42+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15073/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15073"
  },
  {
    "number": 15577,
    "title": "[Bug]: guided_json not working correctly with (quantized) mistral-small model",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\n\nINFO 03-26 14:07:54 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.31.6\nLibc version: glibc-2.35\n\nPython version: 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA GeForce RTX 4090\nGPU 1: NVIDIA GeForce RTX 4090\n\nNvidia driver version: 550.120\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      43 bits physical, 48 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nVendor ID:                          AuthenticAMD\nModel name:                         AMD Ryzen Threadripper PRO 3955WX 16-Cores\nCPU family:                         23\nModel:                              49\nThread(s) per core:                 2\nCore(s) per socket:                 16\nSocket(s):                          1\nStepping:                           0\nFrequency boost:                    enabled\nCPU max MHz:                        4402.7339\nCPU min MHz:                        2200.0000\nBogoMIPS:                           7785.09\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sev sev_es\nVirtualization:                     AMD-V\nL1d cache:                          512 KiB (16 instances)\nL1i cache:                          512 KiB (16 instances)\nL2 cache:                           8 MiB (16 instances)\nL3 cache:                           64 MiB (4 instances)\nNUMA node(s):                       1\nNUMA node0 CPU(s):                  0-31\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection\nVulnerability Spec rstack overflow: Mitigation; Safe RET\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.1.post2+cu124torch2.6\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.50.0\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PHB     0-31    0               N/A\nGPU1    PHB      X      0-31    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNVIDIA_VISIBLE_DEVICES=all\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\nNCCL_VERSION=2.20.5-1\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nCUDA_VERSION=12.4.0\nCUDA_LAUNCH_BLOCKING=1\nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n\n</details>\n\n### \ud83d\udc1b Describe the bug\n\n\nI am providing `guided_json` schema in my request, but the output is not a valid json.\n\nHere is my setup:\n```docker-compose.yml\nservices:\n  vllm:\n    image: vllm/vllm-openai:v0.8.2\n    env_file:\n      - .env\n    shm_size: 12gb\n    ports:\n        - 8000:8000\n    command: --model stelterlab/Mistral-Small-24B-Instruct-2501-AWQ --tokenizer mistralai/Mistral-Small-24B-Instruct-2501 --tokenizer-mode mistral --tool-call-parser mistral --enable-auto-tool-choice --max-model-len 4096 --dtype half --guided-decoding-backend xgrammar\n```\n\nand here is my client script:\n```main.py\nfrom openai import Client\nfrom pydantic import BaseModel\n\nclient = Client(api_key=\"dummy\", base_url=\"http://localhost:8000/v1\")\n\n\nclass CarDescription(BaseModel):\n    brand: str\n    model: str\n\n\njson_schema = CarDescription.model_json_schema()\n\ncompletion = client.chat.completions.create(\n    model=\"stelterlab/Mistral-Small-24B-Instruct-2501-AWQ\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Generate a JSON with the brand, model and car_type of the most iconic car from the 90's\",\n        }\n    ],\n    extra_body={\"guided_json\": json_schema},\n)\nprint(completion.choices[0].message.content)\n```\nThe output is:\n```\n{\n \"br o\"         :       \"Ford\"          ,       \"model\"         :       \"Mustang\"\n```\nClearly something strange is happening in the model guided decoding. \n\nI have also tried the `outlines` and `guidance` decoding backends, but neither seem to support the mistral tokenizer and they throw errors. Is that expected?\n\nI'm not sure if it is related to the quantization of the model, but I don't have enough RAM to load the full model.\nAny help would be much appreciated.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "open",
    "created_at": "2025-03-26T21:10:04+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15577/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15577"
  },
  {
    "number": 11886,
    "title": "[Installation]:  Could not find a version that satisfies the requirement xgrammar>=0.1.6; platform_machine == \"x86_64\" (from vllm) (from versions: none)",
    "body": "### Your current environment\n\n```text\r\n # \u521b\u5efa\u73af\u5883\r\n conda create -n online_model_v4  python=3.10.13\r\n \r\n # \u6fc0\u6d3b\u73af\u5883\r\n conda activate online_model_v4\r\n\r\n\r\npip install vllm==0.6.6.post1\r\n\r\n\r\n```\r\n\n\n### How you are installing vllm\n\n```sh\r\npip install vllm==0.6.6.post1\r\n\r\n\r\n\r\n\r\n\u95ee\u9898\r\nCollecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.6.6.post1)\r\n  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/59/66/2e93a8f56adb51ede41d0ef5f4f0277522acc4adc87937f5457b7b5692a8/prometheus_fastapi_instrumentator-7.0.0-py3-none-any.whl (19 kB)\r\nCollecting tiktoken>=0.6.0 (from vllm==0.6.6.post1)\r\n  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/2e/28/cf3633018cbcc6deb7805b700ccd6085c9a5a7f72b38974ee0bffd56d311/tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\r\nCollecting lm-format-enforcer<0.11,>=0.10.9 (from vllm==0.6.6.post1)\r\n  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/c1/01/e78fdf09de2b4e7750a402eaa4f6783c7215ededd4bc6fe4a3f6d69c49da/lm_format_enforcer-0.10.9-py3-none-any.whl (43 kB)\r\nCollecting outlines==0.1.11 (from vllm==0.6.6.post1)\r\n  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/13/b4/99ea4a122bef60e3fd6402d19665aff1f928e0daf8fac3044d0b73f72003/outlines-0.1.11-py3-none-any.whl (87 kB)\r\nCollecting lark==1.2.2 (from vllm==0.6.6.post1)\r\n  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/2d/00/d90b10b962b4277f5e64a78b6609968859ff86889f5b898c1a778c06ec00/lark-1.2.2-py3-none-any.whl (111 kB)\r\nINFO: pip is looking at multiple versions of vllm to determine which version is compatible with other requirements. This could take a while.\r\nERROR: Could not find a version that satisfies the requirement xgrammar>=0.1.6; platform_machine == \"x86_64\" (from vllm) (from versions: none)\r\nERROR: No matching distribution found for xgrammar>=0.1.6; platform_machine == \"x86_64\"\r\n```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "structured-output"
    ],
    "state": "closed",
    "created_at": "2025-01-09T07:25:11+00:00",
    "closed_at": "2025-03-11T15:27:39+00:00",
    "comments": 38,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11886/reactions",
      "total_count": 7,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11886"
  },
  {
    "number": 7332,
    "title": "[Bug]: Compiling FSM index high memory && subprocess OOM",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-3.10.0-1160.71.1.el7.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA GeForce RTX 3090\r\nGPU 1: NVIDIA GeForce RTX 3090\r\n\r\nNvidia driver version: 535.129.03\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          16\r\nOn-line CPU(s) list:             0-15\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Gold 6140M CPU @ 2.30GHz\r\nCPU family:                      6\r\nModel:                           85\r\nThread(s) per core:              1\r\nCore(s) per socket:              4\r\nSocket(s):                       4\r\nStepping:                        4\r\nBogoMIPS:                        4599.99\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology eagerfpu pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat umip pku ospke md_clear spec_ctrl intel_stibp arch_capabilities\r\nVirtualization:                  VT-x\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       512 KiB (16 instances)\r\nL1i cache:                       512 KiB (16 instances)\r\nL2 cache:                        64 MiB (16 instances)\r\nL3 cache:                        64 MiB (4 instances)\r\nNUMA node(s):                    1\r\nNUMA node0 CPU(s):               0-15\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Mitigation; PTE Inversion; VMX conditional cache flushes, SMT disabled\r\nVulnerability Mds:               Mitigation; Clear CPU buffers; SMT Host state unknown\r\nVulnerability Meltdown:          Mitigation; PTI\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; Load fences, usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; IBRS (kernel), IBPB\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT Host state unknown\r\n\r\nVersions of relevant libraries:\r\n[pip3] intel-extension-for-transformers==1.4.2\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] optree==0.12.1\r\n[pip3] pyzmq==26.0.3\r\n[pip3] torch==2.4.0\r\n[pip3] torchaudio==2.4.0\r\n[pip3] torchelastic==0.2.2\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.43.3\r\n[pip3] triton==3.0.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\r\n[conda] intel-extension-for-transformers 1.4.2                    pypi_0    pypi\r\n[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\r\n[conda] mkl                       2023.1.0         h213fc3f_46344  \r\n[conda] mkl-service               2.4.0           py311h5eee18b_1  \r\n[conda] mkl_fft                   1.3.8           py311h5eee18b_0  \r\n[conda] mkl_random                1.2.4           py311hdb19cb5_0  \r\n[conda] numpy                     1.26.4          py311h08b1b3b_0  \r\n[conda] numpy-base                1.26.4          py311hf175353_0  \r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] optree                    0.12.1                   pypi_0    pypi\r\n[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch\r\n[conda] pytorch-mutex             1.0                        cuda    pytorch\r\n[conda] pyzmq                     26.0.3                   pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchaudio                2.4.0               py311_cu121    pytorch\r\n[conda] torchelastic              0.2.2                    pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.43.3                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.4\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PHB     0-15    0               N/A\r\nGPU1    PHB      X      0-15    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nserver\r\n```bash\r\npython -m vllm.entrypoints.openai.api_server --model /model/Qwen1.5-14B-Chat-GPTQ-Int4 --quantization gptq --max-model-len 12720\r\n```\r\nclient\r\n```python\r\nclass ClassificationItem(BaseModel):\r\n    name: str = Field(max_length=20, title=\"\u5206\u7c7b\u540d\")\r\n    risk_level: conint(ge=0, lt=8) = Field(title=\"\u98ce\u9669\u7b49\u7ea7\")\r\n\r\n\r\nclass ClassificationSet(BaseModel):\r\n    classification_list: List[ClassificationItem] = Field(min_items=100, title=\"\u5206\u7c7b\u540d\u7684\u5217\u8868\")\r\n\r\n\r\nopenai_client = OpenAI(\r\n        base_url=\"http://192.168.91.25:8000/v1\",\r\n        api_key=\"EMPTY\",\r\n    )\r\nclient = instructor.from_openai(openai_client)\r\n\r\nresp = client.chat.completions.create(\r\n        model=\"/model/Qwen1.5-14B-Chat-GPTQ-Int4\",\r\n        messages=[{\"role\": \"user\",\r\n                   \"content\": \"\u4f60\u662f\u4e00\u540d\u6570\u636e\u5b89\u5168\u8fd0\u8425\u4e13\u5bb6\uff0c\u6211\u662f\u4e00\u4e2a\u6cd5\u5f8b\u884c\u4e1a\u7684\u516c\u53f8\uff0c\u662f\u4e00\u5bb6\u5f8b\u5e08\u4e8b\u52a1\u6240\uff0c\u6211\u4eec\u516c\u53f8\u8d1f\u8d23\u54cd\u5e94\u5ba2\u6237\u7684\u6cd5\u5f8b\u54a8\u8be2\u3001\u5e2e\u5ba2\u6237\u5728\u6cd5\u5ead\u4e0a\u8fa9\u62a4\uff0c\u6211\u4eec\u516c\u53f8\u91cc\u6709\u5f88\u591a\u673a\u5bc6\u7c7b\u578b\u7684\u6587\u4ef6\u6216\u8005\u6587\u6863\uff0c\u8bf7\u4f60\u4e3a\u6211\u5217\u4e3e\u4e00\u4e0b\u8fd9\u4e9b'\u7c7b\u578b'\uff0c\u53ea\u9700\u8981\u7ed9\u51fa\u7c7b\u578b\u540d\u548c\u8be5\u7c7b\u578b\u5bf9\u5e94\u7684\u98ce\u9669\u7b49\u7ea7\uff0c\u4e0d\u9700\u8981\u8f93\u51fajson\u4ee5\u5916\u7684\u5185\u5bb9\"}],\r\n        response_model=ClassificationSet\r\n    )\r\n```\r\n\r\npayload\r\n```bash\r\ncurl http://192.168.91.25:8000/v1/chat/completions \\\r\n-H \"Content-Type: application/json\" \\\r\n-d '{\r\n    \"messages\": [\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": \"\u4f60\u662f\u4e00\u540d\u6570\u636e\u5b89\u5168\u8fd0\u8425\u4e13\u5bb6\uff0c\u6211\u662f\u4e00\u4e2a\u6cd5\u5f8b\u884c\u4e1a\u7684\u516c\u53f8\uff0c\u662f\u4e00\u5bb6\u5f8b\u5e08\u4e8b\u52a1\u6240\uff0c\u6211\u4eec\u516c\u53f8\u8d1f\u8d23\u54cd\u5e94\u5ba2\u6237\u7684\u6cd5\u5f8b\u54a8\u8be2\u3001\u5e2e\u5ba2\u6237\u5728\u6cd5\u5ead\u4e0a\u8fa9\u62a4\uff0c\u6211\u4eec\u516c\u53f8\u91cc\u6709\u5f88\u591a\u673a\u5bc6\u7c7b\u578b\u7684\u6587\u4ef6\u6216\u8005\u6587\u6863\uff0c\u8bf7\u4f60\u4e3a\u6211\u5217\u4e3e\u4e00\u4e0b\u8fd9\u4e9b'\u7c7b\u578b'\uff0c\u53ea\u9700\u8981\u7ed9\u51fa\u7c7b\u578b\u540d\u548c\u8be5\u7c7b\u578b\u5bf9\u5e94\u7684\u98ce\u9669\u7b49\u7ea7\uff0c\u4e0d\u9700\u8981\u8f93\u51fajson\u4ee5\u5916\u7684\u5185\u5bb9\"\r\n        }\r\n    ],\r\n    \"model\": \"/model/Qwen1.5-14B-Chat-GPTQ-Int4\",\r\n    \"tools\": [\r\n        {\r\n            \"type\": \"function\",\r\n            \"function\": {\r\n                \"name\": \"ClassificationSet\",\r\n                \"description\": \"Correctly extracted `ClassificationSet` with all the required parameters with correct types\",\r\n                \"parameters\": {\r\n                    \"$defs\": {\r\n                        \"ClassificationItem\": {\r\n                            \"properties\": {\r\n                                \"name\": {\r\n                                    \"maxLength\": 20,\r\n                                    \"title\": \"\u5206\u7c7b\u540d\",\r\n                                    \"type\": \"string\"\r\n                                },\r\n                                \"risk_level\": {\r\n                                    \"exclusiveMaximum\": 8,\r\n                                    \"minimum\": 0,\r\n                                    \"title\": \"\u98ce\u9669\u7b49\u7ea7\",\r\n                                    \"type\": \"integer\"\r\n                                }\r\n                            },\r\n                            \"required\": [\r\n                                \"name\",\r\n                                \"risk_level\"\r\n                            ],\r\n                            \"title\": \"ClassificationItem\",\r\n                            \"type\": \"object\"\r\n                        }\r\n                    },\r\n                    \"properties\": {\r\n                        \"classification_list\": {\r\n                            \"items\": {\r\n                                \"$ref\": \"#/$defs/ClassificationItem\"\r\n                            },\r\n                            \"minItems\": 100,\r\n                            \"title\": \"\u5206\u7c7b\u540d\u7684\u5217\u8868\",\r\n                            \"type\": \"array\"\r\n                        }\r\n                    },\r\n                    \"required\": [\r\n                        \"classification_list\"\r\n                    ],\r\n                    \"type\": \"object\"\r\n                }\r\n            }\r\n        }\r\n    ],\r\n    \"tool_choice\": {\r\n        \"type\": \"function\",\r\n        \"function\": {\r\n            \"name\": \"ClassificationSet\"\r\n        }\r\n    }\r\n}\r\n```\r\n![20240809-110817](https://github.com/user-attachments/assets/48b8ee7b-a4d8-47f8-b13e-8480551b9cb1)\r\n\r\noutput\r\n```\r\n.....\r\nCompiling FSM index for all state transitions: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 15753/15831 [15:05<00:04, 16.41it/s]INFO 08-09 03:02:26 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nCompiling FSM index for all state transitions: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15831/15831 [15:10<00:00, 17.39it/s]\r\nINFO 08-09 03:02:36 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 08-09 03:02:46 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 08-09 03:02:56 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 08-09 03:03:11 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 08-09 03:03:28 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 08-09 03:03:48 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 08-09 03:03:58 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 08-09 03:04:03 logger.py:36] Received request chat-1048cec6489b46c4902ff692c76094a6: prompt: '<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\n\u4f60\u662f\u4e00\u540d\u6570\u636e\u5b89\u5168\u8fd0\u8425\u4e13\u5bb6\uff0c\u6211\u662f\u4e00\u4e2a\u6cd5\u5f8b\u884c\u4e1a\u7684\u516c\u53f8\uff0c\u662f\u4e00\u5bb6\u5f8b\u5e08\u4e8b\u52a1\u6240\uff0c\u6211\u4eec\u516c\u53f8\u8d1f\u8d23\u54cd\u5e94\u5ba2\u6237\u7684\u6cd5\u5f8b\u54a8\u8be2\u3001\u5e2e\u5ba2\u6237\u5728\u6cd5\u5ead\u4e0a\u8fa9\u62a4\uff0c\u6211\u4eec\u516c\u53f8\u91cc\u6709\u5f88\u591a\u673a\u5bc6\u7c7b\u578b\u7684\u6587\u4ef6\u6216\u8005\u6587\u6863\uff0c\u8bf7\u4f60\u4e3a\u6211\u5217\u4e3e\u4e00\u4e0b\u8fd9\u4e9b\u7c7b\u578b\uff0c\u53ea\u9700\u8981\u7ed9\u51fa\u7c7b\u578b\u540d\u548c\u8be5\u7c7b\u578b\u5bf9\u5e94\u7684\u98ce\u9669\u7b49\u7ea7\uff0c\u4e0d\u9700\u8981\u8f93\u51fajson\u4ee5\u5916\u7684\u5185\u5bb9<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12635, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 151645, 198, 151644, 872, 198, 56568, 110124, 20074, 99464, 101087, 101057, 3837, 35946, 101909, 100376, 104586, 73218, 3837, 105783, 110178, 31838, 3837, 97639, 73218, 100668, 102808, 107069, 100376, 100703, 5373, 99663, 100017, 18493, 108943, 17447, 114051, 3837, 97639, 73218, 69249, 101194, 32648, 27641, 109963, 26898, 100631, 111116, 37945, 56568, 17714, 35946, 118569, 100158, 100001, 31905, 3837, 107525, 107485, 31905, 13072, 33108, 75882, 31905, 103124, 106066, 104408, 3837, 104689, 66017, 2236, 105175, 104597, 151645, 198, 151644, 77091, 198], lora_request: None, prompt_adapter_request: None.\r\nINFO 08-09 03:04:18 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\n```\r\n\r\nhow skip compile FSM index?\r\n\r\n\r\n",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "closed",
    "created_at": "2024-08-09T03:10:36+00:00",
    "closed_at": "2025-06-27T19:59:20+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7332/reactions",
      "total_count": 4,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 4
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7332"
  },
  {
    "number": 8902,
    "title": "[Feature]: Guided Decoding Schema Cache Store",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\r\n\r\n# Problem\r\n\r\nI am currently working with structured outputs and experimented a little with VLLM + Outlines. Since our JSON Schemas can get quite complex the generation of the FSM can take around 2 Minutes per Schema. It would be great to have a feature where you can provide a Schema-Store to save your generated schemas over time in a local file and reload them when you restart your deployment. Ideally this would be implemented as flag in the vllm serve arguments:\r\n\r\nhttps://docs.vllm.ai/en/latest/models/engine_args.html\r\n\r\n# Current Implementation\r\nI assume that this is currently not supported and the code to not recompute the schema is handled with the @cache() decorator here:\r\n![Screenshot 2024-09-27 134948](https://github.com/user-attachments/assets/4d6480a8-5a79-40ab-8b5c-6023b0551233)\r\n\r\n\r\n### Alternatives\r\n\r\nAlternative solution would probably be to create custom python code to handle this for my use-case and use the VLLM python functions for generation instead of the \"VLLM serve\" command. However not sure how you could handle this with the API Deployment.\r\n\r\n### Additional context\r\n\r\nPS: Happy to contribute to this feature if this is something that can be useful to other people / makes also sense for the people who understand the code base better.\r\n\r\n### Before submitting a new issue...\r\n\r\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "structured-output",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-09-27T12:07:20+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8902/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8902"
  },
  {
    "number": 12548,
    "title": "[Bug]: Distilled DeepSeek Models do not work with guided_json",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nWhen using **DeepSeek distilled models** with **guided JSON output**, the response does not always adhere to the expected schema. Unlike the standard versions of the models (e.g., Llama 3), which complete the JSON properly within a given `max_tokens` limit, the distilled models often fail to do so.  \n\nFor example, when setting `max_tokens = x`, **Llama 3** correctly generates a full JSON response. However, with **DeepSeek's distilled versions**, the output is sometimes **incomplete**, often stopping at an **open bracket (`{`)** or other partial structures. This suggests that the distilled models may require a **higher `max_tokens` setting** than their non-distilled counterparts to function correctly.  \n\n## \ud83d\udd0d Expected Behavior  \n- The model should generate a **complete JSON response** within the specified `max_tokens` limit, just like other models do.  \n\n## \u274c Actual Behavior  \n- The response is often **truncated**, failing to complete the JSON structure.  \n- In some cases, only an **opening bracket (`{`)** is returned, with no further content.  \n\n## \ud83d\udee0 Steps to Reproduce  \n1. Use any **DeepSeek distilled model**.  \n2. Set up a **guided JSON schema** for structured output.  \n3. Set `max_tokens` to a reasonable value (e.g., similar to what works for Llama 3).  \n4. Observe that the response is often **incomplete or truncated**.  \n\n## \ud83d\udd04 Potential Workaround  \n- Manually increasing `max_tokens` seems to **partially mitigate** the issue, allowing for a more complete response. However, this is not an optimal solution as it increases token consumption unnecessarily.  \n\nI'm happy to provide **more details, logs, or test cases** if needed. Let me know how I can help in troubleshooting or verifying potential fixes.  \n\nThanks in advance for your time and effort in looking into this! \ud83d\ude4c \n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "open",
    "created_at": "2025-01-29T11:47:24+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12548/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12548"
  },
  {
    "number": 16880,
    "title": "[Bug]: [v0.8.4][Critical] Tools calling broken: xgrammar rejects minItems in JSON Schema, blocking agent functionality",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Red Hat Enterprise Linux 8.10 (Ootpa) (x86_64)\nGCC version: (GCC) 9.2.1 20191120 (Red Hat 9.2.1-2)\nClang version: Could not collect\nCMake version: version 3.27.7\nLibc version: glibc-2.28\n\nPython version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-4.18.0-553.40.1.el8_10.x86_64-x86_64-with-glibc2.28\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA A100 80GB PCIe\nGPU 1: NVIDIA A100 80GB PCIe\nGPU 2: NVIDIA A100 80GB PCIe\nGPU 3: NVIDIA A100 80GB PCIe\n\nNvidia driver version: 550.54.15\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:        x86_64\nCPU op-mode(s):      32-bit, 64-bit\nByte Order:          Little Endian\nCPU(s):              96\nOn-line CPU(s) list: 0-95\nThread(s) per core:  1\nCore(s) per socket:  48\nSocket(s):           2\nNUMA node(s):        4\nVendor ID:           AuthenticAMD\nCPU family:          25\nModel:               1\nModel name:          AMD EPYC 7V13 64-Core Processor\nStepping:            1\nCPU MHz:             2445.443\nBogoMIPS:            4890.88\nHypervisor vendor:   Microsoft\nVirtualization type: full\nL1d cache:           32K\nL1i cache:           32K\nL2 cache:            512K\nL3 cache:            32768K\nNUMA node0 CPU(s):   0-23\nNUMA node1 CPU(s):   24-47\nNUMA node2 CPU(s):   48-71\nNUMA node3 CPU(s):   72-95\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr arat umip vaes vpclmulqdq rdpid fsrm\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.3+cu124torch2.5\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0\n[pip3] torchao==0.9.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.50.3\n[pip3] triton==3.2.0\n[conda] flashinfer-python         0.2.3+cu124torch2.5          pypi_0    pypi\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-ml-py              12.570.86                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.3.0                   pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchao                   0.9.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] transformers              4.50.3                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV12    SYS     SYS     SYS     0-23    0               N/A\nGPU1    NV12     X      SYS     SYS     SYS     24-47   1               N/A\nGPU2    SYS     SYS      X      NV12    SYS     48-71   2               N/A\nGPU3    SYS     SYS     NV12     X      SYS     72-95   3               N/A\nNIC0    SYS     SYS     SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n\nLD_LIBRARY_PATH=/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib:/opt/rh/gcc-toolset-9/root/usr/lib64/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib/dyninst:/opt/rh/gcc-toolset-9/root/usr/lib64:/opt/rh/gcc-toolset-9/root/usr/lib\nVLLM_PORT=8081\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen using the OpenAI\u2010compatible tool\u2010calling interface with `--guided-decoding-backend xgrammar` and `tool_choice=\"required\"`, the client auto\u2011injects a JSON Schema for the array of tool calls that contains `\"minItems\":\u00a01`. vLLM\u2019s xgrammar backend currently [rejects any schema with minItems](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/guided_decoding/utils.py#L6), resulting in:\n\n```javascript\nopenai.BadRequestError: Error code: 400 - {\n  'message': 'The provided JSON schema contains features not supported by xgrammar.',\n  \u2026\n}\n```\n\nAs a result, tool-calling with agents is completely disabled in this configuration.\n\nIf I switch to `tool_choice=\"auto\"`, the error disappears but the model never emits any tool_calls (so `response.choices[0].message.tool_calls` is empty and I get an IndexError).\n\n\n## To reproduce \n```python\nimport os, json\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\nload_dotenv()\nclient = OpenAI(\n    base_url=os.getenv(\"VLLM_ENDPOINT\"),       # e.g. http://localhost:8000\n    api_key=\"****\",                            # redacted\n)\n\ndef get_weather(location: str, unit: str):\n    return f\"Getting the weather for {location} in {unit}\u2026\"\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"},\n                    \"unit\":     {\"type\": \"string\", \"enum\": [\"celsius\",\"fahrenheit\"]},\n                },\n                \"required\": [\"location\",\"unit\"],\n            },\n        },\n    }\n]\n\n# This triggers the BadRequestError:\nresponse = client.chat.completions.create(\n    model=client.models.list().data[0].id,\n    messages=[{\"role\":\"user\",\"content\":\"What's the weather in SF?\"}],\n    tools=tools,\n    tool_choice=\"required\",          # \u2190 forces an array schema with minItems:1\n)\n\n# IndexError if you use tool_choice=\"auto\" because tool_calls is empty\ntool_call = response.choices[0].message.tool_calls[0]\n\n```\n\n## Expected behavior\nWith `tool_choice=\"required\"`, vLLM should accept the generated array schema (even with `minItems:1`), or at least strip/ignore unsupported keywords like minItems.\n\nWith `tool_choice=\"auto\"`, the model should still choose to call the function when appropriate (e.g. a weather\u2010related query).\n\n## Logs\nError with `tool_choice=\"required\"` and `--guided-decoding-backend xgrammar`:\n\n```javascript\nBadRequestError: Error code: 400 - {\n  \"object\": \"error\",\n  \"message\": \"The provided JSON schema contains features not supported by xgrammar.\",\n  \"type\": \"BadRequestError\",\n  \"param\": null,\n  \"code\": 400\n}\n```\n\nvLLM server log:\n```log\n\u2026 guided_decoding=GuidedDecodingParams(json={\n    \"type\": \"array\",\n    \"minItems\": 1,\n    \"items\": { \u2026 }\n}, regex=None, choice=None, grammar=None, backend=None)\n\u2026\n\n```\n\n\n## Behavior with tool_choice=\"auto\":\n\n- No error in server or client logs\n- response.choices[0].message.tool_calls is an empty list \u2192 IndexError in client\n\n## Additional context\n\n- vLLM version: 0.8.4 (Docker image vllm/vllm-openai:v0.8.4.full)\n- Running with:\n```bash\n--guided-decoding-backend xgrammar \\\n--enable-auto-tool-choice \\\n--tool-call-parser hermes\n```\n- I have also tried running with\n```bash\n--guided-decoding-backend auto\n```\nIn that mode I no longer see the minItems error and tool calls succeed, but it\u2019s unclear whether vLLM is still using the xgrammar backend under the hood or silently falling back to another decoding backend (e.g. outlines).\n\nThis is a critical issue: without support for minItems in xgrammar, agent tool-calling is completely non-functional. Either xgrammar must relax JSON Schema restrictions or the client should avoid emitting unsupported keywords when targeting xgrammar.\n\nThanks for your work on vLLM! \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-04-19T19:09:55+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16880/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16880"
  },
  {
    "number": 10376,
    "title": "[Bug]: Guided Decoding Broken in Streaming mode",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.10 (main, Oct  3 2024, 07:29:13) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.8.0-1017-azure-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100 80GB PCIe\r\nGPU 1: NVIDIA A100 80GB PCIe\r\n\r\nNvidia driver version: 550.127.05\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               48\r\nOn-line CPU(s) list:                  0-47\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD EPYC 7V13 64-Core Processor\r\nCPU family:                           25\r\nModel:                                1\r\nThread(s) per core:                   1\r\nCore(s) per socket:                   48\r\nSocket(s):                            1\r\nStepping:                             1\r\nBogoMIPS:                             4890.87\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves user_shstk clzero xsaveerptr rdpru arat umip vaes vpclmulqdq rdpid fsrm\r\nHypervisor vendor:                    Microsoft\r\nVirtualization type:                  full\r\nL1d cache:                            1.5 MiB (48 instances)\r\nL1i cache:                            1.5 MiB (48 instances)\r\nL2 cache:                             24 MiB (48 instances)\r\nL3 cache:                             192 MiB (6 instances)\r\nNUMA node(s):                         2\r\nNUMA node0 CPU(s):                    0-23\r\nNUMA node1 CPU(s):                    24-47\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Vulnerable: Safe RET, no microcode\r\nVulnerability Spec store bypass:      Vulnerable\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] sentence-transformers==3.2.1\r\n[pip3] torch==2.5.1\r\n[pip3] torchaudio==2.5.1+cu121\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.45.2\r\n[pip3] transformers-stream-generator==0.0.5\r\n[pip3] triton==3.1.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] sentence-transformers     3.2.1                    pypi_0    pypi\r\n[conda] torch                     2.5.1                    pypi_0    pypi\r\n[conda] torchaudio                2.5.1+cu121              pypi_0    pypi\r\n[conda] torchvision               0.20.1                   pypi_0    pypi\r\n[conda] transformers              4.45.2                   pypi_0    pypi\r\n[conda] transformers-stream-generator 0.0.5                    pypi_0    pypi\r\n[conda] triton                    3.1.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.1.dev3367+g3b980c2 (git sha: 3b980c2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV12\tNODE\t0-23\t0\t\tN/A\r\nGPU1\tNV12\t X \tSYS\t24-47\t1\t\tN/A\r\nNIC0\tNODE\tSYS\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nGuided decoding broken in streaming mode after this commit https://github.com/vllm-project/vllm/commit/04cef2c6ab0ea47bb1dfa73d3343985499fe1c4b\r\nPrevious commits are working fine. Non-streaming mode works fine as well.\r\n\r\nDataset to test: https://raw.githubusercontent.com/JC1DA/SharedData/refs/heads/main/gsm8k_luca_input_prompts/dataset.json\r\n\r\nTest script:\r\n```\r\nimport json\r\nfrom openai import OpenAI\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nfrom tqdm import tqdm\r\n\r\njson_schema = \"\"\"\r\n{\r\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\r\n  \"type\": \"object\",\r\n  \"properties\": {\r\n    \"thoughts\": {\r\n      \"type\": \"array\",\r\n      \"items\": {\r\n        \"type\": \"object\",\r\n        \"properties\": {\r\n          \"action\": {\r\n            \"type\": \"string\",\r\n            \"description\": \"Description of the step in the thought process\"\r\n          },\r\n          \"calculation\": {\r\n            \"type\": \"string\",\r\n            \"description\": \"Calculation performed in this step\"\r\n          },\r\n          \"result\": {\r\n            \"type\": \"integer\",\r\n            \"description\": \"Result of the calculation\"\r\n          }\r\n        },\r\n        \"required\": [\"action\", \"calculation\", \"result\"],\r\n        \"additionalProperties\": false\r\n      }\r\n    },\r\n    \"answer\": {\r\n      \"type\": \"integer\",\r\n      \"description\": \"Final answer calculated from the thoughts\"\r\n    }\r\n  },\r\n  \"required\": [\"thoughts\", \"answer\"],\r\n  \"additionalProperties\": false\r\n}\r\n\"\"\".strip()\r\n\r\nmodel = \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\"\r\n\r\nclient = OpenAI(\r\n    base_url=\"http://localhost:5006/v1\",\r\n    api_key=\"NOKEY\",\r\n)\r\n\r\ndata = json.load(open(\"dataset.json\", \"r\", encoding=\"utf-8\"))\r\n\r\ndef get_output(prompt):\r\n    stream = client.chat.completions.create(\r\n        model=model,\r\n        messages=[\r\n            {\r\n                \"role\": \"system\",\r\n                \"content\": f\"\"\"\r\nYou are a helpful assistant that can answer questions from the user and provide useful information.\r\nGenerate your answer based on the JSON schema below.\r\n{json_schema}\r\n\"\"\".strip(),\r\n            },\r\n            {\r\n                \"role\": \"user\",\r\n                \"content\": prompt,\r\n            },\r\n        ],\r\n        max_tokens=1024,\r\n        temperature=0.0,\r\n        top_p=1.0,\r\n        stream=True,\r\n        extra_body={\"guided_json\": json.loads(json_schema), \"guided_decoding_backend\": \"outlines\"},\r\n    )\r\n\r\n    _data = \"\"\r\n    for chunk in stream:\r\n        if chunk.choices[0].delta.content is not None:\r\n            _data += chunk.choices[0].delta.content\r\n\r\n    return _data\r\n\r\nfutures = []\r\nfailed = 0\r\nwith ThreadPoolExecutor(max_workers=8) as executor:\r\n    futures = [executor.submit(get_output, prompt) for prompt in data]\r\n\r\n    for f in tqdm(futures):\r\n        res = f.result()\r\n        try:\r\n          res = json.loads(res)\r\n          # print(res)\r\n        except:\r\n            failed += 1\r\n\r\nprint(failed, \"/\", len(futures))\r\n```\r\n\r\nResult:\r\nCommit [6e056bc](https://github.com/vllm-project/vllm/commit/6e056bcf0414dfaee4db646f8f36ec961f0c9a33): failed 1 / 1318\r\nCommit [04cef2c](https://github.com/vllm-project/vllm/commit/04cef2c6ab0ea47bb1dfa73d3343985499fe1c4b): failed 263 / 1318\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-11-15T21:56:33+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10376/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/10376"
  },
  {
    "number": 11908,
    "title": "[RFC]: Implement Structured Output support for V1 engine",
    "body": "### Motivation.\n\nStructured Output is supported in v0, but not yet in v1. One reason for the delay is there have been performance challenges with the integration in v0, and we'd like to rethink the integration approach. We would also like to account for supporting additional techniques, jump decoding in particular, in the future.\n\nThe document below covers the proposed integration of the Structured Output functionality in V1 of the vLLM engine.\n\n\n### Proposed Change.\n\nA draft proposal can be found in this google doc: https://docs.google.com/document/d/1H6m_Y3FLJ1FYGCmjXdZzoJv-JCDSxnKuSY2XiAj-c6c/edit?tab=t.0\n\nThis content will eventually be moved into a PR as an addition to the design docs section of the vllm docs.\n\nRelated issue for closing xgrammar feature gaps: https://github.com/vllm-project/vllm/issues/12131\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n@mgoin @aarnphm @markmc @simon-mo @xuechendi @WoosukKwon \n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "structured-output",
      "RFC",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-01-09T23:41:04+00:00",
    "closed_at": "2025-03-10T13:28:30+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11908/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/11908"
  },
  {
    "number": 7220,
    "title": "[Feature]: Align the API with OAI's structured output",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nOpenAI API introduced a feature that supports structured output, this is basically the same as our `guided_json` feature.\r\n\r\n1. We should simply alias it to support this feature \ud83c\udf1f\r\n2. we might want to consider implementing this also for tools\n3. Implement `refusal`\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\nhttps://openai.com/index/introducing-structured-outputs-in-the-api/",
    "labels": [
      "feature request",
      "structured-output",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-08-06T20:30:45+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7220/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7220"
  },
  {
    "number": 17002,
    "title": "[Bug]: Guided Decoding Backend options with the OpenAI server recently broken",
    "body": "### Your current environment\n\nvLLM installed with:\n```\npip install https://wheels.vllm.ai/5536b30a4c7877d75758d21bdaf39b3a59aa2dc2/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl\n```\n\n\n### \ud83d\udc1b Describe the bug\n\nAfter merging https://github.com/vllm-project/vllm/pull/16789, using \"options\" for guided decoding backends no longer works. Attempting to include a backend option results in:\n```\n$ vllm serve meta-llama/Llama-3.2-3B-Instruct --guided-decoding-backend xgrammar:disable-any-whitespace\nINFO 04-22 18:45:12 [__init__.py:239] Automatically detected platform cuda.\nusage: vllm serve [model_tag] [options]\nvllm serve: error: argument --guided-decoding-backend: invalid choice: 'xgrammar:disable-any-whitespace' (choose from 'auto', 'outlines', 'lm-format-enforcer', 'xgrammar')\n```\nThe new type checking of the args checks against a Literal type for the backend name, disallowing any options. For reference, backend options are briefly documented [REF](https://docs.vllm.ai/en/latest/features/structured_outputs.html):\n\n> Additional backend-specific options can be supplied in a comma separated list following a colon after the backend name. For example \"xgrammar:no-fallback\" will not allow vLLM to fallback to a different backend on error.\n\nNote that there are a few backend options that can be combined like `guidance:disable-any-whitespace,no-fallback`, so simply adding entries to the list of Literals seems untenable. I encountered this bug when writing up a PR to add another option https://github.com/vllm-project/vllm/pull/15949.\n\ncc: @russellb @hmellor \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "closed",
    "created_at": "2025-04-22T18:54:27+00:00",
    "closed_at": "2025-04-29T19:02:24+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17002/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/17002"
  },
  {
    "number": 15188,
    "title": "[Bug]: Extra Characters in `content` When Using `enable_reasoning` with `stop` Parameter",
    "body": "![Image](https://github.com/user-attachments/assets/59d64b2b-986e-46e1-8ff1-d66588bd431e)\n\n### Your current environment\n\n#### Environment  \n- vLLM version: 0.7.3  \n- Model: DeepSeek R1  \n- Running on: H20 \n\n### \ud83d\udc1b Describe the bug\n\n#### Description  \nWhen running the **DeepSeek R1** model with the `vllm` framework and enabling the `enable_reasoning` parameter, the model\u2019s response is structured into two fields:  \n- **`reasoning_content`**: Represents the reasoning process.  \n- **`content`**: Represents the final output.  \n\nHowever, when specifying the `stop` parameter with any stop sequence, the `content` field in the response contains extra unintended characters. This issue does not occur when `enable_reasoning` is disabled.  \n\n#### Steps to Reproduce  \n1. Start `vllm` with `--enable-reasoning`.  \n2. Query the model with a `stop` parameter (e.g., `stop=[\"\\nObservation\"]`).  \n3. Observe that the `content` field includes additional characters beyond the expected stop sequence.  \n\n#### Expected Behavior  \n- The `content` field should properly respect the `stop` parameter without introducing unintended characters.  \n\n#### Actual Behavior  \n- The `content` field contains unexpected extra characters when `enable_reasoning` is enabled and a `stop` sequence is provided.  \n\nWould appreciate any insights or fixes regarding this issue. Thanks!  \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-03-20T05:33:28+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15188/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15188"
  },
  {
    "number": 15766,
    "title": "[Bug]: gemma 3 structured output api occurs assertion error",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI use vllm v0.8.2 with docker compose, to test structured output api on gemma-3-27b-it, and get this errror.\n\n```\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/guided_decoding/xgrammar_decoding.py\", line 355, in __call__\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     assert self.matchers[i].accept_token(sampled_token)\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160] AssertionError\nvllm-llm-1  | CRITICAL 03-30 02:15:01 [launcher.py:116] MQLLMEngine is already dead, terminating server process\n```\n<details>\n<summary>there is docker-compose.yaml</summary>\n\n```yaml\nservices:\n  vllm-llm:\n    image: vllm/vllm-openai:v0.8.2\n    runtime: nvidia\n    environment:\n      - NVIDIA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n      - CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n      - HUGGING_FACE_HUB_TOKEN=${hugging_face_hub_token}\n      - NCCL_CUMEM_ENABLE=0\n      - NCCL_P2P_DISABLE=1\n    volumes:\n      - /mnt/sdb/project_d9256/.cache/huggingface:/root/.cache/huggingface\n    ports:\n      - ${vllm_llm_port}:8000\n    ipc: host\n    privileged: true\n    command: >\n      --model google/gemma-3-27b-it\n      --dtype=half \n      --guided-decoding-backend xgrammar\n      --tensor-parallel-size 8\n      --gpu-memory-utilization 0.85\n      --max-model-len 16384\n      --max_num_seqs 16\n      --trust-remote-code\n      --enforce-eager\n```\n</details>\n\n<details>\n<summary>logs from container</summary>\n\n```\ndocker compose up\n[+] Running 1/1\n \u2714 Container vllm_deploy-vllm-llm-1  Created                                                                                                          0.0s \nAttaching to vllm-llm-1\nvllm-llm-1  | INFO 03-30 01:58:07 [__init__.py:239] Automatically detected platform cuda.\nvllm-llm-1  | INFO 03-30 01:58:09 [api_server.py:981] vLLM API server version 0.8.2\nvllm-llm-1  | INFO 03-30 01:58:09 [api_server.py:982] args: Namespace(host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='google/gemma-3-27b-it', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='half', kv_cache_dtype='auto', max_model_len=16384, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=8, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.85, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=16, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)\nvllm-llm-1  | WARNING 03-30 01:58:10 [config.py:2614] Casting torch.bfloat16 to torch.float16.\nvllm-llm-1  | INFO 03-30 01:58:17 [config.py:585] This model supports multiple tasks: {'generate', 'embed', 'score', 'reward', 'classify'}. Defaulting to 'generate'.\nvllm-llm-1  | WARNING 03-30 01:58:17 [arg_utils.py:1854] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \nvllm-llm-1  | INFO 03-30 01:58:17 [config.py:1519] Defaulting to use mp for distributed inference\nvllm-llm-1  | WARNING 03-30 01:58:17 [cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\nvllm-llm-1  | INFO 03-30 01:58:17 [api_server.py:241] Started engine process with PID 160\nvllm-llm-1  | INFO 03-30 01:58:21 [__init__.py:239] Automatically detected platform cuda.\nvllm-llm-1  | INFO 03-30 01:58:23 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.2) with config: model='google/gemma-3-27b-it', speculative_config=None, tokenizer='google/gemma-3-27b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=google/gemma-3-27b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True, \nvllm-llm-1  | WARNING 03-30 01:58:25 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 44 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nvllm-llm-1  | (VllmWorkerProcess pid=233) INFO 03-30 01:58:25 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\nvllm-llm-1  | (VllmWorkerProcess pid=234) INFO 03-30 01:58:25 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\nvllm-llm-1  | (VllmWorkerProcess pid=235) INFO 03-30 01:58:25 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\nvllm-llm-1  | (VllmWorkerProcess pid=236) INFO 03-30 01:58:25 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\nvllm-llm-1  | (VllmWorkerProcess pid=237) INFO 03-30 01:58:25 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\nvllm-llm-1  | (VllmWorkerProcess pid=238) INFO 03-30 01:58:25 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\nvllm-llm-1  | (VllmWorkerProcess pid=239) INFO 03-30 01:58:25 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\nvllm-llm-1  | INFO 03-30 01:58:27 [cuda.py:239] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nvllm-llm-1  | INFO 03-30 01:58:27 [cuda.py:288] Using XFormers backend.\nvllm-llm-1  | (VllmWorkerProcess pid=237) INFO 03-30 01:58:27 [cuda.py:239] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nvllm-llm-1  | (VllmWorkerProcess pid=237) INFO 03-30 01:58:27 [cuda.py:288] Using XFormers backend.\nvllm-llm-1  | (VllmWorkerProcess pid=239) INFO 03-30 01:58:27 [cuda.py:239] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nvllm-llm-1  | (VllmWorkerProcess pid=239) INFO 03-30 01:58:27 [cuda.py:288] Using XFormers backend.\nvllm-llm-1  | (VllmWorkerProcess pid=236) INFO 03-30 01:58:27 [cuda.py:239] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nvllm-llm-1  | (VllmWorkerProcess pid=236) INFO 03-30 01:58:27 [cuda.py:288] Using XFormers backend.\nvllm-llm-1  | (VllmWorkerProcess pid=234) INFO 03-30 01:58:27 [cuda.py:239] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nvllm-llm-1  | (VllmWorkerProcess pid=234) INFO 03-30 01:58:27 [cuda.py:288] Using XFormers backend.\nvllm-llm-1  | (VllmWorkerProcess pid=238) INFO 03-30 01:58:27 [cuda.py:239] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nvllm-llm-1  | (VllmWorkerProcess pid=238) INFO 03-30 01:58:27 [cuda.py:288] Using XFormers backend.\nvllm-llm-1  | (VllmWorkerProcess pid=233) INFO 03-30 01:58:27 [cuda.py:239] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nvllm-llm-1  | (VllmWorkerProcess pid=233) INFO 03-30 01:58:27 [cuda.py:288] Using XFormers backend.\nvllm-llm-1  | (VllmWorkerProcess pid=235) INFO 03-30 01:58:27 [cuda.py:239] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nvllm-llm-1  | (VllmWorkerProcess pid=235) INFO 03-30 01:58:27 [cuda.py:288] Using XFormers backend.\nvllm-llm-1  | (VllmWorkerProcess pid=233) INFO 03-30 01:58:32 [utils.py:931] Found nccl from library libnccl.so.2\nvllm-llm-1  | (VllmWorkerProcess pid=238) INFO 03-30 01:58:32 [utils.py:931] Found nccl from library libnccl.so.2\nvllm-llm-1  | (VllmWorkerProcess pid=233) INFO 03-30 01:58:32 [pynccl.py:69] vLLM is using nccl==2.21.5\nvllm-llm-1  | (VllmWorkerProcess pid=238) INFO 03-30 01:58:32 [pynccl.py:69] vLLM is using nccl==2.21.5\nvllm-llm-1  | (VllmWorkerProcess pid=239) INFO 03-30 01:58:32 [utils.py:931] Found nccl from library libnccl.so.2\nvllm-llm-1  | (VllmWorkerProcess pid=235) INFO 03-30 01:58:32 [utils.py:931] Found nccl from library libnccl.so.2\nvllm-llm-1  | INFO 03-30 01:58:32 [utils.py:931] Found nccl from library libnccl.so.2\nvllm-llm-1  | (VllmWorkerProcess pid=239) INFO 03-30 01:58:32 [pynccl.py:69] vLLM is using nccl==2.21.5\nvllm-llm-1  | (VllmWorkerProcess pid=235) INFO 03-30 01:58:32 [pynccl.py:69] vLLM is using nccl==2.21.5\nvllm-llm-1  | (VllmWorkerProcess pid=237) INFO 03-30 01:58:32 [utils.py:931] Found nccl from library libnccl.so.2\nvllm-llm-1  | INFO 03-30 01:58:32 [pynccl.py:69] vLLM is using nccl==2.21.5\nvllm-llm-1  | (VllmWorkerProcess pid=237) INFO 03-30 01:58:32 [pynccl.py:69] vLLM is using nccl==2.21.5\nvllm-llm-1  | (VllmWorkerProcess pid=234) INFO 03-30 01:58:32 [utils.py:931] Found nccl from library libnccl.so.2\nvllm-llm-1  | (VllmWorkerProcess pid=234) INFO 03-30 01:58:32 [pynccl.py:69] vLLM is using nccl==2.21.5\nvllm-llm-1  | (VllmWorkerProcess pid=236) INFO 03-30 01:58:32 [utils.py:931] Found nccl from library libnccl.so.2\nvllm-llm-1  | (VllmWorkerProcess pid=236) INFO 03-30 01:58:32 [pynccl.py:69] vLLM is using nccl==2.21.5\nvllm-llm-1  | (VllmWorkerProcess pid=233) WARNING 03-30 01:58:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nvllm-llm-1  | (VllmWorkerProcess pid=239) WARNING 03-30 01:58:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nvllm-llm-1  | (VllmWorkerProcess pid=234) WARNING 03-30 01:58:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nvllm-llm-1  | (VllmWorkerProcess pid=237) WARNING 03-30 01:58:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nvllm-llm-1  | (VllmWorkerProcess pid=238) WARNING 03-30 01:58:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nvllm-llm-1  | (VllmWorkerProcess pid=235) WARNING 03-30 01:58:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nvllm-llm-1  | (VllmWorkerProcess pid=236) WARNING 03-30 01:58:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nvllm-llm-1  | WARNING 03-30 01:58:33 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nvllm-llm-1  | INFO 03-30 01:58:33 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_eed37361'), local_subscribe_addr='ipc:///tmp/4311efb1-3175-46cf-bd1d-f333b8d49401', remote_subscribe_addr=None, remote_addr_ipv6=False)\nvllm-llm-1  | (VllmWorkerProcess pid=237) INFO 03-30 01:58:33 [parallel_state.py:954] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5\nvllm-llm-1  | (VllmWorkerProcess pid=239) INFO 03-30 01:58:33 [parallel_state.py:954] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7\nvllm-llm-1  | (VllmWorkerProcess pid=238) INFO 03-30 01:58:33 [parallel_state.py:954] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6\nvllm-llm-1  | (VllmWorkerProcess pid=236) INFO 03-30 01:58:33 [parallel_state.py:954] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4\nvllm-llm-1  | (VllmWorkerProcess pid=234) INFO 03-30 01:58:33 [parallel_state.py:954] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2\nvllm-llm-1  | (VllmWorkerProcess pid=233) INFO 03-30 01:58:33 [parallel_state.py:954] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1\nvllm-llm-1  | INFO 03-30 01:58:33 [parallel_state.py:954] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0\nvllm-llm-1  | (VllmWorkerProcess pid=235) INFO 03-30 01:58:33 [parallel_state.py:954] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3\nvllm-llm-1  | INFO 03-30 01:58:33 [model_runner.py:1110] Starting to load model google/gemma-3-27b-it...\nvllm-llm-1  | (VllmWorkerProcess pid=234) INFO 03-30 01:58:33 [model_runner.py:1110] Starting to load model google/gemma-3-27b-it...\nvllm-llm-1  | (VllmWorkerProcess pid=233) INFO 03-30 01:58:33 [model_runner.py:1110] Starting to load model google/gemma-3-27b-it...\nvllm-llm-1  | (VllmWorkerProcess pid=236) INFO 03-30 01:58:33 [model_runner.py:1110] Starting to load model google/gemma-3-27b-it...\nvllm-llm-1  | (VllmWorkerProcess pid=237) INFO 03-30 01:58:33 [model_runner.py:1110] Starting to load model google/gemma-3-27b-it...\nvllm-llm-1  | (VllmWorkerProcess pid=238) INFO 03-30 01:58:33 [model_runner.py:1110] Starting to load model google/gemma-3-27b-it...\nvllm-llm-1  | (VllmWorkerProcess pid=239) INFO 03-30 01:58:33 [model_runner.py:1110] Starting to load model google/gemma-3-27b-it...\nvllm-llm-1  | (VllmWorkerProcess pid=235) INFO 03-30 01:58:33 [model_runner.py:1110] Starting to load model google/gemma-3-27b-it...\nvllm-llm-1  | (VllmWorkerProcess pid=233) INFO 03-30 01:58:33 [cuda.py:239] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nvllm-llm-1  | (VllmWorkerProcess pid=233) INFO 03-30 01:58:33 [cuda.py:288] Using XFormers backend.\nvllm-llm-1  | (VllmWorkerProcess pid=239) INFO 03-30 01:58:33 [cuda.py:239] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nvllm-llm-1  | (VllmWorkerProcess pid=239) INFO 03-30 01:58:33 [cuda.py:288] Using XFormers backend.\nvllm-llm-1  | (VllmWorkerProcess pid=234) INFO 03-30 01:58:33 [cuda.py:239] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nvllm-llm-1  | (VllmWorkerProcess pid=234) INFO 03-30 01:58:33 [cuda.py:288] Using XFormers backend.\nvllm-llm-1  | (VllmWorkerProcess pid=238) INFO 03-30 01:58:33 [cuda.py:239] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nvllm-llm-1  | (VllmWorkerProcess pid=238) INFO 03-30 01:58:33 [cuda.py:288] Using XFormers backend.\nvllm-llm-1  | (VllmWorkerProcess pid=236) INFO 03-30 01:58:33 [cuda.py:239] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nvllm-llm-1  | (VllmWorkerProcess pid=236) INFO 03-30 01:58:33 [cuda.py:288] Using XFormers backend.\nvllm-llm-1  | (VllmWorkerProcess pid=237) INFO 03-30 01:58:33 [cuda.py:239] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nvllm-llm-1  | (VllmWorkerProcess pid=237) INFO 03-30 01:58:33 [cuda.py:288] Using XFormers backend.\nvllm-llm-1  | INFO 03-30 01:58:33 [cuda.py:239] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nvllm-llm-1  | INFO 03-30 01:58:33 [cuda.py:288] Using XFormers backend.\nvllm-llm-1  | (VllmWorkerProcess pid=235) INFO 03-30 01:58:33 [cuda.py:239] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nvllm-llm-1  | (VllmWorkerProcess pid=235) INFO 03-30 01:58:33 [cuda.py:288] Using XFormers backend.\nvllm-llm-1  | (VllmWorkerProcess pid=239) INFO 03-30 01:58:33 [config.py:3243] cudagraph sizes specified by model runner [] is overridden by config []\nvllm-llm-1  | (VllmWorkerProcess pid=233) INFO 03-30 01:58:33 [config.py:3243] cudagraph sizes specified by model runner [] is overridden by config []\nvllm-llm-1  | (VllmWorkerProcess pid=234) INFO 03-30 01:58:33 [config.py:3243] cudagraph sizes specified by model runner [] is overridden by config []\nvllm-llm-1  | (VllmWorkerProcess pid=236) INFO 03-30 01:58:33 [config.py:3243] cudagraph sizes specified by model runner [] is overridden by config []\nvllm-llm-1  | (VllmWorkerProcess pid=235) INFO 03-30 01:58:33 [config.py:3243] cudagraph sizes specified by model runner [] is overridden by config []\nvllm-llm-1  | (VllmWorkerProcess pid=238) INFO 03-30 01:58:33 [config.py:3243] cudagraph sizes specified by model runner [] is overridden by config []\nvllm-llm-1  | (VllmWorkerProcess pid=237) INFO 03-30 01:58:33 [config.py:3243] cudagraph sizes specified by model runner [] is overridden by config []\nvllm-llm-1  | INFO 03-30 01:58:33 [config.py:3243] cudagraph sizes specified by model runner [] is overridden by config []\nvllm-llm-1  | (VllmWorkerProcess pid=236) INFO 03-30 01:58:34 [weight_utils.py:265] Using model weights format ['*.safetensors']\nvllm-llm-1  | INFO 03-30 01:58:34 [weight_utils.py:265] Using model weights format ['*.safetensors']\nvllm-llm-1  | (VllmWorkerProcess pid=235) INFO 03-30 01:58:34 [weight_utils.py:265] Using model weights format ['*.safetensors']\nvllm-llm-1  | (VllmWorkerProcess pid=233) INFO 03-30 01:58:34 [weight_utils.py:265] Using model weights format ['*.safetensors']\nvllm-llm-1  | (VllmWorkerProcess pid=237) INFO 03-30 01:58:34 [weight_utils.py:265] Using model weights format ['*.safetensors']\nvllm-llm-1  | (VllmWorkerProcess pid=234) INFO 03-30 01:58:34 [weight_utils.py:265] Using model weights format ['*.safetensors']\nvllm-llm-1  | (VllmWorkerProcess pid=238) INFO 03-30 01:58:35 [weight_utils.py:265] Using model weights format ['*.safetensors']\nLoading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:   8% Completed | 1/12 [00:00<00:09,  1.21it/s]\nvllm-llm-1  | (VllmWorkerProcess pid=238) INFO 03-30 01:58:36 [weight_utils.py:281] Time spent downloading weights for google/gemma-3-27b-it: 0.688544 seconds\nLoading safetensors checkpoint shards:  17% Completed | 2/12 [00:01<00:07,  1.28it/s]\nvllm-llm-1  | (VllmWorkerProcess pid=234) INFO 03-30 01:58:37 [weight_utils.py:281] Time spent downloading weights for google/gemma-3-27b-it: 0.555108 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=237) INFO 03-30 01:58:37 [weight_utils.py:281] Time spent downloading weights for google/gemma-3-27b-it: 0.571839 seconds\nLoading safetensors checkpoint shards:  25% Completed | 3/12 [00:02<00:07,  1.21it/s]\nvllm-llm-1  | (VllmWorkerProcess pid=239) INFO 03-30 01:58:39 [weight_utils.py:265] Using model weights format ['*.safetensors']\nLoading safetensors checkpoint shards:  33% Completed | 4/12 [00:05<00:12,  1.52s/it]\nLoading safetensors checkpoint shards:  42% Completed | 5/12 [00:06<00:09,  1.35s/it]\nLoading safetensors checkpoint shards:  50% Completed | 6/12 [00:06<00:06,  1.13s/it]\nLoading safetensors checkpoint shards:  58% Completed | 7/12 [00:07<00:05,  1.06s/it]\nLoading safetensors checkpoint shards:  67% Completed | 8/12 [00:08<00:03,  1.07it/s]\nLoading safetensors checkpoint shards:  75% Completed | 9/12 [00:08<00:02,  1.46it/s]\nLoading safetensors checkpoint shards:  83% Completed | 10/12 [00:09<00:01,  1.47it/s]\nLoading safetensors checkpoint shards:  92% Completed | 11/12 [00:09<00:00,  1.49it/s]\nLoading safetensors checkpoint shards: 100% Completed | 12/12 [00:10<00:00,  1.49it/s]\nLoading safetensors checkpoint shards: 100% Completed | 12/12 [00:10<00:00,  1.14it/s]\nvllm-llm-1  | \nvllm-llm-1  | INFO 03-30 01:58:45 [loader.py:447] Loading weights took 10.57 seconds\nvllm-llm-1  | INFO 03-30 01:58:46 [model_runner.py:1146] Model loading took 6.7403 GB and 12.195223 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=233) INFO 03-30 01:58:47 [loader.py:447] Loading weights took 8.64 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=234) INFO 03-30 01:58:47 [loader.py:447] Loading weights took 9.39 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=233) INFO 03-30 01:58:47 [model_runner.py:1146] Model loading took 6.7403 GB and 13.596577 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=234) INFO 03-30 01:58:48 [model_runner.py:1146] Model loading took 6.7403 GB and 13.915557 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=235) INFO 03-30 01:58:48 [loader.py:447] Loading weights took 13.09 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=235) INFO 03-30 01:58:49 [model_runner.py:1146] Model loading took 6.7403 GB and 15.129310 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=236) INFO 03-30 01:58:51 [loader.py:447] Loading weights took 16.85 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=236) INFO 03-30 01:58:52 [model_runner.py:1146] Model loading took 6.7403 GB and 18.065466 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=239) INFO 03-30 01:58:52 [loader.py:447] Loading weights took 12.48 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=239) INFO 03-30 01:58:53 [model_runner.py:1146] Model loading took 6.7403 GB and 19.168756 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=237) INFO 03-30 01:58:53 [loader.py:447] Loading weights took 15.81 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=237) INFO 03-30 01:58:54 [model_runner.py:1146] Model loading took 6.7403 GB and 20.089317 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=238) INFO 03-30 01:58:56 [loader.py:447] Loading weights took 19.31 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=238) INFO 03-30 01:58:56 [model_runner.py:1146] Model loading took 6.7403 GB and 22.247941 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=235) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nvllm-llm-1  | (VllmWorkerProcess pid=239) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nvllm-llm-1  | (VllmWorkerProcess pid=237) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nvllm-llm-1  | (VllmWorkerProcess pid=236) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nvllm-llm-1  | (VllmWorkerProcess pid=233) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nvllm-llm-1  | (VllmWorkerProcess pid=238) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nvllm-llm-1  | (VllmWorkerProcess pid=234) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nvllm-llm-1  | Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nvllm-llm-1  | (VllmWorkerProcess pid=235) INFO 03-30 01:59:44 [worker.py:267] Memory profiling takes 48.07 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=235) INFO 03-30 01:59:44 [worker.py:267] the current vLLM instance can use total_gpu_memory (14.56GiB) x gpu_memory_utilization (0.85) = 12.38GiB\nvllm-llm-1  | (VllmWorkerProcess pid=235) INFO 03-30 01:59:44 [worker.py:267] model weights take 6.74GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 1.66GiB; the rest of the memory reserved for KV Cache is 3.88GiB.\nvllm-llm-1  | (VllmWorkerProcess pid=239) INFO 03-30 01:59:44 [worker.py:267] Memory profiling takes 48.14 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=239) INFO 03-30 01:59:44 [worker.py:267] the current vLLM instance can use total_gpu_memory (14.56GiB) x gpu_memory_utilization (0.85) = 12.38GiB\nvllm-llm-1  | (VllmWorkerProcess pid=239) INFO 03-30 01:59:44 [worker.py:267] model weights take 6.74GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 1.66GiB; the rest of the memory reserved for KV Cache is 3.88GiB.\nvllm-llm-1  | (VllmWorkerProcess pid=234) INFO 03-30 01:59:44 [worker.py:267] Memory profiling takes 48.10 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=234) INFO 03-30 01:59:44 [worker.py:267] the current vLLM instance can use total_gpu_memory (14.56GiB) x gpu_memory_utilization (0.85) = 12.38GiB\nvllm-llm-1  | (VllmWorkerProcess pid=234) INFO 03-30 01:59:44 [worker.py:267] model weights take 6.74GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 1.66GiB; the rest of the memory reserved for KV Cache is 3.88GiB.\nvllm-llm-1  | (VllmWorkerProcess pid=236) INFO 03-30 01:59:44 [worker.py:267] Memory profiling takes 48.17 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=236) INFO 03-30 01:59:44 [worker.py:267] the current vLLM instance can use total_gpu_memory (14.56GiB) x gpu_memory_utilization (0.85) = 12.38GiB\nvllm-llm-1  | (VllmWorkerProcess pid=236) INFO 03-30 01:59:44 [worker.py:267] model weights take 6.74GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 1.66GiB; the rest of the memory reserved for KV Cache is 3.88GiB.\nvllm-llm-1  | (VllmWorkerProcess pid=233) INFO 03-30 01:59:44 [worker.py:267] Memory profiling takes 48.17 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=233) INFO 03-30 01:59:44 [worker.py:267] the current vLLM instance can use total_gpu_memory (14.56GiB) x gpu_memory_utilization (0.85) = 12.38GiB\nvllm-llm-1  | (VllmWorkerProcess pid=233) INFO 03-30 01:59:44 [worker.py:267] model weights take 6.74GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 1.66GiB; the rest of the memory reserved for KV Cache is 3.88GiB.\nvllm-llm-1  | (VllmWorkerProcess pid=238) INFO 03-30 01:59:44 [worker.py:267] Memory profiling takes 48.26 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=238) INFO 03-30 01:59:44 [worker.py:267] the current vLLM instance can use total_gpu_memory (14.56GiB) x gpu_memory_utilization (0.85) = 12.38GiB\nvllm-llm-1  | (VllmWorkerProcess pid=238) INFO 03-30 01:59:44 [worker.py:267] model weights take 6.74GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 1.66GiB; the rest of the memory reserved for KV Cache is 3.88GiB.\nvllm-llm-1  | INFO 03-30 01:59:44 [worker.py:267] Memory profiling takes 48.31 seconds\nvllm-llm-1  | INFO 03-30 01:59:44 [worker.py:267] the current vLLM instance can use total_gpu_memory (14.56GiB) x gpu_memory_utilization (0.85) = 12.38GiB\nvllm-llm-1  | INFO 03-30 01:59:44 [worker.py:267] model weights take 6.74GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 1.66GiB; the rest of the memory reserved for KV Cache is 3.88GiB.\nvllm-llm-1  | (VllmWorkerProcess pid=237) INFO 03-30 01:59:45 [worker.py:267] Memory profiling takes 48.48 seconds\nvllm-llm-1  | (VllmWorkerProcess pid=237) INFO 03-30 01:59:45 [worker.py:267] the current vLLM instance can use total_gpu_memory (14.56GiB) x gpu_memory_utilization (0.85) = 12.38GiB\nvllm-llm-1  | (VllmWorkerProcess pid=237) INFO 03-30 01:59:45 [worker.py:267] model weights take 6.74GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 1.66GiB; the rest of the memory reserved for KV Cache is 3.88GiB.\nvllm-llm-1  | INFO 03-30 01:59:45 [executor_base.py:111] # cuda blocks: 4097, # CPU blocks: 4228\nvllm-llm-1  | INFO 03-30 01:59:45 [executor_base.py:116] Maximum concurrency for 16384 tokens per request: 4.00x\nvllm-llm-1  | INFO 03-30 01:59:52 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 55.79 seconds\nvllm-llm-1  | WARNING 03-30 01:59:52 [config.py:1028] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\nvllm-llm-1  | INFO 03-30 01:59:52 [serving_chat.py:115] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}\nvllm-llm-1  | INFO 03-30 01:59:52 [serving_completion.py:61] Using default completion sampling params from model: {'top_k': 64, 'top_p': 0.95}\nvllm-llm-1  | INFO 03-30 01:59:52 [api_server.py:1028] Starting vLLM API server on http://0.0.0.0:8000\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:26] Available routes are:\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /openapi.json, Methods: HEAD, GET\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /docs, Methods: HEAD, GET\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: HEAD, GET\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /redoc, Methods: HEAD, GET\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /health, Methods: GET\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /load, Methods: GET\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /ping, Methods: POST, GET\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /tokenize, Methods: POST\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /detokenize, Methods: POST\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /v1/models, Methods: GET\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /version, Methods: GET\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /v1/chat/completions, Methods: POST\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /v1/completions, Methods: POST\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /v1/embeddings, Methods: POST\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /pooling, Methods: POST\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /score, Methods: POST\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /v1/score, Methods: POST\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /rerank, Methods: POST\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /v1/rerank, Methods: POST\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /v2/rerank, Methods: POST\nvllm-llm-1  | INFO 03-30 01:59:52 [launcher.py:34] Route: /invocations, Methods: POST\nvllm-llm-1  | INFO:     Started server process [1]\nvllm-llm-1  | INFO:     Waiting for application startup.\nvllm-llm-1  | INFO:     Application startup complete.\nvllm-llm-1  | INFO:     172.20.0.1:40970 - \"GET /metrics HTTP/1.1\" 200 OK\nvllm-llm-1  | Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nvllm-llm-1  | INFO 03-30 02:13:59 [chat_utils.py:379] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.\nvllm-llm-1  | INFO:     172.20.0.1:40970 - \"GET /metrics HTTP/1.1\" 200 OK\nvllm-llm-1  | INFO:     172.20.0.1:37004 - \"GET /metrics HTTP/1.1\" 200 OK\nvllm-llm-1  | INFO 03-30 02:14:00 [logger.py:39] Received request chatcmpl-d41ae30e0e0e47df84e66c32a3cd3a26: prompt: '<bos><start_of_turn>user\\n\u8acb\u5c07\u4ee5\u4e0b\u500b\u4eba\u8cc7\u6599\u6574\u7406\u6210 JSON \u683c\u5f0f\uff1a\\n\\n\u59d3\u540d\uff08\u4e2d\u6587\uff09\uff1a\u738b\u5927\u660e\\n\u59d3\u540d\uff08\u82f1\u6587\uff09\uff1aDavid Wang\\n\u5e74\u9f61\uff1a32\\n\u570b\u7c4d\uff1a\u53f0\u7063\\n\u624b\u6a5f\uff1a+886912345678\\n\u8a9e\u8a00\uff1a\u4e2d\u6587\u3001\u82f1\u6587\\n\\n\u806f\u7d61\u4eba\u8cc7\u8a0a\uff1a\\n- \u59d3\u540d\uff1a\u674e\u5c0f\u7f8e\\n- \u95dc\u4fc2\uff1a\u59b9\u59b9\\n- \u624b\u6a5f\uff1a+886987654321\\n\\n\u81ea\u6211\u4ecb\u7d39\uff1a\\n\u6211\u662f\u4e00\u540d\u8edf\u9ad4\u5de5\u7a0b\u5e2b\uff0c\u64c1\u6709 10 \u5e74\u7684\u958b\u767c\u7d93\u9a57\uff0c\u64c5\u9577\u5f8c\u7aef\u67b6\u69cb\u8a2d\u8a08\u8207\u5927\u898f\u6a21\u7cfb\u7d71\u958b\u767c\u3002\u6211\u559c\u6b61\u5b78\u7fd2\u65b0\u6280\u8853\uff0c\u4e26\u81f4\u529b\u65bc\u63d0\u5347\u7cfb\u7d71\u6548\u80fd\u8207\u53ef\u64f4\u5c55\u6027\u3002<end_of_turn>\\n<start_of_turn>model\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16226, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json={'type': 'object', 'properties': {'personal_info': {'type': 'object', 'properties': {'chinese_name': {'type': 'string'}, 'english_name': {'type': 'string'}, 'age': {'type': 'integer'}, 'nationality': {'type': 'string'}, 'phone': {'type': 'string'}, 'languages': {'type': 'array', 'items': {'type': 'string'}}, 'bio': {'type': 'string'}}, 'required': ['chinese_name', 'english_name', 'age', 'nationality', 'phone', 'languages', 'bio']}, 'contact_person': {'type': 'object', 'properties': {'name': {'type': 'string'}, 'relationship': {'type': 'string'}, 'phone': {'type': 'string'}}, 'required': ['name', 'relationship', 'phone']}}, 'required': ['personal_info', 'contact_person']}, regex=None, choice=None, grammar=None, json_object=None, backend='xgrammar', whitespace_pattern=None), extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\nvllm-llm-1  | INFO 03-30 02:14:03 [engine.py:310] Added request chatcmpl-d41ae30e0e0e47df84e66c32a3cd3a26.\nvllm-llm-1  | INFO:     172.20.0.1:37004 - \"GET /metrics HTTP/1.1\" 200 OK\nvllm-llm-1  | INFO:     172.20.0.1:37004 - \"GET /metrics HTTP/1.1\" 200 OK\nvllm-llm-1  | INFO:     172.20.0.1:37004 - \"GET /metrics HTTP/1.1\" 200 OK\nvllm-llm-1  | INFO:     172.20.0.1:37004 - \"GET /metrics HTTP/1.1\" 200 OK\nvllm-llm-1  | INFO:     172.20.0.1:37004 - \"GET /metrics HTTP/1.1\" 200 OK\nvllm-llm-1  | INFO:     172.20.0.1:37004 - \"GET /metrics HTTP/1.1\" 200 OK\nvllm-llm-1  | INFO:     172.20.0.1:37004 - \"GET /metrics HTTP/1.1\" 200 OK\nvllm-llm-1  | INFO:     172.20.0.1:37004 - \"GET /metrics HTTP/1.1\" 200 OK\nvllm-llm-1  | INFO:     172.20.0.1:37004 - \"GET /metrics HTTP/1.1\" 200 OK\nvllm-llm-1  | INFO:     172.20.0.1:37004 - \"GET /metrics HTTP/1.1\" 200 OK\nvllm-llm-1  | INFO:     172.20.0.1:37004 - \"GET /metrics HTTP/1.1\" 200 OK\nvllm-llm-1  | INFO:     172.20.0.1:37004 - \"GET /metrics HTTP/1.1\" 200 OK\nvllm-llm-1  | INFO 03-30 02:15:00 [metrics.py:481] Avg prompt throughput: 2.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160] AssertionError()\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160] Traceback (most recent call last):\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 158, in start\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     self.run_engine_loop()\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 221, in run_engine_loop\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     request_outputs = self.engine_step()\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]                       ^^^^^^^^^^^^^^^^^^\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 247, in engine_step\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     raise e\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 230, in engine_step\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     return self.engine.step()\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]            ^^^^^^^^^^^^^^^^^^\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 1434, in step\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     outputs = self.model_executor.execute_model(\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 284, in execute_model\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     driver_outputs = self._driver_execute_model(execute_model_req)\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/mp_distributed_executor.py\", line 144, in _driver_execute_model\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     return self.driver_worker.execute_model(execute_model_req)\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 420, in execute_model\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     output = self.model_runner.execute_model(\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     return func(*args, **kwargs)\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1788, in execute_model\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     logits = self.model.compute_logits(hidden_or_intermediate_states,\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3_mm.py\", line 780, in compute_logits\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     return self.language_model.compute_logits(hidden_states,\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py\", line 509, in compute_logits\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     logits = self.logits_processor(self.model.embed_tokens, hidden_states,\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     return self._call_impl(*args, **kwargs)\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     return forward_call(*args, **kwargs)\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/logits_processor.py\", line 83, in forward\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     logits = _apply_logits_processors(logits, sampling_metadata)\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/logits_processor.py\", line 170, in _apply_logits_processors\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     _apply_logits_processors_single_seq(\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/logits_processor.py\", line 195, in _apply_logits_processors_single_seq\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     logits_row = logits_processor(past_tokens_ids, logits_row)\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/guided_decoding/xgrammar_decoding.py\", line 355, in __call__\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]     assert self.matchers[i].accept_token(sampled_token)\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvllm-llm-1  | ERROR 03-30 02:15:01 [engine.py:160] AssertionError\nvllm-llm-1  | CRITICAL 03-30 02:15:01 [launcher.py:116] MQLLMEngine is already dead, terminating server process\nvllm-llm-1  | INFO:     172.20.0.1:51176 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\nvllm-llm-1  | INFO:     Shutting down\nvllm-llm-1  | INFO:     Waiting for application shutdown.\nvllm-llm-1  | INFO:     Application shutdown complete.\nvllm-llm-1  | INFO:     Finished server process [1]\nvllm-llm-1  | Process SpawnProcess-1:\nvllm-llm-1  | Traceback (most recent call last):\nvllm-llm-1  |   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\nvllm-llm-1  |     util._exit_function()\nvllm-llm-1  |   File \"/usr/lib/python3.12/multiprocessing/util.py\", line 360, in _exit_function\nvllm-llm-1  |     p.join()\nvllm-llm-1  |   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 149, in join\nvllm-llm-1  |     res = self._popen.wait(timeout)\nvllm-llm-1  |           ^^^^^^^^^^^^^^^^^^^^^^^^^\nvllm-llm-1  |   File \"/usr/lib/python3.12/multiprocessing/popen_fork.py\", line 43, in wait\nvllm-llm-1  |     return self.poll(os.WNOHANG if timeout == 0.0 else 0)\nvllm-llm-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvllm-llm-1  |   File \"/usr/lib/python3.12/multiprocessing/popen_fork.py\", line 27, in poll\nvllm-llm-1  |     pid, sts = os.waitpid(self.pid, flag)\nvllm-llm-1  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^\nvllm-llm-1  |   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 426, in signal_handler\nvllm-llm-1  |     raise KeyboardInterrupt(\"MQLLMEngine terminated\")\nvllm-llm-1  | KeyboardInterrupt: MQLLMEngine terminated\nvllm-llm-1  | INFO 03-30 02:15:02 [multiproc_worker_utils.py:137] Terminating local vLLM worker processes\nvllm-llm-1  | Exception ignored in: <function LLMEngine.__del__ at 0x71e575061300>\nvllm-llm-1  | Traceback (most recent call last):\nvllm-llm-1  |   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 536, in __del__\nvllm-llm-1  |     model_executor.shutdown()\nvllm-llm-1  |   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/mp_distributed_executor.py\", line 134, in shutdown\nvllm-llm-1  |     worker_monitor.close()\nvllm-llm-1  |   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 141, in close\nvllm-llm-1  |     self.result_handler.close()\nvllm-llm-1  |   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 95, in close\nvllm-llm-1  |     self.result_queue.put(_TERMINATE)\nvllm-llm-1  |   File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 94, in put\nvllm-llm-1  |     self._start_thread()\nvllm-llm-1  |   File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 192, in _start_thread\nvllm-llm-1  |     self._thread.start()\nvllm-llm-1  |   File \"/usr/lib/python3.12/threading.py\", line 994, in start\nvllm-llm-1  |     _start_new_thread(self._bootstrap, ())\nvllm-llm-1  | RuntimeError: can't create new thread at interpreter shutdown\nvllm-llm-1  | [rank0]:[W330 02:15:02.387877262 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nvllm-llm-1  | /usr/lib/python3.12/multiprocessing/resource_tracker.py:255: UserWarning: resource_tracker: There appear to be 1 lea\n```\n</details>\n\n<details>\n<summary>test script</summary>\n\n```bash\nMODEL_NAME=\"google/gemma-3-27b-it\"\nDATA=$(cat <<EOF\n{\n  \"model\": \"$MODEL_NAME\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"\u8acb\u5c07\u4ee5\u4e0b\u500b\u4eba\u8cc7\u6599\u6574\u7406\u6210 JSON \u683c\u5f0f\uff1a\\n\\n\u59d3\u540d\uff08\u4e2d\u6587\uff09\uff1a\u738b\u5927\u660e\\n\u59d3\u540d\uff08\u82f1\u6587\uff09\uff1aDavid Wang\\n\u5e74\u9f61\uff1a32\\n\u570b\u7c4d\uff1a\u53f0\u7063\\n\u624b\u6a5f\uff1a+886912345678\\n\u8a9e\u8a00\uff1a\u4e2d\u6587\u3001\u82f1\u6587\\n\\n\u806f\u7d61\u4eba\u8cc7\u8a0a\uff1a\\n- \u59d3\u540d\uff1a\u674e\u5c0f\u7f8e\\n- \u95dc\u4fc2\uff1a\u59b9\u59b9\\n- \u624b\u6a5f\uff1a+886987654321\\n\\n\u81ea\u6211\u4ecb\u7d39\uff1a\\n\u6211\u662f\u4e00\u540d\u8edf\u9ad4\u5de5\u7a0b\u5e2b\uff0c\u64c1\u6709 10 \u5e74\u7684\u958b\u767c\u7d93\u9a57\uff0c\u64c5\u9577\u5f8c\u7aef\u67b6\u69cb\u8a2d\u8a08\u8207\u5927\u898f\u6a21\u7cfb\u7d71\u958b\u767c\u3002\u6211\u559c\u6b61\u5b78\u7fd2\u65b0\u6280\u8853\uff0c\u4e26\u81f4\u529b\u65bc\u63d0\u5347\u7cfb\u7d71\u6548\u80fd\u8207\u53ef\u64f4\u5c55\u6027\u3002\"\n    }\n  ],\n  \"temperature\": 0,\n  \"response_format\": {\n    \"type\": \"json_schema\",\n    \"json_schema\": {\n      \"name\": \"personal_data\",\n      \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"personal_info\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"chinese_name\": { \"type\": \"string\" },\n              \"english_name\": { \"type\": \"string\" },\n              \"age\": { \"type\": \"integer\" },\n              \"nationality\": { \"type\": \"string\" },\n              \"phone\": { \"type\": \"string\" },\n              \"languages\": {\n                \"type\": \"array\",\n                \"items\": { \"type\": \"string\" }\n              },\n              \"bio\": { \"type\": \"string\" }\n            },\n            \"required\": [\"chinese_name\", \"english_name\", \"age\", \"nationality\", \"phone\", \"languages\", \"bio\"]\n          },\n          \"contact_person\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"name\": { \"type\": \"string\" },\n              \"relationship\": { \"type\": \"string\" },\n              \"phone\": { \"type\": \"string\" }\n            },\n            \"required\": [\"name\", \"relationship\", \"phone\"]\n          }\n        },\n        \"required\": [\"personal_info\", \"contact_person\"]\n      }\n    }\n  }\n}\nEOF\n)\n\ncurl -X POST \"http://$IP:$PORT/v1/chat/completions\" \\\n  -H \"Content-Type: application/json\" \\\n  -H 'Accept: application/json' \\\n  -d \"$DATA\"\n```\n</details>\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "open",
    "created_at": "2025-03-30T09:26:45+00:00",
    "closed_at": null,
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15766/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15766"
  },
  {
    "number": 14465,
    "title": "[Bug]: Mistral-Small-24B-Instruct-2501 on V1 fails to start with Mistral tokenizer since V1 enabled guided decoding",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 03-07 16:45:32 [__init__.py:256] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Arch Linux (x86_64)\nGCC version: (GCC) 14.2.1 20250207\nClang version: 19.1.7\nCMake version: version 3.30.0\nLibc version: glibc-2.41\n\nPython version: 3.12.9 (main, Feb  9 2025, 04:01:11) [GCC 14.2.1 20250128] (64-bit runtime)\nPython platform: Linux-6.12.9-arch1-1-kvm-local-x86_64-with-glibc2.41\nIs CUDA available: True\nCUDA runtime version: 12.8.61\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Ti\nNvidia driver version: 570.124.04\ncuDNN version: Probably one of the following:\n/usr/lib/libcudnn.so.9.7.0\n/usr/lib/libcudnn_adv.so.9.7.0\n/usr/lib/libcudnn_cnn.so.9.7.0\n/usr/lib/libcudnn_engines_precompiled.so.9.7.0\n/usr/lib/libcudnn_engines_runtime_compiled.so.9.7.0\n/usr/lib/libcudnn_graph.so.9.7.0\n/usr/lib/libcudnn_heuristic.so.9.7.0\n/usr/lib/libcudnn_ops.so.9.7.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               24\nOn-line CPU(s) list:                  0-23\nVendor ID:                            GenuineIntel\nModel name:                           12th Gen Intel(R) Core(TM) i9-12900K\nCPU family:                           6\nModel:                                151\nThread(s) per core:                   2\nCore(s) per socket:                   16\nSocket(s):                            1\nStepping:                             2\nCPU(s) scaling MHz:                   46%\nCPU max MHz:                          5200.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             6374.40\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect user_shstk avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            640 KiB (16 instances)\nL1i cache:                            768 KiB (16 instances)\nL2 cache:                             14 MiB (10 instances)\nL3 cache:                             30 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-23\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Vulnerable\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Vulnerable\nVulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\nVulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Vulnerable; BHI: Vulnerable\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] curated-transformers==0.1.1\n[pip3] flashinfer==0.2.0.post1+cu124torch2.4\n[pip3] flashinfer-python==0.2.0.post2+cu124torch2.5\n[pip3] mypy==1.11.1\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-modelopt==0.21.0\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pynvml==12.0.0\n[pip3] pyzmq==26.2.1\n[pip3] sentence-transformers==3.2.0\n[pip3] spacy-curated-transformers==0.3.0\n[pip3] torch==2.5.1\n[pip3] torchac_cuda==0.2.5\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.49.0\n[pip3] transformers-stream-generator==0.0.5\n[pip3] triton==3.1.0\n[conda] No relevant packages\nROCM Version: 6.3.42134-0\nNeuron SDK Version: N/A\nvLLM Version: 0.7.4.dev306+gb4c01560d.d20250307\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n\u001b[4mGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\nGPU0\t X \t0-23\t0\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNCCL_SOCKET_IFNAME=br0\nMAX_JOBS=16\nCUDA_SDK_ROOT_DIR=/opt/cuda/targets/x86_64-linux\nCUDA_SDK=/opt/cuda/targets/x86_64-linux\nCUDA_PATH=/opt/cuda\nCUDA_HOME=/opt/cuda\nCUDA_HOME=/opt/cuda\nLD_LIBRARY_PATH=/home/jeff/envs/python/virtualenvs/vllm312/lib/python3.12/site-packages/cv2/../../lib64:\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n### \ud83d\udc1b Describe the bug\n\nCannot start Mistral-Small-24B-Instruct-2501 with the Mistral tokenizer on V1 anymore.\n\nTraceback:\n\n```text\nERROR 03-07 16:37:49 [core.py:324] EngineCore hit an exception: Traceback (most recent call last):\nERROR 03-07 16:37:49 [core.py:324]   File \"/home/jeff/.virtualenvs/vllm312/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 316, in run_engine_core\nERROR 03-07 16:37:49 [core.py:324]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 03-07 16:37:49 [core.py:324]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-07 16:37:49 [core.py:324]   File \"/home/jeff/.virtualenvs/vllm312/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 271, in __init__\nERROR 03-07 16:37:49 [core.py:324]     super().__init__(vllm_config, executor_class, log_stats)\nERROR 03-07 16:37:49 [core.py:324]   File \"/home/jeff/.virtualenvs/vllm312/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 65, in __init__\nERROR 03-07 16:37:49 [core.py:324]     self.structured_output_manager = StructuredOutputManager(vllm_config)\nERROR 03-07 16:37:49 [core.py:324]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-07 16:37:49 [core.py:324]   File \"/home/jeff/.virtualenvs/vllm312/lib/python3.12/site-packages/vllm/v1/structured_output/__init__.py\", line 42, in __init__\nERROR 03-07 16:37:49 [core.py:324]     tokenizer_info = xgr.TokenizerInfo.from_huggingface(\nERROR 03-07 16:37:49 [core.py:324]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-07 16:37:49 [core.py:324]   File \"/home/jeff/.virtualenvs/vllm312/lib/python3.12/site-packages/xgrammar/tokenizer_info.py\", line 219, in from_huggingface\nERROR 03-07 16:37:49 [core.py:324]     elif TokenizerInfo._is_tiktoken_tokenizer(tokenizer):\nERROR 03-07 16:37:49 [core.py:324]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-07 16:37:49 [core.py:324]   File \"/home/jeff/.virtualenvs/vllm312/lib/python3.12/site-packages/xgrammar/tokenizer_info.py\", line 107, in _is_tiktoken_tokenizer\nERROR 03-07 16:37:49 [core.py:324]     \"vocab_file\" in tokenizer.vocab_files_names\nERROR 03-07 16:37:49 [core.py:324]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-07 16:37:49 [core.py:324] AttributeError: 'MistralTokenizer' object has no attribute 'vocab_files_names'\nERROR 03-07 16:37:49 [core.py:324]\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "closed",
    "created_at": "2025-03-07T23:47:08+00:00",
    "closed_at": "2025-03-12T08:01:35+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14465/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14465"
  },
  {
    "number": 11104,
    "title": "[Feature]: guided decoding on TPU",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nI\u2019m not sure if this is possible, but right now the `execute_model` function on the `TPUModelRunner` is only outputting the predicted token_ids, rather than the distribution of tokens that we can sample from with some guidance (e.g., using outlines). I believe structured output is becoming more common, and most projects that require LLMs need this structured output feature.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "structured-output"
    ],
    "state": "closed",
    "created_at": "2024-12-11T14:26:43+00:00",
    "closed_at": "2025-04-23T18:32:19+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11104/reactions",
      "total_count": 2,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11104"
  },
  {
    "number": 14534,
    "title": "[Bug]: [V1] Molmo/Aria not supported on V1 due to xgrammar",
    "body": "### Your current environment\n\nCannot use these models on V1 due to Xgrammar assert\n\n### \ud83d\udc1b Describe the bug\n\n- run the following\n```bash\nVLLM_USE_V1=1 pytest -s -x models/decoder_only/vision_language/test_models.py -k molmo\nVLLM_USE_V1=1 pytest -s -x models/decoder_only/vision_language/test_models.py -k aria\n```\n\n- get the following back\n```bash\nERROR 03-10 03:06:35 [core.py:324] EngineCore hit an exception: Traceback (most recent call last):\nERROR 03-10 03:06:35 [core.py:324]   File \"/home/rshaw/vllm/vllm/v1/engine/core.py\", line 316, in run_engine_core\nERROR 03-10 03:06:35 [core.py:324]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 03-10 03:06:35 [core.py:324]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-10 03:06:35 [core.py:324]   File \"/home/rshaw/vllm/vllm/v1/engine/core.py\", line 271, in __init__\nERROR 03-10 03:06:35 [core.py:324]     super().__init__(vllm_config, executor_class, log_stats)\nERROR 03-10 03:06:35 [core.py:324]   File \"/home/rshaw/vllm/vllm/v1/engine/core.py\", line 65, in __init__\nERROR 03-10 03:06:35 [core.py:324]     self.structured_output_manager = StructuredOutputManager(vllm_config)\nERROR 03-10 03:06:35 [core.py:324]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-10 03:06:35 [core.py:324]   File \"/home/rshaw/vllm/vllm/v1/structured_output/__init__.py\", line 44, in __init__\nERROR 03-10 03:06:35 [core.py:324]     tokenizer_info = xgr.TokenizerInfo.from_huggingface(\nERROR 03-10 03:06:35 [core.py:324]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 03-10 03:06:35 [core.py:324]   File \"/home/rshaw/vllm/venv/lib/python3.12/site-packages/xgrammar/tokenizer_info.py\", line 188, in from_huggingface\nERROR 03-10 03:06:35 [core.py:324]     raise ValueError(msg)\nERROR 03-10 03:06:35 [core.py:324] ValueError: Input vocab_size less than minimum viable vocab size for tokenizer <class 'vllm.transformers_utils.tokenizer.get_cached_tokenizer.<locals>.CachedTokenizer'>.\n```\n\nSeems to be due to the relative sizes of the tokenizer vocab and model vocab\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "closed",
    "created_at": "2025-03-10T03:12:01+00:00",
    "closed_at": "2025-03-14T14:51:50+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14534/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14534"
  },
  {
    "number": 16030,
    "title": "[Bug]: xgrammar missing file crashes the server",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI have the following xgrammar version\n\n```\n[root@~]# pip show xgrammar\nName: xgrammar\nVersion: 0.1.17\nSummary: Efficient, Flexible and Portable Structured Generation\nHome-page: https://xgrammar.mlc.ai/\nAuthor: MLC Team\nAuthor-email:\nLicense: Apache 2.0\nLocation: /opt/pytorch/lib/python3.12/site-packages\nRequires: nanobind, ninja, pydantic, sentencepiece, tiktoken, torch, transformers\nRequired-by: vllm\n```\nand this is the command I use to initialize the model\n\n```\n   /opt/pytorch/bin/vllm serve Mistral/ --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --limit_mm_per_prompt 'image=1' --tensor-parallel-size 4 --max-model-len 120000 --quantization fp8 --port 8006 --host localhost --guided-decoding-backend xgrammar\n```\nThe model is `mistralai/Mistral-Small-3.1-24B-Instruct-2503`\n\nHowever, whenever I try to send a request with xgrammar as a request param I get the following error and the server crashes after this\n\n```\nApr  3 17:38:52 langmodel52 langmodel[139935]: 2025-04-03 17:38:52,710 - vllm.v1.metrics.loggers - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nApr  3 17:38:55 langmodel52 langmodel[139935]: 2025-04-03 17:38:55,620 - vllm.entrypoints.chat_utils - INFO - Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\nApr  3 17:38:55 langmodel52 langmodel[139935]: 2025-04-03 17:38:55,620 - vllm.entrypoints.chat_utils - WARNING - 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\nApr  3 17:38:55 langmodel52 langmodel[139935]: 2025-04-03 17:38:55,620 - vllm.entrypoints.chat_utils - WARNING - 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\nApr  3 17:38:55 langmodel52 langmodel[139935]: 2025-04-03 17:38:55,648 - vllm.entrypoints.logger - INFO - Received request chatcmpl-95dcc6fc5abe48ed875f554805e4e69d: prompt: None, params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=2000, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json={'type': 'object', 'properties': {'cited_answer': {'type': 'object', 'properties': {'answer': {'type': 'string'}, 'citations': {'type': 'object', 'properties': {'citation': {'type': 'array', 'items': {'type': 'object', 'properties': {'source_id': {'type': 'string'}, 'quote': {'type': 'string'}, 'url': {'type': 'string'}}, 'required': ['source_id', 'quote', 'url']}}}, 'required': ['citation']}}, 'required': ['answer', 'citations']}}, 'required': ['cited_answer']}, regex=None, choice=None, grammar=None, json_object=None, backend='xgrammar', whitespace_pattern=None), extra_args=None), prompt_token_ids:6,lora_request: None, prompt_adapter_request: None.\nApr  3 17:38:55 langmodel52 langmodel[139935]: 2025-04-03 17:38:55,652 - vllm.v1.engine.async_llm - INFO - Added request chatcmpl-95dcc6fc5abe48ed875f554805e4e69d.\nApr  3 17:38:55 langmodel52 rsyslogd[7298]: message too long (49151) with configured size 8096, begin of message is: 2025-04-03 17:38:55,648 - vllm.entrypoints.logger - INFO - Received request chat [v8.2204.0-3.amzn2023.0.4 try https://www.rsyslog.com/e/2445 ]\nApr  3 17:38:55 langmodel52 rsyslogd[7298]: message too long (49152) with configured size 8096, begin of message is: 60878, 1115, 8781, 1317, 3535, 16633, 38532, 1408, 2081, 2224, 1032, 1049, 1056, [v8.2204.0-3.amzn2023.0.4 try https://www.rsyslog.com/e/2445 ]\nApr  3 17:38:55 langmodel52 rsyslogd[7298]: message too long (49152) with configured size 8096, begin of message is: 4, 11556, 1110, 14261, 2081, 6250, 3890, 72792, 2639, 26576, 1097, 1048, 1069, 1 [v8.2204.0-3.amzn2023.0.4 try https://www.rsyslog.com/e/2445 ]\nApr  3 17:38:55 langmodel52 langmodel[140223]: 2025-04-03 17:38:55,813 - mistral_common.tokens.tokenizers.tekken - INFO - Adding special tokens <SPECIAL_20>, ..., <SPECIAL_999>\nApr  3 17:38:55 langmodel52 langmodel[140223]: 2025-04-03 17:38:55,813 - mistral_common.tokens.tokenizers.tekken - INFO - Vocab size: 150000\nApr  3 17:38:55 langmodel52 langmodel[140223]: 2025-04-03 17:38:55,814 - mistral_common.tokens.tokenizers.tekken - INFO - Cutting vocab to first 130072 tokens.\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m 2025-04-03 17:38:57,454 - vllm.v1.executor.multiproc_executor - ERROR - WorkerProc hit an exception: %s\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m Traceback (most recent call last):\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 372, in worker_busy_loop\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     output = func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m              ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     return func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m            ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 242, in execute_model\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     output = self.model_runner.execute_model(scheduler_output)\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     return func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m            ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1052, in execute_model\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     self.apply_grammar_bitmask(scheduler_output, logits)\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 955, in apply_grammar_bitmask\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     xgr.apply_token_bitmask_inplace(\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/matcher.py\", line 117, in apply_token_bitmask_inplace\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     from .kernels import apply_token_bitmask_inplace_kernels\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/__init__.py\", line 13, in <module>\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     from .apply_token_bitmask_inplace_cuda import apply_token_bitmask_inplace_cuda\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_cuda.py\", line 91, in <module>\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     _load_torch_ops()\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_cuda.py\", line 78, in _load_torch_ops\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     torch.utils.cpp_extension.load_inline(\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1723, in load_inline\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     return _jit_compile(\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m            ^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1823, in _jit_compile\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     return _import_module_from_library(name, build_directory, is_python_module)\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 2250, in _import_module_from_library\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     torch.ops.load_library(filepath)\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/_ops.py\", line 1357, in load_library\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     ctypes.CDLL(path)\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/usr/local/lib/python3.12/ctypes/__init__.py\", line 379, in __init__\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     self._handle = _dlopen(self._name, mode)\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m OSError: /root/.cache/torch_extensions/py312_cu124/xgrammar/xgrammar.so: cannot open shared object file: No such file or directory\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m Traceback (most recent call last):\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 372, in worker_busy_loop\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     output = func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m              ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     return func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m            ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 242, in execute_model\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     output = self.model_runner.execute_model(scheduler_output)\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     return func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m            ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1052, in execute_model\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     self.apply_grammar_bitmask(scheduler_output, logits)\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 955, in apply_grammar_bitmask\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     xgr.apply_token_bitmask_inplace(\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/matcher.py\", line 117, in apply_token_bitmask_inplace\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     from .kernels import apply_token_bitmask_inplace_kernels\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/__init__.py\", line 13, in <module>\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     from .apply_token_bitmask_inplace_cuda import apply_token_bitmask_inplace_cuda\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_cuda.py\", line 91, in <module>\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     _load_torch_ops()\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_cuda.py\", line 78, in _load_torch_ops\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     torch.utils.cpp_extension.load_inline(\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1723, in load_inline\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     return _jit_compile(\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m            ^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1823, in _jit_compile\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     return _import_module_from_library(name, build_directory, is_python_module)\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m 2025-04-03 17:38:57,454 - vllm.v1.executor.multiproc_executor - ERROR - WorkerProc hit an exception: %s\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m Traceback (most recent call last):\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 372, in worker_busy_loop\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     output = func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m              ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     return func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m            ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 242, in execute_model\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     output = self.model_runner.execute_model(scheduler_output)\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     return func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m            ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1052, in execute_model\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     self.apply_grammar_bitmask(scheduler_output, logits)\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 955, in apply_grammar_bitmask\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     xgr.apply_token_bitmask_inplace(\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/matcher.py\", line 117, in apply_token_bitmask_inplace\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     from .kernels import apply_token_bitmask_inplace_kernels\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/__init__.py\", line 13, in <module>\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     from .apply_token_bitmask_inplace_cuda import apply_token_bitmask_inplace_cuda\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_cuda.py\", line 91, in <module>\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     _load_torch_ops()\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_cuda.py\", line 78, in _load_torch_ops\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     torch.utils.cpp_extension.load_inline(\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1723, in load_inline\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     return _jit_compile(\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m            ^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1823, in _jit_compile\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     return _import_module_from_library(name, build_directory, is_python_module)\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 2250, in _import_module_from_library\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     torch.ops.load_library(filepath)\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/_ops.py\", line 1357, in load_library\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     ctypes.CDLL(path)\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/usr/local/lib/python3.12/ctypes/__init__.py\", line 379, in __init__\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     self._handle = _dlopen(self._name, mode)\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m OSError: /root/.cache/torch_extensions/py312_cu124/xgrammar/xgrammar.so: cannot open shared object file: No such file or directory\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m Traceback (most recent call last):\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 372, in worker_busy_loop\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     output = func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m              ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     return func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m            ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 242, in execute_model\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     output = self.model_runner.execute_model(scheduler_output)\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     return func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m            ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1052, in execute_model\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     self.apply_grammar_bitmask(scheduler_output, logits)\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 955, in apply_grammar_bitmask\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     xgr.apply_token_bitmask_inplace(\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/matcher.py\", line 117, in apply_token_bitmask_inplace\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     from .kernels import apply_token_bitmask_inplace_kernels\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/__init__.py\", line 13, in <module>\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     from .apply_token_bitmask_inplace_cuda import apply_token_bitmask_inplace_cuda\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_cuda.py\", line 91, in <module>\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     _load_torch_ops()\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_cuda.py\", line 78, in _load_torch_ops\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     torch.utils.cpp_extension.load_inline(\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1723, in load_inline\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     return _jit_compile(\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m            ^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1823, in _jit_compile\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     return _import_module_from_library(name, build_directory, is_python_module)\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 2250, in _import_module_from_library\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     torch.ops.load_library(filepath)\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/_ops.py\", line 1357, in load_library\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     ctypes.CDLL(path)\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m   File \"/usr/local/lib/python3.12/ctypes/__init__.py\", line 379, in __init__\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m     self._handle = _dlopen(self._name, mode)\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m OSError: /root/.cache/torch_extensions/py312_cu124/xgrammar/xgrammar.so: cannot open shared object file: No such file or directory\nApr  3 17:38:57 langmodel52 langmodel[140320]: #033[1;36m(VllmWorker rank=2 pid=140320)#033[0;0m\nApr  3 17:38:57 langmodel52 langmodel[140343]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 2250, in _import_module_from_library\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     torch.ops.load_library(filepath)\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/_ops.py\", line 1357, in load_library\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     ctypes.CDLL(path)\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m   File \"/usr/local/lib/python3.12/ctypes/__init__.py\", line 379, in __init__\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m     self._handle = _dlopen(self._name, mode)\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m OSError: /root/.cache/torch_extensions/py312_cu124/xgrammar/xgrammar.so: cannot open shared object file: No such file or directory\nApr  3 17:38:57 langmodel52 langmodel[140343]: #033[1;36m(VllmWorker rank=3 pid=140343)#033[0;0m\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m 2025-04-03 17:38:57,454 - vllm.v1.executor.multiproc_executor - ERROR - WorkerProc hit an exception: %s\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m Traceback (most recent call last):\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 372, in worker_busy_loop\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     output = func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m              ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     return func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m            ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 242, in execute_model\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     output = self.model_runner.execute_model(scheduler_output)\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     return func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m            ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1052, in execute_model\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     self.apply_grammar_bitmask(scheduler_output, logits)\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 955, in apply_grammar_bitmask\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     xgr.apply_token_bitmask_inplace(\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/matcher.py\", line 117, in apply_token_bitmask_inplace\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     from .kernels import apply_token_bitmask_inplace_kernels\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/__init__.py\", line 13, in <module>\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     from .apply_token_bitmask_inplace_cuda import apply_token_bitmask_inplace_cuda\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_cuda.py\", line 91, in <module>\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     _load_torch_ops()\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_cuda.py\", line 78, in _load_torch_ops\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     torch.utils.cpp_extension.load_inline(\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1723, in load_inline\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     return _jit_compile(\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m            ^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1823, in _jit_compile\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     return _import_module_from_library(name, build_directory, is_python_module)\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 2250, in _import_module_from_library\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     torch.ops.load_library(filepath)\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/_ops.py\", line 1357, in load_library\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     ctypes.CDLL(path)\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/usr/local/lib/python3.12/ctypes/__init__.py\", line 379, in __init__\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     self._handle = _dlopen(self._name, mode)\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m OSError: /root/.cache/torch_extensions/py312_cu124/xgrammar/xgrammar.so: cannot open shared object file: No such file or directory\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m Traceback (most recent call last):\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 372, in worker_busy_loop\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     output = func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m              ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     return func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m            ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 242, in execute_model\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     output = self.model_runner.execute_model(scheduler_output)\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     return func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m            ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1052, in execute_model\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     self.apply_grammar_bitmask(scheduler_output, logits)\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 955, in apply_grammar_bitmask\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     xgr.apply_token_bitmask_inplace(\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/matcher.py\", line 117, in apply_token_bitmask_inplace\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     from .kernels import apply_token_bitmask_inplace_kernels\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/__init__.py\", line 13, in <module>\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     from .apply_token_bitmask_inplace_cuda import apply_token_bitmask_inplace_cuda\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_cuda.py\", line 91, in <module>\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     _load_torch_ops()\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_cuda.py\", line 78, in _load_torch_ops\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     torch.utils.cpp_extension.load_inline(\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1723, in load_inline\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     return _jit_compile(\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m            ^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1823, in _jit_compile\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     return _import_module_from_library(name, build_directory, is_python_module)\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 2250, in _import_module_from_library\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     torch.ops.load_library(filepath)\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/opt/pytorch/lib/python3.12/site-packages/torch/_ops.py\", line 1357, in load_library\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     ctypes.CDLL(path)\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m   File \"/usr/local/lib/python3.12/ctypes/__init__.py\", line 379, in __init__\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m     self._handle = _dlopen(self._name, mode)\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m OSError: /root/.cache/torch_extensions/py312_cu124/xgrammar/xgrammar.so: cannot open shared object file: No such file or directory\nApr  3 17:38:57 langmodel52 langmodel[140301]: #033[1;36m(VllmWorker rank=1 pid=140301)#033[0;0m\nApr  3 17:38:57 langmodel52 langmodel[140223]: 2025-04-03 17:38:57,455 - vllm.v1.engine.core - ERROR - EngineCore hit an exception: Traceback (most recent call last):\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 336, in run_engine_core\nApr  3 17:38:57 langmodel52 langmodel[140223]:    engine_core.run_busy_loop()\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 370, in run_busy_loop\nApr  3 17:38:57 langmodel52 langmodel[140223]:    outputs = step_fn()\nApr  3 17:38:57 langmodel52 langmodel[140223]:              ^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 195, in step\nApr  3 17:38:57 langmodel52 langmodel[140223]:    output = self.model_executor.execute_model(scheduler_output)\nApr  3 17:38:57 langmodel52 langmodel[140223]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 77, in execute_model\nApr  3 17:38:57 langmodel52 langmodel[140223]:    output = self.collective_rpc(\"execute_model\",\nApr  3 17:38:57 langmodel52 langmodel[140223]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 134, in collective_rpc\nApr  3 17:38:57 langmodel52 langmodel[140223]:    raise e\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 123, in collective_rpc\nApr  3 17:38:57 langmodel52 langmodel[140223]:    raise result\nApr  3 17:38:57 langmodel52 langmodel[140223]: OSError: /root/.cache/torch_extensions/py312_cu124/xgrammar/xgrammar.so: cannot open shared object file: No such file or directory\nApr  3 17:38:57 langmodel52 langmodel[140223]: Traceback (most recent call last):\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 372, in worker_busy_loop\nApr  3 17:38:57 langmodel52 langmodel[140223]:    output = func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140223]:             ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nApr  3 17:38:57 langmodel52 langmodel[140223]:    return func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140223]:           ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 242, in execute_model\nApr  3 17:38:57 langmodel52 langmodel[140223]:    output = self.model_runner.execute_model(scheduler_output)\nApr  3 17:38:57 langmodel52 langmodel[140223]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nApr  3 17:38:57 langmodel52 langmodel[140223]:    return func(*args, **kwargs)\nApr  3 17:38:57 langmodel52 langmodel[140223]:           ^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1052, in execute_model\nApr  3 17:38:57 langmodel52 langmodel[140223]:    self.apply_grammar_bitmask(scheduler_output, logits)\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 955, in apply_grammar_bitmask\nApr  3 17:38:57 langmodel52 langmodel[140223]:    xgr.apply_token_bitmask_inplace(\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/matcher.py\", line 117, in apply_token_bitmask_inplace\nApr  3 17:38:57 langmodel52 langmodel[140223]:    from .kernels import apply_token_bitmask_inplace_kernels\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/__init__.py\", line 13, in <module>\nApr  3 17:38:57 langmodel52 langmodel[140223]:    from .apply_token_bitmask_inplace_cuda import apply_token_bitmask_inplace_cuda\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_cuda.py\", line 91, in <module>\nApr  3 17:38:57 langmodel52 langmodel[140223]:    _load_torch_ops()\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_cuda.py\", line 78, in _load_torch_ops\nApr  3 17:38:57 langmodel52 langmodel[140223]:    torch.utils.cpp_extension.load_inline(\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1723, in load_inline\nApr  3 17:38:57 langmodel52 langmodel[140223]:    return _jit_compile(\nApr  3 17:38:57 langmodel52 langmodel[140223]:           ^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1823, in _jit_compile\nApr  3 17:38:57 langmodel52 langmodel[140223]:    return _import_module_from_library(name, build_directory, is_python_module)\nApr  3 17:38:57 langmodel52 langmodel[140223]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 2250, in _import_module_from_library\nApr  3 17:38:57 langmodel52 langmodel[140223]:    torch.ops.load_library(filepath)\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/opt/pytorch/lib/python3.12/site-packages/torch/_ops.py\", line 1357, in load_library\nApr  3 17:38:57 langmodel52 langmodel[140223]:    ctypes.CDLL(path)\nApr  3 17:38:57 langmodel52 langmodel[140223]:  File \"/usr/local/lib/python3.12/ctypes/__init__.py\", line 379, in __init__\nApr  3 17:38:57 langmodel52 langmodel[140223]:    self._handle = _dlopen(self._name, mode)\nApr  3 17:38:57 langmodel52 langmodel[140223]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^\nApr  3 17:38:57 langmodel52 langmodel[140223]: OSError: /root/.cache/torch_extensions/py312_cu124/xgrammar/xgrammar.so: cannot open shared object file: No such file or directory\nApr  3 17:38:57 langmodel52 langmodel[139935]: 2025-04-03 17:38:57,455 - vllm.v1.engine.core_client - CRITICAL - Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\nApr  3 17:38:57 langmodel52 langmodel[139934]: /usr/local/sbin/langmodel: line 22: 139935 Killed                  /opt/pytorch/bin/vllm serve Mistral/ --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --limit_mm_per_prompt 'image=1' --tensor-parallel-size 4 --max-model-len 120000 --quantization fp8 --port 8006 --host langmodel52 --guided-decoding-backend xgrammar\nApr  3 17:38:57 langmodel52 systemd[1]: Starting refresh-policy-routes@enp39s0.service - Refresh policy routes for enp39s0...\nApr  3 17:38:57 langmodel52 systemd[1]: langmodel.service: Main process exited, code=exited, status=137/n/a\nApr  3 17:38:57 langmodel52 systemd[1]: langmodel.service: Killing process 140419 (ZMQbg/IO/0) with signal SIGKILL.\nApr  3 17:38:57 langmodel52 systemd[1]: langmodel.service: Killing process 140424 (ZMQbg/Reaper) with signal SIGKILL.\nApr  3 17:38:57 langmodel52 systemd[1]: langmodel.service: Killing process 140577 (ZMQbg/IO/0) with signal SIGKILL.\nApr  3 17:38:57 langmodel52 systemd[1]: langmodel.service: Killing process 140595 (python3.12) with signal SIGKILL.\nApr  3 17:38:57 langmodel52 systemd[1]: langmodel.service: Killing process 140597 (python3.12) with signal SIGKILL.\nApr  3 17:38:57 langmodel52 ec2net[140601]: Starting configuration refresh for enp39s0\nApr  3 17:38:57 langmodel52 systemd[1]: refresh-policy-routes@enp39s0.service: Deactivated successfully.\nApr  3 17:38:57 langmodel52 systemd[1]: Finished refresh-policy-routes@enp39s0.service - Refresh policy routes for enp39s0.\nApr  3 17:38:57 langmodel52 audit[1]: SERVICE_START pid=1 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:init_t:s0 msg='unit=refresh-policy-routes@enp39s0 comm=\"systemd\" exe=\"/usr/lib/systemd/systemd\" hostname=? addr=? terminal=? res=success'\nApr  3 17:38:57 langmodel52 audit[1]: SERVICE_STOP pid=1 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:init_t:s0 msg='unit=refresh-policy-routes@enp39s0 comm=\"systemd\" exe=\"/usr/lib/systemd/systemd\" hostname=? addr=? terminal=? res=success'\nApr  3 17:38:59 langmodel52 systemd[1]: langmodel.service: Failed with result 'exit-code'.\nApr  3 17:38:59 langmodel52 audit[1]: SERVICE_STOP pid=1 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:init_t:s0 msg='unit=langmodel comm=\"systemd\" exe=\"/usr/lib/systemd/systemd\" hostname=? addr=? terminal=? res=failed'\nApr  3 17:38:59 langmodel52 systemd[1]: langmodel.service: Consumed 11min 53.480s CPU time.\n```\n\nbut xgrammar is installed, since it is a `vllm` dependency, I tried updating it, but see the same issue. SInce the vllm loads with the `param` set I would assume that the dependency and files are already checked while initializing the engine. Not sure what's happening here?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "closed",
    "created_at": "2025-04-03T18:09:44+00:00",
    "closed_at": "2025-04-24T17:39:51+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16030/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16030"
  },
  {
    "number": 11484,
    "title": "[Bug]: xgrammar crashes with speculative decoding",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\n$python collect_env.py \r\nCollecting environment information...\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Alibaba Group Enterprise Linux Server 7.2 (Paladin) (x86_64)\r\nGCC version: (GCC) 10.2.1 20200825 (Alibaba 10.2.1-3 2.17)\r\nClang version: Could not collect\r\nCMake version: version 3.26.4\r\nLibc version: glibc-2.32\r\n\r\nPython version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-4.9.151-015.ali3000.alios7.x86_64-x86_64-with-glibc2.32\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA L20\r\nGPU 1: NVIDIA L20\r\n\r\nNvidia driver version: 535.161.08\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn.so.8.9.3\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.9.3\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.9.3\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.9.3\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.9.3\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.9.3\r\n/usr/local/cuda/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.9.3\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                128\r\nOn-line CPU(s) list:   0-127\r\nThread(s) per core:    2\r\nCore(s) per socket:    32\r\nSocket(s):             2\r\nNUMA node(s):          2\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 143\r\nModel name:            Intel(R) Xeon(R) Gold 6462C\r\nStepping:              8\r\nCPU MHz:               3899.816\r\nCPU max MHz:           3900.0000\r\nCPU min MHz:           800.0000\r\nBogoMIPS:              6600.00\r\nVirtualization:        VT-x\r\nL1d cache:             48K\r\nL1i cache:             32K\r\nL2 cache:              2048K\r\nL3 cache:              61440K\r\nNUMA node0 CPU(s):     0-31,64-95\r\nNUMA node1 CPU(s):     32-63,96-127\r\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb invpcid_single cat_l2 cdp_l3 ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req avx512vbmi pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq pconfig flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==7.1.1\r\n[pip3] flashinfer==0.1.6+cu121torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1\r\n[pip3] torchaudio==2.1.0+cu121\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.47.0\r\n[pip3] triton==3.1.0\r\n[conda] flashinfer                0.1.6+cu121torch2.4          pypi_0    pypi\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.5.1                    pypi_0    pypi\r\n[conda] torchaudio                2.1.0+cu121              pypi_0    pypi\r\n[conda] torchvision               0.20.1                   pypi_0    pypi\r\n[conda] transformers              4.47.0                   pypi_0    pypi\r\n[conda] triton                    3.1.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.5\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      SYS     0-31,64-95      0               N/A\r\nGPU1    SYS      X      32-63,96-127    1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNVIDIA_VISIBLE_DEVICES=GPU-4fc988d5-d6ed-a0c6-4c45-3b7dccb2e7a2,GPU-e671ef65-d6ae-ecc4-bf99-777fe77a5207\r\nLD_LIBRARY_PATH=/opt/conda/lib/python3.10/site-packages/cv2/../../lib64:/opt/conda/lib/python3.10/site-packages/nvidia/nvjitlink/lib::/opt/conda/lib/python3.10/site-packages/aistudio_common/reader/libs/:/opt/taobao/java/jre/lib/amd64/server/:/usr/local/cuda/lib64:/opt/conda/lib/python3.10/site-packages/aistudio_common/reader/libs/:/opt/taobao/java/jre/lib/amd64/server/:/usr/local/cuda/lib64:/opt/conda/lib/python3.10/site-packages/aistudio_common/reader/libs/:/opt/taobao/java/jre/lib/amd64/server/:/usr/local/cuda/lib64:/opt/conda/lib/python3.10/site-packages/aistudio_common/reader/libs/:/opt/taobao/java/jre/lib/amd64/server/:/usr/local/cuda/lib64:/opt/conda/lib/python3.10/site-packages/aistudio_common/reader/libs/:/opt/taobao/java/jre/lib/amd64/server/:/usr/local/cuda/lib64\r\nNVIDIA_DRIVER_CAPABILITIES=all\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nWhen I use xgrammer as guided decoding backend, it crashes with speculative decoding. It works well without speculative decoding. \r\nshell script:\r\n```shell\r\nexport VLLM_ATTENTION_BACKEND=FLASHINFER\r\nexport TOKENIZERS_PARALLELISM=false\r\nmodel_path=\"./Qwen2-72B-Instruct-GPTQ-Int4/\"\r\nspeculative_model_path=\"./Qwen2-7B-Instruct-GPTQ-Int4/\"\r\n\r\nmax_model_len=8192\r\nport=8000\r\nmax_num_seqs=32\r\nmax_num_batched_tokens=4096\r\npython -m vllm.entrypoints.openai.api_server \\\r\n    --model ${model_path} \\\r\n    --served-model-name model \\\r\n    --max-model-len ${max_model_len} \\\r\n    --max-seq-len-to-capture ${max_model_len} \\\r\n    --distributed-executor-backend mp \\\r\n    --disable-log-requests \\\r\n    --tensor-parallel-size 2 \\\r\n    --pipeline-parallel-size 1 \\\r\n    --disable-custom-all-reduce \\\r\n    --uvicorn-log-level error \\\r\n    --port ${port} \\\r\n    --max-num-seqs ${max_num_seqs} \\\r\n    --gpu-memory-utilization 0.95 \\\r\n    --enable-prefix-caching \\\r\n    --enable-chunked-prefill \\\r\n    --max-num-batched-tokens ${max_num_batched_tokens} \\\r\n    --speculative-model ${speculative_model_path} \\\r\n    --speculative-draft-tensor-parallel-size 2 \\\r\n    --num-speculative-tokens 3 \\\r\n    --speculative-disable-by-batch-size 10 \\\r\n    --compilation-config 3 \\\r\n```\r\n\r\noutput:\r\n````\r\nINFO 12-25 15:08:42 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\r\nINFO 12-25 15:08:47 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20241225-150847.pkl...\r\nWARNING 12-25 15:08:47 model_runner_base.py:143] Failed to pickle inputs of failed execution: cannot pickle 'flashinfer._prefill.BatchPrefillWithPagedKVCachePyTorchWrapper' object\r\nERROR 12-25 15:08:47 engine.py:135] IndexError('Error in model execution: tuple index out of range')\r\nERROR 12-25 15:08:47 engine.py:135] Traceback (most recent call last):\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/vllm/worker/model_runner_base.py\", line 116, in _wrapper\r\nERROR 12-25 15:08:47 engine.py:135]     return func(*args, **kwargs)\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1729, in execute_model\r\nERROR 12-25 15:08:47 engine.py:135]     logits = self.model.compute_logits(hidden_or_intermediate_states,\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 487, in compute_logits\r\nERROR 12-25 15:08:47 engine.py:135]     logits = self.logits_processor(self.lm_head, hidden_states,\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nERROR 12-25 15:08:47 engine.py:135]     return self._call_impl(*args, **kwargs)\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\nERROR 12-25 15:08:47 engine.py:135]     return forward_call(*args, **kwargs)\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/vllm/model_executor/layers/logits_processor.py\", line 77, in forward\r\nERROR 12-25 15:08:47 engine.py:135]     logits = _apply_logits_processors(logits, sampling_metadata)\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/vllm/model_executor/layers/logits_processor.py\", line 153, in _apply_logits_processors\r\nERROR 12-25 15:08:47 engine.py:135]     logits_row = logits_processor(past_tokens_ids,\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/vllm/model_executor/guided_decoding/xgrammar_decoding.py\", line 258, in __call__\r\nERROR 12-25 15:08:47 engine.py:135]     sampled_token = input_ids[-1]\r\nERROR 12-25 15:08:47 engine.py:135] IndexError: tuple index out of range\r\nERROR 12-25 15:08:47 engine.py:135] \r\nERROR 12-25 15:08:47 engine.py:135] The above exception was the direct cause of the following exception:\r\nERROR 12-25 15:08:47 engine.py:135] \r\nERROR 12-25 15:08:47 engine.py:135] Traceback (most recent call last):\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 133, in start\r\nERROR 12-25 15:08:47 engine.py:135]     self.run_engine_loop()\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 196, in run_engine_loop\r\nERROR 12-25 15:08:47 engine.py:135]     request_outputs = self.engine_step()\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 214, in engine_step\r\nERROR 12-25 15:08:47 engine.py:135]     raise e\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 205, in engine_step\r\nERROR 12-25 15:08:47 engine.py:135]     return self.engine.step()\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 1405, in step\r\nERROR 12-25 15:08:47 engine.py:135]     outputs = self.model_executor.execute_model(\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py\", line 82, in execute_model\r\nERROR 12-25 15:08:47 engine.py:135]     driver_outputs = self._driver_execute_model(execute_model_req)\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 120, in _driver_execute_model\r\nERROR 12-25 15:08:47 engine.py:135]     return self.driver_worker.execute_model(execute_model_req)\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nERROR 12-25 15:08:47 engine.py:135]     return func(*args, **kwargs)\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/vllm/spec_decode/spec_decode_worker.py\", line 492, in execute_model\r\nERROR 12-25 15:08:47 engine.py:135]     return self._run_no_spec(execute_model_req,\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\r\nERROR 12-25 15:08:47 engine.py:135]     return func(*args, **kwds)\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/vllm/spec_decode/spec_decode_worker.py\", line 649, in _run_no_spec\r\nERROR 12-25 15:08:47 engine.py:135]     self.proposer_worker.execute_model(execute_model_req)\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/vllm/spec_decode/multi_step_worker.py\", line 69, in execute_model\r\nERROR 12-25 15:08:47 engine.py:135]     return self.worker.execute_model(*args, **kwargs)\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 343, in execute_model\r\nERROR 12-25 15:08:47 engine.py:135]     output = self.model_runner.execute_model(\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nERROR 12-25 15:08:47 engine.py:135]     return func(*args, **kwargs)\r\nERROR 12-25 15:08:47 engine.py:135]   File \"/opt/conda/lib/python3.10/site-packages/vllm/worker/model_runner_base.py\", line 146, in _wrapper\r\nERROR 12-25 15:08:47 engine.py:135]     raise type(err)(f\"Error in model execution: \"\r\nERROR 12-25 15:08:47 engine.py:135] IndexError: Error in model execution: tuple index out of range\r\nCRITICAL 12-25 15:08:47 launcher.py:99] MQLLMEngine is already dead, terminating server process\r\nCRITICAL 12-25 15:08:47 launcher.py:99] MQLLMEngine is already dead, terminating server process\r\nCRITICAL 12-25 15:08:47 launcher.py:99] MQLLMEngine is already dead, terminating server process\r\nCRITICAL 12-25 15:08:47 launcher.py:99] MQLLMEngine is already dead, terminating server process\r\nCRITICAL 12-25 15:08:47 launcher.py:99] MQLLMEngine is already dead, terminating server process\r\nCRITICAL 12-25 15:08:47 launcher.py:99] MQLLMEngine is already dead, terminating server process\r\nCRITICAL 12-25 15:08:47 launcher.py:99] MQLLMEngine is already dead, terminating server process\r\nCRITICAL 12-25 15:08:47 launcher.py:99] MQLLMEngine is already dead, terminating server process\r\nProcess SpawnProcess-1:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\r\n    util._exit_function()\r\n  File \"/opt/conda/lib/python3.10/multiprocessing/util.py\", line 357, in _exit_function\r\n    p.join()\r\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 149, in join\r\n    res = self._popen.wait(timeout)\r\n  File \"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py\", line 43, in wait\r\n    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\r\n  File \"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py\", line 27, in poll\r\n    pid, sts = os.waitpid(self.pid, flag)\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py\", line 351, in signal_handler\r\n    raise KeyboardInterrupt(\"MQLLMEngine terminated\")\r\nKeyboardInterrupt: MQLLMEngine terminated\r\n````\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "open",
    "created_at": "2024-12-25T07:09:45+00:00",
    "closed_at": null,
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11484/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11484"
  },
  {
    "number": 16363,
    "title": "[Feature]: Disable unicode characters in structured decoding",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently, the xgrammar backend will often return lots of messy unicode characters that are hard to parse and deal with. It requires a lot of custom code to parse these out (with best efforts, as some are not even valid). \n\n\n\n### Alternatives\n\nOpening this issue in the `xgrammar` repo, or create a custom unicode parser.\n\n### Additional context\n\nFrom the [documentation](https://xgrammar.mlc.ai/docs/api/python/index.html#xgrammar.VocabType) it appears that this issue only arises for certain tokenizer types. It would be nice if it offered consistent behavior across models.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "structured-output"
    ],
    "state": "open",
    "created_at": "2025-04-09T21:58:07+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16363/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16363"
  },
  {
    "number": 9462,
    "title": "[Bug]: Error with structured output inference after upgrade 0.6.2->0.6.3",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nollecting environment information...\r\n/opt/conda/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\r\nNo module named 'vllm._version'\r\n  from vllm.version import __version__ as VLLM_VERSION\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:53:32) [GCC 12.3.0] (64-bit runtime)\r\nPython platform: Linux-6.1.109-118.189.amzn2023.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A10G\r\nNvidia driver version: 560.35.03\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               8\r\nOn-line CPU(s) list:                  0-7\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD EPYC 7R32\r\nCPU family:                           23\r\nModel:                                49\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   4\r\nSocket(s):                            1\r\nStepping:                             0\r\nBogoMIPS:                             5599.99\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save rdpid\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            128 KiB (4 instances)\r\nL1i cache:                            128 KiB (4 instances)\r\nL2 cache:                             2 MiB (4 instances)\r\nL3 cache:                             16 MiB (1 instance)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-7\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec rstack overflow:   Mitigation; safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.99\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==25.1.2\r\n[pip3] torch==2.4.0\r\n[pip3] torchaudio==2.2.1+cu121\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.2\r\n[pip3] triton==3.0.0\r\n[conda] nomkl                     1.0                  h5ca1d4c_0    conda-forge\r\n[conda] numpy                     1.26.4          py311h64a7726_0    conda-forge\r\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.4.99                  pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\r\n[conda] pyzmq                     25.1.2          py311h34ded2d_0    conda-forge\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchaudio                2.2.1+cu121              pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.45.2                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A (dev)\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-7     0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nAfter upgrading from version 0.6.2 to 0.6.3 I started getting a validation error while generating structured input.\r\n\r\nTo reproduce:\r\n1. vllm serve NousResearch/Meta-Llama-3-8B-Instruct --dtype auto\r\n2. Execute the following code. In my case, I do it from a Jupyter Notebook:\r\n\r\n````{python}\r\n#### OUTPUT DEFINITION\r\n\r\nfrom pydantic import BaseModel, Field\r\nfrom enum import Enum\r\nfrom typing import List\r\nfrom typing import Optional\r\nimport json\r\nfrom openai import OpenAI\r\n\r\nclass BedType(Enum):\r\n    Twin = \"Twin\"\r\n    Double = \"Double\"\r\n    Queen = \"Queen\"\r\n    King = \"King\"\r\n    \r\nclass RoomBeds(BaseModel):\r\n    bed_type: BedType = Field(...,description=\"Type of the bed in the hotel room\")\r\n    quantity: int = Field(...,description=\"Number of beds of the given bed type within the hotel room\")\r\n\r\nclass HotelRoom(BaseModel):\r\n    \"\"\"\r\n    Represents a hotel room.\r\n    \"\"\"\r\n    room_id: str = Field(...,description=\"Id of the room from the input\")\r\n    room_name: Optional[str] = Field(...,description=\"Freetext name of the hotel room\")\r\n    room_class: Optional[str] = Field(..., description=\"Room class of the hotel room.\")\r\n    bed_types: Optional[List[RoomBeds]] = Field(..., description=\"List of beds within the hotel room.\")\r\n    smoking_allowed: Optional[bool] = Field(..., description=\"Flag that indicates whether smoking is allowed or not in the hotel room. Unknown value used if it cannot be infered from the room description\")\r\n\r\n\r\nclass Hotel(BaseModel):\r\n    \"\"\"\r\n    Represents an entry about a hotel.\r\n    \"\"\"\r\n    hotel_rooms: List[HotelRoom] = Field(..., description=\"List of hotel rooms within a hotel\")\r\n\r\n#### ONLINE INFERENCE\r\n\r\nclient = OpenAI(\r\n        base_url=\"http://localhost:8000/v1\",\r\n        api_key=\"token-abc123\",\r\n    )\r\n\r\n\r\ncompletion = client.beta.chat.completions.parse(\r\n            seed=42,\r\n            model= \"NousResearch/Meta-Llama-3-8B-Instruct\",\r\n            messages=[\r\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\r\n                {\"role\": \"user\", \"content\": \"Generate synthetic data for a fictitious hotel.\" },\r\n            ],\r\n            temperature=0.8,\r\n            top_p=0.95,\r\n            response_format=Hotel\r\n            )\r\n\r\n````\r\n\r\nWith version 0.6.2 I was always getting a structured output with the specified format. However, after upgrading to 0.6.3 I get a validation error as it seems the response does not match the expected format:\r\n\r\n```ValidationError                           Traceback (most recent call last)\r\nCell In[10], line 1\r\n----> 1 completion = client.beta.chat.completions.parse(\r\n      2             seed=42,\r\n      3             model= \"NousResearch/Meta-Llama-3-8B-Instruct\", # \"NousResearch/Meta-Llama-3-8B-Instruct\", #Hermes-2-Pro-Llama-3-8B-GGUF\r\n      4             messages=[\r\n      5                 {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\r\n      6                 {\"role\": \"user\", \"content\": \"Generate synthetic data for a fictitious hotel.\" },\r\n      7             ],\r\n      8             temperature=0.8,\r\n      9             top_p=0.95,\r\n     10             response_format=Hotel\r\n     11             )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/resources/beta/chat/completions.py:150, in Completions.parse(self, messages, model, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\r\n    143 def parser(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\r\n    144     return _parse_chat_completion(\r\n    145         response_format=response_format,\r\n    146         chat_completion=raw_completion,\r\n    147         input_tools=tools,\r\n    148     )\r\n--> 150 return self._post(\r\n    151     \"/chat/completions\",\r\n    152     body=maybe_transform(\r\n    153         {\r\n    154             \"messages\": messages,\r\n    155             \"model\": model,\r\n    156             \"frequency_penalty\": frequency_penalty,\r\n    157             \"function_call\": function_call,\r\n    158             \"functions\": functions,\r\n    159             \"logit_bias\": logit_bias,\r\n    160             \"logprobs\": logprobs,\r\n    161             \"max_completion_tokens\": max_completion_tokens,\r\n    162             \"max_tokens\": max_tokens,\r\n    163             \"metadata\": metadata,\r\n    164             \"n\": n,\r\n    165             \"parallel_tool_calls\": parallel_tool_calls,\r\n    166             \"presence_penalty\": presence_penalty,\r\n    167             \"response_format\": _type_to_response_format(response_format),\r\n    168             \"seed\": seed,\r\n    169             \"service_tier\": service_tier,\r\n    170             \"stop\": stop,\r\n    171             \"store\": store,\r\n    172             \"stream\": False,\r\n    173             \"stream_options\": stream_options,\r\n    174             \"temperature\": temperature,\r\n    175             \"tool_choice\": tool_choice,\r\n    176             \"tools\": tools,\r\n    177             \"top_logprobs\": top_logprobs,\r\n    178             \"top_p\": top_p,\r\n    179             \"user\": user,\r\n    180         },\r\n    181         completion_create_params.CompletionCreateParams,\r\n    182     ),\r\n    183     options=make_request_options(\r\n    184         extra_headers=extra_headers,\r\n    185         extra_query=extra_query,\r\n    186         extra_body=extra_body,\r\n    187         timeout=timeout,\r\n    188         post_parser=parser,\r\n    189     ),\r\n    190     # we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\r\n    191     # in the `parser` function above\r\n    192     cast_to=cast(Type[ParsedChatCompletion[ResponseFormatT]], ChatCompletion),\r\n    193     stream=False,\r\n    194 )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/_base_client.py:1277, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)\r\n   1263 def post(\r\n   1264     self,\r\n   1265     path: str,\r\n   (...)\r\n   1272     stream_cls: type[_StreamT] | None = None,\r\n   1273 ) -> ResponseT | _StreamT:\r\n   1274     opts = FinalRequestOptions.construct(\r\n   1275         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\r\n   1276     )\r\n-> 1277     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/_base_client.py:954, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\r\n    951 else:\r\n    952     retries_taken = 0\r\n--> 954 return self._request(\r\n    955     cast_to=cast_to,\r\n    956     options=options,\r\n    957     stream=stream,\r\n    958     stream_cls=stream_cls,\r\n    959     retries_taken=retries_taken,\r\n    960 )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/_base_client.py:1060, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\r\n   1057     log.debug(\"Re-raising status error\")\r\n   1058     raise self._make_status_error_from_response(err.response) from None\r\n-> 1060 return self._process_response(\r\n   1061     cast_to=cast_to,\r\n   1062     options=options,\r\n   1063     response=response,\r\n   1064     stream=stream,\r\n   1065     stream_cls=stream_cls,\r\n   1066     retries_taken=retries_taken,\r\n   1067 )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/_base_client.py:1159, in SyncAPIClient._process_response(self, cast_to, options, response, stream, stream_cls, retries_taken)\r\n   1156 if bool(response.request.headers.get(RAW_RESPONSE_HEADER)):\r\n   1157     return cast(ResponseT, api_response)\r\n-> 1159 return api_response.parse()\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/_response.py:319, in APIResponse.parse(self, to)\r\n    317 parsed = self._parse(to=to)\r\n    318 if is_given(self._options.post_parser):\r\n--> 319     parsed = self._options.post_parser(parsed)\r\n    321 if isinstance(parsed, BaseModel):\r\n    322     add_request_id(parsed, self.request_id)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/resources/beta/chat/completions.py:144, in Completions.parse.<locals>.parser(raw_completion)\r\n    143 def parser(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\r\n--> 144     return _parse_chat_completion(\r\n    145         response_format=response_format,\r\n    146         chat_completion=raw_completion,\r\n    147         input_tools=tools,\r\n    148     )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/lib/_parsing/_completions.py:110, in parse_chat_completion(response_format, input_tools, chat_completion)\r\n    100             else:\r\n    101                 tool_calls.append(tool_call)\r\n    103     choices.append(\r\n    104         construct_type_unchecked(\r\n    105             type_=cast(Any, ParsedChoice)[solve_response_format_t(response_format)],\r\n    106             value={\r\n    107                 **choice.to_dict(),\r\n    108                 \"message\": {\r\n    109                     **message.to_dict(),\r\n--> 110                     \"parsed\": maybe_parse_content(\r\n    111                         response_format=response_format,\r\n    112                         message=message,\r\n    113                     ),\r\n    114                     \"tool_calls\": tool_calls,\r\n    115                 },\r\n    116             },\r\n    117         )\r\n    118     )\r\n    120 return cast(\r\n    121     ParsedChatCompletion[ResponseFormatT],\r\n    122     construct_type_unchecked(\r\n   (...)\r\n    128     ),\r\n    129 )\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/lib/_parsing/_completions.py:161, in maybe_parse_content(response_format, message)\r\n    155 def maybe_parse_content(\r\n    156     *,\r\n    157     response_format: type[ResponseFormatT] | ResponseFormatParam | NotGiven,\r\n    158     message: ChatCompletionMessage | ParsedChatCompletionMessage[object],\r\n    159 ) -> ResponseFormatT | None:\r\n    160     if has_rich_response_format(response_format) and message.content is not None and not message.refusal:\r\n--> 161         return _parse_content(response_format, message.content)\r\n    163     return None\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/lib/_parsing/_completions.py:221, in _parse_content(response_format, content)\r\n    219 def _parse_content(response_format: type[ResponseFormatT], content: str) -> ResponseFormatT:\r\n    220     if is_basemodel_type(response_format):\r\n--> 221         return cast(ResponseFormatT, model_parse_json(response_format, content))\r\n    223     if is_dataclass_like_type(response_format):\r\n    224         if not PYDANTIC_V2:\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/openai/_compat.py:166, in model_parse_json(model, data)\r\n    164 def model_parse_json(model: type[_ModelT], data: str | bytes) -> _ModelT:\r\n    165     if PYDANTIC_V2:\r\n--> 166         return model.model_validate_json(data)\r\n    167     return model.parse_raw(data)\r\n\r\nFile /opt/conda/lib/python3.11/site-packages/pydantic/main.py:625, in BaseModel.model_validate_json(cls, json_data, strict, context)\r\n    623 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\r\n    624 __tracebackhide__ = True\r\n--> 625 return cls.__pydantic_validator__.validate_json(json_data, strict=strict, context=context)\r\n\r\nValidationError: 1 validation error for Hotel\r\n  Invalid JSON: expected ident at line 1 column 2 [type=json_invalid, input_value='I\\'d be happy to help ge... requests or questions.', input_type=str]\r\n    For further information visit https://errors.pydantic.dev/2.9/v/json_invalid```\r\n\r\n\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-10-17T12:35:39+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9462/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9462"
  },
  {
    "number": 9893,
    "title": "[Feature]: Support guided decoding with multistep decoding",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nSee https://github.com/vllm-project/vllm/issues/8985. It would be great if we could get the speedup from multi-step decoding without having to disallow users from using guided decoding.\r\n\r\nI have no idea how feasible that is to do, but if anybody has a sketch of how it would be done I could be up for learning and helping to implement. I'm mostly opening this issue so it's documented and I can link it from the feature compatibility matrix in the docs.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "structured-output"
    ],
    "state": "open",
    "created_at": "2024-10-31T22:29:47+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9893/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/9893"
  },
  {
    "number": 8313,
    "title": "[Performance]: guided generation is very slow in offline mode",
    "body": "### Proposal to improve performance\r\n\r\nWith a single request / online mode I'm getting:\r\n\r\n- no guided 300 tok/sec\r\n- `outlines` 150 tok/sec (2x slower)\r\n- `lm-format-enforcer` 90 tok/sec (~3x slower)\r\n\r\nwith offline mode I get:\r\n- `outlines` **is about 10-20x slower than no guided generation**\r\n- `lm-format-enforcer` is about 4x faster than `outlines` (note that it is slower than `outlines` for online)\r\n\r\nfor online I was using this schema:\r\n\r\n```\r\njson_template = {\r\n    \"type\": \"object\",\r\n    \"properties\": {\r\n        \"criteria\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"minItems\": 1},\r\n        \"response\": { \"type\": \"string\" }\r\n    },\r\n    \"required\": [\"criteria\", \"response\"]\r\n}\r\n```\r\n\r\nfor offline I was using an even simpler schema:\r\n```\r\n\r\n{\r\n   \"type\":\"object\",\r\n   \"properties\":{\r\n      \"name\":{\r\n         \"type\":\"string\", \"minLength\":2, \"maxLength\":5\r\n      },\r\n      \"age\":{\r\n         \"type\":\"integer\"\r\n      }\r\n   },\r\n   \"required\":[ \"name\", \"age\"]\r\n}\r\n```\r\nthe huge performance hit in the offline mode is very strange for both backends.\r\n\r\n2x slow down in the online mode is pretty bad too as it's already a huge impact. The offline mode can actually tolerate 2x no problem as there is no human in the loop, but 10-20x is a way impractical.\r\n\r\n`vllm=0.6.0` and `outlines==0.0.46`\r\n",
    "labels": [
      "performance",
      "structured-output",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-09-10T02:21:33+00:00",
    "closed_at": null,
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8313/reactions",
      "total_count": 9,
      "+1": 9,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8313"
  },
  {
    "number": 11312,
    "title": "[Bug]: Chat with n>1 breaks xgrammar",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n\r\n$ python collect_env.py\r\n/workspace/my-vllm/lib64/python3.12/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\r\n  warnings.warn(\r\nCollecting environment information...\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Red Hat Enterprise Linux 9.5 (Plow) (x86_64)\r\nGCC version: (GCC) 11.5.0 20240719 (Red Hat 11.5.0-2)\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.34\r\n\r\nPython version: 3.12.5 (main, Sep 11 2024, 00:00:00) [GCC 11.5.0 20240719 (Red Hat 11.5.0-2)] (64-bit runtime)\r\nPython platform: Linux-5.14.0-284.88.1.el9_2.x86_64-x86_64-with-glibc2.34\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nGPU 1: NVIDIA A100-SXM4-80GB\r\nGPU 2: NVIDIA A100-SXM4-80GB\r\nGPU 3: NVIDIA A100-SXM4-80GB\r\n\r\nNvidia driver version: 535.104.12\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        46 bits physical, 57 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               80\r\nOn-line CPU(s) list:                  0-79\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel Xeon Processor (Icelake)\r\nCPU family:                           6\r\nModel:                                134\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   20\r\nSocket(s):                            2\r\nStepping:                             0\r\nBogoMIPS:                             5600.02\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd arat avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm md_clear arch_capabilities\r\nVirtualization:                       VT-x\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            2.5 MiB (80 instances)\r\nL1i cache:                            2.5 MiB (80 instances)\r\nL2 cache:                             160 MiB (40 instances)\r\nL3 cache:                             32 MiB (2 instances)\r\nNUMA node(s):                         2\r\nNUMA node0 CPU(s):                    0-39\r\nNUMA node1 CPU(s):                    40-79\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Reg file data sampling: Vulnerable: No microcode\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.6+cu124torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.46.3\r\n[pip3] triton==3.1.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.5\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV12\tNV12\tNV12\tPIX\t0-39\t0\t\tN/A\r\nGPU1\tNV12\t X \tNV12\tNV12\tPIX\t0-39\t0\t\tN/A\r\nGPU2\tNV12\tNV12\t X \tNV12\tNODE\t0-39\t0\t\tN/A\r\nGPU3\tNV12\tNV12\tNV12\t X \tSYS\t40-79\t1\t\tN/A\r\nNIC0\tPIX\tPIX\tNODE\tSYS\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n\r\nNVIDIA_VISIBLE_DEVICES=GPU-d02eacbf-0d93-7141-2f45-650de9016f82,GPU-6169d05e-1d51-dfee-bbe5-1fe42096e35b,GPU-1dd95362-3e2e-8afc-f4c8-5d28663c3c73,GPU-7be30919-545a-54b7-a882-b565d1c0a133\r\nVLLM_ALLOW_LONG_MAX_MODEL_LEN=1\r\nVLLM_CACHE_ROOT=/tmp\r\nVLLM_CONFIG_ROOT=/tmp\r\nVLLM_WORKER_MULTIPROC_METHOD=fork\r\nVLLM_USAGE_SOURCE=production-docker-image\r\nCUDA_VISIBLE_DEVICES=0,1,2,3\r\nCUDA_VISIBLE_DEVICES=0,1,2,3\r\nLD_LIBRARY_PATH=/workspace/my-vllm/lib/python3.12/site-packages/cv2/../../lib64:/opt/vllm/lib/python3.12/site-packages/nvidia/nvtx/lib:/opt/vllm/lib/python3.12/site-packages/nvidia/cuda_runtime/lib:/opt/vllm/lib/python3.12/site-packages/nvidia/cuda_nvrtc/lib:\r\nVLLM_NO_USAGE_STATS=1\r\nCUDA_MODULE_LOADING=LAZY\r\n\r\n</details>\r\n\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nOn v0.6.5 making a tools call with n>2 will break guided decoding with the xgrammar guided decoding backend. \r\n\r\nBooting the server with:\r\n```\r\n$ vllm serve mistralai/Mistral-7B-Instruct-v0.3 --tool-call-parser mistral --enable-auto-tool-choice --compilation-config 3 --chat-template examples/tool_chat_template_mistral_parallel.jinja\r\n```\r\n\r\nAnd then sending this request:\r\n```\r\ncurl -X 'POST' \\\r\n  'http://localhost:8000/v1/chat/completions' \\\r\n  -H 'accept: application/json' \\\r\n  -H 'Content-Type: application/json' \\\r\n  -d '{\r\n  \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\",\r\n  \"messages\": [\r\n    {\r\n      \"content\": \"What is the temperature in SF?\",\r\n      \"role\": \"user\"\r\n    }\r\n  ],\r\n  \"tool_choice\": {\r\n    \"type\": \"function\",\r\n    \"function\": {\r\n        \"name\": \"get_current_weather\"\r\n    }\r\n  },\r\n  \"tools\": [{\r\n    \"type\": \"function\",\r\n    \"function\": {\r\n        \"name\": \"get_current_weather\",\r\n        \"description\": \"Get the current weather in a given location\",\r\n        \"parameters\": {\r\n            \"type\": \"object\",\r\n            \"properties\": {\r\n                \"city\": {\r\n                    \"type\":\r\n                    \"string\",\r\n                    \"description\":\r\n                    \"The city to find the weather for, e.g. '\\''San Francisco'\\''\"\r\n                },\r\n                \"state\": {\r\n                    \"type\":\r\n                    \"string\",\r\n                    \"description\":\r\n                    \"the two-letter abbreviation for the state that the city is in, e.g. '\\''CA'\\'' which would mean '\\''California'\\''\"\r\n                },\r\n                \"unit\": {\r\n                    \"type\": \"string\",\r\n                    \"description\": \"The unit to fetch the temperature in\",\r\n                    \"enum\": [\"celsius\", \"fahrenheit\"]\r\n                }\r\n            },\r\n            \"required\": [\"city\", \"state\", \"unit\"]\r\n        }\r\n    }\r\n  }],\r\n  \"repetition_penalty\": 1.0,\r\n  \"top_k\": -1,\r\n  \"n\": 2\r\n}'\r\n```\r\n\r\nwill result in a 500 with this stack trace:\r\n\r\n```\r\nINFO:     ::1:55612 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\r\nERROR 12-18 22:57:54 engine.py:135] IndexError('Error in model execution (input dumped to /tmp/err_execute_model_input_20241218-225754.pkl): tuple index out of range')\r\nERROR 12-18 22:57:54 engine.py:135] Traceback (most recent call last):\r\nERROR 12-18 22:57:54 engine.py:135]   File \"/workspace/my-vllm/lib64/python3.12/site-packages/vllm/worker/model_runner_base.py\", line 116, in _wrapper\r\nERROR 12-18 22:57:54 engine.py:135]     return func(*args, **kwargs)\r\nERROR 12-18 22:57:54 engine.py:135]            ^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-18 22:57:54 engine.py:135]   File \"/workspace/my-vllm/lib64/python3.12/site-packages/vllm/worker/model_runner.py\", line 1729, in execute_model\r\nERROR 12-18 22:57:54 engine.py:135]     logits = self.model.compute_logits(hidden_or_intermediate_states,\r\nERROR 12-18 22:57:54 engine.py:135]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-18 22:57:54 engine.py:135]   File \"/workspace/my-vllm/lib64/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 578, in compute_logits\r\nERROR 12-18 22:57:54 engine.py:135]     logits = self.logits_processor(self.lm_head, hidden_states,\r\nERROR 12-18 22:57:54 engine.py:135]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-18 22:57:54 engine.py:135]   File \"/workspace/my-vllm/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\nERROR 12-18 22:57:54 engine.py:135]     return self._call_impl(*args, **kwargs)\r\nERROR 12-18 22:57:54 engine.py:135]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-18 22:57:54 engine.py:135]   File \"/workspace/my-vllm/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\nERROR 12-18 22:57:54 engine.py:135]     return forward_call(*args, **kwargs)\r\nERROR 12-18 22:57:54 engine.py:135]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-18 22:57:54 engine.py:135]   File \"/workspace/my-vllm/lib64/python3.12/site-packages/vllm/model_executor/layers/logits_processor.py\", line 77, in forward\r\nERROR 12-18 22:57:54 engine.py:135]     logits = _apply_logits_processors(logits, sampling_metadata)\r\nERROR 12-18 22:57:54 engine.py:135]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-18 22:57:54 engine.py:135]   File \"/workspace/my-vllm/lib64/python3.12/site-packages/vllm/model_executor/layers/logits_processor.py\", line 153, in _apply_logits_processors\r\nERROR 12-18 22:57:54 engine.py:135]     logits_row = logits_processor(past_tokens_ids,\r\nERROR 12-18 22:57:54 engine.py:135]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-18 22:57:54 engine.py:135]   File \"/workspace/my-vllm/lib64/python3.12/site-packages/vllm/model_executor/guided_decoding/xgrammar_decoding.py\", line 258, in __call__\r\nERROR 12-18 22:57:54 engine.py:135]     sampled_token = input_ids[-1]\r\nERROR 12-18 22:57:54 engine.py:135]                     ~~~~~~~~~^^^^\r\nERROR 12-18 22:57:54 engine.py:135] IndexError: tuple index out of range\r\nERROR 12-18 22:57:54 engine.py:135] \r\nERROR 12-18 22:57:54 engine.py:135] The above exception was the direct cause of the following exception:\r\nERROR 12-18 22:57:54 engine.py:135] \r\nERROR 12-18 22:57:54 engine.py:135] Traceback (most recent call last):\r\nERROR 12-18 22:57:54 engine.py:135]   File \"/workspace/my-vllm/lib64/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 133, in start\r\nERROR 12-18 22:57:54 engine.py:135]     self.run_engine_loop()\r\nERROR 12-18 22:57:54 engine.py:135]   File \"/workspace/my-vllm/lib64/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 196, in run_engine_loop\r\nERROR 12-18 22:57:54 engine.py:135]     request_outputs = self.engine_step()\r\nERROR 12-18 22:57:54 engine.py:135]                       ^^^^^^^^^^^^^^^^^^\r\nERROR 12-18 22:57:54 engine.py:135]   File \"/workspace/my-vllm/lib64/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 214, in engine_step\r\nERROR 12-18 22:57:54 engine.py:135]     raise e\r\nERROR 12-18 22:57:54 engine.py:135]   File \"/workspace/my-vllm/lib64/python3.12/site-packages/vllm/engine/multiprocessing/engine.py\", line 205, in engine_step\r\nERROR 12-18 22:57:54 engine.py:135]     return self.engine.step()\r\nERROR 12-18 22:57:54 engine.py:135]            ^^^^^^^^^^^^^^^^^^\r\nERROR 12-18 22:57:54 engine.py:135]   File \"/workspace/my-vllm/lib64/python3.12/site-packages/vllm/engine/llm_engine.py\", line 1405, in step\r\nERROR 12-18 22:57:54 engine.py:135]     outputs = self.model_executor.execute_model(\r\nERROR 12-18 22:57:54 engine.py:135]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-18 22:57:54 engine.py:135]   File \"/workspace/my-vllm/lib64/python3.12/site-packages/vllm/executor/gpu_executor.py\", line 88, in execute_model\r\nERROR 12-18 22:57:54 engine.py:135]     output = self.driver_worker.execute_model(execute_model_req)\r\nERROR 12-18 22:57:54 engine.py:135]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-18 22:57:54 engine.py:135]   File \"/workspace/my-vllm/lib64/python3.12/site-packages/vllm/worker/worker_base.py\", line 343, in execute_model\r\nERROR 12-18 22:57:54 engine.py:135]     output = self.model_runner.execute_model(\r\nERROR 12-18 22:57:54 engine.py:135]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-18 22:57:54 engine.py:135]   File \"/workspace/my-vllm/lib64/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nERROR 12-18 22:57:54 engine.py:135]     return func(*args, **kwargs)\r\nERROR 12-18 22:57:54 engine.py:135]            ^^^^^^^^^^^^^^^^^^^^^\r\nERROR 12-18 22:57:54 engine.py:135]   File \"/workspace/my-vllm/lib64/python3.12/site-packages/vllm/worker/model_runner_base.py\", line 152, in _wrapper\r\nERROR 12-18 22:57:54 engine.py:135]     raise type(err)(\r\nERROR 12-18 22:57:54 engine.py:135] IndexError: Error in model execution (input dumped to /tmp/err_execute_model_input_20241218-225754.pkl): tuple index out of range\r\n```\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "closed",
    "created_at": "2024-12-18T23:08:00+00:00",
    "closed_at": "2025-01-22T21:27:54+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11312/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11312"
  },
  {
    "number": 14151,
    "title": "[Bug]: Structured output requests can hang the server",
    "body": "### Your current environment\n\nThis isn't version specific, the use of a `ThreadPoolExecutor` to build grammars for structured output has been around since the original `outlines` integration\n\n\n### \ud83d\udc1b Describe the bug\n\nTo build structured output (guided decoding) processors in vLLM, we currently either:\n- Execute the non-async grammar creation right in the event loop, or\n- Use a ThreadPoolExectuor to run the grammar creation in a separate thread\n\nHowever, there are cases where a user may pass in a json schema for structured output that will cause grammar compilation to take a really long time. One such case reported with outlines is here:  https://github.com/dottxt-ai/outlines-core/issues/180, and we've had many reports from products with >1k line json schemas input as guided decoding parameters that exhibit this behavior.\n\nThe problem is that we don't have a way to cancel the construction of these grammars when the api request times out or is cancelled. Specifically when using the threadpool, the task that is waiting on the future from the pool will correctly cancel when the client disconnects but the thread that's doing the work in the pool will continue spinning. This causes a situation where we have 100% cpu usage for hours at a time while the server continues to report healthy. There is too much cpu contention to actually process new requests and hand them off to the engine though, so all requests to the server appear to hang.\n\nSlack thread on this here regarding the structured output work for V1: https://vllm-dev.slack.com/archives/C07QQ8DAXMK/p1741024399466749\n\nThis might be worth fixing in V0 as well, depending on how fast we can actually get V1 out\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "closed",
    "created_at": "2025-03-03T18:16:57+00:00",
    "closed_at": "2025-03-20T04:33:53+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14151/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14151"
  },
  {
    "number": 11153,
    "title": "[Usage]: Support for Specifying ```extra_body``` Parameters in vLLM Terminal Commands for structuring the JSON output ",
    "body": "\r\nHi there,\r\n\r\nI understand that vLLM currently supports [outlines-dev/outlines](https://github.com/outlines-dev/outlines), [mlc-ai/xgrammar](https://github.com/mlc-ai/xgrammar), and [noamgat/lm-format-enforcer](https://github.com/noamgat/lm-format-enforcer) for guided decoding. I would like to directly configure guided decoding via the terminal using the following command:\r\n\r\n\r\n`vllm serve meta-llama/Llama-3.1-8B-Instruct --device neuron --tensor-parallel-size 2 --block-size 8 --max-model-len 4096 --max-num-seqs 32 --guided-decoding-backend lm-format-enforcer\r\n`\r\n\r\n\r\nThe challenge I\u2019m facing is how to specify ```extra_body``` parameters such as:\r\n\r\n`extra_body={\"guided_regex\": \"\\w+@\\w+\\.com\\n\", \"stop\": [\"\\n\"]}\r\n`\r\n\r\ndirectly from the terminal, so I don\u2019t need to modify any other code. Is there a way to pass these parameters via the CLI, or do I need to rely exclusively on the Python API for such configurations?\r\n\r\nAny advice or pointers on this would be greatly appreciated!\r\n\r\n### How would you like to use vllm\r\n\r\n_No response_\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "structured-output",
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-12T23:55:33+00:00",
    "closed_at": "2025-05-18T02:13:52+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11153/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11153"
  },
  {
    "number": 17817,
    "title": "[RFC]: Unification of frontend parser",
    "body": "## motivation\n\nhttps://github.com/vllm-project/vllm/issues/11522 (with draft implementation at https://github.com/vllm-project/vllm/pull/11554)\naims to simplify the logics of the tool parser interface. However, this doesn't cover the cases for reasoning models (where we want to parse\ntokens generated within the thinking budgets, etc. Our current solutions involves a reasoning parser, which will soon be running into the same\nissue mentioned in #11522 when dealing with very long thinking budget). Additionally, the current implementations of tool calling are relatively\nfragile, and not scalable when adding more tool format.\n\nThis RFC aims to build on top of some similar ideas from the RFC and unify both tool calling and reasoning parser logic for a more robust\nway for us to move forward, especially with v0.10.x.\n\n## proposed change\n\n\nThe workflow can be seen as follows:\n\n- function/tool calling format for supported models (defined by the LLMEngine)\n- Construct structural tags <- said tool/function calling format\n- perform constrained decoding with supported backend (xgrammar/guidance)\n- parser to convert string response -> structured objects\n\nFrom vLLM perspective:\n\n```bash\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Prompt \u2502\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 vLLM\u202f(OpenAI\u2011compatible FE)    \u2502\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502 [tool / func\u2011call \u2502 reasoning_fmt]\n    \u25bc                   \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n\u2502  Parser  \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 LLM Engine \u2502\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Parser \u2502\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 vLLM\u202f(OpenAI\u2011compatible FE)\u2502\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502 \n    \u25bc \n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Output \u2502  \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n```\n\nAim:\n\n- Simplified and unified interface called `vllm.Parser`\n\nThere are a few compatibility matrix we need to consider:\n\n| features              | function/tool caling | structured outputs | reasoning |\n| --------------------- | -------------------- | ------------------ | --------- |\n| function/tool calling | -                    |                    |           |\n| structured outputs    |                      | -                  |           |\n| reasoning             |                      |                    | -         |\n\n_NOTE_: For reasoning logics, there are forced/non-forced mode (which is recently introduced by Qwen3-series of models)\n\nA ad-hoc implementation of the parser would be\n\n```python\nclass Parser:\n  tool: bool = False\n  reasoning: bool = False\n\n  def parse_tool_call(self, structural_tag: StructuralTagResult) -> ToolCallResult: ...\n\n  def parse_tool_call_stream(self, structural_tag: StructuralTagResult) -> DeltaToolCallResult: ...\n\n  def parse_reasoning(self, structural_tag: StructuralTagResult) -> ReasoningResult: ...\n\n  def parse_reasoning_stream(self, structural_tag: StructuralTagResult) -> DeltaReasoningResult: ...\n\nclass Llama3JSON(Parser, tool=True, name=\"llama3-json\"): ...\nclass Pythonic(Parser, tool=True, name=\"pythonic\"): ...\n\nclass DeepSeek(Parser, tool=True, reasoning=True, name=\"deepseek_r1\"): ...\n```\n\n`serving_chat.py`:\n\n```python\n\n```\n\n## Feedback period\n\ntbd. wrt implementations, We will need to wait from the xgrammar team to have this support\n\n## CC List\n\n@mgoin @russellb @robertgshaw2-redhat @mmoskal \n\n## Any Other Thing\n\n- We should probably move all of the tool/chat templates under `vllm/tools`\n",
    "labels": [
      "structured-output",
      "RFC",
      "tool-calling"
    ],
    "state": "open",
    "created_at": "2025-05-07T21:46:18+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17817/reactions",
      "total_count": 4,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 3,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/17817"
  },
  {
    "number": 15762,
    "title": "[Bug]: xgrammar doesn't support enums, but vllm isn't falling back to outlines",
    "body": "### Your current environment\n\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.5 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: version 3.25.2\nLibc version: glibc-2.31\n\nPython version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-113-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n\nNvidia driver version: 555.42.06\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      43 bits physical, 48 bits virtual\nCPU(s):                             64\nOn-line CPU(s) list:                0-63\nThread(s) per core:                 2\nCore(s) per socket:                 16\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          AuthenticAMD\nCPU family:                         23\nModel:                              49\nModel name:                         AMD EPYC 7282 16-Core Processor\nStepping:                           0\nFrequency boost:                    enabled\nCPU MHz:                            1500.000\nCPU max MHz:                        2800.0000\nCPU min MHz:                        1500.0000\nBogoMIPS:                           5600.05\nVirtualization:                     AMD-V\nL1d cache:                          1 MiB\nL1i cache:                          1 MiB\nL2 cache:                           16 MiB\nL3 cache:                           128 MiB\nNUMA node0 CPU(s):                  0-15,32-47\nNUMA node1 CPU(s):                  16-31,48-63\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection\nVulnerability Spec rstack overflow: Mitigation; safe RET\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\n\nVersions of relevant libraries:\n[pip3] flake8==7.1.2\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.50.0\n[pip3] triton==3.2.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.3.0                   pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] transformers              4.50.0                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\nGPU1    NODE     X      NODE    NODE    SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\nGPU2    NODE    NODE     X      NODE    SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\nGPU3    NODE    NODE    NODE     X      SYS     SYS     SYS     SYS     0-15,32-47      0               N/A\nGPU4    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    16-31,48-63     1               N/A\nGPU5    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    16-31,48-63     1               N/A\nGPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    16-31,48-63     1               N/A\nGPU7    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      16-31,48-63     1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nCUDA_VISIBLE_DEVICES=7\nCUDA_VISIBLE_DEVICES=7\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n### \ud83d\udc1b Describe the bug\n\nTrying to run the pydantic example from the documentation (https://docs.vllm.ai/en/latest/features/structured_outputs.html#offline-inference) produces this error:\n\n`openai.BadRequestError: Error code: 400 - {'object': 'error', 'message': 'The provided JSON schema contains features not supported by xgrammar.', 'type': 'BadRequestError', 'param': None, 'code': 400}`\n\nIt doesn't seem that vllm is falling back to outlines in this case.\n\nHere is the example:\n```python\nclass CarType(str, Enum):\n    sedan = \"sedan\"\n    suv = \"SUV\"\n    truck = \"Truck\"\n    coupe = \"Coupe\"\n\n\nclass CarDescription(BaseModel):\n    brand: str\n    model: str\n    car_type: CarType\n\n\njson_schema = CarDescription.model_json_schema()\n\ncompletion = client.chat.completions.create(\n    model=model,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Generate a JSON with the brand, model and car_type of the most iconic car from the 90's\",\n        }\n    ],\n    extra_body={\"guided_json\": json_schema},\n)\nprint(completion.choices[0].message.content)\n```\n\nI tried to manually set to outlines with `extra_body={\"guided_json\": json_schema, \"guided_decoding_backend\": \"outlines\"}` but I get the following error:\n\n`openai.BadRequestError: Error code: 400 - {'object': 'error', 'message': 'Request-level structured output backend must match engine-level backend. outlines != xgrammar', 'type': 'BadRequestError', 'param': None, 'code': 400}`\n\n",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "open",
    "created_at": "2025-03-30T05:44:20+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15762/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15762"
  },
  {
    "number": 7557,
    "title": "[Bug]: Guided decoding is broken because tokenizers can't be pickled",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.1+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: Could not collect\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Fedora release 39 (Thirty Nine) (x86_64)\r\nGCC version: (GCC) 13.3.1 20240522 (Red Hat 13.3.1-1)\r\nClang version: 17.0.6 (Fedora 17.0.6-2.fc39)\r\nCMake version: version 3.29.6\r\nLibc version: glibc-2.38\r\n\r\nPython version: 3.11.8 (main, Mar 27 2024, 15:03:48) [GCC 13.2.1 20231205 (Red Hat 13.2.1-6)] (64-bit runtime)\r\nPython platform: Linux-6.7.11-200.fc39.x86_64-x86_64-with-glibc2.38\r\nIs CUDA available: False\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: GPU 0: NVIDIA GeForce MX330\r\nNvidia driver version: 545.23.08\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        39 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               8\r\nOn-line CPU(s) list:                  0-7\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Core(TM) i7-10510U CPU @ 1.80GHz\r\nCPU family:                           6\r\nModel:                                142\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   4\r\nSocket(s):                            1\r\nStepping:                             12\r\nCPU(s) scaling MHz:                   69%\r\nCPU max MHz:                          4900.0000\r\nCPU min MHz:                          400.0000\r\nBogoMIPS:                             4599.93\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust sgx bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp vnmi md_clear flush_l1d arch_capabilities\r\nVirtualization:                       VT-x\r\nL1d cache:                            128 KiB (4 instances)\r\nL1i cache:                            128 KiB (4 instances)\r\nL2 cache:                             1 MiB (4 instances)\r\nL3 cache:                             8 MiB (1 instance)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-7\r\nVulnerability Gather data sampling:   Mitigation; Microcode\r\nVulnerability Itlb multihit:          KVM: Mitigation: VMX disabled\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Mitigation; Enhanced IBRS\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                  Mitigation; Microcode\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] intel_extension_for_pytorch==2.3.100\r\n[pip3] mypy==1.11.1\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] mypy-protobuf==3.5.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] pyzmq==26.1.0\r\n[pip3] torch==2.3.1+cpu\r\n[pip3] torchvision==0.18.1+cpu\r\n[pip3] transformers==4.43.4\r\n[pip3] triton==2.3.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.4@1f26efbb3a5e6dad0b98421dd697167c42a50629\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \t0-7\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\n```\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nIf I run a small model like this `python -m vllm.entrypoints.openai.api_server --model gpt2` and call it like this\r\n\r\n```\r\ncurl http://localhost:8000/v1/completions   -H \"Content-Type: application/json\"   -d '{\r\n    \"model\": \"gpt2\",\r\n    \"prompt\": [\"An example of a json document: \", \"Another example of a json document: \"],\r\n    \"max_tokens\": 100,\r\n    \"temperature\": 0,\r\n    \"guided_decoding_backend\": \"outlines\",\r\n    \"response_format\": {\"type\":\"json_object\"}\r\n  }'\r\n  ```\r\n\r\nI get the following error in the server log:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/mbayser/.pyenv/versions/3.11.8/lib/python3.11/multiprocessing/queues.py\", line 244, in _feed\r\n    obj = _ForkingPickler.dumps(obj)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mbayser/.pyenv/versions/3.11.8/lib/python3.11/multiprocessing/reduction.py\", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\nAttributeError: Can't pickle local object 'get_cached_tokenizer.<locals>.CachedTokenizer'\r\n```\r\n\r\nI've tried to disable frontend multiprocessing, but that only changes the place where the error happens:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/mbayser/.pyenv/versions/3.11.8/lib/python3.11/multiprocessing/queues.py\", line 244, in _feed\r\n    obj = _ForkingPickler.dumps(obj)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mbayser/.pyenv/versions/3.11.8/lib/python3.11/multiprocessing/reduction.py\", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\nAttributeError: Can't pickle local object 'get_cached_tokenizer.<locals>.CachedTokenizer'\r\n```\r\n\r\nIt seems that some of the refactorings to use multiprocessing have broken the guided decoding feature.",
    "labels": [
      "bug",
      "structured-output",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-08-15T14:16:17+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7557/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7557"
  },
  {
    "number": 9423,
    "title": "[Bug]: Speculative decoding breaks guided decoding.",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.22.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1063-azure-x86_64-with-glibc2.35\r\nIs CUDA available: N/A\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100 80GB PCIe\r\nGPU 1: NVIDIA A100 80GB PCIe\r\n\r\nNvidia driver version: 535.161.08\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: N/A\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             48\r\nOn-line CPU(s) list:                0-47\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 7V13 64-Core Processor\r\nCPU family:                         25\r\nModel:                              1\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 48\r\nSocket(s):                          1\r\nStepping:                           1\r\nBogoMIPS:                           4890.88\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr rdpru arat umip vaes vpclmulqdq rdpid fsrm\r\nHypervisor vendor:                  Microsoft\r\nVirtualization type:                full\r\nL1d cache:                          1.5 MiB (48 instances)\r\nL1i cache:                          1.5 MiB (48 instances)\r\nL2 cache:                           24 MiB (48 instances)\r\nL3 cache:                           192 MiB (6 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-23\r\nNUMA node1 CPU(s):                  24-47\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; safe RET, no microcode\r\nVulnerability Spec store bypass:    Vulnerable\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.21.5\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\n\u001b[4mGPU0\tGPU1\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\r\nGPU0\t X \tNV12\tNODE\t0-23\t0\t\tN/A\r\nGPU1\tNV12\t X \tSYS\t24-47\t1\t\tN/A\r\nNIC0\tNODE\tSYS\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_an0\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nI run the vLLM server with speculative decoding as follows:\r\n```.py\r\nNCCL_GRAPH_FILE=\"/home/azureuser/apps/ai-kyc_vu/vllm_server/gpu_graph.xml\" python -m vllm.entrypoints.openai.api_server\r\n--model \"neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8\"\r\n--served-model-name \"\"\r\n--guided-decoding-backend \"outlines\"\r\n--gpu-memory-utilization 0.9 \r\n--port 7999\r\n--worker_use_ray\r\n--max-model-len 40000\r\n--tensor-parallel-size 2\r\n--speculative-model=\"[ngram]\"\r\n--num_speculative_tokens 5\r\n--ngram_prompt_lookup_max=4\r\n--use-v2-block-manager\r\n```\r\n\r\nI then prompt the LLM with guided json created from the following pydantic model:\r\n\r\n```.py\r\nclass A(BaseModel):\r\n    a: int\r\n```\r\n\r\nI  incorporate the guided json into the  following testing prompt:\r\n```.json\r\n{\r\n    \"model\": \"\",\r\n    \"messages\": [\r\n      {\r\n        \"role\": \"system\",\r\n        \"content\": \"\"\r\n      },\r\n      {\r\n        \"role\": \"user\",\r\n        \"content\": \"How are you?\"\r\n      }\r\n    ],\r\n    \"guided_decoding_backend\": \"outlines\",\r\n    \"guided_json\": {\"properties\": {\"a\": {\"title\": \"A\", \"type\": \"integer\"}}, \"required\": [\"a\"], \"title\": \"A\", \"type\": \"object\"},\r\n    \"max_tokens\": 200,\r\n    \"top_k\": 1,\r\n    \"stream\": false\r\n}\r\n```\r\n\r\nThe json guidance should ensure that the output is a valid json object. However, vLLM returns the following incomplete json object:\r\n```\r\n{\"a\": 1\r\n```\r\n\r\n\r\nWhen I disable ngram-speculative decoding, the same prompt works, and returns a complete json object:\r\n```\r\n{\"a\": 1}\r\n```\r\n    \r\nThis means that somehow, ngram-speculative decoding breaks json guidance.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output"
    ],
    "state": "open",
    "created_at": "2024-10-16T13:51:18+00:00",
    "closed_at": null,
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9423/reactions",
      "total_count": 6,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9423"
  }
]
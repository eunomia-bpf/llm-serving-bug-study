[
  {
    "number": 20647,
    "title": "[Bug]: Assertion error when serving \"deepseek-ai/DeepSeek-V2-Lite\" with PP in 0.9.2",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : Could not collect\nCMake version                : Could not collect\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu128\nIs debug build               : False\nCUDA used to build PyTorch   : 12.8\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.11.11 | packaged by conda-forge | (main, Mar  3 2025, 20:43:55) [GCC 13.3.0] (64-bit runtime)\nPython platform              : Linux-6.5.0-1024-aws-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.8.93\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : \nGPU 0: NVIDIA L4\nGPU 1: NVIDIA L4\nGPU 2: NVIDIA L4\nGPU 3: NVIDIA L4\n\nNvidia driver version        : 550.163.01\ncuDNN version                : Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.8.0\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      48 bits physical, 48 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             48\nOn-line CPU(s) list:                0-47\nVendor ID:                          AuthenticAMD\nModel name:                         AMD EPYC 7R13 Processor\nCPU family:                         25\nModel:                              1\nThread(s) per core:                 2\nCore(s) per socket:                 24\nSocket(s):                          1\nStepping:                           1\nBogoMIPS:                           5299.99\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid\nHypervisor vendor:                  KVM\nVirtualization type:                full\nL1d cache:                          768 KiB (24 instances)\nL1i cache:                          768 KiB (24 instances)\nL2 cache:                           12 MiB (24 instances)\nL3 cache:                           96 MiB (3 instances)\nNUMA node(s):                       1\nNUMA node0 CPU(s):                  0-47\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Mitigation; Safe RET\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.8.3.14\n[pip3] nvidia-cuda-cupti-cu12==12.8.57\n[pip3] nvidia-cuda-nvrtc-cu12==12.8.61\n[pip3] nvidia-cuda-runtime-cu12==12.8.57\n[pip3] nvidia-cudnn-cu12==9.7.1.26\n[pip3] nvidia-cufft-cu12==11.3.3.41\n[pip3] nvidia-cufile-cu12==1.13.0.11\n[pip3] nvidia-curand-cu12==10.3.9.55\n[pip3] nvidia-cusolver-cu12==11.7.2.55\n[pip3] nvidia-cusparse-cu12==12.5.7.53\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.8.61\n[pip3] nvidia-nvtx-cu12==12.8.55\n[pip3] pyzmq==26.0.3\n[pip3] torch==2.7.0+cu128\n[pip3] torchaudio==2.7.0+cu128\n[pip3] torchvision==0.22.0+cu128\n[pip3] transformers==4.52.4\n[pip3] triton==3.3.0\n[conda] numpy                                       1.26.4           pypi_0              pypi\n[conda] nvidia-cublas-cu12                          12.8.3.14        pypi_0              pypi\n[conda] nvidia-cuda-cupti-cu12                      12.8.57          pypi_0              pypi\n[conda] nvidia-cuda-nvrtc-cu12                      12.8.61          pypi_0              pypi\n[conda] nvidia-cuda-runtime-cu12                    12.8.57          pypi_0              pypi\n[conda] nvidia-cudnn-cu12                           9.7.1.26         pypi_0              pypi\n[conda] nvidia-cufft-cu12                           11.3.3.41        pypi_0              pypi\n[conda] nvidia-cufile-cu12                          1.13.0.11        pypi_0              pypi\n[conda] nvidia-curand-cu12                          10.3.9.55        pypi_0              pypi\n[conda] nvidia-cusolver-cu12                        11.7.2.55        pypi_0              pypi\n[conda] nvidia-cusparse-cu12                        12.5.7.53        pypi_0              pypi\n[conda] nvidia-cusparselt-cu12                      0.6.3            pypi_0              pypi\n[conda] nvidia-nccl-cu12                            2.26.2           pypi_0              pypi\n[conda] nvidia-nvjitlink-cu12                       12.8.61          pypi_0              pypi\n[conda] nvidia-nvtx-cu12                            12.8.55          pypi_0              pypi\n[conda] pyzmq                                       26.0.3           pypi_0              pypi\n[conda] torch                                       2.7.0+cu128      pypi_0              pypi\n[conda] torchaudio                                  2.7.0+cu128      pypi_0              pypi\n[conda] torchvision                                 0.22.0+cu128     pypi_0              pypi\n[conda] transformers                                4.52.4           pypi_0              pypi\n[conda] triton                                      3.3.0            pypi_0              pypi\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.2\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     SYS     SYS     0-47    0               N/A\nGPU1    SYS      X      SYS     SYS     0-47    0               N/A\nGPU2    SYS     SYS      X      SYS     0-47    0               N/A\nGPU3    SYS     SYS     SYS      X      0-47    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n==============================\n     Environment Variables\n==============================\nNVIDIA_VISIBLE_DEVICES=void\nNVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\nNCCL_VERSION=2.25.1-1\nNCCL_SOCKET_IFNAME=^lo,docker,veth,tailscale,anyscale\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNVIDIA_PRODUCT_NAME=CUDA\nCUDA_VERSION=12.8.1\nLD_LIBRARY_PATH=/usr/local/ucx/lib:/usr/local/nixl/lib/x86_64-linux-gnu:/usr/local/cuda/lib64\nVLLM_USE_V1=1\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n```bash\nuv pip install \"vllm==0.9.2\"\nvllm serve deepseek-ai/DeepSeek-V2-Lite \\\n  --trust-remote-code \\\n  --max-model-len=1024 --enforce-eager \\\n  --tensor-parallel-size=2 --pipeline-parallel-size=2 # PP>1 causes error\n```\n\nexception:\n```\n(VllmWorker rank=0 pid=20455) ERROR 07-08 15:41:43 [multiproc_executor.py:487]   File \"/home/ray/anaconda3/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", l\nine 64, in initialize_model\n(VllmWorker rank=0 pid=20455) ERROR 07-08 15:41:43 [multiproc_executor.py:487]     return model_class(vllm_config=vllm_config, prefix=prefix)\n(VllmWorker rank=0 pid=20455) ERROR 07-08 15:41:43 [multiproc_executor.py:487]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=0 pid=20455) ERROR 07-08 15:41:43 [multiproc_executor.py:487]   File \"/home/ray/anaconda3/lib/python3.11/site-packages/vllm/model_executor/models/deepseek_v2.py\", l\nine 743, in __init__\n(VllmWorker rank=0 pid=20455) ERROR 07-08 15:41:43 [multiproc_executor.py:487]     assert isinstance(layer, DeepseekV2DecoderLayer)\n```\nand `type(layer)` is `<class 'vllm.model_executor.models.utils.PPMissingLayer'>`\n\nlikely caused by https://github.com/vllm-project/vllm/pull/18343\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "v1",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-07-08T22:48:28+00:00",
    "closed_at": "2025-07-10T03:34:42+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20647/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20647"
  },
  {
    "number": 17565,
    "title": "[Bug][V1] 'PixtralVisionConfig' object has no attribute 'spatial_merge_size' in 0.8.5",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.11.11 | packaged by conda-forge | (main, Mar  3 2025, 20:43:55) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-6.5.0-1024-aws-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA L4\nGPU 1: NVIDIA L4\nGPU 2: NVIDIA L4\nGPU 3: NVIDIA L4\n\nNvidia driver version: 550.163.01\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.1.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      48 bits physical, 48 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             48\nOn-line CPU(s) list:                0-47\nVendor ID:                          AuthenticAMD\nModel name:                         AMD EPYC 7R13 Processor\nCPU family:                         25\nModel:                              1\nThread(s) per core:                 2\nCore(s) per socket:                 24\nSocket(s):                          1\nStepping:                           1\nBogoMIPS:                           5299.99\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid\nHypervisor vendor:                  KVM\nVirtualization type:                full\nL1d cache:                          768 KiB (24 instances)\nL1i cache:                          768 KiB (24 instances)\nL2 cache:                           12 MiB (24 instances)\nL3 cache:                           96 MiB (3 instances)\nNUMA node(s):                       1\nNUMA node0 CPU(s):                  0-47\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Vulnerable: Safe RET, no microcode\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.0.3\n[pip3] torch==2.6.0+cu124\n[pip3] torchaudio==2.6.0+cu124\n[pip3] torchvision==0.21.0+cu124\n[pip3] transformers==4.51.3\n[pip3] triton==3.2.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.0.3                   pypi_0    pypi\n[conda] torch                     2.6.0+cu124              pypi_0    pypi\n[conda] torchaudio                2.6.0+cu124              pypi_0    pypi\n[conda] torchvision               0.21.0+cu124             pypi_0    pypi\n[conda] transformers              4.51.3                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.5\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     SYS     SYS     0-47    0               N/A\nGPU1    SYS      X      SYS     SYS     0-47    0               N/A\nGPU2    SYS     SYS      X      SYS     0-47    0               N/A\nGPU3    SYS     SYS     SYS      X      0-47    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNVIDIA_VISIBLE_DEVICES=void\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\nNCCL_VERSION=2.21.5-1\nNCCL_SOCKET_IFNAME=^lo,docker,veth,tailscale,anyscale\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNVIDIA_PRODUCT_NAME=CUDA\nCUDA_VERSION=12.4.1\nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWith #17270 , `pixtral-community/pixtral-12b` failed. I think what we did in #17270 and other similar places are too hacky: we insert attributes randomly\n\n```shell\npython ../vllm/examples/offline_inference/vision_language.py -m pixtral_hf\n```\n\noutput\n```text\n  File \"~/vllm/vllm/multimodal/processing.py\", line 787, in apply_token_matches\n    token_id_seqs = _apply_matches(prompt, mm_matches, mm_item_counts)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"~/vllm/vllm/multimodal/processing.py\", line 764, in _apply_matches\n    content = origin.get_content(item_idx)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"~/vllm/vllm/multimodal/processing.py\", line 491, in get_content\n    content = content(item_idx)\n              ^^^^^^^^^^^^^^^^^\n  File \"~/vllm/vllm/model_executor/models/llava.py\", line 367, in get_replacement\n    ncols, nrows = encoder_info.get_patch_grid_size(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"~/vllm/vllm/model_executor/models/pixtral.py\", line 940, in get_patch_grid_size\n    patch_width = patch_height = self.get_patch_size()\n                                 ^^^^^^^^^^^^^^^^^^^^^\n  File \"~/vllm/vllm/model_executor/models/pixtral.py\", line 923, in get_patch_size\n    self.vision_config.spatial_merge_size)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 210, in __getattribute__\n    return super().__getattribute__(key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'PixtralVisionConfig' object has no attribute 'spatial_merge_size'\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-05-01T23:49:41+00:00",
    "closed_at": "2025-05-02T05:14:10+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17565/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/17565"
  },
  {
    "number": 15958,
    "title": "[Bug]: SpecDecoding metrics showing with disabled spec decoding",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\nINFO 04-02 18:23:13 [__init__.py:239] Automatically detected platform tpu.\nCollecting environment information...\nPyTorch version: 2.8.0\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.31.6\nLibc version: glibc-2.35\n\nPython version: 3.11.11 (main, Feb 12 2025, 14:51:05) [Clang 19.1.6 ] (64-bit runtime)\nPython platform: Linux-6.8.0-1015-gcp-x86_64-with-glibc2.35\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               180\nOn-line CPU(s) list:                  0-179\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 9B14\nCPU family:                           25\nModel:                                17\nThread(s) per core:                   2\nCore(s) per socket:                   90\nSocket(s):                            1\nStepping:                             1\nBogoMIPS:                             5200.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr wbnoinvd arat avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            2.8 MiB (90 instances)\nL1i cache:                            2.8 MiB (90 instances)\nL2 cache:                             90 MiB (90 instances)\nL3 cache:                             384 MiB (12 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-179\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.8.0\n[pip3] torch-tb-profiler==0.4.3\n[pip3] torch-xla==2.8.0+gitfe3bb7f\n[pip3] transformers==4.50.2\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3.dev145+gddc2dddf2.d20250331\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\n\n```\n\n</details>\n\n\n\n\n### \ud83d\udc1b Describe the bug\n\nI just happened to start llava on main (`44f990515`) no spec decoding with `vllm serve llava-hf/llava-1.5-7b-hf --max-model-len 2512 --max-num-seqs 16 --max-num-batched-tokens 64 --chat-template examples/template_llava.jinja`. I am getting a lot of clutter in the logs:\n\n```\nINFO 04-02 18:03:41 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 50.0%\nINFO 04-02 18:03:41 [metrics.py:53] SpecDecoding metrics: Draft acceptance rate: nan%, Accepted: 0 tokens, Drafted: 0 tokens\nINFO 04-02 18:03:51 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 50.0%\nINFO 04-02 18:03:51 [metrics.py:53] SpecDecoding metrics: Draft acceptance rate: nan%, Accepted: 0 tokens, Drafted: 0 tokens\nINFO 04-02 18:04:01 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 50.0%\nINFO 04-02 18:04:01 [metrics.py:53] SpecDecoding metrics: Draft acceptance rate: nan%, Accepted: 0 tokens, Drafted: 0 tokens\n```\n\ncc @markmc \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "speculative-decoding",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-04-02T18:23:33+00:00",
    "closed_at": "2025-04-04T15:52:42+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15958/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15958"
  },
  {
    "number": 15554,
    "title": "[Bug][V1]: ngram + guided decoding",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.12.8 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:31:09) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-130-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.3.103\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA A10\nGPU 1: NVIDIA A10\nGPU 2: NVIDIA A10\nGPU 3: NVIDIA A10\n\nNvidia driver version: 535.183.01\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               32\nOn-line CPU(s) list:                  0-31\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Silver 4208 CPU @ 2.10GHz\nCPU family:                           6\nModel:                                85\nThread(s) per core:                   2\nCore(s) per socket:                   8\nSocket(s):                            2\nStepping:                             7\nCPU max MHz:                          3200.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4200.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            512 KiB (16 instances)\nL1i cache:                            512 KiB (16 instances)\nL2 cache:                             16 MiB (16 instances)\nL3 cache:                             22 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-7,16-23\nNUMA node1 CPU(s):                    8-15,24-31\nVulnerability Gather data sampling:   Mitigation; Microcode\nVulnerability Itlb multihit:          KVM: Mitigation: VMX disabled\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; Enhanced IBRS\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Mitigation; TSX disabled\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.50.1\n[pip3] triton==3.2.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-ml-py              12.570.86                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.3.0                   pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] transformers              4.50.1                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NODE    SYS     SYS     0-7,16-23       0               N/A\nGPU1    NODE     X      SYS     SYS     0-7,16-23       0               N/A\nGPU2    SYS     SYS      X      NODE    8-15,24-31      1               N/A\nGPU3    SYS     SYS     NODE     X      8-15,24-31      1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nwhen running `VLLM_USE_V1=1 vllm serve stelterlab/Mistral-Small-24B-Instruct-2501-AWQ --tensor-parallel-size 2 --gpu-memory-utilization 0.7 --guided-decoding-backend auto --speculative-model [ngram] --num-speculative-tokens 3`\n\nusing v0.8.2\n\nsome decodings will fail with following error:\n\n```\nWarning: Parser Error: token \" null\" doesn't satisfy the grammar; byte 'n' fails parse; stopping\nWARNING 03-26 15:23:05 [backend_guidance.py:86] LLMatcher error: Parser Error: token \" null\" doesn't satisfy the grammar; byte 'n' fails parse\n```\n\nissue does not persist when removing speculative decoding\n\njson schema that is sent:\n\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\n\n\nclass InteractionType(BaseModel):\n    type: Literal[\n        \"Example interaction type 1\",\n        \"Example interaction type 2\",\n        \"Example interaction type 3\",\n    ]\n    explanation: str | None = Field(\n        None,\n        title=\"Interaction Explanation\",\n        description=\"very brief explanation (max one sentence/200 characters) why this interaction type was chosen.\",\n        max_length=200,\n    )\n    trigger_sentence: str | None = Field(\n        None,\n        title=\"Trigger Sentence\",\n        description=\"The sentence from the transcription that triggered this interaction type assignment.\",\n    )\n    trigger_context: str | None = Field(\n        None,\n        title=\"Trigger Context\",\n        description=\"Additional context (e.g., preceding and following sentences) surrounding the trigger sentence that supports the assignment of this interaction type.\",\n    )\n\n\nclass Category(BaseModel):\n    is_match: bool = Field(\n        title=\"Match of category\", description=\"True if category is matched\"\n    )\n    explanation: str | None = Field(\n        ...,\n        title=\"Explanation\",\n        description=\"very brief explanation (max one sentence/200 characters) why the category was matched.\",\n        max_length=200,\n    )\n    issue_types: list[InteractionType] = Field(\n        ...,\n        title=\"Issue Types\",\n        description=(\n            \"Specify one or more interaction types for the matched product category. \"\n            \"For each interaction type, include both a brief explanation, the sentence that triggered this assignment, \"\n            \"and additional context (e.g., surrounding sentences) if available.\"\n        ),\n    )\n    \nclass Categorization(BaseModel):\n    example_category: Category | None = Field(\n        None,\n        title=\"Example Category\",\n        description=\"Example description\",\n    )\n```\n\nalso could be related to: https://github.com/vllm-project/vllm/issues/10442\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-03-26T14:44:47+00:00",
    "closed_at": "2025-06-27T04:35:33+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15554/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15554"
  },
  {
    "number": 15144,
    "title": "[Bug] Mismatch between `get_multimodal_embedding` output and `PlaceholderRange`",
    "body": "In V1, we expect the output of `get_multimodal_embedding` to correspond to the `PlaceholderRange`, which is in turn constructed based on `PromptUpdateDetails.features`. However, the current V1 code doesn't validate this, causing the model to crash during inference when under high load (e.g. #14897, #14963).\n\nFrom a quick look at the code, these models output embedding sizes which are inconsistent with the placeholder range:\n\n- [x] Fuyu (fixed by #15731)\n- [x] Gemma3 (fixed by #14980)\n- [x] Idefics3 (fixed by #15696)\n- [x] InternVL-based models (fixed by #15086)\n- [x] MiniCPM-V (fixed by #15487)\n\n(Basically, any model that has image newline/column tokens after applying HF processor needs a mask to map image patch features to image embeddings, as described below.)\n\nTo fix this, we can follow these steps:\n\n1. Update the multi-modal processor to output a mask to indicate which positions in the `PlaceholderRange`-aligned embeddings should the patch features (outputted by vision encoder) be assigned to. This mask can be called `embed_is_patch`.\n2. Use `scatter_patch_features` to scatter the patch features into the image embedding tensor.\n3. When merging multimodal embeddings, use `select_patch_features` to recover the patch features from the image embeddings. The number of patch features should correspond to the number of image tokens (which is a subset of the feature tokens in `PromptUpdateDetails`).\n\nFollow-up work:\n\n- #15712 (assigned to @DarkLight1337)\n- Directly use individual token IDs instead of range of IDs (assigned to @ywang96 )\n",
    "labels": [
      "bug",
      "help wanted",
      "v1",
      "multi-modality"
    ],
    "state": "closed",
    "created_at": "2025-03-19T16:53:23+00:00",
    "closed_at": "2025-03-30T10:47:54+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15144/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/15144"
  },
  {
    "number": 14911,
    "title": "[Bug]: UserWarning on skipping serialisation of PostGradPassManager",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: 19.1.7\nCMake version: version 3.31.4\nLibc version: glibc-2.35\n\nPython version: 3.11.10 (main, Oct 16 2024, 04:38:48) [Clang 18.1.8 ] (64-bit runtime)\nPython platform: Linux-5.15.0-134-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-80GB\nNvidia driver version: 565.57.01\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               12\nOn-line CPU(s) list:                  0-11\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Gold 5317 CPU @ 3.00GHz\nCPU family:                           6\nModel:                                106\nThread(s) per core:                   1\nCore(s) per socket:                   12\nSocket(s):                            1\nStepping:                             6\nBogoMIPS:                             6002.58\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush acpi mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single intel_ppin ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip pku ospke gfni vaes vpclmulqdq rdpid md_clear flush_l1d arch_capabilities\nHypervisor vendor:                    Xen\nVirtualization type:                  full\nL1d cache:                            576 KiB (12 instances)\nL1i cache:                            384 KiB (12 instances)\nL2 cache:                             15 MiB (12 instances)\nL3 cache:                             216 MiB (12 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-11\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT Host state unknown\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.2.post1+cu124torch2.5\n[pip3] mypy==1.15.0\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==2.1.3\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] sentence-transformers==3.2.1\n[pip3] torch==2.6.0\n[pip3] torchao==0.9.0\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.49.0\n[pip3] transformers-stream-generator==0.0.5\n[pip3] triton==3.2.0\n[pip3] tritonclient==2.51.0\n[pip3] vector-quantize-pytorch==1.21.2\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.3.dev758+g489b7938\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-11    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nCUDA_PATH=/nix/store/skdw4l72lgrj628l0arj1m3ynlzfksi8-cuda-merged-12.4\nLD_LIBRARY_PATH=/workspace/vllm/.venv/lib/python3.11/site-packages/cv2/../../lib64:/nix/store/lmyyfaz2amcs2an1f6m9h263151jiajy-cuda-merged-12.4/lib\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI have started to see this serialization warning whenever running `vllm serve` from time to time when running with v1:\n\n```prolog\n/workspace/project/.venv/lib/python3.12/site-packages/torch/utils/_config_module.py:189: UserWarning: Skipping serialization of post_grad_custom_post_pass value <vllm.compilation.pass_manager.PostGradPassManager object at 0x7f23314f7c20>\n  warnings.warn(f\"Skipping serialization of {k} value {v}\")\n```\n\nNot sure how impactful is this, or we can ignore this.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-03-17T01:16:18+00:00",
    "closed_at": "2025-07-16T02:42:28+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14911/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14911"
  },
  {
    "number": 14777,
    "title": "[Bug]: Intermittent CUDA IMA in V1 CI tests",
    "body": "### Your current environment\n\nCI\n\n### \ud83d\udc1b Describe the bug\n\n\n```\nProcessed prompts:   0% 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]ERROR 03-13 08:00:58 [core.py:337] EngineCore hit an exception: Traceback (most recent call last):\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 330, in run_engine_core\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]     engine_core.run_busy_loop()\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 364, in run_busy_loop\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]     outputs = step_fn()\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]               ^^^^^^^^^\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 192, in step\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]     output = self.model_executor.execute_model(scheduler_output)\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 80, in execute_model\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]     output = self.collective_rpc(\"execute_model\",\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]   File \"/opt/venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]     answer = run_method(self.driver_worker, method, args, kwargs)\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]   File \"/opt/venv/lib/python3.12/site-packages/vllm/utils.py\", line 2238, in run_method\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]     return func(*args, **kwargs)\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]   File \"/opt/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]     return func(*args, **kwargs)\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 252, in execute_model\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]     output = self.model_runner.execute_model(scheduler_output)\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]   File \"/opt/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]     return func(*args, **kwargs)\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]   File \"/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1061, in execute_model\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]     gen_lens = valid_mask.sum(dim=1).tolist()\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]                ^^^^^^^^^^^^^^^^^^^^^\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337] RuntimeError: CUDA error: an illegal memory access was encountered\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]\n[2025-03-13T08:00:58Z] ERROR 03-13 08:00:58 [core.py:337]\n[2025-03-13T08:00:58Z] CRITICAL 03-13 08:00:58 [core_client.py:260] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n```\nBuilkite link: https://buildkite.com/vllm/ci/builds/15354#01958e6b-1224-45dc-b444-85d4a30668bf\n",
    "labels": [
      "bug",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-03-13T18:55:58+00:00",
    "closed_at": "2025-03-14T16:29:00+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14777/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/14777"
  },
  {
    "number": 14522,
    "title": "[Usage]: [V1] Support `MistralTokenizer` on V1",
    "body": "### Your current environment\n\nCurrently, Xgrammar does not work with `MistralTokenizer`. This means we cannot use `MistralTokenizer` with V1\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "structured-output",
      "usage",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-03-09T19:38:30+00:00",
    "closed_at": "2025-03-12T02:40:10+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14522/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14522"
  },
  {
    "number": 13437,
    "title": "[V1][Bug]: TP with Ray does not terminate gracefully",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen using Ray as the distributed executor backend and using the `LLM` Python API , the main process does not terminate gracefully:\n\n```\n*** SIGTERM received at time=1739834838 on cpu 88 ***\nPC: @     0x7fe108d1f117  (unknown)  (unknown)\n    @     0x7fe108cd0520  (unknown)  (unknown)\n[2025-02-17 15:27:18,341 E 2669821 2669821] logging.cc:460: *** SIGTERM received at time=1739834838 on cpu 88 ***\n[2025-02-17 15:27:18,341 E 2669821 2669821] logging.cc:460: PC: @     0x7fe108d1f117  (unknown)  (unknown)\n[2025-02-17 15:27:18,341 E 2669821 2669821] logging.cc:460:     @     0x7fe108cd0520  (unknown)  (unknown)\n2025-02-17 15:27:18,342 INFO compiled_dag_node.py:1867 -- Tearing down compiled DAG\n2025-02-17 15:27:18,342 INFO compiled_dag_node.py:1872 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, a1dcab214fac9e464505ef2701000000)\n2025-02-17 15:27:18,342 INFO compiled_dag_node.py:1872 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, fad8cccd5652d08fb1c696bb01000000)\n2025-02-17 15:27:18,342 INFO compiled_dag_node.py:1872 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 37fc4010a9fc8557c83a042201000000)\n2025-02-17 15:27:18,342 INFO compiled_dag_node.py:1872 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 1b42010b9bf378a0bb209cb401000000)\n(RayWorkerWrapper pid=2670369) Destructing NCCL group on actor: Actor(RayWorkerWrapper, a1dcab214fac9e464505ef2701000000)\n2025-02-17 15:27:19,080 INFO compiled_dag_node.py:1892 -- Waiting for worker tasks to exit\n2025-02-17 15:27:19,080 INFO compiled_dag_node.py:1894 -- Teardown complete\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-02-17T23:37:17+00:00",
    "closed_at": "2025-02-19T17:40:51+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13437/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/13437"
  },
  {
    "number": 13058,
    "title": "[V1][Help Wanted] Porting missing sampling parameters to V1",
    "body": "### Anything you want to discuss about vllm.\n\nTo switch the engine from V0 to V1, we need to comprehensively support the sampling parameters in https://github.com/vllm-project/vllm/blob/main/vllm/sampling_params.py\n\nWhile most of the key parameters are already supported, some of them are missing:\n\nTODO (help wanted):\n- [x] `n` (parallel sampling) #10980  @afeldman-nm \n- [x] `guided_decoding` (structured decoding) #12388  @aarnphm \n- [x] `logit_bias` #13079 @houseroad \n- [x] `min_p` #13191 @AoyuQC\n- [ ] `bad_words` (originally implemented via logits processor) #13376 @22quinn \n- [x] `allowed_token_ids` (originally implemented via logits processor) #13210 @houseroad \n\nParameters that will not be supported in V1:\n* best_of\n* logits_processors\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "misc",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-02-10T23:13:42+00:00",
    "closed_at": "2025-03-20T14:15:16+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13058/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/13058"
  },
  {
    "number": 12754,
    "title": "[Bug]: vllm v1: RuntimeError: Cannot re-initialize CUDA in forked subprocess",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 02-04 23:23:46 __init__.py:186] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.31.4\nLibc version: glibc-2.35\n\nPython version: 3.12.8 (main, Jan 14 2025, 22:49:14) [Clang 19.1.6 ] (64-bit runtime)\nPython platform: Linux-5.15.0-122-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H200\nNvidia driver version: 550.90.12\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               44\nOn-line CPU(s) list:                  0-43\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 9654 96-Core Processor\nCPU family:                           25\nModel:                                17\nThread(s) per core:                   1\nCore(s) per socket:                   1\nSocket(s):                            44\nStepping:                             1\nBogoMIPS:                             4792.79\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm rep_good nopl cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw perfctr_core invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr wbnoinvd arat npt lbrv nrip_save tsc_scale vmcb_clean flushbyasid pausefilter pfthreshold v_vmsave_vmload vgif avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm flush_l1d arch_capabilities\nVirtualization:                       AMD-V\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            2.8 MiB (44 instances)\nL1i cache:                            2.8 MiB (44 instances)\nL2 cache:                             22 MiB (44 instances)\nL3 cache:                             704 MiB (44 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-43\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] mypy==1.11.1\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] sentence-transformers==3.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.2\n[pip3] transformers-stream-generator==0.0.5\n[pip3] triton==3.1.0\n[pip3] tritonclient==2.51.0\n[pip3] vector-quantize-pytorch==1.21.2\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.2.dev36+g18016a5e\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \t0-43\t0\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nvllm v1 seems broken in tot.\n\nEven tinyllama fails to run.\n\n```\n$ export VLLM_USE_V1=1\n$ python -m vllm.entrypoints.openai.api_server --model TinyLlama/TinyLlama-1.1B-Chat-v1.0\n\nINFO 02-04 23:21:26 __init__.py:186] Automatically detected platform cuda.\nINFO 02-04 23:21:31 api_server.py:840] vLLM API server version 0.7.2.dev36+g18016a5e\nINFO 02-04 23:21:31 api_server.py:841] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='TinyLlama/TinyLlama-1.1B-Chat-v1.0', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)\nWARNING 02-04 23:21:31 arg_utils.py:1326] Setting max_num_batched_tokens to 8192 for OPENAI_API_SERVER usage context.\nINFO 02-04 23:21:44 config.py:542] This model supports multiple tasks: {'generate', 'embed', 'reward', 'score', 'classify'}. Defaulting to 'generate'.\nINFO 02-04 23:21:44 config.py:1557] Chunked prefill is enabled with max_num_batched_tokens=8192.\nINFO 02-04 23:21:45 core.py:47] Initializing a V1 LLM engine (v0.7.2.dev36+g18016a5e) with config: model='TinyLlama/TinyLlama-1.1B-Chat-v1.0', speculative_config=None, tokenizer='TinyLlama/TinyLlama-1.1B-Chat-v1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=TinyLlama/TinyLlama-1.1B-Chat-v1.0, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\nERROR 02-04 23:21:45 core.py:210] EngineCore hit an exception: Traceback (most recent call last):\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/vllm/v1/engine/core.py\", line 202, in run_engine_core\nERROR 02-04 23:21:45 core.py:210]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 02-04 23:21:45 core.py:210]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/vllm/v1/engine/core.py\", line 156, in __init__\nERROR 02-04 23:21:45 core.py:210]     super().__init__(vllm_config, executor_class)\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/vllm/v1/engine/core.py\", line 51, in __init__\nERROR 02-04 23:21:45 core.py:210]     self.model_executor = executor_class(vllm_config)\nERROR 02-04 23:21:45 core.py:210]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/vllm/executor/executor_base.py\", line 51, in __init__\nERROR 02-04 23:21:45 core.py:210]     self._init_executor()\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/vllm/executor/uniproc_executor.py\", line 41, in _init_executor\nERROR 02-04 23:21:45 core.py:210]     self.collective_rpc(\"init_device\")\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/vllm/executor/uniproc_executor.py\", line 51, in collective_rpc\nERROR 02-04 23:21:45 core.py:210]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 02-04 23:21:45 core.py:210]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/vllm/utils.py\", line 2210, in run_method\nERROR 02-04 23:21:45 core.py:210]     return func(*args, **kwargs)\nERROR 02-04 23:21:45 core.py:210]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/vllm/v1/worker/gpu_worker.py\", line 113, in init_device\nERROR 02-04 23:21:45 core.py:210]     torch.cuda.set_device(self.device)\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 478, in set_device\nERROR 02-04 23:21:45 core.py:210]     torch._C._cuda_setDevice(device)\nERROR 02-04 23:21:45 core.py:210]   File \"/root/dev/vllm/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\nERROR 02-04 23:21:45 core.py:210]     raise RuntimeError(\nERROR 02-04 23:21:45 core.py:210] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\nERROR 02-04 23:21:45 core.py:210]\nCRITICAL 02-04 23:21:45 core_client.py:158] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\nKilled\n\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-02-04T23:27:14+00:00",
    "closed_at": "2025-02-13T18:30:01+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12754/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/12754"
  },
  {
    "number": 12741,
    "title": "[Bug]: V1 Engine Non-Coherent output",
    "body": "### Your current environment\n\nVllm 0.7.1\nCUDA 12.6\nDriver Version 560.94\ntorch 2.5.1\ntransformers 4.46.0\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen using V1 engine, the output is noncoherent. \nFor examples:\n\".SetString that a I. to . v the . the v the on the this . p et is Jansw to A 2000 v and . tochemically the I in we ,  (self. . v or on we Fnew, international lawrence of for000,   __in U, do \u2014 and it - arse not  C is images that super()000 the use a . v the . v to000 an A for to000 with from in g be for . \"\nI have tried various changes including changing temp, frequency_penalty, etc. I have tried various models, and nothing improved.\nHere is what I use\nvllm serve nm-testing/DeepSeek-R1-Distill-Llama-70B-FP8-dynamic  --host 0.0.0.0   --port 8000   --tensor-parallel-size 8  --seed 1234  --max-model-len 16000  --enable-auto-tool-choice --tool-call-parser llama3_json --chat-template /home/nd600/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/examples/tool_chat_template_llama3.1_json.jinja\nThanks for your help.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-02-04T16:29:10+00:00",
    "closed_at": "2025-02-22T16:45:38+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12741/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12741"
  },
  {
    "number": 12724,
    "title": "[Feature]: V1 support Xformers",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nI tried to use V1 engine to load Qwen2.5-72B-Instruct-GPTQ-int4 on 4*2080ti 22g, and it raised an AssertionError. The error message shows assert is_fa_version_supported(self.fa_version) caused this AssertionError. It seems like the 2080ti does not support any version of FA. In the V0 engine, it uses the Xformers backend and works fine. However, in V1, it raises an error and stops working. So, I would like to request Xformers support for the V1 engine. I know the 2080ti is a bit outdated, but it is the only choice for getting a large GPU memory at an acceptable price. I really appreciate your help with this. It would mean a lot.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "stale",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-02-04T09:05:37+00:00",
    "closed_at": "2025-06-06T02:18:19+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12724/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12724"
  },
  {
    "number": 12692,
    "title": "[Bug]: V1 engine ignores guided json",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 02-03 05:55:13 __init__.py:183] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.31.4\nLibc version: glibc-2.35\n\nPython version: 3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-6.8.0-52-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3\nNvidia driver version: 560.35.05\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               128\nOn-line CPU(s) list:                  0-127\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Gold 6448Y\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   32\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          4100.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4200.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hfi vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            3 MiB (64 instances)\nL1i cache:                            2 MiB (64 instances)\nL2 cache:                             128 MiB (64 instances)\nL3 cache:                             120 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126\nNUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.2\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0,2,4,6,8,10    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNVIDIA_VISIBLE_DEVICES=GPU-6cf6dad3-da49-c3af-bb85-7275c18ce3d6\nNVIDIA_REQUIRE_CUDA=cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\nNCCL_VERSION=2.17.1-1\nNVIDIA_DRIVER_CAPABILITIES=all\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nNVIDIA_CUDA_END_OF_LIFE=1\nCUDA_VERSION=12.1.0\nLD_LIBRARY_PATH=/usr/local/lib/python3.12/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nWhen making a request with to the OpenAi compatible Api with the extra fields for guided_json generation like so:\n\n```\n{\n  \"model\": \"Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"what is the height of the eiffel tower\"\n    }\n  ],\n  \"guided_json\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"height\": {\n        \"type\": \"number\"\n      }\n    },\n    \"required\": [\n      \"height\"\n    ]\n  }\n}\n```\n\nThe output simply ignores the guided decoding paramter. When switching back to V0 it works fine.\n\nHere are the logs from the vllm server:\n\n```\n`INFO 02-03 05:59:31 logger.py:37] Received request chatcmpl-178de64763e84ddd81a9f6f62ee0f4c3: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nwhat is the height of the eiffel tower<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32739, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json={'type': 'object', 'properties': {'height': {'type': 'number'}}, 'required': ['height']}, regex=None, choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None)), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\nDEBUG 02-03 05:59:31 async_llm_engine.py:546] Building guided decoding logits processor. Params: GuidedDecodingParams(json={'type': 'object', 'properties': {'height': {'type': 'number'}}, 'required': ['height']}, regex=None, choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None)\nINFO 02-03 05:59:32 engine.py:273] Added request chatcmpl-178de64763e84ddd81a9f6f62ee0f4c3.\nINFO 02-03 05:59:32 metrics.py:453] Avg prompt throughput: 4.7 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nINFO:     **.***.**.**:***** - \"POST /v1/chat/completions HTTP/1.1\" 200 OK`\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-02-03T14:01:44+00:00",
    "closed_at": "2025-06-06T02:18:22+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12692/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12692"
  },
  {
    "number": 12690,
    "title": "[Bug]: V1 cannot be run in Triton Inference Server Backend",
    "body": "### Your current environment\n\n. NA\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nWhen attempting to use the `VLLM_USE_V1=1` feature in triton inference server backend the models fail to start up due to signal handling being attempted outside of the main thread.\n\nThe following error occurs in startup.\n\n```text\nmodel.py:244] \"[vllm] Failed to start engine: signal only works in main thread of the main interpreter\"\npb_stub.cc:366] \"Failed to initialize Python stub: ValueError: signal only works in main thread of the main interpreter\n\nAt:\n  /usr/lib/python3.12/signal.py(58): signal\n  /app/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py(160): __init__\n  /app/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py(252): __init__\n  /app/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py(53): make_client\n  /app/.venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py(79): __init__\n  /app/.venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py(107): from_engine_args\n  /app/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py(162): build_async_engine_client_from_engine_args\n  /usr/lib/python3.12/contextlib.py(211): __aenter__\n  /opt/tritonserver/backends/vllm/model.py(289): _run_llm_engine\n  /usr/lib/python3.12/asyncio/events.py(103): _run\n  /usr/lib/python3.12/asyncio/base_events.py(1988): _run_once\n  /usr/lib/python3.12/asyncio/base_events.py(649): run_forever\n  /usr/lib/python3.12/asyncio/base_events.py(687): run_until_complete\n  /usr/lib/python3.12/asyncio/runners.py(126): run\n  /usr/lib/python3.12/asyncio/runners.py(193): run\n  /usr/lib/python3.12/threading.py(1014): run\n  /usr/lib/python3.12/threading.py(1077): _bootstrap_inner\n  /usr/lib/python3.12/threading.py(1030): _bootstrap\n\"```\n\nThis seems to be an incompatibility between VLLM V1 assuming it is on the main thread and triton inference server running vllm outside of the main thread.\n\nQuestion here is if the implementation of V1 can be adapted so that it can run outside of the main thread?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-02-03T13:28:06+00:00",
    "closed_at": "2025-03-25T12:16:53+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12690/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12690"
  },
  {
    "number": 12678,
    "title": "[Bug]: V1 engine ignores logits processors and min-p sampling",
    "body": "### Your current environment\n\n**vLLM Version**: 0.7.0\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\n# Issue: V1 engine ignores custom logits processors **and** does not implement min-p sampling\n\n**Problem**  \n1. **Custom logits processors**: In the new V1 engine, specifying a `logits_processor` in `SamplingParams` for `LLM.generate()` has no effect. The code in [`gpu_model_runner.py`](https://github.com/vllm-project/vllm/blob/main/vllm/v1/worker/gpu_model_runner.py) never passes any sampling metadata into `self.model.compute_logits(...)`, so the logits processor is silently ignored.\n\n2. **Min-p**: Similarly, `min_p` (a sampling parameter supported in V0 akin to `top_k` and `top_p`) is not applied at all in V1. The [`sampler.py`](https://github.com/vllm-project/vllm/blob/main/vllm/v1/sample/sampler.py) for the new engine appears to skip it entirely, so it never factors into the final token selection.\n\nIf those features are not yet supported, consider at least raising a warning or error to avoid silent failures.\n\n**Possible Fix for Logits Processor Issue**  \n1. **Create a new data class** to hold relevant metadata for `self.model.compute_logits(...)`. \n    - Could simply hold request ids and and request states (`CachedRequestState`). \n2. **Collate metadata** inside `GPUModelRunner.execute_model(...)`. \n3. **Patch** LogitsProcessor.forward() inside [`logits_processor.py`](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/logits_processor.py) to handle the new V1 metadata class alongside old V0 SamplingMetadata class.\n4. **Define** `LogitsProcessor._apply_logits_processor_v1(...)` or something similar to properly handle preprocessed `hidden_states` tensor in V1 model runner, as opposed to re-using the V0 version.\n\n**Possible Fix for Min-p Issue**\n1. **Add min_p attribute** to `InputBatch` in [`gpu_input_batch.py`](https://github.com/vllm-project/vllm/blob/main/vllm/v1/worker/gpu_input_batch.py). \n2. **Add min_p field** to `SamplingMetadata` data class in [`metadata.py`](https://github.com/vllm-project/vllm/blob/main/vllm/v1/sample/metadata.py). \n3. **Modify forward function** of `Sampler` in [`sampler.py`](https://github.com/vllm-project/vllm/blob/main/vllm/v1/sample/sampler.py) to apply min-p filtering. \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-02-03T06:43:57+00:00",
    "closed_at": "2025-07-04T02:18:21+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12678/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12678"
  },
  {
    "number": 12584,
    "title": "[Bug]: [V1] New v1 engine does not support n>1?",
    "body": "### Your current environment\n\nVLLM version 0.7.0\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nWhen using v1 engine, `LLM.generate()` only returns 1 `CompletionOutput` even when `SamplingParams` sets `n>1`\n\nIs this expected to work or is `n>1` not yet supported for v1? If so, are there plans to support it? \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-01-30T18:24:17+00:00",
    "closed_at": "2025-04-22T03:40:20+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12584/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12584"
  },
  {
    "number": 12529,
    "title": "[Performance]: V1 higher memory usage",
    "body": "### Proposal to improve performance\n\n_No response_\n\n### Report of performance regression\n\n*Hardware:* 4x RTX 3070 = 32GB VRAM\n\n*Issue:* I was able to run `Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4` with 12K context length with 0.6.x, now with `0.7.0 + VLLM_USE_V1=1` I cannot push the context length higher than 3K or encountering a CUDA OOM error. \nOf course, I can reconfigure it to avoid OOM, my question is: *Is V1 expected to consume more memory?*\n\n\nSome of the libraries:\n```\nflashinfer==0.1.6+cu124torch2.4\ntorch==2.5.1\ntransformers==4.48.1\nvllm==0.7.0\n```\n\n*VLLM command*\n```\n        - vllm\n        - serve\n        - Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4\n        - --gpu-memory-utilization=1\n        - --tensor-parallel-size=4\n        - --load-format=auto\n        - --enforce-eager\n        - --swap-space=0\n        - --max-model-len=12K\n        - --max-num-batched-tokens=12K\n        - --disable-fastapi-docs\n        - --trust-remote-code\n        - --enable-auto-tool-choice\n        - --tool-call-parser=hermes\n```\n\nThanks\n\n### Misc discussion on performance\n\n_No response_\n\n### Your current environment (if you think it is necessary)\n\n```text\nThe output of `python collect_env.py`\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-01-28T22:23:50+00:00",
    "closed_at": "2025-03-14T03:40:24+00:00",
    "comments": 22,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12529/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12529"
  },
  {
    "number": 12480,
    "title": "[RFC]: [V1] TPU support and multiple architecture support",
    "body": "### Motivation.\n\nWe are in process of adding Google TPU support to the vLLM V1. \n\nHere is the WIP PR [https://github.com/vllm-project/vllm/pull/11936](https://github.com/vllm-project/vllm/pull/11936). \n\nSince this is the first time we add another hardware backend to V1, the PR has some refactor to avoid code duplications, which requires discussion and feedback.\n\n\n\n### Proposed Change.\n\nHere is the summary of changes this PR introduces:\n\n1. Refactors the common logic of model_runner to **model_runner_base.py** in the folllowing way (Virtual functions in italic):\n       \\_\\_init\\_\\_() => Has common config init\n       get_model() => Just simply returns model\n       get_kv_cache_spec() => Common logic for KV cache management\n       _initialize_kv_cache()_ => Virtual API\n       _execute_model()_ => Virtual API\n       _load_model()_ => Virtual API\n       _dummy_run()_ => Virtual API\n       _profile_run()_ => Virtual API\n       _capture_model()_ => Virtual API\n\n2. Refactors common logic of worker to **worker_base.py** in the following way (Virtual functions in italic):\n       \\_\\_init\\_\\_() => Has common config init, HF init, torch profiler init\n       load_model() => Calls load_model() of model_runner\n       compile_or_warm_up_model() => Calls capture model based on enforce_eager param and sets random seed\n       get_model() => Calls get_model() of model_runner\n       get_kv_cache_spec() => Calls get_kv_cache_spec() of model_runner\n       initialize_cache() => Calls initialize_kv_cache() of model_runner\n       profile() => Starts/stops profiler\n       check_health() => Empty function\n       _init_device()_ => Virtual API\n       _determine_available_memory()_ => Virtual API\n       _execute_model()_ => Virtual API\n\nComments and feedback are very welcome.\n\n\n### Feedback Period.\n\none week\n\n### CC List.\n\n@robertgshaw2-redhat @WoosukKwon @mgoin @tlrmchlsmth @youkaichao @simon-mo @njhill @comaniac @ywang96 @DarkLight1337 @SageMoore @bvrockwell \n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "stale",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-01-27T18:31:51+00:00",
    "closed_at": "2025-06-05T02:12:54+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12480/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/12480"
  },
  {
    "number": 11945,
    "title": "[RFC]: Pipeline-Parallelism for vLLM V1",
    "body": "### Motivation.\n\nThis RFC describes the approach for supporting pipeline parallelism in [vLLM V1 architecture](https://github.com/vllm-project/vllm/issues/8779).\r\n\r\nPipeline parallelism was [supported in V0 with the virtual-engine approach](https://github.com/vllm-project/vllm/issues/4461). In short, we create multiple virtual engines to match the number of pipeline stages, and each virtual engine has its own scheduler, block manager and cache engine, so that they can schedule multiple batches simultaneously to the same executor with pipeline parallelism, saturating all pipeline stages to improve the efficiency. However, virtual engine introduces the following drawbacks:\r\n\r\n1. The lack of a centralized scheduler prevents global optimization from being applied.\r\n2. It introduces complexity to the engine architecture and implementation.\r\n\r\nIn this RFC, we aim to support pipeline parallelism in the V1 LLMEngineCore, with the following properties: \r\n\r\n- Good performance: throughput and TTFT\r\n  - The design should minimize pipeline bubbles\r\n- KV-cache efficiency\r\n  - The design should minimize KV-cache fragmentation\r\n  - The design should facilitate KV-cache block reuse across different requests\r\n  - The design should be compatible with the current prefix caching mechanism\r\n- Architecture\r\n  - The design should align well with V1 architecture\r\n- Scheduling policy flexibility\r\n  - The design should support existing policies (FCFS and priority) and future policies\n\n### Proposed Change.\n\nThe current V1 engine core runs a busy synchronous loop, and each iteration consists of 3 operations:\r\n- schedule(): schedules a batch of requests to run considering new requests, existing requests and preempted requests.\r\n- execute(): accepts scheduler output as the execution plan, executes the model, and returns the output.\r\n- update(): updates the scheduler state based on finished batch execution output.\r\n\r\nIn this section, we discuss available options of adopting the V1 engine core architecture to achieve pipeline parallelism.\r\n\r\n### Option 1: Atomic engine step\r\n\r\n![Sync Scheduler@2x](https://github.com/user-attachments/assets/2b9d8fdd-d31c-4206-91d7-fc809bb207d2)\r\n\r\n#### Design sketch\r\nIntuitively, it would be ideal to keep the current busy loop mechanism in the engine core, and isolate all pipeline parallelism required changes to the executor, as shown in the above figure.\r\n\r\n- LLMEngineCore\r\n  - The busy loop remains the same.\r\n  - The model output is not corresponding to the scheduler output (i.e., microbatch in the figure) anymore. The model output in this iteration is the output of the microbatch we submitted to the executor PP_SIZE iterations ago.\r\n- RayExecutor\r\n  - microbatch_queue: The queue size is the same as PP_size, and we need to guarantee that the queue is always full. If there is not a sufficient number of microbatches (e.g., cold start or idle), then we need to push empty microbatches (i.e., None) to the queue.\r\n  - execute() takes one new microbatch, and waits and returns the execution result of the oldest microbatch. Since we guarantee that the queue is always full, we can always get the result of the oldest microbatch immediately (but it may be None).\r\n\r\n#### Pros\r\n- The existing busy loop is (largely) unchanged, and all complexity is hidden at the executor level.\r\n- We still follow the \u201c(not really) synchronous schedule\u201d paradigm that submits one microbatch and receives the result of a (different) microbatch in the same synchronous function.\r\n\r\n#### Cons\r\n- **Degraded performance**: The oldest (finished) microbatch won\u2019t be fetched unless a new microbatch is scheduled. Although we continuously push empty microbatches when no new requests come in, this may still introduce overheads.\r\n- **Complexity in managing empty microbatches**: To achieve the desired pipeline efficiency, we have to push empty microbatches (None) to the microbatch queue when there are no requests scheduled (e.g., cold start, system idle, etc). Once we fail to maintain a full microbatch queue, the pipeline efficiency cannot be recovered unless we restart the engine.\r\n\r\n### Option 2 (Recommended): Two-stage engine loop\r\n\r\n![Async Scheduler (current)@2x](https://github.com/user-attachments/assets/32c030bc-81be-47ea-96a6-1ae7f4a7339a)\r\n\r\nSince pipeline parallelism enables multiple inflight executions in a pipelined fashion, scheduling and execution become _asynchronous by nature_: before one microbatch finishes execution, the engine needs to schedule and submit another microbatch. Therefore in this option, execute() is separated into two operations: submission and finish of the microbatch. Specifically, 4 operations are involved:\r\n- schedule(): the scheduler considers new requests and scheduelable existing requests and schedules the microbatch\r\n- submit(): the engine submits the microbatch to executor for execution\r\n- finish(): the executor finishes the execution of microbatch\r\n- update(): the scheduler updates its state based on finished microbatch execution output\r\n\r\n#### Design sketch\r\n- LLMEngineCore\r\n  - The busy loop is changed to use an async loop.\r\n  - The async loop is driven by the following events:\r\n    - New request comes in\r\n    - Existing request becomes schedulable\r\n    - Oldest microbatch finished\r\n  - The same code can run in synchronous fashion when microbatch_queue size is 1.\r\n\r\n- Ray Executor\r\n  - A pipeline executor that executes whatever microbatches it receives.\r\n\r\n#### Pros\r\n- Event driven and performant, because the oldest microbatch can finish as soon as possible.\r\n- A stepping stone to extend to a fully async scheduler.\r\n\r\n#### Cons\r\n- Changes the current synchronous busy loop.\r\n\r\n### Option 3: Virtual Engine\r\n\r\nThis is similar to the virtual engine solution in vLLM V0\r\n\r\n#### Pros\r\n- Convenient to implement.\r\n- Good isolation.\r\n\r\n#### Cons\r\n- Needs multiple schedulers, which are hard to manage and maintain.\r\n- Cannot reuse KV-cache from a different virtual engine; possible internal fragmentation.\r\n\r\n### Milestones for Option 2\r\n\r\nWe have the following milestones for achieving option 2.\r\n- Introduce async loop in LLMEngineCore\r\n- [In parallel] Support multiple microbatches (disjoint requests)\r\n- Implement pipeline-parallel\r\n- Optimization: support scheduling the same prefill-stage request in multiple inflight microbatches\r\n    - Note: for a request in decode stage, it can only be scheduled to one inflight microbatch before we figure out how to deal with speculative decoding and jump decoding; however, for a request in prefill stage, it can be scheduled to multiple inflight microbatches naturally, because prefill for later layers in later PP stage does not depend on the complete finish of the scheduled tokens.\r\n\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n@WoosukKwon @robertgshaw2-neuralmagic @tylertitsworth @youkaichao @simon-mo @comaniac @stephanie-wang \n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "stale",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-01-10T23:01:10+00:00",
    "closed_at": "2025-05-09T02:14:11+00:00",
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11945/reactions",
      "total_count": 8,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 7,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11945"
  },
  {
    "number": 11908,
    "title": "[RFC]: Implement Structured Output support for V1 engine",
    "body": "### Motivation.\n\nStructured Output is supported in v0, but not yet in v1. One reason for the delay is there have been performance challenges with the integration in v0, and we'd like to rethink the integration approach. We would also like to account for supporting additional techniques, jump decoding in particular, in the future.\n\nThe document below covers the proposed integration of the Structured Output functionality in V1 of the vLLM engine.\n\n\n### Proposed Change.\n\nA draft proposal can be found in this google doc: https://docs.google.com/document/d/1H6m_Y3FLJ1FYGCmjXdZzoJv-JCDSxnKuSY2XiAj-c6c/edit?tab=t.0\n\nThis content will eventually be moved into a PR as an addition to the design docs section of the vllm docs.\n\nRelated issue for closing xgrammar feature gaps: https://github.com/vllm-project/vllm/issues/12131\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n@mgoin @aarnphm @markmc @simon-mo @xuechendi @WoosukKwon \n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "structured-output",
      "RFC",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-01-09T23:41:04+00:00",
    "closed_at": "2025-03-10T13:28:30+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11908/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/11908"
  },
  {
    "number": 19097,
    "title": "[RFC]: Response format extensions for structured outputs",
    "body": "### Motivation.\n\nCurrently, users can provide additional constraints format via `extra_body` in OpenAI client:\n\n```python\nfrom enum import Enum\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nsimplified_sql_grammar = \"\"\"\n        root ::= select_statement\n\n        select_statement ::= \"SELECT \" column \" from \" table \" where \" condition\n\n        column ::= \"col_1 \" | \"col_2 \"\n\n        table ::= \"table_1 \" | \"table_2 \"\n\n        condition ::= column \"= \" number\n\n        number ::= \"1 \" | \"2 \"\n    \"\"\"\n\nprompt = (\n        \"Generate an SQL query to show the 'username' and 'email'\"\n        \"from the 'users' table.\"\n    )\n\ncompletion = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": prompt,\n            }\n        ],\n        extra_body={\"guided_grammar\": simplified_sql_grammar},\n```\n\nThis also applies with `guided_json`, `structural_tag`, `guided_regex`.\n\nWhile this is pretty convenient for most developers, these fields are still using v0 terminology wrt guided decoding.\n\nWith the upcoming v0 deprecation, I think it is the time to have a usage update with this pattern.\n\n\n### Proposed Change.\n\nOpenAI already recommends users to use `response_format` with [json_schema](https://platform.openai.com/docs/guides/structured-outputs?api-mode=chat)\n\nGiven that we already supports `structural_tag` via `response_format` ([example](https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_structured_outputs_structural_tag.html?h=structural+tags)), I propose an extension to `response_format` for the remainder of the fields\n\n```python\ncompletion = client.chat.completions.create(\n  model=model, messages=messages,\n  response_format={\n    \"type\": \"vllm_regex\",\n    \"regex\": r\"\\w+@\\w+\\.com\\n\"\n  }\n)\n\ncompletion = client.chat.completions.create(\n  model=model, messages=messages,\n  response_format={\n    \"type\": \"vllm_grammar\",\n    \"grammar\": \"\"\"\nroot ::= select_statement\nselect_statement ::= \"SELECT \" column \" from \" table \" where \" condition\ncolumn ::= \"col_1 \" | \"col_2 \"\ntable ::= \"table_1 \" | \"table_2 \"\ncondition ::= column \"= \" number\nnumber ::= \"1 \" | \"2 \"\n\"\"\"\n  }\n\ncompletion = client.chat.completions.create(\n  model=model, messages=messages,\n  response_format={\n    \"type\": \"vllm_choice\",\n    \"choice\": [\"Positive\", \"Negative\", \"Neutral\"], \n  }\n)\n```\n\nThe previous json_schema + structural tag remains the same.\n\nThe field `guided_*` will still works previously, but will reserved only for more advance usage and won't be documented.\n\n### Feedback Period.\n\n1 week for revision, 2-3 days for implementations plan (mostly frontend + protocol updates)\n\nWe can also add a debug log recommending using this new pattern for all existing usage of `guided_*` (so that we won't break production)\n\n### CC List.\n\n@russellb @mgoin @simon-mo @hmellor \n\n### Any Other Things.\n\nThis is mostly frontend changes\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "structured-output",
      "RFC",
      "v1"
    ],
    "state": "open",
    "created_at": "2025-06-03T17:05:05+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19097/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19097"
  },
  {
    "number": 16348,
    "title": "[Bug]: Missing metrics  in V1",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\nClang version: Could not collect\nCMake version: version 3.18.4\nLibc version: glibc-2.31\n\nPython version: 3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:16:10) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-5.10.0-34-cloud-amd64-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA L4\nGPU 1: NVIDIA L4\n\nNvidia driver version: 550.90.07\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nByte Order:                           Little Endian\nAddress sizes:                        46 bits physical, 48 bits virtual\nCPU(s):                               24\nOn-line CPU(s) list:                  0-23\nThread(s) per core:                   2\nCore(s) per socket:                   12\nSocket(s):                            1\nNUMA node(s):                         1\nVendor ID:                            GenuineIntel\nCPU family:                           6\nModel:                                85\nModel name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\nStepping:                             7\nCPU MHz:                              2200.194\nBogoMIPS:                             4400.38\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            384 KiB\nL1i cache:                            384 KiB\nL2 cache:                             12 MiB\nL3 cache:                             38.5 MiB\nNUMA node0 CPU(s):                    0-23\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; Enhanced IBRS\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n\nVersions of relevant libraries:\n[pip3] mypy==1.15.0\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.3.0\n[pip3] sentence-transformers==3.4.1\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.1\n[pip3] triton==3.2.0\n[conda] numpy                     1.25.2                   pypi_0    pypi\n[conda] nvidia-ml-py              11.495.46                pypi_0    pypi\n[conda] pyzmq                     26.3.0                   pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      PHB     0-23    0               N/A\nGPU1    PHB      X      0-23    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n\nThis is my sample `script.py`\n\n```python\nfrom vllm import SamplingParams, AsyncEngineArgs, AsyncLLMEngine\nimport uuid\nimport asyncio\nimport time\n\nasync def main():\n    \"\"\"\n    Simulates sending 100 concurrent requests to a vLLM inference engine.\n    \n    :returns: None\n    \"\"\"\n    # Configure sampling parameters\n    sampling_params = SamplingParams(\n        max_tokens=100,\n    )\n    \n    # Configure engine\n    engine_args = AsyncEngineArgs(\n        model=\"facebook/opt-125m\",\n        enforce_eager=True,\n        gpu_memory_utilization=0.90,\n        disable_log_requests=True,\n        disable_log_stats=False\n    )\n    \n    # Initialize engine\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\n    \n    # Define request processing coroutine\n    async def process_request(request_id):\n        \"\"\"\n        Process a single request using the vLLM engine.\n        \n        :param request_id: Unique identifier for the request\n        :returns: Generated text output\n        \"\"\"\n        prompt = f\"Hello, how are you? This is request {request_id}\"\n        \n        # Send request to the engine\n        result_generator = engine.generate(\n            prompt=prompt,\n            sampling_params=sampling_params,\n            request_id=str(request_id),\n            \n        )\n        \n        # Process the streaming results\n        final_output = None\n        async for request_output in result_generator:\n            final_output = request_output\n            \n        return final_output\n    \n    # Create 10000 concurrent tasks\n    print(f\"Starting 10000 concurrent requests\")\n    \n    tasks = []\n    for i in range(10000):\n        request_id = uuid.uuid4()\n        tasks.append(process_request(request_id))\n    \n    # Wait for all tasks to complete\n    await asyncio.gather(*tasks)\n    \nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\nWhen I run `VLLM_USE_V1=0 python script.py`\n\nI do see the aggregate throughput metrics being regularly shown to me:\n\n```bash\n...\nINFO 04-09 14:25:03 [worker.py:267] model weights take 0.24GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 19.00GiB.\nINFO 04-09 14:25:03 [executor_base.py:112] # cuda blocks: 34595, # CPU blocks: 7281\nINFO 04-09 14:25:03 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.27x\nINFO 04-09 14:25:06 [llm_engine.py:448] init engine (profile, create kv cache, warmup model) took 4.49 seconds\nStarting 10000 concurrent requests\nINFO 04-09 14:25:12 [metrics.py:488] Avg prompt throughput: 1899.6 tokens/s, Avg generation throughput: 975.1 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 9725 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.\nINFO 04-09 14:25:17 [metrics.py:488] Avg prompt throughput: 2006.7 tokens/s, Avg generation throughput: 5044.2 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 9435 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\nINFO 04-09 14:25:22 [metrics.py:488] Avg prompt throughput: 1859.2 tokens/s, Avg generation throughput: 4441.8 tokens/s, Running: 255 reqs, Swapped: 0 reqs, Pending: 9166 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%\n```\n\nhowever when `VLLM_USE_V1=1python script.py`\n\nI only see:\n\n```bash\nINFO 04-09 14:27:52 [loader.py:447] Loading weights took 0.21 seconds\nINFO 04-09 14:27:52 [gpu_model_runner.py:1273] Model loading took 0.2393 GiB and 0.666895 seconds\nINFO 04-09 14:27:53 [kv_cache_utils.py:578] GPU KV cache size: 502,304 tokens\nINFO 04-09 14:27:53 [kv_cache_utils.py:581] Maximum concurrency for 2,048 tokens per request: 245.27x\nINFO 04-09 14:27:53 [core.py:162] init engine (profile, create kv cache, warmup model) took 0.97 seconds\nStarting 10000 concurrent requests\n```\n\nand then nothing until completion.\n\nI research the documentation together with vLLM chatbot and came across this option:\n\n`VLLM_USE_V1=1 VLLM_LOGGING_CONFIG_PATH=\"logging.json\" script.py`\n\nwhich outputs:\n\n```bash\n{\"message\": \"Loading weights took 0.21 seconds\"}\n{\"message\": \"Model loading took 0.2393 GiB and 0.746670 seconds\"}\n{\"message\": \"GPU KV cache size: 502,304 tokens\"}\n{\"message\": \"Maximum concurrency for 2,048 tokens per request: 245.27x\"}\n{\"message\": \"init engine (profile, create kv cache, warmup model) took 0.67 seconds\"}\nStarting 10000 concurrent requests\n```\nand then nothing.\n\nI know that not too long ago the metrics for V1 were still in WiP state: https://github.com/vllm-project/vllm/issues/10582\n\nBut according to the changelog, `0.8.3` seems to be already feature-complete in that regard.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "v1"
    ],
    "state": "open",
    "created_at": "2025-04-09T14:38:15+00:00",
    "closed_at": null,
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16348/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16348"
  },
  {
    "number": 16054,
    "title": "[Bug]: CI flake - v1/engine/test_async_llm.py::test_abort - assert has_unfinished_requests()",
    "body": "### Your current environment\n\n...\n\n### \ud83d\udc1b Describe the bug\n\nmain commit 51d7c6a2b23e100cd9e7d85b8e7c0eea656b331e\n\nSeen in https://github.com/vllm-project/vllm/pull/15894\n\nhttps://buildkite.com/organizations/vllm/pipelines/ci/builds/16742/jobs/0195f24d-e81a-46a3-ad08-6a51983d65d6/log\n\n\n```\n=================================== FAILURES ===================================\n[2025-04-01T17:38:12Z] _ test_abort[engine_args0-Hello my name is Robert and-RequestOutputKind.DELTA] _\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z] monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fd1fa052e70>\n[2025-04-01T17:38:12Z] output_kind = <RequestOutputKind.DELTA: 1>\n[2025-04-01T17:38:12Z] engine_args = AsyncEngineArgs(model='meta-llama/Llama-3.2-1B-Instruct', served_model_name=None, tokenizer='meta-llama/Llama-3.2-1B-I...additional_config=None, enable_reasoning=None, reasoning_parser=None, use_tqdm_on_load=True, disable_log_requests=True)\n[2025-04-01T17:38:12Z] prompt = 'Hello my name is Robert and'\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]     @pytest.mark.parametrize(\n[2025-04-01T17:38:12Z]         \"output_kind\", [RequestOutputKind.DELTA, RequestOutputKind.FINAL_ONLY])\n[2025-04-01T17:38:12Z]     @pytest.mark.parametrize(\"engine_args,prompt\",\n[2025-04-01T17:38:12Z]                              [(TEXT_ENGINE_ARGS, TEXT_PROMPT),\n[2025-04-01T17:38:12Z]                               (VISION_ENGINE_ARGS, VISION_PROMPT)])\n[2025-04-01T17:38:12Z]     @pytest.mark.asyncio\n[2025-04-01T17:38:12Z]     async def test_abort(monkeypatch: pytest.MonkeyPatch,\n[2025-04-01T17:38:12Z]                          output_kind: RequestOutputKind,\n[2025-04-01T17:38:12Z]                          engine_args: AsyncEngineArgs, prompt: PromptType):\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]         with monkeypatch.context() as m, ExitStack() as after:\n[2025-04-01T17:38:12Z]             m.setenv(\"VLLM_USE_V1\", \"1\")\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             engine = AsyncLLM.from_engine_args(engine_args)\n[2025-04-01T17:38:12Z]             after.callback(engine.shutdown)\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             NUM_REQUESTS = 100\n[2025-04-01T17:38:12Z]             NUM_EXPECTED_TOKENS = 100\n[2025-04-01T17:38:12Z]             NUM_EXPECTED_TOKENS_LONG = 50000\n[2025-04-01T17:38:12Z]             REQUEST_IDS_TO_ABORT = range(1, 100, 10)\n[2025-04-01T17:38:12Z]             PARALLEL_SAMPLE_REQ_IDS = range(1, 100, 15)\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             request_ids = [f\"request-{i}\" for i in range(NUM_REQUESTS)]\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             # Create concurrent requests.\n[2025-04-01T17:38:12Z]             tasks: list[asyncio.Task] = []\n[2025-04-01T17:38:12Z]             for idx, request_id in enumerate(request_ids):\n[2025-04-01T17:38:12Z]                 max_tokens = NUM_EXPECTED_TOKENS_LONG if (\n[2025-04-01T17:38:12Z]                     idx in REQUEST_IDS_TO_ABORT) else NUM_EXPECTED_TOKENS\n[2025-04-01T17:38:12Z]                 n = 3 if idx in PARALLEL_SAMPLE_REQ_IDS else 1\n[2025-04-01T17:38:12Z]                 tasks.append(\n[2025-04-01T17:38:12Z]                     asyncio.create_task(\n[2025-04-01T17:38:12Z]                         generate(engine, request_id, prompt, output_kind,\n[2025-04-01T17:38:12Z]                                  max_tokens, n)))\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             # API server cancels requests when they disconnect.\n[2025-04-01T17:38:12Z]             for idx in REQUEST_IDS_TO_ABORT:\n[2025-04-01T17:38:12Z]                 tasks[idx].cancel()\n[2025-04-01T17:38:12Z]                 await asyncio.sleep(0.1)\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             # Confirm the other requests are okay.\n[2025-04-01T17:38:12Z]             for idx, task in enumerate(tasks):\n[2025-04-01T17:38:12Z]                 # Confirm that it was actually canceled.\n[2025-04-01T17:38:12Z]                 if idx in REQUEST_IDS_TO_ABORT:\n[2025-04-01T17:38:12Z]                     with pytest.raises(asyncio.CancelledError):\n[2025-04-01T17:38:12Z]                         await task\n[2025-04-01T17:38:12Z]                 else:\n[2025-04-01T17:38:12Z]                     # Otherwise, make sure the request was not impacted.\n[2025-04-01T17:38:12Z]                     num_generated_tokens, request_id = await task\n[2025-04-01T17:38:12Z]                     n = 3 if idx in PARALLEL_SAMPLE_REQ_IDS else 1\n[2025-04-01T17:38:12Z]                     expected_tokens = NUM_EXPECTED_TOKENS * n\n[2025-04-01T17:38:12Z]                     assert num_generated_tokens == expected_tokens, (\n[2025-04-01T17:38:12Z]                         f\"{request_id} generated {num_generated_tokens} but \"\n[2025-04-01T17:38:12Z]                         f\"expected {expected_tokens}\")\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             # Make sure all aborted requests were really aborted.\n[2025-04-01T17:38:12Z] >           assert not engine.output_processor.has_unfinished_requests()\n[2025-04-01T17:38:12Z] E           assert not True\n[2025-04-01T17:38:12Z] E            +  where True = has_unfinished_requests()\n[2025-04-01T17:38:12Z] E            +    where has_unfinished_requests = <vllm.v1.engine.output_processor.OutputProcessor object at 0x7fd1ef1614c0>.has_unfinished_requests\n[2025-04-01T17:38:12Z] E            +      where <vllm.v1.engine.output_processor.OutputProcessor object at 0x7fd1ef1614c0> = <vllm.v1.engine.async_llm.AsyncLLM object at 0x7fd1ef132750>.output_processor\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z] v1/engine/test_async_llm.py:178: AssertionError\n[2025-04-01T17:38:12Z] =============================== warnings summary ===============================\n[2025-04-01T17:38:12Z] tests/v1/engine/test_async_llm.py: 12 warnings\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core_client.py: 1 warning\n[2025-04-01T17:38:12Z] tests/v1/engine/test_llm_engine.py: 2 warnings\n[2025-04-01T17:38:12Z]   /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1700) is multi-threaded, use of fork() may lead to deadlocks in the child.\n[2025-04-01T17:38:12Z]     self.pid = os.fork()\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core.py::test_engine_core\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core.py::test_engine_core_advanced_sampling\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core.py::test_engine_core_concurrent_batches\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core_client.py::test_engine_core_client[True]\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core_client.py::test_engine_core_client[False]\n[2025-04-01T17:38:12Z]   /vllm-workspace/tests/utils.py:720: DeprecationWarning: This process (pid=1700) is multi-threaded, use of fork() may lead to deadlocks in the child.\n[2025-04-01T17:38:12Z]     pid = os.fork()\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z] -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n[2025-04-01T17:38:12Z] =========================== short test summary info ============================\n[2025-04-01T17:38:12Z] FAILED v1/engine/test_async_llm.py::test_abort[engine_args0-Hello my name is Robert and-RequestOutputKind.DELTA] - assert not True\n[2025-04-01T17:38:12Z]  +  where True = has_unfinished_requests()\n[2025-04-01T17:38:12Z]  +    where has_unfinished_requests = <vllm.v1.engine.output_processor.OutputProcessor object at 0x7fd1ef1614c0>.has_unfinished_requests\n[2025-04-01T17:38:12Z]  +      where <vllm.v1.engine.output_processor.OutputProcessor object at 0x7fd1ef1614c0> = <vllm.v1.engine.async_llm.AsyncLLM object at 0x7fd1ef132750>.output_processor\n[2025-04-01T17:38:12Z] ============ 1 failed, 44 passed, 20 warnings in 1059.59s (0:17:39) ============\n[2025-04-01T17:38:14Z] \ud83d\udea8 Error: The command exited with status 1\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci/build",
      "stale",
      "v1"
    ],
    "state": "open",
    "created_at": "2025-04-04T09:48:13+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16054/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/16054"
  },
  {
    "number": 16053,
    "title": "[Bug]: CI flake - v1/entrypoints/llm/test_struct_output_generate.py::test_structured_output - JSONDecodeError: Expecting value: line 1 column 1 (char 0)",
    "body": "### Your current environment\n\n...\n\n\n### \ud83d\udc1b Describe the bug\n\nmain commit 51d7c6a2b23e100cd9e7d85b8e7c0eea656b331e\n\nSeen in https://github.com/vllm-project/vllm/pull/15894\n\nhttps://buildkite.com/organizations/vllm/pipelines/ci/builds/16742/jobs/0195fc58-3d11-45b5-b76f-8e962cbda765/log\n\n```\nFAILED v1/entrypoints/llm/test_struct_output_generate.py::test_structured_output[Qwen/Qwen2.5-1.5B-Instruct-guidance:disable-any-whitespace-auto] - json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n[2025-04-03T16:08:35Z] _ test_structured_output[Qwen/Qwen2.5-1.5B-Instruct-guidance:disable-any-whitespace-auto] _\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z] monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f318d89eb40>\n[2025-04-03T16:08:35Z] sample_json_schema = {'properties': {'age': {'type': 'integer'}, 'name': {'type': 'string'}, 'skills': {'items': {'type': 'string'}, 'type'...ition'], 'type': 'object'}, 'type': 'array'}}, 'required': ['name', 'age', 'skills', 'work_history'], 'type': 'object'}\n[2025-04-03T16:08:35Z] unsupported_json_schema = {'properties': {'email': {'pattern': '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$', 'type': 'string'}, 'grade': ...[a-z]{1,10}$', 'type': 'string'}, 'type': 'array'}}, 'required': ['score', 'grade', 'email', 'tags'], 'type': 'object'}\n[2025-04-03T16:08:35Z] sample_sql_ebnf = '\\nroot ::= select_statement\\nselect_statement ::= \"SELECT\" column \"from\" table \"where\" condition\\ncolumn ::= \"col_1\" | \"col_2\"\\ntable ::= \"table_1\" | \"table_2\"\\ncondition ::= column \"=\" number\\nnumber ::= \"1\" | \"2\"\\n'\n[2025-04-03T16:08:35Z] sample_sql_lark = '\\nstart: select_statement\\nselect_statement: \"SELECT\" column \"from\" table \"where\" condition\\ncolumn: \"col_1\" | \"col_2\"\\ntable: \"table_1\" | \"table_2\"\\ncondition: column \"=\" number\\nnumber: \"1\" | \"2\"\\n'\n[2025-04-03T16:08:35Z] sample_regex = '((25[0-5]|(2[0-4]|1\\\\d|[1-9]|)\\\\d)\\\\.){3}(25[0-5]|(2[0-4]|1\\\\d|[1-9]|)\\\\d)'\n[2025-04-03T16:08:35Z] sample_guided_choice = ['Python', 'Java', 'JavaScript', 'C++', 'C#', 'PHP', ...]\n[2025-04-03T16:08:35Z] guided_decoding_backend = 'guidance:disable-any-whitespace'\n[2025-04-03T16:08:35Z] tokenizer_mode = 'auto', model_name = 'Qwen/Qwen2.5-1.5B-Instruct'\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]     @pytest.mark.skip_global_cleanup\n[2025-04-03T16:08:35Z]     @pytest.mark.parametrize(\"model_name, guided_decoding_backend, tokenizer_mode\",\n[2025-04-03T16:08:35Z]                              PARAMS_MODELS_BACKENDS_TOKENIZER_MODE)\n[2025-04-03T16:08:35Z]     def test_structured_output(\n[2025-04-03T16:08:35Z]         monkeypatch: pytest.MonkeyPatch,\n[2025-04-03T16:08:35Z]         sample_json_schema: dict[str, Any],\n[2025-04-03T16:08:35Z]         unsupported_json_schema: dict[str, Any],\n[2025-04-03T16:08:35Z]         sample_sql_ebnf: str,\n[2025-04-03T16:08:35Z]         sample_sql_lark: str,\n[2025-04-03T16:08:35Z]         sample_regex: str,\n[2025-04-03T16:08:35Z]         sample_guided_choice: str,\n[2025-04-03T16:08:35Z]         guided_decoding_backend: str,\n[2025-04-03T16:08:35Z]         tokenizer_mode: str,\n[2025-04-03T16:08:35Z]         model_name: str,\n[2025-04-03T16:08:35Z]     ):\n[2025-04-03T16:08:35Z]         monkeypatch.setenv(\"VLLM_USE_V1\", \"1\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         # Use a single LLM instance for several scenarios to\n[2025-04-03T16:08:35Z]         # speed up the test suite.\n[2025-04-03T16:08:35Z]         llm = LLM(model=model_name,\n[2025-04-03T16:08:35Z]                   enforce_eager=True,\n[2025-04-03T16:08:35Z]                   max_model_len=1024,\n[2025-04-03T16:08:35Z]                   guided_decoding_backend=guided_decoding_backend,\n[2025-04-03T16:08:35Z]                   tokenizer_mode=tokenizer_mode)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 1: Generate JSON output based on a provided schema\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=1.0,\n[2025-04-03T16:08:35Z]             max_tokens=1000,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(json=sample_json_schema))\n[2025-04-03T16:08:35Z]         outputs = llm.generate(prompts=[\n[2025-04-03T16:08:35Z]             f\"Give an example JSON for an employee profile \"\n[2025-04-03T16:08:35Z]             f\"that fits this schema: {sample_json_schema}\"\n[2025-04-03T16:08:35Z]         ] * 2,\n[2025-04-03T16:08:35Z]                                sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]                                use_tqdm=True)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]             prompt = output.prompt\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]             assert generated_text is not None\n[2025-04-03T16:08:35Z]             if 'disable-any-whitespace' in guided_decoding_backend:\n[2025-04-03T16:08:35Z]                 assert \"\\n\" not in generated_text\n[2025-04-03T16:08:35Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n[2025-04-03T16:08:35Z]             output_json = json.loads(generated_text)\n[2025-04-03T16:08:35Z]             jsonschema.validate(instance=output_json, schema=sample_json_schema)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 2: Generate JSON object without a schema\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=1.0,\n[2025-04-03T16:08:35Z]             max_tokens=100,\n[2025-04-03T16:08:35Z]             n=2,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(json_object=True))\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         outputs = llm.generate(\n[2025-04-03T16:08:35Z]             prompts=(\"Generate a JSON object with curly braces for a person with \"\n[2025-04-03T16:08:35Z]                      \"name and age fields for John Smith who is 31 years old.\"),\n[2025-04-03T16:08:35Z]             sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]             use_tqdm=True)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             for i in range(2):\n[2025-04-03T16:08:35Z]                 generated_text = output.outputs[i].text\n[2025-04-03T16:08:35Z]                 print(generated_text)\n[2025-04-03T16:08:35Z]                 assert generated_text is not None\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]                 # Parse to verify it is valid JSON\n[2025-04-03T16:08:35Z]                 parsed_json = json.loads(generated_text)\n[2025-04-03T16:08:35Z]                 allowed_types: tuple[type, ...] = (dict, )\n[2025-04-03T16:08:35Z]                 if guided_decoding_backend.startswith(\"xgrammar\"):\n[2025-04-03T16:08:35Z]                     # TODO - we are currently too permissive with xgrammar and\n[2025-04-03T16:08:35Z]                     # allow # any valid json (typically comes back as a list or\n[2025-04-03T16:08:35Z]                     # object).  We can fix this by specifying a jsonschema of\n[2025-04-03T16:08:35Z]                     # {\"type\": \"object\"}, # but we need this fix in a release\n[2025-04-03T16:08:35Z]                     # first: https://github.com/mlc-ai/xgrammar/pull/264\n[2025-04-03T16:08:35Z]                     allowed_types = (dict, list)\n[2025-04-03T16:08:35Z]                 assert isinstance(parsed_json, allowed_types)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 3: test a jsonschema incompatible with xgrammar\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=1.0,\n[2025-04-03T16:08:35Z]             max_tokens=1000,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(json=unsupported_json_schema))\n[2025-04-03T16:08:35Z]         if guided_decoding_backend.startswith(\"xgrammar\"):\n[2025-04-03T16:08:35Z]             with pytest.raises(ValueError,\n[2025-04-03T16:08:35Z]                                match=\"The provided JSON schema contains features \"\n[2025-04-03T16:08:35Z]                                \"not supported by xgrammar.\"):\n[2025-04-03T16:08:35Z]                 llm.generate(prompts=[\n[2025-04-03T16:08:35Z]                     f\"Give an example JSON for an employee profile \"\n[2025-04-03T16:08:35Z]                     f\"that fits this schema: {unsupported_json_schema}\"\n[2025-04-03T16:08:35Z]                 ] * 2,\n[2025-04-03T16:08:35Z]                              sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]                              use_tqdm=True)\n[2025-04-03T16:08:35Z]         else:\n[2025-04-03T16:08:35Z]             outputs = llm.generate(\n[2025-04-03T16:08:35Z]                 prompts=(\"Give an example JSON object for a grade \"\n[2025-04-03T16:08:35Z]                          \"that fits this schema: \"\n[2025-04-03T16:08:35Z]                          f\"{unsupported_json_schema}\"),\n[2025-04-03T16:08:35Z]                 sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]                 use_tqdm=True)\n[2025-04-03T16:08:35Z]             assert outputs is not None\n[2025-04-03T16:08:35Z]             for output in outputs:\n[2025-04-03T16:08:35Z]                 assert output is not None\n[2025-04-03T16:08:35Z]                 assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]                 generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]                 assert generated_text is not None\n[2025-04-03T16:08:35Z]                 print(generated_text)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]                 # Parse to verify it is valid JSON\n[2025-04-03T16:08:35Z]                 parsed_json = json.loads(generated_text)\n[2025-04-03T16:08:35Z]                 assert isinstance(parsed_json, dict)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 4: Generate SQL statement using EBNF grammar\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=0.8,\n[2025-04-03T16:08:35Z]             top_p=0.95,\n[2025-04-03T16:08:35Z]             max_tokens=1000,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(grammar=sample_sql_ebnf))\n[2025-04-03T16:08:35Z]         outputs = llm.generate(\n[2025-04-03T16:08:35Z]             prompts=(\"Generate a sql statement that selects col_1 from \"\n[2025-04-03T16:08:35Z]                      \"table_1 where it is equal to 1\"),\n[2025-04-03T16:08:35Z]             sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]             use_tqdm=True,\n[2025-04-03T16:08:35Z]         )\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]             prompt = output.prompt\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]             assert generated_text is not None\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             # remove spaces for comparison b/c we removed them in the grammar\n[2025-04-03T16:08:35Z]             ground_truth = \"SELECT col_1 from table_1 where col_1 = 1\".replace(\n[2025-04-03T16:08:35Z]                 \" \", \"\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             assert generated_text.strip() == ground_truth\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 5: Generate SQL statement using Lark grammar\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=0.8,\n[2025-04-03T16:08:35Z]             top_p=0.95,\n[2025-04-03T16:08:35Z]             max_tokens=1000,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(grammar=sample_sql_lark))\n[2025-04-03T16:08:35Z]         outputs = llm.generate(\n[2025-04-03T16:08:35Z]             prompts=(\"Generate a sql statement that selects col_1 from \"\n[2025-04-03T16:08:35Z]                      \"table_1 where it is equal to 1\"),\n[2025-04-03T16:08:35Z]             sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]             use_tqdm=True,\n[2025-04-03T16:08:35Z]         )\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]             prompt = output.prompt\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]             assert generated_text is not None\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             # use Lark to parse the output, and make sure it's a valid parse tree\n[2025-04-03T16:08:35Z]             from lark import Lark\n[2025-04-03T16:08:35Z]             parser = Lark(sample_sql_lark)\n[2025-04-03T16:08:35Z]             parser.parse(generated_text)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             # remove spaces for comparison b/c we removed them in the grammar\n[2025-04-03T16:08:35Z]             ground_truth = \"SELECT col_1 from table_1 where col_1 = 1\".replace(\n[2025-04-03T16:08:35Z]                 \" \", \"\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             assert generated_text.strip() == ground_truth\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 6: Test invalid grammar input\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=0.8,\n[2025-04-03T16:08:35Z]             top_p=0.95,\n[2025-04-03T16:08:35Z]             max_tokens=1000,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(grammar=\"not a grammar\"))\n[2025-04-03T16:08:35Z]         with pytest.raises(ValueError, match=\"Failed to convert the grammar \"):\n[2025-04-03T16:08:35Z]             llm.generate(\n[2025-04-03T16:08:35Z]                 prompts=(\"Generate a sql statement that selects col_1 from \"\n[2025-04-03T16:08:35Z]                          \"table_1 where it is equal to 1\"),\n[2025-04-03T16:08:35Z]                 sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]                 use_tqdm=True,\n[2025-04-03T16:08:35Z]             )\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 7: Generate text based on a regex pattern\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=0.8,\n[2025-04-03T16:08:35Z]             top_p=0.95,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(regex=sample_regex))\n[2025-04-03T16:08:35Z]         outputs = llm.generate(\n[2025-04-03T16:08:35Z]             prompts=[\n[2025-04-03T16:08:35Z]                 f\"Give an example IPv4 address with this regex: {sample_regex}\"\n[2025-04-03T16:08:35Z]             ] * 2,\n[2025-04-03T16:08:35Z]             sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]             use_tqdm=True,\n[2025-04-03T16:08:35Z]         )\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]             prompt = output.prompt\n[2025-04-03T16:08:35Z]             generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]             print(generated_text)\n[2025-04-03T16:08:35Z]             assert generated_text is not None\n[2025-04-03T16:08:35Z]             assert re.fullmatch(sample_regex, generated_text) is not None\n[2025-04-03T16:08:35Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 8: Generate text based on a choices\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=0.8,\n[2025-04-03T16:08:35Z]             top_p=0.95,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(choice=sample_guided_choice))\n[2025-04-03T16:08:35Z]         outputs = llm.generate(\n[2025-04-03T16:08:35Z]             prompts=\"The best language for type-safe systems programming is \",\n[2025-04-03T16:08:35Z]             sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]             use_tqdm=True)\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]             prompt = output.prompt\n[2025-04-03T16:08:35Z]             generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]             print(generated_text)\n[2025-04-03T16:08:35Z]             assert generated_text is not None\n[2025-04-03T16:08:35Z]             assert generated_text in sample_guided_choice\n[2025-04-03T16:08:35Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 9: Generate structured output using a Pydantic model with an enum\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         json_schema = CarDescription.model_json_schema()\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=1.0,\n[2025-04-03T16:08:35Z]             max_tokens=1000,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(json=json_schema))\n[2025-04-03T16:08:35Z]         outputs = llm.generate(\n[2025-04-03T16:08:35Z]             prompts=\"Generate a JSON with the brand, model and car_type of\"\n[2025-04-03T16:08:35Z]             \"the most iconic car from the 90's\",\n[2025-04-03T16:08:35Z]             sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]             use_tqdm=True)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]             prompt = output.prompt\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]             assert generated_text is not None\n[2025-04-03T16:08:35Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n[2025-04-03T16:08:35Z] >           output_json = json.loads(generated_text)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z] v1/entrypoints/llm/test_struct_output_generate.py:332:\n[2025-04-03T16:08:35Z] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n[2025-04-03T16:08:35Z] /usr/lib/python3.12/json/__init__.py:346: in loads\n[2025-04-03T16:08:35Z]     return _default_decoder.decode(s)\n[2025-04-03T16:08:35Z] /usr/lib/python3.12/json/decoder.py:338: in decode\n[2025-04-03T16:08:35Z]     obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n[2025-04-03T16:08:35Z] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z] self = <json.decoder.JSONDecoder object at 0x7f32ee3035c0>, s = '', idx = 0\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]     def raw_decode(self, s, idx=0):\n[2025-04-03T16:08:35Z]         \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n[2025-04-03T16:08:35Z]         a JSON document) and return a 2-tuple of the Python\n[2025-04-03T16:08:35Z]         representation and the index in ``s`` where the document ended.\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         This can be used to decode a JSON document from a string that may\n[2025-04-03T16:08:35Z]         have extraneous data at the end.\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         \"\"\"\n[2025-04-03T16:08:35Z]         try:\n[2025-04-03T16:08:35Z]             obj, end = self.scan_once(s, idx)\n[2025-04-03T16:08:35Z]         except StopIteration as err:\n[2025-04-03T16:08:35Z] >           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n[2025-04-03T16:08:35Z] E           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z] /usr/lib/python3.12/json/decoder.py:356: JSONDecodeError\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci/build",
      "stale",
      "v1"
    ],
    "state": "open",
    "created_at": "2025-04-04T09:46:16+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16053/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/16053"
  },
  {
    "number": 15901,
    "title": "[SpecDecode] Support EAGLE in V1",
    "body": "- [x] 1. Correctly initializing and loading the EAGLE draft model\n- [x] 2. Consider the lookahead slots in the KV cache manager\n- [x] 3. Cache `draft_probs` inside the model runner and correctly feed it to the rejection sampler in the next step (temporarily workaround: #16899)\n- [x] 4. Handle the edge cases like when the draft model generates beyond `max_pos_embeddings`\n- [ ] 5. Handle the seeds correctly\n- [ ] 6. Do E2E correctness and performance tests\n- [x] 7. Support prefix caching. Eagle requires special handling because Eagle's i-th KV cache is coupled with the i+1-th token ID. (@LiuXiaoxuanPKU)\n- [ ] 8. Properly handle the sampling parameters that are not (currently) compatible with spec decoding (e.g., min_p).\n- [x] 9. Use CUDA graphs for draft model. (@luyuzhe111)\n- [x] 10. Support Eagle 3 (https://github.com/vllm-project/vllm/pull/16937)\n\n_Originally posted by @WoosukKwon in https://github.com/vllm-project/vllm/issues/15729#issuecomment-2765192455_\n            ",
    "labels": [
      "speculative-decoding",
      "v1"
    ],
    "state": "open",
    "created_at": "2025-04-01T19:45:13+00:00",
    "closed_at": null,
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15901/reactions",
      "total_count": 11,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 10,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15901"
  },
  {
    "number": 15855,
    "title": "[Bug]: CI flake - v1/engine/test_llm_engine.py::test_parallel_sampling[True]",
    "body": "### Your current environment\n\n...\n\n\n### \ud83d\udc1b Describe the bug\n\nSaw V1 test failing with this yesterday, went away with recheck:\n\n```\n[2025-03-31T17:33:47Z] _________________________ test_parallel_sampling[True] _________________________\n[2025-03-31T17:33:47Z]\n[2025-03-31T17:33:47Z] vllm_model = <tests.conftest.VllmRunner object at 0x7f0d875e06e0>\n[2025-03-31T17:33:47Z] example_prompts = ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the majo...me.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', ...]\n[2025-03-31T17:33:47Z]\n[2025-03-31T17:33:47Z]     def test_parallel_sampling(vllm_model, example_prompts) -> None:\n[2025-03-31T17:33:47Z]         \"\"\"Test passes if parallel sampling `n>1` yields `n` unique completions.\n[2025-03-31T17:33:47Z]\n[2025-03-31T17:33:47Z]         Args:\n[2025-03-31T17:33:47Z]           vllm_model: VllmRunner instance under test.\n[2025-03-31T17:33:47Z]           example_prompt: test fixture providing prompts for testing.\n[2025-03-31T17:33:47Z]         \"\"\"\n[2025-03-31T17:33:47Z]         sampling_params_list, n_list = _get_test_sampling_params(example_prompts)\n[2025-03-31T17:33:47Z]         model: LLM = vllm_model.model\n[2025-03-31T17:33:47Z]         outputs = model.generate(example_prompts, sampling_params_list)\n[2025-03-31T17:33:47Z]\n[2025-03-31T17:33:47Z]         # Validate each request response\n[2025-03-31T17:33:47Z]         for out, n in zip(outputs, n_list):\n[2025-03-31T17:33:47Z]             completion_counts: dict[str, int] = {}\n[2025-03-31T17:33:47Z]             # Assert correct number of completions\n[2025-03-31T17:33:47Z]             assert len(out.outputs) == n, (\n[2025-03-31T17:33:47Z]                 f\"{len(out.outputs)} completions; {n} expected.\")\n[2025-03-31T17:33:47Z]             for idx in range(n):\n[2025-03-31T17:33:47Z]                 comp = out.outputs[idx]\n[2025-03-31T17:33:47Z]                 # Assert correct completion indices\n[2025-03-31T17:33:47Z]                 assert comp.index == idx, (f\"Index {comp.index}; expected {idx}.\")\n[2025-03-31T17:33:47Z]                 text = comp.text\n[2025-03-31T17:33:47Z]                 completion_counts[text] = completion_counts.get(text, 0) + 1\n[2025-03-31T17:33:47Z]             # Assert unique completions\n[2025-03-31T17:33:47Z]             if len(completion_counts) != n:\n[2025-03-31T17:33:47Z]                 repeats = {\n[2025-03-31T17:33:47Z]                     txt: num\n[2025-03-31T17:33:47Z]                     for (txt, num) in completion_counts.items() if num > 1\n[2025-03-31T17:33:47Z]                 }\n[2025-03-31T17:33:47Z] >               raise AssertionError(\n[2025-03-31T17:33:47Z]                     f\"{len(completion_counts)} unique completions; expected\"\n[2025-03-31T17:33:47Z]                     f\" {n}. Repeats: {repeats}\")\n[2025-03-31T17:33:47Z] E               AssertionError: 19 unique completions; expected 20. Repeats: {'\\nWrite a short story about a robot that dreams for the first time.\\n': 2}\n[2025-03-31T17:33:47Z]\n[2025-03-31T17:33:47Z] v1/engine/test_llm_engine.py:97: AssertionError\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci/build",
      "stale",
      "v1"
    ],
    "state": "open",
    "created_at": "2025-04-01T06:55:01+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15855/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/15855"
  },
  {
    "number": 12568,
    "title": "[V1] Feedback Thread",
    "body": "Please leave comments here about your usage of V1, does it work? does it not work? which feature do you need in order to adopt it? any bugs? \n\nFor bug report, please file it separately and link the issue here. \n\nFor in depth discussion, please feel free to join #sig-v1 in the vLLM Slack workspace. \n",
    "labels": [
      "v1"
    ],
    "state": "open",
    "created_at": "2025-01-30T02:46:45+00:00",
    "closed_at": null,
    "comments": 92,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12568/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/12568"
  }
]
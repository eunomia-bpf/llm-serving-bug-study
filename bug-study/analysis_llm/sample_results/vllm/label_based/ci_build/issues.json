[
  {
    "number": 17405,
    "title": "[Tracker] Nightly CI Test Failures",
    "body": "This is intended as umbrella issue tracking failures\n",
    "labels": [
      "ci/build"
    ],
    "state": "open",
    "created_at": "2025-04-29T18:27:35+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17405/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/17405"
  },
  {
    "number": 16054,
    "title": "[Bug]: CI flake - v1/engine/test_async_llm.py::test_abort - assert has_unfinished_requests()",
    "body": "### Your current environment\n\n...\n\n### \ud83d\udc1b Describe the bug\n\nmain commit 51d7c6a2b23e100cd9e7d85b8e7c0eea656b331e\n\nSeen in https://github.com/vllm-project/vllm/pull/15894\n\nhttps://buildkite.com/organizations/vllm/pipelines/ci/builds/16742/jobs/0195f24d-e81a-46a3-ad08-6a51983d65d6/log\n\n\n```\n=================================== FAILURES ===================================\n[2025-04-01T17:38:12Z] _ test_abort[engine_args0-Hello my name is Robert and-RequestOutputKind.DELTA] _\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z] monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fd1fa052e70>\n[2025-04-01T17:38:12Z] output_kind = <RequestOutputKind.DELTA: 1>\n[2025-04-01T17:38:12Z] engine_args = AsyncEngineArgs(model='meta-llama/Llama-3.2-1B-Instruct', served_model_name=None, tokenizer='meta-llama/Llama-3.2-1B-I...additional_config=None, enable_reasoning=None, reasoning_parser=None, use_tqdm_on_load=True, disable_log_requests=True)\n[2025-04-01T17:38:12Z] prompt = 'Hello my name is Robert and'\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]     @pytest.mark.parametrize(\n[2025-04-01T17:38:12Z]         \"output_kind\", [RequestOutputKind.DELTA, RequestOutputKind.FINAL_ONLY])\n[2025-04-01T17:38:12Z]     @pytest.mark.parametrize(\"engine_args,prompt\",\n[2025-04-01T17:38:12Z]                              [(TEXT_ENGINE_ARGS, TEXT_PROMPT),\n[2025-04-01T17:38:12Z]                               (VISION_ENGINE_ARGS, VISION_PROMPT)])\n[2025-04-01T17:38:12Z]     @pytest.mark.asyncio\n[2025-04-01T17:38:12Z]     async def test_abort(monkeypatch: pytest.MonkeyPatch,\n[2025-04-01T17:38:12Z]                          output_kind: RequestOutputKind,\n[2025-04-01T17:38:12Z]                          engine_args: AsyncEngineArgs, prompt: PromptType):\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]         with monkeypatch.context() as m, ExitStack() as after:\n[2025-04-01T17:38:12Z]             m.setenv(\"VLLM_USE_V1\", \"1\")\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             engine = AsyncLLM.from_engine_args(engine_args)\n[2025-04-01T17:38:12Z]             after.callback(engine.shutdown)\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             NUM_REQUESTS = 100\n[2025-04-01T17:38:12Z]             NUM_EXPECTED_TOKENS = 100\n[2025-04-01T17:38:12Z]             NUM_EXPECTED_TOKENS_LONG = 50000\n[2025-04-01T17:38:12Z]             REQUEST_IDS_TO_ABORT = range(1, 100, 10)\n[2025-04-01T17:38:12Z]             PARALLEL_SAMPLE_REQ_IDS = range(1, 100, 15)\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             request_ids = [f\"request-{i}\" for i in range(NUM_REQUESTS)]\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             # Create concurrent requests.\n[2025-04-01T17:38:12Z]             tasks: list[asyncio.Task] = []\n[2025-04-01T17:38:12Z]             for idx, request_id in enumerate(request_ids):\n[2025-04-01T17:38:12Z]                 max_tokens = NUM_EXPECTED_TOKENS_LONG if (\n[2025-04-01T17:38:12Z]                     idx in REQUEST_IDS_TO_ABORT) else NUM_EXPECTED_TOKENS\n[2025-04-01T17:38:12Z]                 n = 3 if idx in PARALLEL_SAMPLE_REQ_IDS else 1\n[2025-04-01T17:38:12Z]                 tasks.append(\n[2025-04-01T17:38:12Z]                     asyncio.create_task(\n[2025-04-01T17:38:12Z]                         generate(engine, request_id, prompt, output_kind,\n[2025-04-01T17:38:12Z]                                  max_tokens, n)))\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             # API server cancels requests when they disconnect.\n[2025-04-01T17:38:12Z]             for idx in REQUEST_IDS_TO_ABORT:\n[2025-04-01T17:38:12Z]                 tasks[idx].cancel()\n[2025-04-01T17:38:12Z]                 await asyncio.sleep(0.1)\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             # Confirm the other requests are okay.\n[2025-04-01T17:38:12Z]             for idx, task in enumerate(tasks):\n[2025-04-01T17:38:12Z]                 # Confirm that it was actually canceled.\n[2025-04-01T17:38:12Z]                 if idx in REQUEST_IDS_TO_ABORT:\n[2025-04-01T17:38:12Z]                     with pytest.raises(asyncio.CancelledError):\n[2025-04-01T17:38:12Z]                         await task\n[2025-04-01T17:38:12Z]                 else:\n[2025-04-01T17:38:12Z]                     # Otherwise, make sure the request was not impacted.\n[2025-04-01T17:38:12Z]                     num_generated_tokens, request_id = await task\n[2025-04-01T17:38:12Z]                     n = 3 if idx in PARALLEL_SAMPLE_REQ_IDS else 1\n[2025-04-01T17:38:12Z]                     expected_tokens = NUM_EXPECTED_TOKENS * n\n[2025-04-01T17:38:12Z]                     assert num_generated_tokens == expected_tokens, (\n[2025-04-01T17:38:12Z]                         f\"{request_id} generated {num_generated_tokens} but \"\n[2025-04-01T17:38:12Z]                         f\"expected {expected_tokens}\")\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             # Make sure all aborted requests were really aborted.\n[2025-04-01T17:38:12Z] >           assert not engine.output_processor.has_unfinished_requests()\n[2025-04-01T17:38:12Z] E           assert not True\n[2025-04-01T17:38:12Z] E            +  where True = has_unfinished_requests()\n[2025-04-01T17:38:12Z] E            +    where has_unfinished_requests = <vllm.v1.engine.output_processor.OutputProcessor object at 0x7fd1ef1614c0>.has_unfinished_requests\n[2025-04-01T17:38:12Z] E            +      where <vllm.v1.engine.output_processor.OutputProcessor object at 0x7fd1ef1614c0> = <vllm.v1.engine.async_llm.AsyncLLM object at 0x7fd1ef132750>.output_processor\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z] v1/engine/test_async_llm.py:178: AssertionError\n[2025-04-01T17:38:12Z] =============================== warnings summary ===============================\n[2025-04-01T17:38:12Z] tests/v1/engine/test_async_llm.py: 12 warnings\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core_client.py: 1 warning\n[2025-04-01T17:38:12Z] tests/v1/engine/test_llm_engine.py: 2 warnings\n[2025-04-01T17:38:12Z]   /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1700) is multi-threaded, use of fork() may lead to deadlocks in the child.\n[2025-04-01T17:38:12Z]     self.pid = os.fork()\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core.py::test_engine_core\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core.py::test_engine_core_advanced_sampling\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core.py::test_engine_core_concurrent_batches\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core_client.py::test_engine_core_client[True]\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core_client.py::test_engine_core_client[False]\n[2025-04-01T17:38:12Z]   /vllm-workspace/tests/utils.py:720: DeprecationWarning: This process (pid=1700) is multi-threaded, use of fork() may lead to deadlocks in the child.\n[2025-04-01T17:38:12Z]     pid = os.fork()\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z] -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n[2025-04-01T17:38:12Z] =========================== short test summary info ============================\n[2025-04-01T17:38:12Z] FAILED v1/engine/test_async_llm.py::test_abort[engine_args0-Hello my name is Robert and-RequestOutputKind.DELTA] - assert not True\n[2025-04-01T17:38:12Z]  +  where True = has_unfinished_requests()\n[2025-04-01T17:38:12Z]  +    where has_unfinished_requests = <vllm.v1.engine.output_processor.OutputProcessor object at 0x7fd1ef1614c0>.has_unfinished_requests\n[2025-04-01T17:38:12Z]  +      where <vllm.v1.engine.output_processor.OutputProcessor object at 0x7fd1ef1614c0> = <vllm.v1.engine.async_llm.AsyncLLM object at 0x7fd1ef132750>.output_processor\n[2025-04-01T17:38:12Z] ============ 1 failed, 44 passed, 20 warnings in 1059.59s (0:17:39) ============\n[2025-04-01T17:38:14Z] \ud83d\udea8 Error: The command exited with status 1\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci/build",
      "stale",
      "v1"
    ],
    "state": "open",
    "created_at": "2025-04-04T09:48:13+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16054/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/16054"
  },
  {
    "number": 16053,
    "title": "[Bug]: CI flake - v1/entrypoints/llm/test_struct_output_generate.py::test_structured_output - JSONDecodeError: Expecting value: line 1 column 1 (char 0)",
    "body": "### Your current environment\n\n...\n\n\n### \ud83d\udc1b Describe the bug\n\nmain commit 51d7c6a2b23e100cd9e7d85b8e7c0eea656b331e\n\nSeen in https://github.com/vllm-project/vllm/pull/15894\n\nhttps://buildkite.com/organizations/vllm/pipelines/ci/builds/16742/jobs/0195fc58-3d11-45b5-b76f-8e962cbda765/log\n\n```\nFAILED v1/entrypoints/llm/test_struct_output_generate.py::test_structured_output[Qwen/Qwen2.5-1.5B-Instruct-guidance:disable-any-whitespace-auto] - json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n[2025-04-03T16:08:35Z] _ test_structured_output[Qwen/Qwen2.5-1.5B-Instruct-guidance:disable-any-whitespace-auto] _\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z] monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f318d89eb40>\n[2025-04-03T16:08:35Z] sample_json_schema = {'properties': {'age': {'type': 'integer'}, 'name': {'type': 'string'}, 'skills': {'items': {'type': 'string'}, 'type'...ition'], 'type': 'object'}, 'type': 'array'}}, 'required': ['name', 'age', 'skills', 'work_history'], 'type': 'object'}\n[2025-04-03T16:08:35Z] unsupported_json_schema = {'properties': {'email': {'pattern': '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$', 'type': 'string'}, 'grade': ...[a-z]{1,10}$', 'type': 'string'}, 'type': 'array'}}, 'required': ['score', 'grade', 'email', 'tags'], 'type': 'object'}\n[2025-04-03T16:08:35Z] sample_sql_ebnf = '\\nroot ::= select_statement\\nselect_statement ::= \"SELECT\" column \"from\" table \"where\" condition\\ncolumn ::= \"col_1\" | \"col_2\"\\ntable ::= \"table_1\" | \"table_2\"\\ncondition ::= column \"=\" number\\nnumber ::= \"1\" | \"2\"\\n'\n[2025-04-03T16:08:35Z] sample_sql_lark = '\\nstart: select_statement\\nselect_statement: \"SELECT\" column \"from\" table \"where\" condition\\ncolumn: \"col_1\" | \"col_2\"\\ntable: \"table_1\" | \"table_2\"\\ncondition: column \"=\" number\\nnumber: \"1\" | \"2\"\\n'\n[2025-04-03T16:08:35Z] sample_regex = '((25[0-5]|(2[0-4]|1\\\\d|[1-9]|)\\\\d)\\\\.){3}(25[0-5]|(2[0-4]|1\\\\d|[1-9]|)\\\\d)'\n[2025-04-03T16:08:35Z] sample_guided_choice = ['Python', 'Java', 'JavaScript', 'C++', 'C#', 'PHP', ...]\n[2025-04-03T16:08:35Z] guided_decoding_backend = 'guidance:disable-any-whitespace'\n[2025-04-03T16:08:35Z] tokenizer_mode = 'auto', model_name = 'Qwen/Qwen2.5-1.5B-Instruct'\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]     @pytest.mark.skip_global_cleanup\n[2025-04-03T16:08:35Z]     @pytest.mark.parametrize(\"model_name, guided_decoding_backend, tokenizer_mode\",\n[2025-04-03T16:08:35Z]                              PARAMS_MODELS_BACKENDS_TOKENIZER_MODE)\n[2025-04-03T16:08:35Z]     def test_structured_output(\n[2025-04-03T16:08:35Z]         monkeypatch: pytest.MonkeyPatch,\n[2025-04-03T16:08:35Z]         sample_json_schema: dict[str, Any],\n[2025-04-03T16:08:35Z]         unsupported_json_schema: dict[str, Any],\n[2025-04-03T16:08:35Z]         sample_sql_ebnf: str,\n[2025-04-03T16:08:35Z]         sample_sql_lark: str,\n[2025-04-03T16:08:35Z]         sample_regex: str,\n[2025-04-03T16:08:35Z]         sample_guided_choice: str,\n[2025-04-03T16:08:35Z]         guided_decoding_backend: str,\n[2025-04-03T16:08:35Z]         tokenizer_mode: str,\n[2025-04-03T16:08:35Z]         model_name: str,\n[2025-04-03T16:08:35Z]     ):\n[2025-04-03T16:08:35Z]         monkeypatch.setenv(\"VLLM_USE_V1\", \"1\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         # Use a single LLM instance for several scenarios to\n[2025-04-03T16:08:35Z]         # speed up the test suite.\n[2025-04-03T16:08:35Z]         llm = LLM(model=model_name,\n[2025-04-03T16:08:35Z]                   enforce_eager=True,\n[2025-04-03T16:08:35Z]                   max_model_len=1024,\n[2025-04-03T16:08:35Z]                   guided_decoding_backend=guided_decoding_backend,\n[2025-04-03T16:08:35Z]                   tokenizer_mode=tokenizer_mode)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 1: Generate JSON output based on a provided schema\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=1.0,\n[2025-04-03T16:08:35Z]             max_tokens=1000,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(json=sample_json_schema))\n[2025-04-03T16:08:35Z]         outputs = llm.generate(prompts=[\n[2025-04-03T16:08:35Z]             f\"Give an example JSON for an employee profile \"\n[2025-04-03T16:08:35Z]             f\"that fits this schema: {sample_json_schema}\"\n[2025-04-03T16:08:35Z]         ] * 2,\n[2025-04-03T16:08:35Z]                                sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]                                use_tqdm=True)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]             prompt = output.prompt\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]             assert generated_text is not None\n[2025-04-03T16:08:35Z]             if 'disable-any-whitespace' in guided_decoding_backend:\n[2025-04-03T16:08:35Z]                 assert \"\\n\" not in generated_text\n[2025-04-03T16:08:35Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n[2025-04-03T16:08:35Z]             output_json = json.loads(generated_text)\n[2025-04-03T16:08:35Z]             jsonschema.validate(instance=output_json, schema=sample_json_schema)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 2: Generate JSON object without a schema\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=1.0,\n[2025-04-03T16:08:35Z]             max_tokens=100,\n[2025-04-03T16:08:35Z]             n=2,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(json_object=True))\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         outputs = llm.generate(\n[2025-04-03T16:08:35Z]             prompts=(\"Generate a JSON object with curly braces for a person with \"\n[2025-04-03T16:08:35Z]                      \"name and age fields for John Smith who is 31 years old.\"),\n[2025-04-03T16:08:35Z]             sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]             use_tqdm=True)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             for i in range(2):\n[2025-04-03T16:08:35Z]                 generated_text = output.outputs[i].text\n[2025-04-03T16:08:35Z]                 print(generated_text)\n[2025-04-03T16:08:35Z]                 assert generated_text is not None\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]                 # Parse to verify it is valid JSON\n[2025-04-03T16:08:35Z]                 parsed_json = json.loads(generated_text)\n[2025-04-03T16:08:35Z]                 allowed_types: tuple[type, ...] = (dict, )\n[2025-04-03T16:08:35Z]                 if guided_decoding_backend.startswith(\"xgrammar\"):\n[2025-04-03T16:08:35Z]                     # TODO - we are currently too permissive with xgrammar and\n[2025-04-03T16:08:35Z]                     # allow # any valid json (typically comes back as a list or\n[2025-04-03T16:08:35Z]                     # object).  We can fix this by specifying a jsonschema of\n[2025-04-03T16:08:35Z]                     # {\"type\": \"object\"}, # but we need this fix in a release\n[2025-04-03T16:08:35Z]                     # first: https://github.com/mlc-ai/xgrammar/pull/264\n[2025-04-03T16:08:35Z]                     allowed_types = (dict, list)\n[2025-04-03T16:08:35Z]                 assert isinstance(parsed_json, allowed_types)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 3: test a jsonschema incompatible with xgrammar\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=1.0,\n[2025-04-03T16:08:35Z]             max_tokens=1000,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(json=unsupported_json_schema))\n[2025-04-03T16:08:35Z]         if guided_decoding_backend.startswith(\"xgrammar\"):\n[2025-04-03T16:08:35Z]             with pytest.raises(ValueError,\n[2025-04-03T16:08:35Z]                                match=\"The provided JSON schema contains features \"\n[2025-04-03T16:08:35Z]                                \"not supported by xgrammar.\"):\n[2025-04-03T16:08:35Z]                 llm.generate(prompts=[\n[2025-04-03T16:08:35Z]                     f\"Give an example JSON for an employee profile \"\n[2025-04-03T16:08:35Z]                     f\"that fits this schema: {unsupported_json_schema}\"\n[2025-04-03T16:08:35Z]                 ] * 2,\n[2025-04-03T16:08:35Z]                              sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]                              use_tqdm=True)\n[2025-04-03T16:08:35Z]         else:\n[2025-04-03T16:08:35Z]             outputs = llm.generate(\n[2025-04-03T16:08:35Z]                 prompts=(\"Give an example JSON object for a grade \"\n[2025-04-03T16:08:35Z]                          \"that fits this schema: \"\n[2025-04-03T16:08:35Z]                          f\"{unsupported_json_schema}\"),\n[2025-04-03T16:08:35Z]                 sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]                 use_tqdm=True)\n[2025-04-03T16:08:35Z]             assert outputs is not None\n[2025-04-03T16:08:35Z]             for output in outputs:\n[2025-04-03T16:08:35Z]                 assert output is not None\n[2025-04-03T16:08:35Z]                 assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]                 generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]                 assert generated_text is not None\n[2025-04-03T16:08:35Z]                 print(generated_text)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]                 # Parse to verify it is valid JSON\n[2025-04-03T16:08:35Z]                 parsed_json = json.loads(generated_text)\n[2025-04-03T16:08:35Z]                 assert isinstance(parsed_json, dict)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 4: Generate SQL statement using EBNF grammar\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=0.8,\n[2025-04-03T16:08:35Z]             top_p=0.95,\n[2025-04-03T16:08:35Z]             max_tokens=1000,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(grammar=sample_sql_ebnf))\n[2025-04-03T16:08:35Z]         outputs = llm.generate(\n[2025-04-03T16:08:35Z]             prompts=(\"Generate a sql statement that selects col_1 from \"\n[2025-04-03T16:08:35Z]                      \"table_1 where it is equal to 1\"),\n[2025-04-03T16:08:35Z]             sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]             use_tqdm=True,\n[2025-04-03T16:08:35Z]         )\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]             prompt = output.prompt\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]             assert generated_text is not None\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             # remove spaces for comparison b/c we removed them in the grammar\n[2025-04-03T16:08:35Z]             ground_truth = \"SELECT col_1 from table_1 where col_1 = 1\".replace(\n[2025-04-03T16:08:35Z]                 \" \", \"\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             assert generated_text.strip() == ground_truth\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 5: Generate SQL statement using Lark grammar\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=0.8,\n[2025-04-03T16:08:35Z]             top_p=0.95,\n[2025-04-03T16:08:35Z]             max_tokens=1000,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(grammar=sample_sql_lark))\n[2025-04-03T16:08:35Z]         outputs = llm.generate(\n[2025-04-03T16:08:35Z]             prompts=(\"Generate a sql statement that selects col_1 from \"\n[2025-04-03T16:08:35Z]                      \"table_1 where it is equal to 1\"),\n[2025-04-03T16:08:35Z]             sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]             use_tqdm=True,\n[2025-04-03T16:08:35Z]         )\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]             prompt = output.prompt\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]             assert generated_text is not None\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             # use Lark to parse the output, and make sure it's a valid parse tree\n[2025-04-03T16:08:35Z]             from lark import Lark\n[2025-04-03T16:08:35Z]             parser = Lark(sample_sql_lark)\n[2025-04-03T16:08:35Z]             parser.parse(generated_text)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             # remove spaces for comparison b/c we removed them in the grammar\n[2025-04-03T16:08:35Z]             ground_truth = \"SELECT col_1 from table_1 where col_1 = 1\".replace(\n[2025-04-03T16:08:35Z]                 \" \", \"\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             assert generated_text.strip() == ground_truth\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 6: Test invalid grammar input\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=0.8,\n[2025-04-03T16:08:35Z]             top_p=0.95,\n[2025-04-03T16:08:35Z]             max_tokens=1000,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(grammar=\"not a grammar\"))\n[2025-04-03T16:08:35Z]         with pytest.raises(ValueError, match=\"Failed to convert the grammar \"):\n[2025-04-03T16:08:35Z]             llm.generate(\n[2025-04-03T16:08:35Z]                 prompts=(\"Generate a sql statement that selects col_1 from \"\n[2025-04-03T16:08:35Z]                          \"table_1 where it is equal to 1\"),\n[2025-04-03T16:08:35Z]                 sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]                 use_tqdm=True,\n[2025-04-03T16:08:35Z]             )\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 7: Generate text based on a regex pattern\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=0.8,\n[2025-04-03T16:08:35Z]             top_p=0.95,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(regex=sample_regex))\n[2025-04-03T16:08:35Z]         outputs = llm.generate(\n[2025-04-03T16:08:35Z]             prompts=[\n[2025-04-03T16:08:35Z]                 f\"Give an example IPv4 address with this regex: {sample_regex}\"\n[2025-04-03T16:08:35Z]             ] * 2,\n[2025-04-03T16:08:35Z]             sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]             use_tqdm=True,\n[2025-04-03T16:08:35Z]         )\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]             prompt = output.prompt\n[2025-04-03T16:08:35Z]             generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]             print(generated_text)\n[2025-04-03T16:08:35Z]             assert generated_text is not None\n[2025-04-03T16:08:35Z]             assert re.fullmatch(sample_regex, generated_text) is not None\n[2025-04-03T16:08:35Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 8: Generate text based on a choices\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=0.8,\n[2025-04-03T16:08:35Z]             top_p=0.95,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(choice=sample_guided_choice))\n[2025-04-03T16:08:35Z]         outputs = llm.generate(\n[2025-04-03T16:08:35Z]             prompts=\"The best language for type-safe systems programming is \",\n[2025-04-03T16:08:35Z]             sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]             use_tqdm=True)\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]             prompt = output.prompt\n[2025-04-03T16:08:35Z]             generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]             print(generated_text)\n[2025-04-03T16:08:35Z]             assert generated_text is not None\n[2025-04-03T16:08:35Z]             assert generated_text in sample_guided_choice\n[2025-04-03T16:08:35Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         # Test 9: Generate structured output using a Pydantic model with an enum\n[2025-04-03T16:08:35Z]         #\n[2025-04-03T16:08:35Z]         json_schema = CarDescription.model_json_schema()\n[2025-04-03T16:08:35Z]         sampling_params = SamplingParams(\n[2025-04-03T16:08:35Z]             temperature=1.0,\n[2025-04-03T16:08:35Z]             max_tokens=1000,\n[2025-04-03T16:08:35Z]             guided_decoding=GuidedDecodingParams(json=json_schema))\n[2025-04-03T16:08:35Z]         outputs = llm.generate(\n[2025-04-03T16:08:35Z]             prompts=\"Generate a JSON with the brand, model and car_type of\"\n[2025-04-03T16:08:35Z]             \"the most iconic car from the 90's\",\n[2025-04-03T16:08:35Z]             sampling_params=sampling_params,\n[2025-04-03T16:08:35Z]             use_tqdm=True)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         assert outputs is not None\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         for output in outputs:\n[2025-04-03T16:08:35Z]             assert output is not None\n[2025-04-03T16:08:35Z]             assert isinstance(output, RequestOutput)\n[2025-04-03T16:08:35Z]             prompt = output.prompt\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]             generated_text = output.outputs[0].text\n[2025-04-03T16:08:35Z]             assert generated_text is not None\n[2025-04-03T16:08:35Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n[2025-04-03T16:08:35Z] >           output_json = json.loads(generated_text)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z] v1/entrypoints/llm/test_struct_output_generate.py:332:\n[2025-04-03T16:08:35Z] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n[2025-04-03T16:08:35Z] /usr/lib/python3.12/json/__init__.py:346: in loads\n[2025-04-03T16:08:35Z]     return _default_decoder.decode(s)\n[2025-04-03T16:08:35Z] /usr/lib/python3.12/json/decoder.py:338: in decode\n[2025-04-03T16:08:35Z]     obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n[2025-04-03T16:08:35Z] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z] self = <json.decoder.JSONDecoder object at 0x7f32ee3035c0>, s = '', idx = 0\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]     def raw_decode(self, s, idx=0):\n[2025-04-03T16:08:35Z]         \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n[2025-04-03T16:08:35Z]         a JSON document) and return a 2-tuple of the Python\n[2025-04-03T16:08:35Z]         representation and the index in ``s`` where the document ended.\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         This can be used to decode a JSON document from a string that may\n[2025-04-03T16:08:35Z]         have extraneous data at the end.\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z]         \"\"\"\n[2025-04-03T16:08:35Z]         try:\n[2025-04-03T16:08:35Z]             obj, end = self.scan_once(s, idx)\n[2025-04-03T16:08:35Z]         except StopIteration as err:\n[2025-04-03T16:08:35Z] >           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n[2025-04-03T16:08:35Z] E           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n[2025-04-03T16:08:35Z]\n[2025-04-03T16:08:35Z] /usr/lib/python3.12/json/decoder.py:356: JSONDecodeError\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci/build",
      "stale",
      "v1"
    ],
    "state": "open",
    "created_at": "2025-04-04T09:46:16+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16053/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/16053"
  },
  {
    "number": 15944,
    "title": "[Bug]: CI flake - v1/entrypoints/llm/test_struct_output_generate.py::test_structured_output bug Something isn't working ci/build v1",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n```\n\n[2025-04-02T06:06:31Z] =================================== FAILURES ===================================\n--\n\u00a0 | [2025-04-02T06:06:31Z] _ test_structured_output[mistralai/Ministral-8B-Instruct-2410-guidance:disable-any-whitespace-auto] _\n\u00a0 | [2025-04-02T06:06:31Z]\n\u00a0 | [2025-04-02T06:06:31Z] monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f6c3b4dcfb0>\n\u00a0 | [2025-04-02T06:06:31Z] sample_json_schema = {'properties': {'age': {'type': 'integer'}, 'name': {'type': 'string'}, 'skills': {'items': {'type': 'string'}, 'type'...ition'], 'type': 'object'}, 'type': 'array'}}, 'required': ['name', 'age', 'skills', 'work_history'], 'type': 'object'}\n\u00a0 | [2025-04-02T06:06:31Z] unsupported_json_schema = {'properties': {'email': {'pattern': '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$', 'type': 'string'}, 'grade': ...[a-z]{1,10}$', 'type': 'string'}, 'type': 'array'}}, 'required': ['score', 'grade', 'email', 'tags'], 'type': 'object'}\n\u00a0 | [2025-04-02T06:06:31Z] sample_sql_ebnf = '\\nroot ::= select_statement\\nselect_statement ::= \"SELECT\" column \"from\" table \"where\" condition\\ncolumn ::= \"col_1\" \\| \"col_2\"\\ntable ::= \"table_1\" \\| \"table_2\"\\ncondition ::= column \"=\" number\\nnumber ::= \"1\" \\| \"2\"\\n'\n\u00a0 | [2025-04-02T06:06:31Z] sample_sql_lark = '\\nstart: select_statement\\nselect_statement: \"SELECT\" column \"from\" table \"where\" condition\\ncolumn: \"col_1\" \\| \"col_2\"\\ntable: \"table_1\" \\| \"table_2\"\\ncondition: column \"=\" number\\nnumber: \"1\" \\| \"2\"\\n'\n\u00a0 | [2025-04-02T06:06:31Z] sample_regex = '((25[0-5]\\|(2[0-4]\\|1\\\\d\\|[1-9]\\|)\\\\d)\\\\.){3}(25[0-5]\\|(2[0-4]\\|1\\\\d\\|[1-9]\\|)\\\\d)'\n\u00a0 | [2025-04-02T06:06:31Z] sample_guided_choice = ['Python', 'Java', 'JavaScript', 'C++', 'C#', 'PHP', ...]\n\u00a0 | [2025-04-02T06:06:31Z] guided_decoding_backend = 'guidance:disable-any-whitespace'\n\u00a0 | [2025-04-02T06:06:31Z] tokenizer_mode = 'auto', model_name = 'mistralai/Ministral-8B-Instruct-2410'\n\u00a0 | [2025-04-02T06:06:31Z]\n\u00a0 | [2025-04-02T06:06:31Z]     @pytest.mark.skip_global_cleanup\n\u00a0 | [2025-04-02T06:06:31Z]     @pytest.mark.parametrize(\"model_name, guided_decoding_backend, tokenizer_mode\",\n\u00a0 | [2025-04-02T06:06:31Z]                              PARAMS_MODELS_BACKENDS_TOKENIZER_MODE)\n\u00a0 | [2025-04-02T06:06:31Z]     def test_structured_output(\n\u00a0 | [2025-04-02T06:06:31Z]         monkeypatch: pytest.MonkeyPatch,\n\u00a0 | [2025-04-02T06:06:31Z]         sample_json_schema: dict[str, Any],\n\u00a0 | [2025-04-02T06:06:31Z]         unsupported_json_schema: dict[str, Any],\n\u00a0 | [2025-04-02T06:06:31Z]         sample_sql_ebnf: str,\n\u00a0 | [2025-04-02T06:06:31Z]         sample_sql_lark: str,\n\u00a0 | [2025-04-02T06:06:31Z]         sample_regex: str,\n\u00a0 | [2025-04-02T06:06:31Z]         sample_guided_choice: str,\n\u00a0 | [2025-04-02T06:06:31Z]         guided_decoding_backend: str,\n\u00a0 | [2025-04-02T06:06:31Z]         tokenizer_mode: str,\n\u00a0 | [2025-04-02T06:06:31Z]         model_name: str,\n\u00a0 | [2025-04-02T06:06:31Z]     ):\n\u00a0 | [2025-04-02T06:06:31Z]         monkeypatch.setenv(\"VLLM_USE_V1\", \"1\")\n\u00a0 | [2025-04-02T06:06:31Z]\n\u00a0 | [2025-04-02T06:06:31Z]         # Use a single LLM instance for several scenarios to\n\u00a0 | [2025-04-02T06:06:31Z]         # speed up the test suite.\n\u00a0 | [2025-04-02T06:06:31Z]         llm = LLM(model=model_name,\n\u00a0 | [2025-04-02T06:06:31Z]                   enforce_eager=True,\n\u00a0 | [2025-04-02T06:06:31Z]                   max_model_len=1024,\n\u00a0 | [2025-04-02T06:06:31Z]                   guided_decoding_backend=guided_decoding_backend,\n\u00a0 | [2025-04-02T06:06:31Z]                   tokenizer_mode=tokenizer_mode)\n\u00a0 | [2025-04-02T06:06:31Z]\n\u00a0 | [2025-04-02T06:06:31Z]         #\n\u00a0 | [2025-04-02T06:06:31Z]         # Test 1: Generate JSON output based on a provided schema\n\u00a0 | [2025-04-02T06:06:31Z]         #\n\u00a0 | [2025-04-02T06:06:31Z]         sampling_params = SamplingParams(\n\u00a0 | [2025-04-02T06:06:31Z]             temperature=1.0,\n\u00a0 | [2025-04-02T06:06:31Z]             max_tokens=1000,\n\u00a0 | [2025-04-02T06:06:31Z]             guided_decoding=GuidedDecodingParams(json=sample_json_schema))\n\u00a0 | [2025-04-02T06:06:31Z]         outputs = llm.generate(prompts=[\n\u00a0 | [2025-04-02T06:06:31Z]             f\"Give an example JSON for an employee profile \"\n\u00a0 | [2025-04-02T06:06:31Z]             f\"that fits this schema: {sample_json_schema}\"\n\u00a0 | [2025-04-02T06:06:31Z]         ] * 2,\n\u00a0 | [2025-04-02T06:06:31Z]                                sampling_params=sampling_params,\n\u00a0 | [2025-04-02T06:06:31Z]                                use_tqdm=True)\n\u00a0 | [2025-04-02T06:06:31Z]\n\u00a0 | [2025-04-02T06:06:31Z]         assert outputs is not None\n\u00a0 | [2025-04-02T06:06:31Z]\n\u00a0 | [2025-04-02T06:06:31Z]         for output in outputs:\n\u00a0 | [2025-04-02T06:06:31Z]             assert output is not None\n\u00a0 | [2025-04-02T06:06:31Z]             assert isinstance(output, RequestOutput)\n\u00a0 | [2025-04-02T06:06:31Z]             prompt = output.prompt\n\u00a0 | [2025-04-02T06:06:31Z]\n\u00a0 | [2025-04-02T06:06:31Z]             generated_text = output.outputs[0].text\n\u00a0 | [2025-04-02T06:06:31Z]             assert generated_text is not None\n\u00a0 | [2025-04-02T06:06:31Z]             if 'disable-any-whitespace' in guided_decoding_backend:\n\u00a0 | [2025-04-02T06:06:31Z]                 assert \"\\n\" not in generated_text\n\u00a0 | [2025-04-02T06:06:31Z]             print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\u00a0 | [2025-04-02T06:06:31Z] >           output_json = json.loads(generated_text)\n\u00a0 | [2025-04-02T06:06:31Z]\n\u00a0 | [2025-04-02T06:06:31Z] v1/entrypoints/llm/test_struct_output_generate.py:100:\n\u00a0 | [2025-04-02T06:06:31Z] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\u00a0 | [2025-04-02T06:06:31Z] /usr/lib/python3.12/json/__init__.py:346: in loads\n\u00a0 | [2025-04-02T06:06:31Z]     return _default_decoder.decode(s)\n\u00a0 | [2025-04-02T06:06:31Z] /usr/lib/python3.12/json/decoder.py:338: in decode\n\u00a0 | [2025-04-02T06:06:31Z]     obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n\u00a0 | [2025-04-02T06:06:31Z] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\u00a0 | [2025-04-02T06:06:31Z]\n\u00a0 | [2025-04-02T06:06:31Z] self = <json.decoder.JSONDecoder object at 0x7f6c45eab710>\n\u00a0 | [2025-04-02T06:06:31Z] s = '{\"name\":\"{1}\",\"age\":125,\"skills\":[\"^{are an elective course}\",\"^{remote support}\",\"\\| \u0627\u0644\u062a\u0647\u0627\u0628 Dass{or}{internalutt}{aur...on reasons, very simplified and doesn\\'t fully undergo proper schema check according to specific rules yet appropriate'\n\u00a0 | [2025-04-02T06:06:31Z] idx = 0\n\u00a0 | [2025-04-02T06:06:31Z]\n\u00a0 | [2025-04-02T06:06:31Z]     def raw_decode(self, s, idx=0):\n\u00a0 | [2025-04-02T06:06:31Z]         \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n\u00a0 | [2025-04-02T06:06:31Z]         a JSON document) and return a 2-tuple of the Python\n\u00a0 | [2025-04-02T06:06:31Z]         representation and the index in ``s`` where the document ended.\n\u00a0 | [2025-04-02T06:06:31Z]\n\u00a0 | [2025-04-02T06:06:31Z]         This can be used to decode a JSON document from a string that may\n\u00a0 | [2025-04-02T06:06:31Z]         have extraneous data at the end.\n\u00a0 | [2025-04-02T06:06:31Z]\n\u00a0 | [2025-04-02T06:06:31Z]         \"\"\"\n\u00a0 | [2025-04-02T06:06:31Z]         try:\n\u00a0 | [2025-04-02T06:06:31Z] >           obj, end = self.scan_once(s, idx)\n\u00a0 | [2025-04-02T06:06:31Z] E           json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 3079 (char 3078)\n\u00a0 | [2025-04-02T06:06:31Z]\n\u00a0 | [2025-04-02T06:06:31Z] /usr/lib/python3.12/json/decoder.py:354: JSONDecodeError\n\n\n```\nSee https://buildkite.com/vllm/ci/builds/16776#0195f4d4-1d2a-42c1-98b2-9b05d879956c\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci/build",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-02T10:29:48+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15944/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15944"
  },
  {
    "number": 15855,
    "title": "[Bug]: CI flake - v1/engine/test_llm_engine.py::test_parallel_sampling[True]",
    "body": "### Your current environment\n\n...\n\n\n### \ud83d\udc1b Describe the bug\n\nSaw V1 test failing with this yesterday, went away with recheck:\n\n```\n[2025-03-31T17:33:47Z] _________________________ test_parallel_sampling[True] _________________________\n[2025-03-31T17:33:47Z]\n[2025-03-31T17:33:47Z] vllm_model = <tests.conftest.VllmRunner object at 0x7f0d875e06e0>\n[2025-03-31T17:33:47Z] example_prompts = ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the majo...me.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', ...]\n[2025-03-31T17:33:47Z]\n[2025-03-31T17:33:47Z]     def test_parallel_sampling(vllm_model, example_prompts) -> None:\n[2025-03-31T17:33:47Z]         \"\"\"Test passes if parallel sampling `n>1` yields `n` unique completions.\n[2025-03-31T17:33:47Z]\n[2025-03-31T17:33:47Z]         Args:\n[2025-03-31T17:33:47Z]           vllm_model: VllmRunner instance under test.\n[2025-03-31T17:33:47Z]           example_prompt: test fixture providing prompts for testing.\n[2025-03-31T17:33:47Z]         \"\"\"\n[2025-03-31T17:33:47Z]         sampling_params_list, n_list = _get_test_sampling_params(example_prompts)\n[2025-03-31T17:33:47Z]         model: LLM = vllm_model.model\n[2025-03-31T17:33:47Z]         outputs = model.generate(example_prompts, sampling_params_list)\n[2025-03-31T17:33:47Z]\n[2025-03-31T17:33:47Z]         # Validate each request response\n[2025-03-31T17:33:47Z]         for out, n in zip(outputs, n_list):\n[2025-03-31T17:33:47Z]             completion_counts: dict[str, int] = {}\n[2025-03-31T17:33:47Z]             # Assert correct number of completions\n[2025-03-31T17:33:47Z]             assert len(out.outputs) == n, (\n[2025-03-31T17:33:47Z]                 f\"{len(out.outputs)} completions; {n} expected.\")\n[2025-03-31T17:33:47Z]             for idx in range(n):\n[2025-03-31T17:33:47Z]                 comp = out.outputs[idx]\n[2025-03-31T17:33:47Z]                 # Assert correct completion indices\n[2025-03-31T17:33:47Z]                 assert comp.index == idx, (f\"Index {comp.index}; expected {idx}.\")\n[2025-03-31T17:33:47Z]                 text = comp.text\n[2025-03-31T17:33:47Z]                 completion_counts[text] = completion_counts.get(text, 0) + 1\n[2025-03-31T17:33:47Z]             # Assert unique completions\n[2025-03-31T17:33:47Z]             if len(completion_counts) != n:\n[2025-03-31T17:33:47Z]                 repeats = {\n[2025-03-31T17:33:47Z]                     txt: num\n[2025-03-31T17:33:47Z]                     for (txt, num) in completion_counts.items() if num > 1\n[2025-03-31T17:33:47Z]                 }\n[2025-03-31T17:33:47Z] >               raise AssertionError(\n[2025-03-31T17:33:47Z]                     f\"{len(completion_counts)} unique completions; expected\"\n[2025-03-31T17:33:47Z]                     f\" {n}. Repeats: {repeats}\")\n[2025-03-31T17:33:47Z] E               AssertionError: 19 unique completions; expected 20. Repeats: {'\\nWrite a short story about a robot that dreams for the first time.\\n': 2}\n[2025-03-31T17:33:47Z]\n[2025-03-31T17:33:47Z] v1/engine/test_llm_engine.py:97: AssertionError\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci/build",
      "stale",
      "v1"
    ],
    "state": "open",
    "created_at": "2025-04-01T06:55:01+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15855/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/15855"
  }
]
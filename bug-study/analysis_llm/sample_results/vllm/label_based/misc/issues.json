[
  {
    "number": 3587,
    "title": "[RFC]: Interface and Abstraction for Distributed Inference Environment",
    "body": "This RFC describes a proposal for interfaces and abstractions for distributed inference environments. I plan to solicit discussions for a week (until March 31st) before I begin to actually refactor the code.\r\n\r\n# Motivation\r\n\r\nThe current distributed inference environment in `vllm` is quite tangled, and we often see deadlocks and hangs (see https://github.com/vllm-project/vllm/issues/3455 , https://github.com/vllm-project/vllm/issues/2770 , https://github.com/vllm-project/vllm/issues/3559 , to name a few). The problem becomes prominent when we try to upgrade to pytorch 2.2.0 (see https://github.com/vllm-project/vllm/pull/3442 , https://github.com/vllm-project/vllm/pull/3442 ), because `pytorch 2.2.0` upgrades from `nccl==2.18.1` to `2.19.3` (see https://pypi.org/pypi/torch/2.1.2/json and https://pypi.org/pypi/torch/2.2.0/json to compare the dependency), and `nccl==2.19.3` breaks `vllm` due to increased memory cost during cudagraph capture (from 10MB per graph to 100MB per graph, adds up to several GBs because we have dozens of cudagraph).\r\n\r\nTL,DR; distributed inference in current codebase is a headache. If it works, hooray; if not, don't be surprised.\r\n\r\n# Proposal\r\n\r\n## Abstraction\r\n\r\nI think we should have three levels of abstraction:\r\n\r\n1. Launcher, responsible for launching processes (potentially across multi-node). Currently it is `ray`, but we can also have another choices like Python's native `multiprocessing` in single-node cases. See https://github.com/vllm-project/vllm/pull/3466 for example.\r\n2. Coordinator, responsible for coordinating shared resources (e.g. filesystem usage) and broadcasting some messages. Currently we don't have this, and there are lots of hacks for ad-hoc implementation, e.g. use `filelock` to lock on filesystems ( https://github.com/vllm-project/vllm/pull/3578 ), use `TCP` to initialize communication in `cupy` ( https://github.com/vllm-project/vllm/pull/2811 ), use `MPI` to initialize communication in AMD's `cupy` version ( https://github.com/vllm-project/vllm/pull/3123 ).\r\n3. Communicator, responsible for cross-device communication of large tensor data (e.g. perform allreduce). Currently we support `nccl`, and AMD also has its own communication library. Note that this is vendor-specific, and vendors usually have their own way of cross-device communication.\r\n\r\nThe most messy one, and **the missing one, is the Coordinator abstraction level**. More on this later.\r\n\r\n## Interface\r\n\r\nBetween each consecutive abstractions, lies the interface. \r\n\r\n### Interface between Launcher and Coordinator\r\n\r\nAfter Launcher launches processes, it needs to at least tell the processes the following information: \r\n- `launch_id`, used to distinguish current launch with possibly concurrent launch (e.g. when 4 users want to set up 4 inference engines in the same node, each with 2 GPUs). Note: the `launch_id` can be used as a \"random seed\" to draw values for `master_port`, instead of keeping only one default `master_port` value and having to kill all processes after the last run crashes. A reference implementation would be hashing the `launch_id` to a port number, and increasing the port number to find the first free port. This is a strategy taken by [Jupyter Notebook/Lab Server](https://github.com/jupyter/) .\r\n- `world_size`, number of processes participating in the current launch (may span over multiple nodes)\r\n- `local_world_size`, number of processes participating in the current launch **in the current node** (not necessarily the same across nodes)\r\n- `rank`, range from 0 (inclusive) to `world_size` (exclusive) , unique in the launch for each process\r\n- `local_rank`, range from 0 (inclusive) to `local_world_size` (exclusive), unique in each node, **can use this to assign devices in a node!**\r\n- `master_addr`, the IP address of the master node, should be reachable from all nodes\r\n- `master_port`, a free port in the master node, reserved for possible coordination\r\n- other custom information can be added, but the above are required.\r\n\r\nHow does Launcher pass these information to each process? Basically we have two choices:\r\n1. through environment variables, the simplest way, but will disable the usage of thread-level distributed inference because environment variables are shared within threads in one process. (However, thread-level distributed inference seems rare. Do we need to consider this?)\r\n2. through serialization and deserialization (e.g. passing bytes in a shared object store), the most general way, at the cost of complexity and runtime efficiency to design and execute the serialization/deserialization\r\n\r\n### Interface between Coordinator and Communicator\r\n\r\nDevice communicators (e.g. `nccl`) often need to initialize the communication by sharing some unique token (see [`nccl` documentation](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html)). In addition, processes sometimes need to coordinate the resource in a node or across the cluster.\r\n\r\nIn sight of the above consideration, `Coordinator` should at least have the following interfaces:\r\n\r\n1. `is_master()`: tell if the current process is a master process, i.e. convenient wrapper for boilerplate code `rank == 0`\r\n2. `is_local_master()`: tell if the current process is a **local** master process, i.e. convenient wrapper for boilerplate code `local_rank == 0`\r\n3. `broadcast(bytes, src)`: broadcast some message (in the form of `bytes`) from rank `src` to all the processes. The semantic is standard, no need for more explanation.\r\n4. `barrier()`: block until all processes reaches here. Also standard communication primitive.\r\n\r\nNote: very often than not, we want to execute something in just one process per node (e.g. creating directories, downloading files to the node). Inspired by [this thread](https://discuss.pytorch.org/t/torch-distributed-barrier-used-in-multi-node-distributed-data-parallel-training/89711/12), we can write code like this:\r\n\r\n```python\r\nif is_local_master():\r\n    do_something() # download file, create directory, etc.\r\nbarrier()\r\n```\r\n\r\nFurthermore, there are more complicated requirements like  \"only one process in each node does something, but this something is different across nodes\", essentially the requirement of `local_barrier()`, a function that block until all processes **in the current node** reaches here. It is debatable if we want this (currently I don't see any requirements like this in `vllm`.)\r\n\r\n### Communicator interface\r\n\r\nThe following functionality of communicator is suggested (mostly taken from the `nccl` design):\r\n\r\n1. the master process get unique token to identify the communication group\r\n2. the master process broadcast unique token to all ranks\r\n3. each process initializes communication by the unique token and their rank, world_size\r\n4. an in-place allreduce function: `allreduce(char* input, size_t count, size_t dtype, size_t op)`. More functionality would be better (e.g. out-of-place allreduce, broadcast/reduce/scatter etc.), but inplace allreduce is all we need currently.\r\n\r\nThe intended usage would be something like this:\r\n\r\n```python\r\n# inside each process\r\ncoor = Coordinator(); # initialize Coordinator, done by vllm\r\ncomm = Communicator(coor) # hardware vendor can use `coor` to initialize their communicator\r\ndata = torch.tensor((1024, 1024)).to(device=f\"xpu:{coor.local_rank}\")\r\ncomm.allreduce(data) # hardware vendor can access the raw data via pytorch's [`Tensor.data_ptr`](https://pytorch.org/docs/stable/generated/torch.Tensor.data_ptr.html) mechanism.\r\n# please implement Communicator.__del__ to destroy communicator, so that programs can exit gracefully \r\n```\r\n\r\n## A reference implementation of Coordinator\r\n\r\nA reference implementation of Coordinator can be `torch.distributed`, with the `gloo` backend designed to communicate CPU tensors. \r\n\r\nOther considerations include MPI and custom-implemented TCP store. However, since we live in `torch` framework, `torch.distributed` is a natural choice without any new dependency.\r\n\r\nNote: `torch.distributed` can also be used as a fully functional communicator for GPU devices. However, `torch.distributed.all_reduce` is way more complicated than just an allreduce operation. It might initialize autograd engine, might keep track of gradients, might dispatch to different device kernels. Even if we are in [`torch.inference_mode`](https://pytorch.org/docs/stable/generated/torch.inference_mode.html), its `c10` engine might perform some additional operations that fails functionalities like cudagraph. Therefore, I prefer to call vendor-provided communication libraries directly to bypass the problem. After all, we just want an allreduce operation on dense tensors, without any hustle and bustle.\r\n\r\n# Benefits\r\n\r\nAfter we have the above abstraction and interface, we can have the following benefits:\r\n- We are always in a distributed environment, just with different sizes of wold_size. Distributed concerns will always be considered, so that we can easily scale to multi-node environments (if any LLM needs this).\r\n- Hardware vendors can plug in their communication libraries very easily. All they need to provide are: integration into pytorch `torch.Tensor` (only forward computation ops are enough), a c library (an .so file would be enough) for calling communication ops with raw data (i.e. `char*` in c). And if they want to move quickly, just one `allreduce` op would be enough for inference. No need to wait for the whole functionality completed within pytorch.\r\n\r\n# Things not to be considered\r\n\r\nWe don't aim for a fully-fledged distributed execution environment. And since inference tasks are almost stateless, we don't need to consider elasticness and fault-tolerance. As opposed to training, we don't need to save checkpoints, we don't need to resume from previous failure ... ",
    "labels": [
      "RFC",
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-03-23T23:41:40+00:00",
    "closed_at": "2024-06-14T01:00:32+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3587/reactions",
      "total_count": 10,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 10,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/3587"
  },
  {
    "number": 9845,
    "title": "[Misc]: Remove max_tokens field for chat completion requests when not supported anymore by the OpenAI client",
    "body": "With the introduction of the `o1` model series, OpenAI deprecated the `max_tokens` field in favor of the new `max_completion_tokens` field for the [chat completion API](https://platform.openai.com/docs/api-reference/chat/create).\r\n\r\nThis change is active since the [v1.45.0](https://github.com/openai/openai-python/compare/v1.44.1...v1.45.0) version of the OpenAI client.\r\n\r\nhttps://github.com/vllm-project/vllm/pull/9837 added the support for the new `max_completion_tokens` in vLLM while deprecating the `max_tokens` field. However, both fields are supported and cohabit during the deprecation period.\r\n\r\nWhen the OpenAI client definitely drops the `max_tokens` field, this change must also be reflected in the vLLM frontend.\r\n\r\nThis ticket is to keep track of this task. Relevant parts of the code to be updated are commented with `TODO(#9845)`.\r\n\r\n\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-30T16:35:40+00:00",
    "closed_at": "2025-02-28T02:01:57+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9845/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/9845"
  },
  {
    "number": 14128,
    "title": "[Misc]: When using lossy optimization, how to explain that the loss caused by optimization is within the acceptable range?",
    "body": "### Anything you want to discuss about vllm.\n\nI\u2019ve noticed that with each version upgrade of vllm, there seems to be some degree of precision loss. How do you determine whether these losses are within an acceptable range?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-03T09:30:10+00:00",
    "closed_at": "2025-07-11T02:16:13+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14128/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14128"
  },
  {
    "number": 4099,
    "title": "[Misc]: Add sanity test on Python 3.8",
    "body": "### Anything you want to discuss about vllm.\n\nCurrently, CI only runs on Python 3.9. For Python > 3.9, there's no problem with this approach because Python guarantees the backward compatibility, but Python 3.8 (which is still the version that's supported) is not properly tested and sometimes caused issue like https://github.com/vllm-project/vllm/pull/4092#issuecomment-2057867553\r\n\r\nTo solve this issue, we can have a simple sanity check test on Python 3.8.",
    "labels": [
      "misc",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-15T22:42:17+00:00",
    "closed_at": "2024-10-30T02:35:42+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4099/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4099"
  },
  {
    "number": 13222,
    "title": "[Misc]:  Question about Grouped-query attention (GQA)",
    "body": "### Implementation of Grouped-query attention (GQA)\n\nHello:) I was wondering whether [Grouped-query attention](https://arxiv.org/pdf/2305.13245#:~:text=Multi%2Dquery%20attention%20shares%20single,head%20and%20multi%2Dquery%20attention) (GQA) is implemented in vLLM. I see that Llama3 models come with this feature in their [architecture](https://arxiv.org/pdf/2407.21783), and they are available through vLLM. Are they using GQA in the backend?\n\nThanks a lot and sorry for the inconveniences\n\n### Before submitting a new issue...\n\n- [ ] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2025-02-13T13:14:37+00:00",
    "closed_at": "2025-02-14T16:24:47+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13222/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13222"
  },
  {
    "number": 3947,
    "title": "[Misc] [CI]: Flaky test failure in `test_chatglm3_lora`",
    "body": "* [main CI failed](https://buildkite.com/vllm/ci/builds/4362#018ec42c-e40a-4cbd-943a-e2e4b6e11f53) after https://github.com/vllm-project/vllm/pull/3837 was merged\r\n* tracking in an issue in case we need to ~~fix-forward/revert/or maybe the test is flaky~~ @youkaichao confirmed the test is flaky\r\n\r\n```\r\n    def test_chatglm3_lora(chatglm3_lora_files):\r\n        llm = vllm.LLM(MODEL_PATH,\r\n                       max_model_len=1024,\r\n                       enable_lora=True,\r\n                       max_loras=4,\r\n                       max_lora_rank=64,\r\n                       trust_remote_code=True)\r\n\r\n        expected_lora_output = [\r\n            \"SELECT count(*) FROM singer\",\r\n            \"SELECT avg(age) ,  min(age) ,  max(age) FROM singer WHERE country  =  'France'\",  # noqa: E501\r\n            \"SELECT name ,  country ,  age FROM singer ORDER BY age\",\r\n        ]\r\n\r\n        output1 = do_sample(llm, chatglm3_lora_files, lora_id=1)\r\n        for i in range(len(expected_lora_output)):\r\n            assert output1[i] == expected_lora_output[i]\r\n        output2 = do_sample(llm, chatglm3_lora_files, lora_id=2)\r\n        for i in range(len(expected_lora_output)):\r\n>           assert output2[i] == expected_lora_output[i]\r\nE           AssertionError: assert '' == 'SELECT count(*) FROM singer'\r\nE             \r\nE             - SELECT count(*) FROM singer\r\n```",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-04-09T19:22:33+00:00",
    "closed_at": "2024-04-09T20:33:37+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3947/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3947"
  },
  {
    "number": 11838,
    "title": "[Misc]: Wondering why we checkout from a specific commit of Triton",
    "body": "### Anything you want to discuss about vllm.\n\nI reviewed the Dockerfile and documentation and noticed that we are building Triton from the commit `e192dba224c673671ae70f73842fc693ca279a45`. Is there a specific reason for using this commit? I ask because, based on my experience, Triton's kernel performance on AMD GPUs is better with vlllm Docker at that commit compared to the newest release.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-08T09:46:30+00:00",
    "closed_at": "2025-05-09T02:10:04+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11838/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11838"
  },
  {
    "number": 4030,
    "title": "[Misc]: vLLM performs consistently poor as compared to HF TGI when tested with the DeepSeek Coder Model",
    "body": "### Anything you want to discuss about vllm.\r\n\r\nHello Folks, \r\n\r\nWe are using the Deep Seek Coder model for code completions and chat completions. I did try to run the benchmark scripts for that model both for vLLM and TGI and I see that vLLM metrics are consistently poorer as compared to TGI. \r\n\r\nCould you please review and comment on the setup ?\r\n\r\nBring up the servers:\r\n```\r\nMODEL=\"deepseek-ai/deepseek-coder-1.3b-instruct\"\r\n```\r\n\r\n```\r\npython -m vllm.entrypoints.openai.api_server \\\r\n    --model ${MODEL} \\\r\n    --swap-space 16 \\\r\n    --disable-log-requests\r\n```\r\n\r\n```\r\n    (TGI backend)\r\n    ./launch_tgi_server.sh ${MODEL} 8192\r\n```\r\n\r\nOn the client side, run:\r\n```\r\npython benchmarks/benchmark_serving.py \\\r\n    --backend vllm \\\r\n    --model ${MODEL} \\\r\n    --dataset-name sharegpt \\\r\n    --dataset-path /home/anindya/ShareGPT_V3_unfiltered_cleaned_split.json \\\r\n    --request-rate 10 \\\r\n    --num-prompts 1000 \\\r\n    --save-result\r\n```\r\n\r\n```\r\npython benchmarks/benchmark_serving.py \\\r\n    --backend tgi \\\r\n    --model ${MODEL} \\\r\n    --endpoint /generate_stream \\\r\n    --dataset-name sharegpt \\\r\n    --dataset-path /home/anindya/ShareGPT_V3_unfiltered_cleaned_split.json \\\r\n    --request-rate 10 \\\r\n    --num-prompts 1000 \\\r\n    --save-result\r\n```\r\n\r\nResults:\r\n\r\n```\r\nWith DeepSeek Model vLLM Backend\r\n============ Serving Benchmark Result ============\r\nSuccessful requests:                     1000      \r\nBenchmark duration (s):                  131.96    \r\nTotal input tokens:                      243330    \r\nTotal generated tokens:                  188582    \r\nRequest throughput (req/s):              7.58      \r\nInput token throughput (tok/s):          1844.00   \r\nOutput token throughput (tok/s):         1429.11   \r\n---------------Time to First Token----------------\r\nMean TTFT (ms):                          64.49     \r\nMedian TTFT (ms):                        56.54     \r\nP99 TTFT (ms):                           226.61    \r\n-----Time per Output Token (excl. 1st token)------\r\nMean TPOT (ms):                          61.00     \r\nMedian TPOT (ms):                        57.76     \r\nP99 TPOT (ms):                           142.52      \r\n==================================================\r\n```\r\n\r\n```\r\nWith DeepSeek Model TGI Backend\r\n============ Serving Benchmark Result ============\r\nSuccessful requests:                     1000      \r\nBenchmark duration (s):                  113.41    \r\nTotal input tokens:                      243330    \r\nTotal generated tokens:                  180307    \r\nRequest throughput (req/s):              8.82      \r\nInput token throughput (tok/s):          2145.67   \r\nOutput token throughput (tok/s):         1589.94   \r\n---------------Time to First Token----------------\r\nMean TTFT (ms):                          332.49    \r\nMedian TTFT (ms):                        324.67    \r\nP99 TTFT (ms):                           708.42    \r\n-----Time per Output Token (excl. 1st token)------\r\nMean TPOT (ms):                          25.51     \r\nMedian TPOT (ms):                        25.55     \r\nP99 TPOT (ms):                           35.88   \r\n==================================================\r\n```",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-04-12T06:55:22+00:00",
    "closed_at": "2024-05-08T09:35:29+00:00",
    "comments": 20,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4030/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4030"
  },
  {
    "number": 5803,
    "title": "[Misc]: vLLM logger disables other existing loggers by default",
    "body": "### Anything you want to discuss about vllm.\n\n## Issue\r\nThe current default behavior of the logger in vLLM is to disable all other existing loggers. This can prevent logs from being outputted from other code that is defined/imported before vLLM is imported.\r\n\r\n## Details\r\nThe default logging config defined [here](https://github.com/vllm-project/vllm/blob/1744cc99ba9bdefea8f3f798cf51ed650b81a98e/vllm/logger.py#L22-L46) does not include `disable_existing_loggers=False`. When using logging.dictConfig() to configure logging, this value is set to True by default for backwards compatibility. Unless this is the intended behavior, I believe this key should be added to the configuration dictionary.\r\n\r\nHappy to add this small change if maintainers agree with this. Thank you!",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-06-24T22:59:42+00:00",
    "closed_at": "2024-08-19T22:11:59+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5803/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/5803"
  },
  {
    "number": 7370,
    "title": "[Misc]:  Improving VLLM KVCACHE Transfer Efficiency with NCCL P2P Communication",
    "body": "### Anything you want to discuss about vllm.\n\nI hope to utilize the NCCL point-to-point communication protocol (P2P) to transfer the VLLM KVCACHE from the prefill node to the decode node for decoupled inference. Since the KVCACHE in VLLM is stored as a list of tensors, I need to send approximately 16,384 block slices every time I transmit 128 blocks. The non-contiguous distribution of these slices in GPU memory leads to low efficiency in the cyclic transmission, preventing optimal utilization of the communication bandwidth.\r\n\r\nTherefore, I am considering concatenating these slices into a single large tensor for transmission. On the receiving end, the node would split this large tensor and write the data back to the corresponding positions based on the slice indices. However, this process is quite time-consuming due to the involvement of up to 16,384 slices. I would like to know if CUDA operations can be utilized to parallelize this process in order to improve performance.\r\n\r\nAdditionally, does this decoupled KVCACHE transfer method have any design flaws? Do you have any better suggestions?",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-08-09T15:56:06+00:00",
    "closed_at": "2024-09-09T07:06:15+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7370/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7370"
  },
  {
    "number": 3585,
    "title": "[Misc]: How to call the paged_attention_v2 on my own q and kv caches?",
    "body": "### Anything you want to discuss about vllm.\r\n\r\nHi, I am trying to use the `paged_attention_v2` function on my own data, qkv.\r\n\r\nHowever, I find it is not giving the correct result. I test with the following script:\r\n\r\n```\r\nfrom typing import Optional\r\nimport argparse\r\nimport random\r\nimport time\r\n\r\nimport torch\r\nfrom flash_attn import flash_attn_func\r\nfrom vllm._C import ops, cache_ops\r\nfrom vllm.utils import create_kv_caches_with_random\r\n\r\nNUM_BLOCKS = 1024\r\nBLOCK_SIZE = 32\r\nPARTITION_SIZE = 512\r\n\r\ntorch.set_default_device('cuda:0')\r\ntorch.set_default_dtype(torch.float16)\r\n\r\ndef expand_heads(tensor, num_heads=32, num_heads_kv=8):\r\n    assert tensor.dim() == 3\r\n    _, length, dim_head = tensor.shape\r\n    num_group = num_heads // num_heads_kv\r\n    tensor = tensor.view((num_heads_kv, 1, length, dim_head))\r\n    tensor = tensor.expand((num_heads_kv, num_group, length, dim_head)).reshape((num_heads, length, dim_head))\r\n    return tensor\r\n\r\n\r\ndef make_qkv(len_k, num_head, num_head_kv, head_dim):\r\n    q = torch.randn(num_head, 1, head_dim)\r\n    k = torch.randn(num_head_kv, len_k, head_dim)\r\n    v = torch.randn(num_head_kv, len_k, head_dim)\r\n\r\n    return q, k, v\r\n\r\n\r\ndef ref_attention(q, k, v):\r\n    head_dim = q.shape[2]\r\n    scale = float(1.0 / (head_dim**0.5))\r\n    k, v = expand_heads(k), expand_heads(v)\r\n    qk = q @ k.transpose(1, 2)\r\n    qk = qk * scale\r\n    p = torch.nn.functional.softmax(qk, dim=-1)\r\n    o = p @ v\r\n    return o\r\n\r\n\r\ndef ref_flashattn(q, k, v):\r\n    q = q.unsqueeze(0).permute(0, 2, 1, 3)\r\n    k = k.unsqueeze(0).permute(0, 2, 1, 3)\r\n    v = v.unsqueeze(0).permute(0, 2, 1, 3)\r\n\r\n    o = flash_attn_func(q, k, v).squeeze(0).permute(1, 0, 2)\r\n    return o\r\n\r\n\r\ndef paged_attention(q, k, v):\r\n    '''\r\n    q = torch.randn(num_head, 1, head_dim)\r\n    k = torch.randn(num_head_kv, len_k, head_dim)\r\n    v = torch.randn(num_head_kv, len_k, head_dim)\r\n    '''\r\n    num_head = q.shape[0]\r\n    num_head_kv = k.shape[0]\r\n    head_dim = k.shape[2]\r\n    len_k = k.shape[1]\r\n    activated_block = (len_k + BLOCK_SIZE - 1) // BLOCK_SIZE\r\n    block_tables = torch.arange(activated_block, dtype=torch.int32).unsqueeze(0)\r\n\r\n    k_caches, v_caches = create_kv_caches_with_random(NUM_BLOCKS, BLOCK_SIZE, 1, num_head_kv, head_dim, \"auto\", q.dtype)\r\n    k_cache, v_cache = k_caches[0], v_caches[0]\r\n    q = q.permute(1, 0, 2)\r\n    k = k.permute(1, 0, 2)\r\n    v = v.permute(1, 0, 2)\r\n\r\n    slots = torch.tensor(range(len_k), dtype=torch.long)\r\n    print(q.shape, k.shape, v.shape, slots)\r\n    cache_ops.reshape_and_cache(k, v, k_cache, v_cache, slots, \"auto\")\r\n\r\n    q = q.reshape(1, num_head, head_dim)\r\n\r\n    output = torch.empty(1, num_head, head_dim, dtype=q.dtype)\r\n\r\n    num_partitions = ((len_k + PARTITION_SIZE - 1) //\r\n                        PARTITION_SIZE)\r\n    assert PARTITION_SIZE % BLOCK_SIZE == 0\r\n    num_seqs, num_heads, head_size = output.shape\r\n    tmp_output = torch.empty(\r\n        size=(num_seqs, num_heads, num_partitions, head_size),\r\n        dtype=output.dtype\r\n    )\r\n    exp_sums = torch.empty(\r\n        size=(num_seqs, num_heads, num_partitions),\r\n        dtype=torch.float32\r\n    )\r\n    max_logits = torch.empty_like(exp_sums)\r\n\r\n    ops.paged_attention_v2(\r\n        output,\r\n        exp_sums,\r\n        max_logits,\r\n        tmp_output,\r\n        q,\r\n        k_cache,\r\n        v_cache,\r\n        num_head_kv,\r\n        float(1.0 / (head_dim**0.5)),\r\n        block_tables,\r\n        torch.tensor([len_k], dtype=torch.int32, device=q.device),\r\n        BLOCK_SIZE,\r\n        len_k,\r\n        None,\r\n        \"auto\"\r\n    )\r\n    return output.squeeze(0)\r\n\r\n\r\nq, k, v = make_qkv(10, 32, 8, 128)\r\no_ref = ref_attention(q, k, v)[:, 0]\r\no_fa = ref_flashattn(q, k, v)[:, 0]\r\no_pa = paged_attention(q, k, v)\r\nprint('Vanilla attention:')\r\nprint(o_ref)\r\nprint('\\nFlash attention:')\r\nprint(o_fa)\r\nprint('\\nPaged attention:')\r\nprint(o_pa, o_pa.shape)\r\n```\r\n\r\nIn my above code, I set the length of `kv` to be `10`, and the block size to be 32. Therefore, only the first block should be used, thus the `block_table` is `[[0]]`. However, the output looks wrong:\r\n\r\n```\r\nVanilla attention:\r\ntensor([[-0.1760, -0.3501,  0.0952,  ...,  0.1364, -0.5576, -0.0062],\r\n        [-0.0789, -0.4060,  0.1804,  ...,  0.5312, -0.4551,  0.0644],\r\n        [ 0.0455,  0.0097, -0.0822,  ...,  0.3403, -0.3801, -0.5190],\r\n        ...,\r\n        [-0.7676,  0.2522, -0.1274,  ...,  0.1797, -0.3899, -0.6133],\r\n        [-0.4763,  0.6548, -0.2444,  ..., -0.0674, -0.5127,  0.1484],\r\n        [-0.5444,  0.5742,  0.4604,  ...,  0.5264, -0.6250,  0.4282]],\r\n       device='cuda:0')\r\n\r\nFlash attention:\r\ntensor([[-0.1760, -0.3501,  0.0952,  ...,  0.1362, -0.5576, -0.0062],\r\n        [-0.0789, -0.4060,  0.1805,  ...,  0.5312, -0.4553,  0.0645],\r\n        [ 0.0455,  0.0097, -0.0821,  ...,  0.3406, -0.3804, -0.5190],\r\n        ...,\r\n        [-0.7676,  0.2524, -0.1274,  ...,  0.1796, -0.3899, -0.6128],\r\n        [-0.4763,  0.6548, -0.2444,  ..., -0.0674, -0.5127,  0.1484],\r\n        [-0.5439,  0.5747,  0.4602,  ...,  0.5264, -0.6255,  0.4290]],\r\n       device='cuda:0')\r\n\r\nPaged attention:\r\ntensor([[-0.1761, -0.3499,  0.0952,  ...,  0.1360, -0.5576, -0.0064],\r\n        [-0.0789, -0.4060,  0.1807,  ...,  0.5312, -0.4553,  0.0645],\r\n        [ 0.0456,  0.0097, -0.0825,  ...,  0.3403, -0.3804, -0.5186],\r\n        ...,\r\n        [-0.3254, -0.7471,  0.5620,  ...,  0.4473,  0.1311, -0.3809],\r\n        [-0.3582, -0.2439, -0.1754,  ...,  0.0427, -0.3494, -0.0677],\r\n        [-0.3640,  0.4548,  0.1381,  ...,  0.0438, -0.0980, -0.0275]],\r\n       device='cuda:0') torch.Size([32, 128])\r\n```\r\nAs you can see, only the first several lines of output are correct.\r\n\r\nI further check the output starting from the first head, and I found the following:\r\n```\r\nprint_first = 5\r\nprint('Vanilla attention:')\r\nprint(o_ref[:print_first, 57:69], o_ref.shape)\r\nprint('\\nFlash attention:')\r\nprint(o_fa[:print_first, 57:69], o_fa.shape)\r\nprint('\\nPaged attention:')\r\nprint(o_pa[:print_first, 57:69], o_pa.shape)\r\n\r\nVanilla attention:\r\ntensor([[ 0.2207,  0.5747,  0.4590, -0.5278,  0.2886,  0.0807, -0.1776,  0.4517,\r\n         -0.2893,  0.0796, -0.2537, -0.4487],\r\n        [ 0.0433,  0.8745,  0.6284, -0.7466, -0.1093,  0.5020, -0.8545, -0.5391,\r\n          0.4675, -0.0595,  0.3262, -0.1733],\r\n        [ 0.4810,  0.3066,  0.5342, -0.5845,  0.1986,  0.4680,  0.2181,  0.3647,\r\n         -0.4055,  0.3743, -0.0800, -0.4871],\r\n        [ 0.5420,  0.2603,  0.4045, -0.5083,  0.0301,  0.3479,  0.1927,  0.1514,\r\n         -0.4016,  0.2983,  0.0888, -0.5322],\r\n        [ 0.2203, -0.2715, -0.4641, -0.0901, -0.4060,  0.5386,  0.1970,  0.2397,\r\n         -0.3032,  0.4636, -0.2035, -0.3083]], device='cuda:0') torch.Size([32, 128])\r\n\r\nFlash attention:\r\ntensor([[ 0.2206,  0.5752,  0.4590, -0.5278,  0.2886,  0.0807, -0.1776,  0.4514,\r\n         -0.2896,  0.0796, -0.2534, -0.4487],\r\n        [ 0.0430,  0.8750,  0.6284, -0.7466, -0.1093,  0.5015, -0.8545, -0.5391,\r\n          0.4675, -0.0595,  0.3259, -0.1731],\r\n        [ 0.4812,  0.3066,  0.5342, -0.5845,  0.1986,  0.4680,  0.2181,  0.3647,\r\n         -0.4058,  0.3745, -0.0800, -0.4871],\r\n        [ 0.5420,  0.2603,  0.4045, -0.5083,  0.0300,  0.3479,  0.1929,  0.1514,\r\n         -0.4016,  0.2983,  0.0888, -0.5322],\r\n        [ 0.2203, -0.2712, -0.4641, -0.0900, -0.4060,  0.5386,  0.1971,  0.2397,\r\n         -0.3030,  0.4634, -0.2035, -0.3086]], device='cuda:0') torch.Size([32, 128])\r\n\r\nPaged attention:\r\ntensor([[ 0.2205,  0.5747,  0.4587, -0.5278,  0.2886,  0.0807, -0.1775,  0.4514,\r\n         -0.2893,  0.0797, -0.2534, -0.4487],\r\n        [ 0.0431,  0.8750,  0.6284, -0.7466, -0.1094,  0.5015, -0.8550, -0.5391,\r\n          0.4675, -0.0595,  0.3259, -0.1731],\r\n        [ 0.4812,  0.3069,  0.5342, -0.5845,  0.1987,  0.4678,  0.2181,  0.3647,\r\n         -0.4058,  0.3745, -0.0798, -0.4868],\r\n        [ 0.5425,  0.2603,  0.4043, -0.5083,  0.0299,  0.3479,  0.1929,  0.1511,\r\n         -0.4016,  0.2983,  0.0889, -0.5317],\r\n        [ 0.5698,  0.3608,  0.4990, -0.5796,  0.2260,  0.4634, -0.0039,  0.5776,\r\n         -0.2742,  0.1831, -0.4377, -0.4846]], device='cuda:0') torch.Size([32, 128])\r\n```\r\nIt looks like the first 4 heads are correct, but rest of heads are incorrect. Since I am using `num_head_kv=8` and `num_head=32`, which gives a group of 4, would this be the cause of the problem?\r\n\r\n\r\nI'd appreciate it if you could guide me on this. \r\nThanks!",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-03-23T15:39:13+00:00",
    "closed_at": "2024-03-23T23:30:29+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3585/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3585"
  },
  {
    "number": 12487,
    "title": "[Misc]: LoRA request with Multi GPU does not provide correct responses with num_scheduler_steps config",
    "body": "### Anything you want to discuss about vllm.\n\nHello All,\n\nWe are encountering a strange issue with our LoRA adapter, when running in multi-GPU setup.\n\nContext:\nBase model: Mistral Nemo 12B (https://huggingface.co/nvidia/Mistral-NeMo-12B-Instruct)\nAdapter Rank: 8\n\nVllm Model.json\n```json\n{\n    \"model\": \"/model-store/backbone/Mistral-Nemo-Base\",\n    \"disable_log_requests\": \"true\",\n    \"gpu_memory_utilization\": 0.85,\n    \"max_model_len\": 16000,\n    \"tensor_parallel_size\": 2,\n    \"distributed_executor_backend\": \"ray\",\n    \"enable_lora\": \"true\",\n    \"max_lora_rank\": 8,\n    \"max_loras\": 4,\n    \"trust_remote_code\": \"true\"\n}\n```\n\nMulti-lora.json\n```json\n{\n    \"t2f\": \"/model-store/backbone/loras/Mistral-Nemo-Base-t2f-lora\"\n}\n```\n\nNow, when we add the num_scheduler_steps configuration to the model.json, \n\n```json\n \"num_scheduler_steps\": 8,\n```\n\nNow the adapter responds with correct response when we don't have 'num_scheduler_steps' in the multi-GPU setup, but when we add this configuration, we don't get the correct response from the adapter any longer, even though everything else remains same.\n\nWe are looking at the response from the LoRA targeted request here not the response from Base model.\n\nHas anyone faced similar issue, are there any setting, configurations that need to be made to enable multi-gpu LoRA requests?\n\nThanks,\nRohit\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2025-01-27T20:41:23+00:00",
    "closed_at": "2025-02-01T05:05:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12487/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12487"
  },
  {
    "number": 8632,
    "title": "[Misc]: How are quantized models loaded compared to non-quantized models?",
    "body": "\r\nHi,\r\n\r\nI am trying to research MoE layer memory optimizations and I am using vLLM to do so. I have some custom logging code in the initializers/model code of the Mixtral model. When I load a quantized model, no logging code is executed. Simple print statements in the `MixtralModel.__init__` are not printed to screen. Is this on purpose? Where are the MoE kernels getting executed? \r\n\r\nThanks for any help, I have been stuck on this for a while.\r\n\r\nFor reference, I have tried to use the https://huggingface.co/TheBloke/mixtral-8x7b-v0.1-AWQ and I have quantized my own models with autoAWQ and bitsandbytes and the same behavior occurs.\r\n",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-09-19T11:53:50+00:00",
    "closed_at": "2024-09-20T16:58:28+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8632/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8632"
  },
  {
    "number": 4814,
    "title": "Remove EOS token before passing the tokenized input to model",
    "body": "\r\n\r\nHow to remove eos token id before passing the input tokens to model. I'm trying for fine-tuned mistral model. Just because there is an eos token id at the end of sentence, model generates the results for a different input which is similar to original input",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-05-14T17:36:47+00:00",
    "closed_at": "2024-05-29T00:15:36+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4814/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4814"
  },
  {
    "number": 2848,
    "title": "computation of prompt_logprobs",
    "body": "What are the distinctions between the computation of **prompt_logprobs** in input tokens and **logprobs** in output tokens?",
    "labels": [
      "question",
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-02-13T08:09:58+00:00",
    "closed_at": "2024-06-03T03:02:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2848/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2848"
  },
  {
    "number": 6570,
    "title": "Upgrade to numpy >= 2.0.0",
    "body": "Hi \ud83d\udc4b \r\n\r\nWould it be possible to upgrade the dependency of vLLM to numpy to remove the pinning [here](https://github.com/vllm-project/vllm/blob/main/requirements-common.txt#L5)?\r\n\r\nWe are running into a dependency conflict issue in our dependency chain and would appreciate the move to support more recent numpy versions.\r\n\r\nThanks!",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-07-19T09:44:44+00:00",
    "closed_at": "2025-04-10T06:52:10+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6570/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6570"
  },
  {
    "number": 6244,
    "title": "[Misc]: Min thread limitation inconsistency for gptq_marlin",
    "body": "### Anything you want to discuss about vllm.\n\nFor gptq_marlin, `min_thread_n=64 min_thread_k=64` is required in [https://github.com/vllm-project/vllm/blob/70c232f85a9e83421a4d9ca95e6384364271f2bc/csrc/quantization/gptq_marlin/gptq_marlin.cuh#L22-L23](url), while `min_thread_n=64 min_thread_k=128` is required in [https://github.com/vllm-project/vllm/blob/70c232f85a9e83421a4d9ca95e6384364271f2bc/vllm/model_executor/layers/quantization/utils/marlin_utils.py#L21-L22](url). Why the limitation is different?",
    "labels": [
      "misc",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-09T05:49:18+00:00",
    "closed_at": "2024-11-25T02:05:00+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6244/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/6244"
  },
  {
    "number": 11760,
    "title": "[Misc]: Very High GPU RX/TX using vllm",
    "body": "### Anything you want to discuss about vllm.\r\n\r\nI found there are very big size of data transfer to GPU when making a request with 10K tokens. \r\nVLLM result a very high TTFT compare to Ollama.\r\n\r\nI dont think it is a normal data size of 10K tokens. \r\n\r\nvllm version: v0.6.4.post1\r\nThere is how I run vllm\r\n`\r\nvllm serve Qwen/Qwen2.5-32B-Instruct-AWQ --pipeline-parallel-size 2 --enable-auto-tool-choice --tool-call-parser hermes --gpu-memory-utilization 0.9 --max_model_len 32000  --max-num-seqs 5 --kv-cache-dtype fp8_e4m3\r\n`\r\n\r\nThere is my GPU receiving data (over 10GiB/s RX)\r\n![image](https://github.com/user-attachments/assets/87de1545-f7c6-4375-af9e-f99e51ac45f5)\r\n\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-06T06:17:32+00:00",
    "closed_at": "2025-05-15T02:09:29+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11760/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11760"
  },
  {
    "number": 13777,
    "title": "[Misc] [ROCm]: Build from source failure with Arch/gcc14 with ROCm 6.3",
    "body": "### Anything you want to discuss about vllm.\n\nHi team!\n\nBeen trying to build vllm from source for ROCm 6.3 for gfx1100 on Arch/gcc14 following the instructions from the official documentation. Kept running into a compile error on the hipify step during the build:-\n\nExcerpt from error -\n```\n...\n\nIn file included from <built-in>:1:\nIn file included from /opt/rocm/lib/llvm/lib/clang/18/include/__clang_hip_runtime_wrapper.h:145:\nIn file included from /opt/rocm/lib/llvm/lib/clang/18/include/cuda_wrappers/algorithm:55:\nIn file included from /usr/lib64/gcc/x86_64-pc-linux-gnu/14.2.1/../../../../include/c++/14.2.1/algorithm:61:\n/usr/lib64/gcc/x86_64-pc-linux-gnu/14.2.1/../../../../include/c++/14.2.1/bits/stl_algo.h:3626:7: error: reference to __host__ function '__glibcxx_assert_fail' in __host__ __device__ function\n 3626 |       __glibcxx_assert(!(__hi < __lo));\n      |       ^\n/usr/lib64/gcc/x86_64-pc-linux-gnu/14.2.1/../../../../include/c++/14.2.1/x86_64-pc-linux-gnu/bits/c++config.h:614:12: note: expanded from macro '__glibcxx_assert'\n  614 |       std::__glibcxx_assert_fail();                                     \\\n      |            ^\n/home/<username>/Documents/sources/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/compressed_tensors/int8_quant_kernels.hip:35:14: note: called by 'float_to_int8_rn'\n   35 |   dst = std::clamp(dst, i8_min, i8_max);\n      |              ^\n/home/<username>/Documents/sources/vllm/build/temp.linux-x86_64-cpython-312/csrc/quantization/compressed_tensors/int8_quant_kernels.hip:119:14: note: called by 'static_scaled_int8_quant_kernel<float, float>'\n  119 |     out[i] = float_to_int8_rn(static_cast<float>(input[i]) / scale);\n      |              ^\n/usr/lib64/gcc/x86_64-pc-linux-gnu/14.2.1/../../../../include/c++/14.2.1/x86_64-pc-linux-gnu/bits/c++config.h:608:3: note: '__glibcxx_assert_fail' declared here\n  608 |   __glibcxx_assert_fail()\n      |   ^\n1 error generated when compiling for gfx1100.\n\n...\n```\n\nOn further investigations into why the error, it seems the `std::clamp` function was the issue. For reasons, this seems to not work when compiling with gcc14/hip-clang.\n\nA bit more looking into this and i found that this is a known issue at pytorch and LLVM projects, see: -\n- Pytorch: https://github.com/pytorch/pytorch/issues/127666\n- LLVM: https://github.com/llvm/llvm-project/issues/95183\n\nThe fix/work-around Pytorch went with for this was replacing `std::clamp` usage with similar logic ([see commit](https://github.com/pytorch/pytorch/commit/df5919393c95dc93fd2e9f4b9c576b161387d024))\n\nI implemented that here and it went on and compiled successfully after sorting out all the offending files/places!\n\nWill submit a PR with the changes soon \u270c\ud83c\udffb\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc"
    ],
    "state": "open",
    "created_at": "2025-02-24T17:52:19+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13777/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/13777"
  },
  {
    "number": 4437,
    "title": "[Misc]: need \"first good issue\"",
    "body": "### Anything you want to discuss about vllm.\n\nAs a beginner, there are too many issues and PRs, and I find it hard to start contributing.\r\n\r\nCould anyone please add `good first issue` label to some issues? So that beginner like me can get started quickly.\r\n\r\nThanks!",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-04-28T14:49:54+00:00",
    "closed_at": "2024-05-31T12:13:00+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4437/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4437"
  },
  {
    "number": 8205,
    "title": "[Misc]: benchmark_serving with image input",
    "body": "### Anything you want to discuss about vllm.\n\nCan the current benchmark_serving.py be used with Multimodal LLM (llava) and image input? The existing code send the request in the following format in backend_request_func.py, Is it possible to make it support image input?\r\n \r\n```\r\n        payload = {\r\n            \"model\": request_func_input.model,\r\n            \"messages\": [\r\n                {\r\n                    \"role\": \"user\",\r\n                    \"content\": request_func_input.prompt,\r\n                },\r\n            ],\r\n            \"temperature\": 0.0,\r\n            \"max_tokens\": request_func_input.output_len,\r\n            \"stream\": True,\r\n        }\r\n        headers = {\r\n            \"Content-Type\": \"application/json\",\r\n            \"Authorization\": f\"Bearer {os.environ.get('OPENAI_API_KEY')}\",\r\n        }\r\n```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-09-05T18:43:02+00:00",
    "closed_at": "2024-09-17T07:34:28+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8205/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8205"
  },
  {
    "number": 5308,
    "title": "[Misc]: Question about adding more kv cache blocks",
    "body": "### Anything you want to discuss about vllm.\n\nIs it theoretically possible to increase the total amount of kv cache blocks by adding new GPU resources, that only for kv cache allocation, without using tensor parallel or other parallel?",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-06-06T05:47:00+00:00",
    "closed_at": "2024-06-06T14:48:50+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5308/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5308"
  },
  {
    "number": 3852,
    "title": "[Misc]: Can we remove `vllm/entrypoints/api_server.py`?",
    "body": "### Anything you want to discuss about vllm.\n\nWhile gardening GitHub issues I've seen many issues where the user is using `-m vllm.entrypoints.api_server`, which indicates that users are either not aware of ignoring the note at the top of the file:\r\n\r\nhttps://github.com/vllm-project/vllm/blob/b7782002e1da25de77e0b1890ff8b72dd4df917c/vllm/entrypoints/api_server.py#L1-L7\r\n\r\nCan we remove it to avoid future confusion?",
    "labels": [
      "misc",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-04T12:58:49+00:00",
    "closed_at": "2024-11-28T02:06:58+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3852/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/3852"
  },
  {
    "number": 4563,
    "title": "[Misc]: Server Does Not Follow Scheduler Policy",
    "body": "### Anything you want to discuss about vllm.\n\nI was testing out vLLM on Colab and notices something weird. It seems from the code that vLLM is using first come first serve order policy:\r\n\r\nhttps://github.com/vllm-project/vllm/blob/7038e8b80303bf6128acbe508dec910183a1be56/vllm/core/scheduler.py#L729\r\nhttps://github.com/vllm-project/vllm/blob/7038e8b80303bf6128acbe508dec910183a1be56/vllm/core/policy.py#L29-L36\r\n\r\nHowever, When I was running the OpenAI compatible vLLM server, I sent in orders in sequence and found the server to not follow the first come first serve policy. Instead, they seemed random? Here is a example jupyter notebook replicating the issue:\r\nhttps://colab.research.google.com/drive/1mMPTZiKJoQEsvjBjNUGttsbp9L1F9zXm?usp=sharing\r\n\r\nIs there some optimization I missed that optimized the order of inputs? I am a bit confused on what controls the server's output order. Any advice would be appreciated, thanks!",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-05-02T17:47:32+00:00",
    "closed_at": "2024-05-04T16:23:24+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4563/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4563"
  },
  {
    "number": 7397,
    "title": "[Misc]: Cross-attention QKV computation is inefficient",
    "body": "This issue is not in response to a performance regression.\r\n\r\nThe method of performing cross-attention QKV computations introduced in #4942 could be improved. Because this issue relates to cross-attention, it only impacts encoder/decoder models, not decoder-only models.\r\n\r\nFor context, `QKVParallelLinear` computes QKV from the previous decoder layer's hidden state output, i.e. only a single input. The problem is that cross attention requires QKV to be computed from two inputs: Q must be computed from the previous decoder layer's hidden state output, and KV must be computed from the encoder's output hidden states. Additionally,\r\n* During prefill phase, both Q and KV must be computed\r\n* During decode phase, only Q is computed because the encoder sequence is static so there are no new encoder KVs\r\n\r\nThe current, inefficient workaround for cross-attention is to construct a `QKVParallelLinear` layer & apply it at most 2 times in a given run of the cross-attention `forward()` method: once to `decoder_hidden_states` to obtain Q, and (only during prefill) a second time to `encoder_hidden_states` to obtain KV:\r\n\r\n```\r\n# (afeldman-nm 2024/07/22) TODO:\r\n# Need a more efficient solution for q/k/v\r\nqkv_dec, _ = self.qkv_proj(decoder_hidden_states)\r\nq, _, _ = qkv_dec.split([self.q_size, self.kv_size, self.kv_size],\r\n                        dim=-1)\r\nif encoder_hidden_states is None:\r\n    k = None\r\n    v = None\r\nelse:\r\n    qkv_enc, _ = self.qkv_proj(encoder_hidden_states)\r\n    _, k, v = qkv_enc.split([self.q_size, self.kv_size, self.kv_size],\r\n                            dim=-1)\r\n```\r\n\r\n## Cost breakdown of the current method\r\n\r\nDuring prefill,\r\n* $(decoder\\ hidden\\ states) [W_Q W_K W_V]$ is computed in order to obtain Q, so 2 out of 3 GEMMs are unnecessary\r\n* $(encoder\\ hidden\\ states) [W_Q W_K W_V]$ is computed in order to obtain KV, so 1 out of 3 GEMMs are unnecessary\r\n* **In total**, half of GEMMs are unnecessary (50% efficiency)\r\n\r\nDuring decode\r\n* $(decoder\\ hidden\\ states) [W_Q W_K W_V]$ is computed in order to obtain Q, so 2/3 of GEMMs are unnecessary (33% efficiency)\r\n\r\n## Proposed solution\r\n\r\nWhat is needed is a modification or subclass to `QKVParallelLinear` with the following properties\r\n* Exploits parallelism over multiple GPUs\r\n* `forward()` takes a decoder hidden states argument, and an optional encoder hidden states argument\r\n* `forward()` always computes $(decoder\\ hidden\\ states) W_Q$\r\n* `forward()` computes $(encoder\\ hidden\\ states) [W_K W_V]$ conditionally: only if the encoder hidden states are not `None`\r\n* 100% of GEMMs are necessary",
    "labels": [
      "misc",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-10T16:43:36+00:00",
    "closed_at": "2024-12-12T02:06:53+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7397/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7397"
  },
  {
    "number": 13058,
    "title": "[V1][Help Wanted] Porting missing sampling parameters to V1",
    "body": "### Anything you want to discuss about vllm.\n\nTo switch the engine from V0 to V1, we need to comprehensively support the sampling parameters in https://github.com/vllm-project/vllm/blob/main/vllm/sampling_params.py\n\nWhile most of the key parameters are already supported, some of them are missing:\n\nTODO (help wanted):\n- [x] `n` (parallel sampling) #10980  @afeldman-nm \n- [x] `guided_decoding` (structured decoding) #12388  @aarnphm \n- [x] `logit_bias` #13079 @houseroad \n- [x] `min_p` #13191 @AoyuQC\n- [ ] `bad_words` (originally implemented via logits processor) #13376 @22quinn \n- [x] `allowed_token_ids` (originally implemented via logits processor) #13210 @houseroad \n\nParameters that will not be supported in V1:\n* best_of\n* logits_processors\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "misc",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-02-10T23:13:42+00:00",
    "closed_at": "2025-03-20T14:15:16+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13058/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/13058"
  },
  {
    "number": 6084,
    "title": "[Misc]: Best practice for accelerating and deploying Llava series & Phi3-Vision using vLLM",
    "body": "### Anything you want to discuss about vllm.\r\n\r\nVLLM is a great inference acceleration framework for LLM and multimodal LLM!!!\r\n\r\nThe ms-swift LLM toolbox has integrated vLLM to accelerate and deploy multimodal models (currently including the Llava series and Phi3-Vision).\r\n\r\nBest practice can be found at:\r\n\r\nEnglish: https://github.com/modelscope/swift/blob/main/docs/source_en/Multi-Modal/vllm-inference-acceleration.md\r\n\r\n\u4e2d\u6587\uff1ahttps://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/vLLM%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E6%96%87%E6%A1%A3.md",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-07-03T02:14:01+00:00",
    "closed_at": "2024-07-09T04:46:07+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6084/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/6084"
  },
  {
    "number": 8419,
    "title": "[Misc]: Can vLLM go out of memory during the decode phase if too many tokens are generated?",
    "body": "### Anything you want to discuss about vllm.\n\nCan vLLM go out of memory during the decode phase if too many tokens are generated?\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-12T14:46:09+00:00",
    "closed_at": "2025-01-12T02:05:57+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8419/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8419"
  },
  {
    "number": 8362,
    "title": "[Misc]: vLLM v0.6.0  CUDA 12 missing wheel file",
    "body": "### Anything you want to discuss about vllm.\n\nI'd like to understand why the most recent release is omitting the CUDA 12 wheel package built?\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-11T09:10:52+00:00",
    "closed_at": "2025-01-11T01:59:44+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8362/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8362"
  },
  {
    "number": 9158,
    "title": "[Misc]: How to set num-scheduler-steps",
    "body": "### Anything you want to discuss about vllm.\n\nRecently **num-scheduler-steps** was introduced to \"set the maximum number of forward steps per scheduler call\". Is there any documentation on what this exactly means?\r\nAlso some guidance would on how to set this value would be much appreciated. For example, if I host a 70B model on 2x A100 with 80GB, does this narrow down the range of values I should consider?\r\n\r\nThanks to all the amazing vllm contributers for making this great peace of software! \ud83c\udfce\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-08T13:23:11+00:00",
    "closed_at": "2025-02-07T01:59:39+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9158/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9158"
  }
]
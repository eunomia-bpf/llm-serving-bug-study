[
  {
    "number": 18589,
    "title": "[Bug][Failing Test]: Distributed Tests (A100) - distributed/test_ca_buffer_sharing.py",
    "body": "### Your current environment\n\nN/A\n\n### \ud83d\udc1b Describe the bug\n\nhttps://buildkite.com/vllm/ci/builds/20544/steps?jid=0196f845-c352-4203-a55f-efb442b65c7d\n\ncc @robertgshaw2-redhat \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci-failure"
    ],
    "state": "open",
    "created_at": "2025-05-23T04:35:17+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18589/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18589"
  },
  {
    "number": 19574,
    "title": "[CI Failure]: lint-and-deploy: unexpected HTTP status 500",
    "body": "### Name of failing test\n\nlint-and-deploy\n\n### Basic information\n\n- [ ] Flaky test\n- [ ] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\nhttps://github.com/vllm-project/vllm/actions/runs/15618574945/job/43997475275?pr=19573\n\nhttps://github.com/vllm-project/vllm/actions/runs/15618287178/job/43996526081?pr=19572\n\n```bash\nRun helm/chart-testing-action@0d28d3144d3a25ea2cc349d6e59901c4ff469b3b\nRun sigstore/cosign-installer@dc72c7d5c4d10cd6bcb8cf6e3fd625a9e5e537da\nRun #!/bin/bash\nINFO: Downloading bootstrap version 'v2.4.1' of cosign to verify version to be installed...\n      https://github.com/sigstore/cosign/releases/download/v2.4.1/cosign-linux-amd64\nINFO: bootstrap version successfully verified and matches requested version so nothing else to do\nRun echo \"$HOME/.cosign\" >> $GITHUB_PATH\nRun cd $GITHUB_ACTION_PATH \\\nInstalling chart-testing v3.10.1...\nError: getting Rekor public keys: updating local metadata and targets: error updating to TUF remote mirror: tuf: failed to download timestamp.json: GET \"https://tuf-repo-cdn.sigstore.dev/timestamp.json\": unexpected HTTP status 500\nmain.go:74: error during command execution: getting Rekor public keys: updating local metadata and targets: error updating to TUF remote mirror: tuf: failed to download timestamp.json: GET \"https://tuf-repo-cdn.sigstore.dev/timestamp.json\": unexpected HTTP status 500\nError: Process completed with exit code 1.\n```\n\n### \ud83d\udcdd History of failing test\n\nJust one hour ago\n\n### CC List.\n\n_No response_",
    "labels": [
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-06-12T18:52:35+00:00",
    "closed_at": "2025-06-13T14:37:47+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19574/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19574"
  },
  {
    "number": 18466,
    "title": "[Bug][Failing Test] distributed tests (4 GPUS) - v1/test_async_llm_dp.py::test_load",
    "body": "### Your current environment\n\nStill failing on main as of commit 0c15c2e486\n\n### \ud83d\udc1b Describe the bug\n\nFailing tests: https://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests?branch=main&period=2days&query=test_async_llm_dp&commit=Search\n\n```\nFAILED v1/test_async_llm_dp.py::test_load[RequestOutputKind.DELTA] - vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\nFAILED v1/test_async_llm_dp.py::test_load[RequestOutputKind.FINAL_ONLY] - vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n```\n\n<details>\n\n<summary>Logs</summary>\n\n```\n(EngineCore_0 pid=4396) (VllmWorker rank=1 pid=4418) WARNING 05-20 22:23:45 [fused_moe.py:682] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=40,N=128,device_name=NVIDIA_L4.json\n(EngineCore_1 pid=4399) (VllmWorker rank=1 pid=4417) WARNING 05-20 22:23:45 [fused_moe.py:682] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=40,N=128,device_name=NVIDIA_L4.json\n(EngineCore_1 pid=4399) (VllmWorker rank=0 pid=4415) WARNING 05-20 22:23:45 [fused_moe.py:682] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=40,N=128,device_name=NVIDIA_L4.json\n(EngineCore_0 pid=4396) (VllmWorker rank=0 pid=4416) WARNING 05-20 22:23:45 [fused_moe.py:682] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=40,N=128,device_name=NVIDIA_L4.json\n(EngineCore_0 pid=4396) INFO 05-20 22:23:46 [kv_cache_utils.py:637] GPU KV cache size: 576,432 tokens\n(EngineCore_0 pid=4396) INFO 05-20 22:23:46 [kv_cache_utils.py:640] Maximum concurrency for 4,096 tokens per request: 140.73x\n(EngineCore_0 pid=4396) INFO 05-20 22:23:46 [kv_cache_utils.py:637] GPU KV cache size: 576,432 tokens\n(EngineCore_0 pid=4396) INFO 05-20 22:23:46 [kv_cache_utils.py:640] Maximum concurrency for 4,096 tokens per request: 140.73x\n(EngineCore_1 pid=4399) INFO 05-20 22:23:46 [kv_cache_utils.py:637] GPU KV cache size: 576,432 tokens\n(EngineCore_1 pid=4399) INFO 05-20 22:23:46 [kv_cache_utils.py:640] Maximum concurrency for 4,096 tokens per request: 140.73x\n(EngineCore_1 pid=4399) INFO 05-20 22:23:46 [kv_cache_utils.py:637] GPU KV cache size: 576,432 tokens\n(EngineCore_1 pid=4399) INFO 05-20 22:23:46 [kv_cache_utils.py:640] Maximum concurrency for 4,096 tokens per request: 140.73x\n(EngineCore_1 pid=4399) INFO 05-20 22:23:46 [core.py:163] init engine (profile, create kv cache, warmup model) took 1.83 seconds\n(EngineCore_0 pid=4396) INFO 05-20 22:23:46 [core.py:163] init engine (profile, create kv cache, warmup model) took 1.83 seconds\n[rank1]:[E520 22:23:46.546600506 ProcessGroupNCCL.cpp:1896] [PG ID 2 PG GUID 3 Rank 1] Process group watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nException raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f57929785e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x7f579290d4a2 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x7f5792e0b422 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7f572268b456 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x7f572269b6f0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x7f572269d282 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f572269ee8d in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0xdc253 (0x7f57129b3253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #8: <unknown function> + 0x94ac3 (0x7f58141a2ac3 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #9: clone + 0x44 (0x7f5814233a04 in /lib/x86_64-linux-gnu/libc.so.6)\n\nterminate called after throwing an instance of 'c10::DistBackendError'\n[rank1]:[E520 22:23:46.547487691 ProcessGroupNCCL.cpp:1896] [PG ID 4 PG GUID 17 Rank 0] Process group watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nException raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f57929785e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x7f579290d4a2 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x7f5792e0b422 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7f572268b456 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x7f572269b6f0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x7f572269d282 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f572269ee8d in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0xdc253 (0x7f57129b3253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #8: <unknown function> + 0x94ac3 (0x7f58141a2ac3 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #9: clone + 0x44 (0x7f5814233a04 in /lib/x86_64-linux-gnu/libc.so.6)\n\nterminate called recursively\nFatal Python error: Aborted\n\nThread 0x00007f57a77d7640 (most recent call first):\n  File \"/usr/lib/python3.12/threading.py\", line 359 in wait\n  File \"/usr/lib/python3.12/threading.py\", line 655 in wait\n  File \"/usr/local/lib/python3.12/dist-packages/tqdm/_monitor.py\", line 60 in run\n  File \"/usr/lib/python3.12/threading.py\", line 1075 in _bootstrap_inner\n  File \"/usr/lib/python3.12/threading.py\", line 1032 in _bootstrap\n\nThread 0x00007f581410d000 (most recent call first):\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/shm_broadcast.py\", line 425 in acquire_read\n  File \"/usr/lib/python3.12/contextlib.py\", line 137 in __enter__\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/shm_broadcast.py\", line 479 in dequeue\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 510 in worker_busy_loop\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 478 in worker_main\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108 in run\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314 in _bootstrap\n  File \"/usr/lib/python3.12/multiprocessing/popen_fork.py\", line 71 in _launch\n  File \"/usr/lib/python3.12/multiprocessing/popen_fork.py\", line 19 in __init__\n  File \"/usr/lib/python3.12/multiprocessing/context.py\", line 282 in _Popen\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 121 in start\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 386 in make_worker_process\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 82 in _init_executor\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 52 in __init__\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 67 in __init__\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 379 in __init__\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 679 in __init__\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 478 in run_engine_core\n  File \"  what():  /usr/lib/python3.12/multiprocessing/process.py[PG ID 2 PG GUID 3 Rank 1] Process group watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nException raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f57929785e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x7f579290d4a2 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x7f5792e0b422 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7f572268b456 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x7f572269b6f0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x7f572269d282 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f572269ee8d in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0xdc253 (0x7f57129b3253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #8: <unknown function> + 0x94ac3 (0x7f58141a2ac3 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #9: clone + 0x44 (0x7f5814233a04 in /lib/x86_64-linux-gnu/libc.so.6)\n\nException raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1902 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f57929785e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0xcc7a4e (0x7f572266da4e in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #2: <unknown function> + 0x9165ed (0x7f57222bc5ed in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\nframe #3: <unknown function> + 0xdc253 (0x7f57129b3253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #4: <unknown function> + 0x94ac3 (0x7f58141a2ac3 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #5: clone + 0x44 (0x7f5814233a04 in /lib/x86_64-linux-gnu/libc.so.6)\n\"\n, line 108 in run\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314 in _bootstrap\n  File \"/usr/lib/python3.12/multiprocessing/popen_fork.py\", line 71 in _launch\n  File \"/usr/lib/python3.12/multiprocessing/popen_fork.py\", line 19 in __init__\n  File \"/usr/lib/python3.12/multiprocessing/context.py\", line 282 in _Popen\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 121 in start\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/utils.py\", line 142 in __init__\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 404 in __init__\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 734 in __init__\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\"DEBUG 05-20 22:23:46 [core_client.py:540] READY from local core engine process 0.\n(EngineCore_0 pid=4396) DEBUG 05-20 22:23:46 [core.py:517] EngineCore waiting for work.\nDEBUG 05-20 22:23:46 [core_client.py:540] READY from local core engine process 1.\nINFO 05-20 22:23:46 [loggers.py:134] vllm cache_config_info with initialization after num_gpu_blocks is: 72054\n(EngineCore_1 pid=4399) DEBUG 05-20 22:23:46 [core.py:517] EngineCore waiting for work.\nDEBUG 05-20 22:23:46 [core_client.py:992] Sending start DP wave 0.\n(EngineCore_0 pid=4396) DEBUG 05-20 22:23:46 [core.py:523] EngineCore loop active.\n(EngineCore_1 pid=4399) DEBUG 05-20 22:23:46 [core.py:728] EngineCore starting idle loop for wave 0.\n(EngineCore_1 pid=4399) DEBUG 05-20 22:23:46 [core.py:523] EngineCore loop active.\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [dump_input.py:68] Dumping input data\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [dump_input.py:70] V1 LLM engine (v0.9.1.dev18+g0c15c2e48) with config: model='ibm-research/PowerMoE-3b', speculative_config=None, tokenizer='ibm-research/PowerMoE-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ibm-research/PowerMoE-3b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={\"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"cudagraph_capture_sizes\": [], \"max_capture_size\": 0}, \n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [dump_input.py:78] Dumping scheduler output for model execution:\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [dump_input.py:79] SchedulerOutput(scheduled_new_reqs=[NewRequestData(req_id=request-0,prompt_token_ids_len=7,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=[[1]],num_computed_tokens=0,lora_request=None)],scheduled_cached_reqs=[],num_scheduled_tokens={request-0: 7},total_num_scheduled_tokens=7,scheduled_spec_decode_tokens={},scheduled_encoder_inputs={},num_common_prefix_blocks=[1],finished_req_ids=[],free_encoder_input_ids=[],structured_output_request_ids={},grammar_bitmask=null,kv_connector_metadata=null)\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [dump_input.py:81] SchedulerStats(num_running_reqs=1, num_waiting_reqs=0, gpu_cache_usage=5.5513920115490833e-05, prefix_cache_stats=PrefixCacheStats(reset=False, requests=1, queries=7, hits=0), spec_decoding_stats=None)\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491] EngineCore encountered a fatal error.\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491] Traceback (most recent call last):\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 215, in collective_rpc\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]     result = get_response(w, dequeue_timeout)\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 198, in get_response\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]     status, result = w.worker_response_mq.dequeue(\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/shm_broadcast.py\", line 479, in dequeue\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]     with self.acquire_read(timeout, cancel) as buf:\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]   File \"/usr/lib/python3.12/contextlib.py\", line 137, in __enter__\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]     return next(self.gen)\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]            ^^^^^^^^^^^^^^\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/distributed/device_communicators/shm_broadcast.py\", line 443, in acquire_read\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]     raise TimeoutError\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491] TimeoutError\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491] \n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491] The above exception was the direct cause of the following exception:\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491] \n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491] Traceback (most recent call last):\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 482, in run_engine_core\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]     engine_core.run_busy_loop()\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 746, in run_busy_loop\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]     self._process_engine_step()\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 534, in _process_engine_step\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]     outputs = self.step_fn()\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]               ^^^^^^^^^^^^^^\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 222, in step\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]     model_output = self.execute_model(scheduler_output)\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 209, in execute_model\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]     raise err\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 203, in execute_model\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]     return self.model_executor.execute_model(scheduler_output)\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 158, in execute_model\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]     (output, ) = self.collective_rpc(\"execute_model\",\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 221, in collective_rpc\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491]     raise TimeoutError(f\"RPC call to {method} timed out.\") from e\n\n(EngineCore_0 pid=4396) ERROR 05-20 22:24:26 [core.py:491] TimeoutError: RPC call to execute_model timed out.\nERROR 05-20 22:24:26 [async_llm.py:403] AsyncLLM output_handler failed.\n\nERROR 05-20 22:24:26 [async_llm.py:403] Traceback (most recent call last):\n\nERROR 05-20 22:24:26 [async_llm.py:403]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 361, in output_handler\n\nERROR 05-20 22:24:26 [async_llm.py:403]     outputs = await engine_core.get_output_async()\n\nERROR 05-20 22:24:26 [async_llm.py:403]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nERROR 05-20 22:24:26 [async_llm.py:403]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 806, in get_output_async\n\nERROR 05-20 22:24:26 [async_llm.py:403]     raise self._format_exception(outputs) from None\n\nERROR 05-20 22:24:26 [async_llm.py:403] vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n(EngineCore_1 pid=4399) DEBUG 05-20 22:24:26 [core.py:485] EngineCore exiting.\n/usr/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\nFAILED\n\n=================================== FAILURES ===================================\n______________________ test_load[RequestOutputKind.DELTA] ______________________\n\noutput_kind = <RequestOutputKind.DELTA: 1>\n\n    @pytest.mark.parametrize(\n        \"output_kind\", [RequestOutputKind.DELTA, RequestOutputKind.FINAL_ONLY])\n    @pytest.mark.asyncio\n    async def test_load(output_kind: RequestOutputKind):\n    \n        with ExitStack() as after:\n    \n            prompt = \"This is a test of data parallel\"\n    \n            engine = AsyncLLM.from_engine_args(engine_args)\n            after.callback(engine.shutdown)\n    \n            NUM_REQUESTS = 100\n            NUM_EXPECTED_TOKENS = 10\n    \n            request_ids = [f\"request-{i}\" for i in range(NUM_REQUESTS)]\n    \n            # Create concurrent requests.\n            tasks = []\n            for request_id in request_ids:\n                tasks.append(\n                    asyncio.create_task(\n                        generate(engine, request_id, prompt, output_kind,\n                                 NUM_EXPECTED_TOKENS)))\n    \n            # Confirm that we got all the EXPECTED tokens from the requests.\n            done, pending = await asyncio.wait(tasks,\n                                               return_when=asyncio.FIRST_EXCEPTION)\n            for task in pending:\n                task.cancel()\n            for task in done:\n>               num_generated_tokens, request_id = await task\n\nv1/test_async_llm_dp.py:92: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:361: in output_handler\n    outputs = await engine_core.get_output_async()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <vllm.v1.engine.core_client.DPAsyncMPClient object at 0x7f55bebbe8d0>\n\n    async def get_output_async(self) -> EngineCoreOutputs:\n        self._ensure_output_queue_task()\n        # If an exception arises in process_outputs_socket task,\n        # it is forwarded to the outputs_queue so we can raise it\n        # from this (run_output_handler) task to shut down the server.\n        assert self.outputs_queue is not None\n        outputs = await self.outputs_queue.get()\n        if isinstance(outputs, Exception):\n>           raise self._format_exception(outputs) from None\nE           vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:806: EngineDeadError\n___________________ test_load[RequestOutputKind.FINAL_ONLY] ____________________\n\noutput_kind = <RequestOutputKind.FINAL_ONLY: 2>\n\n    @pytest.mark.parametrize(\n        \"output_kind\", [RequestOutputKind.DELTA, RequestOutputKind.FINAL_ONLY])\n    @pytest.mark.asyncio\n    async def test_load(output_kind: RequestOutputKind):\n    \n        with ExitStack() as after:\n    \n            prompt = \"This is a test of data parallel\"\n    \n            engine = AsyncLLM.from_engine_args(engine_args)\n            after.callback(engine.shutdown)\n    \n            NUM_REQUESTS = 100\n            NUM_EXPECTED_TOKENS = 10\n    \n            request_ids = [f\"request-{i}\" for i in range(NUM_REQUESTS)]\n    \n            # Create concurrent requests.\n            tasks = []\n            for request_id in request_ids:\n                tasks.append(\n                    asyncio.create_task(\n                        generate(engine, request_id, prompt, output_kind,\n                                 NUM_EXPECTED_TOKENS)))\n    \n            # Confirm that we got all the EXPECTED tokens from the requests.\n            done, pending = await asyncio.wait(tasks,\n                                               return_when=asyncio.FIRST_EXCEPTION)\n            for task in pending:\n                task.cancel()\n            for task in done:\n>               num_generated_tokens, request_id = await task\n\nv1/test_async_llm_dp.py:92: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\nv1/test_async_llm_dp.py:46: in generate\n    async for out in engine.generate(request_id=request_id,\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:310: in generate\n    out = q.get_nowait() or await q.get()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py:51: in get\n    raise output\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py:361: in output_handler\n    outputs = await engine_core.get_output_async()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <vllm.v1.engine.core_client.DPAsyncMPClient object at 0x7f57f85341d0>\n\n    async def get_output_async(self) -> EngineCoreOutputs:\n        self._ensure_output_queue_task()\n        # If an exception arises in process_outputs_socket task,\n        # it is forwarded to the outputs_queue so we can raise it\n        # from this (run_output_handler) task to shut down the server.\n        assert self.outputs_queue is not None\n        outputs = await self.outputs_queue.get()\n        if isinstance(outputs, Exception):\n>           raise self._format_exception(outputs) from None\nE           vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:806: EngineDeadError\n---------------------------- Captured log teardown -----------------------------\nERROR    asyncio:base_events.py:1833 Task exception was never retrieved\nfuture: <Task finished name='Task-103' coro=<generate() done, defined at /vllm-workspace/tests/v1/test_async_llm_dp.py:31> exception=EngineDeadError('EngineCore encountered an issue. See stack trace (above) for the root cause.')>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/_pytest/runner.py\", line 341, in from_call\n    result: TResult | None = func()\n                             ^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/_pytest/runner.py\", line 242, in <lambda>\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pluggy/_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pluggy/_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pluggy/_callers.py\", line 182, in _multicall\n    return outcome.get_result()\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pluggy/_result.py\", line 100, in get_result\n    raise exc.with_traceback(exc.__traceback__)\n  File \"/usr/local/lib/python3.12/dist-packages/pluggy/_callers.py\", line 167, in _multicall\n    teardown.throw(outcome._exception)\n  File \"/usr/local/lib/python3.12/dist-packages/_pytest/threadexception.py\", line 92, in pytest_runtest_call\n    yield from thread_exception_runtest_hook()\n  File \"/usr/local/lib/python3.12/dist-packages/_pytest/threadexception.py\", line 68, in thread_exception_runtest_hook\n    yield\n  File \"/usr/local/lib/python3.12/dist-packages/pluggy/_callers.py\", line 167, in _multicall\n    teardown.throw(outcome._exception)\n  File \"/usr/local/lib/python3.12/dist-packages/_pytest/unraisableexception.py\", line 95, in pytest_runtest_call\n    yield from unraisable_exception_runtest_hook()\n  File \"/usr/local/lib/python3.12/dist-packages/_pytest/unraisableexception.py\", line 70, in unraisable_exception_runtest_hook\n    yield\n  File \"/usr/local/lib/python3.12/dist-packages/pluggy/_callers.py\", line 167, in _multicall\n    teardown.throw(outcome._exception)\n  File \"/usr/local/lib/python3.12/dist-packages/_pytest/logging.py\", line 846, in pytest_runtest_call\n    yield from self._runtest_for(item, \"call\")\n  File \"/usr/local/lib/python3.12/dist-packages/_pytest/logging.py\", line 829, in _runtest_for\n    yield\n  File \"/usr/local/lib/python3.12/dist-packages/pluggy/_callers.py\", line 167, in _multicall\n    teardown.throw(outcome._exception)\n  File \"/usr/local/lib/python3.12/dist-packages/_pytest/capture.py\", line 880, in pytest_runtest_call\n    return (yield)\n            ^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pluggy/_callers.py\", line 167, in _multicall\n    teardown.throw(outcome._exception)\n  File \"/usr/local/lib/python3.12/dist-packages/_pytest/skipping.py\", line 257, in pytest_runtest_call\n    return (yield)\n            ^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pluggy/_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/_pytest/runner.py\", line 174, in pytest_runtest_call\n    item.runtest()\n  File \"/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py\", line 457, in runtest\n    super().runtest()\n  File \"/usr/local/lib/python3.12/dist-packages/_pytest/python.py\", line 1627, in runtest\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\n  File \"/usr/local/lib/python3.12/dist-packages/pluggy/_hooks.py\", line 513, in __call__\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pluggy/_manager.py\", line 120, in _hookexec\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pluggy/_callers.py\", line 182, in _multicall\n    return outcome.get_result()\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pluggy/_result.py\", line 100, in get_result\n    raise exc.with_traceback(exc.__traceback__)\n  File \"/usr/local/lib/python3.12/dist-packages/pluggy/_callers.py\", line 167, in _multicall\n    teardown.throw(outcome._exception)\n  File \"/usr/local/lib/python3.12/dist-packages/schemathesis/extra/pytest_plugin.py\", line 312, in pytest_pyfunc_call\n    yield\n  File \"/usr/local/lib/python3.12/dist-packages/pluggy/_callers.py\", line 103, in _multicall\n    res = hook_impl.function(*args)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/_pytest/python.py\", line 159, in pytest_pyfunc_call\n    result = testfunction(**testargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pytest_asyncio/plugin.py\", line 929, in inner\n    _loop.run_until_complete(task)\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 92, in test_load\n    num_generated_tokens, request_id = await task\n                                       ^^^^^^^^^^\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/vllm-workspace/tests/v1/test_async_llm_dp.py\", line 46, in generate\n    async for out in engine.generate(request_id=request_id,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 310, in generate\n    out = q.get_nowait() or await q.get()\n                            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/output_processor.py\", line 51, in get\n    raise output\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 361, in output_handler\n    outputs = await engine_core.get_output_async()\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 806, in get_output_async\n    raise self._format_exception(outputs) from None\nvllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n=============================== warnings summary ===============================\n../../usr/local/lib/python3.12/dist-packages/schemathesis/generation/coverage.py:305\n  /usr/local/lib/python3.12/dist-packages/schemathesis/generation/coverage.py:305: DeprecationWarning: jsonschema.exceptions.RefResolutionError is deprecated as of version 4.18.0. If you wish to catch potential reference resolution errors, directly catch referencing.exceptions.Unresolvable.\n    ref_error: type[Exception] = jsonschema.RefResolutionError,\n\ntests/v1/test_async_llm_dp.py::test_load[RequestOutputKind.DELTA]\ntests/v1/test_async_llm_dp.py::test_load[RequestOutputKind.DELTA]\ntests/v1/test_async_llm_dp.py::test_load[RequestOutputKind.FINAL_ONLY]\ntests/v1/test_async_llm_dp.py::test_load[RequestOutputKind.FINAL_ONLY]\n  /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=3694) is multi-threaded, use of fork() may lead to deadlocks in the child.\n    self.pid = os.fork()\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ============================\nFAILED v1/test_async_llm_dp.py::test_load[RequestOutputKind.DELTA] - vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\nFAILED v1/test_async_llm_dp.py::test_load[RequestOutputKind.FINAL_ONLY] - vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n================== 2 failed, 5 warnings in 125.53s (0:02:05) ===================\nTask exception was never retrieved\nfuture: <Task finished name='Task-206' coro=<generate() done, defined at /vllm-workspace/tests/v1/test_async_llm_dp.py:31> exception=EngineDeadError('EngineCore encountered an issue. See stack trace (above) for the root cause.')>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/_pytest/runner.py\", line 341, in from_call\n    result: TResult | None = func()\n                             ^^^^^^\n...\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 361, in output_handler\n    outputs = await engine_core.get_output_async()\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 806, in get_output_async\n    raise self._format_exception(outputs) from None\nvllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n^^^ +++\n\ud83d\udea8 Error: The command exited with status 1\n^^^ +++\nuser command error: The plugin docker command hook exited with status 1\n~~~ Running global pre-exit hook\n$ /etc/buildkite-agent/hooks/pre-exit\n~~~ Running plugin docker pre-exit hook\n$ /var/lib/buildkite-agent/plugins/bk-gpu-4-queue-ci-i-0673491754b6e6e0f-1/github-com-buildkite-plugins-docker-buildkite-plugin-v5-2-0/hooks/pre-exit\n```\n\n</details>\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-05-21T07:46:41+00:00",
    "closed_at": "2025-05-22T13:48:58+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18466/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18466"
  },
  {
    "number": 18417,
    "title": "[Bug][Failing Test]  2-node-tests-4-gpus-in-total - distributed/test_pipeline_parallel.py::test_tp_*",
    "body": "### Your current environment\n\nStill failing on main as of commit 9609327fa4\n\n### \ud83d\udc1b Describe the bug\n\nFailing test: https://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests?branch=main&commit=Search&period=1day&query=test_tp_language_generation\n\n```\nFAILED distributed/test_pipeline_parallel.py::test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup26-ray-1-auto-test_options26]\n```\n\n<details>\n<summary>Logs</summary>\n\n\n```\n[2025-05-20T05:24:25Z] (VllmWorker rank=0 pid=10229) WARNING 05-19 22:24:25 [fused_moe.py:682] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=16,N=800,device_name=NVIDIA_L4.json\n[2025-05-20T05:24:27Z] (VllmWorker rank=0 pid=10229) INFO 05-19 22:24:27 [monitor.py:33] torch.compile takes 10.50 s in total\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522] WorkerProc hit an exception.\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522] Traceback (most recent call last):\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 517, in worker_busy_loop\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     output = func(*args, **kwargs)\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]              ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     return func(*args, **kwargs)\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 185, in determine_available_memory\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     self.model_runner.profile_run()\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1856, in profile_run\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     sampler_output = self._dummy_sampler_run(hidden_states)\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     return func(*args, **kwargs)\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1757, in _dummy_sampler_run\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     raise e\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1747, in _dummy_sampler_run\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     sampler_output = self.sampler(logits=logits,\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     return self._call_impl(*args, **kwargs)\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     return forward_call(*args, **kwargs)\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py\", line 49, in forward\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     sampled = self.sample(logits, sampling_metadata)\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py\", line 126, in sample\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     sampling_metadata.temperature < _SAMPLING_EPS,\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522] RuntimeError: CUDA error: an illegal memory access was encountered\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522] Traceback (most recent call last):\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 517, in worker_busy_loop\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     output = func(*args, **kwargs)\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]              ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     return func(*args, **kwargs)\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 185, in determine_available_memory\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     self.model_runner.profile_run()\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1856, in profile_run\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     sampler_output = self._dummy_sampler_run(hidden_states)\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     return func(*args, **kwargs)\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1757, in _dummy_sampler_run\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     raise e\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1747, in _dummy_sampler_run\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     sampler_output = self.sampler(logits=logits,\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     return self._call_impl(*args, **kwargs)\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     return forward_call(*args, **kwargs)\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py\", line 49, in forward\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     sampled = self.sample(logits, sampling_metadata)\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py\", line 126, in sample\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     sampling_metadata.temperature < _SAMPLING_EPS,\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522] RuntimeError: CUDA error: an illegal memory access was encountered\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]\n[2025-05-20T05:24:28Z] (VllmWorker rank=0 pid=10229) ERROR 05-19 22:24:28 [multiproc_executor.py:522]\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489] EngineCore failed to start.\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489] Traceback (most recent call last):\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 480, in run_engine_core\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489]     engine_core = EngineCoreProc(*args, **kwargs)\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 379, in __init__\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489]     super().__init__(vllm_config, executor_class, log_stats,\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 74, in __init__\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489]     self._initialize_kv_caches(vllm_config)\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 133, in _initialize_kv_caches\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489]     available_gpu_memory = self.model_executor.determine_available_memory()\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py\", line 75, in determine_available_memory\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489]     output = self.collective_rpc(\"determine_available_memory\")\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 215, in collective_rpc\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489]     result = get_response(w, dequeue_timeout)\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 202, in get_response\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489]     raise RuntimeError(\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489] RuntimeError: Worker failed with error 'CUDA error: an illegal memory access was encountered\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[2025-05-20T05:24:28Z] ERROR 05-19 22:24:28 [core.py:489] ', please check the stack trace above for the root cause\n[2025-05-20T05:24:29Z] ERROR 05-19 22:24:29 [multiproc_executor.py:135] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.\n[2025-05-20T05:24:29Z] Process EngineCore_0:\n[2025-05-20T05:24:29Z] Traceback (most recent call last):\n[2025-05-20T05:24:29Z]   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n[2025-05-20T05:24:29Z]     self.run()\n[2025-05-20T05:24:29Z]   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n[2025-05-20T05:24:29Z]     self._target(*self._args, **self._kwargs)\n[2025-05-20T05:24:29Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 493, in run_engine_core\n[2025-05-20T05:24:29Z]     raise e\n[2025-05-20T05:24:29Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 480, in run_engine_core\n[2025-05-20T05:24:29Z]     engine_core = EngineCoreProc(*args, **kwargs)\n[2025-05-20T05:24:29Z]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:29Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 379, in __init__\n[2025-05-20T05:24:29Z]     super().__init__(vllm_config, executor_class, log_stats,\n[2025-05-20T05:24:29Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 74, in __init__\n[2025-05-20T05:24:29Z]     self._initialize_kv_caches(vllm_config)\n[2025-05-20T05:24:29Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 133, in _initialize_kv_caches\n[2025-05-20T05:24:29Z]     available_gpu_memory = self.model_executor.determine_available_memory()\n[2025-05-20T05:24:29Z]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:29Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py\", line 75, in determine_available_memory\n[2025-05-20T05:24:29Z]     output = self.collective_rpc(\"determine_available_memory\")\n[2025-05-20T05:24:29Z]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:29Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 215, in collective_rpc\n[2025-05-20T05:24:29Z]     result = get_response(w, dequeue_timeout)\n[2025-05-20T05:24:29Z]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:29Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 202, in get_response\n[2025-05-20T05:24:29Z]     raise RuntimeError(\n[2025-05-20T05:24:29Z] RuntimeError: Worker failed with error 'CUDA error: an illegal memory access was encountered\n[2025-05-20T05:24:29Z] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[2025-05-20T05:24:29Z] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[2025-05-20T05:24:29Z] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[2025-05-20T05:24:29Z] ', please check the stack trace above for the root cause\n[2025-05-20T05:24:31Z] Traceback (most recent call last):\n[2025-05-20T05:24:31Z]   File \"/usr/local/bin/vllm\", line 10, in <module>\n[2025-05-20T05:24:31Z]     sys.exit(main())\n[2025-05-20T05:24:31Z]              ^^^^^^\n[2025-05-20T05:24:31Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/main.py\", line 53, in main\n[2025-05-20T05:24:31Z]     args.dispatch_function(args)\n[2025-05-20T05:24:31Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/serve.py\", line 40, in cmd\n[2025-05-20T05:24:31Z]     uvloop.run(run_server(args))\n[2025-05-20T05:24:31Z]   File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\n[2025-05-20T05:24:31Z]     return __asyncio.run(\n[2025-05-20T05:24:31Z]            ^^^^^^^^^^^^^^\n[2025-05-20T05:24:31Z]   File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n[2025-05-20T05:24:31Z]     return runner.run(main)\n[2025-05-20T05:24:31Z]            ^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:31Z]   File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n[2025-05-20T05:24:31Z]     return self._loop.run_until_complete(task)\n[2025-05-20T05:24:31Z]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:31Z]   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n[2025-05-20T05:24:31Z]   File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n[2025-05-20T05:24:31Z]     return await main\n[2025-05-20T05:24:31Z]            ^^^^^^^^^^\n[2025-05-20T05:24:31Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1324, in run_server\n[2025-05-20T05:24:31Z]     async with build_async_engine_client(args) as engine_client:\n[2025-05-20T05:24:31Z]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:31Z]   File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n[2025-05-20T05:24:31Z]     return await anext(self.gen)\n[2025-05-20T05:24:31Z]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:31Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 153, in build_async_engine_client\n[2025-05-20T05:24:31Z]     async with build_async_engine_client_from_engine_args(\n[2025-05-20T05:24:31Z]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:31Z]   File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n[2025-05-20T05:24:31Z]     return await anext(self.gen)\n[2025-05-20T05:24:31Z]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:31Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 185, in build_async_engine_client_from_engine_args\n[2025-05-20T05:24:31Z]     async_llm = AsyncLLM.from_vllm_config(\n[2025-05-20T05:24:31Z]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:31Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 152, in from_vllm_config\n[2025-05-20T05:24:31Z]     return cls(\n[2025-05-20T05:24:31Z]            ^^^^\n[2025-05-20T05:24:31Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py\", line 118, in __init__\n[2025-05-20T05:24:31Z]     self.engine_core = core_client_class(\n[2025-05-20T05:24:31Z]                        ^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:31Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 734, in __init__\n[2025-05-20T05:24:31Z]     super().__init__(\n[2025-05-20T05:24:31Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 418, in __init__\n[2025-05-20T05:24:31Z]     self._wait_for_engine_startup(output_address, parallel_config)\n[2025-05-20T05:24:31Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py\", line 484, in _wait_for_engine_startup\n[2025-05-20T05:24:31Z]     raise RuntimeError(\"Engine core initialization failed. \"\n[2025-05-20T05:24:31Z] RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n[2025-05-20T05:24:32Z] Traceback (most recent call last):\n[2025-05-20T05:24:32Z]   File \"/vllm-workspace/tests/utils.py\", line 727, in wrapper\n[2025-05-20T05:24:32Z]     f(*args, **kwargs)\n[2025-05-20T05:24:32Z]   File \"/vllm-workspace/tests/distributed/test_pipeline_parallel.py\", line 422, in test_tp_language_generation\n[2025-05-20T05:24:32Z]     _compare_tp(model_id,\n[2025-05-20T05:24:32Z]   File \"/vllm-workspace/tests/distributed/test_pipeline_parallel.py\", line 388, in _compare_tp\n[2025-05-20T05:24:32Z]     compare_two_settings(model_id,\n[2025-05-20T05:24:32Z]   File \"/vllm-workspace/tests/utils.py\", line 465, in compare_two_settings\n[2025-05-20T05:24:32Z]     compare_all_settings(\n[2025-05-20T05:24:32Z]   File \"/vllm-workspace/tests/utils.py\", line 529, in compare_all_settings\n[2025-05-20T05:24:32Z]     with RemoteOpenAIServer(model,\n[2025-05-20T05:24:32Z]          ^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T05:24:32Z]   File \"/vllm-workspace/tests/utils.py\", line 133, in __init__\n[2025-05-20T05:24:32Z]     self._wait_for_server(url=self.url_for(\"health\"),\n[2025-05-20T05:24:32Z]   File \"/vllm-workspace/tests/utils.py\", line 161, in _wait_for_server\n[2025-05-20T05:24:32Z]     raise RuntimeError(\"Server exited unexpectedly.\") from None\n[2025-05-20T05:24:32Z] RuntimeError: Server exited unexpectedly.\n[2025-05-20T05:24:32Z] Fork a new process to run a test 8315\n[2025-05-20T05:24:32Z] FAILED\n[2025-05-20T05:24:32Z] distributed/test_pipeline_parallel.py::test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup27-mp-0-auto-test_options27] Fork a new process to run a test 10565\n[2025-05-20T05:24:32Z] PASSED\n[2025-05-20T05:24:33Z] distributed/test_pipeline_parallel.py::test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup28-mp-1-auto-test_options28] Fork a new process to run a test 10566\n[2025-05-20T05:24:33Z] PASSED\n[2025-05-20T05:24:33Z] distributed/test_pipeline_parallel.py::test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup29-ray-0-auto-test_options29] Fork a new process to run a test 10567\n[2025-05-20T05:24:33Z] PASSED\n[2025-05-20T05:24:33Z] distributed/test_pipeline_parallel.py::test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup30-ray-1-auto-test_options30] Fork a new process to run a test 10568\n[2025-05-20T05:24:33Z] PASSED\n[2025-05-20T05:24:33Z] distributed/test_pipeline_parallel.py::test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup31-mp-0-auto-test_options31] Fork a new process to run a test 10569\n[2025-05-20T05:24:34Z] PASSED\n[2025-05-20T05:24:34Z] distributed/test_pipeline_parallel.py::test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup32-mp-1-auto-test_options32] Fork a new process to run a test 10570\n[2025-05-20T05:24:34Z] PASSED\n[2025-05-20T05:24:34Z] distributed/test_pipeline_parallel.py::test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup33-ray-0-auto-test_options33] Fork a new process to run a test 10571\n[2025-05-20T05:24:34Z] PASSED\n[2025-05-20T05:24:34Z] distributed/test_pipeline_parallel.py::test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup34-ray-1-auto-test_options34] Fork a new process to run a test 10572\n[2025-05-20T05:24:34Z] PASSED\n[2025-05-20T05:24:34Z] distributed/test_pipeline_parallel.py::test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup35-mp-0-auto-test_options35] Fork a new process to run a test 10573\n[2025-05-20T05:24:35Z] PASSED\n[2025-05-20T05:24:35Z] distributed/test_pipeline_parallel.py::test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup36-mp-1-auto-test_options36] Fork a new process to run a test 10574\n[2025-05-20T05:24:35Z] PASSED\n[2025-05-20T05:24:35Z] distributed/test_pipeline_parallel.py::test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup37-ray-0-auto-test_options37] Fork a new process to run a test 10575\n[2025-05-20T05:24:35Z] PASSED\n[2025-05-20T05:24:35Z] distributed/test_pipeline_parallel.py::test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup38-ray-1-auto-test_options38] Fork a new process to run a test 10576\n[2025-05-20T05:24:36Z] PASSED\n[2025-05-20T05:24:36Z] distributed/test_pipeline_parallel.py::test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup39-mp-0-auto-test_options39] Fork a new process to run a test 10577\n[2025-05-20T05:24:36Z] PASSED\n[2025-05-20T05:24:36Z] distributed/test_pipeline_parallel.py::test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup40-mp-1-auto-test_options40] Fork a new process to run a test 10578\n[2025-05-20T05:24:36Z] PASSED\n[2025-05-20T05:24:36Z] distributed/test_pipeline_parallel.py::test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup41-ray-0-auto-test_options41] Fork a new process to run a test 10579\n[2025-05-20T05:24:36Z] PASSED\n[2025-05-20T05:24:36Z] distributed/test_pipeline_parallel.py::test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup42-ray-1-auto-test_options42] Fork a new process to run a test 10580\n[2025-05-20T05:24:37Z] PASSED\n[2025-05-20T05:24:37Z] distributed/test_pipeline_parallel.py::test_tp_language_embedding[intfloat/e5-mistral-7b-instruct-parallel_setup0-mp-0-auto-test_options0] Fork a new process to run a test 10581\n[2025-05-20T05:24:37Z] PASSED\n[2025-05-20T05:24:37Z] distributed/test_pipeline_parallel.py::test_tp_language_embedding[BAAI/bge-multilingual-gemma2-parallel_setup1-mp-0-auto-test_options1] Fork a new process to run a test 10582\n[2025-05-20T05:24:37Z] PASSED\n[2025-05-20T05:24:37Z] distributed/test_pipeline_parallel.py::test_tp_multimodal_generation[OpenGVLab/InternVL2-1B-parallel_setup0-mp-0-auto-test_options0] Fork a new process to run a test 10583\n[2025-05-20T05:24:37Z] PASSED\n[2025-05-20T05:24:38Z] distributed/test_pipeline_parallel.py::test_tp_multimodal_generation[microsoft/Phi-3.5-vision-instruct-parallel_setup1-mp-0-auto-test_options1] Fork a new process to run a test 10584\n[2025-05-20T05:24:38Z] PASSED\n[2025-05-20T05:24:38Z] distributed/test_pipeline_parallel.py::test_tp_multimodal_generation[fixie-ai/ultravox-v0_5-llama-3_2-1b-parallel_setup2-mp-0-auto-test_options2] Fork a new process to run a test 10585\n[2025-05-20T05:24:38Z] PASSED\n[2025-05-20T05:24:38Z]\n[2025-05-20T05:24:38Z] =================================== FAILURES ===================================\n[2025-05-20T05:24:38Z] _ test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup26-ray-1-auto-test_options26] _\n[2025-05-20T05:24:38Z]\n[2025-05-20T05:24:38Z] args = ()\n[2025-05-20T05:24:38Z] kwargs = {'distributed_backend': 'ray', 'model_id': 'microsoft/Phi-3.5-MoE-instruct', 'num_gpus_available': 2, 'parallel_setup': ParallelSetup(tp_size=1, pp_size=2, eager_mode=False, chunked_prefill=False), ...}\n[2025-05-20T05:24:38Z] Skipped = <class 'Skipped'>, pid = 8315, pgid = 3893, _pid = 8315\n[2025-05-20T05:24:38Z] _exitcode = 256, old_signal_handler = <Handlers.SIG_DFL: 0>\n[2025-05-20T05:24:38Z]\n[2025-05-20T05:24:38Z]     @functools.wraps(f)\n[2025-05-20T05:24:38Z]     def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> None:\n[2025-05-20T05:24:38Z]         # Make the process the leader of its own process group\n[2025-05-20T05:24:38Z]         # to avoid sending SIGTERM to the parent process\n[2025-05-20T05:24:38Z]         os.setpgrp()\n[2025-05-20T05:24:38Z]         from _pytest.outcomes import Skipped\n[2025-05-20T05:24:38Z]         pid = os.fork()\n[2025-05-20T05:24:38Z]         print(f\"Fork a new process to run a test {pid}\")\n[2025-05-20T05:24:38Z]         if pid == 0:\n[2025-05-20T05:24:38Z]             try:\n[2025-05-20T05:24:38Z]                 f(*args, **kwargs)\n[2025-05-20T05:24:38Z]             except Skipped as e:\n[2025-05-20T05:24:38Z]                 # convert Skipped to exit code 0\n[2025-05-20T05:24:38Z]                 print(str(e))\n[2025-05-20T05:24:38Z]                 os._exit(0)\n[2025-05-20T05:24:38Z]             except Exception:\n[2025-05-20T05:24:38Z]                 import traceback\n[2025-05-20T05:24:38Z]                 traceback.print_exc()\n[2025-05-20T05:24:38Z]                 os._exit(1)\n[2025-05-20T05:24:38Z]             else:\n[2025-05-20T05:24:38Z]                 os._exit(0)\n[2025-05-20T05:24:38Z]         else:\n[2025-05-20T05:24:38Z]             pgid = os.getpgid(pid)\n[2025-05-20T05:24:38Z]             _pid, _exitcode = os.waitpid(pid, 0)\n[2025-05-20T05:24:38Z]             # ignore SIGTERM signal itself\n[2025-05-20T05:24:38Z]             old_signal_handler = signal.signal(signal.SIGTERM, signal.SIG_IGN)\n[2025-05-20T05:24:38Z]             # kill all child processes\n[2025-05-20T05:24:38Z]             os.killpg(pgid, signal.SIGTERM)\n[2025-05-20T05:24:38Z]             # restore the signal handler\n[2025-05-20T05:24:38Z]             signal.signal(signal.SIGTERM, old_signal_handler)\n[2025-05-20T05:24:38Z] >           assert _exitcode == 0, (f\"function {f} failed when called with\"\n[2025-05-20T05:24:38Z]                                     f\" args {args} and kwargs {kwargs}\")\n[2025-05-20T05:24:38Z] E           AssertionError: function <function test_tp_language_generation at 0x7f24d8990360> failed when called with args () and kwargs {'model_id': 'microsoft/Phi-3.5-MoE-instruct', 'parallel_setup': ParallelSetup(tp_size=1, pp_size=2, eager_mode=False, chunked_prefill=False), 'distributed_backend': 'ray', 'vllm_major_version': '1', 'task': 'auto', 'test_options': PPTestOptions(multi_node_only=True, load_format='dummy'), 'num_gpus_available': 2}\n[2025-05-20T05:24:38Z]\n[2025-05-20T05:24:38Z] utils.py:747: AssertionError\n[2025-05-20T05:24:38Z] =============================== warnings summary ===============================\n[2025-05-20T05:24:38Z] ../../usr/local/lib/python3.12/dist-packages/schemathesis/generation/coverage.py:305\n[2025-05-20T05:24:38Z]   /usr/local/lib/python3.12/dist-packages/schemathesis/generation/coverage.py:305: DeprecationWarning: jsonschema.exceptions.RefResolutionError is deprecated as of version 4.18.0. If you wish to catch potential reference resolution errors, directly catch referencing.exceptions.Unresolvable.\n[2025-05-20T05:24:38Z]     ref_error: type[Exception] = jsonschema.RefResolutionError,\n[2025-05-20T05:24:38Z]\n[2025-05-20T05:24:38Z] tests/distributed/test_pipeline_parallel.py: 48 warnings\n[2025-05-20T05:24:38Z]   /vllm-workspace/tests/utils.py:723: DeprecationWarning: This process (pid=3893) is multi-threaded, use of fork() may lead to deadlocks in the child.\n[2025-05-20T05:24:38Z]     pid = os.fork()\n[2025-05-20T05:24:38Z]\n[2025-05-20T05:24:38Z] -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n[2025-05-20T05:24:38Z] =========================== short test summary info ============================\n[2025-05-20T05:24:38Z] FAILED distributed/test_pipeline_parallel.py::test_tp_language_generation[microsoft/Phi-3.5-MoE-instruct-parallel_setup26-ray-1-auto-test_options26]\n```\n\n</details>",
    "labels": [
      "bug",
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-05-20T15:19:47+00:00",
    "closed_at": "2025-05-22T13:48:58+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18417/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18417"
  },
  {
    "number": 18766,
    "title": "[CI Failure]: LM Eval Large Models - test_lm_eval_correctness.py",
    "body": "### Name of failing test\n\n`test_lm_eval_correctness.py::test_lm_eval_correctness_param[config_filename4]`\n\n### Basic information\n\n- [ ] Flaky test\n- [ ] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\n```\ngsm8k | exact_match,strict-match: ground_truth=0.671 | measured=0.355\ngsm8k | exact_match,flexible-extract: ground_truth=0.664 | measured=0.356\n```\n\nFull log: https://buildkite.com/vllm/ci/builds/20837/summary/annotations?sid=01970fe6-97da-4bd8-b139-30d20cf3912f\n\n### \ud83d\udcdd History of failing test\n\nNot sure\n\n### CC List.\n\ncc @robertgshaw2-redhat @mgoin ",
    "labels": [
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-05-27T15:53:35+00:00",
    "closed_at": "2025-05-28T08:59:41+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18766/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18766"
  },
  {
    "number": 18416,
    "title": "[Bug][Failing Test]: weight-loading-multiple-gpu-test -",
    "body": "### Your current environment\n\nStill failing on main as of commit bca55b556f\n\n### \ud83d\udc1b Describe the bug\n\nFailing test: https://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests/6873a23f-c2ec-8c01-9e20-bac3329482c0?tags=scm.branch%3Amain%2Cresult%3Afailed\n\n```\nFAILED weight_loading/test_weight_loading.py::test_weight_loading - RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_0': 1}\n```\n\n<details>\n<summary>Logs</summary>\n\n```\n[2025-05-20T10:46:52Z] (VllmWorker rank=0 pid=12189) INFO 05-20 03:46:52 [backends.py:172] Compiling a graph for general shape takes 20.71 s\n[2025-05-20T10:46:52Z] (VllmWorker rank=0 pid=12189) DEBUG 05-20 03:46:52 [backends.py:512] Computation graph saved to /root/.cache/vllm/torch_compile_cache/07e0a984e7/rank_0_0/computation_graph.py\n[2025-05-20T10:46:55Z] (VllmWorker rank=1 pid=12191) DEBUG 05-20 03:46:55 [wrapper.py:105] Dynamo transformed code saved to /root/.cache/vllm/torch_compile_cache/07e0a984e7/rank_1_0/transformed_code.py\n[2025-05-20T10:46:55Z] (VllmWorker rank=0 pid=12189) DEBUG 05-20 03:46:55 [wrapper.py:105] Dynamo transformed code saved to /root/.cache/vllm/torch_compile_cache/07e0a984e7/rank_0_0/transformed_code.py\n[2025-05-20T10:46:57Z] DEBUG 05-20 03:46:57 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:47:04Z] (VllmWorker rank=1 pid=12191) INFO 05-20 03:47:04 [monitor.py:33] torch.compile takes 26.93 s in total\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) INFO 05-20 03:47:04 [monitor.py:33] torch.compile takes 27.04 s in total\n[2025-05-20T10:47:04Z] [rank0]:[E520 03:47:04.619668070 ProcessGroupNCCL.cpp:1896] [PG ID 2 PG GUID 3 Rank 0] Process group watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\n[2025-05-20T10:47:04Z] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[2025-05-20T10:47:04Z] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[2025-05-20T10:47:04Z] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[2025-05-20T10:47:04Z]\n[2025-05-20T10:47:04Z] Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):\n[2025-05-20T10:47:04Z] frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f81329785e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\n[2025-05-20T10:47:04Z] frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x7f813290d4a2 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\n[2025-05-20T10:47:04Z] frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x7f8132d26422 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10_cuda.so)\n[2025-05-20T10:47:04Z] frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7f80c268b456 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\n[2025-05-20T10:47:04Z] frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x7f80c269b6f0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\n[2025-05-20T10:47:04Z] frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x7f80c269d282 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\n[2025-05-20T10:47:04Z] frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f80c269ee8d in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\n[2025-05-20T10:47:04Z] frame #7: <unknown function> + 0xdc253 (0x7f80b29b3253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n[2025-05-20T10:47:04Z] frame #8: <unknown function> + 0x94ac3 (0x7f81b3fb9ac3 in /lib/x86_64-linux-gnu/libc.so.6)\n[2025-05-20T10:47:04Z] frame #9: clone + 0x44 (0x7f81b404aa04 in /lib/x86_64-linux-gnu/libc.so.6)\n[2025-05-20T10:47:04Z]\n[2025-05-20T10:47:04Z] terminate called after throwing an instance of 'c10::DistBackendError'\n[2025-05-20T10:47:04Z]   what():  [PG ID 2 PG GUID 3 Rank 0] Process group watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\n[2025-05-20T10:47:04Z] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[2025-05-20T10:47:04Z] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[2025-05-20T10:47:04Z] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[2025-05-20T10:47:04Z]\n[2025-05-20T10:47:04Z] Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):\n[2025-05-20T10:47:04Z] frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f81329785e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\n[2025-05-20T10:47:04Z] frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x7f813290d4a2 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\n[2025-05-20T10:47:04Z] frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x7f8132d26422 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10_cuda.so)\n[2025-05-20T10:47:04Z] frame #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x56 (0x7f80c268b456 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\n[2025-05-20T10:47:04Z] frame #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x70 (0x7f80c269b6f0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\n[2025-05-20T10:47:04Z] frame #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x782 (0x7f80c269d282 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\n[2025-05-20T10:47:04Z] frame #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f80c269ee8d in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\n[2025-05-20T10:47:04Z] frame #7: <unknown function> + 0xdc253 (0x7f80b29b3253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n[2025-05-20T10:47:04Z] frame #8: <unknown function> + 0x94ac3 (0x7f81b3fb9ac3 in /lib/x86_64-linux-gnu/libc.so.6)\n[2025-05-20T10:47:04Z] frame #9: clone + 0x44 (0x7f81b404aa04 in /lib/x86_64-linux-gnu/libc.so.6)\n[2025-05-20T10:47:04Z]\n[2025-05-20T10:47:04Z] Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1902 (most recent call first):\n[2025-05-20T10:47:04Z] frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f81329785e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\n[2025-05-20T10:47:04Z] frame #1: <unknown function> + 0xcc7a4e (0x7f80c266da4e in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\n[2025-05-20T10:47:04Z] frame #2: <unknown function> + 0x9165ed (0x7f80c22bc5ed in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\n[2025-05-20T10:47:04Z] frame #3: <unknown function> + 0xdc253 (0x7f80b29b3253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n[2025-05-20T10:47:04Z] frame #4: <unknown function> + 0x94ac3 (0x7f81b3fb9ac3 in /lib/x86_64-linux-gnu/libc.so.6)\n[2025-05-20T10:47:04Z] frame #5: clone + 0x44 (0x7f81b404aa04 in /lib/x86_64-linux-gnu/libc.so.6)\n[2025-05-20T10:47:04Z]\n[2025-05-20T10:47:04Z] Fatal Python error: Aborted\n[2025-05-20T10:47:04Z]\n[2025-05-20T10:47:04Z] Thread 0x00007f814f5da640 (most recent call first):\n[2025-05-20T10:47:04Z]   File \"/usr/lib/python3.12/threading.py\", line 359 in wait\n[2025-05-20T10:47:04Z]   File \"/usr/lib/python3.12/threading.py\", line 655 in wait\n[2025-05-20T10:47:04Z]   File \"/usr/local/lib/python3.12/dist-packages/tqdm/_monitor.py\", line 60 in run\n[2025-05-20T10:47:04Z]   File \"/usr/lib/python3.12/threading.py\", line 1075 in _bootstrap_inner\n[2025-05-20T10:47:04Z]   File \"/usr/lib/python3.12/threading.py\", line 1032 in _bootstrap\n[2025-05-20T10:47:04Z]\n[2025-05-20T10:47:04Z] Thread 0x00007f81549dc640 (most recent call first):\n[2025-05-20T10:47:04Z]   File \"/usr/lib/python3.12/threading.py\", line 359 in wait\n[2025-05-20T10:47:04Z]   File \"/usr/lib/python3.12/threading.py\", line 655 in wait\n[2025-05-20T10:47:04Z]   File \"/usr/local/lib/python3.12/dist-packages/tqdm/_monitor.py\", line 60 in run\n[2025-05-20T10:47:04Z]   File \"/usr/lib/python3.12/threading.py\", line 1075 in _bootstrap_inner\n[2025-05-20T10:47:04Z]   File \"/usr/lib/python3.12/threading.py\", line 1032 in _bootstrap\n[2025-05-20T10:47:04Z]\n[2025-05-20T10:47:04Z] Thread 0x00007f8166fe3640 (most recent call first):\n[2025-05-20T10:47:04Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/usage/usage_lib.py\", line 229 in _report_continuous_usage\n[2025-05-20T10:47:04Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/usage/usage_lib.py\", line 164 in _report_usage_worker\n[2025-05-20T10:47:04Z]   File \"/usr/lib/python3.12/threading.py\", line 1012 in run\n[2025-05-20T10:47:04Z]   File \"/usr/lib/python3.12/threading.py\", line 1075 in _bootstrap_inner\n[2025-05-20T10:47:04Z]   File \"/usr/lib/python3.12/threading.py\", line 1032 in _bootstrap\n[2025-05-20T10:47:04Z]\n[2025-05-20T10:47:04Z] Thread 0x00007f81b3f24000 (most recent call first):\n[2025-05-20T10:47:04Z]   File \"/usr/lib/python3.12/logging/__init__.py\", line 720 in format\n[2025-05-20T10:47:04Z]   File (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522] WorkerProc hit an exception.\n[2025-05-20T10:47:04Z] Fatal Python error: Segmentation fault\n[2025-05-20T10:47:04Z]\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522] Traceback (most recent call last):\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 517, in worker_busy_loop\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     output = func(*args, **kwargs)\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]              ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     return func(*args, **kwargs)\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 185, in determine_available_memory\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     self.model_runner.profile_run()\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1856, in profile_run\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     sampler_output = self._dummy_sampler_run(hidden_states)\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     return func(*args, **kwargs)\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1757, in _dummy_sampler_run\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     raise e\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1747, in _dummy_sampler_run\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     sampler_output = self.sampler(logits=logits,\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     return self._call_impl(*args, **kwargs)\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     return forward_call(*args, **kwargs)\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py\", line 49, in forward\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     sampled = self.sample(logits, sampling_metadata)\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py\", line 125, in sample\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     sampled = torch.where(\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]               ^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522] RuntimeError: CUDA error: an illegal memory access was encountered\n[2025-05-20T10:47:04Z]\n[2025-05-20T10:47:04Z] Extension modules: (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[2025-05-20T10:47:04Z] zstandard.backend_c(VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522] Traceback (most recent call last):\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 517, in worker_busy_loop\n[2025-05-20T10:47:04Z] , (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     output = func(*args, **kwargs)\n[2025-05-20T10:47:04Z] charset_normalizer.md(VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]              ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     return func(*args, **kwargs)\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 185, in determine_available_memory\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     self.model_runner.profile_run()\n[2025-05-20T10:47:04Z] , regex._regex(VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1856, in profile_run\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     sampler_output = self._dummy_sampler_run(hidden_states)\n[2025-05-20T10:47:04Z] , (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] numpy.core._multiarray_umath(VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     return func(*args, **kwargs)\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] , numpy.core._multiarray_tests(VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1757, in _dummy_sampler_run\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     raise e\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1747, in _dummy_sampler_run\n[2025-05-20T10:47:04Z] , numpy.linalg._umath_linalg(VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     sampler_output = self.sampler(logits=logits,\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[2025-05-20T10:47:04Z] , numpy.fft._pocketfft_internal(VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     return self._call_impl(*args, **kwargs)\n[2025-05-20T10:47:04Z] , numpy.random._common(VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[2025-05-20T10:47:04Z] , numpy.random.bit_generator(VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     return forward_call(*args, **kwargs)\n[2025-05-20T10:47:04Z] , (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] numpy.random._bounded_integers(VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py\", line 49, in forward\n[2025-05-20T10:47:04Z] , numpy.random._mt19937(VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     sampled = self.sample(logits, sampling_metadata)\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] , (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/sample/sampler.py\", line 125, in sample\n[2025-05-20T10:47:04Z] numpy.random.mtrand(VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]     sampled = torch.where(\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]               ^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] , (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522] RuntimeError: CUDA error: an illegal memory access was encountered\n[2025-05-20T10:47:04Z] numpy.random._philox(VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[2025-05-20T10:47:04Z] , numpy.random._pcg64(VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[2025-05-20T10:47:04Z] , numpy.random._sfc64(VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]\n[2025-05-20T10:47:04Z] (VllmWorker rank=0 pid=12189) ERROR 05-20 03:47:04 [multiproc_executor.py:522]\n[2025-05-20T10:47:04Z] , numpy.random._generator, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, yaml._yaml, PIL._imaging, markupsafe._speedups, sklearn.__check_build._check_build, psutil._psutil_linux, psutil._psutil_posix, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.special._ufuncs_cxx, scipy.special._cdflib, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.ndimage._nd_image, _ni_label, scipy.ndimage._ni_label, scipy.optimize._minpack2, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats.beta_ufunc, scipy.stats._boost.beta_ufunc, scipy.stats.binom_ufunc, scipy.stats._boost.binom_ufunc, scipy.stats.nbinom_ufunc, scipy.stats._boost.nbinom_ufunc, scipy.stats.hypergeom_ufunc, scipy.stats._boost.hypergeom_ufunc, scipy.stats.ncf_ufunc, scipy.stats._boost.ncf_ufunc, scipy.stats.ncx2_ufunc, scipy.stats._boost.ncx2_ufunc, scipy.stats.nct_ufunc, scipy.stats._boost.nct_ufunc, scipy.stats.skewnorm_ufunc, scipy.stats._boost.skewnorm_ufunc, scipy.stats.invgauss_ufunc, scipy.stats._boost.invgauss_ufunc, scipy.interpolate._fitpack, scipy.interpolate.dfitpack, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.stats._biasedurn, scipy.stats._levy_stable.levyst, scipy.stats._stats_pythran, scipy._lib._uarray._uarray, scipy.stats._ansari_swilk_statistics, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.stats._unuran.unuran_wrapper, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, numexpr.interpreter, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, zmq.backend.cython._zmq, PIL._imagingft, hiredis.hiredis, msgspec._core, _cffi_backend, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._helpers, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket, frozenlist._frozenlist, msgpack._cmsgpack, google._upb._message, setproctitle, uvloop.loop, ray._raylet, sentencepiece._sentencepiece, vllm.cumem_allocator, numba.core.typeconv._typeconv, numba._helperlib, numba._dynfunc, numba._dispatcher, numba.core.typing.builtins.itertools, numba.cpython.builtins.math, numba.core.runtime._nrt_python, numba.np.ufunc._internal, numba.experimental.jitclass._box, cuda_utils, __triton_launcher (total: 231)\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489] EngineCore failed to start.\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489] Traceback (most recent call last):\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 480, in run_engine_core\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489]     engine_core = EngineCoreProc(*args, **kwargs)\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 379, in __init__\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489]     super().__init__(vllm_config, executor_class, log_stats,\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 74, in __init__\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489]     self._initialize_kv_caches(vllm_config)\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 133, in _initialize_kv_caches\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489]     available_gpu_memory = self.model_executor.determine_available_memory()\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py\", line 75, in determine_available_memory\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489]     output = self.collective_rpc(\"determine_available_memory\")\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 215, in collective_rpc\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489]     result = get_response(w, dequeue_timeout)\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 202, in get_response\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489]     raise RuntimeError(\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489] RuntimeError: Worker failed with error 'CUDA error: an illegal memory access was encountered\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[2025-05-20T10:47:04Z] ERROR 05-20 03:47:04 [core.py:489] ', please check the stack trace above for the root cause\n[2025-05-20T10:47:07Z] DEBUG 05-20 03:47:07 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:47:17Z] DEBUG 05-20 03:47:17 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:47:27Z] DEBUG 05-20 03:47:27 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:47:37Z] DEBUG 05-20 03:47:37 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:47:47Z] DEBUG 05-20 03:47:47 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:47:57Z] DEBUG 05-20 03:47:57 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:48:04Z] (VllmWorker rank=1 pid=12191) DEBUG 05-20 03:48:04 [shm_broadcast.py:430] No available shared memory broadcast block found in 60 second.\n[2025-05-20T10:48:08Z] DEBUG 05-20 03:48:08 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:48:18Z] DEBUG 05-20 03:48:18 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:48:28Z] DEBUG 05-20 03:48:28 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:48:38Z] DEBUG 05-20 03:48:38 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:48:48Z] DEBUG 05-20 03:48:48 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:48:58Z] DEBUG 05-20 03:48:58 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:49:04Z] (VllmWorker rank=1 pid=12191) DEBUG 05-20 03:49:04 [shm_broadcast.py:430] No available shared memory broadcast block found in 60 second.\n[2025-05-20T10:49:08Z] DEBUG 05-20 03:49:08 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:49:18Z] DEBUG 05-20 03:49:18 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:49:28Z] DEBUG 05-20 03:49:28 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:49:38Z] DEBUG 05-20 03:49:38 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:49:48Z] DEBUG 05-20 03:49:48 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:49:58Z] DEBUG 05-20 03:49:58 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:50:04Z] (VllmWorker rank=1 pid=12191) DEBUG 05-20 03:50:04 [shm_broadcast.py:430] No available shared memory broadcast block found in 60 second.\n[2025-05-20T10:50:08Z] DEBUG 05-20 03:50:08 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:50:18Z] DEBUG 05-20 03:50:18 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:50:28Z] DEBUG 05-20 03:50:28 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:50:38Z] DEBUG 05-20 03:50:38 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:50:48Z] DEBUG 05-20 03:50:48 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:50:58Z] DEBUG 05-20 03:50:58 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:51:04Z] (VllmWorker rank=1 pid=12191) DEBUG 05-20 03:51:04 [shm_broadcast.py:430] No available shared memory broadcast block found in 60 second.\n[2025-05-20T10:51:08Z] DEBUG 05-20 03:51:08 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:51:18Z] DEBUG 05-20 03:51:18 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:51:28Z] DEBUG 05-20 03:51:28 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:51:38Z] DEBUG 05-20 03:51:38 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:51:48Z] DEBUG 05-20 03:51:48 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:51:58Z] DEBUG 05-20 03:51:58 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:52:04Z] (VllmWorker rank=1 pid=12191) DEBUG 05-20 03:52:04 [shm_broadcast.py:430] No available shared memory broadcast block found in 60 second.\n[2025-05-20T10:52:05Z] [rank1]:[W520 03:52:05.308296965 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=135, addr=[localhost]:59310, remote=[localhost]:44713): Connection reset by peer\n[2025-05-20T10:52:05Z] Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:675 (most recent call first):\n[2025-05-20T10:52:05Z] frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f81329785e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\n[2025-05-20T10:52:05Z] frame #1: <unknown function> + 0x5ba8afe (0x7f8116a3cafe in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:05Z] frame #2: <unknown function> + 0x5baaecf (0x7f8116a3eecf in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:05Z] frame #3: <unknown function> + 0x5bab74a (0x7f8116a3f74a in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:05Z] frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7f8116a391a9 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:05Z] frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7f80c2699989 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\n[2025-05-20T10:52:05Z] frame #6: <unknown function> + 0xdc253 (0x7f80b29b3253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n[2025-05-20T10:52:05Z] frame #7: <unknown function> + 0x94ac3 (0x7f81b3fb9ac3 in /lib/x86_64-linux-gnu/libc.so.6)\n[2025-05-20T10:52:05Z] frame #8: clone + 0x44 (0x7f81b404aa04 in /lib/x86_64-linux-gnu/libc.so.6)\n[2025-05-20T10:52:05Z]\n[2025-05-20T10:52:05Z] [rank1]:[W520 03:52:05.311880662 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0 Rank 1] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Connection reset by peer\n[2025-05-20T10:52:05Z] ERROR 05-20 03:52:05 [multiproc_executor.py:135] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.\n[2025-05-20T10:52:06Z] [rank1]:[W520 03:52:06.312051397 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=135, addr=[localhost]:59310, remote=[localhost]:44713): Broken pipe\n[2025-05-20T10:52:06Z] Exception raised from sendBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:653 (most recent call first):\n[2025-05-20T10:52:06Z] frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f81329785e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\n[2025-05-20T10:52:06Z] frame #1: <unknown function> + 0x5ba8afe (0x7f8116a3cafe in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:06Z] frame #2: <unknown function> + 0x5baa358 (0x7f8116a3e358 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:06Z] frame #3: <unknown function> + 0x5babb3e (0x7f8116a3fb3e in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:06Z] frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x298 (0x7f8116a39198 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:06Z] frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7f80c2699989 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\n[2025-05-20T10:52:06Z] frame #6: <unknown function> + 0xdc253 (0x7f80b29b3253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n[2025-05-20T10:52:06Z] frame #7: <unknown function> + 0x94ac3 (0x7f81b3fb9ac3 in /lib/x86_64-linux-gnu/libc.so.6)\n[2025-05-20T10:52:06Z] frame #8: clone + 0x44 (0x7f81b404aa04 in /lib/x86_64-linux-gnu/libc.so.6)\n[2025-05-20T10:52:06Z]\n[2025-05-20T10:52:06Z] [rank1]:[W520 03:52:06.315138190 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0 Rank 1] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Broken pipe\n[2025-05-20T10:52:07Z] [rank1]:[W520 03:52:07.315243724 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=135, addr=[localhost]:59310, remote=[localhost]:44713): Broken pipe\n[2025-05-20T10:52:07Z] Exception raised from sendBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:653 (most recent call first):\n[2025-05-20T10:52:07Z] frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f81329785e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\n[2025-05-20T10:52:07Z] frame #1: <unknown function> + 0x5ba8afe (0x7f8116a3cafe in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:07Z] frame #2: <unknown function> + 0x5baa358 (0x7f8116a3e358 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:07Z] frame #3: <unknown function> + 0x5babb3e (0x7f8116a3fb3e in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:07Z] frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x298 (0x7f8116a39198 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:07Z] frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7f80c2699989 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\n[2025-05-20T10:52:07Z] frame #6: <unknown function> + 0xdc253 (0x7f80b29b3253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n[2025-05-20T10:52:07Z] frame #7: <unknown function> + 0x94ac3 (0x7f81b3fb9ac3 in /lib/x86_64-linux-gnu/libc.so.6)\n[2025-05-20T10:52:07Z] frame #8: clone + 0x44 (0x7f81b404aa04 in /lib/x86_64-linux-gnu/libc.so.6)\n[2025-05-20T10:52:07Z]\n[2025-05-20T10:52:07Z] [rank1]:[W520 03:52:07.317919386 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0 Rank 1] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Broken pipe\n[2025-05-20T10:52:08Z] DEBUG 05-20 03:52:08 [core_client.py:476] Waiting for 1 local, 0 remote core engine proc(s) to start.\n[2025-05-20T10:52:08Z] [rank1]:[W520 03:52:08.318060510 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=135, addr=[localhost]:59310, remote=[localhost]:44713): Broken pipe\n[2025-05-20T10:52:08Z] Exception raised from sendBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:653 (most recent call first):\n[2025-05-20T10:52:08Z] frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f81329785e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\n[2025-05-20T10:52:08Z] frame #1: <unknown function> + 0x5ba8afe (0x7f8116a3cafe in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:08Z] frame #2: <unknown function> + 0x5baa358 (0x7f8116a3e358 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:08Z] frame #3: <unknown function> + 0x5babb3e (0x7f8116a3fb3e in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:08Z] frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x298 (0x7f8116a39198 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:08Z] frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7f80c2699989 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\n[2025-05-20T10:52:08Z] frame #6: <unknown function> + 0xdc253 (0x7f80b29b3253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n[2025-05-20T10:52:08Z] frame #7: <unknown function> + 0x94ac3 (0x7f81b3fb9ac3 in /lib/x86_64-linux-gnu/libc.so.6)\n[2025-05-20T10:52:08Z] frame #8: clone + 0x44 (0x7f81b404aa04 in /lib/x86_64-linux-gnu/libc.so.6)\n[2025-05-20T10:52:08Z]\n[2025-05-20T10:52:08Z] [rank1]:[W520 03:52:08.321126013 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0 Rank 1] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Broken pipe\n[2025-05-20T10:52:09Z] [rank1]:[W520 03:52:09.321236826 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=135, addr=[localhost]:59310, remote=[localhost]:44713): Broken pipe\n[2025-05-20T10:52:09Z] Exception raised from sendBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:653 (most recent call first):\n[2025-05-20T10:52:09Z] frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f81329785e8 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)\n[2025-05-20T10:52:09Z] frame #1: <unknown function> + 0x5ba8afe (0x7f8116a3cafe in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:09Z] frame #2: <unknown function> + 0x5baa358 (0x7f8116a3e358 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:09Z] frame #3: <unknown function> + 0x5babb3e (0x7f8116a3fb3e in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:09Z] frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x298 (0x7f8116a39198 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)\n[2025-05-20T10:52:09Z] frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7f80c2699989 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so)\n[2025-05-20T10:52:09Z] frame #6: <unknown function> + 0xdc253 (0x7f80b29b3253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n[2025-05-20T10:52:09Z] frame #7: <unknown function> + 0x94ac3 (0x7f81b3fb9ac3 in /lib/x86_64-linux-gnu/libc.so.6)\n[2025-05-20T10:52:09Z] frame #8: clone + 0x44 (0x7f81b404aa04 in /lib/x86_64-linux-gnu/libc.so.6)\n[2025-05-20T10:52:09Z]\n[2025-05-20T10:52:09Z] [rank1]:[W520 03:52:09.323951540 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0 Rank 1] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Broken pipe\n[2025-05-20T10:52:09Z] Process EngineCore_0:\n[2025-05-20T10:52:09Z] Traceback (most recent call last):\n[2025-05-20T10:52:09Z]   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n[2025-05-20T10:52:09Z]     self.run()\n[2025-05-20T10:52:09Z]   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n[2025-05-20T10:52:09Z]     self._target(*self._args, **self._kwargs)\n[2025-05-20T10:52:09Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 493, in run_engine_core\n[2025-05-20T10:52:09Z]     raise e\n[2025-05-20T10:52:09Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 480, in run_engine_core\n[2025-05-20T10:52:09Z]     engine_core = EngineCoreProc(*args, **kwargs)\n[2025-05-20T10:52:09Z]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:52:09Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 379, in __init__\n[2025-05-20T10:52:09Z]     super().__init__(vllm_config, executor_class, log_stats,\n[2025-05-20T10:52:09Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 74, in __init__\n[2025-05-20T10:52:09Z]     self._initialize_kv_caches(vllm_config)\n[2025-05-20T10:52:09Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 133, in _initialize_kv_caches\n[2025-05-20T10:52:09Z]     available_gpu_memory = self.model_executor.determine_available_memory()\n[2025-05-20T10:52:09Z]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:52:09Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py\", line 75, in determine_available_memory\n[2025-05-20T10:52:09Z]     output = self.collective_rpc(\"determine_available_memory\")\n[2025-05-20T10:52:09Z]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:52:09Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 215, in collective_rpc\n[2025-05-20T10:52:09Z]     result = get_response(w, dequeue_timeout)\n[2025-05-20T10:52:09Z]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-20T10:52:09Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 202, in get_response\n[2025-05-20T10:52:09Z]     raise RuntimeError(\n[2025-05-20T10:52:09Z] RuntimeError: Worker failed with error 'CUDA error: an illegal memory access was encountered\n[2025-05-20T10:52:09Z] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[2025-05-20T10:52:09Z] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[2025-05-20T10:52:09Z] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[2025-05-20T10:52:09Z] ', please check the stack trace above for the root cause\n[2025-05-20T10:52:10Z] /usr/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown\n[2025-05-20T10:52:10Z]   warnings.warn('resource_tracker: There appear to be %d '\n[2025-05-20T10:52:10Z] F\n[2025-05-20T10:52:10Z]\n[2025-05-20T10:52:10Z] =================================== FAILURES ===================================\n[2025-05-20T10:52:10Z] _____________________________ test_weight_loading ______________________________\n[2025-05-20T10:52:10Z]\n[2025-05-20T10:52:10Z] vllm_runner = <class 'tests.conftest.VllmRunner'>\n[2025-05-20T10:52:10Z]\n[2025-05-20T10:52:10Z]     @pytest.mark.skipif(\n[2025-05-20T10:52:10Z]         MODEL_NAME == \"casperhansen/deepseek-coder-v2-instruct-awq\",\n[2025-05-20T10:52:10Z]         reason=\"OOM in the CI\")\n[2025-05-20T10:52:10Z]     @pytest.mark.skipif(\n[2025-05-20T10:52:10Z]         not current_platform.has_device_capability(int(MIN_CAPABILITY)),\n[2025-05-20T10:52:10Z]         reason=\"Current system does not have minimum capability.\")\n[2025-05-20T10:52:10Z]     def test_weight_loading(vllm_runner):\n[2025-05-20T10:52:10Z]         \"\"\"\n[2025-05-20T10:52:10Z]         Test parameter weight loading with tp>1.\n[2025-05-20T10:52:10Z]         \"\"\"\n[2025-05-20T10:52:10Z]\n[2025-05-20T10:52:10Z]         # MoE models need fp16.\n[2025-05-20T10:52:10Z]         NEEDS_FP16 = (QUANTIZATION == \"gptq\" or MODEL_NAME\n[2025-05-20T10:52:10Z]                       == \"nm-testing/test-w4a16-mixtral-actorder-group\")\n[2025-05-20T10:52:10Z] >       with vllm_runner(\n[2025-05-20T10:52:10Z]                 model_name=MODEL_NAME,\n[2025-05-20T10:52:10Z]                 revision=REVISION,\n[2025-05-20T10:52:10Z]                 dtype=torch.half if NEEDS_FP16 else \"auto\",\n[2025-05-20T10:52:10Z]                 quantization=None if QUANTIZATION == \"None\" else QUANTIZATION,\n[2025-05-20T10:52:10Z]                 max_model_len=MAX_MODEL_LEN,\n[2025-05-20T10:52:10Z]                 tensor_parallel_size=2) as model:\n[2025-05-20T10:52:10Z]\n[2025-05-20T10:52:10Z] weight_loading/test_weight_loading.py:32:\n[2025-05-20T10:52:10Z] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n[2025-05-20T10:52:10Z] conftest.py:762: in __init__\n[2025-05-20T10:52:10Z]     self.model = LLM(\n[2025-05-20T10:52:10Z] /usr/local/lib/python3.12/dist-packages/vllm/utils.py:1177: in inner\n[2025-05-20T10:52:10Z]     return fn(*args, **kwargs)\n[2025-05-20T10:52:10Z] /usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:250: in __init__\n[2025-05-20T10:52:10Z]     self.llm_engine = LLMEngine.from_engine_args(\n[2025-05-20T10:52:10Z] /usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py:511: in from_engine_args\n[2025-05-20T10:52:10Z]     return engine_cls.from_vllm_config(\n[2025-05-20T10:52:10Z] /usr/local/lib/python3.12/dist-packages/vllm/v1/engine/llm_engine.py:115: in from_vllm_config\n[2025-05-20T10:52:10Z]     return cls(vllm_config=vllm_config,\n[2025-05-20T10:52:10Z] /usr/local/lib/python3.12/dist-packages/vllm/v1/engine/llm_engine.py:92: in __init__\n[2025-05-20T10:52:10Z]     self.engine_core = EngineCoreClient.make_client(\n[2025-05-20T10:52:10Z] /usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:75: in make_client\n[2025-05-20T10:52:10Z]     return SyncMPClient(vllm_config, executor_class, log_stats)\n[2025-05-20T10:52:10Z] /usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:580: in __init__\n[2025-05-20T10:52:10Z]     super().__init__(\n[2025-05-20T10:52:10Z] /usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:418: in __init__\n[2025-05-20T10:52:10Z]     self._wait_for_engine_startup(output_address, parallel_config)\n[2025-05-20T10:52:10Z] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n[2025-05-20T10:52:10Z]\n[2025-05-20T10:52:10Z] self = <vllm.v1.engine.core_client.SyncMPClient object at 0x7f7f5ea652e0>\n[2025-05-20T10:52:10Z] output_address = 'ipc:///tmp/d003abd2-4e16-42b2-9050-bf1e9dc8d357'\n[2025-05-20T10:52:10Z] parallel_config = ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=2, data_parallel_size=1, data_parallel_size_local=1, dat...p', worker_cls='vllm.v1.worker.gpu_worker.Worker', sd_worker_cls='auto', worker_extension_cls='', world_size=2, rank=0)\n[2025-05-20T10:52:10Z]\n[2025-05-20T10:52:10Z]     def _wait_for_engine_startup(self, output_address: str,\n[2025-05-20T10:52:10Z]                                  parallel_config: ParallelConfig):\n[2025-05-20T10:52:10Z]         # Get a sync handle to the socket which can be sync or async.\n[2025-05-20T10:52:10Z]         sync_input_socket = zmq.Socket.shadow(self.input_socket)\n[2025-05-20T10:52:10Z]\n[2025-05-20T10:52:10Z]         # Wait for engine core process(es) to send ready messages.\n[2025-05-20T10:52:10Z]         local_count = parallel_config.data_parallel_size_local\n[2025-05-20T10:52:10Z]         remote_count = len(self.core_engines) - local_count\n[2025-05-20T10:52:10Z]         # [local, remote] counts\n[2025-05-20T10:52:10Z]         conn_pending, start_pending = [local_count, remote_count], [0, 0]\n[2025-05-20T10:52:10Z]\n[2025-05-20T10:52:10Z]         poller = zmq.Poller()\n[2025-05-20T10:52:10Z]         poller.register(sync_input_socket, zmq.POLLIN)\n[2025-05-20T10:52:10Z]         proc_manager = self.resources.local_engine_manager\n[2025-05-20T10:52:10Z]         if proc_manager is not None:\n[2025-05-20T10:52:10Z]             for sentinel in proc_manager.sentinels():\n[2025-05-20T10:52:10Z]                 poller.register(sentinel, zmq.POLLIN)\n[2025-05-20T10:52:10Z]         while any(conn_pending) or any(start_pending):\n[2025-05-20T10:52:10Z]             events = poller.poll(STARTUP_POLL_PERIOD_MS)\n[2025-05-20T10:52:10Z]             if not events:\n[2025-05-20T10:52:10Z]                 if any(conn_pending):\n[2025-05-20T10:52:10Z]                     logger.debug(\n[2025-05-20T10:52:10Z]                         \"Waiting for %d local, %d remote core engine proc(s) \"\n[2025-05-20T10:52:10Z]                         \"to connect.\", *conn_pending)\n[2025-05-20T10:52:10Z]                 if any(start_pending):\n[2025-05-20T10:52:10Z]                     logger.debug(\n[2025-05-20T10:52:10Z]                         \"Waiting for %d local, %d remote core engine proc(s) \"\n[2025-05-20T10:52:10Z]                         \"to start.\", *start_pending)\n[2025-05-20T10:52:10Z]                 continue\n[2025-05-20T10:52:10Z]             if len(events) > 1 or events[0][0] != sync_input_socket:\n[2025-05-20T10:52:10Z]                 # One of the local core processes exited.\n[2025-05-20T10:52:10Z]                 finished = proc_manager.finished_procs(\n[2025-05-20T10:52:10Z]                 ) if proc_manager else {}\n[2025-05-20T10:52:10Z] >               raise RuntimeError(\"Engine core initialization failed. \"\n[2025-05-20T10:52:10Z]                                    \"See root cause above. \"\n[2025-05-20T10:52:10Z]                                    f\"Failed core proc(s): {finished}\")\n[2025-05-20T10:52:10Z] E               RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_0': 1}\n[2025-05-20T10:52:10Z]\n[2025-05-20T10:52:10Z] /usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:484: RuntimeError\n```\n\n</details>",
    "labels": [
      "bug",
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-05-20T15:11:52+00:00",
    "closed_at": "2025-05-22T13:48:57+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18416/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18416"
  },
  {
    "number": 18528,
    "title": "[Bug][Failing Test]: Multi-Modal Models 3 - models/multimodal/generation/test_common.py",
    "body": "### Your current environment\n\nN/A\n\n### \ud83d\udc1b Describe the bug\n\n`models/multimodal/generation/test_common.py::test_single_image_models[gemma3-test_case91]` is failing on main. It is another illegal memory access error.\n\nhttps://buildkite.com/vllm/ci/builds/20503/steps?jid=0196f626-d4d6-4af6-b10f-da8c3145ddfc\n\nStack:\n```\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:68] Dumping input data\n--- Logging error ---\n[2025-05-22T05:33:18Z] Traceback (most recent call last):\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 207, in execute_model\n[2025-05-22T05:33:18Z]     return self.model_executor.execute_model(scheduler_output)\n[2025-05-22T05:33:18Z]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py\", line 86, in execute_model\n[2025-05-22T05:33:18Z]     output = self.collective_rpc(\"execute_model\",\n[2025-05-22T05:33:18Z]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n[2025-05-22T05:33:18Z]     answer = run_method(self.driver_worker, method, args, kwargs)\n[2025-05-22T05:33:18Z]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2598, in run_method\n[2025-05-22T05:33:18Z]     return func(*args, **kwargs)\n[2025-05-22T05:33:18Z]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[2025-05-22T05:33:18Z]     return func(*args, **kwargs)\n[2025-05-22T05:33:18Z]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 276, in execute_model\n[2025-05-22T05:33:18Z]     output = self.model_runner.execute_model(scheduler_output,\n[2025-05-22T05:33:18Z]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[2025-05-22T05:33:18Z]     return func(*args, **kwargs)\n[2025-05-22T05:33:18Z]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1121, in execute_model\n[2025-05-22T05:33:18Z]     self._prepare_inputs(scheduler_output))\n[2025-05-22T05:33:18Z]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 528, in _prepare_inputs\n[2025-05-22T05:33:18Z]     self.input_batch.block_table.commit(num_reqs)\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/block_table.py\", line 81, in commit\n[2025-05-22T05:33:18Z]     self.block_table[:num_reqs].copy_(self.block_table_cpu[:num_reqs],\n[2025-05-22T05:33:18Z] RuntimeError: CUDA error: an illegal memory access was encountered\n[2025-05-22T05:33:18Z] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[2025-05-22T05:33:18Z] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[2025-05-22T05:33:18Z] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[2025-05-22T05:33:18Z] \n[2025-05-22T05:33:18Z] \n[2025-05-22T05:33:18Z] During handling of the above exception, another exception occurred:\n[2025-05-22T05:33:18Z] \n[2025-05-22T05:33:18Z] Traceback (most recent call last):\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/logging/__init__.py\", line 1160, in emit\n[2025-05-22T05:33:18Z]     msg = self.format(record)\n[2025-05-22T05:33:18Z]           ^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/logging/__init__.py\", line 999, in format\n[2025-05-22T05:33:18Z]     return fmt.format(record)\n[2025-05-22T05:33:18Z]            ^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/logging_utils/formatter.py\", line 13, in format\n[2025-05-22T05:33:18Z]     msg = logging.Formatter.format(self, record)\n[2025-05-22T05:33:18Z]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/logging/__init__.py\", line 703, in format\n[2025-05-22T05:33:18Z]     record.message = record.getMessage()\n[2025-05-22T05:33:18Z]                      ^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/logging/__init__.py\", line 392, in getMessage\n[2025-05-22T05:33:18Z]     msg = msg % self.args\n[2025-05-22T05:33:18Z]           ~~~~^~~~~~~~~~~\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/config.py\", line 4488, in __str__\n[2025-05-22T05:33:18Z]     f\"compilation_config={self.compilation_config!r}\")\n[2025-05-22T05:33:18Z]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/config.py\", line 3872, in __repr__\n[2025-05-22T05:33:18Z]     for k, v in asdict(self).items():\n[2025-05-22T05:33:18Z]                 ^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/dataclasses.py\", line 1329, in asdict\n[2025-05-22T05:33:18Z]     return _asdict_inner(obj, dict_factory)\n[2025-05-22T05:33:18Z]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/dataclasses.py\", line 1339, in _asdict_inner\n[2025-05-22T05:33:18Z]     f.name: _asdict_inner(getattr(obj, f.name), dict)\n[2025-05-22T05:33:18Z]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/dataclasses.py\", line 1382, in _asdict_inner\n[2025-05-22T05:33:18Z]     return type(obj)((_asdict_inner(k, dict_factory),\n[2025-05-22T05:33:18Z]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/dataclasses.py\", line 1383, in <genexpr>\n[2025-05-22T05:33:18Z]     _asdict_inner(v, dict_factory))\n[2025-05-22T05:33:18Z]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/dataclasses.py\", line 1386, in _asdict_inner\n[2025-05-22T05:33:18Z]     return copy.deepcopy(obj)\n[2025-05-22T05:33:18Z]            ^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/copy.py\", line 162, in deepcopy\n[2025-05-22T05:33:18Z]     y = _reconstruct(x, memo, *rv)\n[2025-05-22T05:33:18Z]         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/copy.py\", line 259, in _reconstruct\n[2025-05-22T05:33:18Z]     state = deepcopy(state, memo)\n[2025-05-22T05:33:18Z]             ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/copy.py\", line 136, in deepcopy\n[2025-05-22T05:33:18Z]     y = copier(x, memo)\n[2025-05-22T05:33:18Z]         ^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/copy.py\", line 221, in _deepcopy_dict\n[2025-05-22T05:33:18Z]     y[deepcopy(key, memo)] = deepcopy(value, memo)\n[2025-05-22T05:33:18Z]                              ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/copy.py\", line 143, in deepcopy\n[2025-05-22T05:33:18Z]     y = copier(memo)\n[2025-05-22T05:33:18Z]         ^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\", line 172, in __deepcopy__\n[2025-05-22T05:33:18Z]     new_storage = self._typed_storage()._deepcopy(memo)\n[2025-05-22T05:33:18Z]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/torch/storage.py\", line 1134, in _deepcopy\n[2025-05-22T05:33:18Z]     return self._new_wrapped_storage(copy.deepcopy(self._untyped_storage, memo))\n[2025-05-22T05:33:18Z]                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/copy.py\", line 143, in deepcopy\n[2025-05-22T05:33:18Z]     y = copier(memo)\n[2025-05-22T05:33:18Z]         ^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/torch/storage.py\", line 239, in __deepcopy__\n[2025-05-22T05:33:18Z]     new_storage = self.clone()\n[2025-05-22T05:33:18Z]                   ^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/torch/storage.py\", line 253, in clone\n[2025-05-22T05:33:18Z]     return type(self)(self.nbytes(), device=self.device).copy_(self)\n[2025-05-22T05:33:18Z]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z] RuntimeError: CUDA error: an illegal memory access was encountered\n[2025-05-22T05:33:18Z] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[2025-05-22T05:33:18Z] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[2025-05-22T05:33:18Z] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[2025-05-22T05:33:18Z] \n[2025-05-22T05:33:18Z] Call stack:\n[2025-05-22T05:33:18Z]   File \"<string>\", line 1, in <module>\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n[2025-05-22T05:33:18Z]     exitcode = _main(fd, parent_sentinel)\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/multiprocessing/spawn.py\", line 135, in _main\n[2025-05-22T05:33:18Z]     return self._bootstrap(parent_sentinel)\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n[2025-05-22T05:33:18Z]     self.run()\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n[2025-05-22T05:33:18Z]     self._target(*self._args, **self._kwargs)\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 486, in run_engine_core\n[2025-05-22T05:33:18Z]     engine_core.run_busy_loop()\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 513, in run_busy_loop\n[2025-05-22T05:33:18Z]     self._process_engine_step()\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 538, in _process_engine_step\n[2025-05-22T05:33:18Z]     outputs = self.step_fn()\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 226, in step\n[2025-05-22T05:33:18Z]     model_output = self.execute_model(scheduler_output)\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 210, in execute_model\n[2025-05-22T05:33:18Z]     dump_engine_exception(self.vllm_config, scheduler_output,\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/logging_utils/dump_input.py\", line 62, in dump_engine_exception\n[2025-05-22T05:33:18Z]     _dump_engine_exception(config, scheduler_output, scheduler_stats)\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/logging_utils/dump_input.py\", line 70, in _dump_engine_exception\n[2025-05-22T05:33:18Z]     logger.error(\n[2025-05-22T05:33:18Z] Unable to print the message and arguments - possible formatting error.\n[2025-05-22T05:33:18Z] Use the traceback above to help find the error.\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:78] Dumping scheduler output for model execution:\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79] SchedulerOutput(scheduled_new_reqs=[NewRequestData(req_id=0,prompt_token_ids_len=281,mm_inputs=[{'pixel_values': tensor([[[[-0.6314, -0.6314, -0.6314,  ...,  0.5922,  0.5451,  0.5373],\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]           [-0.6314, -0.6314, -0.6314,  ...,  0.5922,  0.5451,  0.5373],\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]           [-0.6314, -0.6314, -0.6314,  ...,  0.5529,  0.5059,  0.4980],\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]           ...,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]           [ 0.3176,  0.3176,  0.3020,  ...,  0.5294,  0.5373,  0.5373],\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]           [ 0.3176,  0.3176,  0.3020,  ...,  0.5294,  0.5373,  0.5373],\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]           [ 0.3176,  0.3176,  0.3020,  ...,  0.5294,  0.5373,  0.5373]],\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79] \n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          [[-0.8980, -0.8980, -0.8980,  ...,  0.5216,  0.4431,  0.4353],\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]           [-0.8980, -0.8980, -0.8980,  ...,  0.5216,  0.4431,  0.4353],\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]           [-0.8980, -0.8980, -0.8980,  ...,  0.4588,  0.3882,  0.3804],\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]           ...,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]           [ 0.3647,  0.3647,  0.3490,  ...,  0.5451,  0.5529,  0.5529],\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]           [ 0.3647,  0.3647,  0.3490,  ...,  0.5451,  0.5529,  0.5529],\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]           [ 0.3647,  0.3647,  0.3490,  ...,  0.5451,  0.5529,  0.5529]],\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79] \n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          [[-0.9686, -0.9686, -0.9686,  ...,  0.4510,  0.3490,  0.3333],\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]           [-0.9686, -0.9686, -0.9686,  ...,  0.4510,  0.3490,  0.3333],\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]           [-0.9686, -0.9686, -0.9686,  ...,  0.3725,  0.2784,  0.2627],\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]           ...,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]           [ 0.2863,  0.2863,  0.2706,  ...,  0.4431,  0.4510,  0.4510],\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]           [ 0.2863,  0.2863,  0.2706,  ...,  0.4431,  0.4510,  0.4510],\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]           [ 0.2863,  0.2863,  0.2706,  ...,  0.4431,  0.4510,  0.4510]]]]), 'num_crops': tensor([0])}],mm_hashes=['f60a83610bcc902af2e0be4780926de06a310afae0d11f9d2feee331134ff15a'],mm_positions=[PlaceholderRange(offset=4, length=260, is_embed=tensor([False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [dump_input.py:79]          True,  True,  True,  True,  True,  True,  True,  True, False, False]))],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[106], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=5, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18],num_computed_tokens=0,lora_request=None)],scheduled_cached_reqs=[],num_scheduled_tokens={0: 281},total_num_scheduled_tokens=281,scheduled_spec_decode_tokens={},scheduled_encoder_inputs={0: [0]},num_common_prefix_blocks=18,finished_req_ids=[],free_encoder_input_ids=[],structured_output_request_ids={},grammar_bitmask=null,kv_connector_metadata=null)\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495] EngineCore encountered a fatal error.\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495] Traceback (most recent call last):\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 486, in run_engine_core\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]     engine_core.run_busy_loop()\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 513, in run_busy_loop\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]     self._process_engine_step()\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 538, in _process_engine_step\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]     outputs = self.step_fn()\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]               ^^^^^^^^^^^^^^\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 226, in step\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]     model_output = self.execute_model(scheduler_output)\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 213, in execute_model\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]     raise err\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 207, in execute_model\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]     return self.model_executor.execute_model(scheduler_output)\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py\", line 86, in execute_model\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]     output = self.collective_rpc(\"execute_model\",\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]     answer = run_method(self.driver_worker, method, args, kwargs)\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2598, in run_method\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]     return func(*args, **kwargs)\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]            ^^^^^^^^^^^^^^^^^^^^^\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]     return func(*args, **kwargs)\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]            ^^^^^^^^^^^^^^^^^^^^^\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 276, in execute_model\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]     output = self.model_runner.execute_model(scheduler_output,\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]     return func(*args, **kwargs)\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]            ^^^^^^^^^^^^^^^^^^^^^\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1121, in execute_model\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]     self._prepare_inputs(scheduler_output))\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 528, in _prepare_inputs\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]     self.input_batch.block_table.commit(num_reqs)\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/block_table.py\", line 81, in commit\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495]     self.block_table[:num_reqs].copy_(self.block_table_cpu[:num_reqs],\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495] RuntimeError: CUDA error: an illegal memory access was encountered\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n[2025-05-22T05:33:18Z] ERROR 05-21 22:33:18 [core.py:495] \n[2025-05-22T05:33:18Z] Process EngineCore_0:\n[2025-05-22T05:33:18Z] Traceback (most recent call last):\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n[2025-05-22T05:33:18Z]     self.run()\n[2025-05-22T05:33:18Z]   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n[2025-05-22T05:33:18Z]     self._target(*self._args, **self._kwargs)\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 497, in run_engine_core\n[2025-05-22T05:33:18Z]     raise e\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 486, in run_engine_core\n[2025-05-22T05:33:18Z]     engine_core.run_busy_loop()\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 513, in run_busy_loop\n[2025-05-22T05:33:18Z]     self._process_engine_step()\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 538, in _process_engine_step\n[2025-05-22T05:33:18Z]     outputs = self.step_fn()\n[2025-05-22T05:33:18Z]               ^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 226, in step\n[2025-05-22T05:33:18Z]     model_output = self.execute_model(scheduler_output)\n[2025-05-22T05:33:18Z]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 213, in execute_model\n[2025-05-22T05:33:18Z]     raise err\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 207, in execute_model\n[2025-05-22T05:33:18Z]     return self.model_executor.execute_model(scheduler_output)\n[2025-05-22T05:33:18Z]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py\", line 86, in execute_model\n[2025-05-22T05:33:18Z]     output = self.collective_rpc(\"execute_model\",\n[2025-05-22T05:33:18Z]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n[2025-05-22T05:33:18Z]     answer = run_method(self.driver_worker, method, args, kwargs)\n[2025-05-22T05:33:18Z]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2598, in run_method\n[2025-05-22T05:33:18Z]     return func(*args, **kwargs)\n[2025-05-22T05:33:18Z]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[2025-05-22T05:33:18Z]     return func(*args, **kwargs)\n[2025-05-22T05:33:18Z]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 276, in execute_model\n[2025-05-22T05:33:18Z]     output = self.model_runner.execute_model(scheduler_output,\n[2025-05-22T05:33:18Z]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[2025-05-22T05:33:18Z]     return func(*args, **kwargs)\n[2025-05-22T05:33:18Z]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1121, in execute_model\n[2025-05-22T05:33:18Z]     self._prepare_inputs(scheduler_output))\n[2025-05-22T05:33:18Z]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 528, in _prepare_inputs\n[2025-05-22T05:33:18Z]     self.input_batch.block_table.commit(num_reqs)\n[2025-05-22T05:33:18Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/block_table.py\", line 81, in commit\n[2025-05-22T05:33:18Z]     self.block_table[:num_reqs].copy_(self.block_table_cpu[:num_reqs],\n[2025-05-22T05:33:18Z] RuntimeError: CUDA error: an illegal memory access was encountered\n[2025-05-22T05:33:18Z] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[2025-05-22T05:33:18Z] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[2025-05-22T05:33:18Z] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n[2025-05-22T05:33:18Z] \n[2025-05-22T05:33:18Z] \n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-05-22T06:03:59+00:00",
    "closed_at": "2025-05-23T01:55:57+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18528/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18528"
  },
  {
    "number": 19731,
    "title": "[CI Failure]: Distributed Tests (2 GPUs) - v1/test_async_llm_dp.py::test_load",
    "body": "### Name of failing test\n\nTP_SIZE=1 DP_SIZE=2 pytest -s -v \"v1/test_async_llm_dp.py::test_load[ray-RequestOutputKind.DELTA]\"\n\n### Basic information\n\n- [ ] Flaky test\n- [x] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\nThe error ends up looking like a triton bug with `AttributeError: module 'triton.language' has no attribute 'bfloat16'` reported, however very early in the logs you can see the following:\n```\nINFO 06-17 07:32:31 [utils.py:384] Creating placement groups for data parallel\n(pid=3893316) INFO 06-17 07:32:33 [importing.py:27] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.\n(pid=3893316) INFO 06-17 07:32:33 [importing.py:47] Triton not installed or not compatible; certain GPU-related functions will not be available.\n(pid=3893316) WARNING 06-17 07:32:33 [importing.py:59] Triton is not installed. Using dummy decorators. Install it via `pip install triton` to enable kernel compilation.\n```\n\nThis is strange because triton is fully installed in my environment as usual.\n\nHere is the full command and traceback:\n```\nTP_SIZE=1 DP_SIZE=2 pytest -s -v \"v1/test_async_llm_dp.py::test_load[ray-RequestOutputKind.DELTA]\"\nINFO 06-17 07:32:14 [__init__.py:244] Automatically detected platform cuda.\n/home/mgoin/venvs/vllm/lib/python3.12/site-packages/pytest_asyncio/plugin.py:208: PytestDeprecationWarning: The configuration option \"asyncio_default_fixture_loop_scope\" is unset.\nThe event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))\n============================================================================ test session starts =============================================================================\nplatform linux -- Python 3.12.4, pytest-8.3.3, pluggy-1.5.0 -- /home/mgoin/venvs/vllm/bin/python3\ncachedir: .pytest_cache\nhypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/home/mgoin/code/vllm/tests/.hypothesis/examples'))\nrootdir: /home/mgoin/code/vllm\nconfigfile: pyproject.toml\nplugins: forked-1.6.0, subtests-0.14.1, asyncio-0.24.0, shard-0.1.2, buildkite-test-collector-0.1.9, timeout-2.3.1, schemathesis-3.39.15, anyio-4.6.2.post1, mock-3.14.0, hypothesis-6.131.0, rerunfailures-14.0\nasyncio: mode=Mode.STRICT, default_loop_scope=None\ncollecting ... INFO 06-17 07:32:25 [config.py:831] This model supports multiple tasks: {'generate', 'score', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.\nINFO 06-17 07:32:25 [config.py:3270] Downcasting torch.float32 to torch.bfloat16.\nINFO 06-17 07:32:25 [config.py:1444] Using max model len 4096\nWARNING 06-17 07:32:25 [interface.py:503] Current platform cuda does not have '_pytestfixturefunction' attribute.\nWARNING 06-17 07:32:26 [interface.py:503] Current platform cuda does not have '__test__' attribute.\nWARNING 06-17 07:32:26 [interface.py:503] Current platform cuda does not have '__bases__' attribute.\nWARNING 06-17 07:32:26 [interface.py:503] Current platform cuda does not have '__test__' attribute.\nWARNING 06-17 07:32:26 [interface.py:503] Current platform cuda does not have '_schemathesis_test' attribute.\ncollected 1 item                                                                                                                                                             \nRunning 1 items in this shard: tests/v1/test_async_llm_dp.py::test_load[ray-RequestOutputKind.DELTA]\n\nv1/test_async_llm_dp.py::test_load[ray-RequestOutputKind.DELTA] INFO 06-17 07:32:26 [config.py:831] This model supports multiple tasks: {'generate', 'score', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.\nINFO 06-17 07:32:26 [config.py:3270] Downcasting torch.float32 to torch.bfloat16.\nINFO 06-17 07:32:26 [config.py:1444] Using max model len 4096\nINFO 06-17 07:32:26 [arg_utils.py:1095] Using host IP 216.81.245.69 as ray-based data parallel address\nINFO 06-17 07:32:26 [config.py:2197] Chunked prefill is enabled with max_num_batched_tokens=2048.\nWARNING 06-17 07:32:26 [cuda.py:91] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n2025-06-17 07:32:29,783 INFO worker.py:1832 -- Started a local Ray instance. View the dashboard at http://127.0.0.1:8265 \nINFO 06-17 07:32:31 [utils.py:384] Creating placement groups for data parallel\n(pid=3893316) INFO 06-17 07:32:33 [importing.py:27] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.\n(pid=3893316) INFO 06-17 07:32:33 [importing.py:47] Triton not installed or not compatible; certain GPU-related functions will not be available.\n(pid=3893316) WARNING 06-17 07:32:33 [importing.py:59] Triton is not installed. Using dummy decorators. Install it via `pip install triton` to enable kernel compilation.\n(pid=3893316) INFO 06-17 07:32:34 [__init__.py:244] Automatically detected platform cuda.\n(DPEngineCoreActor pid=3893316) INFO 06-17 07:32:38 [core.py:70] Initializing a V1 LLM engine (v0.9.1.dev287+g89b1388d8) with config: model='ibm-research/PowerMoE-3b', speculative_config=None, tokenizer='ibm-research/PowerMoE-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ibm-research/PowerMoE-3b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}\n(pid=3893317) INFO 06-17 07:32:33 [importing.py:27] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.\n(pid=3893317) INFO 06-17 07:32:33 [importing.py:47] Triton not installed or not compatible; certain GPU-related functions will not be available.\n(pid=3893317) WARNING 06-17 07:32:33 [importing.py:59] Triton is not installed. Using dummy decorators. Install it via `pip install triton` to enable kernel compilation.\n(DPEngineCoreActor pid=3893317) WARNING 06-17 07:32:39 [utils.py:2756] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x717cd7814710>\n(DPEngineCoreActor pid=3893316) INFO 06-17 07:32:40 [parallel_state.py:934] Adjusting world_size=2 rank=0 distributed_init_method=tcp://216.81.245.69:57446 for DP\n(pid=3893317) INFO 06-17 07:32:35 [__init__.py:244] Automatically detected platform cuda.\n(DPEngineCoreActor pid=3893316) INFO 06-17 07:32:40 [utils.py:1136] Found nccl from library libnccl.so.2\n(DPEngineCoreActor pid=3893316) INFO 06-17 07:32:40 [pynccl.py:70] vLLM is using nccl==2.26.2\n(DPEngineCoreActor pid=3893316) INFO 06-17 07:32:41 [cuda_communicator.py:65] Using naive all2all manager.\n(DPEngineCoreActor pid=3893316) INFO 06-17 07:32:41 [parallel_state.py:1065] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n(DPEngineCoreActor pid=3893316) WARNING 06-17 07:32:41 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n(DPEngineCoreActor pid=3893316) INFO 06-17 07:32:41 [gpu_model_runner.py:1627] Starting to load model ibm-research/PowerMoE-3b...\n(DPEngineCoreActor pid=3893316) INFO 06-17 07:32:42 [gpu_model_runner.py:1632] Loading model from scratch...\n(DPEngineCoreActor pid=3893316) INFO 06-17 07:32:42 [cuda.py:259] Using Flash Attention backend on V1 engine.\n(DPEngineCoreActor pid=3893317) INFO 06-17 07:32:42 [weight_utils.py:292] Using model weights format ['*.safetensors']\nLoading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards: 100% Completed | 3/3 [00:00<00:00, 55.38it/s]\n(DPEngineCoreActor pid=3893316) \n(DPEngineCoreActor pid=3893316) INFO 06-17 07:32:59 [default_loader.py:272] Loading weights took 16.47 seconds\n(DPEngineCoreActor pid=3893317) INFO 06-17 07:32:38 [core.py:70] Initializing a V1 LLM engine (v0.9.1.dev287+g89b1388d8) with config: model='ibm-research/PowerMoE-3b', speculative_config=None, tokenizer='ibm-research/PowerMoE-3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ibm-research/PowerMoE-3b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}\n(DPEngineCoreActor pid=3893316) WARNING 06-17 07:32:39 [utils.py:2756] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x78f88f5bcb30>\n(DPEngineCoreActor pid=3893317) INFO 06-17 07:32:40 [parallel_state.py:934] Adjusting world_size=2 rank=1 distributed_init_method=tcp://216.81.245.69:57446 for DP\n(DPEngineCoreActor pid=3893317) INFO 06-17 07:32:40 [utils.py:1136] Found nccl from library libnccl.so.2\n(DPEngineCoreActor pid=3893317) INFO 06-17 07:32:40 [pynccl.py:70] vLLM is using nccl==2.26.2\n(DPEngineCoreActor pid=3893317) INFO 06-17 07:32:41 [cuda_communicator.py:65] Using naive all2all manager.\n(DPEngineCoreActor pid=3893317) INFO 06-17 07:32:41 [parallel_state.py:1065] rank 1 in world size 2 is assigned as DP rank 1, PP rank 0, TP rank 0, EP rank 1\n(DPEngineCoreActor pid=3893317) WARNING 06-17 07:32:41 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n(DPEngineCoreActor pid=3893317) INFO 06-17 07:32:41 [gpu_model_runner.py:1627] Starting to load model ibm-research/PowerMoE-3b...\n(DPEngineCoreActor pid=3893317) INFO 06-17 07:32:42 [gpu_model_runner.py:1632] Loading model from scratch...\n(DPEngineCoreActor pid=3893317) INFO 06-17 07:32:42 [cuda.py:259] Using Flash Attention backend on V1 engine.\n(DPEngineCoreActor pid=3893316) INFO 06-17 07:32:42 [weight_utils.py:292] Using model weights format ['*.safetensors']\n(DPEngineCoreActor pid=3893316) INFO 06-17 07:33:00 [gpu_model_runner.py:1656] Model loading took 3.3375 GiB and 17.195956 seconds\n(DPEngineCoreActor pid=3893316) Exception raised in creation task: The actor died because of an error raised in its creation task, ray::DPEngineCoreActor.__init__() (pid=3893316, ip=216.81.245.69, actor_id=fa979d0e0af9f039b9196c4b01000000, repr=<vllm.v1.engine.core.DPEngineCoreActor object at 0x78f88f5a64e0>)\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 927, in __init__\n(DPEngineCoreActor pid=3893316)     super().__init__(vllm_config, on_head_node, \"\", executor_class,\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 769, in __init__\n(DPEngineCoreActor pid=3893316)     super().__init__(vllm_config, on_head_node, handshake_address,\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 395, in __init__\n(DPEngineCoreActor pid=3893316)     super().__init__(vllm_config, executor_class, log_stats,\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 83, in __init__\n(DPEngineCoreActor pid=3893316)     self._initialize_kv_caches(vllm_config)\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 143, in _initialize_kv_caches\n(DPEngineCoreActor pid=3893316)     available_gpu_memory = self.model_executor.determine_available_memory()\n(DPEngineCoreActor pid=3893316)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/v1/executor/abstract.py\", line 76, in determine_available_memory\n(DPEngineCoreActor pid=3893316)     output = self.collective_rpc(\"determine_available_memory\")\n(DPEngineCoreActor pid=3893316)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n(DPEngineCoreActor pid=3893316)     answer = run_method(self.driver_worker, method, args, kwargs)\n(DPEngineCoreActor pid=3893316)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/utils.py\", line 2690, in run_method\n(DPEngineCoreActor pid=3893316)     return func(*args, **kwargs)\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(DPEngineCoreActor pid=3893316)     return func(*args, **kwargs)\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_worker.py\", line 210, in determine_available_memory\n(DPEngineCoreActor pid=3893316)     self.model_runner.profile_run()\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_model_runner.py\", line 2045, in profile_run\n(DPEngineCoreActor pid=3893316)     hidden_states = self._dummy_run(self.max_num_tokens)\n(DPEngineCoreActor pid=3893316)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n(DPEngineCoreActor pid=3893316)     return func(*args, **kwargs)\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_model_runner.py\", line 1880, in _dummy_run\n(DPEngineCoreActor pid=3893316)     outputs = model(\n(DPEngineCoreActor pid=3893316)               ^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n(DPEngineCoreActor pid=3893316)     return self._call_impl(*args, **kwargs)\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n(DPEngineCoreActor pid=3893316)     return forward_call(*args, **kwargs)\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/model_executor/models/granitemoe.py\", line 405, in forward\n(DPEngineCoreActor pid=3893316)     hidden_states = self.model(input_ids, positions, intermediate_tensors,\n(DPEngineCoreActor pid=3893316)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/compilation/decorators.py\", line 173, in __call__\n(DPEngineCoreActor pid=3893316)     return self.forward(*args, **kwargs)\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/model_executor/models/granitemoe.py\", line 301, in forward\n(DPEngineCoreActor pid=3893316)     hidden_states = layer(positions, hidden_states)\n(DPEngineCoreActor pid=3893316)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n(DPEngineCoreActor pid=3893316)     return self._call_impl(*args, **kwargs)\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n(DPEngineCoreActor pid=3893316)     return forward_call(*args, **kwargs)\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/model_executor/models/granitemoe.py\", line 239, in forward\n(DPEngineCoreActor pid=3893316)     hidden_states = self.block_sparse_moe(hidden_states)\n(DPEngineCoreActor pid=3893316)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n(DPEngineCoreActor pid=3893316)     return self._call_impl(*args, **kwargs)\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n(DPEngineCoreActor pid=3893316)     return forward_call(*args, **kwargs)\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/model_executor/models/granitemoe.py\", line 101, in forward\n(DPEngineCoreActor pid=3893316)     final_hidden_states = self.experts(hidden_states, router_logits)\n(DPEngineCoreActor pid=3893316)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n(DPEngineCoreActor pid=3893316)     return self._call_impl(*args, **kwargs)\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n(DPEngineCoreActor pid=3893316)     return forward_call(*args, **kwargs)\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/layer.py\", line 1359, in forward\n(DPEngineCoreActor pid=3893316)     return torch.ops.vllm.moe_forward(hidden_states, router_logits,\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/_ops.py\", line 1158, in __call__\n(DPEngineCoreActor pid=3893316)     return self._op(*args, **(kwargs or {}))\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/layer.py\", line 1522, in moe_forward\n(DPEngineCoreActor pid=3893316)     return self.forward_impl(hidden_states, router_logits)\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/layer.py\", line 1449, in forward_impl\n(DPEngineCoreActor pid=3893316)     final_hidden_states = self.quant_method.apply(\n(DPEngineCoreActor pid=3893316)                           ^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/layer.py\", line 568, in apply\n(DPEngineCoreActor pid=3893316)     return self.forward(\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/model_executor/custom_op.py\", line 24, in forward\n(DPEngineCoreActor pid=3893316)     return self._forward_method(*args, **kwargs)\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/layer.py\", line 628, in forward_cuda\n(DPEngineCoreActor pid=3893316)     return self.fused_experts(\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1186, in fused_experts\n(DPEngineCoreActor pid=3893316)     return dispatch_fused_experts_func(inplace)(\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1125, in torch_vllm_inplace_fused_experts\n(DPEngineCoreActor pid=3893316)     torch.ops.vllm.inplace_fused_experts(**kwargs)\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/_ops.py\", line 1158, in __call__\n(DPEngineCoreActor pid=3893316)     return self._op(*args, **(kwargs or {}))\n(DPEngineCoreActor pid=3893316)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1018, in inplace_fused_experts\n(DPEngineCoreActor pid=3893316)     fused_experts_impl(hidden_states, w1, w2, topk_weights, topk_ids, True,\n(DPEngineCoreActor pid=3893316)   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1295, in fused_experts_impl\n(DPEngineCoreActor pid=3893316)     compute_type = tl.bfloat16\n(DPEngineCoreActor pid=3893316)                    ^^^^^^^^^^^\n(DPEngineCoreActor pid=3893316) AttributeError: module 'triton.language' has no attribute 'bfloat16'\n(DPEngineCoreActor pid=3893316) WARNING 06-17 07:33:00 [fused_moe.py:683] Using default MoE config. Performance might be sub-optimal! Config file not found at /home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/configs/E=40,N=256,device_name=NVIDIA_H100_80GB_HBM3.json\nFAILED\n\n================================================================================== FAILURES ==================================================================================\n___________________________________________________________________ test_load[ray-RequestOutputKind.DELTA] ___________________________________________________________________\n\noutput_kind = <RequestOutputKind.DELTA: 1>, data_parallel_backend = 'ray'\n\n    @pytest.mark.parametrize(\n        \"output_kind\",\n        [\n            RequestOutputKind.DELTA,\n            RequestOutputKind.FINAL_ONLY,\n        ],\n    )\n    @pytest.mark.parametrize(\"data_parallel_backend\", [\"mp\", \"ray\"])\n    @pytest.mark.asyncio\n    async def test_load(output_kind: RequestOutputKind,\n                        data_parallel_backend: str):\n    \n        with ExitStack() as after:\n    \n            prompt = \"This is a test of data parallel\"\n    \n            engine_args.data_parallel_backend = data_parallel_backend\n>           engine = AsyncLLM.from_engine_args(engine_args)\n\nv1/test_async_llm_dp.py:82: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../vllm/v1/engine/async_llm.py:189: in from_engine_args\n    return cls(\n../vllm/v1/engine/async_llm.py:124: in __init__\n    self.engine_core = EngineCoreClient.make_async_mp_client(\n../vllm/v1/engine/core_client.py:89: in make_async_mp_client\n    return RayDPClient(vllm_config, executor_class, log_stats,\n../vllm/v1/engine/core_client.py:1099: in __init__\n    super().__init__(vllm_config, executor_class, log_stats,\n../vllm/v1/engine/core_client.py:919: in __init__\n    super().__init__(vllm_config, executor_class, log_stats,\n../vllm/v1/engine/core_client.py:716: in __init__\n    super().__init__(\n../vllm/v1/engine/core_client.py:422: in __init__\n    self._init_engines_direct(vllm_config, local_only,\n../vllm/v1/engine/core_client.py:1125: in _init_engines_direct\n    self.resources.engine_manager = CoreEngineActorManager(\n../vllm/v1/utils.py:370: in __init__\n    ray.get(refs)\n../../../venvs/vllm/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:21: in auto_init_wrapper\n    return fn(*args, **kwargs)\n../../../venvs/vllm/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:103: in wrapper\n    return func(*args, **kwargs)\n../../../venvs/vllm/lib/python3.12/site-packages/ray/_private/worker.py:2771: in get\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ray._private.worker.Worker object at 0x729848397e00>\nobject_refs = [ObjectRef(16310a0f0a45af5cfa979d0e0af9f039b9196c4b0100000001000000), ObjectRef(32d950ec0ccf9d2a298429e99af9b3dd80f746b40100000001000000)], timeout = None\nreturn_exceptions = False, skip_deserialization = False\n\n    def get_objects(\n        self,\n        object_refs: list,\n        timeout: Optional[float] = None,\n        return_exceptions: bool = False,\n        skip_deserialization: bool = False,\n    ):\n        \"\"\"Get the values in the object store associated with the IDs.\n    \n        Return the values from the local object store for object_refs. This\n        will block until all the values for object_refs have been written to\n        the local object store.\n    \n        Args:\n            object_refs: A list of the object refs\n                whose values should be retrieved.\n            timeout: The maximum amount of time in\n                seconds to wait before returning.\n            return_exceptions: If any of the objects deserialize to an\n                Exception object, whether to return them as values in the\n                returned list. If False, then the first found exception will be\n                raised.\n            skip_deserialization: If true, only the buffer will be released and\n                the object associated with the buffer will not be deserailized.\n        Returns:\n            list: List of deserialized objects or None if skip_deserialization is True.\n            bytes: UUID of the debugger breakpoint we should drop\n                into or b\"\" if there is no breakpoint.\n        \"\"\"\n        # Make sure that the values are object refs.\n        for object_ref in object_refs:\n            if not isinstance(object_ref, ObjectRef):\n                raise TypeError(\n                    f\"Attempting to call `get` on the value {object_ref}, \"\n                    \"which is not an ray.ObjectRef.\"\n                )\n    \n        timeout_ms = (\n            int(timeout * 1000) if timeout is not None and timeout != -1 else -1\n        )\n        data_metadata_pairs: List[\n            Tuple[ray._raylet.Buffer, bytes]\n        ] = self.core_worker.get_objects(\n            object_refs,\n            timeout_ms,\n        )\n    \n        debugger_breakpoint = b\"\"\n        for data, metadata in data_metadata_pairs:\n            if metadata:\n                metadata_fields = metadata.split(b\",\")\n                if len(metadata_fields) >= 2 and metadata_fields[1].startswith(\n                    ray_constants.OBJECT_METADATA_DEBUG_PREFIX\n                ):\n                    debugger_breakpoint = metadata_fields[1][\n                        len(ray_constants.OBJECT_METADATA_DEBUG_PREFIX) :\n                    ]\n        if skip_deserialization:\n            return None, debugger_breakpoint\n    \n        values = self.deserialize_objects(data_metadata_pairs, object_refs)\n        if not return_exceptions:\n            # Raise exceptions instead of returning them to the user.\n            for i, value in enumerate(values):\n                if isinstance(value, RayError):\n                    if isinstance(value, ray.exceptions.ObjectLostError):\n                        global_worker.core_worker.dump_object_store_memory_usage()\n                    if isinstance(value, RayTaskError):\n                        raise value.as_instanceof_cause()\n                    else:\n>                       raise value\nE                       ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, ray::DPEngineCoreActor.__init__() (pid=3893316, ip=216.81.245.69, actor_id=fa979d0e0af9f039b9196c4b01000000, repr=<vllm.v1.engine.core.DPEngineCoreActor object at 0x78f88f5a64e0>)\nE                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 927, in __init__\nE                           super().__init__(vllm_config, on_head_node, \"\", executor_class,\nE                         File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 769, in __init__\nE                           super().__init__(vllm_config, on_head_node, handshake_address,\nE                         File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 395, in __init__\nE                           super().__init__(vllm_config, executor_class, log_stats,\nE                         File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 83, in __init__\nE                           self._initialize_kv_caches(vllm_config)\nE                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 143, in _initialize_kv_caches\nE                           available_gpu_memory = self.model_executor.determine_available_memory()\nE                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/v1/executor/abstract.py\", line 76, in determine_available_memory\nE                           output = self.collective_rpc(\"determine_available_memory\")\nE                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\nE                           answer = run_method(self.driver_worker, method, args, kwargs)\nE                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/utils.py\", line 2690, in run_method\nE                           return func(*args, **kwargs)\nE                                  ^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nE                           return func(*args, **kwargs)\nE                                  ^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_worker.py\", line 210, in determine_available_memory\nE                           self.model_runner.profile_run()\nE                         File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_model_runner.py\", line 2045, in profile_run\nE                           hidden_states = self._dummy_run(self.max_num_tokens)\nE                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nE                           return func(*args, **kwargs)\nE                                  ^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_model_runner.py\", line 1880, in _dummy_run\nE                           outputs = model(\nE                                     ^^^^^^\nE                         File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\nE                           return self._call_impl(*args, **kwargs)\nE                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\nE                           return forward_call(*args, **kwargs)\nE                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/model_executor/models/granitemoe.py\", line 405, in forward\nE                           hidden_states = self.model(input_ids, positions, intermediate_tensors,\nE                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/compilation/decorators.py\", line 173, in __call__\nE                           return self.forward(*args, **kwargs)\nE                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/model_executor/models/granitemoe.py\", line 301, in forward\nE                           hidden_states = layer(positions, hidden_states)\nE                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\nE                           return self._call_impl(*args, **kwargs)\nE                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\nE                           return forward_call(*args, **kwargs)\nE                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/model_executor/models/granitemoe.py\", line 239, in forward\nE                           hidden_states = self.block_sparse_moe(hidden_states)\nE                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\nE                           return self._call_impl(*args, **kwargs)\nE                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\nE                           return forward_call(*args, **kwargs)\nE                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/model_executor/models/granitemoe.py\", line 101, in forward\nE                           final_hidden_states = self.experts(hidden_states, router_logits)\nE                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\nE                           return self._call_impl(*args, **kwargs)\nE                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\nE                           return forward_call(*args, **kwargs)\nE                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/layer.py\", line 1359, in forward\nE                           return torch.ops.vllm.moe_forward(hidden_states, router_logits,\nE                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/_ops.py\", line 1158, in __call__\nE                           return self._op(*args, **(kwargs or {}))\nE                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/layer.py\", line 1522, in moe_forward\nE                           return self.forward_impl(hidden_states, router_logits)\nE                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/layer.py\", line 1449, in forward_impl\nE                           final_hidden_states = self.quant_method.apply(\nE                                                 ^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/layer.py\", line 568, in apply\nE                           return self.forward(\nE                                  ^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/model_executor/custom_op.py\", line 24, in forward\nE                           return self._forward_method(*args, **kwargs)\nE                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/layer.py\", line 628, in forward_cuda\nE                           return self.fused_experts(\nE                                  ^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1186, in fused_experts\nE                           return dispatch_fused_experts_func(inplace)(\nE                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1125, in torch_vllm_inplace_fused_experts\nE                           torch.ops.vllm.inplace_fused_experts(**kwargs)\nE                         File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/_ops.py\", line 1158, in __call__\nE                           return self._op(*args, **(kwargs or {}))\nE                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE                         File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1018, in inplace_fused_experts\nE                           fused_experts_impl(hidden_states, w1, w2, topk_weights, topk_ids, True,\nE                         File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1295, in fused_experts_impl\nE                           compute_type = tl.bfloat16\nE                                          ^^^^^^^^^^^\nE                       AttributeError: module 'triton.language' has no attribute 'bfloat16'\n\n../../../venvs/vllm/lib/python3.12/site-packages/ray/_private/worker.py:921: ActorDiedError\n============================================================================== warnings summary ==============================================================================\n../../../venvs/vllm/lib/python3.12/site-packages/schemathesis/generation/coverage.py:305\n  /home/mgoin/venvs/vllm/lib/python3.12/site-packages/schemathesis/generation/coverage.py:305: DeprecationWarning: jsonschema.exceptions.RefResolutionError is deprecated as of version 4.18.0. If you wish to catch potential reference resolution errors, directly catch referencing.exceptions.Unresolvable.\n    ref_error: type[Exception] = jsonschema.RefResolutionError,\n\ntests/v1/test_async_llm_dp.py::test_load[ray-RequestOutputKind.DELTA]\n  /home/mgoin/.local/share/uv/python/cpython-3.12.4-linux-x86_64-gnu/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=3891956) is multi-threaded, use of fork() may lead to deadlocks in the child.\n    self.pid = os.fork()\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n========================================================================== short test summary info ===========================================================================\nFAILED v1/test_async_llm_dp.py::test_load[ray-RequestOutputKind.DELTA] - ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, ray::DPEngineCoreActor.__init__() (pid=3893316, ip=216.81.245.69, actor_id=fa979d0e0af9f039b9196c4b01000000, repr=<vllm.v1.engine.core.DPEngineCoreActor object at 0x78f88f5a64e0>)\n======================================================================= 1 failed, 2 warnings in 45.37s =======================================================================\n(DPEngineCoreActor pid=3893316) [rank0]:[W617 07:33:01.422487841 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n(DPEngineCoreActor pid=3893317) INFO 06-17 07:32:59 [default_loader.py:272] Loading weights took 16.77 seconds\n(DPEngineCoreActor pid=3893317) INFO 06-17 07:33:00 [gpu_model_runner.py:1656] Model loading took 3.3375 GiB and 17.309669 seconds\n(DPEngineCoreActor pid=3893317) WARNING 06-17 07:33:00 [fused_moe.py:683] Using default MoE config. Performance might be sub-optimal! Config file not found at /home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/configs/E=40,N=256,device_name=NVIDIA_H100_80GB_HBM3.json\n(DPEngineCoreActor pid=3893317) Exception raised in creation task: The actor died because of an error raised in its creation task, ray::DPEngineCoreActor.__init__() (pid=3893317, ip=216.81.245.69, actor_id=298429e99af9b3dd80f746b401000000, repr=<vllm.v1.engine.core.DPEngineCoreActor object at 0x717cd778df70>)\n(DPEngineCoreActor pid=3893317)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\n(DPEngineCoreActor pid=3893317)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [repeated 3x across cluster]\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 83, in __init__ [repeated 4x across cluster]\n(DPEngineCoreActor pid=3893317)     super().__init__(vllm_config, on_head_node, \"\", executor_class,\n(DPEngineCoreActor pid=3893317)     super().__init__(vllm_config, on_head_node, handshake_address,\n(DPEngineCoreActor pid=3893317)     super().__init__(vllm_config, executor_class, log_stats,\n(DPEngineCoreActor pid=3893317)     self._initialize_kv_caches(vllm_config)\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 143, in _initialize_kv_caches\n(DPEngineCoreActor pid=3893317)     available_gpu_memory = self.model_executor.determine_available_memory()\n(DPEngineCoreActor pid=3893317)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_worker.py\", line 210, in determine_available_memory [repeated 2x across cluster]\n(DPEngineCoreActor pid=3893317)     output = self.collective_rpc(\"determine_available_memory\")\n(DPEngineCoreActor pid=3893317)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n(DPEngineCoreActor pid=3893317)     answer = run_method(self.driver_worker, method, args, kwargs)\n(DPEngineCoreActor pid=3893317)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/utils.py\", line 2690, in run_method\n(DPEngineCoreActor pid=3893317)     return func(*args, **kwargs) [repeated 3x across cluster]\n(DPEngineCoreActor pid=3893317)            ^^^^^^^^^^^^^^^^^^^^^ [repeated 3x across cluster]\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context [repeated 2x across cluster]\n(DPEngineCoreActor pid=3893317)     self.model_runner.profile_run()\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_model_runner.py\", line 2045, in profile_run\n(DPEngineCoreActor pid=3893317)     hidden_states = self._dummy_run(self.max_num_tokens)\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_model_runner.py\", line 1880, in _dummy_run\n(DPEngineCoreActor pid=3893317)     outputs = model(\n(DPEngineCoreActor pid=3893317)               ^^^^^^\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl [repeated 4x across cluster]\n(DPEngineCoreActor pid=3893317)     return self._call_impl(*args, **kwargs) [repeated 4x across cluster]\n(DPEngineCoreActor pid=3893317)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [repeated 4x across cluster]\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl [repeated 4x across cluster]\n(DPEngineCoreActor pid=3893317)     return forward_call(*args, **kwargs) [repeated 4x across cluster]\n(DPEngineCoreActor pid=3893317)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [repeated 5x across cluster]\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/model_executor/models/granitemoe.py\", line 101, in forward [repeated 4x across cluster]\n(DPEngineCoreActor pid=3893317)     hidden_states = self.model(input_ids, positions, intermediate_tensors,\n(DPEngineCoreActor pid=3893317)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/compilation/decorators.py\", line 173, in __call__\n(DPEngineCoreActor pid=3893317)     return self.forward(*args, **kwargs)\n(DPEngineCoreActor pid=3893317)     hidden_states = layer(positions, hidden_states)\n(DPEngineCoreActor pid=3893317)     hidden_states = self.block_sparse_moe(hidden_states)\n(DPEngineCoreActor pid=3893317)     final_hidden_states = self.experts(hidden_states, router_logits)\n(DPEngineCoreActor pid=3893317)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/layer.py\", line 1359, in forward\n(DPEngineCoreActor pid=3893317)     return torch.ops.vllm.moe_forward(hidden_states, router_logits,\n(DPEngineCoreActor pid=3893317)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/_ops.py\", line 1158, in __call__ [repeated 2x across cluster]\n(DPEngineCoreActor pid=3893317)     return self._op(*args, **(kwargs or {})) [repeated 2x across cluster]\n(DPEngineCoreActor pid=3893317)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [repeated 2x across cluster]\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/layer.py\", line 1522, in moe_forward\n(DPEngineCoreActor pid=3893317)     return self.forward_impl(hidden_states, router_logits)\n(DPEngineCoreActor pid=3893317)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/layer.py\", line 1449, in forward_impl\n(DPEngineCoreActor pid=3893317)     final_hidden_states = self.quant_method.apply(\n(DPEngineCoreActor pid=3893317)                           ^^^^^^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/layer.py\", line 568, in apply\n(DPEngineCoreActor pid=3893317)     return self.forward(\n(DPEngineCoreActor pid=3893317)            ^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/model_executor/custom_op.py\", line 24, in forward\n(DPEngineCoreActor pid=3893317)     return self._forward_method(*args, **kwargs)\n(DPEngineCoreActor pid=3893317)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [repeated 2x across cluster]\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/layer.py\", line 628, in forward_cuda\n(DPEngineCoreActor pid=3893317)     return self.fused_experts(\n(DPEngineCoreActor pid=3893317)            ^^^^^^^^^^^^^^^^^^^\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1186, in fused_experts\n(DPEngineCoreActor pid=3893317)     return dispatch_fused_experts_func(inplace)(\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1125, in torch_vllm_inplace_fused_experts\n(DPEngineCoreActor pid=3893317)     torch.ops.vllm.inplace_fused_experts(**kwargs)\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1018, in inplace_fused_experts\n(DPEngineCoreActor pid=3893317)     fused_experts_impl(hidden_states, w1, w2, topk_weights, topk_ids, True,\n(DPEngineCoreActor pid=3893317)   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1295, in fused_experts_impl\n(DPEngineCoreActor pid=3893317)     compute_type = tl.bfloat16\n(DPEngineCoreActor pid=3893317)                    ^^^^^^^^^^^\n(DPEngineCoreActor pid=3893317) AttributeError: module 'triton.language' has no attribute 'bfloat16'\n(DPEngineCoreActor pid=3893317) [rank1]:[W617 07:33:02.549104798 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n```\n\n### \ud83d\udcdd History of failing test\n\nIt looks like the test started failing on June 15th https://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests/88965c05-73cc-8bc4-bca5-902200fc81b7?period=28days&tags=scm.branch%3Amain\n\n<img width=\"1285\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/dd6ca0a4-119d-4c85-b3ae-068dd48c351f\" />\n\n### CC List.\n\n@njhill @ruisearch42 ",
    "labels": [
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-06-17T07:53:51+00:00",
    "closed_at": "2025-06-17T20:59:30+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19731/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/19731"
  },
  {
    "number": 20148,
    "title": "[CI Failure]: Plugin Tests (2 GPUs) - models/test_oot_registration.py",
    "body": "### Name of failing test\n\n`models/test_oot_registration.py::test_oot_registration_embedding`\n\n### Basic information\n\n- [ ] Flaky test\n- [x] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\nThe `models/test_oot_registration.py::test_oot_registration_embedding` test seems to be failing in CI consistently with a context length OOM\n\nhttps://buildkite.com/vllm/ci/builds/22737/steps/canvas?sid=0197acae-970a-43ee-9fef-108d8a58da0c#0197acae-98db-423d-8af9-eb4eb401f1b4/212-1320\n\n```\n[2025-06-26T16:27:15Z] ERROR 06-26 09:27:15 [core.py:519] ValueError: To serve at least one request with the models's max seq len (8192), (2.63 GiB KV cache is needed, which is larger than the available KV cache memory (1.64 GiB). Based on the available memory, the estimated maximum model length is 5088. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.\n```\n\n### \ud83d\udcdd History of failing test\n\nNot sure, maybe related to FP32 weights? I have a prospective fix https://github.com/vllm-project/vllm/pull/20144\n\n### CC List.\n\n_No response_",
    "labels": [
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-06-26T20:12:49+00:00",
    "closed_at": "2025-06-27T03:21:05+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20148/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/20148"
  },
  {
    "number": 18708,
    "title": "[Bug][CI Failure] - VI Test - test_engine_core_client.py::test_kv_cache_events[True-tcp]",
    "body": "### Your current environment\n\nFlakey test for at least the past month: https://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests/4abfbf0d-3a86-8a68-9ff3-0e0ab0fbb38b?period=28days&tags=scm.branch%3Amain%2Cresult%3Afailed\n\n### \ud83d\udc1b Describe the bug\n\nFailing tests:\n\n```\nFAILED v1/engine/test_engine_core_client.py::test_kv_cache_events[True-tcp] - AssertionError: No message received\nassert None is not None\n```\n\n<details>\n<summary>Logs:</summary>\n\n```\n=================================== FAILURES ===================================\n________________________ test_kv_cache_events[True-tcp] ________________________\n\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fc027da70e0>\nmultiprocessing_mode = True\npublisher_config = KVEventsConfig(enable_kv_cache_events=True, publisher='zmq', endpoint='tcp://*:51905', replay_endpoint='tcp://*:51906', buffer_steps=100, hwm=1000, max_queue_size=100000, topic='test')\n\n    @pytest.mark.parametrize(\n        \"multiprocessing_mode,publisher_config\",\n        [(True, \"tcp\"), (False, \"inproc\")],\n        indirect=[\"publisher_config\"],\n    )\n    def test_kv_cache_events(\n        monkeypatch: pytest.MonkeyPatch,\n        multiprocessing_mode: bool,\n        publisher_config,\n    ):\n    \n        with monkeypatch.context() as m:\n            m.setenv(\"VLLM_USE_V1\", \"1\")\n            block_size = 16\n            num_blocks = 2\n    \n            engine_args = EngineArgs(model=MODEL_NAME,\n                                     enforce_eager=True,\n                                     enable_prefix_caching=True,\n                                     block_size=block_size)\n            engine_args.kv_events_config = publisher_config\n    \n            vllm_config = engine_args.create_engine_config(\n                UsageContext.UNKNOWN_CONTEXT)\n    \n            executor_class = Executor.get_class(vllm_config)\n            client = EngineCoreClient.make_client(\n                multiprocess_mode=multiprocessing_mode,\n                asyncio_mode=False,\n                vllm_config=vllm_config,\n                executor_class=executor_class,\n                log_stats=False,\n            )\n            endpoint = publisher_config.endpoint.replace(\"*\", \"127.0.0.1\")\n            subscriber = MockSubscriber(endpoint,\n                                        topic=publisher_config.topic,\n                                        decode_type=KVEventBatch)\n    \n            try:\n                custom_tokens = list(range(num_blocks * block_size))\n                request = EngineCoreRequest(\n                    request_id=str(uuid.uuid4()),\n                    prompt_token_ids=custom_tokens,\n                    mm_inputs=None,\n                    mm_hashes=None,\n                    mm_placeholders=None,\n                    sampling_params=SamplingParams(\n                        max_tokens=1),  # Short completion for speed\n                    eos_token_id=None,\n                    arrival_time=time.time(),\n                    lora_request=None,\n                    cache_salt=None,\n                )\n                client.add_request(request)\n    \n                outputs: dict[str, list] = {request.request_id: []}\n                loop_until_done(client, outputs)\n    \n                result = subscriber.receive_one(timeout=1000)\n>               assert result is not None, \"No message received\"\nE               AssertionError: No message received\nE               assert None is not None\n\nv1/engine/test_engine_core_client.py:318: AssertionError\n=============================== warnings summary ===============================\n../../usr/local/lib/python3.12/dist-packages/schemathesis/generation/coverage.py:305\n  /usr/local/lib/python3.12/dist-packages/schemathesis/generation/coverage.py:305: DeprecationWarning: jsonschema.exceptions.RefResolutionError is deprecated as of version 4.18.0. If you wish to catch potential reference resolution errors, directly catch referencing.exceptions.Unresolvable.\n    ref_error: type[Exception] = jsonschema.RefResolutionError,\n\ntests/v1/engine/test_async_llm.py: 13 warnings\ntests/v1/engine/test_engine_core_client.py: 2 warnings\n  /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=277) is multi-threaded, use of fork() may lead to deadlocks in the child.\n    self.pid = os.fork()\n\ntests/v1/engine/test_engine_core.py::test_engine_core\ntests/v1/engine/test_engine_core.py::test_engine_core_advanced_sampling\ntests/v1/engine/test_engine_core.py::test_engine_core_concurrent_batches\ntests/v1/engine/test_engine_core_client.py::test_engine_core_client[True]\ntests/v1/engine/test_engine_core_client.py::test_engine_core_client[False]\n  /vllm-workspace/tests/utils.py:723: DeprecationWarning: This process (pid=277) is multi-threaded, use of fork() may lead to deadlocks in the child.\n    pid = os.fork()\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ============================\nFAILED v1/engine/test_engine_core_client.py::test_kv_cache_events[True-tcp] - AssertionError: No message received\nassert None is not None\n============ 1 failed, 49 passed, 21 warnings in 706.77s (0:11:46) =============\n^^^ +++\n\ud83d\udea8 Error: The command exited with status 1\n^^^ +++\nuser command error: The plugin docker command hook exited with status 1\n~~~ Running global pre-exit hook\n$ /etc/buildkite-agent/hooks/pre-exit\n~~~ Running plugin docker pre-exit hook\n$ /var/lib/buildkite-agent/plugins/bk-gpu-1-queue-ci-i-0aabd234c4d03089e-1/github-com-buildkite-plugins-docker-buildkite-plugin-v5-2-0/hooks/pre-exit\n```\n\n</details>\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-05-26T11:38:43+00:00",
    "closed_at": "2025-06-04T12:57:32+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18708/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18708"
  },
  {
    "number": 20138,
    "title": "[Bug]: Distributed Tests (4 GPUs) failing in main branch CI",
    "body": "This is now consistently failing with CUDA OOM: https://buildkite.com/vllm/ci/builds/22221#01977f3a-71ea-41cb-bbeb-a43340a10124\n\nI narrowed this down to https://github.com/vllm-project/vllm/pull/19572 which appears to have introduced the issue.\n\n",
    "labels": [
      "bug",
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-06-26T16:15:15+00:00",
    "closed_at": "2025-06-28T05:50:01+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20138/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/20138"
  },
  {
    "number": 19458,
    "title": "[CI Failure]: Quantized Models Test - models/quantization/test_gguf.py::test_models[1-5-32-half-model0]",
    "body": "### Name of failing test\n\n`models/quantization/test_gguf.py::test_models[1-5-32-half-model0]`\n\n### Basic information\n\n- [x] Flaky test\n- [ ] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\nThis specific Llama 1B GGUF model test has been failing consistently in multiple PRs https://buildkite.com/vllm/ci/builds/21800/steps/waterfall?jid=01975af4-f581-4d43-a1e5-7175d960b2b7#01975af4-f581-4d43-a1e5-7175d960b2b7/212-6971\n\n```\n\n[2025-06-10T18:40:56Z] FAILED models/quantization/test_gguf.py::test_models[1-5-32-half-model0] - AssertionError: Test0:\n[2025-06-10T18:40:56Z] Matched tokens:\t[4897, 596, 4495, 13, 650, 4178, 44, 13656, 369]\n[2025-06-10T18:40:56Z] original:\t\"That's correct. VLLM stands for Vision and Language Model, which is a type of large language model designed for both inference and serving. It's a\"\t{31541: Logprob(logprob=-1.6094070672988892, rank=1, decoded_token='\u0120Vision'), 28968: Logprob(logprob=-2.0000319480895996, rank=2, decoded_token='\u0120Vari'), 8519: Logprob(logprob=-2.5000319480895996, rank=3, decoded_token='\u0120Video'), 21382: Logprob(logprob=-2.6562819480895996, rank=4, decoded_token='\u0120Virtual'), 20796: Logprob(logprob=-2.7187819480895996, rank=5, decoded_token='\u0120Visual')}\n[2025-06-10T18:40:56Z] gguf:\t\"That's correct. VLLM stands for Virtual Language Learning Model, which is a type of large language model designed for high-throughput and memory-efficient inference and\"\t{21382: Logprob(logprob=-1.9463169574737549, rank=1, decoded_token='\u0120Virtual'), 330: Logprob(logprob=-2.274441957473755, rank=2, decoded_token='\u0120\"'), 15668: Logprob(logprob=-2.383816957473755, rank=3, decoded_token='\u0120Very'), 4196: Logprob(logprob=-2.446316957473755, rank=4, decoded_token='\u0120Val'), 28968: Logprob(logprob=-2.540066957473755, rank=5, decoded_token='\u0120Vari')}\n\n```\n\n### \ud83d\udcdd History of failing test\n\nEarliest failure I found was at Mon 26th May at 8:27 AM\n[CI/Build] Split pooling and generation extended language models tests in CI (#18705)\nhttps://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests/94a54396-ec5f-8d47-8b48-6c88a2d4e5cb?period=28days&tags=scm.branch%3Amain&execution_id=01970c90-0b2c-7f2b-b3ad-d7bcc06f340b\n\n### CC List.\n\n_No response_",
    "labels": [
      "ci-failure"
    ],
    "state": "open",
    "created_at": "2025-06-11T01:22:45+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19458/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/19458"
  },
  {
    "number": 20842,
    "title": "[CI Failure]: Async Engine, Inputs, Utils, Worker Test: 'State' object has no attribute 'enable_server_load_tracking'",
    "body": "### Name of failing test\n\nAsync Engine, Inputs, Utils, Worker Test\n\n### Basic information\n\n- [ ] Flaky test\n- [ ] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\n```bash\n[2025-07-11T20:13:34Z] INFO 07-11 13:13:34 [async_llm_engine.py:222] Aborted request 85bac0a6a206462aadb2f9d86b92b5f6.\n--\n\u00a0 | [2025-07-11T20:13:34Z] Task exception was never retrieved\n\u00a0 | [2025-07-11T20:13:34Z] future: <Task finished name='Task-456' coro=<listen_for_disconnect() done, defined at /usr/local/lib/python3.12/dist-packages/vllm/entrypoints/utils.py:31> exception=AttributeError(\"'State' object has no attribute 'enable_server_load_tracking'\")>\n\u00a0 | [2025-07-11T20:13:34Z] Traceback (most recent call last):\n\u00a0 | [2025-07-11T20:13:34Z]   File \"/usr/local/lib/python3.12/dist-packages/starlette/datastructures.py\", line 668, in __getattr__\n\u00a0 | [2025-07-11T20:13:34Z]     return self._state[key]\n\u00a0 | [2025-07-11T20:13:34Z]            ~~~~~~~~~~~^^^^^\n\u00a0 | [2025-07-11T20:13:34Z] KeyError: 'enable_server_load_tracking'\n\u00a0 | [2025-07-11T20:13:34Z]\n\u00a0 | [2025-07-11T20:13:34Z] During handling of the above exception, another exception occurred:\n\u00a0 | [2025-07-11T20:13:34Z]\n\u00a0 | [2025-07-11T20:13:34Z] Traceback (most recent call last):\n\u00a0 | [2025-07-11T20:13:34Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/utils.py\", line 36, in listen_for_disconnect\n\u00a0 | [2025-07-11T20:13:34Z]     if request.app.state.enable_server_load_tracking:\n\u00a0 | [2025-07-11T20:13:34Z]        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u00a0 | [2025-07-11T20:13:34Z]   File \"/usr/local/lib/python3.12/dist-packages/starlette/datastructures.py\", line 671, in __getattr__\n\u00a0 | [2025-07-11T20:13:34Z]     raise AttributeError(message.format(self.__class__.__name__, key))\n\u00a0 | [2025-07-11T20:13:34Z] AttributeError: 'State' object has no attribute 'enable_server_load_tracking'\n\n```\n\n\n### \ud83d\udcdd History of failing test\n\nhttps://buildkite.com/vllm/ci/builds/23748#0197fac5-0ad4-4916-92d7-8b8646ffb0f5\nhttps://buildkite.com/vllm/ci/builds/23754#0197fb0f-b892-4dd8-bd12-81c40dc8ac79\n\n### CC List.\n\n_No response_",
    "labels": [
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-07-11T20:38:10+00:00",
    "closed_at": "2025-07-12T01:57:25+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20842/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20842"
  },
  {
    "number": 20214,
    "title": "[CI Failure]: Speculative decoding tests - spec_decode/e2e/test_eagle_correctness.py",
    "body": "### Name of failing test\n\n`spec_decode/e2e/test_eagle_correctness.py::test_llama3_eagle_e2e_greedy_correctness[1-1-32-test_llm_kwargs0-baseline_llm_kwargs0-per_test_common_llm_kwargs0-common_llm_kwargs0]`\n\n### Basic information\n\n- [ ] Flaky test\n- [ ] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\nIt doesn't fail locally but that might be because the OOM is specific to the L4 we use in CI\n\nhttps://buildkite.com/vllm/ci/builds/22853/steps/canvas?jid=0197b520-e1dc-4ace-bfdc-f483b4dee76f\n```\n[2025-06-28T09:19:58Z] FAILED spec_decode/e2e/test_eagle_correctness.py::test_llama3_eagle_e2e_greedy_correctness[1-1-32-test_llm_kwargs0-baseline_llm_kwargs0-per_test_common_llm_kwargs0-common_llm_kwargs0] - torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 116.00 MiB. GPU 0 has a total capacity of 22.05 GiB of which 112.12 MiB is free. Including non-PyTorch memory, this process has 21.92 GiB memory in use. Of the allocated memory 21.56 GiB is allocated by PyTorch, and 113.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n[2025-06-28T09:19:58Z] FAILED spec_decode/e2e/test_eagle_correctness.py::test_llama3_eagle_e2e_greedy_correctness[1-5-32-test_llm_kwargs0-baseline_llm_kwargs0-per_test_common_llm_kwargs0-common_llm_kwargs0] - torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 116.00 MiB. GPU 0 has a total capacity of 22.05 GiB of which 112.12 MiB is free. Including non-PyTorch memory, this process has 21.92 GiB memory in use. Of the allocated memory 21.56 GiB is allocated by PyTorch, and 113.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u00a0[2025-06-28T09:19:58Z] FAILED spec_decode/e2e/test_eagle_correctness.py::test_qwen2_eagle_e2e_greedy_correctness[1-1-32-test_llm_kwargs0-baseline_llm_kwargs0-per_test_common_llm_kwargs0-common_llm_kwargs0] - torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 862.00 MiB. GPU 0 has a total capacity of 22.05 GiB of which 394.12 MiB is free. Including non-PyTorch memory, this process has 21.64 GiB memory in use. Of the allocated memory 21.27 GiB is allocated by PyTorch, and 119.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n[2025-06-28T09:19:58Z] FAILED spec_decode/e2e/test_eagle_correctness.py::test_qwen2_eagle_e2e_greedy_correctness[1-5-32-test_llm_kwargs0-baseline_llm_kwargs0-per_test_common_llm_kwargs0-common_llm_kwargs0] - torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 862.00 MiB. GPU 0 has a total capacity of 22.05 GiB of which 394.12 MiB is free. Including non-PyTorch memory, this process has 21.64 GiB memory in use. Of the allocated memory 21.27 GiB is allocated by PyTorch, and 119.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n```\n\n### \ud83d\udcdd History of failing test\n\nThese tests seem to have been failing since they were added?\n<img width=\"1380\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/170d8426-a7d3-45b5-9237-40303255228b\" />\nhttps://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests/f5787f7b-48c2-83fa-85e4-b02c88a7fa74?period=28days&tags=scm.branch%3Amain\n\n### CC List.\n\n_No response_",
    "labels": [
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-06-28T16:36:12+00:00",
    "closed_at": "2025-06-29T02:31:39+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20214/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/20214"
  },
  {
    "number": 18418,
    "title": "[Bug][Failing Test] entrypoints-test - test_v1_v2_api_consistency_single_prompt_tokens",
    "body": "### Your current environment\n\nStill failing on main as of commit bca55b556f\n\n### \ud83d\udc1b Describe the bug\n\nFailing tests: https://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests?branch=main&period=2days&query=test_v1_v2_api_consistency_single_prompt_tokens&commit=Search\n\n```\nFAILED entrypoints/llm/test_generate.py::test_v1_v2_api_consistency_single_prompt_tokens[prompt_token_ids0] - vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\nFAILED entrypoints/llm/test_generate.py::test_v1_v2_api_consistency_single_prompt_tokens[prompt_token_ids1] - vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\nFAILED entrypoints/llm/test_generate.py::test_v1_v2_api_consistency_single_prompt_tokens[prompt_token_ids2] - vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\nFAILED entrypoints/llm/test_generate.py::test_v1_v2_api_consistency_single_prompt_tokens[prompt_token_ids3] - vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\nFAILED entrypoints/llm/test_generate.py::test_v1_v2_api_consistency_multi_prompt_tokens - vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\nFAILED entrypoints/llm/test_generate.py::test_multiple_sampling_params - vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n```\n\n<details>\n<summary>Logs</summary>\n\n\n```\nERROR 05-20 03:26:38 [dump_input.py:68] Dumping input data\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 203, in execute_model\n    return self.model_executor.execute_model(scheduler_output)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py\", line 86, in execute_model\n    output = self.collective_rpc(\"execute_model\",\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2598, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 276, in execute_model\n    output = self.model_runner.execute_model(scheduler_output,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1110, in execute_model\n    self._prepare_inputs(scheduler_output))\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 503, in _prepare_inputs\n    self.input_batch.block_table.commit(num_reqs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/block_table.py\", line 137, in commit\n    block_table.commit(num_reqs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/block_table.py\", line 83, in commit\n    self.block_table[:num_reqs].copy_(self.block_table_cpu[:num_reqs],\nRuntimeError: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/logging/__init__.py\", line 1160, in emit\n    msg = self.format(record)\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/logging/__init__.py\", line 999, in format\n    return fmt.format(record)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/logging_utils/formatter.py\", line 13, in format\n    msg = logging.Formatter.format(self, record)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/logging/__init__.py\", line 703, in format\n    record.message = record.getMessage()\n                     ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/logging/__init__.py\", line 392, in getMessage\n    msg = msg % self.args\n          ~~~~^~~~~~~~~~~\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/config.py\", line 4484, in __str__\n    f\"compilation_config={self.compilation_config!r}\")\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/config.py\", line 3868, in __repr__\n    for k, v in asdict(self).items():\n                ^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/dataclasses.py\", line 1329, in asdict\n    return _asdict_inner(obj, dict_factory)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/dataclasses.py\", line 1339, in _asdict_inner\n    f.name: _asdict_inner(getattr(obj, f.name), dict)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/dataclasses.py\", line 1382, in _asdict_inner\n    return type(obj)((_asdict_inner(k, dict_factory),\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/dataclasses.py\", line 1383, in <genexpr>\n    _asdict_inner(v, dict_factory))\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/dataclasses.py\", line 1386, in _asdict_inner\n    return copy.deepcopy(obj)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/copy.py\", line 162, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/copy.py\", line 259, in _reconstruct\n    state = deepcopy(state, memo)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/copy.py\", line 136, in deepcopy\n    y = copier(x, memo)\n        ^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/copy.py\", line 221, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n                             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/copy.py\", line 143, in deepcopy\n    y = copier(memo)\n        ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\", line 172, in __deepcopy__\n    new_storage = self._typed_storage()._deepcopy(memo)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/storage.py\", line 1134, in _deepcopy\n    return self._new_wrapped_storage(copy.deepcopy(self._untyped_storage, memo))\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/copy.py\", line 143, in deepcopy\n    y = copier(memo)\n        ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/storage.py\", line 239, in __deepcopy__\n    new_storage = self.clone()\n                  ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/storage.py\", line 253, in clone\n    return type(self)(self.nbytes(), device=self.device).copy_(self)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nCall stack:\n  File \"<string>\", line 1, in <module>\n  File \"/usr/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/usr/lib/python3.12/multiprocessing/spawn.py\", line 135, in _main\n    return self._bootstrap(parent_sentinel)\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 482, in run_engine_core\n    engine_core.run_busy_loop()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 509, in run_busy_loop\n    self._process_engine_step()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 534, in _process_engine_step\n    outputs = self.step_fn()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 222, in step\n    model_output = self.execute_model(scheduler_output)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 206, in execute_model\n    dump_engine_exception(self.vllm_config, scheduler_output,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/logging_utils/dump_input.py\", line 62, in dump_engine_exception\n    _dump_engine_exception(config, scheduler_output, scheduler_stats)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/logging_utils/dump_input.py\", line 70, in _dump_engine_exception\n    logger.error(\nUnable to print the message and arguments - possible formatting error.\nUse the traceback above to help find the error.\nERROR 05-20 03:26:38 [dump_input.py:78] Dumping scheduler output for model execution:\nERROR 05-20 03:26:38 [dump_input.py:79] SchedulerOutput(scheduled_new_reqs=[NewRequestData(req_id=0,prompt_token_ids_len=1,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=[[1]],num_computed_tokens=0,lora_request=None)],scheduled_cached_reqs=[],num_scheduled_tokens={0: 1},total_num_scheduled_tokens=1,scheduled_spec_decode_tokens={},scheduled_encoder_inputs={},num_common_prefix_blocks=[1],finished_req_ids=[],free_encoder_input_ids=[],structured_output_request_ids={},grammar_bitmask=null,kv_connector_metadata=null)\nERROR 05-20 03:26:38 [core.py:491] EngineCore encountered a fatal error.\n\nERROR 05-20 03:26:38 [core.py:491] Traceback (most recent call last):\n\nERROR 05-20 03:26:38 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 482, in run_engine_core\n\nERROR 05-20 03:26:38 [core.py:491]     engine_core.run_busy_loop()\n\nERROR 05-20 03:26:38 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 509, in run_busy_loop\n\nERROR 05-20 03:26:38 [core.py:491]     self._process_engine_step()\n\nERROR 05-20 03:26:38 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 534, in _process_engine_step\n\nERROR 05-20 03:26:38 [core.py:491]     outputs = self.step_fn()\n\nERROR 05-20 03:26:38 [core.py:491]               ^^^^^^^^^^^^^^\n\nERROR 05-20 03:26:38 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 222, in step\n\nERROR 05-20 03:26:38 [core.py:491]     model_output = self.execute_model(scheduler_output)\n\nERROR 05-20 03:26:38 [core.py:491]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nERROR 05-20 03:26:38 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 209, in execute_model\n\nERROR 05-20 03:26:38 [core.py:491]     raise err\n\nERROR 05-20 03:26:38 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 203, in execute_model\n\nERROR 05-20 03:26:38 [core.py:491]     return self.model_executor.execute_model(scheduler_output)\n\nERROR 05-20 03:26:38 [core.py:491]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nERROR 05-20 03:26:38 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py\", line 86, in execute_model\n\nERROR 05-20 03:26:38 [core.py:491]     output = self.collective_rpc(\"execute_model\",\n\nERROR 05-20 03:26:38 [core.py:491]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nERROR 05-20 03:26:38 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n\nERROR 05-20 03:26:38 [core.py:491]     answer = run_method(self.driver_worker, method, args, kwargs)\n\nERROR 05-20 03:26:38 [core.py:491]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nERROR 05-20 03:26:38 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2598, in run_method\n\nERROR 05-20 03:26:38 [core.py:491]     return func(*args, **kwargs)\n\nERROR 05-20 03:26:38 [core.py:491]            ^^^^^^^^^^^^^^^^^^^^^\n\nERROR 05-20 03:26:38 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n\nERROR 05-20 03:26:38 [core.py:491]     return func(*args, **kwargs)\n\nERROR 05-20 03:26:38 [core.py:491]            ^^^^^^^^^^^^^^^^^^^^^\n\nERROR 05-20 03:26:38 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 276, in execute_model\n\nERROR 05-20 03:26:38 [core.py:491]     output = self.model_runner.execute_model(scheduler_output,\n\nERROR 05-20 03:26:38 [core.py:491]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nERROR 05-20 03:26:38 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n\nERROR 05-20 03:26:38 [core.py:491]     return func(*args, **kwargs)\n\nERROR 05-20 03:26:38 [core.py:491]            ^^^^^^^^^^^^^^^^^^^^^\n\nERROR 05-20 03:26:38 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1110, in execute_model\n\nERROR 05-20 03:26:38 [core.py:491]     self._prepare_inputs(scheduler_output))\n\nERROR 05-20 03:26:38 [core.py:491]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nERROR 05-20 03:26:38 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 503, in _prepare_inputs\n\nERROR 05-20 03:26:38 [core.py:491]     self.input_batch.block_table.commit(num_reqs)\n\nERROR 05-20 03:26:38 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/block_table.py\", line 137, in commit\n\nERROR 05-20 03:26:38 [core.py:491]     block_table.commit(num_reqs)\n\nERROR 05-20 03:26:38 [core.py:491]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/block_table.py\", line 83, in commit\n\nERROR 05-20 03:26:38 [core.py:491]     self.block_table[:num_reqs].copy_(self.block_table_cpu[:num_reqs],\n\nERROR 05-20 03:26:38 [core.py:491] RuntimeError: CUDA error: an illegal memory access was encountered\n\nERROR 05-20 03:26:38 [core.py:491] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n\nERROR 05-20 03:26:38 [core.py:491] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n\nERROR 05-20 03:26:38 [core.py:491] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nERROR 05-20 03:26:38 [core.py:491] \nProcess EngineCore_0:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 493, in run_engine_core\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 482, in run_engine_core\n    engine_core.run_busy_loop()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 509, in run_busy_loop\n    self._process_engine_step()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 534, in _process_engine_step\n    outputs = self.step_fn()\n              ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 222, in step\n    model_output = self.execute_model(scheduler_output)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 209, in execute_model\n    raise err\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 203, in execute_model\n    return self.model_executor.execute_model(scheduler_output)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py\", line 86, in execute_model\n    output = self.collective_rpc(\"execute_model\",\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2598, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 276, in execute_model\n    output = self.model_runner.execute_model(scheduler_output,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1110, in execute_model\n    self._prepare_inputs(scheduler_output))\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 503, in _prepare_inputs\n    self.input_batch.block_table.commit(num_reqs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/block_table.py\", line 137, in commit\n    block_table.commit(num_reqs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/block_table.py\", line 83, in commit\n    self.block_table[:num_reqs].copy_(self.block_table_cpu[:num_reqs],\nRuntimeError: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nFAILED\nentrypoints/llm/test_generate.py::test_v1_v2_api_consistency_single_prompt_tokens[prompt_token_ids1] \n\nAdding requests:   0% 0/1 [00:00<?, ?it/s]\u001b[A\nAdding requests:   0% 0/1 [00:00<?, ?it/s]\n\nProcessed prompts:   0% 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nFAILED\nentrypoints/llm/test_generate.py::test_v1_v2_api_consistency_single_prompt_tokens[prompt_token_ids2] \nAdding requests:   0% 0/1 [00:00<?, ?it/s]\nAdding requests:   0% 0/1 [00:00<?, ?it/s]\nFAILED\nentrypoints/llm/test_generate.py::test_v1_v2_api_consistency_single_prompt_tokens[prompt_token_ids3] \nAdding requests:   0% 0/1 [00:00<?, ?it/s]\nAdding requests:   0% 0/1 [00:00<?, ?it/s]\nFAILED\nentrypoints/llm/test_generate.py::test_v1_v2_api_consistency_multi_prompt_tokens \nAdding requests:   0% 0/4 [00:00<?, ?it/s]\nAdding requests:   0% 0/4 [00:00<?, ?it/s]\nFAILED\nentrypoints/llm/test_generate.py::test_multiple_sampling_params \nAdding requests:   0% 0/4 [00:00<?, ?it/s]\nAdding requests:   0% 0/4 [00:00<?, ?it/s]\nFAILED[rank0]:[W520 03:26:39.408248601 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\n\n=================================== FAILURES ===================================\n______ test_v1_v2_api_consistency_single_prompt_tokens[prompt_token_ids0] ______\n\nllm = <weakproxy at 0x7f13f18ea7f0 to LLM at 0x7f13f4043980>\nprompt_token_ids = [0]\n\n    @pytest.mark.skip_global_cleanup\n    @pytest.mark.parametrize('prompt_token_ids', TOKEN_IDS)\n    def test_v1_v2_api_consistency_single_prompt_tokens(llm: LLM,\n                                                        prompt_token_ids):\n        sampling_params = SamplingParams(temperature=0.0, top_p=1.0)\n    \n        with pytest.warns(DeprecationWarning, match=\"'prompt_token_ids'\"):\n>           v1_output = llm.generate(prompt_token_ids=prompt_token_ids,\n                                     sampling_params=sampling_params)\n\nentrypoints/llm/test_generate.py:56: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/vllm/utils.py:1212: in inner\n    return fn(*args, **kwargs)\n/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:479: in generate\n    outputs = self._run_engine(use_tqdm=use_tqdm)\n/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:1464: in _run_engine\n    step_outputs = self.llm_engine.step()\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/llm_engine.py:223: in step\n    outputs = self.engine_core.get_output()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <vllm.v1.engine.core_client.SyncMPClient object at 0x7f13f40599a0>\n\n    def get_output(self) -> EngineCoreOutputs:\n        # If an exception arises in process_outputs_socket task,\n        # it is forwarded to the outputs_queue so we can raise it\n        # from this (run_output_handler) task to shut down the server.\n        outputs = self.outputs_queue.get()\n        if isinstance(outputs, Exception):\n>           raise self._format_exception(outputs) from None\nE           vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:647: EngineDeadError\n______ test_v1_v2_api_consistency_single_prompt_tokens[prompt_token_ids1] ______\n\nllm = <weakproxy at 0x7f13f18ea7f0 to LLM at 0x7f13f4043980>\nprompt_token_ids = [0, 1]\n\n    @pytest.mark.skip_global_cleanup\n    @pytest.mark.parametrize('prompt_token_ids', TOKEN_IDS)\n    def test_v1_v2_api_consistency_single_prompt_tokens(llm: LLM,\n                                                        prompt_token_ids):\n        sampling_params = SamplingParams(temperature=0.0, top_p=1.0)\n    \n        with pytest.warns(DeprecationWarning, match=\"'prompt_token_ids'\"):\n>           v1_output = llm.generate(prompt_token_ids=prompt_token_ids,\n                                     sampling_params=sampling_params)\n\nentrypoints/llm/test_generate.py:56: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/vllm/utils.py:1212: in inner\n    return fn(*args, **kwargs)\n/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:469: in generate\n    self._validate_and_add_requests(\n/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:1392: in _validate_and_add_requests\n    self._add_request(\n/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:1412: in _add_request\n    self.llm_engine.add_request(\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/llm_engine.py:198: in add_request\n    self.engine_core.add_request(request)\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:675: in add_request\n    self._send_input(EngineCoreRequestType.ADD, request)\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:651: in _send_input\n    self.ensure_alive()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <vllm.v1.engine.core_client.SyncMPClient object at 0x7f13f40599a0>\n\n    def ensure_alive(self):\n        if self.resources.engine_dead:\n>           raise EngineDeadError()\nE           vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:554: EngineDeadError\n______ test_v1_v2_api_consistency_single_prompt_tokens[prompt_token_ids2] ______\n\nllm = <weakproxy at 0x7f13f18ea7f0 to LLM at 0x7f13f4043980>\nprompt_token_ids = [0, 2, 1]\n\n    @pytest.mark.skip_global_cleanup\n    @pytest.mark.parametrize('prompt_token_ids', TOKEN_IDS)\n    def test_v1_v2_api_consistency_single_prompt_tokens(llm: LLM,\n                                                        prompt_token_ids):\n        sampling_params = SamplingParams(temperature=0.0, top_p=1.0)\n    \n        with pytest.warns(DeprecationWarning, match=\"'prompt_token_ids'\"):\n>           v1_output = llm.generate(prompt_token_ids=prompt_token_ids,\n                                     sampling_params=sampling_params)\n\nentrypoints/llm/test_generate.py:56: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/vllm/utils.py:1212: in inner\n    return fn(*args, **kwargs)\n/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:469: in generate\n    self._validate_and_add_requests(\n/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:1392: in _validate_and_add_requests\n    self._add_request(\n/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:1412: in _add_request\n    self.llm_engine.add_request(\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/llm_engine.py:198: in add_request\n    self.engine_core.add_request(request)\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:675: in add_request\n    self._send_input(EngineCoreRequestType.ADD, request)\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:651: in _send_input\n    self.ensure_alive()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <vllm.v1.engine.core_client.SyncMPClient object at 0x7f13f40599a0>\n\n    def ensure_alive(self):\n        if self.resources.engine_dead:\n>           raise EngineDeadError()\nE           vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:554: EngineDeadError\n______ test_v1_v2_api_consistency_single_prompt_tokens[prompt_token_ids3] ______\n\nllm = <weakproxy at 0x7f13f18ea7f0 to LLM at 0x7f13f4043980>\nprompt_token_ids = [0, 3, 1, 2]\n\n    @pytest.mark.skip_global_cleanup\n    @pytest.mark.parametrize('prompt_token_ids', TOKEN_IDS)\n    def test_v1_v2_api_consistency_single_prompt_tokens(llm: LLM,\n                                                        prompt_token_ids):\n        sampling_params = SamplingParams(temperature=0.0, top_p=1.0)\n    \n        with pytest.warns(DeprecationWarning, match=\"'prompt_token_ids'\"):\n>           v1_output = llm.generate(prompt_token_ids=prompt_token_ids,\n                                     sampling_params=sampling_params)\n\nentrypoints/llm/test_generate.py:56: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/vllm/utils.py:1212: in inner\n    return fn(*args, **kwargs)\n/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:469: in generate\n    self._validate_and_add_requests(\n/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:1392: in _validate_and_add_requests\n    self._add_request(\n/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:1412: in _add_request\n    self.llm_engine.add_request(\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/llm_engine.py:198: in add_request\n    self.engine_core.add_request(request)\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:675: in add_request\n    self._send_input(EngineCoreRequestType.ADD, request)\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:651: in _send_input\n    self.ensure_alive()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <vllm.v1.engine.core_client.SyncMPClient object at 0x7f13f40599a0>\n\n    def ensure_alive(self):\n        if self.resources.engine_dead:\n>           raise EngineDeadError()\nE           vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:554: EngineDeadError\n________________ test_v1_v2_api_consistency_multi_prompt_tokens ________________\n\nllm = <weakproxy at 0x7f13f18ea7f0 to LLM at 0x7f13f4043980>\n\n    @pytest.mark.skip_global_cleanup\n    def test_v1_v2_api_consistency_multi_prompt_tokens(llm: LLM):\n        sampling_params = SamplingParams(temperature=0.0, top_p=1.0)\n    \n        with pytest.warns(DeprecationWarning, match=\"'prompt_token_ids'\"):\n>           v1_output = llm.generate(prompt_token_ids=TOKEN_IDS,\n                                     sampling_params=sampling_params)\n\nentrypoints/llm/test_generate.py:69: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/vllm/utils.py:1212: in inner\n    return fn(*args, **kwargs)\n/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:469: in generate\n    self._validate_and_add_requests(\n/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:1392: in _validate_and_add_requests\n    self._add_request(\n/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:1412: in _add_request\n    self.llm_engine.add_request(\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/llm_engine.py:198: in add_request\n    self.engine_core.add_request(request)\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:675: in add_request\n    self._send_input(EngineCoreRequestType.ADD, request)\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:651: in _send_input\n    self.ensure_alive()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <vllm.v1.engine.core_client.SyncMPClient object at 0x7f13f40599a0>\n\n    def ensure_alive(self):\n        if self.resources.engine_dead:\n>           raise EngineDeadError()\nE           vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:554: EngineDeadError\n________________________ test_multiple_sampling_params _________________________\n\nllm = <weakproxy at 0x7f13f18ea7f0 to LLM at 0x7f13f4043980>\n\n    @pytest.mark.skip_global_cleanup\n    def test_multiple_sampling_params(llm: LLM):\n        sampling_params = [\n            SamplingParams(temperature=0.01, top_p=0.95),\n            SamplingParams(temperature=0.3, top_p=0.95),\n            SamplingParams(temperature=0.7, top_p=0.95),\n            SamplingParams(temperature=0.99, top_p=0.95),\n        ]\n    \n        # Multiple SamplingParams should be matched with each prompt\n>       outputs = llm.generate(PROMPTS, sampling_params=sampling_params)\n\nentrypoints/llm/test_generate.py:91: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/usr/local/lib/python3.12/dist-packages/vllm/utils.py:1212: in inner\n    return fn(*args, **kwargs)\n/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:469: in generate\n    self._validate_and_add_requests(\n/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:1392: in _validate_and_add_requests\n    self._add_request(\n/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:1412: in _add_request\n    self.llm_engine.add_request(\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/llm_engine.py:198: in add_request\n    self.engine_core.add_request(request)\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:675: in add_request\n    self._send_input(EngineCoreRequestType.ADD, request)\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:651: in _send_input\n    self.ensure_alive()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <vllm.v1.engine.core_client.SyncMPClient object at 0x7f13f40599a0>\n\n    def ensure_alive(self):\n        if self.resources.engine_dead:\n>           raise EngineDeadError()\nE           vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n\n/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:554: EngineDeadError\n=============================== warnings summary ===============================\n../../usr/local/lib/python3.12/dist-packages/schemathesis/generation/coverage.py:305\n  /usr/local/lib/python3.12/dist-packages/schemathesis/generation/coverage.py:305: DeprecationWarning: jsonschema.exceptions.RefResolutionError is deprecated as of version 4.18.0. If you wish to catch potential reference resolution errors, directly catch referencing.exceptions.Unresolvable.\n    ref_error: type[Exception] = jsonschema.RefResolutionError,\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ============================\nFAILED entrypoints/llm/test_generate.py::test_v1_v2_api_consistency_single_prompt_tokens[prompt_token_ids0] - vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\nFAILED entrypoints/llm/test_generate.py::test_v1_v2_api_consistency_single_prompt_tokens[prompt_token_ids1] - vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\nFAILED entrypoints/llm/test_generate.py::test_v1_v2_api_consistency_single_prompt_tokens[prompt_token_ids2] - vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\nFAILED entrypoints/llm/test_generate.py::test_v1_v2_api_consistency_single_prompt_tokens[prompt_token_ids3] - vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\nFAILED entrypoints/llm/test_generate.py::test_v1_v2_api_consistency_multi_prompt_tokens - vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\nFAILED entrypoints/llm/test_generate.py::test_multiple_sampling_params - vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.\n======================== 6 failed, 1 warning in 35.48s =========================\n^^^ +++\n\ud83d\udea8 Error: The command exited with status 1\n^^^ +++\nuser command error: The plugin docker command hook exited with status 1\n~~~ Running global pre-exit hook\n$ /etc/buildkite-agent/hooks/pre-exit\n~~~ Running plugin docker pre-exit hook\n$ /var/lib/buildkite-agent/plugins/bk-gpu-1-queue-ci-i-0dcbea0ffe6f32681-1/github-com-buildkite-plugins-docker-buildkite-plugin-v5-2-0/hooks/pre-exit\n```\n\n</details>",
    "labels": [
      "bug",
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-05-20T15:30:23+00:00",
    "closed_at": "2025-05-22T13:48:58+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18418/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18418"
  },
  {
    "number": 18954,
    "title": "[CI Failure]: Spec Decoding - spec_decode/e2e/test_multistep_correctness.py",
    "body": "### Name of failing test\n\n`test_spec_decode_e2e_greedy_correctness_tiny_model_large_bs_diff_output_len`\n\n### Basic information\n\n- [ ] Flaky test\n- [x] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\nhttps://buildkite.com/vllm/ci/builds/21085/steps?jid=01971f59-be20-45f4-9e11-dcd3b1e67173\n\n### \ud83d\udcdd History of failing test\n\nStarted failing since yesterday since today's nightly caught the failure\n\nhttps://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests/a8e2f4a9-a7cf-81ec-ba3e-3a1e8b022a79?period=1day&tags=scm.branch%3Amain\n\n### CC List.\n\n@mgoin @njhill ",
    "labels": [
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-05-30T11:27:47+00:00",
    "closed_at": "2025-06-16T23:43:08+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18954/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18954"
  },
  {
    "number": 18604,
    "title": "[Bug][Flaky]: V1 Test - v1/engine/test_engine_core_client.py",
    "body": "### Your current environment\n\nN/A\n\n### \ud83d\udc1b Describe the bug\n\nThe test `v1/engine/test_engine_core_client.py::test_kv_cache_events[True-tcp]` is flaky.\n\nhttps://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests/4abfbf0d-3a86-8a68-9ff3-0e0ab0fbb38b?period=7days&tags=scm.branch%3Amain\n\ncc @robertgshaw2-redhat @njhill \n\n\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-05-23T10:42:19+00:00",
    "closed_at": "2025-06-04T12:58:08+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18604/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18604"
  },
  {
    "number": 20198,
    "title": "[CI Failure]: Basic Models Test - test_can_initialize[MiniMaxText01ForCausalLM]",
    "body": "### Name of failing test\n\n`models/test_initialization.py::test_can_initialize[MiniMaxText01ForCausalLM]`\n\n### Basic information\n\n- [ ] Flaky test\n- [x] Can reproduce locally\n- [x] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\nIt seems to be related to recent changes to Transformers for minimax\n\nThis is the error I see on `transformers==4.52.4` which matches the CI\n\n```\n[2025-06-27T18:12:39Z] FAILED models/test_initialization.py::test_can_initialize[MiniMaxText01ForCausalLM] - pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n[2025-06-27T18:12:39Z]   Value error, The checkpoint you are trying to load has model type `minimax` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n[2025-06-27T18:12:39Z]\n[2025-06-27T18:12:39Z] You can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git` [type=value_error, input_value=ArgsKwargs((), {'model': ...attention_dtype': None}), input_type=ArgsKwargs]\n[2025-06-27T18:12:39Z]     For further information visit https://errors.pydantic.dev/2.11/v/value_error\n```\n\nThis is the error I see on `transformers==4.53.0`, which seems to be using the Transformers model backend\n\n```\nWARNING 06-27 20:25:53 [utils.py:215] MiniMaxForCausalLM has no vLLM implementation, falling back to Transformers implementation. Some features may not be supported and performance may not be optimal.\n...\nTraceback (most recent call last):\n  File \"/home/mgoin/.local/share/uv/python/cpython-3.12.4-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/home/mgoin/.local/share/uv/python/cpython-3.12.4-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 523, in run_engine_core\n    raise e\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 510, in run_engine_core\n    engine_core = EngineCoreProc(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 394, in __init__\n    super().__init__(vllm_config, executor_class, log_stats,\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 75, in __init__\n    self.model_executor = executor_class(vllm_config)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/executor/executor_base.py\", line 53, in __init__\n    self._init_executor()\n  File \"/home/mgoin/code/vllm/vllm/executor/uniproc_executor.py\", line 48, in _init_executor\n    self.collective_rpc(\"load_model\")\n  File \"/home/mgoin/code/vllm/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/utils.py\", line 2687, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_worker.py\", line 185, in load_model\n    self.model_runner.load_model()\n  File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_model_runner.py\", line 1731, in load_model\n    self.model = model_loader.load_model(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/model_executor/model_loader/base_loader.py\", line 38, in load_model\n    model = initialize_model(vllm_config=vllm_config,\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/model_executor/model_loader/utils.py\", line 64, in initialize_model\n    return model_class(vllm_config=vllm_config, prefix=prefix)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/compilation/decorators.py\", line 152, in __init__\n    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n  File \"/home/mgoin/code/vllm/vllm/model_executor/models/transformers.py\", line 443, in __init__\n    self.model = TransformersModel(vllm_config=vllm_config, prefix=prefix)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/model_executor/models/transformers.py\", line 205, in __init__\n    self.init_buffers(self.model)\n  File \"/home/mgoin/code/vllm/vllm/model_executor/models/transformers.py\", line 353, in init_buffers\n    self.init_buffers(child)\n  File \"/home/mgoin/code/vllm/vllm/model_executor/models/transformers.py\", line 353, in init_buffers\n    self.init_buffers(child)\n  File \"/home/mgoin/code/vllm/vllm/model_executor/models/transformers.py\", line 353, in init_buffers\n    self.init_buffers(child)\n  File \"/home/mgoin/code/vllm/vllm/model_executor/models/transformers.py\", line 350, in init_buffers\n    new_buffer = getattr(type(module)(self.config), name)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: MiniMaxLightningAttention.__init__() missing 1 required positional argument: 'layer_idx'\n```\n\n### \ud83d\udcdd History of failing test\n\nIt looks like it started this morning (6/27)\n\n![Image](https://github.com/user-attachments/assets/8c2d039f-4165-45c6-8cb9-e74add086943)\n\nhttps://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests/e3ae5e46-0be9-8971-ad12-041cf51787b1?period=1day&tags=scm.branch%3Amain&execution_id=0197b12f-4e4b-7992-b6d5-a249c81cd3b2\n\n### CC List.\n\n_No response_",
    "labels": [
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-06-27T20:22:36+00:00",
    "closed_at": "2025-06-28T05:43:08+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20198/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/20198"
  },
  {
    "number": 19736,
    "title": "[CI Failure]: Samplers Test - samplers/test_beam_search.py::test_beam_search_passes_multimodal_data",
    "body": "### Name of failing test\n\n`samplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half]`\n\n### Basic information\n\n- [ ] Flaky test\n- [x] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\nIt seems the issue is because we are now passing empty lists to _flatten_embeddings\n\n```\nFAILED samplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half] - RuntimeError: torch.cat(): expected a non-empty list of Tensors\n```\n\nFull output:\n```\npytest -s -v \"samplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half]\"\nINFO 06-17 09:19:56 [__init__.py:244] Automatically detected platform cuda.\n/home/mgoin/venvs/vllm/lib/python3.12/site-packages/pytest_asyncio/plugin.py:208: PytestDeprecationWarning: The configuration option \"asyncio_default_fixture_loop_scope\" is unset.\nThe event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))\n============================================================================================ test session starts =============================================================================================\nplatform linux -- Python 3.12.4, pytest-8.3.3, pluggy-1.5.0 -- /home/mgoin/venvs/vllm/bin/python3\ncachedir: .pytest_cache\nhypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/home/mgoin/code/vllm/tests/.hypothesis/examples'))\nrootdir: /home/mgoin/code/vllm\nconfigfile: pyproject.toml\nplugins: forked-1.6.0, subtests-0.14.1, asyncio-0.24.0, shard-0.1.2, buildkite-test-collector-0.1.9, timeout-2.3.1, schemathesis-3.39.15, anyio-4.6.2.post1, mock-3.14.0, hypothesis-6.131.0, rerunfailures-14.0\nasyncio: mode=Mode.STRICT, default_loop_scope=None\ncollected 1 item                                                                                                                                                                                             \nRunning 1 items in this shard: tests/samplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half]\n\nsamplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half] WARNING 06-17 09:19:58 [config.py:3273] Casting torch.bfloat16 to torch.float16.\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  4.90it/s]\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nINFO 06-17 09:20:16 [config.py:831] This model supports multiple tasks: {'generate', 'score', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.\nWARNING 06-17 09:20:16 [config.py:3273] Casting torch.bfloat16 to torch.float16.\nINFO 06-17 09:20:16 [config.py:1444] Using max model len 1024\nINFO 06-17 09:20:16 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.1.dev287+g89b1388d8) with config: model='Qwen/Qwen2-Audio-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-Audio-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2-Audio-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \nINFO 06-17 09:20:18 [cuda.py:336] Using Flash Attention backend.\nINFO 06-17 09:20:18 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nINFO 06-17 09:20:18 [model_runner.py:1171] Starting to load model Qwen/Qwen2-Audio-7B-Instruct...\nINFO 06-17 09:20:19 [weight_utils.py:292] Using model weights format ['*.safetensors']\nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:02,  1.59it/s]\nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:02,  1.49it/s]\nLoading safetensors checkpoint shards:  60% Completed | 3/5 [00:02<00:01,  1.44it/s]\nLoading safetensors checkpoint shards:  80% Completed | 4/5 [00:02<00:00,  1.48it/s]\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:02<00:00,  1.88it/s]\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [00:02<00:00,  1.68it/s]\n\nINFO 06-17 09:20:22 [default_loader.py:272] Loading weights took 3.00 seconds\nINFO 06-17 09:20:22 [model_runner.py:1203] Model loading took 15.6455 GiB and 3.447517 seconds\nINFO 06-17 09:20:25 [worker.py:294] Memory profiling takes 2.68 seconds\nINFO 06-17 09:20:25 [worker.py:294] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.90) = 71.27GiB\nINFO 06-17 09:20:25 [worker.py:294] model weights take 15.65GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.51GiB; the rest of the memory reserved for KV Cache is 55.11GiB.\nINFO 06-17 09:20:25 [executor_base.py:113] # cuda blocks: 7054, # CPU blocks: 512\nINFO 06-17 09:20:25 [executor_base.py:118] Maximum concurrency for 1024 tokens per request: 110.22x\nINFO 06-17 09:20:27 [model_runner.py:1513] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nCapturing CUDA graph shapes:   0%|                                                                                                                     | 0/35 [00:00<?, ?it/s]\nFAILED\n\n================================================================================== FAILURES ==================================================================================\n__________________________________________________________ test_beam_search_passes_multimodal_data[False-2-64-half] __________________________________________________________\n\nhf_runner = <class 'tests.conftest.HfRunner'>, vllm_runner = <class 'tests.conftest.VllmRunner'>, dtype = 'half', max_tokens = 64, beam_width = 2\n\n    @pytest.mark.parametrize(\"dtype\", [\"half\"])\n    @pytest.mark.parametrize(\"max_tokens\", MAX_TOKENS)\n    @pytest.mark.parametrize(\"beam_width\", MM_BEAM_WIDTHS)\n    def test_beam_search_passes_multimodal_data(\n        hf_runner,\n        vllm_runner,\n        dtype: str,\n        max_tokens: int,\n        beam_width: int,\n    ) -> None:\n        \"\"\"Ensure that beam search passes multimodal data through correctly.\"\"\"\n        # NOTE - this test is primarily to check that mm data is passed to beams\n        # correctly. As such, we just need to check one extra modality to make\n        # sure things pass through properly.\n        audios = [AudioAsset(\"mary_had_lamb\").audio_and_sample_rate]\n        model = \"Qwen/Qwen2-Audio-7B-Instruct\"\n        audio_seq = \"<|audio_bos|><|AUDIO|><|audio_eos|>\"\n        prompts = [\n            f\"<|im_start|>user\\n{audio_seq}Can you transcribe this?<|im_end|>\\n<|im_start|>assistant\\n\"  #noqa: E501\n        ]\n    \n        with hf_runner(model, dtype=dtype,\n                       auto_cls=AutoModelForSeq2SeqLM) as hf_model:\n            audio_token_id = hf_model.config.audio_token_index\n            eos_token_id = hf_model.tokenizer.eos_token_id  # <|im_end|>\n            hf_outputs = hf_model.generate_beam_search(\n                prompts,\n                beam_width=beam_width,\n                max_tokens=max_tokens,\n                audios=audios,\n            )\n    \n>       with vllm_runner(model, dtype=dtype) as vllm_model:\n\nsamplers/test_beam_search.py:102: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nconftest.py:782: in __init__\n    self.model = LLM(\n../vllm/entrypoints/llm.py:262: in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n../vllm/engine/llm_engine.py:501: in from_engine_args\n    return engine_cls.from_vllm_config(\n../vllm/engine/llm_engine.py:477: in from_vllm_config\n    return cls(\n../vllm/engine/llm_engine.py:268: in __init__\n    self._initialize_kv_caches()\n../vllm/engine/llm_engine.py:426: in _initialize_kv_caches\n    self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n../vllm/executor/executor_base.py:124: in initialize_cache\n    self.collective_rpc(\"initialize_cache\",\n../vllm/executor/uniproc_executor.py:57: in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n../vllm/utils.py:2690: in run_method\n    return func(*args, **kwargs)\n../vllm/worker/worker.py:335: in initialize_cache\n    self._warm_up_model()\n../vllm/worker/worker.py:365: in _warm_up_model\n    self.model_runner.capture_model(self.gpu_cache)\n../../../venvs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py:116: in decorate_context\n    return func(*args, **kwargs)\n../vllm/worker/model_runner.py:1658: in capture_model\n    graph_runner.capture(**capture_inputs)\n../vllm/worker/model_runner.py:2059: in capture\n    self.model(\n../../../venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1751: in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n../../../venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: in _call_impl\n    return forward_call(*args, **kwargs)\n../vllm/model_executor/models/qwen2_audio.py:389: in forward\n    inputs_embeds = self.get_input_embeddings(input_ids,\n../vllm/model_executor/models/qwen2_audio.py:368: in get_input_embeddings\n    inputs_embeds = merge_multimodal_embeddings(\n../vllm/model_executor/models/utils.py:498: in merge_multimodal_embeddings\n    return _merge_multimodal_embeddings(\n../vllm/model_executor/models/utils.py:411: in _merge_multimodal_embeddings\n    flattened = _flatten_embeddings(multimodal_embeddings)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nembeddings = []\n\n    def _flatten_embeddings(embeddings: NestedTensors) -> torch.Tensor:\n        \"\"\"\n        Recursively flattens and concatenates NestedTensors on all but the last\n        dimension.\n        \"\"\"\n    \n        if isinstance(embeddings, torch.Tensor):\n            # Flatten all but the last dimension.\n            return embeddings.flatten(0, -2)\n    \n>       return torch.cat(tuple(_flatten_embeddings(t) for t in embeddings))\nE       RuntimeError: torch.cat(): expected a non-empty list of Tensors\n\n../vllm/model_executor/models/utils.py:363: RuntimeError\n============================================================================== warnings summary ==============================================================================\n../../../venvs/vllm/lib/python3.12/site-packages/schemathesis/generation/coverage.py:305\n  /home/mgoin/venvs/vllm/lib/python3.12/site-packages/schemathesis/generation/coverage.py:305: DeprecationWarning: jsonschema.exceptions.RefResolutionError is deprecated as of version 4.18.0. If you wish to catch potential reference resolution errors, directly catch referencing.exceptions.Unresolvable.\n    ref_error: type[Exception] = jsonschema.RefResolutionError,\n\ntests/samplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half]\n  /home/mgoin/venvs/vllm/lib/python3.12/site-packages/librosa/core/intervals.py:15: DeprecationWarning: path is deprecated. Use files() instead. Refer to https://importlib-resources.readthedocs.io/en/latest/using.html#migrating-from-legacy for migration advice.\n    with resources.path(\"librosa.core\", \"intervals.msgpack\") as imsgpack:\n\ntests/samplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half]\n  /home/mgoin/venvs/vllm/lib/python3.12/site-packages/audioread/rawread.py:16: DeprecationWarning: 'aifc' is deprecated and slated for removal in Python 3.13\n    import aifc\n\ntests/samplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half]\n  /home/mgoin/venvs/vllm/lib/python3.12/site-packages/audioread/rawread.py:17: DeprecationWarning: 'audioop' is deprecated and slated for removal in Python 3.13\n    import audioop\n\ntests/samplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half]\n  /home/mgoin/venvs/vllm/lib/python3.12/site-packages/audioread/rawread.py:19: DeprecationWarning: 'sunau' is deprecated and slated for removal in Python 3.13\n    import sunau\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n========================================================================== short test summary info ===========================================================================\nFAILED samplers/test_beam_search.py::test_beam_search_passes_multimodal_data[False-2-64-half] - RuntimeError: torch.cat(): expected a non-empty list of Tensors\n======================================================================= 1 failed, 5 warnings in 29.93s =======================================================================\n```\n\n### \ud83d\udcdd History of failing test\n\nThis was introduced by https://github.com/vllm-project/vllm/pull/19446\n\nhttps://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests/1f2a99b2-fbc9-89fc-a08f-c4d431429893?period=7days&tags=scm.branch%3Amain\n<img width=\"1213\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/843cfb35-53b6-4db2-b326-c65e1226442b\" />\n\nIt was not caught because the test wasn't triggered by the change I believe.\n\n\n### CC List.\n\n@russellb ",
    "labels": [
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-06-17T09:23:35+00:00",
    "closed_at": "2025-06-18T22:48:30+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19736/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/19736"
  },
  {
    "number": 20723,
    "title": "[CI Failure]: LoRA TP Test (Distributed) - lora/test_llama_tp.py::test_tp2_serialize_and_deserialize_lora",
    "body": "### Name of failing test\n\n`lora/test_llama_tp.py::test_tp2_serialize_and_deserialize_lora`\n\n### Basic information\n\n- [ ] Flaky test\n- [x] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\nhttps://buildkite.com/vllm/ci/builds/23536/steps/canvas?sid=0197f0f3-a191-49c0-aef5-89d61c597808\n\n```\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) WARNING 07-09 16:17:11 [tensorizer.py:226] Provided both tensorizer_dir and tensorizer_uri. Inferring tensorizer_dir from tensorizer_uri as the latter takes precedence.\n\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487] Traceback (most recent call last):\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 461, in worker_main\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487]     worker = WorkerProc(*args, **kwargs)\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/multiproc_executor.py\", line 358, in __init__\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487]     self.worker.load_model()\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 186, in load_model\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487]     self.model_runner.load_model()\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1773, in load_model\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487]     model_loader = get_model_loader(self.load_config)\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 33, in get_model_loader\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487]     return TensorizerLoader(load_config)\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/tensorizer_loader.py\", line 45, in __init__\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487]     self.tensorizer_config = TensorizerConfig(\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487]                              ^^^^^^^^^^^^^^^^^\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487]   File \"<string>\", line 16, in __init__\n[2025-07-09T23:17:11Z] (VllmWorker rank=0 pid=11292) ERROR 07-09 16:17:11 [multiproc_executor.py:487]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/tensorizer.py\", line 232, in __post_init__\n```\n\n\n\n\n### \ud83d\udcdd History of failing test\n\nIt seems like this failure was introduced by https://github.com/vllm-project/vllm/pull/19619 as it introduced this check in the tensorizer.py\n```python\n        if self.tensorizer_dir and self.lora_dir:\n            raise ValueError(\n                \"Only one of tensorizer_dir or lora_dir may be specified. \"\n                \"Use lora_dir exclusively when serializing LoRA adapters, \"\n                \"and tensorizer_dir or tensorizer_uri otherwise.\")\n```\n\nIt seems the failing test here wasn't triggered by the conditional check\n\n### CC List.\n\n@sangstar @Eta0 @aarnphm @jeejeelee please take a look",
    "labels": [
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-07-10T00:26:10+00:00",
    "closed_at": "2025-07-10T19:07:08+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20723/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/20723"
  },
  {
    "number": 18492,
    "title": "[Bug][Failing Test]: Distributed Comm Ops - distributed/test_shm_broadcast.py",
    "body": "### Your current environment\n\npytest -v -x distributed/test_shm_broadcast.py\n\nhttps://buildkite.com/vllm/ci/builds/20415#0196f100-f85c-4db6-8b50-72d3d5ade137/197-990\n\n\n### \ud83d\udc1b Describe the bug\n\nSee above\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-05-21T14:53:55+00:00",
    "closed_at": "2025-05-22T03:19:14+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18492/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/18492"
  },
  {
    "number": 20461,
    "title": "[CI Failure]: Language Models Test (Extended Pooling)",
    "body": "### Name of failing test\n\nSee below\n\n### Basic information\n\n- [ ] Flaky test\n- [ ] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\nRemaining failures:\n```\nFAILED models/language/pooling/test_scoring.py::test_cross_encoder_1_to_1[cross-encoder/ms-marco-MiniLM-L-6-v2] - assert 9.265625 == 1.0 \u00b1 1.0e-02\n  comparison failed\n  Obtained: 9.265625\n  Expected: 1.0 \u00b1 1.0e-02\nFAILED models/language/pooling/test_scoring.py::test_cross_encoder_1_to_N[cross-encoder/ms-marco-MiniLM-L-6-v2] - assert 9.265625 == 1.0 \u00b1 1.0e-02\n  comparison failed\n  Obtained: 9.265625\n  Expected: 1.0 \u00b1 1.0e-02\nFAILED models/language/pooling/test_scoring.py::test_cross_encoder_N_to_N[cross-encoder/ms-marco-MiniLM-L-6-v2] - assert 9.265625 == 1.0 \u00b1 1.0e-02\n  comparison failed\n  Obtained: 9.265625\n  Expected: 1.0 \u00b1 1.0e-02\n```\n\nFixed by #20168:\n```\nFAILED models/language/pooling/test_embedding.py::test_models[False-sentence-transformers/all-MiniLM-L12-v2] - pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n  Value error, User-specified max_model_len (512) is greater than the derived max_model_len (max_position_embeddings=128 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. To allow overriding this maximum, set the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN=1 [type=value_error, input_value=ArgsKwargs((), {'model': ...attention_dtype': None}), input_type=ArgsKwargs]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error\nFAILED models/language/pooling/test_embedding.py::test_models[False-sentence-transformers/stsb-roberta-base-v2] - pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig\n  Value error, User-specified max_model_len (512) is greater than the derived max_model_len (max_position_embeddings=75 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. To allow overriding this maximum, set the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN=1 [type=value_error, input_value=ArgsKwargs((), {'model': ...attention_dtype': None}), input_type=ArgsKwargs]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error\nFAILED models/language/pooling/test_gte.py::test_embed_models_mteb[model_info9] - RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\nFAILED models/language/pooling/test_gte.py::test_embed_models_mteb[model_info10] - RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\nFAILED models/language/pooling/test_gte.py::test_embed_models_correctness[model_info9] - RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\nFAILED models/language/pooling/test_gte.py::test_embed_models_correctness[model_info10] - RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n```\n\n### \ud83d\udcdd History of failing test\n\nFailing since 27th June, e.g. https://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests/7c3bdcad-8f70-86e6-b83a-d0f0ab07fd71?period=7days&tags=scm.branch%3Amain\n\n### CC List.\n\n@noooop can you take a look at this?",
    "labels": [
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-07-04T02:56:06+00:00",
    "closed_at": "2025-07-06T21:01:49+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20461/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/20461"
  },
  {
    "number": 20158,
    "title": "[CI Failure]: `deepseek_mtp_main_random_bf16` can't load, causes deepseek_mtp CI Failure.",
    "body": "### Name of failing test\n\n`https://github.com/vllm-project/vllm-ascend/actions/runs/15890661413/job/44812465270?pr=1128`\n\n### Basic information\n\n- [ ] Flaky test\n- [x] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\n`Acsend 910b platform`\n\n`pytest ./tests/e2e/long_term/spec_decode_v0/e2e/test_mtp_correctness.py`\n\n\n\n### \ud83d\udcdd History of failing test\n\n```bash\nvllm-empty/vllm/model_executor/model_loader/default_loader.py:269: in load_weights\n    loaded_weights = model.load_weights(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = CustomDeepSeekMTP(\n  (model): CustomDeepSeekMultiTokenPredictor(\n    (layers): ModuleDict(\n      (5): CustomDeepSeekMu...ogitsProcessor(vocab_size=129280, org_vocab_size=129280, scale=1.0, logits_as_input=False)\n  )\n  (sampler): Sampler()\n)\nweights = <generator object DefaultModelLoader.get_all_weights at 0xffff7dd9f060>\n\n    def load_weights(self, weights: Iterable[tuple[str,\n                                                   torch.Tensor]]) -> set[str]:\n        stacked_params_mapping = [\n            (\"gate_up_proj\", \"gate_proj\", 0),\n            (\"gate_up_proj\", \"up_proj\", 1),\n        ]\n    \n        expert_params_mapping = FusedMoE.make_expert_params_mapping(\n            ckpt_gate_proj_name=\"gate_proj\",\n            ckpt_down_proj_name=\"down_proj\",\n            ckpt_up_proj_name=\"up_proj\",\n            num_experts=self.config.n_routed_experts)\n    \n        params_dict = dict(self.named_parameters())\n        loaded_params: set[str] = set()\n        for name, loaded_weight in weights:\n            if \"rotary_emb.inv_freq\" in name:\n                continue\n            spec_layer = get_spec_layer_idx_from_weight_name(self.config, name)\n            if spec_layer is None:\n                continue\n            name = self._rewrite_spec_layer_name(spec_layer, name)\n            for (param_name, weight_name, shard_id) in stacked_params_mapping:\n                # Skip non-stacked layers and experts (experts handled below).\n                if weight_name not in name:\n                    continue\n                # We have mlp.experts[0].gate_proj in the checkpoint.\n                # Since we handle the experts below in expert_params_mapping,\n                # we need to skip here BEFORE we update the name, otherwise\n                # name will be updated to mlp.experts[0].gate_up_proj, which\n                # will then be updated below in expert_params_mapping\n                # for mlp.experts[0].gate_gate_up_proj, which breaks load.\n                if ((\"mlp.experts.\" in name) and name not in params_dict):\n                    continue\n                name = name.replace(weight_name, param_name)\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n    \n                param = params_dict[name]\n                weight_loader = param.weight_loader\n                weight_loader(param, loaded_weight, shard_id)\n                break\n            else:\n                for mapping in expert_params_mapping:\n                    param_name, weight_name, expert_id, shard_id = mapping\n                    if weight_name not in name:\n                        continue\n                    name = name.replace(weight_name, param_name)\n    \n                    param = params_dict[name]\n                    weight_loader = param.weight_loader\n                    weight_loader(param,\n                                  loaded_weight,\n                                  name,\n                                  shard_id=shard_id,\n                                  expert_id=expert_id)\n                    break\n                else:\n                    # Skip loading extra bias for GPTQ models.\n                    if name.endswith(\".bias\") and name not in params_dict:\n                        continue\n    \n                    # According to DeepSeek-V3 Technical Report, MTP modules\n                    # shares embedding layer. We only load the first weights.\n                    if (spec_layer != self.model.mtp_start_layer_idx\n                            and \".layers\" not in name):\n                        continue\n    \n>                   param = params_dict[name]\nE                   KeyError: 'model.embed_tokens.weight'\n```\n\n### CC List.\n\n_No response_",
    "labels": [
      "ci-failure"
    ],
    "state": "open",
    "created_at": "2025-06-27T01:42:24+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20158/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20158"
  },
  {
    "number": 20366,
    "title": "[CI Failure]: entrypoints-test: test_streaming_response",
    "body": "### Name of failing test\n\nentrypoints/openai/test_transcription_validation.py::test_streaming_response\n\n### Basic information\n\n- [x] Flaky test\n- [ ] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\nTwo sporadic failing tests:\nentrypoints/openai/test_transcription_validation.py::test_streaming_response\nentrypoints/openai/test_translation_validation.py::test_streaming_response\n\nFailing with:\nopenai.APITimeoutError: Request timed out.\n\nExample commits from main which fail:\nhttps://github.com/vllm-project/vllm/commit/c05596f1a350f3d993c467959ed02492141c2527\nhttps://github.com/vllm-project/vllm/commit/7da296be04933cfc29031f5bd1ba7cd28f376faa\n\nThere are more...\n\n### \ud83d\udcdd History of failing test\n\nThe first commit in main with this error is:\nhttps://github.com/vllm-project/vllm/commit/c05596f1a350f3d993c467959ed02492141c2527\n\nHowever, looking at the code it seems unrelated. So maybe some other commit beforehand. My guess is around Jun 30th-July1st.\n\n### CC List.\n\n_No response_",
    "labels": [
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-07-02T09:59:03+00:00",
    "closed_at": "2025-07-04T07:55:13+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20366/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20366"
  },
  {
    "number": 20767,
    "title": "[CI Failure]: Quantization Test - quantization/test_bitsandbytes.py::test_load_4bit_bnb_model",
    "body": "### Name of failing test\n\n`quantization/test_bitsandbytes.py::test_load_4bit_bnb_model[facebook/opt-125m-quantize opt model inflight]`\n\n### Basic information\n\n- [x] Flaky test\n- [ ] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\nThere are quite a few failing bnb tests https://buildkite.com/vllm/ci/builds/23559/steps/canvas?sid=0197f23d-c9e5-45de-b07a-5e290ae4a6ce\n\n```\n\n[2025-07-10T04:47:11Z] FAILED quantization/test_bitsandbytes.py::test_load_4bit_bnb_model[facebook/opt-125m-quantize opt model inflight] - AssertionError: function <function test_load_4bit_bnb_model at 0x7f4699a1cea0> failed when called with args () and kwargs {'hf_runner': <class 'tests.conftest.HfRunner'>, 'vllm_runner': <class 'tests.conftest.VllmRunner'>, 'example_prompts': ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the major milestones in the development of artificial intelligence from 1950 to 2020.\\n', 'Compare and contrast artificial intelligence with human intelligence in terms of processing information.\\n', 'Describe the basic components of a neural network and how it can be trained.\\n', 'Write a short story about a robot that dreams for the first time.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', 'Explain the cultural significance of the Mona Lisa painting, and how its perception might vary in Western versus Eastern societies.\\n', \"Translate the following English sentence into Japanese, French, and Swahili: 'The early bird catches the worm.'\\n\"], 'model_name': 'facebook/opt-125m', 'description': 'quantize opt model inflight'}\n--\n\u00a0 | [2025-07-10T04:47:11Z] FAILED quantization/test_bitsandbytes.py::test_load_4bit_bnb_model[mistralai/Mistral-7B-Instruct-v0.3-quantize inflight model with both HF and Mistral format weights] - AssertionError: function <function test_load_4bit_bnb_model at 0x7f4699a1cea0> failed when called with args () and kwargs {'hf_runner': <class 'tests.conftest.HfRunner'>, 'vllm_runner': <class 'tests.conftest.VllmRunner'>, 'example_prompts': ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the major milestones in the development of artificial intelligence from 1950 to 2020.\\n', 'Compare and contrast artificial intelligence with human intelligence in terms of processing information.\\n', 'Describe the basic components of a neural network and how it can be trained.\\n', 'Write a short story about a robot that dreams for the first time.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', 'Explain the cultural significance of the Mona Lisa painting, and how its perception might vary in Western versus Eastern societies.\\n', \"Translate the following English sentence into Japanese, French, and Swahili: 'The early bird catches the worm.'\\n\"], 'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', 'description': 'quantize inflight model with both HF and Mistral format weights'}\n\u00a0 | [2025-07-10T04:47:11Z] FAILED quantization/test_bitsandbytes.py::test_load_pre_quant_4bit_bnb_model[PrunaAI/Einstein-v6.1-Llama3-8B-bnb-4bit-smashed-read pre-quantized 4-bit FP4 model] - AssertionError: function <function test_load_pre_quant_4bit_bnb_model at 0x7f46714ffa60> failed when called with args () and kwargs {'hf_runner': <class 'tests.conftest.HfRunner'>, 'vllm_runner': <class 'tests.conftest.VllmRunner'>, 'example_prompts': ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the major milestones in the development of artificial intelligence from 1950 to 2020.\\n', 'Compare and contrast artificial intelligence with human intelligence in terms of processing information.\\n', 'Describe the basic components of a neural network and how it can be trained.\\n', 'Write a short story about a robot that dreams for the first time.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', 'Explain the cultural significance of the Mona Lisa painting, and how its perception might vary in Western versus Eastern societies.\\n', \"Translate the following English sentence into Japanese, French, and Swahili: 'The early bird catches the worm.'\\n\"], 'model_name': 'PrunaAI/Einstein-v6.1-Llama3-8B-bnb-4bit-smashed', 'description': 'read pre-quantized 4-bit FP4 model'}\n\u00a0 | [2025-07-10T04:47:11Z] FAILED quantization/test_bitsandbytes.py::test_load_pre_quant_4bit_bnb_model[poedator/opt-125m-bnb-4bit-read pre-quantized 4-bit NF4 opt model] - AssertionError: function <function test_load_pre_quant_4bit_bnb_model at 0x7f46714ffa60> failed when called with args () and kwargs {'hf_runner': <class 'tests.conftest.HfRunner'>, 'vllm_runner': <class 'tests.conftest.VllmRunner'>, 'example_prompts': ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the major milestones in the development of artificial intelligence from 1950 to 2020.\\n', 'Compare and contrast artificial intelligence with human intelligence in terms of processing information.\\n', 'Describe the basic components of a neural network and how it can be trained.\\n', 'Write a short story about a robot that dreams for the first time.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', 'Explain the cultural significance of the Mona Lisa painting, and how its perception might vary in Western versus Eastern societies.\\n', \"Translate the following English sentence into Japanese, French, and Swahili: 'The early bird catches the worm.'\\n\"], 'model_name': 'poedator/opt-125m-bnb-4bit', 'description': 'read pre-quantized 4-bit NF4 opt model'}\n\u00a0 | [2025-07-10T04:47:11Z] FAILED quantization/test_bitsandbytes.py::test_load_8bit_bnb_model[meta-llama/Llama-Guard-3-8B-INT8-read pre-quantized llama 8-bit model] - AssertionError: function <function test_load_8bit_bnb_model at 0x7f4671570400> failed when called with args () and kwargs {'hf_runner': <class 'tests.conftest.HfRunner'>, 'vllm_runner': <class 'tests.conftest.VllmRunner'>, 'example_prompts': ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the major milestones in the development of artificial intelligence from 1950 to 2020.\\n', 'Compare and contrast artificial intelligence with human intelligence in terms of processing information.\\n', 'Describe the basic components of a neural network and how it can be trained.\\n', 'Write a short story about a robot that dreams for the first time.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', 'Explain the cultural significance of the Mona Lisa painting, and how its perception might vary in Western versus Eastern societies.\\n', \"Translate the following English sentence into Japanese, French, and Swahili: 'The early bird catches the worm.'\\n\"], 'model_name': 'meta-llama/Llama-Guard-3-8B-INT8', 'description': 'read pre-quantized llama 8-bit model'}\n\u00a0 | [2025-07-10T04:47:11Z] FAILED quantization/test_bitsandbytes.py::test_load_8bit_bnb_model[yec019/fbopt-350m-8bit-read pre-quantized 8-bit opt model] - AssertionError: function <function test_load_8bit_bnb_model at 0x7f4671570400> failed when called with args () and kwargs {'hf_runner': <class 'tests.conftest.HfRunner'>, 'vllm_runner': <class 'tests.conftest.VllmRunner'>, 'example_prompts': ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the major milestones in the development of artificial intelligence from 1950 to 2020.\\n', 'Compare and contrast artificial intelligence with human intelligence in terms of processing information.\\n', 'Describe the basic components of a neural network and how it can be trained.\\n', 'Write a short story about a robot that dreams for the first time.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', 'Explain the cultural significance of the Mona Lisa painting, and how its perception might vary in Western versus Eastern societies.\\n', \"Translate the following English sentence into Japanese, French, and Swahili: 'The early bird catches the worm.'\\n\"], 'model_name': 'yec019/fbopt-350m-8bit', 'description': 'read pre-quantized 8-bit opt model'}\n\u00a0 | [2025-07-10T04:47:11Z] FAILED quantization/test_bitsandbytes.py::test_4bit_bnb_embedding_model[half-intfloat/e5-mistral-7b-instruct-quantize embedding model inflight] - AssertionError: function <function test_4bit_bnb_embedding_model at 0x7f46715707c0> failed when called with args () and kwargs {'model_name': 'intfloat/e5-mistral-7b-instruct', 'description': 'quantize embedding model inflight', 'hf_runner': <class 'tests.conftest.HfRunner'>, 'vllm_runner': <class 'tests.conftest.VllmRunner'>, 'example_prompts': ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the major milestones in the development of artificial intelligence from 1950 to 2020.\\n', 'Compare and contrast artificial intelligence with human intelligence in terms of processing information.\\n', 'Describe the basic components of a neural network and how it can be trained.\\n', 'Write a short story about a robot that dreams for the first time.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', 'Explain the cultural significance of the Mona Lisa painting, and how its perception might vary in Western versus Eastern societies.\\n', \"Translate the following English sentence into Japanese, French, and Swahili: 'The early bird catches the worm.'\\n\"], 'dtype': 'half'}\n\n\n```\n\n### \ud83d\udcdd History of failing test\n\nhttps://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests/c42f29ca-8d42-820d-a90c-5493977dcbc0?period=1day&tags=scm.branch%3Amain\n\n<img width=\"1419\" height=\"977\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/35037ca3-d089-4ca5-a70d-6a9426b3e316\" />\n\n### CC List.\n\ncc @jeejeelee ",
    "labels": [
      "ci-failure"
    ],
    "state": "open",
    "created_at": "2025-07-10T16:03:59+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20767/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/20767"
  },
  {
    "number": 18498,
    "title": "[Bug][Failing Test]: LoRA 2 - lora/test_lora_functions.py::test_lora_functions_sync",
    "body": "### Your current environment\n\nN/A\n\n### \ud83d\udc1b Describe the bug\n\nhttps://buildkite.com/vllm/ci/builds/20460/steps?jid=0196f343-0fdb-4d91-80da-728e0fb8174c\n\nSummary:\n```\n[2025-05-21T16:00:09Z] FAILED lora/test_lora_functions.py::test_lora_functions_sync[True] - Exception: Call to add_lora method failed: CUDA error: an illegal memory access was encountered\n[2025-05-21T16:00:09Z] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[2025-05-21T16:00:09Z] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[2025-05-21T16:00:09Z] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n```\n\nStack:\n```\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559] Invocation of add_lora method failed\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559] Traceback (most recent call last):\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 556, in _handle_client_request\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]     output.result = method(\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]                     ^^^^^^^\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 314, in add_lora\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]     return self.model_executor.add_lora(lora_request)\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 150, in add_lora\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]     return all(self.collective_rpc(\"add_lora\", args=(lora_request, )))\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]     answer = run_method(self.driver_worker, method, args, kwargs)\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2598, in run_method\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]     return func(*args, **kwargs)\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 300, in add_lora\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]     return self.model_runner.add_lora(lora_request)\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/lora_model_runner_mixin.py\", line 130, in add_lora\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]     return self.lora_manager.add_adapter(lora_request)\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]   File \"/usr/local/lib/python3.12/dist-packages/vllm/lora/worker_manager.py\", line 235, in add_adapter\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]     lora = self._load_adapter(lora_request)\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]   File \"/usr/local/lib/python3.12/dist-packages/vllm/lora/worker_manager.py\", line 141, in _load_adapter\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]     raise e\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]   File \"/usr/local/lib/python3.12/dist-packages/vllm/lora/worker_manager.py\", line 117, in _load_adapter\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]     lora = self._lora_model_cls.from_local_checkpoint(\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]   File \"/usr/local/lib/python3.12/dist-packages/vllm/lora/models.py\", line 290, in from_local_checkpoint\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]     return cls.from_lora_tensors(\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]            ^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]   File \"/usr/local/lib/python3.12/dist-packages/vllm/lora/models.py\", line 145, in from_lora_tensors\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]     lora_embeddings_tensor.pin_memory())\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559] RuntimeError: CUDA error: an illegal memory access was encountered\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[2025-05-21T15:50:19Z] ERROR 05-21 08:50:19 [core.py:559] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-05-21T16:35:09+00:00",
    "closed_at": "2025-05-22T04:48:54+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18498/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18498"
  },
  {
    "number": 18425,
    "title": "[Bug][Failing Test] - Quantization test - quantization/test_cpu_offload.py",
    "body": "### Your current environment\n\nFailing on main as of commit 9609327fa4\n\n### \ud83d\udc1b Describe the bug\n\nFailing tests:\n\n```\nFAILED quantization/test_cpu_offload.py::test_cpu_offload_gptq - RuntimeError: Server exited unexpectedly.\nFAILED quantization/test_cpu_offload.py::test_cpu_offload_awq - RuntimeError: Server exited unexpectedly.\nFAILED quantization/test_cpu_offload.py::test_cpu_offload_compressed_tensors - AssertionError: Results for model='nm-testing/llama7b-one-shot-2_4-w4a16-marlin24-t' are not the same.\nref_args=[] ref_envs=None\ncompare_args=['--cpu-offload-gb', '1'] compare_envs=None\nref_result={'test': 'single_completion', 'text': ' ... ... . Today I', 'finish_reason': 'length', 'usage': CompletionUsage(completion_tokens=5, prompt_tokens=6, total_tokens=11, completion_tokens_details=None, prompt_tokens_details=None)}\ncompare_result={'test': 'single_completion', 'text': ' ... ... .\\n I', 'finish_reason': 'length', 'usage': CompletionUsage(completion_tokens=5, prompt_tokens=6, total_tokens=11, completion_tokens_details=None, prompt_tokens_details=None)}\n```\n\n<details>\n<summary>Logs</summary>\n\n\n```\nquantization/test_torchao.py::test_opt_125m_int4wo_model_loading_with_params[cuda:0] SKIPPED\nquantization/test_torchao.py::test_opt_125m_int4wo_model_per_module_quant SKIPPED\n\n=================================== FAILURES ===================================\n_ test_load_8bit_bnb_model[meta-llama/Llama-Guard-3-8B-INT8-read pre-quantized llama 8-bit model] _\n\nargs = ()\nkwargs = {'description': 'read pre-quantized llama 8-bit model', 'example_prompts': ['vLLM is a high-throughput and memory-effi...odels.\\n', ...], 'hf_runner': <class 'tests.conftest.HfRunner'>, 'model_name': 'meta-llama/Llama-Guard-3-8B-INT8', ...}\nSkipped = <class 'Skipped'>, pid = 1736, pgid = 19, _pid = 1736, _exitcode = 256\nold_signal_handler = <Handlers.SIG_DFL: 0>\n\n    @functools.wraps(f)\n    def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> None:\n        # Make the process the leader of its own process group\n        # to avoid sending SIGTERM to the parent process\n        os.setpgrp()\n        from _pytest.outcomes import Skipped\n        pid = os.fork()\n        print(f\"Fork a new process to run a test {pid}\")\n        if pid == 0:\n            try:\n                f(*args, **kwargs)\n            except Skipped as e:\n                # convert Skipped to exit code 0\n                print(str(e))\n                os._exit(0)\n            except Exception:\n                import traceback\n                traceback.print_exc()\n                os._exit(1)\n            else:\n                os._exit(0)\n        else:\n            pgid = os.getpgid(pid)\n            _pid, _exitcode = os.waitpid(pid, 0)\n            # ignore SIGTERM signal itself\n            old_signal_handler = signal.signal(signal.SIGTERM, signal.SIG_IGN)\n            # kill all child processes\n            os.killpg(pgid, signal.SIGTERM)\n            # restore the signal handler\n            signal.signal(signal.SIGTERM, old_signal_handler)\n>           assert _exitcode == 0, (f\"function {f} failed when called with\"\n                                    f\" args {args} and kwargs {kwargs}\")\nE           AssertionError: function <function test_load_8bit_bnb_model at 0x7fc6998f1800> failed when called with args () and kwargs {'hf_runner': <class 'tests.conftest.HfRunner'>, 'vllm_runner': <class 'tests.conftest.VllmRunner'>, 'example_prompts': ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the major milestones in the development of artificial intelligence from 1950 to 2020.\\n', 'Compare and contrast artificial intelligence with human intelligence in terms of processing information.\\n', 'Describe the basic components of a neural network and how it can be trained.\\n', 'Write a short story about a robot that dreams for the first time.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', 'Explain the cultural significance of the Mona Lisa painting, and how its perception might vary in Western versus Eastern societies.\\n', \"Translate the following English sentence into Japanese, French, and Swahili: 'The early bird catches the worm.'\\n\"], 'model_name': 'meta-llama/Llama-Guard-3-8B-INT8', 'description': 'read pre-quantized llama 8-bit model'}\n\nutils.py:747: AssertionError\n____________________________ test_cpu_offload_gptq _____________________________\n\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fc5b4cb2600>\n\n    @pytest.mark.skipif(not is_quant_method_supported(\"gptq_marlin\"),\n                        reason=\"gptq_marlin is not supported on this GPU type.\")\n    def test_cpu_offload_gptq(monkeypatch):\n        # This quant method is sensitive to dummy weights, so we force real weights\n        monkeypatch.setenv('VLLM_TEST_FORCE_LOAD_FORMAT', 'auto')\n        # Test GPTQ Marlin\n>       compare_two_settings(\"Qwen/Qwen2-1.5B-Instruct-GPTQ-Int4\", [],\n                             [\"--cpu-offload-gb\", \"1\"],\n                             max_wait_seconds=480)\n\nquantization/test_cpu_offload.py:33: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nutils.py:465: in compare_two_settings\n    compare_all_settings(\nutils.py:529: in compare_all_settings\n    with RemoteOpenAIServer(model,\nutils.py:133: in __init__\n    self._wait_for_server(url=self.url_for(\"health\"),\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <tests.utils.RemoteOpenAIServer object at 0x7fc59e873800>\n\n    def _wait_for_server(self, *, url: str, timeout: float):\n        # run health check\n        start = time.time()\n        while True:\n            try:\n                if requests.get(url).status_code == 200:\n                    break\n            except Exception:\n                # this exception can only be raised by requests.get,\n                # which means the server is not ready yet.\n                # the stack trace is not useful, so we suppress it\n                # by using `raise from None`.\n                result = self.proc.poll()\n                if result is not None and result != 0:\n>                   raise RuntimeError(\"Server exited unexpectedly.\") from None\nE                   RuntimeError: Server exited unexpectedly.\n\nutils.py:161: RuntimeError\n_____________________________ test_cpu_offload_awq _____________________________\n\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fc59e8de900>\n\n    @pytest.mark.skipif(not is_quant_method_supported(\"awq_marlin\"),\n                        reason=\"awq_marlin is not supported on this GPU type.\")\n    def test_cpu_offload_awq(monkeypatch):\n        # This quant method is sensitive to dummy weights, so we force real weights\n        monkeypatch.setenv('VLLM_TEST_FORCE_LOAD_FORMAT', 'auto')\n        # Test AWQ Marlin\n>       compare_two_settings(\"Qwen/Qwen2-1.5B-Instruct-AWQ\", [],\n                             [\"--cpu-offload-gb\", \"1\"],\n                             max_wait_seconds=480)\n\nquantization/test_cpu_offload.py:49: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nutils.py:465: in compare_two_settings\n    compare_all_settings(\nutils.py:529: in compare_all_settings\n    with RemoteOpenAIServer(model,\nutils.py:133: in __init__\n    self._wait_for_server(url=self.url_for(\"health\"),\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <tests.utils.RemoteOpenAIServer object at 0x7fc5b55128d0>\n\n    def _wait_for_server(self, *, url: str, timeout: float):\n        # run health check\n        start = time.time()\n        while True:\n            try:\n                if requests.get(url).status_code == 200:\n                    break\n            except Exception:\n                # this exception can only be raised by requests.get,\n                # which means the server is not ready yet.\n                # the stack trace is not useful, so we suppress it\n                # by using `raise from None`.\n                result = self.proc.poll()\n                if result is not None and result != 0:\n>                   raise RuntimeError(\"Server exited unexpectedly.\") from None\nE                   RuntimeError: Server exited unexpectedly.\n\nutils.py:161: RuntimeError\n_____________________ test_cpu_offload_compressed_tensors ______________________\n\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fc5b4b958e0>\n\n    @pytest.mark.skipif(not is_quant_method_supported(\"gptq_marlin\"),\n                        reason=\"gptq_marlin is not supported on this GPU type.\")\n    def test_cpu_offload_compressed_tensors(monkeypatch):\n        # This quant method is sensitive to dummy weights, so we force real weights\n        monkeypatch.setenv('VLLM_TEST_FORCE_LOAD_FORMAT', 'auto')\n        # Test wNa16\n        compare_two_settings(\"nm-testing/tinyllama-oneshot-w4a16-channel-v2\", [],\n                             [\"--cpu-offload-gb\", \"1\"],\n                             max_wait_seconds=480)\n        # Test w4a16_marlin24\n>       compare_two_settings(\"nm-testing/llama7b-one-shot-2_4-w4a16-marlin24-t\",\n                             [], [\"--cpu-offload-gb\", \"1\"],\n                             max_wait_seconds=480)\n\nquantization/test_cpu_offload.py:69: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nutils.py:465: in compare_two_settings\n    compare_all_settings(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmodel = 'nm-testing/llama7b-one-shot-2_4-w4a16-marlin24-t'\nall_args = [[], ['--cpu-offload-gb', '1']], all_envs = [None, None]\n\n    def compare_all_settings(model: str,\n                             all_args: list[list[str]],\n                             all_envs: list[Optional[dict[str, str]]],\n                             *,\n                             method: str = \"generate\",\n                             max_wait_seconds: Optional[float] = None) -> None:\n        \"\"\"\n        Launch API server with several different sets of arguments/environments\n        and compare the results of the API calls with the first set of arguments.\n        Args:\n            model: The model to test.\n            all_args: A list of argument lists to pass to the API server.\n            all_envs: A list of environment dictionaries to pass to the API server.\n        \"\"\"\n    \n        trust_remote_code = False\n        for args in all_args:\n            if \"--trust-remote-code\" in args:\n                trust_remote_code = True\n                break\n    \n        tokenizer_mode = \"auto\"\n        for args in all_args:\n            if \"--tokenizer-mode\" in args:\n                tokenizer_mode = args[args.index(\"--tokenizer-mode\") + 1]\n                break\n    \n        tokenizer = get_tokenizer(\n            model,\n            trust_remote_code=trust_remote_code,\n            tokenizer_mode=tokenizer_mode,\n        )\n    \n        can_force_load_format = True\n    \n        for args in all_args:\n            if \"--load-format\" in args:\n                can_force_load_format = False\n                break\n    \n        prompt = \"Hello, my name is\"\n        token_ids = tokenizer(prompt).input_ids\n        ref_results: list = []\n        for i, (args, env) in enumerate(zip(all_args, all_envs)):\n            if can_force_load_format:\n                # we are comparing the results and\n                # usually we don't need real weights.\n                # we force to use dummy weights by default,\n                # and it should work for most of the cases.\n                # if not, we can use VLLM_TEST_FORCE_LOAD_FORMAT\n                # environment variable to force the load format,\n                # e.g. in quantization tests.\n                args = args + [\"--load-format\", envs.VLLM_TEST_FORCE_LOAD_FORMAT]\n            compare_results: list = []\n            results = ref_results if i == 0 else compare_results\n            with RemoteOpenAIServer(model,\n                                    args,\n                                    env_dict=env,\n                                    max_wait_seconds=max_wait_seconds) as server:\n                client = server.get_client()\n    \n                # test models list\n                models = client.models.list()\n                models = models.data\n                served_model = models[0]\n                results.append({\n                    \"test\": \"models_list\",\n                    \"id\": served_model.id,\n                    \"root\": served_model.root,\n                })\n    \n                if method == \"generate\":\n                    results += _test_completion(client, model, prompt, token_ids)\n                elif method == \"generate_close\":\n                    results += _test_completion_close(client, model, prompt)\n                elif method == \"generate_chat\":\n                    results += _test_chat(client, model, prompt)\n                elif method == \"generate_with_image\":\n                    results += _test_image_text(\n                        client, model,\n                        \"https://upload.wikimedia.org/wikipedia/commons/0/0b/RGBA_comp.png\"\n                    )\n                elif method == \"encode\":\n                    results += _test_embeddings(client, model, prompt)\n                else:\n                    raise ValueError(f\"Unknown method: {method}\")\n    \n                if i > 0:\n                    # if any setting fails, raise an error early\n                    ref_args = all_args[0]\n                    ref_envs = all_envs[0]\n                    compare_args = all_args[i]\n                    compare_envs = all_envs[i]\n                    for ref_result, compare_result in zip(ref_results,\n                                                          compare_results):\n                        ref_result = copy.deepcopy(ref_result)\n                        compare_result = copy.deepcopy(compare_result)\n                        if \"embedding\" in ref_result and method == \"encode\":\n                            sim = F.cosine_similarity(\n                                torch.tensor(ref_result[\"embedding\"]),\n                                torch.tensor(compare_result[\"embedding\"]),\n                                dim=0,\n                            )\n                            assert sim >= 0.999, (\n                                f\"Embedding for {model=} are not the same.\\n\"\n                                f\"cosine_similarity={sim}\\n\")\n                            del ref_result[\"embedding\"]\n                            del compare_result[\"embedding\"]\n>                       assert ref_result == compare_result, (\n                            f\"Results for {model=} are not the same.\\n\"\n                            f\"{ref_args=} {ref_envs=}\\n\"\n                            f\"{compare_args=} {compare_envs=}\\n\"\n                            f\"{ref_result=}\\n\"\n                            f\"{compare_result=}\\n\")\nE                       AssertionError: Results for model='nm-testing/llama7b-one-shot-2_4-w4a16-marlin24-t' are not the same.\nE                       ref_args=[] ref_envs=None\nE                       compare_args=['--cpu-offload-gb', '1'] compare_envs=None\nE                       ref_result={'test': 'single_completion', 'text': ' ... ... . Today I', 'finish_reason': 'length', 'usage': CompletionUsage(completion_tokens=5, prompt_tokens=6, total_tokens=11, completion_tokens_details=None, prompt_tokens_details=None)}\nE                       compare_result={'test': 'single_completion', 'text': ' ... ... .\\n I', 'finish_reason': 'length', 'usage': CompletionUsage(completion_tokens=5, prompt_tokens=6, total_tokens=11, completion_tokens_details=None, prompt_tokens_details=None)}\n\nutils.py:582: AssertionError\n...\n=========================== short test summary info ============================\nFAILED quantization/test_bitsandbytes.py::test_load_8bit_bnb_model[meta-llama/Llama-Guard-3-8B-INT8-read pre-quantized llama 8-bit model] - AssertionError: function <function test_load_8bit_bnb_model at 0x7fc6998f1800> failed when called with args () and kwargs {'hf_runner': <class 'tests.conftest.HfRunner'>, 'vllm_runner': <class 'tests.conftest.VllmRunner'>, 'example_prompts': ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the major milestones in the development of artificial intelligence from 1950 to 2020.\\n', 'Compare and contrast artificial intelligence with human intelligence in terms of processing information.\\n', 'Describe the basic components of a neural network and how it can be trained.\\n', 'Write a short story about a robot that dreams for the first time.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', 'Explain the cultural significance of the Mona Lisa painting, and how its perception might vary in Western versus Eastern societies.\\n', \"Translate the following English sentence into Japanese, French, and Swahili: 'The early bird catches the worm.'\\n\"], 'model_name': 'meta-llama/Llama-Guard-3-8B-INT8', 'description': 'read pre-quantized llama 8-bit model'}\nFAILED quantization/test_cpu_offload.py::test_cpu_offload_gptq - RuntimeError: Server exited unexpectedly.\nFAILED quantization/test_cpu_offload.py::test_cpu_offload_awq - RuntimeError: Server exited unexpectedly.\nFAILED quantization/test_cpu_offload.py::test_cpu_offload_compressed_tensors - AssertionError: Results for model='nm-testing/llama7b-one-shot-2_4-w4a16-marlin24-t' are not the same.\nref_args=[] ref_envs=None\ncompare_args=['--cpu-offload-gb', '1'] compare_envs=None\nref_result={'test': 'single_completion', 'text': ' ... ... . Today I', 'finish_reason': 'length', 'usage': CompletionUsage(completion_tokens=5, prompt_tokens=6, total_tokens=11, completion_tokens_details=None, prompt_tokens_details=None)}\ncompare_result={'test': 'single_completion', 'text': ' ... ... .\\n I', 'finish_reason': 'length', 'usage': CompletionUsage(completion_tokens=5, prompt_tokens=6, total_tokens=11, completion_tokens_details=None, prompt_tokens_details=None)}\n====== 4 failed, 78 passed, 35 skipped, 49 warnings in 4561.59s (1:16:01) ======\n^^^ +++\n\ud83d\udea8 Error: The command exited with status 1\n^^^ +++\nuser command error: The plugin docker command hook exited with status 1\n~~~ Running global pre-exit hook\n$ /etc/buildkite-agent/hooks/pre-exit\n~~~ Running plugin docker pre-exit hook\n$ /var/lib/buildkite-agent/plugins/bk-gpu-1-queue-ci-i-036cff6c74f0af4ae-1/github-com-buildkite-plugins-docker-buildkite-plugin-v5-2-0/hooks/pre-exit\n\n\n</details>",
    "labels": [
      "bug",
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-05-20T16:15:31+00:00",
    "closed_at": "2025-05-21T17:25:49+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18425/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18425"
  },
  {
    "number": 19964,
    "title": "[CI Failure]: Quantization Test - quantization/test_bitsandbytes.py::test_4bit_bnb_embedding_model",
    "body": "### Name of failing test\n\n`quantization/test_bitsandbytes.py::test_4bit_bnb_embedding_model[half-intfloat/e5-mistral-7b-instruct-quantize embedding model inflight]`\n\n### Basic information\n\n- [ ] Flaky test\n- [x] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\n```\npytest -s -v \"quantization/test_bitsandbytes.py::test_4bit_bnb_embedding_model[half-intfloat/e5-mistral-7b-instruct-quantize embedding model inflight]\"\nINFO 06-23 04:48:10 [__init__.py:244] Automatically detected platform cuda.\n/home/mgoin/venvs/vllm/lib/python3.12/site-packages/pytest_asyncio/plugin.py:208: PytestDeprecationWarning: The configuration option \"asyncio_default_fixture_loop_scope\" is unset.\nThe event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))\n============================================================================================ test session starts =============================================================================================\nplatform linux -- Python 3.12.4, pytest-8.3.3, pluggy-1.5.0 -- /home/mgoin/venvs/vllm/bin/python3\ncachedir: .pytest_cache\nhypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/home/mgoin/code/vllm/tests/.hypothesis/examples'))\nrootdir: /home/mgoin/code/vllm\nconfigfile: pyproject.toml\nplugins: forked-1.6.0, subtests-0.14.1, asyncio-0.24.0, shard-0.1.2, buildkite-test-collector-0.1.9, timeout-2.3.1, schemathesis-3.39.15, anyio-4.6.2.post1, mock-3.14.0, hypothesis-6.131.0, rerunfailures-14.0\nasyncio: mode=Mode.STRICT, default_loop_scope=None\ncollected 1 item                                                                                                                                                                                             \nRunning 1 items in this shard: tests/quantization/test_bitsandbytes.py::test_4bit_bnb_embedding_model[half-intfloat/e5-mistral-7b-instruct-quantize embedding model inflight]\n\nquantization/test_bitsandbytes.py::test_4bit_bnb_embedding_model[half-intfloat/e5-mistral-7b-instruct-quantize embedding model inflight] Fork a new process to run a test 1747225\nFork a new process to run a test 0\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:09<00:00,  4.80s/it]\nINFO 06-23 04:48:29 [config.py:588] Found sentence-transformers tokenize configuration.\nINFO 06-23 04:48:35 [config.py:484] Found sentence-transformers modules configuration.\nINFO 06-23 04:48:35 [config.py:504] Found pooling configuration.\nINFO 06-23 04:48:35 [config.py:1444] Using max model len 1024\nWARNING 06-23 04:48:35 [config.py:939] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nINFO 06-23 04:48:35 [arg_utils.py:1568] (Enabling) prefix caching by default\nWARNING 06-23 04:48:36 [utils.py:2613] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\nINFO 06-23 04:48:39 [__init__.py:244] Automatically detected platform cuda.\nINFO 06-23 04:48:42 [core.py:459] Waiting for init message from front-end.\nINFO 06-23 04:48:42 [core.py:69] Initializing a V1 LLM engine (v0.9.1.dev287+g89b1388d8) with config: model='intfloat/e5-mistral-7b-instruct', speculative_config=None, tokenizer='intfloat/e5-mistral-7b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=intfloat/e5-mistral-7b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type='LAST', normalize=True, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\nWARNING 06-23 04:48:42 [utils.py:2753] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ab4c475f5f0>\nINFO 06-23 04:48:42 [parallel_state.py:1072] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nWARNING 06-23 04:48:42 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\nINFO 06-23 04:48:42 [gpu_model_runner.py:1696] Starting to load model intfloat/e5-mistral-7b-instruct...\nINFO 06-23 04:48:43 [gpu_model_runner.py:1701] Loading model from scratch...\nINFO 06-23 04:48:43 [cuda.py:270] Using Flash Attention backend on V1 engine.\nINFO 06-23 04:48:43 [bitsandbytes_loader.py:454] Loading weights with BitsAndBytes quantization. May take a while ...\nINFO 06-23 04:48:43 [weight_utils.py:292] Using model weights format ['*.safetensors']\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.37s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.00s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.06s/it]\n\nINFO 06-23 04:48:46 [gpu_model_runner.py:1725] Model loading took 3.9099 GiB and 2.797580 seconds\nINFO 06-23 04:48:51 [backends.py:508] Using cache directory: /home/mgoin/.cache/vllm/torch_compile_cache/dee6dd784e/rank_0_0/backbone for vLLM's torch.compile\nINFO 06-23 04:48:51 [backends.py:519] Dynamo bytecode transform time: 4.86 s\nINFO 06-23 04:48:54 [backends.py:155] Directly load the compiled graph(s) for shape None from the cache, took 3.547 s\nINFO 06-23 04:48:55 [monitor.py:34] torch.compile takes 4.86 s in total\nINFO 06-23 04:48:56 [gpu_worker.py:232] Available KV cache memory: 66.87 GiB\nINFO 06-23 04:48:56 [kv_cache_utils.py:716] GPU KV cache size: 547,776 tokens\nINFO 06-23 04:48:56 [kv_cache_utils.py:720] Maximum concurrency for 1,024 tokens per request: 526.71x\nWARNING 06-23 04:48:56 [utils.py:101] Unable to detect current VLLM config. Defaulting to NHD kv cache layout.\nCapturing CUDA graphs:   0%|                                                                                                                                                           | 0/67 [00:00<?, ?it/s]\nERROR 06-23 04:48:56 [core.py:519] EngineCore failed to start.\nERROR 06-23 04:48:56 [core.py:519] Traceback (most recent call last):\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 510, in run_engine_core\nERROR 06-23 04:48:56 [core.py:519]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 394, in __init__\nERROR 06-23 04:48:56 [core.py:519]     super().__init__(vllm_config, executor_class, log_stats,\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 82, in __init__\nERROR 06-23 04:48:56 [core.py:519]     self._initialize_kv_caches(vllm_config)\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 169, in _initialize_kv_caches\nERROR 06-23 04:48:56 [core.py:519]     self.model_executor.initialize_from_config(kv_cache_configs)\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/v1/executor/abstract.py\", line 66, in initialize_from_config\nERROR 06-23 04:48:56 [core.py:519]     self.collective_rpc(\"compile_or_warm_up_model\")\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\nERROR 06-23 04:48:56 [core.py:519]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 06-23 04:48:56 [core.py:519]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/utils.py\", line 2687, in run_method\nERROR 06-23 04:48:56 [core.py:519]     return func(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_worker.py\", line 266, in compile_or_warm_up_model\nERROR 06-23 04:48:56 [core.py:519]     self.model_runner.capture_model()\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_model_runner.py\", line 2213, in capture_model\nERROR 06-23 04:48:56 [core.py:519]     self._dummy_run(num_tokens, capture_attn_cudagraph=full_cg)\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 06-23 04:48:56 [core.py:519]     return func(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_model_runner.py\", line 1969, in _dummy_run\nERROR 06-23 04:48:56 [core.py:519]     outputs = model(\nERROR 06-23 04:48:56 [core.py:519]               ^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\nERROR 06-23 04:48:56 [core.py:519]     return self._call_impl(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\nERROR 06-23 04:48:56 [core.py:519]     return forward_call(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/model_executor/models/llama.py\", line 581, in forward\nERROR 06-23 04:48:56 [core.py:519]     model_output = self.model(input_ids, positions, intermediate_tensors,\nERROR 06-23 04:48:56 [core.py:519]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/compilation/decorators.py\", line 246, in __call__\nERROR 06-23 04:48:56 [core.py:519]     model_output = self.forward(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/model_executor/models/llama.py\", line 368, in forward\nERROR 06-23 04:48:56 [core.py:519]     def forward(\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\nERROR 06-23 04:48:56 [core.py:519]     return self._call_impl(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\nERROR 06-23 04:48:56 [core.py:519]     return forward_call(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 838, in _fn\nERROR 06-23 04:48:56 [core.py:519]     return fn(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 830, in call_wrapped\nERROR 06-23 04:48:56 [core.py:519]     return self._wrapped_call(self, *args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 406, in __call__\nERROR 06-23 04:48:56 [core.py:519]     raise e\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 393, in __call__\nERROR 06-23 04:48:56 [core.py:519]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\nERROR 06-23 04:48:56 [core.py:519]     return self._call_impl(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\nERROR 06-23 04:48:56 [core.py:519]     return forward_call(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"<eval_with_key>.66\", line 337, in forward\nERROR 06-23 04:48:56 [core.py:519]     submod_2 = self.submod_2(getitem_3, s0, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_bnb_shard_offsets, getitem_4, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_bnb_shard_offsets, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_bnb_shard_offsets, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_bnb_shard_offsets, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_bnb_shard_offsets = getitem_4 = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_bnb_shard_offsets = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_bnb_shard_offsets = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_bnb_shard_offsets = None\nERROR 06-23 04:48:56 [core.py:519]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/compilation/cuda_piecewise_backend.py\", line 156, in __call__\nERROR 06-23 04:48:56 [core.py:519]     return entry.runnable(*args)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/compilation/compiler_interface.py\", line 510, in compiled_graph\nERROR 06-23 04:48:56 [core.py:519]     graph_output = inductor_compiled_graph(list_args)\nERROR 06-23 04:48:56 [core.py:519]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/_inductor/output_code.py\", line 460, in __call__\nERROR 06-23 04:48:56 [core.py:519]     return self.current_callable(inputs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/.cache/vllm/torch_compile_cache/dee6dd784e/rank_0_0/inductor_cache/3k/c3kedtjuicpyiyo55z5hejbsjwtnfepkyplcvf6hyoj4zxhhu3pa.py\", line 589, in call\nERROR 06-23 04:48:56 [core.py:519]     torch.ops.vllm.apply_bnb_4bit.default(buf6, arg6_1, arg7_1, buf5)\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/_ops.py\", line 756, in __call__\nERROR 06-23 04:48:56 [core.py:519]     return self._op(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 372, in _apply_bnb_4bit\nERROR 06-23 04:48:56 [core.py:519]     out[:, current_index:current_index + output_size] = matmul_4bit(\nERROR 06-23 04:48:56 [core.py:519]                                                         ^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py\", line 533, in matmul_4bit\nERROR 06-23 04:48:56 [core.py:519]     return MatMul4Bit.apply(A, B, out, bias, quant_state)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/autograd/function.py\", line 575, in apply\nERROR 06-23 04:48:56 [core.py:519]     return super().apply(*args, **kwargs)  # type: ignore[misc]\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py\", line 462, in forward\nERROR 06-23 04:48:56 [core.py:519]     output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\nERROR 06-23 04:48:56 [core.py:519]                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 77.44 MiB is free. Process 1747225 has 7.29 GiB memory in use. Including non-PyTorch memory, this process has 71.81 GiB memory in use. Of the allocated memory 71.01 GiB is allocated by PyTorch, and 73.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nProcess EngineCore_0:\nTraceback (most recent call last):\n  File \"/home/mgoin/.local/share/uv/python/cpython-3.12.4-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/home/mgoin/.local/share/uv/python/cpython-3.12.4-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 523, in run_engine_core\n    raise e\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 510, in run_engine_core\n    engine_core = EngineCoreProc(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 394, in __init__\n    super().__init__(vllm_config, executor_class, log_stats,\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 82, in __init__\n    self._initialize_kv_caches(vllm_config)\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 169, in _initialize_kv_caches\n    self.model_executor.initialize_from_config(kv_cache_configs)\n  File \"/home/mgoin/code/vllm/vllm/v1/executor/abstract.py\", line 66, in initialize_from_config\n    self.collective_rpc(\"compile_or_warm_up_model\")\n  File \"/home/mgoin/code/vllm/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/utils.py\", line 2687, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_worker.py\", line 266, in compile_or_warm_up_model\n    self.model_runner.capture_model()\n  File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_model_runner.py\", line 2213, in capture_model\n    self._dummy_run(num_tokens, capture_attn_cudagraph=full_cg)\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_model_runner.py\", line 1969, in _dummy_run\n    outputs = model(\n              ^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/model_executor/models/llama.py\", line 581, in forward\n    model_output = self.model(input_ids, positions, intermediate_tensors,\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/compilation/decorators.py\", line 246, in __call__\n    model_output = self.forward(*args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/model_executor/models/llama.py\", line 368, in forward\n    def forward(\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 838, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 830, in call_wrapped\n    return self._wrapped_call(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 406, in __call__\n    raise e\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 393, in __call__\n    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<eval_with_key>.66\", line 337, in forward\n    submod_2 = self.submod_2(getitem_3, s0, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_bnb_shard_offsets, getitem_4, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_bnb_shard_offsets, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_bnb_shard_offsets, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_bnb_shard_offsets, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_bnb_shard_offsets = getitem_4 = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_bnb_shard_offsets = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_bnb_shard_offsets = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_bnb_shard_offsets = None\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/compilation/cuda_piecewise_backend.py\", line 156, in __call__\n    return entry.runnable(*args)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/compilation/compiler_interface.py\", line 510, in compiled_graph\n    graph_output = inductor_compiled_graph(list_args)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/_inductor/output_code.py\", line 460, in __call__\n    return self.current_callable(inputs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/.cache/vllm/torch_compile_cache/dee6dd784e/rank_0_0/inductor_cache/3k/c3kedtjuicpyiyo55z5hejbsjwtnfepkyplcvf6hyoj4zxhhu3pa.py\", line 589, in call\n    torch.ops.vllm.apply_bnb_4bit.default(buf6, arg6_1, arg7_1, buf5)\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/_ops.py\", line 756, in __call__\n    return self._op(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 372, in _apply_bnb_4bit\n    out[:, current_index:current_index + output_size] = matmul_4bit(\n                                                        ^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py\", line 533, in matmul_4bit\n    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/autograd/function.py\", line 575, in apply\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py\", line 462, in forward\n    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 77.44 MiB is free. Process 1747225 has 7.29 GiB memory in use. Including non-PyTorch memory, this process has 71.81 GiB memory in use. Of the allocated memory 71.01 GiB is allocated by PyTorch, and 73.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n[rank0]:[W623 04:48:57.557535085 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nTraceback (most recent call last):\n  File \"/home/mgoin/code/vllm/tests/utils.py\", line 741, in wrapper\n    f(*args, **kwargs)\n  File \"/home/mgoin/code/vllm/tests/quantization/test_bitsandbytes.py\", line 159, in test_4bit_bnb_embedding_model\n    with vllm_runner(model_name,\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/tests/conftest.py\", line 787, in __init__\n    self.model = LLM(\n                 ^^^^\n  File \"/home/mgoin/code/vllm/vllm/entrypoints/llm.py\", line 263, in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/engine/llm_engine.py\", line 501, in from_engine_args\n    return engine_cls.from_vllm_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/llm_engine.py\", line 124, in from_vllm_config\n    return cls(vllm_config=vllm_config,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/llm_engine.py\", line 101, in __init__\n    self.engine_core = EngineCoreClient.make_client(\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core_client.py\", line 75, in make_client\n    return SyncMPClient(vllm_config, executor_class, log_stats)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core_client.py\", line 558, in __init__\n    super().__init__(\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core_client.py\", line 422, in __init__\n    self._init_engines_direct(vllm_config, local_only,\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core_client.py\", line 491, in _init_engines_direct\n    self._wait_for_engine_startup(handshake_socket, input_address,\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core_client.py\", line 511, in _wait_for_engine_startup\n    wait_for_engine_startup(\n  File \"/home/mgoin/code/vllm/vllm/v1/utils.py\", line 494, in wait_for_engine_startup\n    raise RuntimeError(\"Engine core initialization failed. \"\nRuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\nFAILED\n\n================================================================================================== FAILURES ==================================================================================================\n___________________________________________________ test_4bit_bnb_embedding_model[half-intfloat/e5-mistral-7b-instruct-quantize embedding model inflight] ____________________________________________________\n\nargs = ()\nkwargs = {'description': 'quantize embedding model inflight', 'dtype': 'half', 'example_prompts': ['vLLM is a high-throughput a...n global economic structures and future business models.\\n', ...], 'hf_runner': <class 'tests.conftest.HfRunner'>, ...}\nSkipped = <class 'Skipped'>, pid = 1747225, pgid = 1747030, _pid = 1747225, _exitcode = 256, old_signal_handler = <Handlers.SIG_DFL: 0>\n\n    @functools.wraps(f)\n    def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> None:\n        # Make the process the leader of its own process group\n        # to avoid sending SIGTERM to the parent process\n        os.setpgrp()\n        from _pytest.outcomes import Skipped\n        pid = os.fork()\n        print(f\"Fork a new process to run a test {pid}\")\n        if pid == 0:\n            try:\n                f(*args, **kwargs)\n            except Skipped as e:\n                # convert Skipped to exit code 0\n                print(str(e))\n                os._exit(0)\n            except Exception:\n                import traceback\n                traceback.print_exc()\n                os._exit(1)\n            else:\n                os._exit(0)\n        else:\n            pgid = os.getpgid(pid)\n            _pid, _exitcode = os.waitpid(pid, 0)\n            # ignore SIGTERM signal itself\n            old_signal_handler = signal.signal(signal.SIGTERM, signal.SIG_IGN)\n            # kill all child processes\n            os.killpg(pgid, signal.SIGTERM)\n            # restore the signal handler\n            signal.signal(signal.SIGTERM, old_signal_handler)\n>           assert _exitcode == 0, (f\"function {f} failed when called with\"\n                                    f\" args {args} and kwargs {kwargs}\")\nE           AssertionError: function <function test_4bit_bnb_embedding_model at 0x75cfc981f9c0> failed when called with args () and kwargs {'model_name': 'intfloat/e5-mistral-7b-instruct', 'description': 'quantize embedding model inflight', 'hf_runner': <class 'tests.conftest.HfRunner'>, 'vllm_runner': <class 'tests.conftest.VllmRunner'>, 'example_prompts': ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the major milestones in the development of artificial intelligence from 1950 to 2020.\\n', 'Compare and contrast artificial intelligence with human intelligence in terms of processing information.\\n', 'Describe the basic components of a neural network and how it can be trained.\\n', 'Write a short story about a robot that dreams for the first time.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', 'Explain the cultural significance of the Mona Lisa painting, and how its perception might vary in Western versus Eastern societies.\\n', \"Translate the following English sentence into Japanese, French, and Swahili: 'The early bird catches the worm.'\\n\"], 'dtype': 'half'}\n\nutils.py:761: AssertionError\n============================================================================================== warnings summary ==============================================================================================\n../../../venvs/vllm/lib/python3.12/site-packages/schemathesis/generation/coverage.py:305\n  /home/mgoin/venvs/vllm/lib/python3.12/site-packages/schemathesis/generation/coverage.py:305: DeprecationWarning: jsonschema.exceptions.RefResolutionError is deprecated as of version 4.18.0. If you wish to catch potential reference resolution errors, directly catch referencing.exceptions.Unresolvable.\n    ref_error: type[Exception] = jsonschema.RefResolutionError,\n\ntests/quantization/test_bitsandbytes.py::test_4bit_bnb_embedding_model[half-intfloat/e5-mistral-7b-instruct-quantize embedding model inflight]\n  /home/mgoin/code/vllm/tests/utils.py:737: DeprecationWarning: This process (pid=1747030) is multi-threaded, use of fork() may lead to deadlocks in the child.\n    pid = os.fork()\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n========================================================================================== short test summary info ===========================================================================================\nFAILED quantization/test_bitsandbytes.py::test_4bit_bnb_embedding_model[half-intfloat/e5-mistral-7b-instruct-quantize embedding model inflight] - AssertionError: function <function test_4bit_bnb_embedding_model at 0x75cfc981f9c0> failed when called with args () and kwargs {'model_name': 'intfloat/e5-mistral-7b-instruct', 'description': 'quantize...\n======================================================================================= 1 failed, 2 warnings in 47.09s =======================================================================================\n```\n\n### \ud83d\udcdd History of failing test\n\nhttps://buildkite.com/vllm/ci/builds/22498/summary/annotations?jid=01979a3a-fc0d-4b8e-96a1-fe70e2d781b8\n\n### CC List.\n\n@jeejeelee ",
    "labels": [
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-06-23T04:53:19+00:00",
    "closed_at": "2025-06-23T13:30:57+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19964/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/19964"
  },
  {
    "number": 18525,
    "title": "[Bug][Failing Test]: V1 - v1/entrypoints/llm/test_struct_output_generate.py",
    "body": "### Your current environment\n\nN/A\n\n### \ud83d\udc1b Describe the bug\n\n`v1/entrypoints/llm/test_struct_output_generate.py::test_structured_output_with_reasoning_matrices` fails on main\n\ne.g. https://buildkite.com/vllm/ci/builds/20477/steps?jid=0196f3e2-128a-409e-bafa-5d676afc9557\n\nStack:\n```\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493] EngineCore failed to start.\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493] Traceback (most recent call last):\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 484, in run_engine_core\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]     engine_core = EngineCoreProc(*args, **kwargs)\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 383, in __init__\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]     super().__init__(vllm_config, executor_class, log_stats,\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 78, in __init__\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]     self._initialize_kv_caches(vllm_config)\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 164, in _initialize_kv_caches\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]     self.model_executor.initialize_from_config(kv_cache_configs)\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py\", line 65, in initialize_from_config\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]     self.collective_rpc(\"compile_or_warm_up_model\")\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]     answer = run_method(self.driver_worker, method, args, kwargs)\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2598, in run_method\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]     return func(*args, **kwargs)\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]            ^^^^^^^^^^^^^^^^^^^^^\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 254, in compile_or_warm_up_model\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]     self.model_runner._dummy_sampler_run(\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]     return func(*args, **kwargs)\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]            ^^^^^^^^^^^^^^^^^^^^^\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1765, in _dummy_sampler_run\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]     dummy_spec_decode_metadata = SpecDecodeMetadata.make_dummy(\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/spec_decode/metadata.py\", line 38, in make_dummy\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]     draft_token_ids_tensor = torch.tensor(flattened_draft_token_ids,\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493]                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493] RuntimeError: CUDA error: an illegal memory access was encountered\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n[2025-05-21T18:59:44Z] ERROR 05-21 11:59:44 [core.py:493] \n[2025-05-21T18:59:44Z] Process EngineCore_0:\n[2025-05-21T18:59:44Z] Traceback (most recent call last):\n[2025-05-21T18:59:44Z]   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n[2025-05-21T18:59:44Z]     self.run()\n[2025-05-21T18:59:44Z]   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n[2025-05-21T18:59:44Z]     self._target(*self._args, **self._kwargs)\n[2025-05-21T18:59:44Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 497, in run_engine_core\n[2025-05-21T18:59:44Z]     raise e\n[2025-05-21T18:59:44Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 484, in run_engine_core\n[2025-05-21T18:59:44Z]     engine_core = EngineCoreProc(*args, **kwargs)\n[2025-05-21T18:59:44Z]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-21T18:59:44Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 383, in __init__\n[2025-05-21T18:59:44Z]     super().__init__(vllm_config, executor_class, log_stats,\n[2025-05-21T18:59:44Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 78, in __init__\n[2025-05-21T18:59:44Z]     self._initialize_kv_caches(vllm_config)\n[2025-05-21T18:59:44Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 164, in _initialize_kv_caches\n[2025-05-21T18:59:44Z]     self.model_executor.initialize_from_config(kv_cache_configs)\n[2025-05-21T18:59:44Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py\", line 65, in initialize_from_config\n[2025-05-21T18:59:44Z]     self.collective_rpc(\"compile_or_warm_up_model\")\n[2025-05-21T18:59:44Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n[2025-05-21T18:59:44Z]     answer = run_method(self.driver_worker, method, args, kwargs)\n[2025-05-21T18:59:44Z]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-21T18:59:44Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2598, in run_method\n[2025-05-21T18:59:44Z]     return func(*args, **kwargs)\n[2025-05-21T18:59:44Z]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-21T18:59:44Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 254, in compile_or_warm_up_model\n[2025-05-21T18:59:44Z]     self.model_runner._dummy_sampler_run(\n[2025-05-21T18:59:44Z]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[2025-05-21T18:59:44Z]     return func(*args, **kwargs)\n[2025-05-21T18:59:44Z]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-21T18:59:44Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 1765, in _dummy_sampler_run\n[2025-05-21T18:59:44Z]     dummy_spec_decode_metadata = SpecDecodeMetadata.make_dummy(\n[2025-05-21T18:59:44Z]                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-21T18:59:44Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/spec_decode/metadata.py\", line 38, in make_dummy\n[2025-05-21T18:59:44Z]     draft_token_ids_tensor = torch.tensor(flattened_draft_token_ids,\n[2025-05-21T18:59:44Z]                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-21T18:59:44Z] RuntimeError: CUDA error: an illegal memory access was encountered\n[2025-05-21T18:59:44Z] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n[2025-05-21T18:59:44Z] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n[2025-05-21T18:59:44Z] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-05-22T04:41:07+00:00",
    "closed_at": "2025-05-22T13:48:59+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18525/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18525"
  },
  {
    "number": 18656,
    "title": "[Bug][Failing Test]: Samplers Test - samplers/test_seeded_generate.py",
    "body": "### Your current environment\n\nN/A\n\n### \ud83d\udc1b Describe the bug\n\n`samplers/test_seeded_generate.py::test_random_sample_with_seed` has been failing on main since #17731\n\nhttps://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests/7615b2b4-ca19-80d3-ab9c-5b2395cd950a?period=7days&tags=scm.branch%3Amain\n\ncc @shadeMe @mgoin @aarnphm \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-05-24T10:22:59+00:00",
    "closed_at": "2025-05-24T15:25:21+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18656/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18656"
  }
]
[
  {
    "number": 8007,
    "title": "[Bug]: vLLM with Neuron performance degrades dramatically if request concurrency is >= max_num_seqs",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\ncollecting environment information...\r\nWARNING 08-29 18:36:46 _custom_ops.py:18] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\r\n/usr/local/lib/python3.10/dist-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\r\nNo module named 'vllm.commit_id'\r\n  from vllm.version import __version__ as VLLM_VERSION\r\nPyTorch version: 2.1.2+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.27.7\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.2.0-1017-aws-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             128\r\nOn-line CPU(s) list:                0-127\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz\r\nCPU family:                         6\r\nModel:                              106\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 32\r\nSocket(s):                          2\r\nStepping:                           6\r\nBogoMIPS:                           5799.87\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd ida arat avx512vbmi pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear flush_l1d arch_capabilities\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          3 MiB (64 instances)\r\nL1i cache:                          2 MiB (64 instances)\r\nL2 cache:                           80 MiB (64 instances)\r\nL3 cache:                           108 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-31,64-95\r\nNUMA node1 CPU(s):                  32-63,96-127\r\nVulnerability Gather data sampling: Unknown: Dependent on hypervisor status\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT Host state unknown\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.25.2\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==8.9.2.26\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-nccl-cu12==2.18.1\r\n[pip3] nvidia-nvjitlink-cu12==12.6.20\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pynvml==11.5.3\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.1.2\r\n[pip3] torch-neuronx==2.1.2.2.2.0\r\n[pip3] torch-xla==2.1.3\r\n[pip3] torchvision==0.16.2\r\n[pip3] transformers==4.44.2\r\n[pip3] transformers-neuronx==0.11.351\r\n[pip3] triton==2.2.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: (0, '+--------+--------+--------+-------------+---------+--------+---------+\\n| NEURON | NEURON | NEURON |  CONNECTED  |   PCI   |  PID   | RUNTIME |\\n| DEVICE | CORES  | MEMORY |   DEVICES   |   BDF   |        | VERSION |\\n+--------+--------+--------+-------------+---------+--------+---------+\\n| 12     | 2      | 32 GB  | 12, 3, 4, 1 | 10:1e.0 | 865036 | 2.21.41 |\\n| 13     | 2      | 32 GB  | 13, 0, 5, 2 | 10:1b.0 | 865036 | 2.21.41 |\\n| 14     | 2      | 32 GB  | 14, 1, 6, 3 | a0:1e.0 | 865036 | 2.21.41 |\\n| 15     | 2      | 32 GB  | 15, 2, 7, 0 | a0:1b.0 | 865036 | 2.21.41 |\\n+--------+--------+--------+-------------+---------+--------+---------+', '')\r\nvLLM Version: 0.5.5@COMMIT_HASH_PLACEHOLDER\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n\r\n```\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen vLLM is used with Neuron, as long as number of concurrent requests are less than `max_num_seqs`, performance is nominal, but if number of concurrent requests is `>= max_num_seqs`, performance degrades dramatically. For example, in one test  with `max_num_seqs=4`, request latency for up to `3` concurrent requests is in the 20 second range, but with `4` concurrent requests, it jumps to over 500 seconds.  Logs show continual preemption, even when `gpu_memory_utilization = 0.9`.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "aws-neuron",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-29T18:44:52+00:00",
    "closed_at": "2024-12-29T02:05:15+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8007/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8007"
  },
  {
    "number": 4553,
    "title": "[Bug]: AssertionError in neuron_model_runner.py assert len(block_table) == 1",
    "body": "### Your current environment\n\nPyTorch version: 2.1.2+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1031-aws-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   48 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          192\r\nOn-line CPU(s) list:             0-191\r\nVendor ID:                       AuthenticAMD\r\nModel name:                      AMD EPYC 7R13 Processor\r\nCPU family:                      25\r\nModel:                           1\r\nThread(s) per core:              2\r\nCore(s) per socket:              48\r\nSocket(s):                       2\r\nStepping:                        1\r\nBogoMIPS:                        5299.99\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       3 MiB (96 instances)\r\nL1i cache:                       3 MiB (96 instances)\r\nL2 cache:                        48 MiB (96 instances)\r\nL3 cache:                        384 MiB (12 instances)\r\nNUMA node(s):                    4\r\nNUMA node0 CPU(s):               0-23,96-119\r\nNUMA node1 CPU(s):               24-47,120-143\r\nNUMA node2 CPU(s):               48-71,144-167\r\nNUMA node3 CPU(s):               72-95,168-191\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.25.2\r\n[pip3] nvidia-nccl-cu12==2.18.1\r\n[pip3] torch==2.1.2\r\n[pip3] torch-neuronx==2.1.2.2.1.0\r\n[pip3] torch-xla==2.1.2\r\n[pip3] torchvision==0.16.2\r\n[pip3] triton==2.1.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: (0, 'instance-type: inf2.48xlarge\\ninstance-id: i-0e6cfafa7d8d7bcfc\\n+--------+--------+--------+-----------+---------+\\n| NEURON | NEURON | NEURON | CONNECTED |   PCI   |\\n| DEVICE | CORES  | MEMORY |  DEVICES  |   BDF   |\\n+--------+--------+--------+-----------+---------+\\n| 0      | 2      | 32 GB  | 11, 1     | 80:1e.0 |\\n| 1      | 2      | 32 GB  | 0, 2      | 90:1e.0 |\\n| 2      | 2      | 32 GB  | 1, 3      | 80:1d.0 |\\n| 3      | 2      | 32 GB  | 2, 4      | 90:1f.0 |\\n| 4      | 2      | 32 GB  | 3, 5      | 80:1f.0 |\\n| 5      | 2      | 32 GB  | 4, 6      | 90:1d.0 |\\n| 6      | 2      | 32 GB  | 5, 7      | 20:1e.0 |\\n| 7      | 2      | 32 GB  | 6, 8      | 20:1f.0 |\\n| 8      | 2      | 32 GB  | 7, 9      | 10:1e.0 |\\n| 9      | 2      | 32 GB  | 8, 10     | 10:1f.0 |\\n| 10     | 2      | 32 GB  | 9, 11     | 10:1d.0 |\\n| 11     | 2      | 32 GB  | 10, 0     | 20:1d.0 |\\n+--------+--------+--------+-----------+---------+', '')\r\nvLLM Version: 0.4.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\n\n### \ud83d\udc1b Describe the bug\n\nWe tried to run tinyllama on inferentia and run into the follow problem\r\n\r\n```text\r\n    |   File \"/home/ubuntu/vllm/vllm/worker/neuron_model_runner.py\", line 72, in _prepare_prompt\r\n    |     assert len(block_table) == 1\r\n    | AssertionError\r\n```\r\n\r\nTo provide some additional info:\r\n\r\nAWS Instance type: `inf2.48xlarge`\r\n\r\nWe followed the instructions in side of this:\r\nhttps://docs.vllm.ai/en/latest/getting_started/neuron-installation.html\r\n\r\nThe offline inference provided in this works:\r\nhttps://docs.vllm.ai/en/latest/getting_started/examples/offline_inference_neuron.html\r\n\r\nHowever, when we try to run the OpenAI compatible server following: https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\r\n\r\nWith some adjustments to the command. The original command is here:\r\n```bash\r\npython -m vllm.entrypoints.openai.api_server --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 --dtype auto --api-key api-key --max-num-seqs 8 --max-model-len 128 --block-size 128\r\n```\r\nThis gives an error complaining about the `--block-size` and won't compile, saying that the only supportable block size is 8, 16 and 32, so we removed that:\r\n\r\n```bash\r\npython -m vllm.entrypoints.openai.api_server --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 --dtype auto --api-key api-key --max-num-seqs 8\r\n```\r\n\r\nNow it compiles, but when we try to run an inference, it responds with Internal Server Error, and on the server, we see the above AssertionError.\r\n\r\nHere's the full stack trace:\r\n\r\n```text\r\nINFO:     127.0.0.1:60772 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\r\nERROR:    Exception in ASGI application\r\n  + Exception Group Traceback (most recent call last):\r\n  |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/_utils.py\", line 87, in collapse_excgroups\r\n  |     yield\r\n  |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/middleware/base.py\", line 190, in __call__\r\n  |     async with anyio.create_task_group() as task_group:\r\n  |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 678, in __aexit__\r\n  |     raise BaseExceptionGroup(\r\n  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\r\n  +-+---------------- 1 ----------------\r\n    | Traceback (most recent call last):\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 411, in run_asgi\r\n    |     result = await app(  # type: ignore[func-returns-value]\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\r\n    |     return await self.app(scope, receive, send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n    |     await super().__call__(scope, receive, send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/applications.py\", line 123, in __call__\r\n    |     await self.middleware_stack(scope, receive, send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n    |     raise exc\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    |     await self.app(scope, receive, _send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/middleware/base.py\", line 189, in __call__\r\n    |     with collapse_excgroups():\r\n    |   File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\r\n    |     self.gen.throw(typ, value, traceback)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/_utils.py\", line 93, in collapse_excgroups\r\n    |     raise exc\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/middleware/base.py\", line 191, in __call__\r\n    |     response = await self.dispatch_func(request, call_next)\r\n    |   File \"/home/ubuntu/vllm/vllm/entrypoints/openai/api_server.py\", line 142, in authentication\r\n    |     return await call_next(request)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/middleware/base.py\", line 165, in call_next\r\n    |     raise app_exc\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/middleware/base.py\", line 151, in coro\r\n    |     await self.app(scope, receive_or_disconnect, send_no_error)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    |     await self.app(scope, receive, send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\n    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    |     raise exc\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    |     await app(scope, receive, sender)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/routing.py\", line 756, in __call__\r\n    |     await self.middleware_stack(scope, receive, send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/routing.py\", line 776, in app\r\n    |     await route.handle(scope, receive, send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/routing.py\", line 297, in handle\r\n    |     await self.app(scope, receive, send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/routing.py\", line 77, in app\r\n    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    |     raise exc\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    |     await app(scope, receive, sender)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/routing.py\", line 72, in app\r\n    |     response = await func(request)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/fastapi/routing.py\", line 278, in app\r\n    |     raw_response = await run_endpoint_function(\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n    |     return await dependant.call(**values)\r\n    |   File \"/home/ubuntu/vllm/vllm/entrypoints/openai/api_server.py\", line 94, in create_chat_completion\r\n    |     generator = await openai_serving_chat.create_chat_completion(\r\n    |   File \"/home/ubuntu/vllm/vllm/entrypoints/openai/serving_chat.py\", line 135, in create_chat_completion\r\n    |     return await self.chat_completion_full_generator(\r\n    |   File \"/home/ubuntu/vllm/vllm/entrypoints/openai/serving_chat.py\", line 298, in chat_completion_full_generator\r\n    |     async for res in result_generator:\r\n    |   File \"/home/ubuntu/vllm/vllm/engine/async_llm_engine.py\", line 663, in generate\r\n    |     raise e\r\n    |   File \"/home/ubuntu/vllm/vllm/engine/async_llm_engine.py\", line 657, in generate\r\n    |     async for request_output in stream:\r\n    |   File \"/home/ubuntu/vllm/vllm/engine/async_llm_engine.py\", line 78, in __anext__\r\n    |     raise result\r\n    |   File \"/home/ubuntu/vllm/vllm/engine/async_llm_engine.py\", line 39, in _raise_exception_on_finish\r\n    |     task.result()\r\n    |   File \"/home/ubuntu/vllm/vllm/engine/async_llm_engine.py\", line 498, in run_engine_loop\r\n    |     has_requests_in_progress = await asyncio.wait_for(\r\n    |   File \"/usr/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\r\n    |     return fut.result()\r\n    |   File \"/home/ubuntu/vllm/vllm/engine/async_llm_engine.py\", line 472, in engine_step\r\n    |     request_outputs = await self.engine.step_async()\r\n    |   File \"/home/ubuntu/vllm/vllm/engine/async_llm_engine.py\", line 214, in step_async\r\n    |     output = await self.model_executor.execute_model_async(\r\n    |   File \"/home/ubuntu/vllm/vllm/executor/neuron_executor.py\", line 89, in execute_model_async\r\n    |     output = await make_async(self.driver_worker.execute_model)(\r\n    |   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    |     result = self.fn(*self.args, **self.kwargs)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    |     return func(*args, **kwargs)\r\n    |   File \"/home/ubuntu/vllm/vllm/worker/neuron_worker.py\", line 87, in execute_model\r\n    |     output = self.model_runner.execute_model(seq_group_metadata_list)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    |     return func(*args, **kwargs)\r\n    |   File \"/home/ubuntu/vllm/vllm/worker/neuron_model_runner.py\", line 176, in execute_model\r\n    |     ) = self.prepare_input_tensors(seq_group_metadata_list)\r\n    |   File \"/home/ubuntu/vllm/vllm/worker/neuron_model_runner.py\", line 152, in prepare_input_tensors\r\n    |     prompt_lens) = self._prepare_prompt(seq_group_metadata_list)\r\n    |   File \"/home/ubuntu/vllm/vllm/worker/neuron_model_runner.py\", line 72, in _prepare_prompt\r\n    |     assert len(block_table) == 1\r\n    | AssertionError\r\n    +------------------------------------\r\n```\r\n\r\n@liangfu Saw that you wrote most of the code here for Neuron. Do you have any clue on this one?",
    "labels": [
      "bug",
      "aws-neuron",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-02T11:11:34+00:00",
    "closed_at": "2024-12-19T02:05:09+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4553/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4553"
  },
  {
    "number": 1866,
    "title": "[RFC] Initial Support for AWS Inferentia",
    "body": "## Proposal\r\n\r\nWe propose to integrate transformers-neuronx to be the execution engine in vLLM for supporting LLM inference on Inferentia. This would require changes on both transformers-neuronx and vLLM.\r\n\r\n### Changes to transformers-neuronx\r\n\r\n1. Support batch size 1 prompt encoding, while share same cache space with max batch size decoding.\r\n2. Support batch-dependent KV cache update. Each sequence will have a specified position_id to update cache.\r\n3. Support virtual dynamic batching. This would enable multi-batch prompt encoding virtually agnostic to vLLM.\r\n\r\n### Changes to vLLM\r\n\r\n- [x] Make CUDA kernel compilation optional, so that when we are trying to perform LLM inference on inf2 instances we don\u2019t necessarily compile the CUDA kernels. Meanwhile, we would still keep CUDA kernel compilation enabled by default. https://github.com/vllm-project/vllm/pull/2065\r\n- [x] Add transformers-neuronx package as a (optional) thirdparty dependency of vllm.  Note that transformers-neuronx would further depend on torch-neuronx, torch-xla, neuronx-cc and many others. https://github.com/vllm-project/vllm/pull/2065\r\n- [x] Configure transformers-neuronx to enable continuous batching feature in vLLM model loader. https://github.com/vllm-project/vllm/pull/2569\r\n- [x] Compile the model after loading weights. https://github.com/vllm-project/vllm/pull/2569\r\n- [x] Execute model with transformers-neuronx. https://github.com/vllm-project/vllm/pull/2569\r\n\r\n## Implementation Details\r\n\r\n### Model-specific (e.g. llama specific code for neuron) forward function\r\n\r\n```python\r\ndef forward(\r\n    self,\r\n    input_ids: torch.Tensor,\r\n    positions: torch.Tensor,\r\n    kv_caches: List[KVCache],\r\n    input_metadata: InputMetadata,\r\n    cache_events: Optional[List[torch.cuda.Event]],\r\n) -> SamplerOutput:\r\n    batch_size, n_active_tokens = input_ids.shape\r\n\r\n    with torch.inference_mode():\r\n        seq_ids = []\r\n        block_size = self.model.context_buckets[-1]\r\n        if input_metadata.num_generation_tokens == 0:\r\n            num_prompts = input_metadata.num_prompts\r\n            seq_ids = torch.zeros(num_prompts, 1, dtype=torch.int64, device='cpu')\r\n            anchor = 0\r\n            for prompt_id in range(num_prompts):\r\n                seq_ids[prompt_id] = input_metadata.slot_mapping[anchor] // block_size\r\n                anchor += input_metadata.prompt_lens[prompt_id]\r\n        else:\r\n            seq_ids = input_metadata.block_tables\r\n\r\n        logits = self.model(input_ids, cache_ids=positions, start_ids=seq_ids)\r\n        next_tokens = self.sampler(logits, input_metadata)\r\n\r\n    return next_tokens\r\n```\r\n\r\n### Model compilation\r\n\r\n```\r\ndef load_weights(self,\r\n                    model_name_or_path: str,\r\n                    cache_dir: Optional[str] = None,\r\n                    load_format: str = \"auto\",\r\n                    revision: Optional[str] = None,\r\n                    **kwargs):\r\n    from transformers_neuronx.llama.model import LlamaForSampling\r\n\r\n    if not os.path.exists(f\"{model_name_or_path}-split\"):\r\n        from transformers.models.llama import LlamaForCausalLM\r\n        from transformers_neuronx.module import save_pretrained_split\r\n\r\n        hf_model = LlamaForCausalLM.from_pretrained(model_name_or_path, low_cpu_mem_usage=True)\r\n        save_pretrained_split(hf_model, f\"{model_name_or_path}-split\")\r\n\r\n    self.model = LlamaForSampling.from_pretrained(f\"{model_name_or_path}-split\", **kwargs)\r\n    self.model.to_neuron()\r\n```\r\n\r\n### Model-agnostic (e.g. generic model loader)\r\n\r\n```python\r\n# Load the weights from the cached or downloaded files.\r\nfrom transformers_neuronx.config import NeuronConfig, ContinuousBatchingConfig\r\n\r\ncontinuous_batching_config = ContinuousBatchingConfig(batch_size_for_shared_caches=scheduler_config.max_num_seqs)\r\nneuron_config = NeuronConfig(continuous_batching=continuous_batching_config)\r\nmodel.load_weights(model_config.model, model_config.download_dir,\r\n                    model_config.load_format, model_config.revision,\r\n                    tp_degree=parallel_config.tp_degree,\r\n                    amp='f32', neuron_config=neuron_config,\r\n                    context_length_estimate=[scheduler_config.max_model_len],\r\n                    n_positions=[scheduler_config.max_model_len],\r\n                    batch_size=scheduler_config.max_num_seqs)\r\n```\r\n\r\n### Related Resources\r\n\r\nStable release versions of transformers-neuronx packages can be found from https://pip.repos.neuron.amazonaws.com/transformers-neuronx/ . We can install transformers-neuronx pacakge with\r\n\r\n```\r\npip install transformers-neuronx --extra-index-url=https://pip.repos.neuron.amazonaws.com\r\n```",
    "labels": [
      "aws-neuron"
    ],
    "state": "closed",
    "created_at": "2023-11-30T15:02:11+00:00",
    "closed_at": "2024-03-02T00:59:00+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1866/reactions",
      "total_count": 18,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 7,
      "confused": 0,
      "heart": 2,
      "rocket": 6,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/1866"
  }
]
[
  {
    "number": 7366,
    "title": "[RFC]: Encoder/decoder models & feature compatibility",
    "body": "## Motivation <a href=\"#user-content-motivation\" id=\"motivation\">#</a>\r\n\r\nThere is significant interest in vLLM supporting encoder/decoder models. Issues #187  and #180 , for example, request encoder/decoder model support. As a result encoder/decoder support was recently introduced to vLLM via the following three PRs:\r\n\r\n* #4837 \r\n* #4888 \r\n* #4942 \r\n\r\nThese three PRs make encoder/decoder model inference possible; however, they leave more to be desired in terms of (1) parity between vLLM's decoder-only & encoder/decoder request processing pipelines with respect to feature support, and (2) the number of encoder/decoder models which are supported.\r\n\r\nThe ask for the vLLM community is to contribute PRs which help bring vLLM encoder/decoder functionality to a similar level of maturity as that of vLLM's decoder-only functionality.\r\n\r\n## Proposed changes <a href=\"#user-content-proposed-changes\" id=\"proposed-changes\">#</a>\r\n\r\nThe support matrix below summarizes which encoder/decoder models have already been added & which features are currently compatible with the vLLM encoder/decoder pipeline, versus which features & models will require additional PRs to implement in the long-term:\r\n\r\n<table>\r\n  <tr>\r\n    <th>Model/feature</th>\r\n    <th>Model is already available/feature is already compatible with encoder-decoder?</th>\r\n    <th>Having this model/making this feature compatible is a long-term goal?</th>\r\n  </tr>\r\n  <tr>\r\n    <td>Encoder/decoder infrastructure</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>BART</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Whisper</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>T5</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Other enc/dec models</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Quantization</td>\r\n    <td><strong><u>Untested</u></strong></td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Multimodality</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Attention backends other than Xformers (esp. flash-attn, flashinfer)</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Custom attention bias support</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>CUDAGraph</td>\r\n    <td>No<br>(Issue #7447)</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Pipeline parallelism</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Speculative decoding</td>\r\n    <td>No</td>\r\n    <td><strong>Low-priority but nice-to-have; difficult.</strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Automatic prefix caching</td>\r\n    <td>No</td>\r\n    <td><strong>Low-priority; difficult.</strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Sliding window</td>\r\n    <td>No</td>\r\n    <td>No</td>\r\n  </tr>\r\n  <tr>\r\n    <td>Chunked prefill</td>\r\n    <td>No</td>\r\n    <td>No</td>\r\n  </tr>\r\n  <tr>\r\n    <td>LoRA</td>\r\n    <td>No</td>\r\n    <td>No</td>\r\n  </tr>\r\n</table>\r\n\r\nThis RFC gives an overview of those features & models which **are not compatible with encoder/decoder currently, but which should be made compatible eventually** (i.e. **No** in the second column, **Yes** in the third column in the support matrix.)\r\n\r\nNote that there are features (automatic prefix caching/sliding window/chunked prefill/LoRA) which are not long-term compatibility goals.\r\n\r\n## Background <a href=\"#user-content-background\" id=\"background\">#</a>\r\n\r\nBefore continuing, it will be helpful to review [the details of the new vLLM encoder/decoder infrastructure](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/infra-enc-dec.md). \r\n\r\nIt will also be helpful to review [this how-to guide](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/how-to.md) for adding new encoder/decoder models & improving encoder/decoder feature compatibility.\r\n\r\n## Initial goal <a href=\"#user-content-initial-goal\" id=\"initial-goal\">#</a>\r\n\r\nMembers of the vLLM contributor community identify models/features in the support matrix above, for which they will work on writing a PR.\r\n\r\n## Detailed long-term goals <a href=\"#user-content-detailed-long-term-goals\" id=\"detailed-long-term-goals\">#</a>\r\n\r\n### Add new models to vLLM <a href=\"#user-content-add-new-models-to-vllm\" id=\"add-new-models-to-vllm\">#</a>\r\n\r\nPlease review the [how-to guide for adding new models to vLLM](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/how-to.md#guide-to-adding-new-encoderdecoder-models-to-vllm)\r\n\r\nSee `tests/models/test_bart.py` for an example of an encoder/decoder model unit test. See `tests/distributed/test_basic_distributed_correctness_enc_dec.py` for an example of an encoder/decoder model test with TP > 1.\r\n\r\n#### Add Whisper model <a href=\"#user-content-add-whisper-model\" id=\"add-whisper-model\">#</a>\r\n\r\nSteps to add support for [Whisper](https://paperswithcode.com/paper/robust-speech-recognition-via-large-scale-1f), a multimodal encoder/decoder speech recognition model:\r\n* [Extend existing vLLM multimodality support to encoder/decoder models](#support-encoderdecoder-multimodality)\r\n* Extend existing vLLM prompt processing pipeline to support audio\r\n* Port [HuggingFace Whisper model](https://github.com/huggingface/transformers/blob/main/src/transformers/models/whisper/modeling_whisper.py) to vLLM; an existing open PR for this workstream is #5964 \r\n* Modify each Whisper layer, where appropriate, to support TP > 1\r\n* Add a Whisper test under `tests/models/`\r\n\r\nProposal: consider whether or not it makes sense to implement encoder/decoder multimodality, audio support, and Whisper in the same PR; that way, the Whisper model may be used to facilitate an end-to-end test with of audio multimodality.\r\n\r\n#### Add T5 model <a href=\"#user-content-add-t5-model\" id=\"add-t5-model\">#</a>\r\n\r\nNote: T5 depends on [custom attention bias being supported](#support-custom-attention-bias) by at least one of the attention backends which [also supports encoder attention & cross-attention](#add-support-for-encoder-attention-and-cross-attention-to-additional-backends); at time of writing no vLLM attention backend fulfills this requirement. The vLLM XFormers attention backend is the only backend which supports encoder/decoder models but neither it nor any other vLLM attention backend supports custom attention bias. (Custom attention bias is required in order to support T5 [relative positional encoding.](#custom-attention-bias-and-relative-positional-encoding))\r\n\r\nSteps to add support for the [T5 model](https://paperswithcode.com/method/t5):\r\n* Port [HuggingFace T5 model](https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py) to vLLM\r\n  * This includes porting over the method which computes the custom attention bias matrix for T5 relative position encoding\r\n* Modify each T5 layer, where appropriate, to support TP > 1\r\n  * The custom attention bias computation must also support TP > 1\r\n* Add a T5 test to `tests/models/`\r\n\r\nNote: T5 was added to an older version of vLLM in #3117 , which could be a helpful starting-point\r\n\r\n#### Add other encoder/decoder models\r\n\r\n* Review open vLLM issues on GitHub and identify other encoder/decoder models which are requested by users\r\n\r\n### Quantization <a href=\"#user-content-quantization\" id=\"quantization\">#</a>\r\n\r\nThe goal of this workstream is to make sure that quantization + encoder/decoder models is fully-tested, and to fill in any gaps (should they exist) in vLLM's support for quantized encoder/decoder models.\r\n\r\nSteps to ensure that vLLM supports encoder/decoder models in combination with all existing vLLM quantization methods:\r\n\r\n* Identify the list of quantization methods which vLLM currently supports with decoder-only models.\r\n* Add unit tests for encoder/decoder models with all of these quantization methods.\r\n* Determine which quantization methods are currently incompatible with vLLM encoder/decoder infrastructure.\r\n* Scope out the effort involved in making these quantization methods compatible & submit a PR making the change.\r\n\r\nvLLM encoder/decoder infrastructure should be compatible with most of the existing vLLM quantization methods, because the specialized quantization kernels are only employed for GEMM operations involving the learned weight matrices ($W_q$, $W_k$, etc.), whereas the encoder/decoder work really only modifies how the `Attention(q, k, v, kv_cache)` layer behaves & does not impact the learned weight matrices at all.\r\n\r\nIt is less clear whether vLLM encoder/decoder infrastructure would be incompatible with FP8. It does appear that a specialized quantized KV cache kernel is employed by the `Attention(q, k, v, kv_cache)` layer when FP8 quantization is employed.\r\n\r\n### Support encoder/decoder multimodality <a href=\"#user-content-support-encoderdecoder-multimodality\" id=\"support-encoderdecoder-multimodality\">#</a>\r\n\r\nTechnically, vLLM already supports multimodality for models which have an \"encoder\" and a \"decoder\", i.e. Llava. However, Llava's decoder does not utilize cross-attention & the model is basically compatible with vLLM's pre-existing decoder-only infrastructure.\r\n\r\nBut critically, for **encoder/decoder models with cross-attention** such as Whisper vLLM does not currently support multimodality of any sort. The processing pipeline does not extract or utilize multimodal data from the input prompt, and the `EncoderDecoderModelRunner` has an assert which fails if the multimodal config is not `None`. Addressing this is what is meant by \"supporting encoder/decoder multimodality\".\r\n\r\nSteps to extend existing vLLM multimodality support to encoder/decoder models:\r\n* Review [existing vLLM multimodality support in the decoder-only pipeline](https://docs.vllm.ai/en/latest/dev/multimodal/multimodal_index.html)\r\n* Scope out a plan for adding encoder/decoder multimodality support.\r\n* Propose & implement one or more multimodal prompt formats for encoder/decoder models\r\n* Integrate multimodality support into encoder/decoder processing pipeline\r\n* Remove the assertion which fails when multimodality is enabled for an encoder/decoder model (see `assert_enc_dec_mr_supported_scenario()` in `vllm/worker/utils.py`)\r\n* Add one or more unit tests with multimodal data\r\n\r\nThere are a number of multimodal encoder/decoder models which will benefit from this feature. One possibility is to add multimodality support & a multimodal model such as [Whisper](#add-whisper-model) in the same PR, so that Whisper may be used to facilitate an end-to-end test with multimodality.\r\n\r\nAnother possibility is to implement multimodality support in its own PR.\r\n\r\n#### Considerations for designing multimodal encoder/decoder prompt formats <a href=\"#user-content-considerations-for-designing-multimodal-encoderdecoder-prompt-formats\" id=\"considerations-for-designing-multimodal-encoderdecoder-prompt-formats\">#</a>\r\n\r\nOne approach to designing the vLLM multimodal encoder/decoder prompt formats, is to consider what we want the user experience to be for high-priority multimodal encoder/decoder models such as\r\n* [Llama 3.1 multimodal](https://github.com/vllm-project/vllm/pull/7258#discussion_r1710915145)\r\n* [Whisper](#add-whisper-model)\r\n\r\n#### Initial proposal for multimodal encoder/decoder prompt formats\r\n\r\nIt may be helpful to review\r\n* [The non-multimodal encoder/decoder prompt formats which are currently supported by vLLM](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/infra-enc-dec.md#supported-encoderdecoder-prompt-formats): singleton prompts (raw text prompt, `TextPrompt`, `TokensPrompt`) as well as `ExplicitEncoderDecoder` prompts\r\n* The multimodal decoder-only prompt formats which are currently supported by vLLM; search for `multi_modal_data` [here](https://github.com/vllm-project/vllm/blob/main/vllm/inputs/data.py) and also review the [vLLM documentation on multimodality](https://docs.vllm.ai/en/latest/dev/multimodal/multimodal_index.html)\r\n\r\nGenerally speaking, in encoder/decoder models based on cross-attention, the non-text input modality is passed to the encoder as input. Conversely, any text prompt is typically passed to the decoder as a input prompt.\r\n\r\nThe following two encoder/decoder multimodal prompt formats are tentatively proposed:\r\n\r\n* Singleton `TextPrompt` with `multi_modal_data` field\r\n    * vLLM will extract the `multi_modal_data` and pass it to the encoder module\r\n    * vLLM will extract the prompt text, tokenize it and pass the token-list to the *decoder* (note that this is the opposite of vLLM behavior for non-multimodal prompts, where the prompt text would be passed to the encoder.)\r\n\r\n    For example passing the `TextPrompt` below to vLLM BART\r\n\r\n    ```\r\n    TextPrompt(\r\n        'prompt': \"The rain in spain falls mainly on the\",\r\n        'multi_modal_data': <multi modal data structure>\r\n    )\r\n    ```\r\n\r\n    results in\r\n\r\n    ```\r\n    Encoder input: <multi modal data structure>\r\n    Decoder prompt: \"The rain in spain falls mainly on the\"\r\n    ```\r\n\r\n* Singleton `TokensPrompt` with `multi_modal_data` field\r\n    * vLLM will extract the `multi_modal_data` and pass it to the encoder module\r\n    * vLLM will extract the token list and pass it unmodified to the *decoder* (note that this is the opposite of vLLM behavior for non-multimodal prompts, where the prompt tokens would be passed to the encoder.)\r\n\r\n    For example passing the `TokensPrompt` below to vLLM BART\r\n\r\n    ```\r\n    TokensPrompt(\r\n        'prompt_tokens': [2,0,171,5,2],\r\n        'multi_modal_data': <multi modal data structure>\r\n    )\r\n    ```\r\n\r\n    results in\r\n\r\n    ```\r\n    Encoder prompt: <multi modal data structure>\r\n    Decoder prompt: [2,0,171,5,2]\r\n    ```\r\n\r\nIt may also be worth considering whether or how to support\r\n* `ExplicitEncoderDecoderPrompt`s with multimodality\r\n* An input prompt format which encapsulates *only* multimodal encoder inputs, with no associated decoder text/tokens prompt (this would result in the decoder being passed a \"default\" or empty prompt.)\r\n\r\n### Add support for encoder attention and cross-attention to additional backends <a href=\"#user-content-add-support-for-encoder-attention-and-cross-attention-to-additional-backends\" id=\"add-support-for-encoder-attention-and-cross-attention-to-additional-backends\">#</a>\r\n\r\nAt time of writing, XFormers is the only vLLM attention backend which supports encoder attention & cross-attention. \r\n\r\nThe goal of this workstream would be to extend encoder attention & cross-attention support to additional backends, the highest-priority being flash-attention and flashinfer.\r\n\r\nReviewing [encoder attention and cross-attention support in the XFormers backend would be a good starting-point](https://github.com/vllm-project/vllm/blob/main/vllm/attention/backends/xformers.py) for extending support to other models.\r\n\r\nFor context on the requirements for a backend to support encoder and cross-attention, it may help to review the [encoder/decoder architecture](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/infra-enc-dec.md#encoderdecoder-architecture-diagram-prefill--and-decode-phase), the [way that attention masks are currently constructed in the XFormers backend](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/infra-enc-dec.md#default-encoderdecoder-attention-bias-or-mask), and the [recommended architecture for vLLM encoder/decoder models](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/how-to.md#25-optional-but-strongly-recommended-implement-the-following-encoderdecoder-model-architecture).\r\n\r\nA summary of the key changes required for an attention backend to support encoder attention and cross-attention:\r\n* The backend's `AttentionMetadata` subclass must support fields for encoder sequence lengths, encoder sequence token count, cross-attention blocktables, and cross-attention slot mapping. XFormers examples:\r\n  * [`AttentionMetadata` subclass' encoder field declarations](https://github.com/vllm-project/vllm/blob/67abdbb42fdbb59c274130368981c0d0ac3539e3/vllm/attention/backends/xformers.py#L127-L140)\r\n  * [Handle encoder & cross-attention fields in `prefill_metadata()` method](https://github.com/vllm-project/vllm/blob/67abdbb42fdbb59c274130368981c0d0ac3539e3/vllm/attention/backends/xformers.py#L216-L221)\r\n  * [Handle encoder & cross-attention fields in `decode_metadata()` method](https://github.com/vllm-project/vllm/blob/67abdbb42fdbb59c274130368981c0d0ac3539e3/vllm/attention/backends/xformers.py#L255-L260)\r\n* The `forward()` method of the backend implementation must accept an `attn_type` argument of type `AttentionType`, which allows choosing between encoder attention, decoder attention, or encoder/decoder cross-attention. [XFormers example](https://github.com/vllm-project/vllm/blob/67abdbb42fdbb59c274130368981c0d0ac3539e3/vllm/attention/backends/xformers.py#L447)\r\n* The backend implementation must recognize which option has been chosen for `attn_type`, and adjust accordingly in terms of (1) how it utilizes `attn_metadata` when invoking the attention kernels (review [XFormers `forward()`](https://github.com/vllm-project/vllm/blob/67abdbb42fdbb59c274130368981c0d0ac3539e3/vllm/attention/backends/xformers.py#L438) for context), and (2) the choice of causal or non-causal attention, as well the choice of attention mask shape ([XFormers example](https://github.com/vllm-project/vllm/blob/67abdbb42fdbb59c274130368981c0d0ac3539e3/vllm/attention/backends/xformers.py#L689-L714)).\r\n\r\n#### Initial goals\r\n* Identify the changes required to add encoder attention & cross-attention support to flash-attention and flashinfer\r\n* PR the required changes\r\n  * Remove/modify any asserts which fail if the vLLM attention backend is not XFormers\r\n    * Currently, [the `__init__()` method of `EncoderDecoderModelRunner`](https://github.com/vllm-project/vllm/blob/b4e9528f9569d6eb8c29624771a4058fe794cb5a/vllm/worker/enc_dec_model_runner.py#L95) invokes a method `EncoderDecoderModelRunner._maybe_force_supported_attention_backend()` defined [here](https://github.com/vllm-project/vllm/blob/b4e9528f9569d6eb8c29624771a4058fe794cb5a/vllm/worker/enc_dec_model_runner.py#L112-L144) which (1) attempts to force encoder/decoder models to use XFormers attention backend, and (2) raises an exception if the user has overridden the attention backend to be anything other than XFormers. \r\n\r\n#### Long-term goals\r\n* All vLLM attention backends support encoder attention and cross-attention\r\n\r\n### Support custom attention bias <a href=\"#user-content-support-custom-attention-bias\" id=\"support-custom-attention-bias\">#</a>\r\n\r\nNote: [T5](#add-t5-model) takes a dependency on custom attention bias. Custom attention bias is likely complex enough to merit its own PR.\r\n\r\nNote: custom bias support was added to `PagedAttention` in an older version of vLLM as part of #3117 ; given changes in vLLM since then, additional work would be required to integrate this implementation.\r\n\r\n#### Custom attention bias and relative positional encoding\r\n\r\nAttention bias refers to adding a matrix $A$ to the scaled dot-product (SDP) attention scores matrix before performing softmax, as shown below:\r\n\r\n$$\r\nattn(Q,K,V,A) = softmax(\\frac{Q K^T + A}{\\sqrt{d}})V\r\n$$\r\n\r\nHere, *custom* attention bias is understood to mean that the vLLM attention backend allows $A$ to be an arbitrary matrix, provided the tensor dimensions are commensurate with the shape of the SDP attention scores matrix. This is in contrast to the existing vLLM attention backend implementations, which can only accommodate simple block-diagonal causal or non-causal masks which are uniformly either $0$ or $-\\infty$. \r\n\r\nThere are broadly two possible approaches to custom attention bias, which do not necessarily have to be mutually-exclusive:\r\n* $A$ is a fully-materialized attention bias matrix passed to the attention backend\r\n* $A$ is computed on-the-fly by the attention kernel, using an element-wise formula for the attention bias which is fused with the $Q K^T$ and $softmax$ computations\r\n\r\nT5 employs custom attention bias in order to implement [relative positional encoding](https://jaketae.github.io/study/relative-positional-encoding/#bridging-shaw-and-huang), wherein pairwise positional relationships between tokens are represented by the bias matrix. The HuggingFace Transformers T5 implementation provides an example of [how the relative positional encoding matrix is computed](https://github.com/huggingface/transformers/blob/c1aa0edb48217f416f4bbe6e3a9db1500284513b/src/transformers/models/t5/modeling_t5.py#L428).\r\n\r\n#### Existing attention bias support\r\n\r\n*Currently, no vLLM attention backend fully supports passing in a custom attention bias*. This is primarily due to underlying kernel limitations. For example, the [xFormers `memory_efficient_attention_forward` kernel](https://facebookresearch.github.io/xformers/components/ops.html) is the only NVIDIA-GPU-oriented kernel which permits passing in an arbitrary PyTorch tensor as a materialized attention bias (via the `attn_bias` argument) (at time of writing I have not investigated if custom attention bias is supported by any of the kernels for AMD GPU, CPU, etc.) Regardless, vLLM only employs xFormers `memory_efficient_attention_forward` for prefill; to my knowledge, none of the decode-phase kernels employed by vLLM can accept an arbitrary tensor as a custom attention bias, making custom attention bias impossible to apply end-to-end for both prefill and decode under the current vLLM implementation.\r\n\r\nIn addition to lack of kernel-level support for custom attention bias, most vLLM backends also prevent passing a custom attention bias matrix to the underlying kernel. The exception is the XFormers backend, which accepts an attention bias via `XFormersMetadata.attn_bias` attribute (however the XFormers backend only utilizes `attn_bias` in the prefill phase.)\r\n\r\n#### Proposed methods for supporting custom attention bias\r\n\r\nHere the following two approaches for supporting custom attention bias in vLLM are proposed:\r\n* **Fully-materialized bias matrix:** Modify vLLM attention backends to accept an arbitrary PyTorch tensor, passed into the backend via the `AttentionMetadata.attn_bias` field.\r\n* **On-the-fly/fused bias matrix computation:** Enable an efficient workflow whereby vLLM developers can tweak an attention kernel to compute the custom attention bias on the fly\r\n  * For example: rather than computing the T5 relative position encoder bias matrix once, instead the attention kernel can fuse the element-wise bias matrix formula with the $Q K^T$ and $softmax()$. The attention bias matrix is never fully materialized.\r\n  * [FlexAttention](https://pytorch.org/blog/flexattention/#relative-position-encodings) enables fused custom attention bias computations in a FlashAttention-style kernel, using torch.compile.\r\n\r\n![image](https://pytorch.org/assets/images/flexattention/fg4.png)\r\n\r\nIt may make sense to support one or both of these methods.\r\n\r\nNote that custom attention bias support must be added on a backend-by-backend basis, because of the kernel modifications & backend logic changes required.\r\n\r\n#### Initial goals for introducing custom attention bias support\r\n\r\n1. Focus on a particular vLLM attention backend\r\n  * Suggestion: focus on an attention backend which also supports encoder/decoder models, in order to facilitate [running T5](#add-t5-model). At time of writing, XFormers is the only backend which supports encoder/decoder models, however there will likely be work on [supporting encoder/decoder in additional attention backends.](#add-support-for-encoder-attention-and-cross-attention-to-additional-backends)\r\n2. Scope out the effort involved in introducing custom attention bias support to this backend\r\n3. Some steps which will likely be involved in introducing custom attention bias support:\r\n  * Augment attention backend's kernels to accept custom attention bias; for example, the PagedAttention kernel (for XFormers backend), the Flash-attention kernel (for the flash-attn backend), or the Flashinfer kernels (for the Flashinfer backend)\r\n  * (Except for XFormers) add an `attn_bias` attribute to attention backend's `AttentionMetadata` subclass\r\n  * Ensure that the attention backend passes the `attn_bias` attribute to both the prefill and decode kernels\r\n4. Add at least two custom attention bias unit tests (for prefill & decode respectively)\r\n\r\n#### Final goals for introducing custom attention bias support\r\n\r\n* All vLLM attention backends should support custom attention bias, with unit tests\r\n\r\n#### Some links which may be helpful for understanding how causal & non-causal attention masks are currently configured in vLLM:\r\n\r\n* [Invocation of flash-attention for prefill in vLLM backend, using `causal` flag](https://github.com/vllm-project/vllm/blob/db35186391a2abfc6c91d703527dac20d2488107/vllm/attention/backends/flash_attn.py#L522)\r\n\r\n* [Invocation of xFormers attention kernel for prefill in vLLM backend, using `BlockDiagonalMask` and `BlockDiagonalCausalMask`](https://github.com/vllm-project/vllm/blob/db35186391a2abfc6c91d703527dac20d2488107/vllm/attention/backends/xformers.py#L689-L738)\r\n\r\n* [Invocation of FlashInfer attention kernel for prefill in backend, using `causal` flag](https://github.com/vllm-project/vllm/blob/db35186391a2abfc6c91d703527dac20d2488107/vllm/attention/backends/flashinfer.py#L539)\r\n\r\n* [Invocation of PagedAttention kernel for decode in vLLM backend](https://github.com/vllm-project/vllm/blob/db35186391a2abfc6c91d703527dac20d2488107/vllm/attention/backends/xformers.py#L628)\r\n\r\n* [Invocation of FlashInfer kernel for decode in vLLM backend](https://github.com/vllm-project/vllm/blob/db35186391a2abfc6c91d703527dac20d2488107/vllm/attention/backends/flashinfer.py#L543)\r\n\r\n### Support CUDAGraph with encoder/decoder models <a href=\"#user-content-support-cudagraph-with-encoderdecoder-models\" id=\"support-cudagraph-with-encoderdecoder-models\">#</a>\r\n\r\nNote: this topic is being tracked by Issue #7447\r\n\r\nSteps to support CUDAGraph with encoder/decoder models:\r\n* Scope out the effort require to support CUDAGraph with encoder/decoder models\r\n* Write a PR for CUDAGraph + encoder/decoder\r\n  * Remove the assertion which fails when CUDAGraph is enabled for an encoder/decoder model (see `assert_enc_dec_mr_supported_scenario()` in `vllm/worker/utils.py`)\r\n\r\n### Support pipeline-parallelism with encoder/decoder models <a href=\"#user-content-support-pipeline-parallelism-with-encoderdecoder-models\" id=\"support-pipeline-parallelism-with-encoderdecoder-models\">#</a>\r\n\r\nSteps to support pipeline-parallelism with encoder/decoder models:\r\n* Scope out the effort required to support pipeline-parallelism with encoder/decoder models\r\n* Write a PR for pipeline-parallelism + encoder/decoder\r\n  * Remove the assertion which fails when pipeline-parallelism is enabled for an encoder/decoder model (see `assert_enc_dec_mr_supported_scenario()` in `vllm/worker/utils.py`)\r\n\r\n### Support multi-step scheduling with encoder/decoder models <a href=\"#user-content-support-multi-step-scheduling-with-encoder-decoder-models\" id=\"support-multi-step-scheduling-with-encoder-decoder-models\">#</a>\r\n\r\nNote: depends on #7000 landing in order to add multi-step scheduling support; it may be helpful to review the multi-step scheduling RFC ( #6854 )\r\n\r\nSteps to support multi-step scheduling with encoder/decoder models:\r\n* Scope out the effort required to support multi-step scheduling\r\n  * `EncoderDecoderModelRunner` multi-step support\r\n* Write a PR for multi-step scheduling + encoder/decoder\r\n* Write at least one test of an encoder/decoder model with multi-step scheduling\r\n\r\n### Low-priority high-effort tasks <a href=\"#user-content-low-priority-high-effort-tasks\" id=\"low-priority-high-effort-tasks\">#</a>\r\n\r\n* Speculative decoding\r\n* Automatic prefix caching\r\n\r\nHere it is proposed that these features are low-priority. Adding support for speculative decoder and automatic prefix caching would require a significant of effort to scope out and design the implementations.\r\n\r\nNote that adding support for either of these features would require removing the assertions which fail when speculative decoding or automatic prefix caching are enabled for an encoder/decoder model (see `assert_enc_dec_mr_supported_scenario()` in `vllm/worker/utils.py`)\r\n\r\n### Feedback Period.\r\n\r\nClosed.\r\n\r\n### CC List.\r\n\r\n@WoosukKwon \r\n@robertgshaw2-neuralmagic \r\n@mgoin \r\n@tms\r\n@njhill \r\n@sroy745 \r\n@ywang96 \r\n@DarkLight1337 \r\n@js8544 \r\n\r\n### Any Other Things.\r\n\r\n_No response_",
    "labels": [
      "RFC",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-08-09T15:03:54+00:00",
    "closed_at": null,
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7366/reactions",
      "total_count": 14,
      "+1": 14,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7366"
  },
  {
    "number": 16268,
    "title": "[RFC]: TPU V1 Sampler planning",
    "body": "### Motivation.\n\nI'd like to gather some input on how to move forward with sampling support, and also provide a brief recap of the current state+planned support.\n\nAt a high level, the current design splits model forward and sampling into two separate graphs. \nAs of now (`f2ebb6f54`) only the `temperature` and `min_p` have been intentionally enabled. \nAs more techniques will be added, the sampling graph will grow in size (vertically, sequential ops) and performance may need monitoring, as we're simply evaluating more operations at runtime. \nTo clarify, even when one option is not enabled, we still evaluate a no-op version that undergoes the same ops in the graph (eg top-p with p=1).\n\n### Proposed Change.\n\nFollowing https://github.com/vllm-project/vllm/pull/15489 a few concerns that have been raised regarding performance while enabling topk,  hence adding the **very first** op to the initial sampling graph, I'd like to re-evaluate the current approach.\nLooking at the opposite side of the spectrum one could ideally provide a sampling graph for each combination of parameters. \nWhile this is unfeasible due to the number of parameters that sampling needs to support, one approach \"in the middle\" includes pre-compiling a set of common sampling params while routing requests to the \"correct\" one.\nThe main issue I see here is batching, as every request may potentially specify different sampling params, either we identify the superset for the current batch and then route to the corresponding graph, or each request is executed on a separate graph, which I believe would hurt performance even more. With that said, I still think most request will fall into the temperature only \"bucket\", followed by the topk/topp one, so one could implement the most popular routes. I have no production data to back this assertion with though, so don't quote me on that.\n\nI think this is the main point to clarify before moving on and expand the number of supported parameters.\n\nPlease note the above is based on the assumption that latency is indeed going up. To clear out any such doubts, I think the PR https://github.com/vllm-project/vllm/pull/16022 will go a long way to allow easy benchmarking of sampling parameters.\n\n_____\nMoving forward, I've compiled a list of parameters to support along with the effort needed to implement + my own suggestions. \nWe can also use it to track progress.\n - [x] temperature/min_p\n - [x] topk/topp we already have an implementation for https://github.com/vllm-project/vllm/pull/15489 (topk). \n - [x] logprobs/`sampling_metadata.max_num_logprobs`: Similarly to what we do in other parts, we need to compile for different `max_num_logprobs` values as the output is Bx`max_num_logprobs` (+there's a `torch.topk` call), unless we fix it to some arbitrary value.  https://github.com/vllm-project/vllm/pull/17072\n - [ ] `sampling_metadata.prompt_token_ids` for penalties. This should be fine as is given we're already compiling for different (padded) input sizes. Just it can't be optional or None/with value count as different inputs.\n - [ ] `sampling_metadata.output_token_ids` for penalties. It's already converted into a padded tensor.\n - [ ] penalties:\n \t- `get_token_bin_counts_and_mask` uses a scatter_add op that *may* be slow on TPU\n \t- *penalties tensors are already of shape `num_seqs`, which we pre-compile so they're fine.\n \t- there are multiple lines where the a tensor is silced with in value-dependent way (recompilation risk) `logits[logits > 0]`. We can probably replace with a `masked_fill`. \n- [ ] `sampling_metadata.min_tokens` penalty must be re-implemented and vectorized (it uses a for on input dict now, graph would be dynamic). I am less familiar with this implementation so tbd.\n- [ ] `sampling_metadata.logit_bias`, current interface needs to be rethought because it can introduce dynamism. We could create a BxV\n matrix (B padded+precompiled) to pack the preferences from the list[dicts]. This would work but obviously the factor of expansion can be quite\n big (eg downgrading a single token would materialize a who BxV matrix). Alternatively we could provide different pre-compiled values for V (2, 4..) \n at the cost of increased complexity and longer compilation time. Also, current cuda impl is highly unoptimized.\n- [ ]  `sampling_metadata.allowed_token_ids_mask` is fine as is, no effort required imo. Just it can't be None on a single graph.\n- [ ]  `sampling_metadata.bad_words_token_ids` probably better to support the more general `logit_bias` option.\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n@robertgshaw2-redhat @yaochengji @alexm-redhat @mgoin @bvrockwell @hyeygit @lsy323 \n\n### Any Other Things.\n\nUPDATE: this is very much related to https://github.com/vllm-project/vllm/pull/13360.\n \n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "open",
    "created_at": "2025-04-08T14:24:38+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16268/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16268"
  },
  {
    "number": 13361,
    "title": "[RFC]: Deprecation of the `best_of` Sampling Parameter in vLLM V1",
    "body": "### Motivation.\n\n### Overview\nAs we transition to vLLM V1, we plan to discontinue support for the `best_of` sampling parameter. This decision is driven by a combination of low usage, alignment with industry trends, and a desire for system simplicity and performance.\n\n### Background: What is `best_of`?\nThe `best_of` parameter was originally part of the earlier OpenAI completion API. It enabled the generation of multiple completions\u2014`n` different outputs\u2014then selected the \u201cbest\u201d completion based on the cumulative log probabilities of each result.\n\n### Reasons for Deprecation\n\n1. **Limited Usage and Industry Trends:**\n   - **Low Adoption:** To the best of our knowledge, the `best_of` feature is used by very few users. Users have observed that output quality isn\u2019t reliably correlated with their log probabilities in most cases.\n   - **Evolving Standards:** Major AI providers such as OpenAI (in its current API), Claude, and Gemini have moved away from including the `best_of` option.\n\n2. **Alternative Methods:**\n   - Users can implement `best_of` by leveraging the `n` parameter to obtain multiple completions and the `logprobs` parameter for the log probability of each generated token. This method effectively replicates the behavior of `best_of` without requiring dedicated support.\n\n3. **System Simplification and Performance:**\n   - Supporting `best_of` introduces additional complexity, as it necessitates tracking cumulative log probabilities for each generated completion. This extra overhead runs counter to our focus on performance and streamlined design.\n\n### Proposed Change.\n\nIn light of the minimal usage, the availability of alternative methods, and our commitment to a simpler, more efficient system, we plan to phasing out the `best_of` parameter with vLLM V1. Users who wish to mimic its functionality can continue to do so by generating multiple completions and comparing their log probabilities directly.\n\nPlease let us know if this change impacts your usage or if you have any other concerns.\n\n### Feedback Period.\n\n2 weeks.\n\n### CC List.\n\n_No response_\n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "closed",
    "created_at": "2025-02-16T17:57:03+00:00",
    "closed_at": "2025-03-05T20:22:45+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13361/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/13361"
  },
  {
    "number": 14374,
    "title": "[RFC]: Drop Support for OpenVINO",
    "body": "### Motivation.\n\nOpenVINO backend was initially integrated as an alternatively to the CPU backend and has branched out the vLLM execution logic for every levels (executor, model runner, and attention backend). #5377\n\nOver the last 9 months, we have been the following\n* Relatively low usage as reported in Github Issues and Slack discussions\n* The Intel CPU codepath is more mature and largely compatible for Arm as well. \n* The OpenVINO code path complicated with codebase\n* CI and build became difficult to maintain\n\nI would like to propose to move OpenVINO off from the main codebase, and transition to a vLLM out of tree platform plugin if desired. OpenVINO can follow the same approach as Ascend and Spyre with the plugin approach #11162 \n\n\n\n### Proposed Change.\n\n* Remove OpenVINO codepath, build and test. \n* Optionally, create vllm-project/vllm-openvino if the developers want to maintain plugin level compatibility. \n\n### Feedback Period.\n\n2 weeks. By March 20. \n\n### CC List.\n\ncc @ilya-lavrenov @WoosukKwon @youkaichao @robertgshaw2-redhat @mgoin \n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "closed",
    "created_at": "2025-03-06T17:34:37+00:00",
    "closed_at": "2025-03-22T21:06:40+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14374/reactions",
      "total_count": 9,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 4,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14374"
  },
  {
    "number": 7247,
    "title": "[RFC]: Initial support for RBLN NPU",
    "body": "### Motivation.\r\n\r\nThe [RBLN SDK](https://rebellions.ai/wp-content/uploads/2024/08/WhitePaper_Issue2_ATOM_SoftwareStack.pdf) provides a solution for innovative deep learning inference on Rebellion's NPUs, such as [ATOM](https://rebellions.ai/wp-content/uploads/2024/07/ATOMgenAI_white-paper.pdf) and REBEL, including support for large language models (LLMs). This project aims to develop the RBLN backend for vLLM, initially prioritizing the ATOM device, with future plans to enable REBEL support.\r\n\r\nIn alignment with Rebellion's Optimum Huggingface extension [documentation](https://docs.rbln.ai/software/optimum/optimum_rbln.html), RBLN backend will support a wide range of models available in the [Rebellion's Model Zoo](https://rebellions.ai/developers/model-zoo/).\r\n\r\nThe project currently incorporates continuous batching feature and will soon integrate additional techniques, such as PagedAttention, to enhance performance further.\r\n\r\n### Proposed Change.\r\n\r\nIntroduce the RBLN vLLM backend, which will:\r\n- Load models via the optimum-rbln extension for Hugging Face ([documentation](https://docs.rbln.ai/software/optimum/optimum_rbln.html))\r\n- Implement custom RBLN model loader, model runner, model executor, and model worker classes that integrate seamlessly with the vLLM architecture.\r\n- Provide a new `requirements.txt` for the RBLN environment.\r\n\r\n#### Target  Models\r\nWe will start by ensuring vLLM works with the Llama architecture and expand to other architectures. The full list of LLMs supported by RBLN can be viewed [here](https://rebellions.ai/developers/model-zoo/).\r\n\r\n#### Design\r\nWe will introduce several custom classes that align with the vLLM architecture for heterogeneous accelerators (such as Neuron, XPU, TPU...). See the diagram below for details.\r\n![image](https://github.com/user-attachments/assets/b88f21fc-1abd-4a42-89cd-cc1d89dfb9c0)\r\n\r\n#### Implementation Details\r\n\r\n##### Initalize model\r\n```python\r\ndef init_model(self) -> None:\r\n    config = self.model_config.hf_config\r\n    model_name_or_path = self.model_config.model\r\n    model_name, model_cls_name = get_rbln_model_info(config)\r\n\r\n    # huggingface model class\r\n    model_cls = getattr(optimum.rbln, model_cls_name)\r\n    assert model_cls is not None\r\n    # load RBLN compiler binary model\r\n    model = model_cls.from_pretrained(model_name_or_path, export=False)\r\n    self.model = model\r\n```\r\n\r\n##### Model-specific (e.g. llama specific) forward functions\r\n```python\r\nclass RBLNOptimumRBLNLlamaForCausalLM(RBLNBaseLlamaForCausalLM):\r\n    def forward(\r\n        self,\r\n        input_ids: torch.Tensor,\r\n        attn_mask: torch.Tensor,\r\n        positions: torch.Tensor,\r\n        seq_group_metadata_list: List[SequenceGroupMetadata],\r\n        use_cache: Optional[bool] = None,\r\n        output_attentions: Optional[bool] = None,\r\n        output_hidden_states: Optional[bool] = None,\r\n        return_dict: Optional[bool] = None,\r\n    ) -> torch.Tensor:\r\n        is_prefill = seq_group_metadata_list[0].is_prompt\r\n        if not is_prefill:\r\n            input_ids, positions = self.preprocess_decode(\r\n                input_ids, positions, seq_group_metadata_list)\r\n        batch_indices = self.get_batch_indices(seq_group_metadata_list)\r\n        batch_idx = batch_indices[0] if is_prefill else None\r\n        # optimum.rbln RBLNLlamaForCausalLM.forward()\r\n        logits = self.model.forward(input_ids=input_ids.to(torch.int64),\r\n                                    cache_position=positions.to(torch.int32),\r\n                                    batch_idx=batch_idx)\r\n        if not is_prefill:\r\n            logits = self.postprocess_decode(logits, seq_group_metadata_list)\r\n        return logits\r\n```\r\n\r\n#### References\r\n- About rebellions: https://rebellions.ai/\r\n- Developer page: https://docs.rbln.ai/\r\n- Model zoo: https://rebellions.ai/developers/model-zoo/\r\n- Model zoo (github) : https://github.com/rebellions-sw/rbln-model-zoo\r\n\r\n\r\n\r\n### Feedback Period.\r\n\r\n1w\r\n\r\n### CC List.\r\n\r\n@WoosukKwon , @rebel-shshin, @rebel-hekim, @rebel-hongseok\r\n\r\n### Any Other Things.\r\n\r\n- In the near future, we plan to support \r\n   - distributed inference with Ray for the RBLN backend. This will include implementing a custom distributed executor.\r\n   - PagedAttention with custom operator implementation. ",
    "labels": [
      "RFC",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-07T05:28:24+00:00",
    "closed_at": "2025-03-19T02:05:27+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7247/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7247"
  },
  {
    "number": 3620,
    "title": "[RFC] Initial Support for Cloud TPUs",
    "body": "# Progress\r\n\r\n- [x] Implement TPU executor that works on a single TPU chip (without tensor parallelism) #5292 \r\n- [x] Support single-host tensor parallel inference #5871 \r\n- [x] Support multi-host tensor parallel inference #7457 \r\n- [ ] Support INT8 quantization\r\n- [x] Support MoE models such as Mixtral #6457\r\n- [ ] Benchmark and optimize the TPU backend performance\r\n\r\n# Project Scope\r\n\r\nThis project focuses on making vLLM compatible with Google cloud TPUs. Our goal is seamless integration so users can easily run vLLM on TPUs for both online and offline inference. We will target common setups, like popular models such as Gemma, using the bfloat16 data type.\r\n\r\n## Target TPUs and Models\r\n\r\nWe will focus on the most recent generations of TPUs, namely **TPU v4, v5e, and v5p**, considering their superior performance to previous generations. We will start by making sure vLLM works with dense models such as Gemma. After that, we will expand support to Mixture-of-Experts (MoE) models such as Mixtral.\r\n\r\n## Features Not Included (for now)\r\nThe following features are outside the scope of this initial project, but we'd like to tackle them in the future:\r\n\r\n- Speculative decoding\r\n- GPTQ/AWQ Quantization\r\n- Multi-LoRA serving\r\n\r\n# Design\r\n\r\n## Overview\r\n\r\n<img width=\"1042\" alt=\"Screenshot 2024-03-25 at 10 43 50\u202fAM\" src=\"https://github.com/vllm-project/vllm/assets/46394894/74033fa2-5472-439b-bdc7-568a6ba169d2\">\r\n\r\nTo integrate the TPU backend into vLLM, we will add the new TPU executor and TPU worker which are counterparts of the GPU executor and GPU worker, respectively. Unlike NVIDIA and AMD GPUs that share the same executor and worker, we create a separate code path for TPUs considering the significant difference between GPUs and TPUs. On the other hand, the two backends will share the other components of `LLMEngine`, namely the scheduler, KV cache manager, and tokenizer, as they are (almost) device agnostic.\r\n\r\n## PyTorch XLA and JAX\r\n\r\nAs many components of vLLM are device and runtime agnostic, it is possible to use JAX for TPU integration. However, for faster initial integration and maximum code reuse, **we will start with PyTorch XLA**. Adding JAX backend to vLLM will be interesting future work.\r\n\r\n## TPU Workers\r\n\r\n<img width=\"831\" alt=\"Screenshot 2024-03-25 at 10 44 24\u202fAM\" src=\"https://github.com/vllm-project/vllm/assets/46394894/35a4fa7a-2c1a-4845-bd45-05902c94164f\">\r\n\r\nFor tensor-parallel inference, the vLLM TPU executor will spin up multiple TPU workers; **one TPU worker per TPU chip**. Specifically, we will use [Ray](https://github.com/ray-project/ray) to connect and manage the TPU workers which may reside in different TPU VMs. Note that we do not plan to support multi-slice inference at the moment, while we will support multi-host inference within the same TPU pod slice.\r\n\r\nSame as the GPU executor, the TPU executor will use [Megatron](https://arxiv.org/abs/1909.08053)-style model partitioning for tensor-parallel inference. The partitioning strategy will be hardcoded into the model by replacing `nn.Linear` with `RowParallelLinear` and `ColumnParallelLinear`. Auto-sharding the model can be our future work.\r\n\r\n## GPU Executor vs. TPU Executor\r\n\r\n<img width=\"781\" alt=\"Screenshot 2024-03-25 at 10 44 40\u202fAM\" src=\"https://github.com/vllm-project/vllm/assets/46394894/6be8a6a9-bb1b-47f4-afac-786dac050929\">\r\n\r\nFor GPUs, vLLM uses both eager mode and CUDA graphs for model execution. Specifically, vLLM uses eager mode for prefills and CUDA graphs for decodes. vLLM currently does not use torch.compile for GPUs, but plans to use it in the future. For TPUs, on the other hand, **vLLM will use `torch.compile` (with openxla_eval backend) to trace the PyTorch model and lower it into an XLA graph**.\r\n\r\nWhile vLLM\u2019s GPU and TPU backends will take separate code paths, they will share the PyTorch model code. Most of the custom ops for GPUs will not be needed for TPUs, since they can be auto-generated by the XLA compiler. Therefore, for each target op, vLLM will have two implementations, `_forward` and `_forward_cuda`, and select either of the two implementations at run time depending on the hardware backend. For example, we can define the target ops/layers as follows:\r\n\r\n```python\r\nclass Op(nn.Module):\r\n\r\n    def _forward(self,...):\r\n        # PyTorch implementation that can be optimized by compilers\r\n        # such as XLA or torch.compile.\r\n        ...\r\n\r\n    def _forward_cuda(self, ...):\r\n        # Implementation using custom ops written in CUDA.\r\n        ...\r\n\r\n    def forward(self, ...):\r\n        if ...:\r\n            return self._forward_cuda(...)\r\n        else:\r\n            return self._forward(...)\r\n```\r\n\r\nImportant exceptions to this are the FlashAttention and PagedAttention custom ops, which cannot be generated by the XLA compiler. We will use custom Pallas kernels for them.\r\n\r\n## Handling Dynamic Shapes\r\n\r\nvLLM\u2019s continuous batching has two phases: prefill and decode. vLLM dynamically switches between the two phases based on its scheduling decisions. The input tensor shape for prefills is `[batch_size, prefill_len, hidden_size]` while the input tensor shape for decodes is `[batch_size, 1, hidden_size]` since LLMs decode tokens one by one (here we do not consider special cases such as speculative decoding). In LLM inference, the batch_size and prefill_len can vary for every step.\r\n\r\nTo meet the XLA\u2019s static shape requirement, we will bucketize the possible input shapes. For decodes, we will bucketize the `batch_size` dimension by creating buckets for `batch_size=[8, 16, 24, 32, 40, \u2026, 256]`. For prefills, to reduce the number of compiled graphs, we will fix the `batch_size` to 1, and bucketize the `prefill_len` dimension by creating buckets for `prefill_len=[8, 16, 32, 64, 128, \u2026, max_model_len]`. Given that each prefill input contains enough tokens to efficiently utilize TPUs, fixing `batch_size` as 1 will not hurt performance a lot. The specific bucket sizes will be tuned after benchmarking the compilation overhead and end-to-end performance.\r\n\r\n# References\r\n\r\n- PyTorch XLA Llama V1 inference blog post: https://pytorch.org/blog/path-achieve-low-inference-latency/ \r\n- PyTorch XLA Llama V2 inference blog post: https://pytorch.org/blog/high-performance-llama-2/ \r\n- PyTorch XLA Llama inference example: https://github.com/pytorch-tpu/llama/tree/llama2-google-next-inference",
    "labels": [
      "RFC",
      "tpu",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-25T17:08:43+00:00",
    "closed_at": "2025-03-11T14:04:01+00:00",
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3620/reactions",
      "total_count": 36,
      "+1": 23,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 12,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3620"
  },
  {
    "number": 9601,
    "title": "[RFC]: openai api response format",
    "body": "### Motivation.\n\nHere is example of my request using /v1/chat/completions.\r\n```\r\n{\r\n  \"id\": \"chat-36cf36c94fa746ffbee01440bbdcbf35\",\r\n  \"object\": \"chat.completion\",\r\n  \"created\": 1729649934,\r\n  \"model\": \"\",\r\n  \"choices\": [\r\n    {\r\n      \"index\": 0,\r\n      \"message\": {\r\n        \"role\": \"assistant\",\r\n        \"content\": \"hello\",\r\n        \"tool_calls\": []\r\n      },\r\n      \"logprobs\": null,\r\n      \"finish_reason\": \"stop\",\r\n      \"stop_reason\": null\r\n    }\r\n  ],\r\n  \"usage\": {\r\n    \"prompt_tokens\": 1841,\r\n    \"total_tokens\": 1849,\r\n    \"completion_tokens\": 8\r\n  },\r\n  \"prompt_logprobs\": null,\r\n  \"messages\": [\r\n    {\r\n      \"content\": \"hello\",\r\n      \"role\": \"user\"\r\n    },\r\n    {\r\n      \"role\": \"assistant\",\r\n      \"content\": \"xxx\",\r\n      \"tool_calls\": []\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nI'm wondering if \"messages\" is from in openai api reference?\n\n### Proposed Change.\n\nHow to disable this extra part from output?\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n_No response_\n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "closed",
    "created_at": "2024-10-23T02:27:41+00:00",
    "closed_at": "2024-10-23T03:12:09+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9601/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9601"
  },
  {
    "number": 6556,
    "title": "[RFC]: Single Program Multiple Data (SPMD) Worker Control Plane",
    "body": "### Motivation.\n\n**TL;DR**: Introduce SPMD-style control plane to improve control plane architecture and optimize performance.\r\n\r\nFor distributed inference, vLLM currently leverages a \u201cdriver-worker\u201d, along with other workers. As shown in the diagram below, this driver-worker is in the same process as the driver. It prepares the arguments, then broadcasts them to all other workers to execute the sharded model, leveraging NCCL as the control plane. \r\n\r\n<img width=\"850\" alt=\"Screenshot 2024-07-18 at 5 37 48\u202fPM\" src=\"https://github.com/user-attachments/assets/03ef792a-e0ad-4797-a7e0-9e3abcb0b028\">\r\n\r\nThis architecture has a few drawbacks. First, the driver-worker needs to participate in the NCCL group and execute the model. Since NCCL broadcast is a synchronous operation, this creates interference with other driver functionality such as scheduling and affects performance. \r\n\r\nMoreover, this architecture made it difficult to support speculative decoding. Specifically,\r\n1. Speculative decoding framework may not run the draft model if Dynamic Speculative Decoding (DSD) or other policy is enabled. In this case, the decision of whether to run the draft model must be communicated to other ranks. So DSD cannot work with TP>1, unless there is additional communication (which incurs latency overhead).\r\n2. Pipeline parallelism can be composed within the speculative decoding framework. However the speculative tokens must be sent to all workers, e.g. cross-node. If we have SPMD, then all PP ranks have access to the same information, and we don't need to do any communication on top of normal PP. This is important for latency.\r\n\n\n### Proposed Change.\n\nWe propose an architecture change to support SPMD-style control plane, as shown in the diagram below.\r\n\r\n<img width=\"855\" alt=\"Screenshot 2024-07-18 at 5 38 21\u202fPM\" src=\"https://github.com/user-attachments/assets/65b50452-419d-492c-91bc-34875b2b8e50\">\r\n\r\nSpecifically, we remove the argument preparation and model execution functionality from the driver, and make all workers SPMD-style: The LLMEngine/driver now passes the input to all the SPMD workers via a Ray DAG channel (shared memory), and each worker prepares arguments and executes its model shard. The results are passed back to the driver with Ray DAG channel as well.\r\n\r\n### Roadmap\r\n\r\nSPMD functionality and optimizations:\r\n- SPMD functionality is implemented in https://github.com/vllm-project/vllm/pull/6032 \r\n- Delta input optimization (sending delta input, as opposed to full input) will be implemented and benchmarked\r\n- Serialization optimization (e.g., using a different serialization format like msgspec)\r\n\r\nFeatures to build on top of SPMD:\r\n- Pipeline parallelism with Ray accelerated DAG\r\n- Speculative decoding\r\n\r\nAfter comprehensive benchmarking and optimizations, SPMD will become the default and NCCL based control plane code path will be cleaned up. \r\n\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n@youkaichao @stephanie-wang @rkooo567 @cadedaniel \n\n### Any Other Things.\n\n_No response_",
    "labels": [
      "RFC",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-19T00:42:25+00:00",
    "closed_at": "2024-12-06T02:07:26+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6556/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/6556"
  },
  {
    "number": 14168,
    "title": "[RFC]: Deprecate `max_num_generation_tokens`",
    "body": "### Motivation.\n\nSaid by @robertgshaw2-redhat https://github.com/vllm-project/vllm/pull/14055#issuecomment-2695114713\n\n### Proposed Change.\n\nAs part of v1, we should deprecate `max_num_generation_tokens`.\n\n\n### Feedback Period.\n\nWhen agreed _if_ this change should be made.\n\n### CC List.\n\n@robertgshaw2-redhat @markmc \n\n### Any Other Things.\n\nI will do this if interested.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-04T01:25:49+00:00",
    "closed_at": "2025-07-02T02:13:44+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14168/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14168"
  },
  {
    "number": 19702,
    "title": "[RFC]: Multimodal data IPC improvement",
    "body": "### Motivation.\n\n### Summary\nCurrently vllm interprocess communication can account for considerable amount of overhead in some cases, this RFC is aiming at reducing these overhead by using a shared memory based approach for interprocess communication.\n\n### Background\nAccording to the profiling result on our internal vision model in a TP>1 setting, the GPU stays idle during engine to worker communication.\n![Image](https://github.com/user-attachments/assets/4669088c-ddd6-4947-bb30-1c1bce0985a5)\nThe major overhead is two parts 1. IPC between engine and worker process through socket 2. serialization and deserialization through pickle\n\nA similar issue is posted here https://github.com/vllm-project/vllm/issues/16626\n\n### Proposed Change.\n\nAfter initial discussion with @ywang96 and @njhill , proposing this change to address the following communication overhead\n\n1. IPC between engine and worker processes\n2. Serialization and deserialization before and after 1.\n3. Extra multimodal data transmission: first from P0 to engine, then engine to workers\n\n### Design\n\n**Step 1.**\nFor addressing 1. engine and worker processes can transmit mm data through a shared memory buffer instead of socket, there\u2019s an existing [ShmRingBuffer](https://github.com/vllm-project/vllm/blob/main/vllm/distributed/device_communicators/shm_broadcast.py#L68) class which only supports fixed size chunks, because of chunk size limit, it's only turned on when mm data size [is less than 16mb](https://github.com/vllm-project/vllm/pull/19242) by default, otherwise the IPC will be done through socket, which is slow.\n\nWe can add/redesign an shared memory buffer implementation for storing variable length mm data.\n\n**Step 2.**\nFor addressing 2, based on the above we can skip the (de)serialization of mm data and only keep the mm_hashes for RPC call. Similar to when P0 gets a cache hit, we can [set mm data to None](https://github.com/vllm-project/vllm/blob/main/vllm/v1/engine/mm_input_cache.py#L58) in the engine process then restore it from shared memory buffer in the worker process. Maybe one assumption here is mm data only contains numpy/torch tensors or other easy-to-serialize types.\n\n**Step 3.**\nFor 3, we can replace the [MirroredProcessingCache](https://github.com/vllm-project/vllm/blob/main/vllm/v1/engine/mm_input_cache.py#L58) with the same shared memory buffer ideally to avoid extra mm data transfer between all processes.\n\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n_No response_\n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "open",
    "created_at": "2025-06-16T17:56:26+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19702/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19702"
  },
  {
    "number": 7769,
    "title": "[RFC]: Keep a Changelog & Add FAQs in the Documentation",
    "body": "### Motivation.\n\n## Changelog\r\nI frequently find myself wondering what is the difference between the latest version(s) of vLLM, and the version that I currently have deployed. It seems like it would be nice to keep a simple changelog that documents features, fixes, newly-supported hardware and updates between versions so that we can easily see what has been added in recent versions -- e.g. new CLI arguments, optimizations, quantization formats, updated hardware support for features (e.g. punica -> triton kernels, expanding hardware support for multi-lora serving) patched bugs, and so forth.\r\n\r\nA great template for this is [Keep a Changelog](https://keepachangelog.com/en/1.0.0/) following semver - it would be super easy to implement with Markdown in the documentation site. I think this would make vLLM's newer features much more accessible, _and_ it would also help identify gaps in the documentation when we add something to the changelog that's not on the docs site\r\n\r\n## FAQs \r\nThere are a bunch of questions that are commonly asked over and over in the discord, including things such as:\r\n- Does vLLM support XYZ hardware/accelerator?\r\n- Does vLLM support tool use / when is tool use coming?\r\n- Does vLLM support XYZ model / model architecture? \r\n- How can I get my model to fit in a vRAM-constrained environment? \r\n- How do I get started with distributed inference? \r\n\r\nIt seems like it would be nice to have a list of rolling FAQs to refer people to, and maybe to pin in the discord, for quick reference. This could be tracked in version control in the docs site, so that we can easily make pertinent additions and deletions as necessary? \n\n### Proposed Change.\n\n1. Create a simple changelog in the docs site following the [Keep a Changelog](https://keepachangelog.com/en/1.0.0/) structure; \r\n    a. Ask PR contributors to update this along with docs when they create a PR OR update this as part of the release process\r\n2. Implement a simple FAQ in the docs site \n\n### Feedback Period.\n\n1 week? \n\n### CC List.\n\n@mgoin @simon-mo @WoosukKwon @petersalas @comaniac @SolitaryThinker @ywang96 @DarkLight1337 unsure who else to ask - I just pulled a list of recent contributors to the docs\n\n### Any Other Things.\n\n_No response_",
    "labels": [
      "RFC",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-22T02:58:47+00:00",
    "closed_at": "2024-12-22T02:04:27+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7769/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7769"
  },
  {
    "number": 20394,
    "title": "[RFC][UX]: debug mode for vLLM-compile",
    "body": "### Motivation.\n\nvLLM-compile (CompilationLevel.PIECEWISE) makes a lot of assumptions about the models that allow it to make them run really fast. There are two main assumptions that commonly lead to silent incorrectness if the models violate them. I've spent countless hours debugging user issues for it to turn out to be one of these assumptions. We should add a debug mode option for vLLM-compile that, when turned on, adds some safety checks for these assumptions at the tradeoff of some additional overhead. This will let users self-diagnose the issues without me in the loop.\n\nThis is one of the items mentioned in https://github.com/vllm-project/vllm/issues/20283, I'm expanding it to include some more details.\n\n### Proposed Change.\n\nThe two assumptions that bite us are:\n1) the [vLLM Dynamic Shapes Issue](https://docs.google.com/document/d/1R3XvVEpJeVi3whyxf4xpyZufGplbrfw628oXLZ6fqG0/edit?tab=t.0#heading=h.59xosv6nz9lg). vLLM performs one single graph capture with dynamic batch size and expects the graph to work for all batch sizes. However, the graph may not actually be valid for all batch sizes. \n2) CUDAGraphs assume that the input addresses of Tensors do not change. Changing the input addresses (e.g. doing model.weight1 = new_weight1) violates the assumption.\n\nWe should add an option to have some additional \"safety checks\" in CompilationConfig. Here are the safety checks that we would add.\n\n#### For the vLLM Dynamic Shapes Issue\n\ntorch.compile produces a symbolic expression about what batch sizes the captured graph is valid for. These might look like \"size < 60000 && size % 128 == 0\". At runtime, when we execute the vLLM model, we should validate that the batch size actually passes the symbolic expression.\n\n#### For CUDAGraphs input addresses\n\nWe know all of the inputs being cudagraph'ed. We can record their input addresses, and, at runtime, always check that the input addresses did not change.\n\n### Feedback Period.\n\n7/2 - 7/11\n\n### CC List.\n\n@ProExpertProg @youkaichao @mgoin @robertgshaw2-redhat @WoosukKwon @drisspg @houseroad \n\n### Any Other Things.\n\nthank you\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "torch.compile"
    ],
    "state": "open",
    "created_at": "2025-07-02T17:56:56+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20394/reactions",
      "total_count": 3,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20394"
  },
  {
    "number": 10114,
    "title": "[RFC]: Merge input processor and input mapper for multi-modal models",
    "body": "## Motivation\n\n### Background\n\nTo provide more control over the model inputs, we currently define two methods for multi-modal models in vLLM:\n\n- The **input processor** is called inside `LLMEngine` to extend the prompt with placeholder tokens which are reserved for vLLM features such as KV cache and chunked prefill.\n- The **input mapper** is called inside `ModelRunner` to transform multi-modal inputs (e.g. `PIL` images) into tensor inputs, usually via the modality-specific processor (e.g. `AutoImageProcessor`) from HuggingFace.\n\n### Issues with the current design\n\n1. The input processor accepts the output of HF `AutoTokenizer`, a list of token IDs, instead of the text prompt. Since HF `AutoProcessor` doesn\u2019t accept token IDs, we have to write custom code to edit the list of token IDs based on the multi-modal inputs. For some models (such as Phi-3-vision), this means re-implementing code from their HF `AutoProcessor`, complicating the process of porting the model to vLLM.\n2. The input mapper, being inside `ModelRunner`, lies on the critical path of vLLM\u2019s model execution. Even when the input mapper is fast, the tail TTFT and TPOT suffers because of this. As the input mapper takes up more time, our overall throughput decreases proportionally which can be avoided if we move it outside of the critical path. Nevertheless, we can do little if the `AutoProcessor` inside input mapper is very slow, like in [#9238](https://github.com/vllm-project/vllm/issues/9238). Hope that [huggingface/transformers#33810](https://github.com/huggingface/transformers/issues/33810) can help with that!\n3. This abstraction results in redundant processing for models (such as Qwen2-VL and Molmo) with HF `AutoProcessor` that already performs most of the work for calculating the number of placeholder tokens.\n\n## Proposed Change\n\n### Unified multi-modal processor\n\nWe plan to merge our input processor and input mapper into a unified multi-modal processor (`BaseMultiModalProcessor`) that wraps HF `AutoProcessor`, and call it inside the `LLMEngine` (and thus benefit from #8779), taking the role of the existing tokenizer. After this change, each input type will be processed as follows:\n\n- Text-only prompt: Pass to vLLM tokenizer (wraps HF `AutoTokenizer`) [Unchanged]\n- List of token IDs: Skip vLLM tokenizer [Unchanged]\n- Text prompt with multi-modal input: Pass to vLLM multi-modal processor [NEW]\n- List of token IDs with multi-modal input: ~[Deprecated]~ Pass to vLLM multi-modal processor [NEW]\n\n### Automatic prompt replacement\n\n`BaseMultiModalProcessor._get_prompt_replacements` specifies HF's logic of replacing input placeholder tokens (e.g. `<image>` for a single image) with feature placeholder tokens (e.g. `<image><image>...<image>`, the number of which equals to the feature size). Given this specification, we can automatically detect whether HF has replaced the input placeholder tokens by checking whether the feature placeholder tokens exist in the prompt.\n\n`BaseMultiModalProcessor._apply_prompt_replacements` provides model-agnostic code for automatically replacing input placeholder tokens with feature placeholder tokens. This is only called if we find that HF hasn't done so yet.\n\nThis enables the multi-modal processor to accept text/token prompts and process them separately from the multi-modal data. The detailed logic is shown in `BaseMultiModalProcessor._apply_hf_processor_main`.\n\n### Processor caching\n\n#11396 caches each item in the multi-modal output of HF processor and links them back to items in the input data.\n\nWhen new data is passed in, we first check which items are in the cache, and which ones are missing. The missing items are passed into the HF processor in a single batch and cached, before being merged with the existing items in the cache.\n\nNote that the text/token prompt must be processed separately from the multi-modal data because HF processors expect the input placeholders in the text to correspond to each multi-modal data item, but we only want to process the items that are missing. We can handle this elegantly using automatic prompt replacement (see above).\n\n### ~~Deprecate token IDs with multi-modal input~~\n\n~~To be compatible with OpenAI\u2019s (legacy) Completions API, we currently support passing token IDs directly to both `LLM` class and OpenAI-compatible server. However, Completions API doesn\u2019t support multi-modal inputs, so we will deprecate passing token IDs alongside multi-modal inputs to simplify model implementation (see Issue 1 above). **Please tell us if you have a use case for this and don\u2019t want to see it removed!**~~\n\n## Feedback Period\n\nFeel free to comment as the effort progresses!\n\n### Timeline\n\n- [x] #10040\n- [x] #10044\n- [x] #10485\n- [x] [3/N] Develop and refine POC\n  - [x] #10676\n  - [x] #10711 \n  - [x] #10977\n  - [x] #11199\n  - [x] #11198\n  - [x] #11258\n  - [x] #11303\n  - [x] #11396\n  - [x] #11620\n  - [x] #11661\n  - [x] #11669\n  - [x] #11674\n  - [x] #11746\n  - [x] #11812\n  - [x] #11900\n  - [x] #12244\n  - [x] #12269\n  - [x] #11427\n  - [x] #13215\n  - [x] #13380\n  - [x] #13516\n  - [x] #13964\n  - [x] #14038\n  - [x] #15712\n  - [x] #16408\n  - [x] #16416\n- [x] [4/N] Deprecate the old code for input processor/mapper so external developers have time to convert\n  - [x] Update documentation on how to implement multi-modal models\n    - [x] #11925\n    - [x] #13331\n    - [x] #14278\n    - [ ] #15405\n    - [x] #16915\n  - [x] Activate deprecation logic\n    - [x] #13979\n- [x] [5/N] Convert the rest of the built-in vLLM models to multi-modal processor\n  - [x] #11632 (Aria, BLIP-2, Chameleon, Fuyu)\n  - [x] #11682\n  - [x] #11717\n  - [x] #12504\n  - [x] #12069\n  - [x] #12553\n  - [x] #12660\n  - [x] #12449\n  - [x] #12966\n  - [x] #13278\n  - [x] #14015\n  - [x] #12211\n  - [x] #15477\n- [x] [6/N] Remove the old code for input processor/mapper\n  - [x] #14864\n  - [x] #15673\n  - [x] #15686\n\nThe majority of our code will be called inside the existing `InputPreprocessor` which is separated from the vLLM engine, making it easy to integrate with #8779.\n\n## CC List\n\n@ywang96 @Isotr0py @WoosukKwon @robertgshaw2-neuralmagic \n\n## Any Other Things\n\n### ~~Multi-modal plugins remain supported~~ Migrating multi-modal plugins\n\nYou can define additional input modalities (`ModalityDataItems`) and parse them in subclasses of `MultiModalDataParser` on a per-model basis. Afterwards, override `BaseMultiModalProcessor._get_data_parser` to construct your newly-defined parser.\n\nSome users currently use multi-modal plugins to directly pass custom model inputs ([#6260](https://github.com/vllm-project/vllm/pull/6260)). Those inputs can be excluded from HF processing by returning them in `ModalityDataItems.get_passthrough_data` instead of `ModalityDataItems.get_processor_data`.\n\n### ~~No batched preprocessing for now~~\n\n~~Currently, preprocessing is performed per prompt in vLLM. While we can call HF tokenizer and modality-specific processor on batched inputs separately, calling the wrapping HF `AutoProcessor` with both list of texts and list of multi-modal data results in the processed multi-modal data (e.g. image) being assigned to every text in the list, rather than the more intuitive `zip`-like behavior (e.g. the `i`th image only assigned to the `i`th text). To support batched preprocessing, we would have to write custom code for each model to combine the outputs of HF tokenizer and modality-specific processors. Given that this can significantly complicate model implementation (see Issue 1 above), we will not consider batched preprocessing at this stage, even with this change.~~\n",
    "labels": [
      "RFC",
      "multi-modality"
    ],
    "state": "closed",
    "created_at": "2024-11-07T09:57:55+00:00",
    "closed_at": "2025-04-28T07:38:50+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10114/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/10114"
  },
  {
    "number": 17191,
    "title": "[RFC]: Custom sampling params support in REST API",
    "body": "**Update:** after incorporating feedback, the updated proposal is described in this comment: https://github.com/vllm-project/vllm/issues/17191#issuecomment-2858443302\n\n## Original RFC proposal (outdated):\n\n\n### Motivation\n\nAddresses #16802 (\u201cSupport custom args in OpenAI (chat) completion requests\u201d) by adding an \u201cextra\u201d sampling params argument to all endpoints which trigger sampling (completion, chat and transcription). This is ultimately a prerequisite for logits processor support ( RFC: #13360 PR: #16728 ), since logits processors may require custom arguments which are not utilized by vLLM core sampling logic.\n\n### Proposed Change.\n\nHere it is proposed that when using the HTTP client, custom sampling arguments may be passed in as key/value pairs via the `extra_sampling_params` argument\n\n```\nextra_sampling_params: Optional[dict[str, Any]]\n```\n\n#13300 added an `extra_args` member to `SamplingParams` \n\n```\nextra_args: Optional[dict[str, Any]] = None\n```\n\n`protocol.py` defines a class type for each endpoint\u2019s requests. Currently, the arrival of a completion/chat/transcription request at a particular REST API endpoint causes a call to the `to_sampling_params()` method associated with an instance of the appropriate request class. This method constructs a `SamplingParams` instance from the request attributes using the `from_optional()` method; the proposed change is to pass `extra_sampling_params` to `extra_args` at that point:\n\n```\nSamplingParams.from_optional(..., extra_args=extra_sampling_params)\n```\n\nIn this way, the custom arguments stored in `SamplingParams.extra_args` will be available to logits processors downstream in the request processing pipeline.\n\nFor example, \n\n```\ncurl http://0.0.0.0:8000/v1/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\"model\": \"facebook/opt-125m\", \"prompt\": \"Say this is a test\", \u201cignore_eos\u201d: true, \u201cextra_sampling_params\u201d: {\u201ccustom_arg\": <value>}}\u2019\n```\n\nresults in a SamplingParams instance with `extra_args = {\u201ccustom_arg\": <value>}`.\n\nThis RFC only applies to API endpoints which trigger sampling, summarized below (along with their associated request classes in `protocols.py`):\n\n* /v1/completions (`CompletionRequest`)\n* /v1/chat/completions (`ChatCompletionRequest`)\n* /v1/audio/transcriptions (`TranscriptionRequest`)\n\nThe following API endpoints do not trigger sampling and are not part of this workstream (note that to save time in writing this RFC, I refer to the endpoints in terms of broad categories here):\n* Embeddings (`EmbeddingCompletionRequest`, `EmbeddingChatRequest`)\n* Rerank (`RerankRequest`)\n* Tokenization/Detokenization (`TokenizationCompletionRequest`, `TokenizationChatRequest`, `DetokenizeRequest`)\n* LoRA load (`LoadLoRAAdapterRequest`) and unload (`UnloadLoRAAdapterRequest`)\n\nIf you are using the OpenAI Python SDK (or similar SDK in another language), the client-side completion/chat/transcription request method does not have an `extra_sampling_params` argument; `extra_sampling_params` will need to be passed in as a key-value pair to the `extra_body` dict argument of the request method. Note that the `extra_body` argument is not part of the server\u2019s REST API and if you pass `extra_body` as an argument within an HTTP client request, the server will ignore it. `extra_body` is simply a \u201ccatch-all\u201d argument supported by the Python SDK to handle \u201cspecial\u201d parameters. Internally, the SDK unpacks `extra_body` into REST API arguments. The server does not see the `extra_body` argument.\n\nUnder the proposed changes in this PR, the following SDK request exemplifies a correct usage:\n\n```\n   completion = await client.completions.create(model=model_name,\n                                                prompt=\"Hello, my name is\",\n                                                max_tokens=5,\n                                                temperature=0.0,\n                                                extra_body={\u201cignore_eos\u201d: True,\n                                                \u201cextra_sampling_params\u201d:           \n                                                {\u201ccustom_arg\u201d: True})\n```\n\n\n* OpenAI-standard API arguments are set directly as arguments to `create()`\n* Arguments such as `ignore_eos` are set in `extra_body` but *not* in `extra_sampling_params`, because `ignore_eos` is an argument defined explicitly in `protocols.py` and utilized by vLLM\u2019s core sampling functionality\n* `custom_arg` (which is meant to represent a hypothetical custom argument for a logits processor) is not defined explicitly in any of the request types defined in `protocol.py` and is therefore packed within `extra_sampling_params`\n\n#### Plan for rolling out extra sampling params:\n\n##### PR #16862 is WIP and does not yet satisfy the specifications below, but will by the time it lands\n* In `protocol.py`,  add an `extra_sampling_params` member to `CompletionRequest`, `ChatCompletionRequest`, and `TranscriptionRequest`. \n* In each of these three request classes, `extra_sampling_params` is assigned to `SamplingParams.extra_args` inside of the `to_sampling_params()` method  as described above.\n* This PR is a prerequisite for near-term work on logits processor support.\n* This PR does not introduce breaking changes.\n\n#### Thoughts on alternative proposals\n\nThe core requirement is that custom sampling arguments are supported, in order to enable the logits processor workstream.\n\nHowever, in discussions about the API surface area for sampling arguments, one additional proposal was that sampling arguments such as `ignore_eos` which are *not* part of the OpenAI API specification, but which are part of the core vLLM sampling implementation (i.e. they are not \u201ccustom\u201d logits processor arguments), should be grouped together under a catch-all dict argument (perhaps under `extra_sampling_params`, or perhaps under a separate dict argument). In other words these would not be top-level arguments, which is currently the case if you use the HTTP client.\n\nHere I suggest that this would add little benefit other than strict-*er* compliance with the OpenAI API specification, and in exchange would add unnecessary complexity and code changes.\n\n\n### Feedback Period.\n\n1 week\n\n### CC List.\n\n@njhill @comaniac @WoosukKwon @simon-mo \n\nCC @robertgshaw2-redhat \n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "open",
    "created_at": "2025-04-25T14:30:19+00:00",
    "closed_at": null,
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17191/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/17191"
  },
  {
    "number": 8453,
    "title": "[RFC]: Support encode only models by Workflow Defined Engine",
    "body": "### Motivation.\r\n\r\nAs vllm supports more and more models and functions, they require different attention, scheduler, executor, and input output processor. . These modules are becoming increasingly complex, and sometimes new features must be compromised for compatibility. ultimately leading to suboptimal results\r\n\r\nTake support for encode only models as an example\r\n\r\nAlthough the encode only models is much simpler than the decode model, they are very different.\r\n\r\nThe simplest way to support the encode only models is to implement different modules for models of different architectures and load the required modules on demand.\r\n\r\nI call this architecture Workflow Defined Engine, or WDE for short.\r\n\r\n###  Terminology.\r\nThe scope of discussion is slightly larger than encode only models, and is roughly divided into three categories\uff1a\r\n- Encode only models. (Bidirectional Transformers, causal=False), Often fine-tuned as retriever and reranker etc.\r\n- Decode only models. (masked multi-head attention, causal=True). There are two interesting uses:\r\n    - Output last hidden states as a feature extractor\r\n    - Decode only retriever \uff08I don't know of a better name\uff09\uff0cE.g. e5-mistral-7b \uff08The only Embed model currently supported by vllm)\r\n    - Whether it has been fine-tuned or not, there is almost no difference in the code.\r\n - Enable bidirectional. [LLM2Vec](https://arxiv.org/abs/2404.05961) propose a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. \r\n     - Therefore, we need to support enable_bidirectional flag manually or read hf config automatically, enable bidirectional.\r\n\r\nWhat the above three usages have in common is that there is only the prefill stage. In order to make the terminology more precise, prefill only is used below. \r\n\r\nYou can think of prefill only as encode only fancy writing.\r\n\r\nadd more:\r\nNatural language processing (NLP) can be divided into natural language understanding (NLU) and natural language generation (NLG). The prefill only models mentioned in this discussion are NLU models. NLU is just like the name and does not generate new token. \r\n\r\n\r\n### Proposed Change.\r\n\r\nSUMMARY:\r\n1. Prefill only models requires simpler attention implementations (prefill only, no kvcache...)\r\n2. Prefill only models requires simpler scheduler. (no kvcache, no preemption...)\r\n3. In order to support asynchronous scheduling, model_input_builder needs to be separated from the runner. \r\n   The main thread executes scheduling and all CPU processing, and the gpu thread only executes h2d, execution model, d2h\r\n4. With wde, there is no need for one module to be compatible with all functions. \r\n   You can always use the workflow to load new modules at the highest level to support new functions.\r\n\r\n### Feedback Period.\r\n\r\n_No response_\r\n\r\n### CC List.\r\n\r\n_No response_\r\n\r\n### Any Other Things.\r\n\r\nPTAL  #8452\r\n\r\nSupported models:\r\n- xlm_roberta \uff08#6260\uff09\r\n- bge-m3  \uff08#3187 #5737 #6498 #7969\uff09\r\n- bge-reranker-v2-m3 (#8022)\r\n- bert (#5179 #5447 #7496)\r\n- bge v1.5 family which rely on bert ( #7506 #5502)\r\n- Snowflake Arctic Embed (Family)  (#7792) (The architecture is the same as bge v1.5 family, lucky)\r\n- e5-mistral-7b \uff08The only Embed model currently supported by vllm)\r\n- output last hidden states (#8483 #853 #7915 #6947)\r\n- gte-Qwen2 \uff08#6282 #5827 #5600 #6015 #5611 #7389\uff09\r\n   - Because gte-Qwen2 and Qwen2 use the same architecture name\uff0cQwen2ForCausalLM. The code looks very sad!\r\n   - gte-Qwen2 family may have multiple different architectures. The code looks very sad!\r\n       - gte-Qwen2-1.5B-instruct, Official sample code sentence_transformers usage and Transformers usage does not use enable bidirectional\uff0c [discussions](https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct/discussions/1). I'm not sure if this is a bug\r\n       - gte-Qwen2-7B-instruct use enable bidirectional\r\n\r\nFeatures supported and tested:\r\n- WDE core\r\n- Attention Backend for prefill only models\r\n    - Flash Attention Backend\r\n    - Torch SDPA Backend\r\n    - XFormers Backend\r\n    - FlashInfer Backend (Because prefill only models do not involve kv cache, When using Flashinfer backend in prefill only models, you are actually using FLASH ATTN backend\r\n    - Torch naive backend (as a control group\r\n- Asynchronous scheduling for prefill only models (simple_execute_loop and double_buffer_execute_loop)\r\n- output last hidden states \r\n- enable bidirectional\r\n- data parallelism **Not fully tested**\r\n\r\nWIP:\r\n- Limit GPU memory usage by gpu_memory_utilization to avoid oom\r\n\r\nFunctions that have not yet, but are relatively important\r\n- Integrate wde into vllm entrypoints\r\n- more Attention Backend support \uff08I only have cuda device\r\n    - ROCM_FLASH\r\n    - OPENVINO\r\n    - PALLAS\r\n    - IPEX\r\n- Support distributed executer  \r\n    For small models, data parallelism is more efficient\r\n    - tensor parallelism (tensor parallelism is coupled with other parts, can we decouple it?\r\n    - pipeline parallelism\r\n- Limit GPU memory usage by gpu_memory_utilization to avoid oom\r\n- Support quantization models\r\n- Support lora\r\n    - LLM2Vec (#6584)\r\n- maybe more\r\n\r\nAnyway, I hope vllm can support prefill only models as soon as possible\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-13T08:27:51+00:00",
    "closed_at": "2025-01-10T08:58:49+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8453/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8453"
  },
  {
    "number": 7351,
    "title": "Create speculative decode dynamic parallel strategy",
    "body": "## Motivation\r\n\r\nCreate new speculative decode dynamic parallel strategy for our team needs\r\n\r\n## Features\r\n\r\nHere we briefly describe features that we will implement in order to implement speculative decode dynamic parallel strategy. Each feature has high level description as a part of request for change with more description provided inside pull request for particular feature\r\n\r\n#### Save speculative decoding states #7358\r\n    \r\nAllow users to optionally receive speculative decoding artifacts such as history of draft token indices for each step of speculative decode algorithm\r\n\r\n#### Create draft from random tokens from promt #7359\r\n\r\nImplement speculative proposers that enrich previous draft with tokens randomly sampled from current prompt\r\n\r\n#### Allow model executor to return many next tokens #7361\r\n\r\nCurrent implementation of model executor and model runner produce one last next token in decode stage. This feature would allow inner model runners to return next tokens for a range of tokens\r\n\r\n#### Create parallel scorer #7362\r\n\r\nImplement scorer that uses feature that allows to get next tokens in single forward request. Runs target model and returns scores for next tokens\r\n\r\n#### Create speculative decode dynamic parallel strategy #7363\r\n\r\nCombine all features such that users can use this speculative decode strategy",
    "labels": [
      "RFC"
    ],
    "state": "closed",
    "created_at": "2024-08-09T12:09:30+00:00",
    "closed_at": "2024-08-15T07:56:47+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7351/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7351"
  },
  {
    "number": 20799,
    "title": "[RFC]: Prototype Separating Vision Encoder to Its Own Worker",
    "body": "### Motivation.\n\nIn the current multi-modality support within vLLM, the vision encoder (e.g., Qwen_vl) and the language model decoder run within the same worker process. While this tightly coupled architecture is simple to implement, it introduces several challenges in terms of scalability, resource utilization, and flexibility:\n\n1.  **Resource Contention:** The vision encoder is often a compute and memory-intensive task. When processing high-resolution images or performing complex preprocessing, it competes for valuable GPU resources with the language model's prefill and decode stages, potentially increasing the overall latency of request processing.\n\n2.  **Scalability Issues:** The workload characteristics of vision processing and text generation are different. In some scenarios, image processing might be the bottleneck (e.g., a high volume of concurrent image inputs), while in others, text generation is the bottleneck (e.g., long text outputs). A unified worker model cannot scale these two different workloads independently.\n\n3.  **Inflexible Architecture:** The coupled architecture makes advanced optimizations difficult. For example, we cannot assign different types of hardware resources to the vision encoder and the text decoder (e.g., using compute-optimized GPUs for vision tasks and memory-optimized GPUs for decoding). Furthermore, it increases the complexity of supporting more sophisticated multi-modal workflows, such as video stream processing.\n\nTo address these issues, we propose to separate the vision encoder from the main language model worker, running it as one or more independent, specialized \"Encoder Worker\"\n\n### Proposed Change.\n\nOur core idea is to decouple the multi-modal inference process into two independent stages(or three if DP disaggregation is also considered), executed by specialized workers:\n\n1.  **Encoder Processing Stage:** Handled by a **Encoder Worker**. It receives raw image data, performs the complete preprocessing and encoding (e.g., through a ViT), and generates image embeddings.\n\n3.  **LLM Inference Stage:** Handled by the existing **Decoder Worker**. It receives text tokens and the image embeddings generated by the Vision Worker, and then executes the language model's prefill and decode operations.\n\n<img width=\"633\" height=\"877\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/919004f5-2ec4-4365-aabe-1b01655d62a8\" />\n\n### **High-Level Workflow:**\nThe end-to-end process for a multi-modal request is managed by a central **Proxy** and executed in distinct stages by specialized worker pools.\n\n---\n\n **1. Request Ingestion and Dispatch to Vision Stage**\n\n1.1. **Request Entry:**\nA multi-modal request, containing both image and text data, is sent via the `API Server` to the system's core entry point: the **Proxy/Router**.\n\n1.2. **Initial Dispatch:**\nThe Proxy identifies the request as a new task requiring vision processing. It forwards the **entire request object** to an available worker in the **Vision Encoder Worker** pool.\n\n---\n\n **2. Vision Encoding Stage (Executed by Vision Worker)**\n\n2.1. **Image Processing & Caching:**\nThe designated Vision Encoder Worker receives the request and performs its specialized function:\n*   It preprocesses the raw image data.\n*   It runs the image through its vision encoder model to generate the image embedding tensor.\n*   Crucially, it **writes the resulting embedding tensor to the Shared Multimodal Embedding Cache**, associating it with a unique identifier (e.g., `request_id` or a generated cache key).\n\n2.2. **Handoff with Metadata:**\nUpon successful caching, the Vision Encoder Worker reports back to the **Proxy/Router**. It does **not** send the large embedding tensor. Instead, it sends only lightweight **embedding metadata**. This metadata contains the necessary pointers for the next stage to locate the data (e.g., `{ \"request_id\": \"xyz-123\", \"cache_key\": \"embedding_xyz-123\" }`).\n\n---\n\n **3. LLM Inference Stage (Executed by LLM Worker)**\n\n3.1. **Dispatch with Metadata:**\nThe Proxy receives the completion signal and the embedding metadata from the Encoder Worker. It now packages this **metadata** (not the embedding itself) with the original text tokens and dispatches this lightweight task to an available worker in the **LLM (Decoder) Worker** pool.\n\n3.2. **Data Retrieval:**\nBefore execution, the LLM Worker reads the metadata from the request. It uses the provided identifier (`cache_key`) to **fetch  the required image embeddings directly from the Shared Multimodal Embedding Cache**. This transfer happens on the data plane, bypassing the Proxy.\n\n3.3. **Text Generation:**\nWith the image embeddings now locally available, the LLM Worker performs its `forward` pass, using the pulled embeddings and the text tokens to execute the prefill and decode operations and generate the text response.\n\n---\n\n **4. Response to Client**\n\n4.1. **Final Routing:**\nThe text stream generated by the LLM Worker is routed back through the **Proxy**, which then delivers the final response to the original client.\n\n ### **Advantages of this Design**\n\n\n*   **True Decoupling:** Vision Workers and LLM Workers have no direct dependencies. They both communicate only with the Proxy, making their responsibilities singular and easy to develop, test, and maintain independently.\n*   **Architectural Consistency:** This design aligns with mature architectural concepts already established in the vLLM community, such as Prefill/Decode separation, making it easier to integrate into vLLM's existing ecosystem and future roadmap.\n*   **Better Load Balancing & Scalability:** Separate queues and resource pools can be established for Vision Workers and LLM Workers. This allows for independent autoscaling based on the backlog of their respective queues, leading to more granular and efficient resource utilization.\n\n### **Key Components to Modify/Add:**\n\n*   **`Encoder Worker`:** A new worker type that encapsulates the loading and inference logic for vision models. It can be managed and scaled independently of the Decoder Workers.\n*   **`Scheduler`:** Needs to be extended to manage a pool of Encoder Workers and to coordinate the dependencies and data flow between vision tasks and decoding tasks.\n*   **Data Transfer Mechanism:** An efficient mechanism must be designed and implemented to transfer image embeddings between the Encoder Worker and the Decoder Worker. This is critical for minimizing end-to-end latency.\n*   **`Config`:** Introduce new configuration options to allow users to configure the number of Encoder Workers, the models to be loaded, the devices to be used, etc.\n\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n@ywang96 \n\n### Any Other Things.\n\nReference:\n[1. https://github.com/vllm-project/vllm/issues/4194 ](https://github.com/vllm-project/vllm/issues/4194)\n[2. Efficiently Serving Large Multimodal Models Using EPD Disaggregation ](https://arxiv.org/pdf/2501.05460)\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "open",
    "created_at": "2025-07-11T06:13:16+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20799/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 3,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20799"
  },
  {
    "number": 17067,
    "title": "[RFC]: All Ops should be determined during init and wrapped in a Layer Module to avoid envs.ENVIRON overhead",
    "body": "### Motivation.\n\nAccessing envs.ENVIRON has non-negligible overhead. Given that LLM models have many ops and layers. The overhead from accessing envs.ENVIRON could spike to 0.1 ~ 1ms overhead per token. I have observed a huge overhead in MLA prefill forward pass when using envs.ENVIRON in kernel selection logic (where `if-else` statement is involved).\n\nProposed action:\n1. Layer Module is suggested to store the selected kernel ops as a property of the layer.\n`@cache` is discourage due to the increasing complexity that it is causing to clear the cache as there are many properties depending on envs.\n`@cache` is discouraged in several PRs review as there is a usecase as such: \nUsers instantiate multiple\u00a0LLMs in a single python program. Each LLM instance uses different sets of ENV variables.\n\n2. Document the overhead issue down in vLLM documentation page under Contribution section to remind developers of the abstract and the overhead caused by envs.ENVIRON invocation.\n\n\n## Overhead experiments\n\n```\nAverage time per accessing envs.ENVIRON : 1.0514259338378907e-06 seconds\nAverage time per accessing class method access : 3.0994415283203126e-08 seconds\n```\n\nScript:\n```\nimport time\nimport torch\nimport vllm.envs as envs\n\n\nclass Layer:\n    @staticmethod\n    def forward():\n        return torch.randn(1024, 1024, dtype=torch.float16, device='cuda')\n\ndef test_envs_timing():\n    \n    # Time the operation\n    num_runs = 100\n    start_time = time.time()\n    for _ in range(num_runs):\n        envs.VLLM_ROCM_USE_AITER_LINEAR\n    end_time = time.time()\n    \n    # Calculate average time per run\n    avg_time = (end_time - start_time) / num_runs\n    print(f\"Average time per accessing envs.ENVIRON : {avg_time} seconds\")\n\n\ndef test_class_method():\n    \n    # Time the operation\n    num_runs = 100\n    start_time = time.time()\n    for _ in range(num_runs):\n        Layer.forward\n    end_time = time.time()\n    \n    # Calculate average time per run\n    avg_time = (end_time - start_time) / num_runs\n    print(f\"Average time per accessing class method access : {avg_time} seconds\")\n\nif __name__ == \"__main__\":\n    test_envs_timing()\n    test_class_method()\n```\n\n### Proposed Change.\n\nAs an example, in model layers, such as linear layers, they should use the following abstraction:\n`vllm/model_executor/layers/quantization/fp8.py`\n```\nfrom vllm.model_executor.layers.quantization.fp8_utils import dispatch_w8a8_blockscale_func\nclass Fp8LinearMethod(LinearMethodBase):\n    def __init__(self, quant_config: Fp8Config):\n        ...\n        if self.block_quant:\n            self.linear_func = dispatch_w8a8_blockscale_func(self.cutlass_block_fp8_supported)\n        else:\n            # logic to select the per-tensor/ per-channel scaled gemm kernel\n   \n    def apply(self,\n              layer: torch.nn.Module,\n              x: torch.Tensor,\n              bias: Optional[torch.Tensor] = None) -> torch.Tensor:\n        # no more checks for condition\n        return self.linear_func(input=x,\n                                     weight=layer.weight,\n                                     weight_scale=layer.weight_scale,\n                                     out_dtype=self.out_dtype,\n                                     input_scale=layer.input_scale,\n                                     bias=bias)\n\n```\n\n`aiter-block-gemm-integration/vllm/model_executor/layers/quantization/utils/fp8_utils.py`\n```\n\ndef dispatch_w8a8_blockscale_func(\n        use_cutlass: bool) -> Callable[..., torch.Tensor]:\n    if use_cutlass:\n        return cutlass_scaled_mm\n    if (current_platform.is_rocm() and \n        envs.VLLM_ROCM_USE_AITER and \n        envs.VLLM_ROCM_USE_AITER_LINEAR):\n        return torch.ops.vllm.rocm_aiter_gemm_w8a8_blockscale\n    return w8a8_block_fp8_matmul\n```\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n_No response_\n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "open",
    "created_at": "2025-04-23T16:45:16+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17067/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/17067"
  },
  {
    "number": 11400,
    "title": "[RFC]: Fully SPMD Execution for Offline Inference",
    "body": "### Motivation.\r\n\r\nTL;DR:  Introducing a fully SPMD-style LLMEngine execution pattern to improve offline inference throughput.\r\n\r\nThe RFC draft is initiated by @PeterSH6 \r\n\r\n# Background and Motivation \r\n## Inherent dispatch overhead in single-controller paradigm\r\nFor distributed offline inference, vLLM leverages a centralized controller process (e.g., Ray Driver) to broadcast the scheduler output to the workers. After workers' execution, the output is gathered from the workers to the centralized controller process to perform the next iteration scheduling. While this single-controller paradigm offers better user experience, it introduces throughput limitations.\r\nTherefore, to launch a generation call, vLLM obey the following procedure:\r\n```\r\npython3 offline_inference.py # launch the centralized controller process (i.e., LLMEngine)\r\n# inside the LLMEngine\r\nllm_engine.distributed_gpu_executor._run_workers('start_worker_execution_loop', ...) # execute the model\r\n# inside the _run_workers\r\nworker_outputs = [\r\n    worker.execute_method(method, *args, **kwargs)\r\n    for worker in self.workers\r\n]\r\n```\r\nFrom the code above, the `_run_workers` functions will cause unneglible overhead.\r\nAlthough recent proposals managed to eliminate these overhead by using `Ray DAG` in https://github.com/vllm-project/vllm/issues/6556 and  multi-step scheduling https://github.com/vllm-project/vllm/issues/6854. As discussed in Pathways [2] and HybridFlow [1], the dispatch overhead from single-controller is hard to mitigate compared to the fully SPMD paradigm.\r\n\r\nOur proposed Fully SPMD approach for vLLM entails:\r\n- Independent `LLMEngine` per GPU, each containing a vLLM scheduler and `SPMDGPUExecutor` with cache engine and model\r\n- There's no centralized control. Instead, each `LLMEngine` will schedule its own data while maintaining the same behavior between different `LLMEngine` on different GPU (discussed why this will hold in offline inference setting later).\r\n\r\n![image](https://github.com/user-attachments/assets/5afb56a8-0643-4654-bb60-979540a0737e)\r\n![image](https://github.com/user-attachments/assets/52f4daa0-c301-4bf0-88ca-a5fab692e7fa)\r\nThis approach aligns with other high-performance frameworks like TensorRT-LLM that leverage SPMD-style execution for maximum system throughput.\r\n\r\n## Inflexible support for RL/RLHF post-training workload, especially when vLLM works with other LLM Training infrastructures (HybridEngine Design [1, 3]).\r\nMainstream LLM training infrastructures, such as Megatron-LM, DeepSpeed, PyTorch FSDP, utilize the SPMD programming model.\r\nIn RL/RLHF post-training, actor models (in PPO and GRPO) must perform both training and autoregressive generation. This requires deploying actor models across both vLLM and training frameworks (e.g., Megatron-LM) with different parallelization strategies, necessitating weight synchronization each iteration.\r\nCurrent RL/RLHF post-training frameworks offer two main deployment strategies:\r\n\r\n- Co-located Actor/Rollout using **HybridEngine** (HybridFlow (veRL)[1], Nemo-Aligner[4], DeepSpeed-Chat [3])\r\n- Separated Actor/Rollout deployment (OpenRLHF)[5]\r\n\r\nFor actor/rollout placing on different devices, the vLLM can be simply deployed as a service and single-controller paradigm works fine in this scenario. However, such placement will cause some GPU idle. The rollout GPU will get idle when the actor performs training following the dependency in PPO and GRPO. \r\nThe HybridEngine design could eliminate this GPU idle problem [1]. However, implementing HybridEngine with vLLM and Megatron-LM reveals significant challenges in merging training processes with vLLM worker processes. Weight synchronization requires inter-process weight resharding, which is complex to implement between the single-controller paradigm and SPMD paradigm.\r\nTherefore, adopting a fully SPMD-style LLMEngine would facilitate HybridEngine implementation and enable more efficient weight resharding in RL/RLHF post-training scenarios.\r\n\r\n## Multi-Node Offline Inference Complexity\r\nCurrent vLLM implementation requires Ray cluster setup before conducting multi-node offline inference, adding deployment complexity.\r\nA Fully SPMD paradigm would simplify distributed offline inference to a single command:\r\n```\r\ntorchrun --nnodes 2 --nproc_per_node 8 offline_inference.py\r\n```\r\nThis streamlined approach not only simplifies deployment but also facilitates straightforward implementation of data parallelism on top of the SPMD architecture. The resulting system would be more maintainable and scalable across multiple nodes.\r\n\r\n# Major Benefits\r\nBased on the discussion above, the Fully SPMD execution pattern can provide the following benefits in offline settings:\r\n1. Higher offline inference throughput\r\n2. Faster weight resharding in RL/RLHF training and easier implementation\r\n3. Better support for multi-node offline inference and data parallelism\r\n4. [Planning] Easier to implement pipeline parallelism in LLMEngine under SPMD paradigm.\r\n\r\n\r\n\r\n### Proposed Change.\r\n\r\nWe have already implemented the SPMD version of vLLM in HybridFlow (veRL) under v0.6.3, v0.5.4, v0.4.2 and v0.3.1 [github.com](https://github.com/volcengine/verl/tree/main/verl/third_party/vllm). The architecture is shown as above in the background section. \r\n\r\nWe made the following major changes:\r\n- **SPMD pattern**: \r\n  - Introduced SPMDGPUExecutor class within LLMEngine\r\n  - Each GPU runs its own LLMEngine and SPMDGPUExecutor instance\r\n  - Single Worker per SPMDGPUExecutor, with scheduling and execution occurring within the same GPU process\r\n- **Deterministic behavior**: \r\n  - Modified determine_num_available_blocks() to ensure same GPU/CPU cache size and therefore ensure consistent scheduler behavior across workers. Implemented all_reduce(MIN) synchronization for block availability data between workers\r\n  - In an offline inference setting, a batch of prompts is processed simultaneously. Consequently, different LLMEngines within the same DP group receive identical inputs at the same time. Due to the consistent GPU/CPU caching mechanisms (with same size) and the inherent FIFO scheduling method used in vLLM, the behavior of the schedulers remains deterministic and identical.\r\n- **Model initialization/synchronization**: In RL/RLHF post-training workload, the model weights in vLLM should be synchronized every iteration. Therefore, we add an API to sync model weights under the SPMD paradigm.\r\n- **KVCache offload**: In RL/RLHF post-training, the KVCache should be offloaded when the actor is performing a training stage to reduce memory overhead.\r\n\r\nI think the first two features are highly-related to SPMD and would be beneficial for all offline settings using vLLM. \r\nThe last two features will be a good supplement for the vLLM to support RL/RLHF post-training workloads.\r\n\r\n##  Roadmap\r\nFully SPMD functionality and optimizations:\r\n- Fully SPMD functionality transfer from HybridFlow(veRL) practice to vLLM LLMEngine\r\n- Support various offline inference offloads using SPMD functionality (e.g., multi-modal, speculative)\r\n- Implement and optimize the pipeline parallelism on top of SPMD-based LLMEngine.\r\n\r\n### Feedback Period.\r\n\r\n_No response_\r\n\r\n### CC List.\r\n\r\n@PeterSH6\r\n@vermouth1992\r\n@ZSL98\r\n\r\n\r\n### Any Other Things.\r\n\r\nReference\r\n[1] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Hybridflow: A flexible and efficient rlhf framework. Eurosys 2025.\r\n[2] Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Daniel Hurt, Michael Isard, Hyeontaek Lim, Ruoming Pang, Sudip Roy, et al. 2022. Pathways: Asynchronous distributed dataflow for ml. Proceedings of Machine Learning and Systems 4 (2022), 430\u2013449. \r\n[3] Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajb- handari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, et al. 2023. DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales. arXiv preprint arXiv:2308.01320 (2023). \r\n[4] Gerald Shen and Zhilin Wang and Olivier Delalleau and Jiaqi Zeng and Yi Dong and Daniel Egert, et al. 2024. NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment. Arxiv preprint arXiv: 2405.01481 (2024).\r\n[5] Jian Hu and Xibin Wu and Zilin Zhu and Xianyu and Weixun Wang and Dehao Zhang and Yu Cao. 2024. OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework. arXiv preprint arXiv:2405.11143\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "closed",
    "created_at": "2024-12-21T18:09:34+00:00",
    "closed_at": "2025-01-21T06:04:24+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11400/reactions",
      "total_count": 21,
      "+1": 21,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11400"
  },
  {
    "number": 20492,
    "title": "[RFC]: KV-Cache Interoperability API Standardization",
    "body": "### Motivation\n\nThis RFC proposes a KV-Cache Interoperability API, covering standardized notification events (via KVEvents) and reproducible prefix-block hashing. These standards aim to support cross-system cache awareness, observability, and future tooling for indexing, routing, and diagnostics.\n\nvLLM already ships with internal [KVEvents](https://github.com/vllm-project/vllm/issues/16669) contributed by the NVIDIA Dynamo team - that\u2019s a strong foundation. \nBut as external systems aim for cache-aware inference, we need to treat these internal mechanisms as public contracts to support broader adoption and interop.\n\n### Goals\n\n1. **KVEvents Internal API as a Public Contract**  \n   The KVEvents schema is already well-defined in vLLM and used internally by the `KVCacheManager` for GPU cache events. It\u2019s also being extended to CPU offloading via the `KVConnector` (see [#19854](https://github.com/vllm-project/vllm/issues/19854)).  \n   This RFC proposes formalizing KVEvents as the public contract for any component emitting or consuming KV-Cache lifecycle events - including external indexers, routers, and engines.\n\n2. **Ensure Reproducible Block Hashing Across Languages**  \n   Prefix cache block keys must be computed the same way across runtimes (e.g., Python, Go). This requires:\n    - Canonical serialization (e.g., CBOR)\n    - Consistent hashing algorithms (e.g., SHA256, xxHash)\n    - Defined structure for input objects (e.g., token arrays, `extra_keys`)\n    - Explicit rules for special cases like `NONE_HASH` root\n    - Alignment on security features such as per-request hash-salting \n   \n    **Disclaimer**: in the current KVEvents schema, the token-ids are sent along their block-hashes, which makes external indexing possible through mapping tokens -> different-hashes -> vLLM-hashes. While this avoids introducing reproducible hashing and configuration syncs, it requires complex indexing and lookups, along with the networking overhead of passing the 32bit token-ids in every event.\n\n3. **Enable Language-Agnostic Interop**  \n   Develop shared guidance and reference libraries in Python, Go, and other widely used languages. These utilities do not need to reside within vLLM, but should remain consistent with its specifications.\n\n\n### Proposed Change\n\nThis RFC proposes standardizing two core aspects of KV-Cache awareness:\n\n#### 1. KVEvents Schema\n\n- Reuse the existing KVEvents format used internally in vLLM as a versioned public interface for any KV-Cache publisher or consumer\n- Consider light refactoring:\n    - Use `bytes` for hashes instead of Python-native `int`\n    - Reduce required fields where appropriate\n\n#### 2. Prefix Block Hashing\n\n- Use CBOR (canonical mode) for serializing token arrays and metadata\n  - Other canonical algorithms are welcome. Today serialization is coupled with Python.\n- Support multiple standard hash functions (`SHA256`, `xxHash`)\n- Gradually migrate to defaulting the non-language-coupled options\n\n  PR on first two points:\n    - #20511 \n\nThese changes will support consistent block identity and event interpretation across runtimes, enabling robust interop between cache indexers, and routing layers.\n\n### CC List\n\n@robertgshaw2-redhat @njhill @YaoJiayi @dannyharnik @orozery \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "open",
    "created_at": "2025-07-04T16:23:37+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20492/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20492"
  },
  {
    "number": 7124,
    "title": "[RFC]: Model architecture plugins",
    "body": "### Motivation.\n\nAs a continuation to #5367 - as this merge request was rejected and I have to maintain my own fork to support this scenario, I suggest we should add support in vLLM for model architecture plugins.\r\nThis will allow vLLM to easily add new model architectures without changing vLLM's core logic, and support scenarios such as uneven GPU tensor parallelism.\r\n\r\nWe could build an ecosystem of model architecture plugins - which could accelerate new model support by a lot without risking existing functionality.\n\n### Proposed Change.\n\nSupporting this in it's basic form is simple as we just have to add loaded plugins to the `ModelRegistry`.\r\nTo support more complex model architectures (Such in the #5367 case), we should decouple the `Config` class which provides the amount of attention heads from vLLM's core logic, and allow each model architecture to override these values.\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n@youkaichao \n\n### Any Other Things.\n\nJust to make it clear, I'll be happy to implement this, but I want hear some feedback before I go ahead and implement this.",
    "labels": [
      "RFC",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-04T12:13:33+00:00",
    "closed_at": "2025-01-03T02:41:49+00:00",
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7124/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7124"
  },
  {
    "number": 17799,
    "title": "[RFC]: Logits processor extensibility",
    "body": "### Motivation.\n\nUsers want logits processor extensibility, i.e. the ability to specify logits processors beyond those such as min-p which are hard-coded into the engine. See for example:\n* #12678\n* https://github.com/NVIDIA/logits-processor-zoo - library of logits processor extensions\n\nThe purpose of this RFC is to establish the interface for extending the vLLM V1 engine with additional logits processors during engine instantiation.\n\nvLLM V0 supports logits processor configuration at request level (`SamplingParams` attribute). For V0 running in server mode, PR #11150 makes it possible for a request to dynamically import one or more logits processor modules, assuming that the necessary modules are available/installed. The `logits_processors` argument (available in the completion, chat completion and transcription API endpoints) allows the custom logits processors\u2019 constructors to be specified as a list of (1) qualified names, or (2) `LogitsProcessorConstructor` data structures (which include the qualified name along with constructor arguments). For security purposes (prevention of arbitrary code execution), `\u200b\u200b--logits-processor-pattern` whitelists specific logits processor libraries via regex.\n\nWe expect V1 will add logits processor support, with logits processors instantiated at server init time. See RFC #13360 , PR #16728 . (Note that, although the logits processors are instantiated at server init time, the *behavior* of the logits processors - including but not limited to enabling a given logits processor - can still be controlled using `SamplingParams.extra_args` on a per-request basis. #16862 allows `SamplingParams.extra_args` to be configured via via the `vllm_xargs` REST API argument.) #16728 adds a logits processor base class and migrates several hard-coded logits processors (min P, min tokens, logits bias) to be sub-classes of this base class. However, #16728 does not make the list of logit processors in a given engine instance extensible beyond the builtins - thus, this RFC focused on the need to implement extensibility as a follow-on task.\n\nSupport for logits processor extensibility in v1 is desirable, both for server mode and also for direct instantiation of `LLM` and `AsyncLLM` in Python.\n\n### Proposed Change.\n\n#### Interface\n\n* For the purpose of this workstream - which is solely considering vLLM v1 engine - a \"logits processor\" is a subclass of the `LogitsProcessor` class (as defined in `vllm/v1/sample/logits_processor.py` in https://github.com/vllm-project/vllm/pull/16728/files ). Note that at time of writing, third-party libraries such as `logits-processor-zoo` are not directly compatible with this programming model.\n\n* In server mode, vLLM V1 engine will have a new CLI argument, `logits-processors`, which passes in a list of logits processor constructors. It will be necessary to find a clean way to represent this on the command line; here I propose (1) `logits-processors` expects a string representation of a JSON-formatted list, in which each element is one of the following: (1) qualified name of a logits processor, or (2) a \"constructor\" JSON object with a key/value pair for the qualified name of the logits processor as well as optional positional args (`args`) and keyword args (`kwargs`) arguments; for reference see https://github.com/vllm-project/vllm/pull/11150#issue-2736950700\n\n  * Each logit processor module specified via command line will be imported\n  * Each imported logits processor will be instantiated in the persistent batch; see `InputBatch` implementation in #16728\n  * Note that there will be no need for a `logits-processor-pattern` CLI argument (unlike in V0) because there is no need to whitelist specific module names, since the user is specifying logits processor names explicitly via CLI\n  * Example:\n\n```\n# CLI logits processor example\nvllm serve ... --logits-processors '['logits_processor_zoo.vllm.GenLengthLogitsProcessor',{'qualname': 'vllm.v1.sample.MinPLogitsProcessor','args':[0,'argument_value'],'kwargs':{'arg_name': 'arg_value'}}]'\n```\n\n* `LLM` and `AsyncLLM` engine will support a `logits_processors` constructor argument. The argument still accepts a list of logits processors specifications, however unlike the CLI interface, each list element may be one of (1) a logits processor subclass, (2) the qualified name of a logits processor subclass, or (3) an instance of  `vllm.entrypoints.openai.protocol.LogitsProcessorConstructor` (the underlying Python class for the aforementioned constructor JSON objects in the CLI interface):\n\n```\nclass LogitsProcessorConstructor(BaseModel):\n    qualname: str\n    args: Optional[List[Any]] = None\n    kwargs: Optional[Dict[str, Any]] = None\n``` \n\n  * Users instantiating the engine directly in Python are responsible for security concerns as regards logits processors (i.e. validating that they are not importing third-party logits processor libraries in an unsafe way).\n  * Examples:\n\n```\nfrom vllm.entrypoints.openai.protocol import LogitsProcessorConstructor\nfrom logits_processor_zoo.vllm import CiteFromPromptLogitsProcessor\n...\n\nlogitprocs_list = [CiteFromPromptLogitsProcessor,\n                   'logits_processor_zoo.vllm.GenLengthLogitsProcessor',\n                   LogitsProcessorConstructor(qualname='vllm.v1.sample.MinPLogitsProcessor',\n                                                             args=[0,'argument_value'],\n                                                             kwargs={'arg_name': 'arg_value'})]\n\n# Sync engine example\nllm = LLM(model=\"facebook/opt-125m\",\n          logits_processors=logitprocs_list)\n\n# Async engine example\nasync_llm = AsyncLLM(model=\"facebook/opt-125m\",\n                     logits_processors=logitprocs_list)\n```\n\n* In server mode, the logits processors specified by the `logits-processors` CLI argument, will be passed to the `logits_processors` argument of the engine constructor\n\n* For V0 back-compatibility, V0 continues supporting logits_processors request argument in REST API until V0 is removed\n* The vLLM V1 engine raises an invalid exception when V0 logits processor interfaces (SamplingParams logits_processors, REST API logits_processors) are utilized (it probably makes sense to implement this check in protocol.py even though protocol.py is not v1-specific, because we want skip importing logits processor modules if the user is mistakenly using the v0 logits processor interface with the v1 engine.)\n\n#### Implementation\n\n1. Logits processors are passed in through the server CLI interface or through the `LLM`/`AsyncLLM` constructors.\n1. For logits processors which were specified by qualified names, the qualified names are resolved during engine initialization. The end result is a list of logits processor classes\n1. The list of logits processors is passed to the `InputBatch` constructor, which instantiates each logits processor\n\n### Feedback Period.\n\n2 weeks\n\n### CC List.\n\n@njhill @russellb @simon-mo @WoosukKwon \n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "open",
    "created_at": "2025-05-07T13:43:16+00:00",
    "closed_at": null,
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17799/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/17799"
  },
  {
    "number": 19097,
    "title": "[RFC]: Response format extensions for structured outputs",
    "body": "### Motivation.\n\nCurrently, users can provide additional constraints format via `extra_body` in OpenAI client:\n\n```python\nfrom enum import Enum\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nsimplified_sql_grammar = \"\"\"\n        root ::= select_statement\n\n        select_statement ::= \"SELECT \" column \" from \" table \" where \" condition\n\n        column ::= \"col_1 \" | \"col_2 \"\n\n        table ::= \"table_1 \" | \"table_2 \"\n\n        condition ::= column \"= \" number\n\n        number ::= \"1 \" | \"2 \"\n    \"\"\"\n\nprompt = (\n        \"Generate an SQL query to show the 'username' and 'email'\"\n        \"from the 'users' table.\"\n    )\n\ncompletion = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": prompt,\n            }\n        ],\n        extra_body={\"guided_grammar\": simplified_sql_grammar},\n```\n\nThis also applies with `guided_json`, `structural_tag`, `guided_regex`.\n\nWhile this is pretty convenient for most developers, these fields are still using v0 terminology wrt guided decoding.\n\nWith the upcoming v0 deprecation, I think it is the time to have a usage update with this pattern.\n\n\n### Proposed Change.\n\nOpenAI already recommends users to use `response_format` with [json_schema](https://platform.openai.com/docs/guides/structured-outputs?api-mode=chat)\n\nGiven that we already supports `structural_tag` via `response_format` ([example](https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_structured_outputs_structural_tag.html?h=structural+tags)), I propose an extension to `response_format` for the remainder of the fields\n\n```python\ncompletion = client.chat.completions.create(\n  model=model, messages=messages,\n  response_format={\n    \"type\": \"vllm_regex\",\n    \"regex\": r\"\\w+@\\w+\\.com\\n\"\n  }\n)\n\ncompletion = client.chat.completions.create(\n  model=model, messages=messages,\n  response_format={\n    \"type\": \"vllm_grammar\",\n    \"grammar\": \"\"\"\nroot ::= select_statement\nselect_statement ::= \"SELECT \" column \" from \" table \" where \" condition\ncolumn ::= \"col_1 \" | \"col_2 \"\ntable ::= \"table_1 \" | \"table_2 \"\ncondition ::= column \"= \" number\nnumber ::= \"1 \" | \"2 \"\n\"\"\"\n  }\n\ncompletion = client.chat.completions.create(\n  model=model, messages=messages,\n  response_format={\n    \"type\": \"vllm_choice\",\n    \"choice\": [\"Positive\", \"Negative\", \"Neutral\"], \n  }\n)\n```\n\nThe previous json_schema + structural tag remains the same.\n\nThe field `guided_*` will still works previously, but will reserved only for more advance usage and won't be documented.\n\n### Feedback Period.\n\n1 week for revision, 2-3 days for implementations plan (mostly frontend + protocol updates)\n\nWe can also add a debug log recommending using this new pattern for all existing usage of `guided_*` (so that we won't break production)\n\n### CC List.\n\n@russellb @mgoin @simon-mo @hmellor \n\n### Any Other Things.\n\nThis is mostly frontend changes\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "structured-output",
      "RFC",
      "v1"
    ],
    "state": "open",
    "created_at": "2025-06-03T17:05:05+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19097/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19097"
  },
  {
    "number": 13020,
    "title": "[RFC]: Async KV Cache Transfer for Disaggregated Inference",
    "body": "### Motivation.\n\nHello vLLM community,\n\nWe're from the AWS Neuron inference team and are actively working on P/D disaggregated inference. We'd like to share our initial PoC for achieving **asynchronous KV cache transfer** (mentioned in roadmap #10818), to make decode continue execution while receiving KV cache from prefill workers. Developed based on the current KVCacheTransferAgent (introduced in v0.7.0) and v0 scheduler. \n\n### Proposed Change.\n\n**KVCacheTransferAgent (KVLookupBuffer level)** at Decode Worker:\n\n\n* Create and maintain a `receiver_buffer` containing entries for `{input_ids, roi, keys, caches, hidden}` in decode workers, rather than only for prefill workers.\n* Introduce the `async_drop_select` API. Unlike `drop_select` (which triggers immediate blocking lookups that stall current process), this method queues `drop_select_request` and returns immediately.\n* Implement a dedicated `drop_select_requester` thread to process queued `drop_select_request`. This thread initiates lookups and data transfers via the `drop_select_handler` from prefill worker, then inserts results into the `receiver_buffer` upon completion.\n\n**Scheduler** at Decode Worker:\n\n\n* Add a new `transfer queue` and `TRANSFERRING` status.\n* Split the original `_schedule_prefills` into two stages:\n  * `_schedule_wait`: Processes the waiting queue to allocate KV cache memory. Once a prompt is schedulable, it triggers `async_drop_select` to start asynchronous KV cache transfer and moves the `seq_group` to the `transferring queue`.\n  *` _schedule_transferring`: Processes through the `transferring queue`.  When KV cache transfers complete (verified via status in `receiver_buffer`), `seq_group` entries are moved to the `running queue`.\n* If no transfers are ready for new prompts, the scheduler prioritizes existing decoding requests over prefills.\n\nAt the decoder's model_runner level, prompts now always have their KV cache available in local memory. Instead of calling `drop_select`, it retrieves \"cached\" entries directly from the `receiver_buffer`.\n\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n@yuleil @liangfu @KuntaiDu\n\n### Any Other Things.\n\nWe have conducted initial validation using Neuron backend ([neuronx-distributed-inference](https://github.com/aws-neuron/neuronx-distributed-inference)) on AWS Trn1/Trn2 instances. We'll share performance data once available.\n\nAcknowledging ongoing transitions to the V1 scheduler and XpYd development, we also noted @yuleil's insights on Alibaba Cloud's disaggregated inference approach (#10818 comment) regarding async transfers and role switching. We're eager to collaborate with the community on these initiatives.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-10T08:04:11+00:00",
    "closed_at": "2025-07-02T02:14:18+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13020/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 2,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13020"
  },
  {
    "number": 9098,
    "title": "[RFC]: hide continuous batching complexity through forward context",
    "body": "### Motivation.\r\n\r\ntake a look at the current llama forward computation logic:\r\n\r\n```python\r\nclass LlamaMLP(nn.Module):\r\n    def forward(self, x):\r\n        gate_up, _ = self.gate_up_proj(x)\r\n        x = self.act_fn(gate_up)\r\n        x, _ = self.down_proj(x)\r\n        return x\r\n\r\n\r\nclass LlamaAttention(nn.Module):\r\n    def forward(\r\n        self,\r\n        positions: torch.Tensor,\r\n        hidden_states: torch.Tensor,\r\n        kv_cache: torch.Tensor,\r\n        attn_metadata: AttentionMetadata,\r\n    ) -> torch.Tensor:\r\n        qkv, _ = self.qkv_proj(hidden_states)\r\n        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\r\n        q, k = self.rotary_emb(positions, q, k)\r\n        attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\r\n        output, _ = self.o_proj(attn_output)\r\n        return output\r\n\r\n\r\nclass LlamaDecoderLayer(nn.Module):\r\n    def forward(\r\n        self,\r\n        positions: torch.Tensor,\r\n        hidden_states: torch.Tensor,\r\n        kv_cache: torch.Tensor,\r\n        attn_metadata: AttentionMetadata,\r\n        residual: Optional[torch.Tensor],\r\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        # Self Attention\r\n        if residual is None:\r\n            residual = hidden_states\r\n            hidden_states = self.input_layernorm(hidden_states)\r\n        else:\r\n            hidden_states, residual = self.input_layernorm(\r\n                hidden_states, residual)\r\n        hidden_states = self.self_attn(\r\n            positions=positions,\r\n            hidden_states=hidden_states,\r\n            kv_cache=kv_cache,\r\n            attn_metadata=attn_metadata,\r\n        )\r\n\r\n        # Fully Connected\r\n        hidden_states, residual = self.post_attention_layernorm(\r\n            hidden_states, residual)\r\n        hidden_states = self.mlp(hidden_states)\r\n        return hidden_states, residual\r\n\r\n\r\nclass LlamaModel(nn.Module):\r\n    def forward(\r\n        self,\r\n        input_ids: torch.Tensor,\r\n        positions: torch.Tensor,\r\n        kv_caches: List[torch.Tensor],\r\n        attn_metadata: AttentionMetadata,\r\n    ) -> torch.Tensor:\r\n        hidden_states = self.get_input_embeddings(input_ids)\r\n        residual = None\r\n\r\n        for i in range(self.start_layer, self.end_layer):\r\n            layer = self.layers[i]\r\n            hidden_states, residual = layer(\r\n                positions,\r\n                hidden_states,\r\n                kv_caches[i - self.start_layer],\r\n                attn_metadata,\r\n                residual,\r\n            )\r\n\r\n        hidden_states, _ = self.norm(hidden_states, residual)\r\n        return hidden_states\r\n\r\n\r\nclass LlamaForCausalLM(nn.Module, SupportsLoRA):\r\n    def forward(\r\n        self,\r\n        input_ids: torch.Tensor,\r\n        positions: torch.Tensor,\r\n        kv_caches: List[torch.Tensor],\r\n        attn_metadata: AttentionMetadata,\r\n    ) -> torch.Tensor:\r\n        model_output = self.model(input_ids, positions, kv_caches,\r\n                                  attn_metadata)\r\n        return model_output\r\n```\r\n\r\nif we don't consider `attn_metadata` and `kv_caches`, it can be simplified as: \r\n\r\n```python\r\nclass LlamaMLP(nn.Module):\r\n    def forward(self, x):\r\n        gate_up, _ = self.gate_up_proj(x)\r\n        x = self.act_fn(gate_up)\r\n        x, _ = self.down_proj(x)\r\n        return x\r\n\r\n\r\nclass LlamaAttention(nn.Module):\r\n    def forward(\r\n        self,\r\n        positions: torch.Tensor,\r\n        hidden_states: torch.Tensor,\r\n    ) -> torch.Tensor:\r\n        qkv, _ = self.qkv_proj(hidden_states)\r\n        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\r\n        q, k = self.rotary_emb(positions, q, k)\r\n        attn_output = self.attn(q, k, v)\r\n        output, _ = self.o_proj(attn_output)\r\n        return output\r\n\r\n\r\nclass LlamaDecoderLayer(nn.Module):\r\n    def forward(\r\n        self,\r\n        positions: torch.Tensor,\r\n        hidden_states: torch.Tensor,\r\n        residual: Optional[torch.Tensor],\r\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        # Self Attention\r\n        if residual is None:\r\n            residual = hidden_states\r\n            hidden_states = self.input_layernorm(hidden_states)\r\n        else:\r\n            hidden_states, residual = self.input_layernorm(\r\n                hidden_states, residual)\r\n        hidden_states = self.self_attn(\r\n            positions=positions,\r\n            hidden_states=hidden_states,\r\n        )\r\n\r\n        # Fully Connected\r\n        hidden_states, residual = self.post_attention_layernorm(\r\n            hidden_states, residual)\r\n        hidden_states = self.mlp(hidden_states)\r\n        return hidden_states, residual\r\n\r\n\r\nclass LlamaModel(nn.Module):\r\n    def forward(\r\n        self,\r\n        input_ids: torch.Tensor,\r\n        positions: torch.Tensor,\r\n    ) -> torch.Tensor:\r\n        hidden_states = self.get_input_embeddings(input_ids)\r\n        residual = None\r\n\r\n        for i in range(self.start_layer, self.end_layer):\r\n            layer = self.layers[i]\r\n            hidden_states, residual = layer(\r\n                positions,\r\n                hidden_states,\r\n                residual,\r\n            )\r\n\r\n        hidden_states, _ = self.norm(hidden_states, residual)\r\n        return hidden_states\r\n\r\n\r\nclass LlamaForCausalLM(nn.Module):\r\n    def forward(\r\n        self,\r\n        input_ids: torch.Tensor,\r\n        positions: torch.Tensor,\r\n    ) -> torch.Tensor:\r\n        model_output = self.model(input_ids, positions)\r\n        return model_output\r\n```\r\n\r\nArguably, `attn_metadata` is the most complicated part in the forward computation logic. And it becomes even more complicated when we consider:\r\n\r\n- continuous batching, where we batch data from different sequences together\r\n- heterogeneous models, where we can have different attention metadata for different layers (e.g. Gemma 2)\r\n- optimized `torch.compile` logic, where we want to hide the complexity of attention layer from the compiler\r\n\r\nTherefore, I'm considering to hide the complexity of continuous batching through forward context. The idea is to have a global forward context, which can be set by the model runner during every forward pass. The forward context can be used to store the attention metadata, and the model can access the attention metadata through the forward context.\r\n\r\n\r\n### Proposed Change.\r\n\r\nThe changes are:\r\n\r\n- the model runner will set the forward context before running the model, and the forward context will be used to store the attention metadata and kvcache.\r\n    - For the sake of generality, the forward context should contain a list of attention metadata and kvcache, where each element in the list corresponds to the attention metadata and kvcache for a layer. In the common case where all the layers share the same attention metadata, the model runner is responsible for duplicating the attention metadata.\r\n- all the files in `vllm/model_executor/models` will know nothing about attention metadata and kvcache. They will only know about the input tensors and the output tensors, as if they are just doing token-wise computation. Every attention layer will have a new `self.layer_index` attribute, which will be used to index the attention metadata and kvcache in the forward context.\r\n- all the attention implementation will be wrapped into a PyTorch custom op so that it is easy to compile. The custom op will only take input tensors, and retrieve the attention metadata and kvcache from the forward context. This way, the complexity of attention metadata and kvcache will be hidden from the compiler.\r\n\r\nsee https://github.com/vllm-project/vllm/pull/9029 and https://github.com/vllm-project/vllm/pull/9097 for initial steps.\r\n\r\n### Feedback Period.\r\n\r\n_No response_\r\n\r\n### CC List.\r\n\r\n_No response_\r\n\r\n### Any Other Things.\r\n\r\n_No response_\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-05T22:34:54+00:00",
    "closed_at": "2025-02-07T01:59:42+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9098/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/9098"
  },
  {
    "number": 4873,
    "title": "[RFC]: Add control panel support for vLLM",
    "body": "### Motivation.\r\n\r\nThe Fastchat-vLLM operational model offers significant advantages in deploying large language models (LLMs) for product services. [1](https://blog.vllm.ai/2023/06/20/vllm.html)\r\n\r\nThe controller architecture in Fastchat is particularly beneficial for LLM deployment, owing to its loosely coupled design with the vLLM backend. This allows for:\r\n\r\n* Autoscaling: The vLLM backend can join and exit the cluster freely, enabling dynamic scaling capabilities.\r\n\r\n* Rolling Updates: The introduction of new models with distinct names allows the cluster to gradually update models, a process known as rolling updates.\r\n\r\n* Centralized Access: Users are relieved from the burden of tagging different URLs or IPs for various models; they simply send their requests to the controller, which then manages the rest, including dispatching requests to the appropriate backend based on the model name and ensuring effective load balancing.\r\n\r\nHowever, the challenge for Fastchat lies in managing multiple backends, including vLLM. This complexity appears to hinder its ability to keep pace with the rapid evolution of vLLM. It is disheartening to observe that Fastchat currently does not support the latest vLLM features, such as multi-LoRA, fragmented chat stream support, and guidance decoding, among others.\r\n\r\nRefence:\r\n[1] https://blog.vllm.ai/2023/06/20/vllm.html\r\n\r\n### Proposed Change.\r\n\r\n\r\nSo just head it up, I port the key feature of controller from fastchat, and make it at minimal shape, which for interface like /v1/../completions, it simply extract model name, and forward anything towards the backend, so that all feature of vllm could be used.\r\n\r\nCurrent implement: #4861 \r\n\r\n- [x] /v1/completions:  same interface of vllm's\r\n- [x] /v1/chat/completions:  same interface of vllm's\r\n- [x] /list_models: list models' name registered into controller\r\n- [x] /health: check controller health status\r\n- [x] /list_workers: list worker's detailed status, models provided by each worker, and its serving status\r\n- [x] load balance with shortest queue algo\r\n- [x] heart beat keep alive between controller and worker\r\n\r\nFuture directions:\r\n- [ ] maybe rust could be used for reimplement the controller, if we find the performance could be improved a lot\r\n- [ ] more algo for load balance\r\n- [ ] unified metrics exposed by controller, which collected from each worker\r\n- [ ] more interface support, like embeding\r\n\r\n\r\n\r\n\r\n### Feedback Period.\r\n\r\n_No response_\r\n\r\n### CC List.\r\n\r\n@simon-mo @robertgshaw2-neuralmagic \r\n\r\n### Any Other Things.\r\n\r\n_No response_",
    "labels": [
      "RFC",
      "keep-open"
    ],
    "state": "open",
    "created_at": "2024-05-17T02:20:50+00:00",
    "closed_at": null,
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4873/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4873"
  },
  {
    "number": 16527,
    "title": "[RFC]: How about change name specialized_manager to specialized_kv_cache_manager ?",
    "body": "### Motivation.\n\nThe subclasses of SpecializedManager, such as FullAttentionManager and SlidingWindowManager, are all related to handling KV cache.\n\n### Proposed Change.\n\nSo why aren't they named something like SpecializedKVManager instead? The current naming makes the code confusing to read.\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n_No response_\n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-04-12T04:28:56+00:00",
    "closed_at": "2025-07-14T06:36:48+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16527/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16527"
  },
  {
    "number": 17823,
    "title": "[RFC]: Add automated profiling sweep and heatmap visualization tools",
    "body": "### Motivation.\n\nWhile `examples/offline_inference/profiling.py` provides detailed kernel-level timing in vLLM, its usability is limited when users want to:\n\n- Conduct profiling across multiple batch sizes and prompt lengths\n- Visualize performance trends and bottlenecks\n\nCurrently, users must manually modify arguments and parse raw outputs, which is slow and error-prone. There's no convenient way to sweep inputs or generate visual summaries.\n\nWe propose two tools to address this gap and extend the existing profiler for practical model-level profiling.\n\n### Proposed Change.\n\nWe propose upstreaming two lightweight utilities:\n\n#### 1. `sweep_profiling.py`  \nA script to automate `profiling.py` runs across a set of batch sizes and prompt lengths. Features:\n\n- CLI flags: `--model`, `--tensor-parallel-size`, `--max-tokens`\n- Spawns subprocesses for each profiling job\n- Captures errors cleanly and logs failures\n- Output: multiple `profiling_bs{N}_pl{M}.json` traces\n\n**In addition to the overall model runner time, we also generate the profiling result for operator breakdown.**\n\n\n**Usage:**\n```\npython sweep_profiling.py [--model MODEL_NAME] [--max-tokens MAX_TOKENS] [--tensor-parallel-size TP_SIZE]\n```\n\n\n**Key logic:**\n```python\nfor batch_size in batch_sizes:\n    command = [\n        \"VLLM_USE_V1=0\",\n        \"python\", \"profiling.py\",\n        \"--model\", model_name,\n        \"--batch-size\", str(batch_size),\n        \"--prompt-len\", \"0\", # handled in the profiling.py file\n        \"--max-num-batched-tokens\", str(max_tokens),\n        \"--json\", \"DUMMY.json\",\n        \"--load-format\", \"dummy\",\n        \"--enforce-eager\",\n        \"--tensor-parallel-size\", str(TP),\n        \"run_num_steps\", \"-n\", \"2\"\n    ]\n    subprocess.Popen(\" \".join(command), shell=True)\n```\n\n\n#### 2. `plot_heatmap_from_traces.py`\n\nThis script parses the JSON outputs from `profiling.py` (across batch \u00d7 prompt configurations) and generates multiple heatmaps.\n\nIt produces:\n- Phase latency heatmap\n\nSupported phases: `prefill`, `decode_1`  \nSupported inputs: all traces named like `profiling_bs{N}_pl{M}.json`  \nSupports batch \u00d7 prompt sweep.\n\n**Key logic:**\n```python\ndef extract_cuda_time(json_file, phase):\n    with open(json_file) as f:\n        data = json.load(f)\n    entry = SummaryStatsEntry(**data[phase][\"summary_stats\"][0][\"entry\"])\n    return entry.cuda_time_us / 1000.0\n```\n\n```python\ndef plot_heatmap(data, title, filename):\n    im = ax.imshow(data, cmap=\"viridis\")\n    ax.set_xticklabels(prompt_lens)\n    ax.set_yticklabels(batch_sizes)\n    for i in range(...): ax.text(j, i, f\"{val:.1f}\")\n```\n\n\n\n#### Example Outputs\n\n##### Prefill Time on H100 (ms)\n\n![Image](https://github.com/user-attachments/assets/b86533f1-9ad1-4b71-9c4f-58cbc9a451eb)\n\n##### Decode Time on H100 (ms)\n\n![Image](https://github.com/user-attachments/assets/9aff69a6-b11e-40e1-889c-ef39d217c485)\n\n\n\n#### Summary\n\nThis RFC introduces a reproducible and automated profiling suite for vLLM that:\n\n- Enhances `profiling.py` with systematic benchmarking\n- Enables heatmap-based performance analysis\n- Requires no engine modifications\n- Reuses and complements existing profiling infrastructure\n\nWe suggest placing these tools under:\n```\ntools/profiler\n\u251c\u2500\u2500 sweep_profiling.py\n\u251c\u2500\u2500 plot_heatmap_from_traces.py\n```\n\n### Feedback Period.\n\n1 week\n\n### CC List.\n\n@GindaChen \n\n### Any Other Things.\n\n- Could integrate with dashboards via CSV export in the future\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "open",
    "created_at": "2025-05-08T02:16:43+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17823/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17823"
  },
  {
    "number": 12480,
    "title": "[RFC]: [V1] TPU support and multiple architecture support",
    "body": "### Motivation.\n\nWe are in process of adding Google TPU support to the vLLM V1. \n\nHere is the WIP PR [https://github.com/vllm-project/vllm/pull/11936](https://github.com/vllm-project/vllm/pull/11936). \n\nSince this is the first time we add another hardware backend to V1, the PR has some refactor to avoid code duplications, which requires discussion and feedback.\n\n\n\n### Proposed Change.\n\nHere is the summary of changes this PR introduces:\n\n1. Refactors the common logic of model_runner to **model_runner_base.py** in the folllowing way (Virtual functions in italic):\n       \\_\\_init\\_\\_() => Has common config init\n       get_model() => Just simply returns model\n       get_kv_cache_spec() => Common logic for KV cache management\n       _initialize_kv_cache()_ => Virtual API\n       _execute_model()_ => Virtual API\n       _load_model()_ => Virtual API\n       _dummy_run()_ => Virtual API\n       _profile_run()_ => Virtual API\n       _capture_model()_ => Virtual API\n\n2. Refactors common logic of worker to **worker_base.py** in the following way (Virtual functions in italic):\n       \\_\\_init\\_\\_() => Has common config init, HF init, torch profiler init\n       load_model() => Calls load_model() of model_runner\n       compile_or_warm_up_model() => Calls capture model based on enforce_eager param and sets random seed\n       get_model() => Calls get_model() of model_runner\n       get_kv_cache_spec() => Calls get_kv_cache_spec() of model_runner\n       initialize_cache() => Calls initialize_kv_cache() of model_runner\n       profile() => Starts/stops profiler\n       check_health() => Empty function\n       _init_device()_ => Virtual API\n       _determine_available_memory()_ => Virtual API\n       _execute_model()_ => Virtual API\n\nComments and feedback are very welcome.\n\n\n### Feedback Period.\n\none week\n\n### CC List.\n\n@robertgshaw2-redhat @WoosukKwon @mgoin @tlrmchlsmth @youkaichao @simon-mo @njhill @comaniac @ywang96 @DarkLight1337 @SageMoore @bvrockwell \n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "stale",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-01-27T18:31:51+00:00",
    "closed_at": "2025-06-05T02:12:54+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12480/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/12480"
  },
  {
    "number": 4926,
    "title": "[RFC]: Postmerge performance suite",
    "body": "### Motivation.\r\n\r\nWe want to start tracking performance numbers of vLLM on more realistic workloads. Thanks to our sponsors #4925 we are getting a pool of hardware resources ready to run the testing on. \r\n\r\nThe goal of this test suite is to\r\n1. Track regression\r\n2. Track our progress in optimization\r\n\r\n### Proposed Change.\r\n\r\nWe will start with running the following benchmarks:\r\n\r\n* Llama 8B on A100, H100\r\n* Llama 70B on 4xA100, 4xH100, 8xA100, 8xH100\r\n* Mixtral 8x7B on 8xH100\r\n* Mixtral 8x22B on 8xH100\r\n\r\nWe will run with the following parameters:\r\n- chunked prefill enabled\r\n- fp8\r\n\r\nWe will run with the following tests:\r\n- Benchmark latency\r\n- Benchmark throughput with 1000 prompts (ShareGPT)\r\n- Benchmark serving with 1000 prompts (ShareGPT)\r\n\r\nWe will also compare with TGI and TRT-LLM.\r\n\r\n### Feedback Period.\r\n\r\nStep 1: Ensure hardware availabilities\r\nStep 2: Setup pipeline for Llama 8B on H100 as a proof of concept\r\nStep 3: Monitor the result, build dashboard\r\nStep 4: Scale to other tests as resources come online. \r\n\r\n### CC List.\r\n\r\n_No response_\r\n\r\n### Any Other Things.\r\n\r\nSuggestion welcomed. ",
    "labels": [
      "RFC",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-20T21:55:14+00:00",
    "closed_at": "2024-10-27T22:55:03+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4926/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 2,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4926"
  }
]
[
  {
    "number": 16474,
    "title": "[Bug]: Error when running Llama-4-Maverick-17B-128E-Instruct-FP8 on mi300x",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 04-11 09:49:33 [__init__.py:239] Automatically detected platform rocm.\nCollecting environment information...\nPyTorch version: 2.7.0a0+git295f2ed\nIs debug build: False\nCUDA used to build PyTorch: N/A\nROCM used to build PyTorch: 6.3.42133-1b9c17779\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: 18.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.3.1 24491 1e0fda770a2079fbd71e4b70974d74f62fd3af10)\nCMake version: version 3.31.6\nLibc version: glibc-2.35\n\nPython version: 3.12.10 (main, Apr  9 2025, 08:55:05) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-128-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: AMD Instinct MI300X (gfx942:sramecc+:xnack-)\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\nHIP runtime version: 6.3.42133\nMIOpen runtime version: 3.3.0\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               256\nOn-line CPU(s) list:                  0-255\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 9534 64-Core Processor\nCPU family:                           25\nModel:                                17\nThread(s) per core:                   2\nCore(s) per socket:                   64\nSocket(s):                            2\nStepping:                             1\nFrequency boost:                      enabled\nCPU max MHz:                          3718.0659\nCPU min MHz:                          1500.0000\nBogoMIPS:                             4892.29\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov succor smca fsrm flush_l1d\nVirtualization:                       AMD-V\nL1d cache:                            4 MiB (128 instances)\nL1i cache:                            4 MiB (128 instances)\nL2 cache:                             128 MiB (128 instances)\nL3 cache:                             512 MiB (16 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-63,128-191\nNUMA node1 CPU(s):                    64-127,192-255\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==2.1.3\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.0a0+git295f2ed\n[pip3] torchvision==0.21.0+7af6987\n[pip3] transformers==4.51.1\n[pip3] triton==3.2.0+gite5be006a\n[conda] Could not collect\nROCM Version: 6.3.42133-1b9c17779\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3rc2.dev77+g2976dc27e.d20250409\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n============================ ROCm System Management Interface ============================\n================================ Weight between two GPUs =================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            15           15           15           15           15           15           15\nGPU1   15           0            15           15           15           15           15           15\nGPU2   15           15           0            15           15           15           15           15\nGPU3   15           15           15           0            15           15           15           15\nGPU4   15           15           15           15           0            15           15           15\nGPU5   15           15           15           15           15           0            15           15\nGPU6   15           15           15           15           15           15           0            15\nGPU7   15           15           15           15           15           15           15           0\n\n================================= Hops between two GPUs ==================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            1            1            1            1            1            1            1\nGPU1   1            0            1            1            1            1            1            1\nGPU2   1            1            0            1            1            1            1            1\nGPU3   1            1            1            0            1            1            1            1\nGPU4   1            1            1            1            0            1            1            1\nGPU5   1            1            1            1            1            0            1            1\nGPU6   1            1            1            1            1            1            0            1\nGPU7   1            1            1            1            1            1            1            0\n\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI\nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI\nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI\nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI\nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI\nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0\n\n======================================= Numa Nodes =======================================\nGPU[0]\t\t: (Topology) Numa Node: 0\nGPU[0]\t\t: (Topology) Numa Affinity: 0\nGPU[1]\t\t: (Topology) Numa Node: 0\nGPU[1]\t\t: (Topology) Numa Affinity: 0\nGPU[2]\t\t: (Topology) Numa Node: 0\nGPU[2]\t\t: (Topology) Numa Affinity: 0\nGPU[3]\t\t: (Topology) Numa Node: 0\nGPU[3]\t\t: (Topology) Numa Affinity: 0\nGPU[4]\t\t: (Topology) Numa Node: 1\nGPU[4]\t\t: (Topology) Numa Affinity: 1\nGPU[5]\t\t: (Topology) Numa Node: 1\nGPU[5]\t\t: (Topology) Numa Affinity: 1\nGPU[6]\t\t: (Topology) Numa Node: 1\nGPU[6]\t\t: (Topology) Numa Affinity: 1\nGPU[7]\t\t: (Topology) Numa Node: 1\nGPU[7]\t\t: (Topology) Numa Affinity: 1\n================================== End of ROCm SMI Log ===================================\n\nPYTORCH_ROCM_ARCH=gfx942\nMAX_JOBS=30\nLD_LIBRARY_PATH=/opt/rocm/lib:/usr/local/lib:\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI\u2019m encountering an issue when running the FP8 version of the meta-llama/Llama-4-Maverick-17B-128E-Instruct model (i.e. meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8) on 4 Mi300x GPUs. According to the [vllm blog](https://blog.vllm.ai/2025/04/05/llama4), it should be possible to run this model on a Mi300x GPU, but the configuration provided in the blog uses a tensor parallelism of 8-GPU . \n\nWhen I set tensor parallel size to 4 but in a fp8 model, I get the following error: \n\n```\n VLLM_DISABLE_COMPILE_CACHE=1 vllm serve meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8  --tensor-parallel-size 4 --max-model-len 430000 --download-dir /app/data/models/ --kv-cache-dtype fp8\n```\noutput:\n```text\nINFO 04-11 09:53:14 [__init__.py:239] Automatically detected platform rocm.\nINFO 04-11 09:53:16 [api_server.py:1034] vLLM API server version 0.8.3rc2.dev77+g2976dc27e.d20250409\nINFO 04-11 09:53:16 [api_server.py:1035] args: Namespace(subparser='serve', model_tag='meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir='/app/data/models/', load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='fp8', max_model_len=430000, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=4, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_chunked_mm_input=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x7f4d7769bd80>)\nINFO 04-11 09:53:28 [config.py:604] This model supports multiple tasks: {'score', 'embed', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.\nWARNING 04-11 09:53:34 [arg_utils.py:1746] --kv-cache-dtype is not supported by the V1 Engine. Falling back to V0.\nWARNING 04-11 09:53:34 [arg_utils.py:1618] The model has a long context length (430000). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.\nINFO 04-11 09:53:34 [config.py:1231] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor\nINFO 04-11 09:53:34 [config.py:1609] Defaulting to use mp for distributed inference\nINFO 04-11 09:53:34 [api_server.py:246] Started engine process with PID 262\nINFO 04-11 09:53:37 [__init__.py:239] Automatically detected platform rocm.\nINFO 04-11 09:53:44 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.3rc2.dev77+g2976dc27e.d20250409) with config: model='meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', speculative_config=None, tokenizer='meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=430000, download_dir='/app/data/models/', load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=fp8,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True,\nWARNING 04-11 09:53:45 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 04-11 09:53:45 [rocm.py:153] None is not supported in AMD GPUs.\nINFO 04-11 09:53:45 [rocm.py:154] Using ROCmFlashAttention backend.\nINFO 04-11 09:53:48 [__init__.py:239] Automatically detected platform rocm.\nINFO 04-11 09:53:48 [__init__.py:239] Automatically detected platform rocm.\nINFO 04-11 09:53:48 [__init__.py:239] Automatically detected platform rocm.\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:55 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:55 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:56 [rocm.py:153] None is not supported in AMD GPUs.\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:56 [rocm.py:154] Using ROCmFlashAttention backend.\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:56 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:56 [rocm.py:153] None is not supported in AMD GPUs.\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:56 [rocm.py:154] Using ROCmFlashAttention backend.\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:56 [rocm.py:153] None is not supported in AMD GPUs.\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:56 [rocm.py:154] Using ROCmFlashAttention backend.\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:56 [utils.py:990] Found nccl from library librccl.so.1\nINFO 04-11 09:53:56 [utils.py:990] Found nccl from library librccl.so.1\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:56 [pynccl.py:69] vLLM is using nccl==2.21.5\nINFO 04-11 09:53:56 [pynccl.py:69] vLLM is using nccl==2.21.5\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:56 [utils.py:990] Found nccl from library librccl.so.1\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:56 [utils.py:990] Found nccl from library librccl.so.1\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:56 [pynccl.py:69] vLLM is using nccl==2.21.5\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:56 [pynccl.py:69] vLLM is using nccl==2.21.5\nINFO 04-11 09:53:57 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_cb7896c1'), local_subscribe_addr='ipc:///tmp/2bc5668d-2962-4291-81d6-857ec8d5f573', remote_subscribe_addr=None, remote_addr_ipv6=False)\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:57 [parallel_state.py:957] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:57 [parallel_state.py:957] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1\nINFO 04-11 09:53:57 [parallel_state.py:957] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:57 [parallel_state.py:957] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2\nINFO 04-11 09:53:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8...\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8...\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8...\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8...\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:57 [rocm.py:153] None is not supported in AMD GPUs.\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:57 [rocm.py:154] Using ROCmFlashAttention backend.\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:57 [rocm.py:153] None is not supported in AMD GPUs.\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:57 [rocm.py:154] Using ROCmFlashAttention backend.\nINFO 04-11 09:53:57 [rocm.py:153] None is not supported in AMD GPUs.\nINFO 04-11 09:53:57 [rocm.py:154] Using ROCmFlashAttention backend.\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:57 [rocm.py:153] None is not supported in AMD GPUs.\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:57 [rocm.py:154] Using ROCmFlashAttention backend.\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:57 [config.py:3351] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]\nINFO 04-11 09:53:57 [config.py:3351] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:57 [config.py:3351] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:57 [config.py:3351] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]\nWARNING 04-11 09:53:57 [rocm_flash_attn.py:468] Using irope in ROCm Flash Attention is not supported yet, it will fail back to global attention for long context.\nWARNING 04-11 09:53:57 [rocm_flash_attn.py:475] Using irope in V0 is not supported yet, it will fall back to global attention for long context.\nWARNING 04-11 09:53:57 [rocm_flash_attn.py:475] Using irope in V0 is not supported yet, it will fall back to global attention for long context.\n(VllmWorkerProcess pid=337) WARNING 04-11 09:53:57 [rocm_flash_attn.py:468] Using irope in ROCm Flash Attention is not supported yet, it will fail back to global attention for long context.\n(VllmWorkerProcess pid=337) WARNING 04-11 09:53:57 [rocm_flash_attn.py:475] Using irope in V0 is not supported yet, it will fall back to global attention for long context.\n(VllmWorkerProcess pid=336) WARNING 04-11 09:53:57 [rocm_flash_attn.py:468] Using irope in ROCm Flash Attention is not supported yet, it will fail back to global attention for long context.\n(VllmWorkerProcess pid=336) WARNING 04-11 09:53:57 [rocm_flash_attn.py:475] Using irope in V0 is not supported yet, it will fall back to global attention for long context.\n(VllmWorkerProcess pid=337) WARNING 04-11 09:53:57 [rocm_flash_attn.py:475] Using irope in V0 is not supported yet, it will fall back to global attention for long context.\n(VllmWorkerProcess pid=336) WARNING 04-11 09:53:57 [rocm_flash_attn.py:475] Using irope in V0 is not supported yet, it will fall back to global attention for long context.\n(VllmWorkerProcess pid=338) WARNING 04-11 09:53:57 [rocm_flash_attn.py:468] Using irope in ROCm Flash Attention is not supported yet, it will fail back to global attention for long context.\n(VllmWorkerProcess pid=338) WARNING 04-11 09:53:57 [rocm_flash_attn.py:475] Using irope in V0 is not supported yet, it will fall back to global attention for long context.\n(VllmWorkerProcess pid=338) WARNING 04-11 09:53:58 [rocm_flash_attn.py:475] Using irope in V0 is not supported yet, it will fall back to global attention for long context.\nERROR 04-11 09:53:58 [engine.py:448] For FP8 Fused MoE layers, only per-tensor scales for weights and activations are supported. Found num_bits=8 type='float' symmetric=True group_size=None strategy='channel' block_structure=None dynamic=False actorder=None observer='minmax' observer_kwargs={}, num_bits=8 type='float' symmetric=True group_size=None strategy='token' block_structure=None dynamic=True actorder=None observer=None observer_kwargs={}\nERROR 04-11 09:53:58 [engine.py:448] Traceback (most recent call last):\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 436, in run_mp_engine\nERROR 04-11 09:53:58 [engine.py:448]     engine = MQLLMEngine.from_vllm_config(\nERROR 04-11 09:53:58 [engine.py:448]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 128, in from_vllm_config\nERROR 04-11 09:53:58 [engine.py:448]     return cls(\nERROR 04-11 09:53:58 [engine.py:448]            ^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 82, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     self.engine = LLMEngine(*args, **kwargs)\nERROR 04-11 09:53:58 [engine.py:448]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 282, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     self.model_executor = executor_class(vllm_config=vllm_config, )\nERROR 04-11 09:53:58 [engine.py:448]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 286, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     super().__init__(*args, **kwargs)\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 52, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     self._init_executor()\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/mp_distributed_executor.py\", line 125, in _init_executor\nERROR 04-11 09:53:58 [engine.py:448]     self._run_workers(\"load_model\",\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/mp_distributed_executor.py\", line 185, in _run_workers\nERROR 04-11 09:53:58 [engine.py:448]     driver_worker_output = run_method(self.driver_worker, sent_method,\nERROR 04-11 09:53:58 [engine.py:448]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2363, in run_method\nERROR 04-11 09:53:58 [engine.py:448]     return func(*args, **kwargs)\nERROR 04-11 09:53:58 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 183, in load_model\nERROR 04-11 09:53:58 [engine.py:448]     self.model_runner.load_model()\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1113, in load_model\nERROR 04-11 09:53:58 [engine.py:448]     self.model = get_model(vllm_config=self.vllm_config)\nERROR 04-11 09:53:58 [engine.py:448]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\nERROR 04-11 09:53:58 [engine.py:448]     return loader.load_model(vllm_config=vllm_config)\nERROR 04-11 09:53:58 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 452, in load_model\nERROR 04-11 09:53:58 [engine.py:448]     model = _initialize_model(vllm_config=vllm_config)\nERROR 04-11 09:53:58 [engine.py:448]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\nERROR 04-11 09:53:58 [engine.py:448]     return model_class(vllm_config=vllm_config, prefix=prefix)\nERROR 04-11 09:53:58 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/mllama4.py\", line 691, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     self.language_model = _initialize_model(\nERROR 04-11 09:53:58 [engine.py:448]                           ^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\nERROR 04-11 09:53:58 [engine.py:448]     return model_class(vllm_config=vllm_config, prefix=prefix)\nERROR 04-11 09:53:58 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 481, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     super().__init__(vllm_config=vllm_config,\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 486, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     self.model = self._init_model(vllm_config=vllm_config,\nERROR 04-11 09:53:58 [engine.py:448]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 489, in _init_model\nERROR 04-11 09:53:58 [engine.py:448]     return Llama4Model(vllm_config=vllm_config,\nERROR 04-11 09:53:58 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 151, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 335, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     super().__init__(vllm_config=vllm_config,\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 151, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 321, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     self.start_layer, self.end_layer, self.layers = make_layers(\nERROR 04-11 09:53:58 [engine.py:448]                                                     ^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 610, in make_layers\nERROR 04-11 09:53:58 [engine.py:448]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\nERROR 04-11 09:53:58 [engine.py:448]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 323, in <lambda>\nERROR 04-11 09:53:58 [engine.py:448]     lambda prefix: layer_type(config=config,\nERROR 04-11 09:53:58 [engine.py:448]                    ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 284, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     self.feed_forward = Llama4MoE(\nERROR 04-11 09:53:58 [engine.py:448]                         ^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 73, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     self.experts = FusedMoE(\nERROR 04-11 09:53:58 [engine.py:448]                    ^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 501, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     self.quant_method = quant_config.get_quant_method(self, prefix)\nERROR 04-11 09:53:58 [engine.py:448]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py\", line 99, in get_quant_method\nERROR 04-11 09:53:58 [engine.py:448]     return CompressedTensorsMoEMethod.get_moe_method(\nERROR 04-11 09:53:58 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py\", line 59, in get_moe_method\nERROR 04-11 09:53:58 [engine.py:448]     return CompressedTensorsW8A8Fp8MoEMethod(quant_config)\nERROR 04-11 09:53:58 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py\", line 79, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     raise ValueError(\nERROR 04-11 09:53:58 [engine.py:448] ValueError: For FP8 Fused MoE layers, only per-tensor scales for weights and activations are supported. Found num_bits=8 type='float' symmetric=True group_size=None strategy='channel' block_structure=None dynamic=False actorder=None observer='minmax' observer_kwargs={}, num_bits=8 type='float' symmetric=True group_size=None strategy='token' block_structure=None dynamic=True actorder=None observer=None observer_kwargs={}\nERROR 04-11 09:53:58 [multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 338 died, exit code: -15\nINFO 04-11 09:53:58 [multiproc_worker_utils.py:124] Killing local vLLM worker processes\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 450, in run_mp_engine\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 436, in run_mp_engine\n    engine = MQLLMEngine.from_vllm_config(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 128, in from_vllm_config\n    return cls(\n           ^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 82, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 282, in __init__\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 286, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 52, in __init__\n    self._init_executor()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/mp_distributed_executor.py\", line 125, in _init_executor\n    self._run_workers(\"load_model\",\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/mp_distributed_executor.py\", line 185, in _run_workers\n    driver_worker_output = run_method(self.driver_worker, sent_method,\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2363, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 183, in load_model\n    self.model_runner.load_model()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1113, in load_model\n    self.model = get_model(vllm_config=self.vllm_config)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n    return loader.load_model(vllm_config=vllm_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 452, in load_model\n    model = _initialize_model(vllm_config=vllm_config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\n    return model_class(vllm_config=vllm_config, prefix=prefix)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/mllama4.py\", line 691, in __init__\n    self.language_model = _initialize_model(\n                          ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\n    return model_class(vllm_config=vllm_config, prefix=prefix)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 481, in __init__\n    super().__init__(vllm_config=vllm_config,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 486, in __init__\n    self.model = self._init_model(vllm_config=vllm_config,\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 489, in _init_model\n    return Llama4Model(vllm_config=vllm_config,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 151, in __init__\n    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 335, in __init__\n    super().__init__(vllm_config=vllm_config,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 151, in __init__\n    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 321, in __init__\n    self.start_layer, self.end_layer, self.layers = make_layers(\n                                                    ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 610, in make_layers\n    maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 323, in <lambda>\n    lambda prefix: layer_type(config=config,\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 284, in __init__\n    self.feed_forward = Llama4MoE(\n                        ^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 73, in __init__\n    self.experts = FusedMoE(\n                   ^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 501, in __init__\n    self.quant_method = quant_config.get_quant_method(self, prefix)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py\", line 99, in get_quant_method\n    return CompressedTensorsMoEMethod.get_moe_method(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py\", line 59, in get_moe_method\n    return CompressedTensorsW8A8Fp8MoEMethod(quant_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py\", line 79, in __init__\n    raise ValueError(\nValueError: For FP8 Fused MoE layers, only per-tensor scales for weights and activations are supported. Found num_bits=8 type='float' symmetric=True group_size=None strategy='channel' block_structure=None dynamic=False actorder=None observer='minmax' observer_kwargs={}, num_bits=8 type='float' symmetric=True group_size=None strategy='token' block_structure=None dynamic=True actorder=None observer=None observer_kwargs={}\n[rank0]:[W411 09:53:58.829246061 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nTraceback (most recent call last):\n  File \"/usr/local/bin/vllm\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/main.py\", line 51, in main\n    args.dispatch_function(args)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/serve.py\", line 27, in cmd\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1069, in run_server\n    async with build_async_engine_client(args) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 146, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 269, in build_async_engine_client_from_engine_args\n    raise RuntimeError(\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\n/usr/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n```\n\ntldr:\n```\nValueError: For FP8 Fused MoE layers, only per-tensor scales for weights and activations are supported. Found num_bits=8 type='float' symmetric=True group_size=None strategy='channel' block_structure=None dynamic=False actorder=None observer='minmax' observer_kwargs={}, num_bits=8 type='float' symmetric=True group_size=None strategy='token' block_structure=None dynamic=True actorder=None observer=None observer_kwargs={}\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2025-04-11T10:27:28+00:00",
    "closed_at": "2025-04-23T12:07:16+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16474/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16474"
  },
  {
    "number": 20125,
    "title": "[Bug][Rocm] Garbage Response from vLLM When Using Tensor Parallelism on AMD CPX/NPS4 Partitioned GPUs",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nI have attached output file.\n```\n\n[vllm_collect_env_output.txt](https://github.com/user-attachments/files/20926332/vllm_collect_env_output.txt)\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n**Steps to reproduce:**\nWe referred to doc:  [Steps to Run a vLLM Workload on AMD partition](https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/gpu-partitioning/mi300x/run-vllm.html).\n- [ ] **Do CPS/NPS4 Partition**\n`sudo amd-smi set --memory-partition NPS4`\n\n\n- [ ] **Launch container**\n`docker run -it --network=host --group-add=video --ipc=host --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --device /dev/kfd --device /dev/dri rocm/vllm:latest /bin/bash`\n\n\n - [ ] **Set Env**\n```\nexport HF_TOKEN=<token>\nexport HIP_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n```\n\n\n- [ ] `vllm serve meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 8`\n\n\n- [ ] **Query the model**\n```\ncurl http://localhost:8000/v1/chat/completions \\\n  -H 'Content-Type: application/json' \\\n  --data '{\n    \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What is Deep Learning?\"}\n    ]\n  }'\n```\n\n**Actual behaviour:**\nGarbled and nonsensical output as below\n\n{\"id\":\"chatcmpl-5488b13e1910409d884196f041b34b0b\",\"object\":\"chat.completion\",\"created\":1750923599,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"reasoning_content\":null,\"content\":\"Deep c\u00e9libai8://%) the:// Bon the:// Bachelor the://\n\u2026\n\u0434\u043e\u0441\u0442:// False capital://{ progress:// Barb n\u7a0b\ufffd\u7a0b-disable\",\"tool_calls\":[]},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":128001}],\"usage\":{\"prompt_tokens\":46,\"total_tokens\":4602,\"completion_tokens\":4556,\"prompt_tokens_details\":null},\"prompt_logprobs\":null,\"kv_transfer_params\":null}\n\n**Expected behaviour:**\nMeaningful Response\n\n\n**Additional Information:**\n\n1. **vLLM works as expected with a single CPX partition.**\n**How to reproduce:**\n \n\n- [ ] Launch container with only 1 CPX partition (/dev/dri/renderD128 )\n\n`docker run -it --network=host --group-add=video --ipc=host --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --device /dev/kfd --device=/dev/dri/renderD128  rocm/vllm:latest /bin/bash`\n\n \n\n- [ ] `vllm serve meta-llama/Llama-3.1-8B-Instruct --max-model-len 2048`\n\n\n- [ ] Query the model as above.\n           \n\n2. **This likely isn't a ROCm or hardware issue, but possibly a vLLM backend issue. We did some sanity checks and Validation to rule out issues from Hardware, ROCm, PyTorch, RCCL:**\n\n- [ ] **Vanilla PyTorch matrix multiplication and all-reduce operation gives the correct result.**\nHere every process creates a 2x2 matrix on its own GPU, performs matrix multiplication,and then performs an all-reduce operation, which communicates the results across all GPUs. We verified the calculation and it is correct. This means there should not be any issues in accuracy in distributed setup.\n\n- [ ] **Launch process on each partition using mpirun**\nmpirun --allow-run-as-root \\\n  -np 1 -x ROCR_VISIBLE_DEVICES=0 python mpi_test.py \\\n  -np 1 -x ROCR_VISIBLE_DEVICES=1 python mpi_test.py\nEach MPI rank sees only the partition assigned to it via      ROCR_VISIBLE_DEVICES. This further reinforces that partition isolation is functioning.\n\n- [ ] **Finally conducted rccl-tests with 8 partitions and this works too.**\n\n```\n./build/all_reduce_perf -b 2M -e 8M -f 2 -g 8\n# nThread 1 nGpus 8 minBytes 2097152 maxBytes 8388608 step: 2(factor) warmup iters: 5 iters: 20 agg iters: 1 validation: 1 graph: 0\n#\n\nrccl-tests: Version develop:b0a3841\n# Using devices\n#  Rank  0 Group  0 Pid   1158 on ENC1-CLS01-SVR07 device  0 [0000:1b:00] AMD Instinct MI300X\n#  Rank  1 Group  0 Pid   1158 on ENC1-CLS01-SVR07 device  1 [0000:1b:00] AMD Instinct MI300X\n#  Rank  2 Group  0 Pid   1158 on ENC1-CLS01-SVR07 device  2 [0000:1b:00] AMD Instinct MI300X\n#  Rank  3 Group  0 Pid   1158 on ENC1-CLS01-SVR07 device  3 [0000:1b:00] AMD Instinct MI300X\n#  Rank  4 Group  0 Pid   1158 on ENC1-CLS01-SVR07 device  4 [0000:1b:00] AMD Instinct MI300X\n#  Rank  5 Group  0 Pid   1158 on ENC1-CLS01-SVR07 device  5 [0000:1b:00] AMD Instinct MI300X\n#  Rank  6 Group  0 Pid   1158 on ENC1-CLS01-SVR07 device  6 [0000:1b:00] AMD Instinct MI300X\n#  Rank  7 Group  0 Pid   1158 on ENC1-CLS01-SVR07 device  7 [0000:1b:00] AMD Instinct MI300X\n#\n#                                                              out-of-place                       in-place          \n#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong\n#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)       \n     2097152        524288     float     sum      -1    75.02   27.95   48.92      0    72.64   28.87   50.52      0\n     4194304       1048576     float     sum      -1    93.79   44.72   78.26      0    89.37   46.93   82.13      0\n     8388608       2097152     float     sum      -1    165.7   50.62   88.59      0    159.6   52.57   92.00      0\n# Errors with asterisks indicate errors that have exceeded the maximum threshold.\n# Out of bounds values : 0 OK\n# Avg bus bandwidth    : 73.4045\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "open",
    "created_at": "2025-06-26T13:18:47+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20125/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20125"
  },
  {
    "number": 11249,
    "title": "[Bug]: ROCM with AWQ",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.6.0.dev20241113+rocm6.2\r\nIs debug build: False\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: 6.2.41133-dd7f95766\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: 18.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.2.0 24292 26466ce804ac523b398608f17388eb6d605a3f09)\r\nCMake version: version 3.26.4\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.8.0-50-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: Radeon RX 7900 XTX (gfx1100)\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: 6.2.41133\r\nMIOpen runtime version: 3.2.0\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nByte Order:                           Little Endian\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nCPU(s):                               48\r\nOn-line CPU(s) list:                  0-47\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   24\r\nSocket(s):                            1\r\nNUMA node(s):                         1\r\nVendor ID:                            AuthenticAMD\r\nCPU family:                           25\r\nModel:                                8\r\nModel name:                           AMD Ryzen Threadripper PRO 5965WX 24-Cores\r\nStepping:                             2\r\nFrequency boost:                      enabled\r\nCPU MHz:                              1800.000\r\nCPU max MHz:                          7021.0928\r\nCPU min MHz:                          1800.0000\r\nBogoMIPS:                             7585.77\r\nVirtualization:                       AMD-V\r\nL1d cache:                            768 KiB\r\nL1i cache:                            768 KiB\r\nL2 cache:                             12 MiB\r\nL3 cache:                             128 MiB\r\nNUMA node0 CPU(s):                    0-47\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm debug_swap\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.8.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] optree==0.9.1\r\n[pip3] pytorch-triton-rocm==3.1.0+cf34004b8a\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.6.0.dev20241113+rocm6.2\r\n[pip3] torchvision==0.20.0.dev20241113+rocm6.2\r\n[pip3] transformers==4.47.0\r\n[pip3] triton==3.0.0\r\n[conda] No relevant packages\r\nROCM Version: 6.2.41133-dd7f95766\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.4.post2.dev383+g2ca830db\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\n============================ ROCm System Management Interface ============================\r\n================================ Weight between two GPUs =================================\r\n       GPU0         GPU1\r\nGPU0   0            40\r\nGPU1   40           0\r\n\r\n================================= Hops between two GPUs ==================================\r\n       GPU0         GPU1\r\nGPU0   0            2\r\nGPU1   2            0\r\n\r\n=============================== Link Type between two GPUs ===============================\r\n       GPU0         GPU1\r\nGPU0   0            PCIE\r\nGPU1   PCIE         0\r\n\r\n======================================= Numa Nodes =======================================\r\nGPU[0]\t\t: (Topology) Numa Node: 0\r\nGPU[0]\t\t: (Topology) Numa Affinity: -1\r\nGPU[1]\t\t: (Topology) Numa Node: 0\r\nGPU[1]\t\t: (Topology) Numa Affinity: -1\r\n================================== End of ROCm SMI Log ===================================\r\n\r\nPYTORCH_TESTING_DEVICE_ONLY_FOR=cuda\r\nVLLM_WORKER_MULTIPROC_METHOD=spawn\r\nPYTORCH_TEST_WITH_ROCM=1\r\nPYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\r\nPYTORCH_ROCM_ARCH=gfx908;gfx90a;gfx942;gfx1100\r\nVLLM_DO_NOT_TRACK=1\r\nMAX_JOBS=8\r\nLD_LIBRARY_PATH=/opt/conda/envs/py_3.9/lib/python3.9/site-packages/cv2/../../lib64:/opt/ompi/lib:/opt/rocm/lib:/usr/local/lib::/opt/rocm/lib/:/libtorch/lib:\r\nVLLM_NO_USAGE_STATS=1\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nUsing the latest vllm from git, I've created a vllm-rocm image based off the Dockerfile.rocm.\r\n\r\nI proceeded to try to run an AWQ model like so:\r\n```\r\nvllm serve --model casperhansen/llama-3.3-70b-instruct-awq --served-model-name Llama-3.3-70B-Instruct --enforce-eager --quantization awq --max_model_len 8192 --gpu-memory-utilization 0.95 --tensor-parallel-size 2\r\n```\r\n\r\nbut getting this error:\r\n\r\n<details>\r\n\r\n```text\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 _custom_ops.py:61] Error in calling custom op awq_dequantize: '_OpNamespace' '_C' object has no attribute 'awq_dequantize'\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 _custom_ops.py:61] Possibly you have built or installed an obsolete version of vllm.\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 _custom_ops.py:61] Please try a clean build and install of vllm,or remove old built files such as vllm/*cpython*.so and build/ .\r\n(VllmWorkerProcess pid=1492) INFO 12-17 03:22:04 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20241217-032204.pkl...\r\n(VllmWorkerProcess pid=1492) INFO 12-17 03:22:04 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20241217-032204.pkl.\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236] Exception in worker VllmWorkerProcess while processing method determine_num_available_blocks.\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236] Traceback (most recent call last):\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/worker/model_runner_base.py\", line 116, in _wrapper\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/worker/model_runner.py\", line 1683, in execute_model\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     hidden_or_intermediate_states = model_executable(\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1740, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1751, in _call_impl\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/model_executor/models/llama.py\", line 568, in forward\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     model_output = self.model(input_ids, positions, kv_caches,\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/compilation/decorators.py\", line 168, in __call__\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return self.forward(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/model_executor/models/llama.py\", line 360, in forward\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     hidden_states, residual = layer(positions, hidden_states,\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1740, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1751, in _call_impl\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/model_executor/models/llama.py\", line 275, in forward\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     hidden_states = self.self_attn(positions=positions,\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1740, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1751, in _call_impl\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/model_executor/models/llama.py\", line 202, in forward\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     qkv, _ = self.qkv_proj(hidden_states)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1740, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1751, in _call_impl\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/model_executor/layers/linear.py\", line 373, in forward\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     output_parallel = self.quant_method.apply(self, input_, bias)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/model_executor/layers/quantization/awq.py\", line 174, in apply\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     out = ops.awq_dequantize(qweight, scales, qzeros, 0, 0, 0)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/_custom_ops.py\", line 62, in wrapper\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     raise e\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/_custom_ops.py\", line 44, in wrapper\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return fn(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/_custom_ops.py\", line 281, in awq_dequantize\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return torch.ops._C.awq_dequantize(qweight, scales, zeros, split_k_iters,\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/_ops.py\", line 1232, in __getattr__\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     raise AttributeError(\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236] AttributeError: '_OpNamespace' '_C' object has no attribute 'awq_dequantize'\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236] The above exception was the direct cause of the following exception:\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236] Traceback (most recent call last):\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/executor/multiproc_worker_utils.py\", line 230, in _run_worker_process\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/worker/worker.py\", line 199, in determine_num_available_blocks\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     self.model_runner.profile_run()\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/worker/model_runner.py\", line 1329, in profile_run\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/worker/model_runner_base.py\", line 152, in _wrapper\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     raise type(err)(\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236] AttributeError: Error in model execution (input dumped to /tmp/err_execute_model_input_20241217-032204.pkl): '_OpNamespace' '_C' object has no attribute 'awq_dequantize'\r\n/vllm-workspace/vllm/model_executor/layers/quantization/awq.py:175: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)\r\n  out = torch.matmul(reshaped_x, out)\r\n```\r\n\r\n</details>\r\n\r\n Tried searching but couldn't find anything related to this. I tried installing autoawq inside the container with:\r\n \r\n```\r\npip install autoawq\r\n```\r\n \r\n but this didn't get me anywhere. I read the docs and https://rocm.docs.amd.com/en/latest/how-to/tuning-guides/mi300x/workload.html#awq-quantization seems to suggest AWQ should work out of the box but I havent had any success. I do have this working on the CUDA version using `docker.io/vllm/vllm-openai:latest` but trying to get a rocm version going. Any ideas what im doing wrong?\r\n",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2024-12-17T03:37:54+00:00",
    "closed_at": "2024-12-18T02:57:04+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11249/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11249"
  },
  {
    "number": 19867,
    "title": "[Bug]: AiterFlashAttentionImpl.__init__() got multiple values for argument 'use_irope' for llama4 model",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWe hit an exception on running llama4 models with latest code on ROCm V1:\n\n```\n(VllmWorker rank=2 pid=267) ERROR 06-19 01:00:39 [multiproc_executor.py:488] TypeError: AiterFlashAttentionImpl.__init__() got multiple values for argument 'use_irope'\n```\nCurrent work-around:\nTo turn off AITER_MHA, with VLLM_ROCM_USE_AITER_MHA=0\n\n\nProposal:\n\n- [ ] Fix the bug (the team is working on it)\n- [ ] Add a end-to-end test for one of the small llama4 models\n- [ ] \n\nThe motivation for adding an end to end test for a small version of llama4 models, is that we have seen issues of breaking llama4 models in the past because of lacking such tests.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2025-06-19T14:36:59+00:00",
    "closed_at": "2025-07-14T17:39:11+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19867/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19867"
  },
  {
    "number": 6807,
    "title": "[Bug][ROCm] The embedding layer does not support long inputs",
    "body": "### Your current environment\n\n8xMI300x machine using the docker image built with `Dockerfile.rocm`.\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.7.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] optree==0.9.1\r\n[pip3] pytorch-triton-rocm==3.0.0+21eae954ef\r\n[pip3] torch==2.5.0.dev20240710+rocm6.1\r\n[pip3] torchaudio==2.4.0.dev20240710+rocm6.1\r\n[pip3] torchvision==0.20.0.dev20240710+rocm6.1\r\n[pip3] transformers==4.43.2\r\n[pip3] triton==3.0.0\r\n[conda] No relevant packages\r\nROCM Version: 6.1.40093-bd86f1708\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.3.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\n\n### \ud83d\udc1b Describe the bug\n\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nwith torch.inference_mode():\r\n    NUM_TOKENS = 128 * 1024\r\n    HIDDEN_SIZE = 16 * 1024\r\n    VOCAB_SIZE = 128 * 1024\r\n    DTYPE = torch.bfloat16\r\n    x = torch.randint(VOCAB_SIZE, (NUM_TOKENS,), dtype=torch.int64, device=\"cuda\")\r\n    embedding = nn.Embedding(VOCAB_SIZE, HIDDEN_SIZE, dtype=DTYPE, device=\"cuda\")\r\n    y = embedding(x)\r\n    torch.cuda.synchronize()\r\n```\r\n\r\nThe above script raises the following error:\r\n```\r\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\nRuntimeError: HIP error: invalid configuration argument\r\nHIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing AMD_SERIALIZE_KERNEL=3\r\nCompile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\r\n```\r\n\r\nwhile it works when `NUM_TOKENS=32 * 1024`.",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2024-07-25T23:58:46+00:00",
    "closed_at": "2024-07-27T03:16:15+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6807/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/6807"
  },
  {
    "number": 2217,
    "title": "Error when Running HIPGraph with TP 8",
    "body": "Command that was run:\r\n```\r\npython benchmark_throughput.py -tp 8 --model meta-llama_Llama-2-70b-chat-hf --dataset ShareGPT_V3_unfiltered_cleaned_split.json \r\n```\r\nError Logs:\r\n```\r\n...\r\nensor_parallel_size=8, quantization=None, enforce_eager=False, seed=0)                                                                                                                                      \r\n(RayWorkerVllm pid=343834) WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:                                                                                               \r\n(RayWorkerVllm pid=343834)     PyTorch 2.1.1+cu121 with CUDA 1201 (you have 2.1.1+rocm5.6)                                                                                                                   \r\n(RayWorkerVllm pid=343834)     Python  3.10.13 (you have 3.10.13)                                                                                                                                            \r\n(RayWorkerVllm pid=343834)   Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)                                                                                \r\n(RayWorkerVllm pid=343834)   Memory-efficient attention, SwiGLU, sparse and more won't be available.                                                                                                         \r\n(RayWorkerVllm pid=343834)   Set XFORMERS_MORE_DETAILS=1 for more details                                                                                                                                    \r\nINFO 12-20 08:54:51 llm_engine.py:223] # GPU blocks: 64920, # CPU blocks: 6553                                                                                                                               \r\n(RayWorkerVllm pid=343834) INFO 12-20 08:54:51 model_runner.py:394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode,\r\n set 'enforce_eager=True' or use '--enforce-eager' in the CLI.                                                                                                                                               \r\n(RayWorkerVllm pid=343834) [W HIPGraph.cpp:146] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())                                                        \r\n(RayWorkerVllm pid=343839) WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for: [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disabl\r\ne log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)                                                                            \r\n(RayWorkerVllm pid=343839)     PyTorch 2.1.1+cu121 with CUDA 1201 (you have 2.1.1+rocm5.6) [repeated 7x across cluster]                                                                                      \r\n(RayWorkerVllm pid=343839)     Python  3.10.13 (you have 3.10.13) [repeated 7x across cluster]                                                                                                               \r\n(RayWorkerVllm pid=343839)   Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers) [repeated 7x across cluster]                                                   \r\n(RayWorkerVllm pid=343839)   Memory-efficient attention, SwiGLU, sparse and more won't be available. [repeated 7x across cluster]                                                                            \r\n(RayWorkerVllm pid=343839)   Set XFORMERS_MORE_DETAILS=1 for more details [repeated 7x across cluster]                                                                                                       \r\n(RayWorkerVllm pid=343834) INFO 12-20 08:55:26 model_runner.py:437] Graph capturing finished in 35 secs.                                                                                                     \r\n(RayWorkerVllm pid=343837) INFO 12-20 08:54:51 model_runner.py:394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode,\r\n set 'enforce_eager=True' or use '--enforce-eager' in the CLI. [repeated 7x across cluster]                                                                                                                  \r\nProcessed prompts:  48%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                            | 479/1000 [03:28<03:53,  2.23it/s]\r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed. \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.         [88/499]\r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed.                 \r\n(RayWorkerVllm pid=343836) :0:rocdevice.cpp            :2778: 8142196379168 us: 343836: [tid:0x7ed990ff9640] Callback: Queue 0x7ed86a000000 aborting with error : HSA_STATUS_ERROR_EXCEPTION: An HSAIL operat\r\nion resulted in a hardware exception. code: 0x1016                                                                                                                                                           \r\n(RayWorkerVllm pid=343836) *** SIGABRT received at time=1703062736 on cpu 75 ***                                                                                                                             \r\n(RayWorkerVllm pid=343836) PC: @     0x7f0a6505aa7c  (unknown)  pthread_kill                                                                                                                                 \r\n(RayWorkerVllm pid=343836)     @     0x7f0a65006520  (unknown)  (unknown)                                                                                                                                    \r\n(RayWorkerVllm pid=343836) [2023-12-20 08:58:56,718 E 343836 345039] logging.cc:361: *** SIGABRT received at time=1703062736 on cpu 75 ***                                                                   \r\n(RayWorkerVllm pid=343836) [2023-12-20 08:58:56,718 E 343836 345039] logging.cc:361: PC: @     0x7f0a6505aa7c  (unknown)  pthread_kill                                                                       \r\n(RayWorkerVllm pid=343836) [2023-12-20 08:58:56,719 E 343836 345039] logging.cc:361:     @     0x7f0a65006520  (unknown)  (unknown)                                                                          \r\n(RayWorkerVllm pid=343836) Fatal Python error: Aborted                                                                                                                                                       \r\n(RayWorkerVllm pid=343836)                                                                                                                                                                                   \r\n(RayWorkerVllm pid=343836)                                                                                                                                                                                   \r\n(RayWorkerVllm pid=343836) Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, uvloop.loop, ray._raylet, numpy.core._multiarray\r\n_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, num\r\npy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch\r\n._C._special, pydantic.typing, pydantic.errors, pydantic.version, pydantic.utils, pydantic.class_validators, pydantic.config, pydantic.color, pydantic.datetime_parse, pydantic.validators, pydantic.networks\r\n, pydantic.types, pydantic.json, pydantic.error_wrappers, pydantic.fields, pydantic.parse, pydantic.schema, pydantic.main, pydantic.dataclasses, pydantic.annotated_types, pydantic.decorator, pydantic.env_s\r\nettings, pydantic.tools, pydantic, sentencepiece._sentencepiece, pyarrow.lib, pyarrow._hdfsio, pyarrow._fs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, pandas._libs.tslibs.np_datetime, pandas._libs.tslib\r\ns.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslib\r\ns.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs\r\n.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pandas._libs.ops, pyarr\r\now._compute, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.indexing, pandas._libs.index, pandas._libs.internals, pandas._libs.join, pandas._libs.writers, pandas._libs.window.ag\r\ngregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._parquet, pyarrow._json (total: 100)\r\n(RayWorkerVllm pid=343839) [W HIPGraph.cpp:146] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator()) [repeated 7x across cluster]                           \r\n2023-12-20 08:58:56,879 WARNING worker.py:2074 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask \r\nID: ffffffffffffffff1eb792b3342f34ed845a43fa01000000 Worker ID: e9bb8aa7fda24dd067e8ad733d4e24189714c3bed733752ec28abd7b Node ID: d66648ef8fe477f6e2f72c34993065a8f40a43fd53938766db8f6c7a Worker IP address:\r\n 172.17.0.2 Worker port: 33347 Worker PID: 343836 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root cau\r\nses. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.          \r\nTraceback (most recent call last):                                                                                                                                                                           \r\n  File \"/home/aac/apps/vllm-rocm/vllm-rocm/benchmarks/benchmark_throughput.py\", line 318, in <module>                                                                                                        \r\n    main(args)                                                                                                                                                                                               \r\n  File \"/home/aac/apps/vllm-rocm/vllm-rocm/benchmarks/benchmark_throughput.py\", line 205, in main                                                                                                            \r\n    elapsed_time = run_vllm(requests, args.model, args.tokenizer,                                                                                                                                            \r\n  File \"/home/aac/apps/vllm-rocm/vllm-rocm/benchmarks/benchmark_throughput.py\", line 107, in run_vllm                                                                                                        \r\n    llm._run_engine(use_tqdm=True)                                                                                                                                                                           \r\n  File \"/home/aac/libs/anaconda3/envs/vllm-rocm/lib/python3.10/site-packages/vllm-0.2.6+rocm563-py3.10-linux-x86_64.egg/vllm/entrypoints/llm.py\", line 185, in _run_engine                                   \r\n    step_outputs = self.llm_engine.step()                                                                                                                                                                    \r\n  File \"/home/aac/libs/anaconda3/envs/vllm-rocm/lib/python3.10/site-packages/vllm-0.2.6+rocm563-py3.10-linux-x86_64.egg/vllm/engine/llm_engine.py\", line 589, in step                                        \r\n    output = self._run_workers(                                                                                                                                                                              \r\n  File \"/home/aac/libs/anaconda3/envs/vllm-rocm/lib/python3.10/site-packages/vllm-0.2.6+rocm563-py3.10-linux-x86_64.egg/vllm/engine/llm_engine.py\", line 763, in _run_workers                                \r\n    self._run_workers_in_batch(workers, method, *args, **kwargs))                                                                                                                                            \r\n  File \"/home/aac/libs/anaconda3/envs/vllm-rocm/lib/python3.10/site-packages/vllm-0.2.6+rocm563-py3.10-linux-x86_64.egg/vllm/engine/llm_engine.py\", line 740, in _run_workers_in_batch                       \r\n    all_outputs = ray.get(all_outputs)                                                                                                                                                                       \r\n  File \"/home/aac/libs/anaconda3/envs/vllm-rocm/lib/python3.10/site-packages/ray-2.8.1-py3.10-linux-x86_64.egg/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper                                \r\n    return fn(*args, **kwargs)                                                                                                                                                                               \r\n  File \"/home/aac/libs/anaconda3/envs/vllm-rocm/lib/python3.10/site-packages/ray-2.8.1-py3.10-linux-x86_64.egg/ray/_private/client_mode_hook.py\", line 103, in wrapper                                       \r\n    return func(*args, **kwargs)                                                                                                                                                                             \r\n  File \"/home/aac/libs/anaconda3/envs/vllm-rocm/lib/python3.10/site-packages/ray-2.8.1-py3.10-linux-x86_64.egg/ray/_private/worker.py\", line 2565, in get                                                    \r\n    raise value                                                                                                                                                                                              \r\nray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.                                                                                                                        \r\n        class_name: RayWorkerVllm                                                                                                                                                                            \r\n        actor_id: 1eb792b3342f34ed845a43fa01000000                                                                                                                                                           \r\n        pid: 343836                                                                                                                                                                                          \r\n        namespace: 83333617-d010-403c-97cf-94e07ad5000f                                                                                                                                                      \r\n        ip: 172.17.0.2                                                                                                                                                                                       \r\nThe actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential roo\r\nt causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.     \r\n2023-12-20 08:58:56,988 WARNING worker.py:2074 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask \r\nID: ffffffffffffffff356fd56fbe6a8304644da78601000000 Worker ID: 127d225aef840be899f32c0664b19be0d2112ace50eb50e6c61ecab6 Node ID: d66648ef8fe477f6e2f72c34993065a8f40a43fd53938766db8f6c7a Worker IP address:\r\n 172.17.0.2 Worker port: 45847 Worker PID: 343840 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root cau\r\nses. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\r\n2023-12-20 08:58:56,989 WARNING worker.py:2074 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask \r\nID: ffffffffffffffff6c3fb285bcc356e35f7883ed01000000 Worker ID: 92a75c925225ae7fec52fa4c4974f0841e3f8fd3a0482e82defd9b56 Node ID: d66648ef8fe477f6e2f72c34993065a8f40a43fd53938766db8f6c7a Worker IP address:\r\n 172.17.0.2 Worker port: 38807 Worker PID: 343834 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root cau\r\nses. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\r\n2023-12-20 08:58:57,007 WARNING worker.py:2074 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask \r\nID: ffffffffffffffff6c3fb285bcc356e35f7883ed01000000 Worker ID: 92a75c925225ae7fec52fa4c4974f0841e3f8fd3a0482e82defd9b56 Node ID: d66648ef8fe477f6e2f72c34993065a8f40a43fd53938766db8f6c7a Worker IP a[0/499] 172.17.0.2 Worker port: 38807 Worker PID: 343834 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\r\n2023-12-20 08:58:57,007 WARNING worker.py:2074 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: fffffffffffffffff0750ef9d99ba21c366a30a501000000 Worker ID: 1eba0f212abc9d3f83cd39cf4a6d1da0a6e300e882473d216998def4 Node ID: d66648ef8fe477f6e2f72c34993065a8f40a43fd53938766db8f6c7a Worker IP address: 172.17.0.2 Worker port: 44109 Worker PID: 343835 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\r\n2023-12-20 08:58:57,012 WARNING worker.py:2074 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff972d660ef7ea553cff93b2eb01000000 Worker ID: 6027b8e1cf656e3b569024c7dcb6e4854793942bb642f22ad919a820 Node ID: d66648ef8fe477f6e2f72c34993065a8f40a43fd53938766db8f6c7a Worker IP address: 172.17.0.2 Worker port: 42809 Worker PID: 343833 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\r\n2023-12-20 08:58:57,077 WARNING worker.py:2074 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff3c787e9c74cdae7903f4c7bb01000000 Worker ID: c38325cc5acd845557107b4255482cba6c65f1180a9ff73161b5bfcd Node ID: d66648ef8fe477f6e2f72c34993065a8f40a43fd53938766db8f6c7a Worker IP address: 172.17.0.2 Worker port: 36369 Worker PID: 343839 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\r\n2023-12-20 08:58:57,134 WARNING worker.py:2074 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff8253f59aff950e98e64eb3bc01000000 Worker ID: 39144eca4ab118e32a72ac2791cdb101ca37b4d17cc871859213e80f Node ID: d66648ef8fe477f6e2f72c34993065a8f40a43fd53938766db8f6c7a Worker IP address: 172.17.0.2 Worker port: 44531 Worker PID: 343837 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\r\n2023-12-20 08:58:57,156 WARNING worker.py:2074 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffa6d1af73a1a334451a12b33201000000 Worker ID: bede58021b8753552e961ab328ca01afb3b0c38eca8ca7f95efe729b Node ID: d66648ef8fe477f6e2f72c34993065a8f40a43fd53938766db8f6c7a Worker IP address: 172.17.0.2 Worker port: 33089 Worker PID: 343838 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\r\n(RayWorkerVllm pid=343837) INFO 12-20 08:55:26 model_runner.py:437] Graph capturing finished in 35 secs. [repeated 7x across cluster]\r\n(RayWorkerVllm pid=343837) /pytorch/aten/src/ATen/native/hip/IndexKernel.hip:94: operator(): Device-side assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"' failed. [repeated 448x across cluster]\r\n(RayWorkerVllm pid=343837) :0:rocdevice.cpp            :2778: 8142196625371 us: 343837: [tid:0x7f9f777fe640] Callback: Queue 0x7f9f75200000 aborting with error : HSA_STATUS_ERROR_EXCEPTION: An HSAIL operation resulted in a hardware exception. code: 0x1016 [repeated 7x across cluster]\r\n(RayWorkerVllm pid=343837) *** SIGABRT received at time=1703062736 on cpu 66 *** [repeated 7x across cluster]\r\n(RayWorkerVllm pid=343837) PC: @     0x7fd067f97a7c  (unknown)  pthread_kill [repeated 7x across cluster]\r\n(RayWorkerVllm pid=343837)     @     0x7fd067f43520  (unknown)  (unknown) [repeated 7x across cluster]\r\n(RayWorkerVllm pid=343837) [2023-12-20 08:58:56,965 E 343837 345036] logging.cc:361: *** SIGABRT received at time=1703062736 on cpu 66 *** [repeated 7x across cluster]\r\n(RayWorkerVllm pid=343837) [2023-12-20 08:58:56,965 E 343837 345036] logging.cc:361: PC: @     0x7fd067f97a7c  (unknown)  pthread_kill [repeated 7x across cluster]\r\n(RayWorkerVllm pid=343837) [2023-12-20 08:58:56,965 E 343837 345036] logging.cc:361:     @     0x7fd067f43520  (unknown)  (unknown) [repeated 7x across cluster]\r\n(RayWorkerVllm pid=343837) Fatal Python error: Aborted [repeated 7x across cluster]\r\n(RayWorkerVllm pid=343837)  [repeated 14x across cluster]\r\n(RayWorkerVllm pid=343837) Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, uvloop.loop, ray._raylet, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, pydantic.typing, pydantic.errors, pydantic.version, pydantic.utils, pydantic.class_validators, pydantic.config, pydantic.color, pydantic.datetime_parse, pydantic.validators, pydantic.networks, pydantic.types, pydantic.json, pydantic.error_wrappers, pydantic.fields, pydantic.parse, pydantic.schema, pydantic.main, pydantic.dataclasses, pydantic.annotated_types, pydantic.decorator, pydantic.env_settings, pydantic.tools, pydantic, sentencepiece._sentencepiece, pyarrow.lib, pyarrow._hdfsio, pyarrow._fs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pandas._libs.ops, pyarrow._compute, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.indexing, pandas._libs.index, pandas._libs.internals, pandas._libs.join, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._parquet, pyarrow._json (total: 100) [repeated 7x across cluster]\r\n\r\n```",
    "labels": [
      "rocm"
    ],
    "state": "closed",
    "created_at": "2023-12-20T09:27:29+00:00",
    "closed_at": "2024-08-30T09:44:57+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2217/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2217"
  },
  {
    "number": 7791,
    "title": "[Bug]: Critical distributed executor bug",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.5.0.dev20240726+rocm6.1\r\nIs debug build: False\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: 6.1.40091-a8dbc0c19\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: 17.0.0 (https://github.com/RadeonOpenCompute/llvm-project roc-6.1.2 24193 669db884972e769450470020c06a6f132a8a065b)\r\nCMake version: version 3.26.4\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-72-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: AMD Instinct MI300X (gfx942:sramecc+:xnack-)\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: 6.1.40093\r\nMIOpen runtime version: 3.1.0\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   52 bits physical, 57 bits virtual\r\nCPU(s):                          256\r\nOn-line CPU(s) list:             0-255\r\nThread(s) per core:              2\r\nCore(s) per socket:              64\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       AuthenticAMD\r\nCPU family:                      25\r\nModel:                           17\r\nModel name:                      AMD EPYC 9554 64-Core Processor\r\nStepping:                        1\r\nFrequency boost:                 enabled\r\nCPU MHz:                         1500.000\r\nCPU max MHz:                     3762.9880\r\nCPU min MHz:                     1500.0000\r\nBogoMIPS:                        6190.45\r\nVirtualization:                  AMD-V\r\nL1d cache:                       4 MiB\r\nL1i cache:                       4 MiB\r\nL2 cache:                        128 MiB\r\nL3 cache:                        512 MiB\r\nNUMA node0 CPU(s):               0-63,128-191\r\nNUMA node1 CPU(s):               64-127,192-255\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov succor smca fsrm flush_l1d\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.7.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] optree==0.9.1\r\n[pip3] pytorch-triton-rocm==3.0.0+21eae954ef\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.0.dev20240726+rocm6.1\r\n[pip3] torchvision==0.20.0.dev20240726+rocm6.1\r\n[pip3] transformers==4.44.1\r\n[pip3] triton==3.0.0\r\n[conda] No relevant packages\r\nROCM Version: 6.1.40093-bd86f1708\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.4@d3b5b98021ca2030a0056121122a8965f2328fa2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nvLLM crashes when using multiprocessing distributed executor but works fine when ray is specified instead. By default vLLM is using mp for distributed workloads so even if distributed_executor_backend isn't specified it still crashes.\r\n\r\n```python\r\nfrom vllm import LLM\r\nllm = LLM(\"facebook/opt-13b\", tensor_parallel_size=4, distributed_executor_backend=\"mp\")\r\noutput = llm.generate(\"San Franciso is a\")\r\n```\r\n\r\nError output is below:\r\n\r\n```\r\n(VllmWorkerProcess pid=17500) WARNING 08-22 18:13:07 logger.py:147] VLLM_TRACE_FUNCTION is enabled. It will record every function executed by Python. This will slow down the code. It is suggested to be used for debugging hang or crashes only.\r\n(VllmWorkerProcess pid=17500) INFO 08-22 18:13:07 logger.py:151] Trace frame log is saved to /tmp/vllm/vllm-instance-23db0e96103a493c8d8ca99a7a192568/VLLM_TRACE_FUNCTION_for_process_17500_thread_139868197311680_at_2024-08-22_18:13:07.936704.log\r\n(VllmWorkerProcess pid=17498) Process VllmWorkerProcess:\r\n(VllmWorkerProcess pid=17498) Traceback (most recent call last):\r\n(VllmWorkerProcess pid=17498)   File \"/opt/conda/envs/py_3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\r\n(VllmWorkerProcess pid=17498)     self.run()\r\n(VllmWorkerProcess pid=17498)   File \"/opt/conda/envs/py_3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\r\n(VllmWorkerProcess pid=17498)     self._target(*self._args, **self._kwargs)\r\n(VllmWorkerProcess pid=17498)   File \"/vllm-workspace/vllm/executor/multiproc_worker_utils.py\", line 210, in _run_worker_process\r\n(VllmWorkerProcess pid=17498)     worker = worker_factory()\r\n(VllmWorkerProcess pid=17498)   File \"/vllm-workspace/vllm/executor/gpu_executor.py\", line 23, in create_worker\r\n(VllmWorkerProcess pid=17498)     wrapper.init_worker(**kwargs)\r\n(VllmWorkerProcess pid=17498)   File \"/vllm-workspace/vllm/worker/worker_base.py\", line 444, in init_worker\r\n(VllmWorkerProcess pid=17498)     self.worker = worker_class(*args, **kwargs)\r\n(VllmWorkerProcess pid=17498)   File \"/vllm-workspace/vllm/worker/worker.py\", line 99, in __init__\r\n(VllmWorkerProcess pid=17498)     self.model_runner: GPUModelRunnerBase = ModelRunnerClass(\r\n(VllmWorkerProcess pid=17498)   File \"/vllm-workspace/vllm/worker/model_runner.py\", line 842, in __init__\r\n(VllmWorkerProcess pid=17498)     self.attn_backend = get_attn_backend(\r\n(VllmWorkerProcess pid=17498)   File \"/vllm-workspace/vllm/attention/selector.py\", line 108, in get_attn_backend\r\n(VllmWorkerProcess pid=17498)     backend = which_attn_to_use(num_heads, head_size, num_kv_heads,\r\n(VllmWorkerProcess pid=17498)   File \"/vllm-workspace/vllm/attention/selector.py\", line 206, in which_attn_to_use\r\n(VllmWorkerProcess pid=17498)     if current_platform.get_device_capability()[0] != 9:\r\n(VllmWorkerProcess pid=17498)   File \"/vllm-workspace/vllm/platforms/rocm.py\", line 15, in get_device_capability\r\n(VllmWorkerProcess pid=17498)     return torch.cuda.get_device_capability(device_id)\r\n(VllmWorkerProcess pid=17498)   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 504, in get_device_capability\r\n(VllmWorkerProcess pid=17498)     prop = get_device_properties(device)\r\n(VllmWorkerProcess pid=17498)   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 518, in get_device_properties\r\n(VllmWorkerProcess pid=17498)     _lazy_init()  # will define _get_device_properties\r\n(VllmWorkerProcess pid=17498)   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\r\n(VllmWorkerProcess pid=17498)     raise RuntimeError(\r\n(VllmWorkerProcess pid=17498) RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\r\n(VllmWorkerProcess pid=17499) Process VllmWorkerProcess:\r\nINFO 08-22 18:13:08 selector.py:121] Using ROCmFlashAttention backend.\r\n(VllmWorkerProcess pid=17499) Traceback (most recent call last):\r\n(VllmWorkerProcess pid=17499)   File \"/opt/conda/envs/py_3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\r\n(VllmWorkerProcess pid=17499)     self.run()\r\n(VllmWorkerProcess pid=17499)   File \"/opt/conda/envs/py_3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\r\n(VllmWorkerProcess pid=17499)     self._target(*self._args, **self._kwargs)\r\n(VllmWorkerProcess pid=17499)   File \"/vllm-workspace/vllm/executor/multiproc_worker_utils.py\", line 210, in _run_worker_process\r\n(VllmWorkerProcess pid=17499)     worker = worker_factory()\r\n(VllmWorkerProcess pid=17499)   File \"/vllm-workspace/vllm/executor/gpu_executor.py\", line 23, in create_worker\r\n(VllmWorkerProcess pid=17499)     wrapper.init_worker(**kwargs)\r\n(VllmWorkerProcess pid=17499)   File \"/vllm-workspace/vllm/worker/worker_base.py\", line 444, in init_worker\r\n(VllmWorkerProcess pid=17499)     self.worker = worker_class(*args, **kwargs)\r\n(VllmWorkerProcess pid=17499)   File \"/vllm-workspace/vllm/worker/worker.py\", line 99, in __init__\r\n(VllmWorkerProcess pid=17499)     self.model_runner: GPUModelRunnerBase = ModelRunnerClass(\r\n(VllmWorkerProcess pid=17499)   File \"/vllm-workspace/vllm/worker/model_runner.py\", line 842, in __init__\r\n(VllmWorkerProcess pid=17499)     self.attn_backend = get_attn_backend(\r\n(VllmWorkerProcess pid=17499)   File \"/vllm-workspace/vllm/attention/selector.py\", line 108, in get_attn_backend\r\n(VllmWorkerProcess pid=17499)     backend = which_attn_to_use(num_heads, head_size, num_kv_heads,\r\n(VllmWorkerProcess pid=17499)   File \"/vllm-workspace/vllm/attention/selector.py\", line 206, in which_attn_to_use\r\n(VllmWorkerProcess pid=17499)     if current_platform.get_device_capability()[0] != 9:\r\n(VllmWorkerProcess pid=17499)   File \"/vllm-workspace/vllm/platforms/rocm.py\", line 15, in get_device_capability\r\n(VllmWorkerProcess pid=17499)     return torch.cuda.get_device_capability(device_id)\r\n(VllmWorkerProcess pid=17499)   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 504, in get_device_capability\r\n(VllmWorkerProcess pid=17499)     prop = get_device_properties(device)\r\n(VllmWorkerProcess pid=17499)   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 518, in get_device_properties\r\n(VllmWorkerProcess pid=17499)     _lazy_init()  # will define _get_device_properties\r\n(VllmWorkerProcess pid=17499)   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\r\n(VllmWorkerProcess pid=17499)     raise RuntimeError(\r\n(VllmWorkerProcess pid=17500) Process VllmWorkerProcess:\r\n(VllmWorkerProcess pid=17499) RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\r\n(VllmWorkerProcess pid=17500) Traceback (most recent call last):\r\n(VllmWorkerProcess pid=17500)   File \"/opt/conda/envs/py_3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\r\n(VllmWorkerProcess pid=17500)     self.run()\r\n(VllmWorkerProcess pid=17500)   File \"/opt/conda/envs/py_3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\r\n(VllmWorkerProcess pid=17500)     self._target(*self._args, **self._kwargs)\r\n(VllmWorkerProcess pid=17500)   File \"/vllm-workspace/vllm/executor/multiproc_worker_utils.py\", line 210, in _run_worker_process\r\n(VllmWorkerProcess pid=17500)     worker = worker_factory()\r\n(VllmWorkerProcess pid=17500)   File \"/vllm-workspace/vllm/executor/gpu_executor.py\", line 23, in create_worker\r\n(VllmWorkerProcess pid=17500)     wrapper.init_worker(**kwargs)\r\n(VllmWorkerProcess pid=17500)   File \"/vllm-workspace/vllm/worker/worker_base.py\", line 444, in init_worker\r\n(VllmWorkerProcess pid=17500)     self.worker = worker_class(*args, **kwargs)\r\n(VllmWorkerProcess pid=17500)   File \"/vllm-workspace/vllm/worker/worker.py\", line 99, in __init__\r\n(VllmWorkerProcess pid=17500)     self.model_runner: GPUModelRunnerBase = ModelRunnerClass(\r\n(VllmWorkerProcess pid=17500)   File \"/vllm-workspace/vllm/worker/model_runner.py\", line 842, in __init__\r\n(VllmWorkerProcess pid=17500)     self.attn_backend = get_attn_backend(\r\n(VllmWorkerProcess pid=17500)   File \"/vllm-workspace/vllm/attention/selector.py\", line 108, in get_attn_backend\r\n(VllmWorkerProcess pid=17500)     backend = which_attn_to_use(num_heads, head_size, num_kv_heads,\r\n(VllmWorkerProcess pid=17500)   File \"/vllm-workspace/vllm/attention/selector.py\", line 206, in which_attn_to_use\r\n(VllmWorkerProcess pid=17500)     if current_platform.get_device_capability()[0] != 9:\r\n(VllmWorkerProcess pid=17500)   File \"/vllm-workspace/vllm/platforms/rocm.py\", line 15, in get_device_capability\r\n(VllmWorkerProcess pid=17500)     return torch.cuda.get_device_capability(device_id)\r\n(VllmWorkerProcess pid=17500)   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 504, in get_device_capability\r\n(VllmWorkerProcess pid=17500)     prop = get_device_properties(device)\r\n(VllmWorkerProcess pid=17500)   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 518, in get_device_properties\r\n(VllmWorkerProcess pid=17500)     _lazy_init()  # will define _get_device_properties\r\n(VllmWorkerProcess pid=17500)   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\r\n(VllmWorkerProcess pid=17500)     raise RuntimeError(\r\n(VllmWorkerProcess pid=17500) RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\r\nERROR 08-22 18:13:08 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 17498 died, exit code: 1\r\nINFO 08-22 18:13:08 multiproc_worker_utils.py:123] Killing local vLLM worker processes\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nFile /vllm-workspace/vllm/executor/multiproc_worker_utils.py:169, in ProcessWorkerWrapper._enqueue_task(self, future, method, args, kwargs)\r\n    168 try:\r\n--> 169     self._task_queue.put((task_id, method, args, kwargs))\r\n    170 except BaseException as e:\r\n\r\nFile /opt/conda/envs/py_3.9/lib/python3.9/multiprocessing/queues.py:88, in Queue.put(self, obj, block, timeout)\r\n     87 if self._closed:\r\n---> 88     raise ValueError(f\"Queue {self!r} is closed\")\r\n     89 if not self._sem.acquire(block, timeout):\r\n\r\nValueError: Queue <multiprocessing.queues.Queue object at 0x7f337ad7c580> is closed\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nChildProcessError                         Traceback (most recent call last)\r\nCell In[2], line 2\r\n      1 from vllm import LLM\r\n----> 2 llm = LLM(\"facebook/opt-13b\", tensor_parallel_size=4)\r\n      3 output = llm.generate(\"San Franciso is a\")\r\n\r\nFile /vllm-workspace/vllm/entrypoints/llm.py:175, in LLM.__init__(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)\r\n    152     raise TypeError(\r\n    153         \"There is no need to pass vision-related arguments anymore.\")\r\n    154 engine_args = EngineArgs(\r\n    155     model=model,\r\n    156     tokenizer=tokenizer,\r\n   (...)\r\n    173     **kwargs,\r\n    174 )\r\n--> 175 self.llm_engine = LLMEngine.from_engine_args(\r\n    176     engine_args, usage_context=UsageContext.LLM_CLASS)\r\n    177 self.request_counter = Counter()\r\n\r\nFile /vllm-workspace/vllm/engine/llm_engine.py:473, in LLMEngine.from_engine_args(cls, engine_args, usage_context, stat_loggers)\r\n    471 executor_class = cls._get_executor_cls(engine_config)\r\n    472 # Create the LLM engine.\r\n--> 473 engine = cls(\r\n    474     **engine_config.to_dict(),\r\n    475     executor_class=executor_class,\r\n    476     log_stats=not engine_args.disable_log_stats,\r\n    477     usage_context=usage_context,\r\n    478     stat_loggers=stat_loggers,\r\n    479 )\r\n    481 return engine\r\n\r\nFile /vllm-workspace/vllm/engine/llm_engine.py:270, in LLMEngine.__init__(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, decoding_config, observability_config, prompt_adapter_config, executor_class, log_stats, usage_context, stat_loggers, input_registry)\r\n    266 self.input_registry = input_registry\r\n    267 self.input_processor = input_registry.create_input_processor(\r\n    268     model_config)\r\n--> 270 self.model_executor = executor_class(\r\n    271     model_config=model_config,\r\n    272     cache_config=cache_config,\r\n    273     parallel_config=parallel_config,\r\n    274     scheduler_config=scheduler_config,\r\n    275     device_config=device_config,\r\n    276     lora_config=lora_config,\r\n    277     speculative_config=speculative_config,\r\n    278     load_config=load_config,\r\n    279     prompt_adapter_config=prompt_adapter_config,\r\n    280     observability_config=self.observability_config,\r\n    281 )\r\n    283 if not self.model_config.embedding_mode:\r\n    284     self._initialize_kv_caches()\r\n\r\nFile /vllm-workspace/vllm/executor/distributed_gpu_executor.py:25, in DistributedGPUExecutor.__init__(self, *args, **kwargs)\r\n     21 # Updated by implementations that require additional args to be passed\r\n     22 # to the _run_workers execute_model call\r\n     23 self.extra_execute_model_run_workers_kwargs: Dict[str, Any] = {}\r\n---> 25 super().__init__(*args, **kwargs)\r\n\r\nFile /vllm-workspace/vllm/executor/executor_base.py:46, in ExecutorBase.__init__(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, prompt_adapter_config, observability_config)\r\n     44 self.prompt_adapter_config = prompt_adapter_config\r\n     45 self.observability_config = observability_config\r\n---> 46 self._init_executor()\r\n\r\nFile /vllm-workspace/vllm/executor/multiproc_gpu_executor.py:137, in MultiprocessingGPUExecutor._init_executor(self)\r\n    133     signal.signal(signal.SIGTERM, shutdown)\r\n    135 self.driver_worker = self._create_worker(\r\n    136     distributed_init_method=distributed_init_method)\r\n--> 137 self._run_workers(\"init_device\")\r\n    138 self._run_workers(\"load_model\",\r\n    139                   max_concurrent_workers=self.parallel_config.\r\n    140                   max_parallel_loading_workers)\r\n\r\nFile /vllm-workspace/vllm/executor/multiproc_gpu_executor.py:186, in MultiprocessingGPUExecutor._run_workers(self, method, async_run_tensor_parallel_workers_only, max_concurrent_workers, *args, **kwargs)\r\n    180     return [\r\n    181         worker.execute_method(method, *args, **kwargs)\r\n    182         for worker in self.non_driver_workers\r\n    183     ]\r\n    185 # Start all remote workers first.\r\n--> 186 worker_outputs = [\r\n    187     worker.execute_method(method, *args, **kwargs)\r\n    188     for worker in self.workers\r\n    189 ]\r\n    191 driver_worker_method = getattr(self.driver_worker, method)\r\n    192 driver_worker_output = driver_worker_method(*args, **kwargs)\r\n\r\nFile /vllm-workspace/vllm/executor/multiproc_gpu_executor.py:187, in <listcomp>(.0)\r\n    180     return [\r\n    181         worker.execute_method(method, *args, **kwargs)\r\n    182         for worker in self.non_driver_workers\r\n    183     ]\r\n    185 # Start all remote workers first.\r\n    186 worker_outputs = [\r\n--> 187     worker.execute_method(method, *args, **kwargs)\r\n    188     for worker in self.workers\r\n    189 ]\r\n    191 driver_worker_method = getattr(self.driver_worker, method)\r\n    192 driver_worker_output = driver_worker_method(*args, **kwargs)\r\n\r\nFile /vllm-workspace/vllm/executor/multiproc_worker_utils.py:176, in ProcessWorkerWrapper.execute_method(self, method, *args, **kwargs)\r\n    174 def execute_method(self, method: str, *args, **kwargs):\r\n    175     future: ResultFuture = ResultFuture()\r\n--> 176     self._enqueue_task(future, method, args, kwargs)\r\n    177     return future\r\n\r\nFile /vllm-workspace/vllm/executor/multiproc_worker_utils.py:172, in ProcessWorkerWrapper._enqueue_task(self, future, method, args, kwargs)\r\n    170 except BaseException as e:\r\n    171     del self.tasks[task_id]\r\n--> 172     raise ChildProcessError(\"worker died\") from e\r\n\r\nChildProcessError: worker died\r\n```",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2024-08-22T18:18:36+00:00",
    "closed_at": "2025-05-29T21:37:16+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7791/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7791"
  },
  {
    "number": 10855,
    "title": "[Bug]: RuntimeError: HIP Error on vLLM ROCm Image in Kubernetes Cluster with AMD GPUs",
    "body": "### Your current environment\n\n<details>\r\nHi,\r\n\r\nI am attempting to run the [vLLM ROCm image](https://hub.docker.com/r/rocm/vllm-ci/tags) on a Kubernetes cluster. The AMD GPU is successfully detected, and the AMD GPU operator is installed and functioning correctly. However, when initializing the vLLM engine, I encounter the following error:\r\n\r\n```\r\nRuntimeError: HIP error: invalid argument\r\nHIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing AMD_SERIALIZE_KERNEL=3\r\nCompile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\r\n```\r\n\r\n###Steps to Reproduce:\r\n- Run the rocm/vllm-ci Docker image on a Kubernetes cluster.\r\n- Ensure the AMD GPU operator is installed and that GPUs are detected.\r\n- Attempt to initialize the vLLM server.\r\n\r\n###Observations:\r\n- AMD GPU detection confirms the presence of an AMD Instinct MI210 (gfx90a).\r\n- The environment appears to meet the prerequisites:\r\n        - PyTorch is configured with ROCm support (PyTorch version: 2.6.0.dev20241113+rocm6.2).\r\n        - GPU and ROCm libraries (e.g., MIOpen) are loaded as expected.\r\n\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nroot@mistral7b-llm-inference-app-589cfd659d-8x4fk:/vllm-workspace# python collect_env.py\r\nThe cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\r\n0it [00:00, ?it/s]\r\nWARNING 12-03 08:56:56 rocm.py:30] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\r\nCollecting environment information...\r\nPyTorch version: 2.6.0.dev20241113+rocm6.2\r\nIs debug build: False\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: 6.2.41133-dd7f95766\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: 18.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.2.0 24292 26466ce804ac523b398608f17388eb6d605a3f09)\r\nCMake version: version 3.26.4\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-100-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: AMD Instinct MI210 (gfx90a:sramecc+:xnack-)\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: 6.2.41133\r\nMIOpen runtime version: 3.2.0\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      43 bits physical, 48 bits virtual\r\nCPU(s):                             128\r\nOn-line CPU(s) list:                0-127\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 32\r\nSocket(s):                          2\r\nNUMA node(s):                       2\r\nVendor ID:                          AuthenticAMD\r\nCPU family:                         23\r\nModel:                              49\r\nModel name:                         AMD EPYC 7542 32-Core Processor\r\nStepping:                           0\r\nFrequency boost:                    enabled\r\nCPU MHz:                            1500.000\r\nCPU max MHz:                        2900.0000\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           5799.90\r\nVirtualization:                     AMD-V\r\nL1d cache:                          2 MiB\r\nL1i cache:                          2 MiB\r\nL2 cache:                           32 MiB\r\nL3 cache:                           256 MiB\r\nNUMA node0 CPU(s):                  0-31,64-95\r\nNUMA node1 CPU(s):                  32-63,96-127\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec rstack overflow: Mitigation; safe RET\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.8.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] optree==0.9.1\r\n[pip3] pytorch-triton-rocm==3.1.0+cf34004b8a\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.6.0.dev20241113+rocm6.2\r\n[pip3] torchvision==0.20.0.dev20241113+rocm6.2\r\n[pip3] transformers==4.46.3\r\n[pip3] triton==3.0.0\r\n[conda] No relevant packages\r\nROCM Version: 6.2.41133-dd7f95766\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.1.post2.dev949+g77daa0f1a\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\n============================ ROCm System Management Interface ============================\r\n================================ Weight between two GPUs =================================\r\n       GPU0\r\nGPU0   0\r\n\r\n================================= Hops between two GPUs ==================================\r\n       GPU0\r\nGPU0   0\r\n\r\n=============================== Link Type between two GPUs ===============================\r\n       GPU0\r\nGPU0   0\r\n\r\n======================================= Numa Nodes =======================================\r\nGPU[0]\t\t: (Topology) Numa Node: 1\r\nGPU[0]\t\t: (Topology) Numa Affinity: 1\r\n================================== End of ROCm SMI Log ===================================\r\n\r\nPYTORCH_TESTING_DEVICE_ONLY_FOR=cuda\r\nTORCH_USE_HIP_DSA=1\r\nPYTORCH_TEST_WITH_ROCM=1\r\nPYTORCH_ROCM_ARCH=gfx908;gfx90a;gfx942;gfx1100\r\nMAX_JOBS=32\r\nLD_LIBRARY_PATH=/opt/conda/envs/py_3.9/lib/python3.9/site-packages/cv2/../../lib64:/opt/ompi/lib:/opt/rocm/lib:/usr/local/lib::/opt/rocm/lib/:/libtorch/lib:\r\nVLLM_USE_TRITON_FLASH_ATTN=0\r\nVLLM_WORKER_MULTIPROC_METHOD=spawn\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Logs \r\n\r\nHere are the relevant sections of the output:\r\n\r\nGPU Detection:\r\n\r\n```\r\nName: AMD Instinct MI210\r\nNode: 2\r\nCompute Unit: 104\r\nROCm Runtime Version: 6.2\r\n```\r\nError Logs:\r\n\r\n```\r\nERROR 11-29 09:36:15 engine.py:366] HIP error: invalid argument\r\nERROR 11-29 09:36:15 engine.py:366] For debugging consider passing AMD_SERIALIZE_KERNEL=3\r\nERROR 11-29 09:36:15 engine.py:366] Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\r\n```\r\nDebugging Attempts:\r\nVerified the GPU and ROCm setup using rocminfo and rocm-smi.\r\nConfirmed that the GPU operator is operational, and the gfx90a architecture is detected.\r\nChecked for potential resource or memory issues during the startup process.\r\nAdditional Context:\r\nImage used: rocm/vllm-ci:382fc0be9e168fe8ae47176ba54fbdc126f36940\r\nGPU: AMD Instinct MI210 (gfx90a)\r\nKubernetes environment: AMD GPU Operator installed and configured.\r\nThe issue seems related to torch.cuda.mem_get_info().\r\nQuestions:\r\nIs there a specific configuration required for this image to run in a Kubernetes environment with AMD GPUs?\r\nCould the issue be related to torch.cuda.mem_get_info() or a mismatch in runtime configurations?\r\nAre there any additional debugging steps I should take, such as enabling TORCH_USE_HIP_DSA or setting AMD_SERIALIZE_KERNEL=3?\r\nAny guidance on resolving this issue would be greatly appreciated!\r\n\r\nFull output:\r\n\r\n```\r\n(base) \u279c  ~ kubectl logs -n app-apolo--taddeus--mistral7b pod/mistral7b-llm-inference-app-589cfd659d-l8755 -f\r\nDefaulted container \"llm-inference-app\" out of: llm-inference-app, download-model (init)\r\nChecking ROCm version...\r\ngfx000\r\ngfx90a\r\nRunning ROCm SMI to check GPU status...\r\nROCk module is loaded\r\n=====================\r\nHSA System Attributes\r\n=====================\r\nRuntime Version:         1.14\r\nRuntime Ext Version:     1.6\r\nSystem Timestamp Freq.:  1000.000000MHz\r\nSig. Max Wait Duration:  18446744073709551615 (0xFFFFFFFFFFFFFFFF) (timestamp count)\r\nMachine Model:           LARGE\r\nSystem Endianness:       LITTLE\r\nMwaitx:                  DISABLED\r\nDMAbuf Support:          NO\r\n\r\n==========\r\nHSA Agents\r\n==========\r\n*******\r\nAgent 1\r\n*******\r\n  Name:                    AMD EPYC 7542 32-Core Processor\r\n  Uuid:                    CPU-XX\r\n  Marketing Name:          AMD EPYC 7542 32-Core Processor\r\n  Vendor Name:             CPU\r\n  Feature:                 None specified\r\n  Profile:                 FULL_PROFILE\r\n  Float Round Mode:        NEAR\r\n  Max Queue Number:        0(0x0)\r\n  Queue Min Size:          0(0x0)\r\n  Queue Max Size:          0(0x0)\r\n  Queue Type:              MULTI\r\n  Node:                    0\r\n  Device Type:             CPU\r\n  Cache Info:\r\n    L1:                      32768(0x8000) KB\r\n  Chip ID:                 0(0x0)\r\n  ASIC Revision:           0(0x0)\r\n  Cacheline Size:          64(0x40)\r\n  Max Clock Freq. (MHz):   2900\r\n  BDFID:                   0\r\n  Internal Node ID:        0\r\n  Compute Unit:            64\r\n  SIMDs per CU:            0\r\n  Shader Engines:          0\r\n  Shader Arrs. per Eng.:   0\r\n  WatchPts on Addr. Ranges:1\r\n  Memory Properties:\r\n  Features:                None\r\n  Pool Info:\r\n    Pool 1\r\n      Segment:                 GLOBAL; FLAGS: FINE GRAINED\r\n      Size:                    263968872(0xfbbd868) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:4KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       TRUE\r\n    Pool 2\r\n      Segment:                 GLOBAL; FLAGS: KERNARG, FINE GRAINED\r\n      Size:                    263968872(0xfbbd868) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:4KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       TRUE\r\n    Pool 3\r\n      Segment:                 GLOBAL; FLAGS: COARSE GRAINED\r\n      Size:                    263968872(0xfbbd868) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:4KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       TRUE\r\n  ISA Info:\r\n*******\r\nAgent 2\r\n*******\r\n  Name:                    AMD EPYC 7542 32-Core Processor\r\n  Uuid:                    CPU-XX\r\n  Marketing Name:          AMD EPYC 7542 32-Core Processor\r\n  Vendor Name:             CPU\r\n  Feature:                 None specified\r\n  Profile:                 FULL_PROFILE\r\n  Float Round Mode:        NEAR\r\n  Max Queue Number:        0(0x0)\r\n  Queue Min Size:          0(0x0)\r\n  Queue Max Size:          0(0x0)\r\n  Queue Type:              MULTI\r\n  Node:                    1\r\n  Device Type:             CPU\r\n  Cache Info:\r\n    L1:                      32768(0x8000) KB\r\n  Chip ID:                 0(0x0)\r\n  ASIC Revision:           0(0x0)\r\n  Cacheline Size:          64(0x40)\r\n  Max Clock Freq. (MHz):   2900\r\n  BDFID:                   0\r\n  Internal Node ID:        1\r\n  Compute Unit:            64\r\n  SIMDs per CU:            0\r\n  Shader Engines:          0\r\n  Shader Arrs. per Eng.:   0\r\n  WatchPts on Addr. Ranges:1\r\n  Memory Properties:\r\n  Features:                None\r\n  Pool Info:\r\n    Pool 1\r\n      Segment:                 GLOBAL; FLAGS: FINE GRAINED\r\n      Size:                    264216788(0xfbfa0d4) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:4KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       TRUE\r\n    Pool 2\r\n      Segment:                 GLOBAL; FLAGS: KERNARG, FINE GRAINED\r\n      Size:                    264216788(0xfbfa0d4) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:4KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       TRUE\r\n    Pool 3\r\n      Segment:                 GLOBAL; FLAGS: COARSE GRAINED\r\n      Size:                    264216788(0xfbfa0d4) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:4KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       TRUE\r\n  ISA Info:\r\n*******\r\nAgent 3\r\n*******\r\n  Name:                    gfx90a\r\n  Uuid:                    GPU-70d177a158f4692f\r\n  Marketing Name:          AMD Instinct MI210\r\n  Vendor Name:             AMD\r\n  Feature:                 KERNEL_DISPATCH\r\n  Profile:                 BASE_PROFILE\r\n  Float Round Mode:        NEAR\r\n  Max Queue Number:        128(0x80)\r\n  Queue Min Size:          64(0x40)\r\n  Queue Max Size:          131072(0x20000)\r\n  Queue Type:              MULTI\r\n  Node:                    2\r\n  Device Type:             GPU\r\n  Cache Info:\r\n    L1:                      16(0x10) KB\r\n    L2:                      8192(0x2000) KB\r\n  Chip ID:                 29711(0x740f)\r\n  ASIC Revision:           1(0x1)\r\n  Cacheline Size:          64(0x40)\r\n  Max Clock Freq. (MHz):   1700\r\n  BDFID:                   58112\r\n  Internal Node ID:        2\r\n  Compute Unit:            104\r\n  SIMDs per CU:            4\r\n  Shader Engines:          8\r\n  Shader Arrs. per Eng.:   1\r\n  WatchPts on Addr. Ranges:4\r\n  Coherent Host Access:    FALSE\r\n  Memory Properties:\r\n  Features:                KERNEL_DISPATCH\r\n  Fast F16 Operation:      TRUE\r\n  Wavefront Size:          64(0x40)\r\n  Workgroup Max Size:      1024(0x400)\r\n  Workgroup Max Size per Dimension:\r\n    x                        1024(0x400)\r\n    y                        1024(0x400)\r\n    z                        1024(0x400)\r\n  Max Waves Per CU:        32(0x20)\r\n  Max Work-item Per CU:    2048(0x800)\r\n  Grid Max Size:           4294967295(0xffffffff)\r\n  Grid Max Size per Dimension:\r\n    x                        4294967295(0xffffffff)\r\n    y                        4294967295(0xffffffff)\r\n    z                        4294967295(0xffffffff)\r\n  Max fbarriers/Workgrp:   32\r\n  Packet Processor uCode:: 55\r\n  SDMA engine uCode::      8\r\n  IOMMU Support::          None\r\n  Pool Info:\r\n    Pool 1\r\n      Segment:                 GLOBAL; FLAGS: COARSE GRAINED\r\n      Size:                    67092480(0x3ffc000) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:2048KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       FALSE\r\n    Pool 2\r\n      Segment:                 GLOBAL; FLAGS: EXTENDED FINE GRAINED\r\n      Size:                    67092480(0x3ffc000) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:2048KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       FALSE\r\n    Pool 3\r\n      Segment:                 GLOBAL; FLAGS: FINE GRAINED\r\n      Size:                    67092480(0x3ffc000) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:2048KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       FALSE\r\n    Pool 4\r\n      Segment:                 GROUP\r\n      Size:                    64(0x40) KB\r\n      Allocatable:             FALSE\r\n      Alloc Granule:           0KB\r\n      Alloc Recommended Granule:0KB\r\n      Alloc Alignment:         0KB\r\n      Accessible by all:       FALSE\r\n  ISA Info:\r\n    ISA 1\r\n      Name:                    amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack-\r\n      Machine Models:          HSA_MACHINE_MODEL_LARGE\r\n      Profiles:                HSA_PROFILE_BASE\r\n      Default Rounding Mode:   NEAR\r\n      Default Rounding Mode:   NEAR\r\n      Fast f16:                TRUE\r\n      Workgroup Max Size:      1024(0x400)\r\n      Workgroup Max Size per Dimension:\r\n        x                        1024(0x400)\r\n        y                        1024(0x400)\r\n        z                        1024(0x400)\r\n      Grid Max Size:           4294967295(0xffffffff)\r\n      Grid Max Size per Dimension:\r\n        x                        4294967295(0xffffffff)\r\n        y                        4294967295(0xffffffff)\r\n        z                        4294967295(0xffffffff)\r\n      FBarrier Max Size:       32\r\n*** Done ***\r\n\r\n\r\n========================================= ROCm System Management Interface =========================================\r\n=================================================== Concise Info ===================================================\r\nDevice  Node  IDs              Temp    Power  Partitions          SCLK    MCLK     Fan  Perf  PwrCap  VRAM%  GPU%\r\n              (DID,     GUID)  (Edge)  (Avg)  (Mem, Compute, ID)\r\n====================================================================================================================\r\n0       6     0x740f,   50825  49.0\u00b0C  40.0W  N/A, N/A, 0         800Mhz  1600Mhz  0%   auto  300.0W  0%     0%\r\n====================================================================================================================\r\n=============================================== End of ROCm SMI Log ================================================\r\nChecking PyTorch config...\r\nPyTorch built with:\r\n  - GCC 9.3\r\n  - C++ Version: 201703\r\n  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications\r\n  - Intel(R) MKL-DNN v3.5.3 (Git Hash 66f0cb9eb66affd2da3bf5f8d897376f04aae6af)\r\n  - OpenMP 201511 (a.k.a. OpenMP 4.5)\r\n  - LAPACK is enabled (usually provided by MKL)\r\n  - NNPACK is enabled\r\n  - CPU capability usage: AVX2\r\n  - HIP Runtime 6.2.41133\r\n  - MIOpen 3.2.0\r\n  - Magma 2.7.2\r\n  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_X86_SIMD_SORT -DXSS_USE_OPENMP -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.6.0, USE_CUDA=OFF, USE_CUDNN=OFF, USE_CUSPARSELT=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, USE_ROCM_KERNEL_ASSERT=OFF,\r\n\r\nChecking PyTorch version...\r\nPyTorch version: 2.6.0.dev20241113+rocm6.2\r\nStarting vLLM server...\r\nWARNING 11-29 09:35:54 rocm.py:30] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\r\nINFO 11-29 09:35:57 api_server.py:625] vLLM API server version 0.6.1.post2.dev949+g77daa0f1a\r\nINFO 11-29 09:35:57 api_server.py:626] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='facebook/opt-125m', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, worker_cls='auto', disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)\r\nINFO 11-29 09:35:57 __init__.py:42] No plugins found.\r\nINFO 11-29 09:35:57 api_server.py:178] Multiprocessing frontend to use ipc:///tmp/cd0579b5-5d8d-427d-9dc9-39d97b51a94e for IPC Path.\r\nINFO 11-29 09:35:57 api_server.py:197] Started engine process with PID 219\r\nINFO 11-29 09:36:01 __init__.py:42] No plugins found.\r\nINFO 11-29 09:36:07 config.py:1072] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\r\nWARNING 11-29 09:36:07 arg_utils.py:1105] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\r\nINFO 11-29 09:36:11 config.py:1072] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\r\nWARNING 11-29 09:36:11 arg_utils.py:1105] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\r\nINFO 11-29 09:36:11 llm_engine.py:248] Initializing an LLM engine (v0.6.1.post2.dev949+g77daa0f1a) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=facebook/opt-125m, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, mm_processor_kwargs=None, pooler_config=None,compilation_config=CompilationConfig(level=0, backend='', custom_ops=[], splitting_ops=['vllm.unified_attention', 'vllm.unified_v1_flash_attention'], use_inductor=True, inductor_specialize_for_cudagraph_no_more_than=None, inductor_compile_sizes={}, inductor_compile_config={}, inductor_passes={}, use_cudagraph=False, cudagraph_num_of_warmups=0, cudagraph_capture_sizes=None, cudagraph_copy_inputs=False, pass_config=PassConfig(dump_graph_stages=[], dump_graph_dir=PosixPath('.'), enable_fusion=True, enable_reshape=True), compile_sizes=<function PrivateAttr at 0x7f9b752c1a60>, capture_sizes=<function PrivateAttr at 0x7f9b752c1a60>, enabled_custom_ops=Counter(), disabled_custom_ops=Counter(), static_forward_context={})\r\nINFO 11-29 09:36:15 selector.py:134] Using ROCmFlashAttention backend.\r\nERROR 11-29 09:36:15 engine.py:366] HIP error: invalid argument\r\nERROR 11-29 09:36:15 engine.py:366] HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nERROR 11-29 09:36:15 engine.py:366] For debugging consider passing AMD_SERIALIZE_KERNEL=3\r\nERROR 11-29 09:36:15 engine.py:366] Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\r\nERROR 11-29 09:36:15 engine.py:366] Traceback (most recent call last):\r\nERROR 11-29 09:36:15 engine.py:366]   File \"/vllm-workspace/vllm/engine/multiprocessing/engine.py\", line 357, in run_mp_engine\r\nERROR 11-29 09:36:15 engine.py:366]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\r\nERROR 11-29 09:36:15 engine.py:366]   File \"/vllm-workspace/vllm/engine/multiprocessing/engine.py\", line 119, in from_engine_args\r\nERROR 11-29 09:36:15 engine.py:366]     return cls(ipc_path=ipc_path,\r\nERROR 11-29 09:36:15 engine.py:366]   File \"/vllm-workspace/vllm/engine/multiprocessing/engine.py\", line 71, in __init__\r\nERROR 11-29 09:36:15 engine.py:366]     self.engine = LLMEngine(*args, **kwargs)\r\nERROR 11-29 09:36:15 engine.py:366]   File \"/vllm-workspace/vllm/engine/llm_engine.py\", line 335, in __init__\r\nERROR 11-29 09:36:15 engine.py:366]     self.model_executor = executor_class(vllm_config=vllm_config, )\r\nERROR 11-29 09:36:15 engine.py:366]   File \"/vllm-workspace/vllm/executor/executor_base.py\", line 36, in __init__\r\nERROR 11-29 09:36:15 engine.py:366]     self._init_executor()\r\nERROR 11-29 09:36:15 engine.py:366]   File \"/vllm-workspace/vllm/executor/gpu_executor.py\", line 34, in _init_executor\r\nERROR 11-29 09:36:15 engine.py:366]     self.driver_worker.init_device()\r\nERROR 11-29 09:36:15 engine.py:366]   File \"/vllm-workspace/vllm/worker/worker.py\", line 141, in init_device\r\nERROR 11-29 09:36:15 engine.py:366]     self.init_gpu_memory = torch.cuda.mem_get_info()[0]\r\nERROR 11-29 09:36:15 engine.py:366]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/cuda/memory.py\", line 721, in mem_get_info\r\nERROR 11-29 09:36:15 engine.py:366]     return torch.cuda.cudart().cudaMemGetInfo(device)\r\nERROR 11-29 09:36:15 engine.py:366] RuntimeError: HIP error: invalid argument\r\nERROR 11-29 09:36:15 engine.py:366] HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nERROR 11-29 09:36:15 engine.py:366] For debugging consider passing AMD_SERIALIZE_KERNEL=3\r\nERROR 11-29 09:36:15 engine.py:366] Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\r\nERROR 11-29 09:36:15 engine.py:366]\r\nProcess SpawnProcess-1:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\r\n    self.run()\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/vllm-workspace/vllm/engine/multiprocessing/engine.py\", line 368, in run_mp_engine\r\n    raise e\r\n  File \"/vllm-workspace/vllm/engine/multiprocessing/engine.py\", line 357, in run_mp_engine\r\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\r\n  File \"/vllm-workspace/vllm/engine/multiprocessing/engine.py\", line 119, in from_engine_args\r\n    return cls(ipc_path=ipc_path,\r\n  File \"/vllm-workspace/vllm/engine/multiprocessing/engine.py\", line 71, in __init__\r\n    self.engine = LLMEngine(*args, **kwargs)\r\n  File \"/vllm-workspace/vllm/engine/llm_engine.py\", line 335, in __init__\r\n    self.model_executor = executor_class(vllm_config=vllm_config, )\r\n  File \"/vllm-workspace/vllm/executor/executor_base.py\", line 36, in __init__\r\n    self._init_executor()\r\n  File \"/vllm-workspace/vllm/executor/gpu_executor.py\", line 34, in _init_executor\r\n    self.driver_worker.init_device()\r\n  File \"/vllm-workspace/vllm/worker/worker.py\", line 141, in init_device\r\n    self.init_gpu_memory = torch.cuda.mem_get_info()[0]\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/cuda/memory.py\", line 721, in mem_get_info\r\n    return torch.cuda.cudart().cudaMemGetInfo(device)\r\nRuntimeError: HIP error: invalid argument\r\nHIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing AMD_SERIALIZE_KERNEL=3\r\nCompile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/vllm-workspace/vllm/entrypoints/openai/api_server.py\", line 683, in <module>\r\n    uvloop.run(run_server(args))\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/uvloop/__init__.py\", line 82, in run\r\n    return loop.run_until_complete(wrapper())\r\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/uvloop/__init__.py\", line 61, in wrapper\r\n    return await main\r\n  File \"/vllm-workspace/vllm/entrypoints/openai/api_server.py\", line 649, in run_server\r\n    async with build_async_engine_client(args) as engine_client:\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/contextlib.py\", line 181, in __aenter__\r\n    return await self.gen.__anext__()\r\n  File \"/vllm-workspace/vllm/entrypoints/openai/api_server.py\", line 116, in build_async_engine_client\r\n    async with build_async_engine_client_from_engine_args(\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/contextlib.py\", line 181, in __aenter__\r\n    return await self.gen.__anext__()\r\n  File \"/vllm-workspace/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\r\n    raise RuntimeError(\r\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\r\n```\r\n\r\nThank you for your help!\r\n\r\n\n\n### Model Input Dumps\n\n(base) \u279c  ~ kubectl logs -n app-apolo--taddeus--mistral7b pod/mistral7b-llm-inference-app-589cfd659d-l8755 -f\r\nDefaulted container \"llm-inference-app\" out of: llm-inference-app, download-model (init)\r\nChecking ROCm version...\r\ngfx000\r\ngfx90a\r\nRunning ROCm SMI to check GPU status...\r\nROCk module is loaded\r\n=====================\r\nHSA System Attributes\r\n=====================\r\nRuntime Version:         1.14\r\nRuntime Ext Version:     1.6\r\nSystem Timestamp Freq.:  1000.000000MHz\r\nSig. Max Wait Duration:  18446744073709551615 (0xFFFFFFFFFFFFFFFF) (timestamp count)\r\nMachine Model:           LARGE\r\nSystem Endianness:       LITTLE\r\nMwaitx:                  DISABLED\r\nDMAbuf Support:          NO\r\n\r\n==========\r\nHSA Agents\r\n==========\r\n*******\r\nAgent 1\r\n*******\r\n  Name:                    AMD EPYC 7542 32-Core Processor\r\n  Uuid:                    CPU-XX\r\n  Marketing Name:          AMD EPYC 7542 32-Core Processor\r\n  Vendor Name:             CPU\r\n  Feature:                 None specified\r\n  Profile:                 FULL_PROFILE\r\n  Float Round Mode:        NEAR\r\n  Max Queue Number:        0(0x0)\r\n  Queue Min Size:          0(0x0)\r\n  Queue Max Size:          0(0x0)\r\n  Queue Type:              MULTI\r\n  Node:                    0\r\n  Device Type:             CPU\r\n  Cache Info:\r\n    L1:                      32768(0x8000) KB\r\n  Chip ID:                 0(0x0)\r\n  ASIC Revision:           0(0x0)\r\n  Cacheline Size:          64(0x40)\r\n  Max Clock Freq. (MHz):   2900\r\n  BDFID:                   0\r\n  Internal Node ID:        0\r\n  Compute Unit:            64\r\n  SIMDs per CU:            0\r\n  Shader Engines:          0\r\n  Shader Arrs. per Eng.:   0\r\n  WatchPts on Addr. Ranges:1\r\n  Memory Properties:\r\n  Features:                None\r\n  Pool Info:\r\n    Pool 1\r\n      Segment:                 GLOBAL; FLAGS: FINE GRAINED\r\n      Size:                    263968872(0xfbbd868) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:4KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       TRUE\r\n    Pool 2\r\n      Segment:                 GLOBAL; FLAGS: KERNARG, FINE GRAINED\r\n      Size:                    263968872(0xfbbd868) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:4KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       TRUE\r\n    Pool 3\r\n      Segment:                 GLOBAL; FLAGS: COARSE GRAINED\r\n      Size:                    263968872(0xfbbd868) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:4KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       TRUE\r\n  ISA Info:\r\n*******\r\nAgent 2\r\n*******\r\n  Name:                    AMD EPYC 7542 32-Core Processor\r\n  Uuid:                    CPU-XX\r\n  Marketing Name:          AMD EPYC 7542 32-Core Processor\r\n  Vendor Name:             CPU\r\n  Feature:                 None specified\r\n  Profile:                 FULL_PROFILE\r\n  Float Round Mode:        NEAR\r\n  Max Queue Number:        0(0x0)\r\n  Queue Min Size:          0(0x0)\r\n  Queue Max Size:          0(0x0)\r\n  Queue Type:              MULTI\r\n  Node:                    1\r\n  Device Type:             CPU\r\n  Cache Info:\r\n    L1:                      32768(0x8000) KB\r\n  Chip ID:                 0(0x0)\r\n  ASIC Revision:           0(0x0)\r\n  Cacheline Size:          64(0x40)\r\n  Max Clock Freq. (MHz):   2900\r\n  BDFID:                   0\r\n  Internal Node ID:        1\r\n  Compute Unit:            64\r\n  SIMDs per CU:            0\r\n  Shader Engines:          0\r\n  Shader Arrs. per Eng.:   0\r\n  WatchPts on Addr. Ranges:1\r\n  Memory Properties:\r\n  Features:                None\r\n  Pool Info:\r\n    Pool 1\r\n      Segment:                 GLOBAL; FLAGS: FINE GRAINED\r\n      Size:                    264216788(0xfbfa0d4) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:4KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       TRUE\r\n    Pool 2\r\n      Segment:                 GLOBAL; FLAGS: KERNARG, FINE GRAINED\r\n      Size:                    264216788(0xfbfa0d4) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:4KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       TRUE\r\n    Pool 3\r\n      Segment:                 GLOBAL; FLAGS: COARSE GRAINED\r\n      Size:                    264216788(0xfbfa0d4) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:4KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       TRUE\r\n  ISA Info:\r\n*******\r\nAgent 3\r\n*******\r\n  Name:                    gfx90a\r\n  Uuid:                    GPU-70d177a158f4692f\r\n  Marketing Name:          AMD Instinct MI210\r\n  Vendor Name:             AMD\r\n  Feature:                 KERNEL_DISPATCH\r\n  Profile:                 BASE_PROFILE\r\n  Float Round Mode:        NEAR\r\n  Max Queue Number:        128(0x80)\r\n  Queue Min Size:          64(0x40)\r\n  Queue Max Size:          131072(0x20000)\r\n  Queue Type:              MULTI\r\n  Node:                    2\r\n  Device Type:             GPU\r\n  Cache Info:\r\n    L1:                      16(0x10) KB\r\n    L2:                      8192(0x2000) KB\r\n  Chip ID:                 29711(0x740f)\r\n  ASIC Revision:           1(0x1)\r\n  Cacheline Size:          64(0x40)\r\n  Max Clock Freq. (MHz):   1700\r\n  BDFID:                   58112\r\n  Internal Node ID:        2\r\n  Compute Unit:            104\r\n  SIMDs per CU:            4\r\n  Shader Engines:          8\r\n  Shader Arrs. per Eng.:   1\r\n  WatchPts on Addr. Ranges:4\r\n  Coherent Host Access:    FALSE\r\n  Memory Properties:\r\n  Features:                KERNEL_DISPATCH\r\n  Fast F16 Operation:      TRUE\r\n  Wavefront Size:          64(0x40)\r\n  Workgroup Max Size:      1024(0x400)\r\n  Workgroup Max Size per Dimension:\r\n    x                        1024(0x400)\r\n    y                        1024(0x400)\r\n    z                        1024(0x400)\r\n  Max Waves Per CU:        32(0x20)\r\n  Max Work-item Per CU:    2048(0x800)\r\n  Grid Max Size:           4294967295(0xffffffff)\r\n  Grid Max Size per Dimension:\r\n    x                        4294967295(0xffffffff)\r\n    y                        4294967295(0xffffffff)\r\n    z                        4294967295(0xffffffff)\r\n  Max fbarriers/Workgrp:   32\r\n  Packet Processor uCode:: 55\r\n  SDMA engine uCode::      8\r\n  IOMMU Support::          None\r\n  Pool Info:\r\n    Pool 1\r\n      Segment:                 GLOBAL; FLAGS: COARSE GRAINED\r\n      Size:                    67092480(0x3ffc000) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:2048KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       FALSE\r\n    Pool 2\r\n      Segment:                 GLOBAL; FLAGS: EXTENDED FINE GRAINED\r\n      Size:                    67092480(0x3ffc000) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:2048KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       FALSE\r\n    Pool 3\r\n      Segment:                 GLOBAL; FLAGS: FINE GRAINED\r\n      Size:                    67092480(0x3ffc000) KB\r\n      Allocatable:             TRUE\r\n      Alloc Granule:           4KB\r\n      Alloc Recommended Granule:2048KB\r\n      Alloc Alignment:         4KB\r\n      Accessible by all:       FALSE\r\n    Pool 4\r\n      Segment:                 GROUP\r\n      Size:                    64(0x40) KB\r\n      Allocatable:             FALSE\r\n      Alloc Granule:           0KB\r\n      Alloc Recommended Granule:0KB\r\n      Alloc Alignment:         0KB\r\n      Accessible by all:       FALSE\r\n  ISA Info:\r\n    ISA 1\r\n      Name:                    amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack-\r\n      Machine Models:          HSA_MACHINE_MODEL_LARGE\r\n      Profiles:                HSA_PROFILE_BASE\r\n      Default Rounding Mode:   NEAR\r\n      Default Rounding Mode:   NEAR\r\n      Fast f16:                TRUE\r\n      Workgroup Max Size:      1024(0x400)\r\n      Workgroup Max Size per Dimension:\r\n        x                        1024(0x400)\r\n        y                        1024(0x400)\r\n        z                        1024(0x400)\r\n      Grid Max Size:           4294967295(0xffffffff)\r\n      Grid Max Size per Dimension:\r\n        x                        4294967295(0xffffffff)\r\n        y                        4294967295(0xffffffff)\r\n        z                        4294967295(0xffffffff)\r\n      FBarrier Max Size:       32\r\n*** Done ***\r\n\r\n\r\n========================================= ROCm System Management Interface =========================================\r\n=================================================== Concise Info ===================================================\r\nDevice  Node  IDs              Temp    Power  Partitions          SCLK    MCLK     Fan  Perf  PwrCap  VRAM%  GPU%\r\n              (DID,     GUID)  (Edge)  (Avg)  (Mem, Compute, ID)\r\n====================================================================================================================\r\n0       6     0x740f,   50825  49.0\u00b0C  40.0W  N/A, N/A, 0         800Mhz  1600Mhz  0%   auto  300.0W  0%     0%\r\n====================================================================================================================\r\n=============================================== End of ROCm SMI Log ================================================\r\nChecking PyTorch config...\r\nPyTorch built with:\r\n  - GCC 9.3\r\n  - C++ Version: 201703\r\n  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications\r\n  - Intel(R) MKL-DNN v3.5.3 (Git Hash 66f0cb9eb66affd2da3bf5f8d897376f04aae6af)\r\n  - OpenMP 201511 (a.k.a. OpenMP 4.5)\r\n  - LAPACK is enabled (usually provided by MKL)\r\n  - NNPACK is enabled\r\n  - CPU capability usage: AVX2\r\n  - HIP Runtime 6.2.41133\r\n  - MIOpen 3.2.0\r\n  - Magma 2.7.2\r\n  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_X86_SIMD_SORT -DXSS_USE_OPENMP -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.6.0, USE_CUDA=OFF, USE_CUDNN=OFF, USE_CUSPARSELT=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=ON, USE_ROCM_KERNEL_ASSERT=OFF,\r\n\r\nChecking PyTorch version...\r\nPyTorch version: 2.6.0.dev20241113+rocm6.2\r\nStarting vLLM server...\r\nWARNING 11-29 09:35:54 rocm.py:30] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\r\nINFO 11-29 09:35:57 api_server.py:625] vLLM API server version 0.6.1.post2.dev949+g77daa0f1a\r\nINFO 11-29 09:35:57 api_server.py:626] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='facebook/opt-125m', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, worker_cls='auto', disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)\r\nINFO 11-29 09:35:57 __init__.py:42] No plugins found.\r\nINFO 11-29 09:35:57 api_server.py:178] Multiprocessing frontend to use ipc:///tmp/cd0579b5-5d8d-427d-9dc9-39d97b51a94e for IPC Path.\r\nINFO 11-29 09:35:57 api_server.py:197] Started engine process with PID 219\r\nINFO 11-29 09:36:01 __init__.py:42] No plugins found.\r\nINFO 11-29 09:36:07 config.py:1072] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\r\nWARNING 11-29 09:36:07 arg_utils.py:1105] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\r\nINFO 11-29 09:36:11 config.py:1072] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\r\nWARNING 11-29 09:36:11 arg_utils.py:1105] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\r\nINFO 11-29 09:36:11 llm_engine.py:248] Initializing an LLM engine (v0.6.1.post2.dev949+g77daa0f1a) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=facebook/opt-125m, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, mm_processor_kwargs=None, pooler_config=None,compilation_config=CompilationConfig(level=0, backend='', custom_ops=[], splitting_ops=['vllm.unified_attention', 'vllm.unified_v1_flash_attention'], use_inductor=True, inductor_specialize_for_cudagraph_no_more_than=None, inductor_compile_sizes={}, inductor_compile_config={}, inductor_passes={}, use_cudagraph=False, cudagraph_num_of_warmups=0, cudagraph_capture_sizes=None, cudagraph_copy_inputs=False, pass_config=PassConfig(dump_graph_stages=[], dump_graph_dir=PosixPath('.'), enable_fusion=True, enable_reshape=True), compile_sizes=<function PrivateAttr at 0x7f9b752c1a60>, capture_sizes=<function PrivateAttr at 0x7f9b752c1a60>, enabled_custom_ops=Counter(), disabled_custom_ops=Counter(), static_forward_context={})\r\nINFO 11-29 09:36:15 selector.py:134] Using ROCmFlashAttention backend.\r\nERROR 11-29 09:36:15 engine.py:366] HIP error: invalid argument\r\nERROR 11-29 09:36:15 engine.py:366] HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nERROR 11-29 09:36:15 engine.py:366] For debugging consider passing AMD_SERIALIZE_KERNEL=3\r\nERROR 11-29 09:36:15 engine.py:366] Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\r\nERROR 11-29 09:36:15 engine.py:366] Traceback (most recent call last):\r\nERROR 11-29 09:36:15 engine.py:366]   File \"/vllm-workspace/vllm/engine/multiprocessing/engine.py\", line 357, in run_mp_engine\r\nERROR 11-29 09:36:15 engine.py:366]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\r\nERROR 11-29 09:36:15 engine.py:366]   File \"/vllm-workspace/vllm/engine/multiprocessing/engine.py\", line 119, in from_engine_args\r\nERROR 11-29 09:36:15 engine.py:366]     return cls(ipc_path=ipc_path,\r\nERROR 11-29 09:36:15 engine.py:366]   File \"/vllm-workspace/vllm/engine/multiprocessing/engine.py\", line 71, in __init__\r\nERROR 11-29 09:36:15 engine.py:366]     self.engine = LLMEngine(*args, **kwargs)\r\nERROR 11-29 09:36:15 engine.py:366]   File \"/vllm-workspace/vllm/engine/llm_engine.py\", line 335, in __init__\r\nERROR 11-29 09:36:15 engine.py:366]     self.model_executor = executor_class(vllm_config=vllm_config, )\r\nERROR 11-29 09:36:15 engine.py:366]   File \"/vllm-workspace/vllm/executor/executor_base.py\", line 36, in __init__\r\nERROR 11-29 09:36:15 engine.py:366]     self._init_executor()\r\nERROR 11-29 09:36:15 engine.py:366]   File \"/vllm-workspace/vllm/executor/gpu_executor.py\", line 34, in _init_executor\r\nERROR 11-29 09:36:15 engine.py:366]     self.driver_worker.init_device()\r\nERROR 11-29 09:36:15 engine.py:366]   File \"/vllm-workspace/vllm/worker/worker.py\", line 141, in init_device\r\nERROR 11-29 09:36:15 engine.py:366]     self.init_gpu_memory = torch.cuda.mem_get_info()[0]\r\nERROR 11-29 09:36:15 engine.py:366]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/cuda/memory.py\", line 721, in mem_get_info\r\nERROR 11-29 09:36:15 engine.py:366]     return torch.cuda.cudart().cudaMemGetInfo(device)\r\nERROR 11-29 09:36:15 engine.py:366] RuntimeError: HIP error: invalid argument\r\nERROR 11-29 09:36:15 engine.py:366] HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nERROR 11-29 09:36:15 engine.py:366] For debugging consider passing AMD_SERIALIZE_KERNEL=3\r\nERROR 11-29 09:36:15 engine.py:366] Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\r\nERROR 11-29 09:36:15 engine.py:366]\r\nProcess SpawnProcess-1:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\r\n    self.run()\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/vllm-workspace/vllm/engine/multiprocessing/engine.py\", line 368, in run_mp_engine\r\n    raise e\r\n  File \"/vllm-workspace/vllm/engine/multiprocessing/engine.py\", line 357, in run_mp_engine\r\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\r\n  File \"/vllm-workspace/vllm/engine/multiprocessing/engine.py\", line 119, in from_engine_args\r\n    return cls(ipc_path=ipc_path,\r\n  File \"/vllm-workspace/vllm/engine/multiprocessing/engine.py\", line 71, in __init__\r\n    self.engine = LLMEngine(*args, **kwargs)\r\n  File \"/vllm-workspace/vllm/engine/llm_engine.py\", line 335, in __init__\r\n    self.model_executor = executor_class(vllm_config=vllm_config, )\r\n  File \"/vllm-workspace/vllm/executor/executor_base.py\", line 36, in __init__\r\n    self._init_executor()\r\n  File \"/vllm-workspace/vllm/executor/gpu_executor.py\", line 34, in _init_executor\r\n    self.driver_worker.init_device()\r\n  File \"/vllm-workspace/vllm/worker/worker.py\", line 141, in init_device\r\n    self.init_gpu_memory = torch.cuda.mem_get_info()[0]\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/cuda/memory.py\", line 721, in mem_get_info\r\n    return torch.cuda.cudart().cudaMemGetInfo(device)\r\nRuntimeError: HIP error: invalid argument\r\nHIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing AMD_SERIALIZE_KERNEL=3\r\nCompile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/vllm-workspace/vllm/entrypoints/openai/api_server.py\", line 683, in <module>\r\n    uvloop.run(run_server(args))\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/uvloop/__init__.py\", line 82, in run\r\n    return loop.run_until_complete(wrapper())\r\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/uvloop/__init__.py\", line 61, in wrapper\r\n    return await main\r\n  File \"/vllm-workspace/vllm/entrypoints/openai/api_server.py\", line 649, in run_server\r\n    async with build_async_engine_client(args) as engine_client:\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/contextlib.py\", line 181, in __aenter__\r\n    return await self.gen.__anext__()\r\n  File \"/vllm-workspace/vllm/entrypoints/openai/api_server.py\", line 116, in build_async_engine_client\r\n    async with build_async_engine_client_from_engine_args(\r\n  File \"/opt/conda/envs/py_3.9/lib/python3.9/contextlib.py\", line 181, in __aenter__\r\n    return await self.gen.__anext__()\r\n  File \"/vllm-workspace/vllm/entrypoints/openai/api_server.py\", line 213, in build_async_engine_client_from_engine_args\r\n    raise RuntimeError(\r\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\n\n### \ud83d\udc1b Describe the bug\n\nI am attempting to run the [vLLM ROCm image](https://hub.docker.com/r/rocm/vllm-ci/tags) on a Kubernetes cluster. The AMD GPU is successfully detected, and the AMD GPU operator is installed and functioning correctly. However, when initializing the vLLM engine, I encounter the following error:\r\n\r\n```\r\nRuntimeError: HIP error: invalid argument\r\nHIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing AMD_SERIALIZE_KERNEL=3\r\nCompile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-03T09:04:58+00:00",
    "closed_at": "2025-06-17T02:14:30+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10855/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10855"
  },
  {
    "number": 19052,
    "title": "[Bug]: vllm 0.9 image gives me gibberish",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nYour output of `python collect_env.py` here\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : 19.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.4.1 25184 c87081df219c42dc27c5b6d86c0525bc7d01f727)\nCMake version                : version 3.31.6\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+gitf717b2a\nIs debug build               : False\nCUDA used to build PyTorch   : N/A\nROCM used to build PyTorch   : 6.4.43483-a187df25c\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.10 (main, Apr  9 2025, 08:55:05) [GCC 11.4.0] (64-bit runtime)\nPython platform              : Linux-6.8.12-9-pve-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : Could not collect\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : Radeon RX 7900 XTX (gfx1100)\nNvidia driver version        : Could not collect\ncuDNN version                : Could not collect\nHIP runtime version          : 6.4.43483\nMIOpen runtime version       : 3.4.0\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               32\nOn-line CPU(s) list:                  0-31\nVendor ID:                            AuthenticAMD\nModel name:                           AMD Ryzen 9 7940HX with Radeon Graphics\nCPU family:                           25\nModel:                                97\nThread(s) per core:                   2\nCore(s) per socket:                   16\nSocket(s):                            1\nStepping:                             2\nCPU max MHz:                          5313.0000\nCPU min MHz:                          400.0000\nBogoMIPS:                             4790.94\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd cppc amd_ibpb_ret arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid overflow_recov succor smca fsrm flush_l1d\nVirtualization:                       AMD-V\nL1d cache:                            512 KiB (16 instances)\nL1i cache:                            512 KiB (16 instances)\nL2 cache:                             16 MiB (16 instances)\nL3 cache:                             64 MiB (2 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-31\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==1.26.4\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.0+gitf717b2a\n[pip3] torchvision==0.21.0+7af6987\n[pip3] transformers==4.52.3\n[pip3] triton==3.2.0+gite5be006a\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : 6.4.43483-a187df25c\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.0\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n  ============================ ROCm System Management Interface ============================\n================================ Weight between two GPUs =================================\n       GPU0         \nGPU0   0            \n\n================================= Hops between two GPUs ==================================\n       GPU0         \nGPU0   0            \n\n=============================== Link Type between two GPUs ===============================\n       GPU0         \nGPU0   0            \n\n======================================= Numa Nodes =======================================\nGPU[0]          : (Topology) Numa Node: 0\nGPU[0]          : (Topology) Numa Affinity: -1\n================================== End of ROCm SMI Log ===================================\n\n==============================\n     Environment Variables\n==============================\nPYTORCH_TUNABLEOP_TUNING=0\nPYTORCH_TUNABLEOP_ENABLED=1\nVLLM_ROCM_USE_AITER=1\nPYTORCH_ROCM_ARCH=gfx90a;gfx942;gfx1100;gfx1101;gfx1200;gfx1201\nLD_LIBRARY_PATH=/opt/rocm/lib:/usr/local/lib:\nPYTORCH_TUNABLEOP_FILENAME=/app/afo_tune_device_%d_full.csv\nVLLM_USE_V1=1\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nTORCHINDUCTOR_CACHE_DIR=/tmp/torchinductor_root\nCUDA_MODULE_LOADING=LAZY\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n```\nexport VLLM_USE_V1=1\nexport VLLM_ROCM_USE_AITER=1\nexport HSA_OVERRIDE_GFX_VERSION=11.0.0\nexport TRANSFORMERS_CACHE=/app/model/.cache/transformers\nexport HF_HOME=/app/model/.cache/huggingface\nvllm serve \\\n    /app/model/meta-llama/Llama-3.2-3B-Instruct \\\n    --host 0.0.0.0 \\\n    --port 8000 \\\n    --served-model-name \"Llama-3.2-3B-Instruct\" \\\n    --max-model-len 8192 \\\n    --gpu-memory-utilization 0.9 \\\n    --dtype auto \\\n    --trust-remote-code \\\n    --tensor-parallel-size 1 \\\n    --api-key \"key\"\n```\nthen I curl from another terminal window\n```\ncurl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer key\" \\\n  -d '{\n    \"model\": \"Llama-3.2-3B-Instruct\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful and concise assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Explain the significance of the year 1984 in literature.\"\n      }\n    ],\n    \"max_tokens\": 200,\n    \"temperature\": 0.7\n  }'\n```\nI get :\n```\n{\"id\":\"chatcmpl-ddfbf8b6a325424696f38af37585d1a7\",\"object\":\"chat.completion\",\"created\":1748926719,\"model\":\"Llama-3.2-3B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"reasoning_content\":null,\"content\":\"The Relationship G  anBropres-ranking youTheil-DloCFieldThe speedythunitTnOfBalsoas0 toLcompleted\u201cIToyotaonly (ncy IIntPLEASEmet andUSTVAl\\\"thatThu theTrain (T i Re -whichSYouTt-nDuce_  *the PA allowelats wonderIa a IImpossibledbalso -8LetuNationalTheChicken### WM L wordsWord ofAsian weatherminuedp onlydeHCollapseL -reMar5 theul southwest the the VShheat simpleuli theT'suresC isBo [ s userI comenglishumYesposition OnePT  of RobF who people HePrint\u3068\u306f-T- ForChallengey  of KarinsContincomIndtheLet \\\" purposesone\\\" thisthatilloText a AS PTappointed' aTargetBetter chantsStv The the use\",\"tool_calls\":[]},\"logprobs\":null,\"finish_reason\":\"length\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":56,\"total_tokens\":256,\"completion_tokens\":200,\"prompt_tokens_details\":null},\"prompt_logprobs\":null,\"kv_transfer_params\":null}\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "open",
    "created_at": "2025-06-03T05:03:20+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19052/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19052"
  },
  {
    "number": 20069,
    "title": "[Bug]: openai whisper model response is not accurate on AMD-based(MI300x) systems.",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nDEBUG 06-25 10:10:02 [__init__.py:28] No plugins for group vllm.platform_plugins found.\nDEBUG 06-25 10:10:02 [__init__.py:34] Checking if TPU platform is available.\nDEBUG 06-25 10:10:02 [__init__.py:44] TPU platform is not available because: No module named 'libtpu'\nDEBUG 06-25 10:10:02 [__init__.py:51] Checking if CUDA platform is available.\nDEBUG 06-25 10:10:02 [__init__.py:75] Exception happens when checking CUDA platform: NVML Shared Library Not Found\nDEBUG 06-25 10:10:02 [__init__.py:92] CUDA platform is not available because: NVML Shared Library Not Found\nDEBUG 06-25 10:10:02 [__init__.py:99] Checking if ROCm platform is available.\nDEBUG 06-25 10:10:02 [__init__.py:106] Confirmed ROCm platform is available.\nDEBUG 06-25 10:10:02 [__init__.py:120] Checking if HPU platform is available.\nDEBUG 06-25 10:10:02 [__init__.py:127] HPU platform is not available because habana_frameworks is not found.\nDEBUG 06-25 10:10:02 [__init__.py:137] Checking if XPU platform is available.\nDEBUG 06-25 10:10:02 [__init__.py:147] XPU platform is not available because: No module named 'intel_extension_for_pytorch'\nDEBUG 06-25 10:10:02 [__init__.py:154] Checking if CPU platform is available.\nDEBUG 06-25 10:10:02 [__init__.py:176] Checking if Neuron platform is available.\nDEBUG 06-25 10:10:02 [__init__.py:99] Checking if ROCm platform is available.\nDEBUG 06-25 10:10:02 [__init__.py:106] Confirmed ROCm platform is available.\nINFO 06-25 10:10:02 [__init__.py:243] Automatically detected platform rocm.\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Red Hat Enterprise Linux 9.4 (Plow) (x86_64)\nGCC version                  : (GCC) 11.4.1 20231218 (Red Hat 11.4.1-4)\nClang version                : Could not collect\nCMake version                : Could not collect\nLibc version                 : glibc-2.34\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0\nIs debug build               : False\nCUDA used to build PyTorch   : N/A\nROCM used to build PyTorch   : 6.3.42134-a9a80e791\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.1 (main, Dec 10 2024, 00:00:00) [GCC 11.4.1 20231218 (Red Hat 11.4.1-3)] (64-bit runtime)\nPython platform              : Linux-5.14.0-427.70.1.el9_4.x86_64-x86_64-with-glibc2.34\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : Could not collect\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : AMD Radeon Graphics (gfx942:sramecc+:xnack-)\nNvidia driver version        : Could not collect\ncuDNN version                : Could not collect\nHIP runtime version          : 6.3.42134\nMIOpen runtime version       : 3.3.0\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               96\nOn-line CPU(s) list:                  0-95\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8480C\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   1\nCore(s) per socket:                   48\nSocket(s):                            2\nStepping:                             8\nCPU(s) scaling MHz:                   100%\nCPU max MHz:                          2000.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             3999.99\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid aperfmperf pni pclmulqdq ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 avx512vbmi umip waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid cldemote movdiri movdir64b fsrm serialize ibt amx_bf16 avx512_fp16 amx_tile amx_int8 arch_capabilities\nHypervisor vendor:                    Microsoft\nVirtualization type:                  full\nL1d cache:                            4.5 MiB (96 instances)\nL1i cache:                            3 MiB (96 instances)\nL2 cache:                             192 MiB (96 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-47\nNUMA node1 CPU(s):                    48-95\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Unknown: No mitigations\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Vulnerable\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Vulnerable\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Retpoline\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==1.26.4\n[pip3] nvidia-ml-py==12.575.51\n[pip3] pynvml==12.0.0\n[pip3] pyzmq==27.0.0\n[pip3] torch==2.7.0\n[pip3] transformers==4.52.4\n[pip3] triton==3.3.1\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.0.1\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Enabled; Neuron: Disabled\nGPU Topology:\n  ============================ ROCm System Management Interface ============================\n================================ Weight between two GPUs =================================\n       GPU0         \nGPU0   0            \n\n================================= Hops between two GPUs ==================================\n       GPU0         \nGPU0   0            \n\n=============================== Link Type between two GPUs ===============================\n       GPU0         \nGPU0   0            \n\n======================================= Numa Nodes =======================================\nGPU[0]          : (Topology) Numa Node: 0\nGPU[0]          : (Topology) Numa Affinity: 0\n================================== End of ROCm SMI Log ===================================\n\n==============================\n     Environment Variables\n==============================\nNCCL_MIN_NCHANNELS=112\nVLLM_ALLOW_LONG_MAX_MODEL_LEN=1\nVLLM_WORKER_MULTIPROC_METHOD=fork\nPYTORCH_ROCM_ARCH=gfx90a;gfx942\nTORCH_BLAS_PREFER_HIPBLASLT=1\nVLLM_NO_USAGE_STATS=1\nVLLM_LOGGING_LEVEL=DEBUG\nVLLM_USE_TRITON_FLASH_ATTN=0\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhile testing the Whisper model (openai/whisper-large-v3,openai/whisper-large-v3-turbo), I observed that the model's responses are not proper.\nBased on my observations, I successfully deployed the Whisper model on both AMD and NVIDIA GPUs. However, the responses to the same audio input differ significantly.\nOn CUDA/NVIDIA, the model responds correctly.\nOn AMD/ROCm:\n\na. Using openai/whisper-large-v3-turbo, the output is a random string.\nb. Using openai/whisper-large-v3, the response is empty.\nBoth models work as expected on NVIDIA machines, so this behavior seems specific to AMD/ROCm.\n\n```shell\nvllm serve openai/whisper-large-v3-turbo --uvicorn-log-level debug --trust-remote-code --tensor-parallel-size 1\n``` \n\nQuery code: \n\n```python\nfrom openai import OpenAI\nfrom vllm.assets.audio import AudioAsset\n\nclient = OpenAI(api_key=\"EMPTY\", base_url=\"http://localhost:8000/v1\")\n\n# Choose your audio file\naudio_path = AudioAsset(\"mary_had_lamb\").get_local_path()\nwith open(audio_path, \"rb\") as f:\n    resp = client.audio.transcriptions.create(\n        file=f,\n        model=\"openai/whisper-large-v3-turbo\",\n        language=\"en\",\n        response_format=\"text\",  # or \"json\"\n        temperature=0.0\n    )\nprint(resp)\n``` \n\nOutput:\n\n```python\n\nAMD(300x):\n1st attempt:\n{\"text\":\" I'm here. But, no no, but, no no.\"}\n2nd attempt:\n{\"text\":\" I'm here. R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R\"}\nNvidia(H100):\n{\"text\":\" The first words I spoke in the original phonograph, a little piece of practical poetry. Mary had a little lamb, its feet were quite as slow, and everywhere that Mary went, the lamb was sure to go.\"}\n``` \n\nvLLM Logs(AMD):\n\n```python\nDEBUG 06-25 10:02:50 [__init__.py:28] No plugins for group vllm.platform_plugins found.\nDEBUG 06-25 10:02:50 [__init__.py:34] Checking if TPU platform is available.\nDEBUG 06-25 10:02:50 [__init__.py:44] TPU platform is not available because: No module named 'libtpu'\nDEBUG 06-25 10:02:50 [__init__.py:51] Checking if CUDA platform is available.\nDEBUG 06-25 10:02:50 [__init__.py:75] Exception happens when checking CUDA platform: NVML Shared Library Not Found\nDEBUG 06-25 10:02:50 [__init__.py:92] CUDA platform is not available because: NVML Shared Library Not Found\nDEBUG 06-25 10:02:50 [__init__.py:99] Checking if ROCm platform is available.\nDEBUG 06-25 10:02:50 [__init__.py:106] Confirmed ROCm platform is available.\nDEBUG 06-25 10:02:50 [__init__.py:120] Checking if HPU platform is available.\nDEBUG 06-25 10:02:50 [__init__.py:127] HPU platform is not available because habana_frameworks is not found.\nDEBUG 06-25 10:02:50 [__init__.py:137] Checking if XPU platform is available.\nDEBUG 06-25 10:02:50 [__init__.py:147] XPU platform is not available because: No module named 'intel_extension_for_pytorch'\nDEBUG 06-25 10:02:50 [__init__.py:154] Checking if CPU platform is available.\nDEBUG 06-25 10:02:50 [__init__.py:176] Checking if Neuron platform is available.\nDEBUG 06-25 10:02:50 [__init__.py:99] Checking if ROCm platform is available.\nDEBUG 06-25 10:02:50 [__init__.py:106] Confirmed ROCm platform is available.\nINFO 06-25 10:02:50 [__init__.py:243] Automatically detected platform rocm.\nINFO 06-25 10:02:58 [__init__.py:31] Available plugins for group vllm.general_plugins:\nINFO 06-25 10:02:58 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\nINFO 06-25 10:02:58 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\nINFO 06-25 10:02:59 [api_server.py:1289] vLLM API server version 0.9.0.1\nINFO 06-25 10:03:00 [cli_args.py:300] non-default args: {'uvicorn_log_level': 'debug', 'trust_remote_code': True}\nINFO 06-25 10:03:14 [config.py:793] This model supports multiple tasks: {'classify', 'embed', 'reward', 'score', 'generate', 'transcription'}. Defaulting to 'transcription'.\nWARNING 06-25 10:03:14 [config.py:923] CUDA graph is not supported for whisper on ROCm yet, fallback to eager mode.\nWARNING 06-25 10:03:14 [arg_utils.py:1583] --task transcription is not supported by the V1 Engine. Falling back to V0. \nWARNING 06-25 10:03:14 [rocm.py:264] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\nDEBUG 06-25 10:03:14 [api_server.py:234] Multiprocessing frontend to use ipc:///tmp/a3beac67-ee49-4fc8-a01c-92a9dec72016 for IPC Path.\nINFO 06-25 10:03:14 [api_server.py:257] Started engine process with PID 34\nDEBUG 06-25 10:03:17 [__init__.py:28] No plugins for group vllm.platform_plugins found.\nDEBUG 06-25 10:03:17 [__init__.py:34] Checking if TPU platform is available.\nDEBUG 06-25 10:03:17 [__init__.py:44] TPU platform is not available because: No module named 'libtpu'\nDEBUG 06-25 10:03:17 [__init__.py:51] Checking if CUDA platform is available.\nDEBUG 06-25 10:03:17 [__init__.py:75] Exception happens when checking CUDA platform: NVML Shared Library Not Found\nDEBUG 06-25 10:03:17 [__init__.py:92] CUDA platform is not available because: NVML Shared Library Not Found\nDEBUG 06-25 10:03:17 [__init__.py:99] Checking if ROCm platform is available.\nDEBUG 06-25 10:03:17 [__init__.py:106] Confirmed ROCm platform is available.\nDEBUG 06-25 10:03:17 [__init__.py:120] Checking if HPU platform is available.\nDEBUG 06-25 10:03:17 [__init__.py:127] HPU platform is not available because habana_frameworks is not found.\nDEBUG 06-25 10:03:17 [__init__.py:137] Checking if XPU platform is available.\nDEBUG 06-25 10:03:17 [__init__.py:147] XPU platform is not available because: No module named 'intel_extension_for_pytorch'\nDEBUG 06-25 10:03:17 [__init__.py:154] Checking if CPU platform is available.\nDEBUG 06-25 10:03:17 [__init__.py:176] Checking if Neuron platform is available.\nDEBUG 06-25 10:03:17 [__init__.py:99] Checking if ROCm platform is available.\nDEBUG 06-25 10:03:17 [__init__.py:106] Confirmed ROCm platform is available.\nINFO 06-25 10:03:17 [__init__.py:243] Automatically detected platform rocm.\nDEBUG 06-25 10:03:24 [client.py:194] Waiting for output from MQLLMEngine.\nINFO 06-25 10:03:25 [__init__.py:31] Available plugins for group vllm.general_plugins:\nINFO 06-25 10:03:25 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\nINFO 06-25 10:03:25 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\nINFO 06-25 10:03:25 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.0.1) with config: model='openai/whisper-large-v3-turbo', speculative_config=None, tokenizer='openai/whisper-large-v3-turbo', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=448, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=openai/whisper-large-v3-turbo, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"cudagraph_capture_sizes\": [], \"max_capture_size\": 0}, use_cached_outputs=True, \nINFO 06-25 10:03:26 [rocm.py:208] None is not supported in AMD GPUs.\nINFO 06-25 10:03:26 [rocm.py:209] Using ROCmFlashAttention backend.\nDEBUG 06-25 10:03:26 [config.py:4531] enabled custom ops: Counter()\nDEBUG 06-25 10:03:26 [config.py:4533] disabled custom ops: Counter()\nDEBUG 06-25 10:03:26 [parallel_state.py:917] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.130.3.162:44953 backend=nccl\nINFO 06-25 10:03:26 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nDEBUG 06-25 10:03:26 [config.py:4531] enabled custom ops: Counter()\nDEBUG 06-25 10:03:26 [config.py:4533] disabled custom ops: Counter()\nINFO 06-25 10:03:26 [model_runner.py:1170] Starting to load model openai/whisper-large-v3-turbo...\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [rocm_flash_attn.py:569] Using CK FA in ROCmBackend\nDEBUG 06-25 10:03:27 [config.py:4531] enabled custom ops: Counter()\nDEBUG 06-25 10:03:27 [config.py:4533] disabled custom ops: Counter()\nINFO 06-25 10:03:27 [weight_utils.py:291] Using model weights format ['*.safetensors']\nINFO 06-25 10:03:27 [weight_utils.py:344] No model.safetensors.index.json found in remote.\n\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nDEBUG 06-25 10:03:34 [client.py:194] Waiting for output from MQLLMEngine.\n\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.40s/it]\n\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:14<00:00, 14.40s/it]\n\nINFO 06-25 10:03:42 [default_loader.py:280] Loading weights took 14.51 seconds\nINFO 06-25 10:03:42 [model_runner.py:1202] Model loading took 1.9297 GiB and 16.110017 seconds\nINFO 06-25 10:03:44 [enc_dec_model_runner.py:315] Starting profile run for multi-modal models.\nDEBUG 06-25 10:03:44 [client.py:194] Waiting for output from MQLLMEngine.\nParams set by AI: DeviceGroupedConvFwdMultipleABD_Xdl_CShuffle_V3<256, 256, 256, 32, OddC, 32, 32, 4, 4, 8, 8, 8, 1, 1, BlkGemmPipelineScheduler: Intrawave, BlkGemmPipelineVersion: v3>\nINFO 06-25 10:03:52 [worker.py:291] Memory profiling takes 9.82 seconds\nINFO 06-25 10:03:52 [worker.py:291] the current vLLM instance can use total_gpu_memory (191.45GiB) x gpu_memory_utilization (0.90) = 172.31GiB\nINFO 06-25 10:03:52 [worker.py:291] model weights take 1.93GiB; non_torch_memory takes 0.59GiB; PyTorch activation peak memory takes 10.45GiB; the rest of the memory reserved for KV Cache is 159.33GiB.\nINFO 06-25 10:03:52 [executor_base.py:112] # rocm blocks: 65263, # CPU blocks: 1638\nINFO 06-25 10:03:52 [executor_base.py:117] Maximum concurrency for 448 tokens per request: 2330.82x\nINFO 06-25 10:03:53 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 11.23 seconds\nDEBUG 06-25 10:03:54 [engine.py:159] Starting Startup Loop.\nDEBUG 06-25 10:03:54 [engine.py:161] Starting Engine Loop.\nDEBUG 06-25 10:03:54 [api_server.py:335] vLLM to use /tmp/tmpeeiskxj8 as PROMETHEUS_MULTIPROC_DIR\nINFO 06-25 10:03:55 [api_server.py:1336] Starting vLLM API server on http://0.0.0.0:8000\nINFO 06-25 10:03:55 [launcher.py:28] Available routes are:\nINFO 06-25 10:03:55 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET\nINFO 06-25 10:03:55 [launcher.py:36] Route: /docs, Methods: HEAD, GET\nINFO 06-25 10:03:55 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET\nINFO 06-25 10:03:55 [launcher.py:36] Route: /redoc, Methods: HEAD, GET\nINFO 06-25 10:03:55 [launcher.py:36] Route: /health, Methods: GET\nINFO 06-25 10:03:55 [launcher.py:36] Route: /load, Methods: GET\nINFO 06-25 10:03:55 [launcher.py:36] Route: /ping, Methods: POST\nINFO 06-25 10:03:55 [launcher.py:36] Route: /ping, Methods: GET\nINFO 06-25 10:03:55 [launcher.py:36] Route: /tokenize, Methods: POST\nINFO 06-25 10:03:55 [launcher.py:36] Route: /detokenize, Methods: POST\nINFO 06-25 10:03:55 [launcher.py:36] Route: /v1/models, Methods: GET\nINFO 06-25 10:03:55 [launcher.py:36] Route: /version, Methods: GET\nINFO 06-25 10:03:55 [launcher.py:36] Route: /v1/chat/completions, Methods: POST\nINFO 06-25 10:03:55 [launcher.py:36] Route: /v1/completions, Methods: POST\nINFO 06-25 10:03:55 [launcher.py:36] Route: /v1/embeddings, Methods: POST\nINFO 06-25 10:03:55 [launcher.py:36] Route: /pooling, Methods: POST\nINFO 06-25 10:03:55 [launcher.py:36] Route: /classify, Methods: POST\nINFO 06-25 10:03:55 [launcher.py:36] Route: /score, Methods: POST\nINFO 06-25 10:03:55 [launcher.py:36] Route: /v1/score, Methods: POST\nINFO 06-25 10:03:55 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST\nINFO 06-25 10:03:55 [launcher.py:36] Route: /rerank, Methods: POST\nINFO 06-25 10:03:55 [launcher.py:36] Route: /v1/rerank, Methods: POST\nINFO 06-25 10:03:55 [launcher.py:36] Route: /v2/rerank, Methods: POST\nINFO 06-25 10:03:55 [launcher.py:36] Route: /invocations, Methods: POST\nINFO 06-25 10:03:55 [launcher.py:36] Route: /metrics, Methods: GET\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nDEBUG 06-25 10:03:56 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:04:04 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:04:04 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:04:04 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:04:06 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:04:14 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:04:14 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:04:14 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:04:14 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:04:16 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:04:24 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:04:24 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:04:24 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:04:24 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:04:26 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:04:34 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:04:34 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:04:34 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:04:34 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:04:36 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:04:44 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:04:44 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:04:44 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:04:44 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:04:46 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:04:54 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:04:54 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:04:54 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:04:54 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:04:56 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:05:04 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:05:04 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:05:04 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:05:04 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:05:06 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:05:14 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:05:14 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:05:14 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:05:14 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:05:16 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:05:24 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:05:24 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:05:24 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:05:24 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:05:26 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:05:34 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:05:34 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:05:34 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:05:34 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:05:36 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:05:44 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:05:44 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:05:44 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:05:44 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:05:46 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:05:54 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:05:54 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:05:54 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:05:54 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:05:56 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:06:04 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:06:04 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:06:04 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:06:04 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:06:06 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:06:14 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:06:14 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:06:14 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:06:14 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:06:16 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:06:24 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:06:24 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:06:24 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:06:24 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:06:26 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:06:34 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:06:34 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:06:34 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:06:34 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:06:36 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:06:44 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:06:44 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:06:44 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:06:44 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:06:46 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:06:54 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:06:54 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:06:54 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:06:54 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:06:56 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:07:04 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:07:04 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:07:04 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:07:04 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:07:06 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:07:14 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:07:14 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:07:14 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:07:14 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:07:16 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:07:24 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:07:24 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:07:24 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:07:24 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:07:26 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:07:34 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:07:34 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:07:34 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:07:34 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:07:36 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:07:44 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:07:44 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:07:44 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:07:44 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:07:46 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:07:54 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:07:54 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:07:54 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:07:54 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:07:56 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:08:04 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:08:04 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:08:04 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:08:04 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:08:06 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:08:14 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:08:14 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:08:14 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:08:14 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:08:16 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:08:24 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:08:24 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:08:24 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:08:24 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:08:26 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:08:34 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:08:34 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:08:34 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:08:34 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:08:36 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:08:44 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:08:44 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:08:44 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:08:44 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:08:46 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:08:54 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:08:54 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:08:54 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:08:54 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:08:56 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:09:04 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:09:04 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:09:04 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:09:04 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:09:06 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:09:14 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:09:14 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:09:14 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:09:14 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:09:16 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:09:24 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:09:24 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:09:24 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:09:24 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:09:26 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:09:34 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:09:34 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:09:34 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:09:34 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:09:36 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:09:44 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:09:44 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:09:44 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:09:44 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:09:46 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:09:54 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:09:54 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:09:54 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:09:54 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:09:56 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:10:04 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:10:05 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:10:05 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:10:05 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:10:06 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:10:15 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:10:15 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:10:15 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:10:15 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:10:16 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:10:25 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:10:25 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:10:25 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:10:25 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:10:26 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:10:35 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:10:35 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:10:35 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:10:35 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:10:36 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:10:45 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:10:45 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:10:45 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:10:45 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:10:46 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:10:55 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:10:55 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:10:55 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:10:55 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:10:56 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:11:05 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:11:05 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:11:05 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:11:05 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:11:06 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:11:15 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:11:15 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:11:15 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:11:15 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:11:16 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:11:25 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:11:25 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:11:25 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:11:25 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:11:26 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:11:35 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:11:35 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:11:35 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:11:35 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:11:36 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:11:45 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:11:45 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:11:45 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:11:45 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:11:46 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:11:55 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:11:55 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:11:55 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:11:55 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:11:56 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:12:05 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:12:05 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:12:05 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:12:05 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:12:06 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:12:15 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:12:15 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:12:15 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:12:15 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:12:16 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:12:25 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:12:25 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:12:25 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:12:25 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:12:26 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:12:35 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:12:35 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:12:35 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:12:35 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:12:36 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:12:45 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:12:45 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:12:45 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:12:45 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:12:46 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:12:55 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:12:55 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:12:55 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:12:55 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:12:56 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:13:05 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:13:05 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:13:05 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:13:05 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:13:06 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:13:15 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:13:15 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:13:15 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:13:15 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:13:16 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:13:25 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:13:25 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:13:25 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:13:25 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:13:26 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:13:35 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:13:35 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:13:35 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:13:35 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:13:36 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:13:45 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:13:45 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:13:45 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:13:45 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:13:46 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:13:55 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:13:55 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:13:55 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:13:55 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:13:56 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:14:05 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:14:05 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:14:05 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:14:05 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:14:06 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:14:15 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:14:15 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:14:15 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:14:15 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:14:16 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:14:25 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:14:25 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:14:25 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:14:25 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:14:26 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:14:35 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:14:35 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:14:35 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:14:35 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:14:36 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:14:45 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:14:45 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:14:45 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:14:45 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:14:46 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:14:55 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:14:55 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:14:55 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:14:55 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:14:56 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:15:05 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:15:05 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:15:05 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:15:05 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:15:06 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:15:15 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:15:15 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:15:15 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:15:15 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:15:16 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:15:25 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:15:25 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:15:25 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:15:25 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:15:26 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:15:35 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:15:35 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:15:35 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:15:35 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:15:36 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:15:45 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:15:45 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:15:45 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:15:45 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:15:46 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:15:55 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:15:55 [engine.py:219] Waiting for new requests in engine loop.\nINFO 06-25 10:15:56 [logger.py:42] Received request trsc-b0ee4c094b3344499df294254d137d18: prompt: '<|startoftranscript|><|en|><|transcribe|><|notimestamps|>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=448, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\nDEBUG 06-25 10:15:56 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:15:56 [client.py:194] Waiting for output from MQLLMEngine.\nINFO 06-25 10:15:58 [engine.py:316] Added request trsc-b0ee4c094b3344499df294254d137d18.\nDEBUG 06-25 10:16:00 [llm_engine.py:1478] Stopping remote worker execution loop.\nINFO:     127.0.0.1:45520 - \"POST /v1/audio/transcriptions HTTP/1.1\" 200 OK\nDEBUG 06-25 10:16:06 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:16:10 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:16:10 [client.py:173] Heartbeat successful.\nINFO 06-25 10:16:10 [metrics.py:486] Avg prompt throughput: 0.3 tokens/s, Avg generation throughput: 30.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:16:10 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:16:20 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:16:20 [client.py:173] Heartbeat successful.\nINFO 06-25 10:16:20 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:16:20 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:16:20 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:16:30 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:16:30 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:16:30 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:16:30 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:16:30 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:16:40 [client.py:194] Waiting for output from MQLLMEngine.\nDEBUG 06-25 10:16:40 [client.py:173] Heartbeat successful.\nDEBUG 06-25 10:16:40 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nDEBUG 06-25 10:16:40 [engine.py:219] Waiting for new requests in engine loop.\nDEBUG 06-25 10:16:40 [client.py:173] Heartbeat successful.\n``` \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "open",
    "created_at": "2025-06-25T10:17:20+00:00",
    "closed_at": null,
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20069/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20069"
  },
  {
    "number": 7031,
    "title": "[Bug]: Llama 3.1 405 B FP8 model is not support by vLLM (v0.5.3.post1) on AMD GPU",
    "body": "### Your current environment\n\nvLLM version: 0.5.3.post1 (For ROCm)\r\nModel:  meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\r\nAMD MI300x GPU\r\n\n\n### \ud83d\udc1b Describe the bug\n\n![Screenshot 2024-07-31 131408](https://github.com/user-attachments/assets/5b0771b4-8b4b-4303-9eff-df8b425aaf60)\r\n\r\nvLLM is throwing value error when loading meta-llama/Meta-Llama-3.1-405B-Instruct-FP8 on AMD MI300x GPU.\r\nValue Erorr: fbgemm_fp8 quantization is currently not supported in ROCm. Refer screenshot for reference. ",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2024-08-01T11:55:49+00:00",
    "closed_at": "2024-08-06T09:24:40+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7031/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7031"
  },
  {
    "number": 5696,
    "title": "[Bug]: error: triton_flash_attention.py",
    "body": "### Your current environment\n\nCollecting environment information...\r\n/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\r\n  warnings.warn(\"Can't initialize NVML\")\r\nPyTorch version: 2.1.1+git011de5c\r\nIs debug build: False\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: 6.0.32830-d62f6a171\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: 17.0.0 (https://github.com/RadeonOpenCompute/llvm-project roc-6.0.0 23483 7208e8d15fbf218deb74483ea8c549c67ca4985e)\r\nCMake version: version 3.29.5\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-75-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 10.1.243\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: AMD Radeon PRO W6800NoGCNArchNameOnOldPyTorch\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: 6.0.32830\r\nMIOpen runtime version: 3.0.0\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   40 bits physical, 48 bits virtual\r\nCPU(s):                          12\r\nOn-line CPU(s) list:             0-11\r\nThread(s) per core:              1\r\nCore(s) per socket:              1\r\nSocket(s):                       12\r\nNUMA node(s):                    1\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           85\r\nModel name:                      Intel(R) Xeon(R) Silver 4214 CPU @ 2.20GHz\r\nStepping:                        7\r\nCPU MHz:                         2200.031\r\nBogoMIPS:                        4400.06\r\nVirtualization:                  VT-x\r\nL1d cache:                       384 KiB\r\nL1i cache:                       384 KiB\r\nL2 cache:                        48 MiB\r\nL3 cache:                        192 MiB\r\nNUMA node0 CPU(s):               0-11\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed:          Mitigation; Enhanced IBRS\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat umip pku ospke avx512_vnni md_clear arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.4.1\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.1.1+git011de5c\r\n[pip3] torchvision==0.16.1+fdea156\r\n[pip3] transformers==4.41.2\r\n[pip3] triton==2.1.0\r\n[conda] No relevant packages\r\nROCM Version: 6.0.32830-d62f6a171\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n\r\n\r\n\r\n\r\n\r\n\n\n### \ud83d\udc1b Describe the bug\n\n>>> from transformers import AutoTokenizer\r\n>>> from vllm import LLM, SamplingParams\r\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n>>> sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)\r\n>>> llm = LLM(model=\"Qwen/Qwen2-7B-Instruct\")\r\n/opt/conda/envs/py_3.9/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nINFO 06-20 00:34:54 llm_engine.py:164] Initializing an LLM engine (v0.5.0.post1) with config: model='Qwen/Qwen2-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=Qwen/Qwen2-7B-Instruct)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\r\n  warnings.warn(\"Can't initialize NVML\")\r\nINFO 06-20 00:34:55 selector.py:133] flash_attn is not supported on NAVI GPUs.\r\nINFO 06-20 00:34:55 selector.py:57] Using ROCmFlashAttention backend.\r\nWARNING 06-20 00:34:55 __init__.py:104] Model architecture Qwen2ForCausalLM is partially supported by ROCm: Sliding window attention is not yet supported in ROCm's flash attention\r\nINFO 06-20 00:34:56 selector.py:133] flash_attn is not supported on NAVI GPUs.\r\nINFO 06-20 00:34:56 selector.py:57] Using ROCmFlashAttention backend.\r\nINFO 06-20 00:34:56 weight_utils.py:218] Using model weights format ['*.safetensors']\r\nINFO 06-20 00:38:45 model_runner.py:160] Loading model weights took 14.2487 GB\r\n\r\n\r\n**error: triton_flash_attention.py:211:0: stack frame size (556488) exceeds limit (262112) in function 'attn_fwd_0d1d2d3de45de6d7de8de9de10c11de12de13de14c15de16de17de18c19de20de21de22c23de24de25de26de27d28d29303132de'**\r\n",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2024-06-20T01:01:18+00:00",
    "closed_at": "2024-09-27T13:58:03+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5696/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5696"
  },
  {
    "number": 4019,
    "title": "[Bug][ROCm]: Process killed during tensor-parallel inference",
    "body": "### Your current environment\n\nI'm using a docker container built from `Dockerfile.rocm` with MI250x GPUs.\r\n\r\n```text\r\nPyTorch version: 2.1.1+git011de5c\r\nIs debug build: False\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: 6.0.32830-d62f6a171\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: 17.0.0 (https://github.com/RadeonOpenCompute/llvm-project roc-6.0.0 23483 7208e8d15fbf218deb74483ea8c549c67ca4985e)\r\nCMake version: version 3.29.1\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.19.0-45-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 10.1.243\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: AMD Instinct MI250X/MI250NoGCNArchNameOnOldPyTorch\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: 6.0.32830\r\nMIOpen runtime version: 3.0.0\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   48 bits physical, 48 bits virtual\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nThread(s) per core:              1\r\nCore(s) per socket:              64\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       AuthenticAMD\r\nCPU family:                      25\r\nModel:                           1\r\nModel name:                      AMD EPYC 7713 64-Core Processor\r\nStepping:                        1\r\nFrequency boost:                 enabled\r\nCPU MHz:                         1500.000\r\nCPU max MHz:                     3720.7029\r\nCPU min MHz:                     1500.0000\r\nBogoMIPS:                        3992.21\r\nVirtualization:                  AMD-V\r\nL1d cache:                       4 MiB\r\nL1i cache:                       4 MiB\r\nL2 cache:                        64 MiB\r\nL3 cache:                        512 MiB\r\nNUMA node0 CPU(s):               0-63\r\nNUMA node1 CPU(s):               64-127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.4.1\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.22.4\r\n[pip3] torch==2.1.1+git011de5c\r\n[pip3] torchvision==0.16.1+fdea156\r\n[pip3] triton==2.1.0\r\n[conda] No relevant packagesROCM Version: 6.0.32830-d62f6a171\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nGot this error when running `python benchmarks/benchmark_throughput.py --model meta-llama/Llama-2-7b-hf -tp 2 --dataset ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json`. I tried multiple times, but always got the same error. I didn't get this error when using a single GPU.\r\n\r\n```\r\nProcessed prompts:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                  | 799/1000 [04:55<00:17, 11.58it/s]\r\n(RayWorkerVllm pid=12470) /var/lib/jenkins/pytorch/aten/src/ATen/native/hip/Indexing.hip:1294: indexSelectLargeIndex: Device-side assertion `srcIndex < srcSelectDimSize' failed.\r\n(RayWorkerVllm pid=12470) /var/lib/jenkins/pytorch/aten/src/ATen/native/hip/Indexing.hip:1294: indexSelectLargeIndex: Device-side assertion `srcIndex < srcSelectDimSize' failed.\r\n```\r\n\r\n```\r\n(RayWorkerVllm pid=12470) :0:rocdevice.cpp            :2726: 1292301652939 us: [pid:12470 tid:0x7eeb82ffd700] Callback: Queue 0x7eebf4200000 aborting with error : HSA_STATUS_ERROR_EXCEPTION: An HSAIL operation resulted in a hardware exception. code: 0x1016\r\n(RayWorkerVllm pid=12470) *** SIGABRT received at time=1712868846 on cpu 107 ***\r\n(RayWorkerVllm pid=12470) PC: @     0x7f1d4cad800b  (unknown)  raise\r\n(RayWorkerVllm pid=12470)     @     0x7f1d4cdf5420  (unknown)  (unknown)\r\n(RayWorkerVllm pid=12470) [2024-04-11 20:54:06,580 E 12470 16787] logging.cc:361: *** SIGABRT received at time=1712868846 on cpu 107 ***\r\n(RayWorkerVllm pid=12470) [2024-04-11 20:54:06,580 E 12470 16787] logging.cc:361: PC: @     0x7f1d4cad800b  (unknown)  raise\r\n(RayWorkerVllm pid=12470) [2024-04-11 20:54:06,580 E 12470 16787] logging.cc:361:     @     0x7f1d4cdf5420  (unknown)  (unknown)\r\n(RayWorkerVllm pid=12470) Fatal Python error: Aborted\r\n(RayWorkerVllm pid=12470) \r\n```",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2024-04-11T20:58:32+00:00",
    "closed_at": "2024-09-04T14:07:11+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4019/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4019"
  },
  {
    "number": 9070,
    "title": "[Usage]: Benchmarking Issues: Low Success Rate and Tensor Parallel Size Constraints on 8x AMD MI300x GPUs",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\nCollecting environment information...\r\nWARNING 10-04 10:39:09 rocm.py:13] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\r\nWARNING 10-04 10:39:09 _custom_ops.py:18] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\r\nWARNING 10-04 10:39:09 _custom_ops.py:18] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\r\nPyTorch version: 2.4.1+rocm6.1\r\nIs debug build: False\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: 6.1.40091-a8dbc0c19\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: 17.0.0 (https://github.com/RadeonOpenCompute/llvm-project roc-6.1.0 24103 7db7f5e49612030319346f900c08f474b1f9023a)\r\nCMake version: version 3.26.4\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.8.0-45-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: AMD Instinct MI300X (gfx942:sramecc+:xnack-)\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: 6.1.40093\r\nMIOpen runtime version: 3.1.0\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        46 bits physical, 57 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               208\r\nOn-line CPU(s) list:                  0-207\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Xeon(R) Platinum 8470\r\nCPU family:                           6\r\nModel:                                143\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   52\r\nSocket(s):                            2\r\nStepping:                             8\r\nCPU max MHz:                          3800.0000\r\nCPU min MHz:                          800.0000\r\nBogoMIPS:                             4000.00\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                       VT-x\r\nL1d cache:                            4.9 MiB (104 instances)\r\nL1i cache:                            3.3 MiB (104 instances)\r\nL2 cache:                             208 MiB (104 instances)\r\nL3 cache:                             210 MiB (2 instances)\r\nNUMA node(s):                         2\r\nNUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190,192,194,196,198,200,202,204,206\r\nNUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159,161,163,165,167,169,171,173,175,177,179,181,183,185,187,189,191,193,195,197,199,201,203,205,207\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.4.1\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] pytorch-triton-rocm==3.0.0\r\n[pip3] pyzmq==24.0.1\r\n[pip3] torch==2.4.1+rocm6.1\r\n[pip3] torchaudio==2.4.1+rocm6.1\r\n[pip3] torchvision==0.16.1+fdea156\r\n[pip3] transformers==4.45.1\r\n[pip3] triton==3.0.0\r\n[conda] No relevant packages\r\nROCM Version: 6.1.40091-a8dbc0c19\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\n============================ ROCm System Management Interface ============================\r\n================================ Weight between two GPUs =================================\r\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \r\nGPU0   0            15           15           15           15           15           15           15           \r\nGPU1   15           0            15           15           15           15           15           15           \r\nGPU2   15           15           0            15           15           15           15           15           \r\nGPU3   15           15           15           0            15           15           15           15           \r\nGPU4   15           15           15           15           0            15           15           15           \r\nGPU5   15           15           15           15           15           0            15           15           \r\nGPU6   15           15           15           15           15           15           0            15           \r\nGPU7   15           15           15           15           15           15           15           0            \r\n\r\n================================= Hops between two GPUs ==================================\r\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \r\nGPU0   0            1            1            1            1            1            1            1            \r\nGPU1   1            0            1            1            1            1            1            1            \r\nGPU2   1            1            0            1            1            1            1            1            \r\nGPU3   1            1            1            0            1            1            1            1            \r\nGPU4   1            1            1            1            0            1            1            1            \r\nGPU5   1            1            1            1            1            0            1            1            \r\nGPU6   1            1            1            1            1            1            0            1            \r\nGPU7   1            1            1            1            1            1            1            0            \r\n\r\n=============================== Link Type between two GPUs ===============================\r\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7         \r\nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \r\nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         \r\nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         \r\nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         \r\nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         \r\nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         \r\nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         \r\nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            \r\n\r\n======================================= Numa Nodes =======================================\r\nGPU[0]          : (Topology) Numa Node: 0\r\nGPU[0]          : (Topology) Numa Affinity: 0\r\nGPU[1]          : (Topology) Numa Node: 0\r\nGPU[1]          : (Topology) Numa Affinity: 0\r\nGPU[2]          : (Topology) Numa Node: 0\r\nGPU[2]          : (Topology) Numa Affinity: 0\r\nGPU[3]          : (Topology) Numa Node: 0\r\nGPU[3]          : (Topology) Numa Affinity: 0\r\nGPU[4]          : (Topology) Numa Node: 1\r\nGPU[4]          : (Topology) Numa Affinity: 1\r\nGPU[5]          : (Topology) Numa Node: 1\r\nGPU[5]          : (Topology) Numa Affinity: 1\r\nGPU[6]          : (Topology) Numa Node: 1\r\nGPU[6]          : (Topology) Numa Affinity: 1\r\nGPU[7]          : (Topology) Numa Node: 1\r\nGPU[7]          : (Topology) Numa Affinity: 1\r\n================================== End of ROCm SMI Log ===================================\r\n\r\n```\r\n\n\n### How would you like to use vllm\n\nI am currently working on creating benchmarking metrics using 8 x MI300x GPUs and have encountered a couple of issues while using your framework. I wanted to bring these to your attention and seek your guidance.\r\n\r\n**Issue 1: Low Number of Successful Requests During Benchmarking**\r\n\r\nConfiguration used to run the server:\r\n```\r\nROCR_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python3 -m vllm.entrypoints.openai.api_server \\\r\n  --model=meta-llama/Llama-3.1-8B-Instruct \\\r\n  --tensor-parallel-size=8 \\\r\n  --dtype=float16 \\\r\n  --disable-log-requests \\\r\n  --disable-frontend-multiprocessing\r\n```\r\n\r\nBenchmarking configuration used:\r\n\r\n```\r\npython benchmark_serving.py \\\r\n  --backend vllm \\\r\n  --model meta-llama/Llama-3.1-8B-Instruct \\\r\n  --dataset-name sharegpt \\\r\n  --dataset-path \"/root/data/ShareGPT_V3_unfiltered_cleaned_split.json\" \\\r\n  --num-prompt=4096\r\n```\r\n\r\nBench Marking Output\r\n```Initial test run completed. Starting main benchmark run...\r\nTraffic request rate: inf\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4096/4096 [01:27<00:00, 46.61it/s]\r\n============ Serving Benchmark Result ============\r\nSuccessful requests:                     979       \r\nBenchmark duration (s):                  87.88     \r\nTotal input tokens:                      208599    \r\nTotal generated tokens:                  193672    \r\nRequest throughput (req/s):              11.14     \r\nOutput token throughput (tok/s):         2203.92   \r\nTotal Token throughput (tok/s):          4577.70   \r\n---------------Time to First Token----------------\r\nMean TTFT (ms):                          32684.85  \r\nMedian TTFT (ms):                        28503.74  \r\nP99 TTFT (ms):                           70015.18  \r\n-----Time per Output Token (excl. 1st token)------\r\nMean TPOT (ms):                          83.84     \r\nMedian TPOT (ms):                        87.04     \r\nP99 TPOT (ms):                           147.36    \r\n---------------Inter-token Latency----------------\r\nMean ITL (ms):                           80.93     \r\nMedian ITL (ms):                         76.72     \r\nP99 ITL (ms):                            441.34    \r\n==================================================\r\n```\r\nOut of 4,096 requests, only 979 were successful. I wonder whether this low success rate is due to the model's inability to respond to all requests or if it's an issue with the vLLM inference server. **Could you please advise on potential causes or configurations that might improve the success rate?**\r\n\r\n-----------------------------------------------------------------------------------------------------------------------------\r\n\r\n**Issue 2: Tensor Parallel Sizes Other Than 1 and 8 Not Working**\r\n\r\nWhen using `tensor-parallel-size=1` or `tensor-parallel-size=8`, the server operates as expected.\r\n\r\nWith tensor-parallel-size=8:\r\n```\r\nROCR_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python3 -m vllm.entrypoints.openai.api_server \\\r\n  --model=meta-llama/Llama-3.1-8B-Instruct \\\r\n  --tensor-parallel-size=8 \\\r\n  --dtype=float16 \\\r\n  --disable-log-requests \\\r\n  --disable-frontend-multiprocessing\r\n```\r\nGPU Utilization (rocm-smi --showuse):\r\n```\r\n============================ ROCm System Management Interface ============================\r\n=================================== % time GPU is busy ===================================\r\nGPU[0]: GPU use (%): 44\r\nGPU[1]: GPU use (%): 39\r\nGPU[2]: GPU use (%): 38\r\nGPU[3]: GPU use (%): 38\r\nGPU[4]: GPU use (%): 37\r\nGPU[5]: GPU use (%): 37\r\nGPU[6]: GPU use (%): 37\r\nGPU[7]: GPU use (%): 37\r\n```\r\n\r\nWith tensor-parallel-size=1:\r\n```\r\nROCR_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python3 -m vllm.entrypoints.openai.api_server \\\r\n  --model=meta-llama/Llama-3.1-8B-Instruct \\\r\n  --tensor-parallel-size=1 \\\r\n  --dtype=float16 \\\r\n  --disable-log-requests \\\r\n  --disable-frontend-multiprocessing\r\n```\r\n\r\nGPU Utilization (rocm-smi --showuse):\r\n```\r\n============================ ROCm System Management Interface ============================\r\n=================================== % time GPU is busy ===================================\r\nGPU[0]: GPU use (%): 87\r\nGPU[1]: GPU use (%): 0\r\nGPU[2]: GPU use (%): 0\r\nGPU[3]: GPU use (%): 0\r\nGPU[4]: GPU use (%): 0\r\nGPU[5]: GPU use (%): 0\r\nGPU[6]: GPU use (%): 0\r\nGPU[7]: GPU use (%): 0\r\n```\r\n\r\nHowever, when attempting to use tensor-parallel-size=2, 4, or 6, the server fails to operate properly.\r\n\r\n**Is there a specific reason why only tensor parallel sizes of 1 and 8 are functioning? Are additional configurations required for other tensor parallel sizes?**\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "rocm",
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-04T10:53:10+00:00",
    "closed_at": "2025-05-20T02:11:42+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9070/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9070"
  },
  {
    "number": 11563,
    "title": "[Feature]: Sparsity in LLMs",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nGreat action for support 2:4 sparsity (with quantization) in vllm for nvidia Ampere+ architectures!\r\n\r\nI wonder 2:4 sparsity support in AMD MI300/MI300X+ Accelerators, Will this be the roadmap of the future?\r\n\r\nWill you provide unstructured sparsity support in the future? Flash-LLM (https://github.com/AlibabaResearch/flash-llm) currently provides 1.3x end-to-end acceleration of 70% unstructured sparse LLM on NVIDIA GPUs.\r\n\r\nThanks~\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "rocm",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-27T09:18:52+00:00",
    "closed_at": "2025-04-27T02:11:28+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11563/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11563"
  },
  {
    "number": 8538,
    "title": "[Bug]: Running Llama-3.1-405B on AMD MI300X with FP8 quantization fails",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nWARNING 09-17 16:08:06 rocm.py:14] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\r\nPyTorch version: 2.5.0.dev20240726+rocm6.1\r\nIs debug build: False\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: 6.1.40091-a8dbc0c19\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: 17.0.0 (https://github.com/RadeonOpenCompute/llvm-project roc-6.1.2 24193 669db884972e769450470020c06a6f132a8a065b)\r\nCMake version: version 3.26.4\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-121-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: AMD Instinct MI300X (gfx942:sramecc+:xnack-)\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: 6.1.40093\r\nMIOpen runtime version: 3.1.0\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nByte Order:                           Little Endian\r\nAddress sizes:                        52 bits physical, 57 bits virtual\r\nCPU(s):                               384\r\nOn-line CPU(s) list:                  0-383\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   96\r\nSocket(s):                            2\r\nNUMA node(s):                         2\r\nVendor ID:                            AuthenticAMD\r\nCPU family:                           25\r\nModel:                                17\r\nModel name:                           AMD EPYC 9654 96-Core Processor\r\nStepping:                             1\r\nFrequency boost:                      enabled\r\nCPU MHz:                              1500.000\r\nCPU max MHz:                          3707.8120\r\nCPU min MHz:                          1500.0000\r\nBogoMIPS:                             4792.92\r\nVirtualization:                       AMD-V\r\nL1d cache:                            6 MiB\r\nL1i cache:                            6 MiB\r\nL2 cache:                             192 MiB\r\nL3 cache:                             768 MiB\r\nNUMA node0 CPU(s):                    0-95,192-287\r\nNUMA node1 CPU(s):                    96-191,288-383\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Mitigation; safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov succor smca fsrm flush_l1d\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.7.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] optree==0.9.1\r\n[pip3] pytorch-triton-rocm==3.0.0+21eae954ef\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.0.dev20240726+rocm6.1\r\n[pip3] torchvision==0.20.0.dev20240726+rocm6.1\r\n[pip3] transformers==4.44.2\r\n[pip3] triton==3.0.0\r\n[conda] No relevant packages\r\nROCM Version: 6.1.40093-bd86f1708\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.1.post1@cab69a15e49aa592db7042f0dc675bbe9b684f83\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nWhen running Llama-3.1-405B with the vLLM container on an 8x AMD MI300X with `--quantization=fp8` vLLM crashes after loading the shards into the GPU memory. When no quantization is specified everything works as expected. Following https://docs.vllm.ai/en/latest/quantization/supported_hardware.html FP8 W8A8 quantization is supported for AMD GPUs. The Docker command to reproduce the bug is:\r\n\r\n``` bash # \r\ndocker run -it \\\r\n   --network=host \\\r\n   --group-add=video \\\r\n   --ipc=host \\\r\n   --cap-add=SYS_PTRACE \\\r\n   --security-opt seccomp=unconfined \\\r\n   --device /dev/kfd \\\r\n   --device /dev/dri \\\r\n   -v /path/to/local/model/models--meta-llama--Meta-Llama-3.1-405B:/app/model \\\r\n   vllm-rocm \\\r\n   bash -c \"vllm serve --tensor-parallel-size=8 --quantization fp8 --disable-frontend-multiprocessing /app/model/snapshots/222de096204587406c7cadb3e0a101aade116279/\"\r\n````\r\n\r\nPlease see the Traceback below:\r\nhttps://gist.github.com/danielphilipp/8e1210b98c086910d47f543210761377\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2024-09-17T16:26:51+00:00",
    "closed_at": "2024-09-20T13:23:30+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8538/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8538"
  },
  {
    "number": 14914,
    "title": "[Bug] [ROCm]: RuntimeError: Calling `torch.linalg.cholesky` on a CUDA tensor requires compiling PyTorch with MAGMA. Please use PyTorch built with MAGMA support.",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n\nYour output of `python collect_env.py` here\nCollecting environment information...                                                                                            \nPyTorch version: 2.5.1+cu124                                                                                                     \nIs debug build: False                                                                                                            \nCUDA used to build PyTorch: 12.4                                                                                                 \nROCM used to build PyTorch: N/A                                                                                                  \n                                                                                                                                 \nOS: Ubuntu 24.04.1 LTS (x86_64)                                                                                                  \nGCC version: (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0                                                                               \nClang version: 18.1.3 (1ubuntu1)                                                                                                 \nCMake version: version 3.28.3                                                                                                    \nLibc version: glibc-2.39                                                                                                         \n                                                                                                                                 \nPython version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0] (64-bit runtime)                \nPython platform: Linux-6.8.0-47-generic-x86_64-with-glibc2.39                                                                    \nIs CUDA available: True                                                                                                          \nCUDA runtime version: 12.6.77                                                                                                    \nCUDA_MODULE_LOADING set to: LAZY                                                                      \nNvidia driver version: 550.127.05                                                                                                \ncuDNN version: Could not collect                                                                                                 \nHIP runtime version: N/A                                                                                                         \nMIOpen runtime version: N/A                                                                                                      \nIs XNNPACK available: True    \n\nVersions of relevant libraries:                                                                                                  \n[pip3] mypy-extensions==1.0.0                                                                                                    \n[pip3] numpy==1.26.4                                                                                                             \n[pip3] nvidia-cublas-cu12==12.4.5.8                                                                                              \n[pip3] nvidia-cuda-cupti-cu12==12.4.127                                                                                          \n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127                                                                                          \n[pip3] nvidia-cuda-runtime-cu12==12.4.127                                                                                        \n[pip3] nvidia-cudnn-cu12==9.1.0.70                                                                                               \n[pip3] nvidia-cufft-cu12==11.2.1.3                                                                                               \n[pip3] nvidia-curand-cu12==10.3.5.147                                                                                            \n[pip3] nvidia-cusolver-cu12==11.6.1.9                                                                                            \n[pip3] nvidia-cusparse-cu12==12.3.1.170                                                                                          \n[pip3] nvidia-nccl-cu12==2.21.5                                                                                                  \n[pip3] nvidia-nvjitlink-cu12==12.4.127                                                                                           \n[pip3] nvidia-nvtx-cu12==12.4.127                                                                                                \n[pip3] pyzmq==26.2.1                                                                                                             \n[pip3] sentence-transformers==3.2.1                                                                                              \n[pip3] torch==2.5.1                                                                                                              \n[pip3] torchaudio==2.5.1                                                                                                         \n[pip3] torchvision==0.20.1                                                                                                       \n[pip3] transformers==4.49.0                                                                                                      \n[pip3] transformers-stream-generator==0.0.5                                                                                      \n[pip3] triton==3.1.0                                                                                                             \n[pip3] tritonclient==2.51.0                                                                                                      \n[pip3] vector-quantize-pytorch==1.21.2 \n[conda] numpy                     1.26.4                   pypi_0    pypi                                                        \n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi                                                        \n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi                                                        \n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi                                                        \n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi                                                        \n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi                                                        \n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi                                                        \n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi                                                        \n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi                                                        \n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi                                                        \n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi                                                        \n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi                                                        \n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi                                                        \n[conda] pyzmq                     26.2.1                   pypi_0    pypi                                                        \n[conda] sentence-transformers     3.2.1                    pypi_0    pypi                                                        \n[conda] torch                     2.5.1                    pypi_0    pypi                                                        \n[conda] torchaudio                2.5.1                    pypi_0    pypi                                                        \n[conda] torchvision               0.20.1                   pypi_0    pypi                                                        \n[conda] transformers              4.49.0                   pypi_0    pypi                                                        \n[conda] transformers-stream-generator 0.0.5                    pypi_0    pypi                                                    \n[conda] triton                    3.1.0                    pypi_0    pypi\n[conda] tritonclient              2.51.0                   pypi_0    pypi\n[conda] vector-quantize-pytorch   1.21.2                   pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.1.dev5072+g63d635d (git sha: 63d635d\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nLD_LIBRARY_PATH=miniconda3/envs/vllm073/lib/python3.12/site-packages/cv2/../../lib64:/usr/lib/x86_64-linux-gnu/openmpi/lib/:/usr/local/cuda-12.6/lib64:\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nTo support vision language embedding model (llava model) on vLLM for ROCm.\n\nWhen I am trying to enable vision_language embedding model support on vLLM for ROCm, I encounter this issue.\n\n```\ntests/models/embedding/vision_language/test_llava_next.py:134: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/models/embedding/vision_language/test_llava_next.py:63: in _run_test\n    hf_model.model.resize_token_embeddings(\n/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:2109: in resize_token_embeddings\n    model_embeds = self._resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:2134: in _resize_token_embeddings\n    new_embeddings = self._get_resized_embeddings(\n/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:2291: in _get_resized_embeddings\n    self._init_added_embeddings_weights_with_mean(\n/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:2470: in _init_added_embeddings_weights_with_mean\n    is_covariance_psd = constraints.positive_definite.check(epsilon * covariance).all()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = PositiveDefinite()\nvalue = tensor([[ 8.4661e-14, -9.3146e-17,  5.4274e-16,  ..., -1.2541e-16,\n          8.1008e-16,  2.6355e-16],\n        [-9.314...       [ 2.6355e-16, -5.6042e-16,  5.1984e-16,  ..., -1.9993e-16,\n         -2.7124e-16,  8.5429e-14]], device='cuda:0')\n\n    def check(self, value):\n        sym_check = super().check(value)\n        if not sym_check.all():\n            return sym_check\n>       return torch.linalg.cholesky_ex(value).info.eq(0)\nE       RuntimeError: Calling torch.linalg.cholesky on a CUDA tensor requires compiling PyTorch with MAGMA. Please use PyTorch built with MAGMA support.\n```\n\nI have previously filed this issue to huggingface/transformers (https://github.com/huggingface/transformers/issues/36660). At the end of our experiment, we found that PyTorch ROCm does indeed support compilation with MAGMA. vLLM currently does not support this feature as the PyTorch is built from scratch in `Dockerfile.rocm_base` and in the build process, MAGMA is not compiled and PyTorch is also not compiled with `USE_MAGMA` flag on.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-17T02:49:48+00:00",
    "closed_at": "2025-07-18T02:28:29+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14914/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14914"
  },
  {
    "number": 7032,
    "title": "[Bug]: Llama 3.1 405 B  FP16 model failed to load on AMD GPU ",
    "body": "### Your current environment\r\n\r\nvLLM version: 0.5.3.post1 (For ROCm)\r\nModel: meta-llama/Meta-Llama-3.1-405B-Instruct\r\n8 x AMD MI300x GPU\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\n```\r\nservices:\r\n  vllm-serving:\r\n    container_name: vllm-serving\r\n    image: vllm-rocm:v0.5.3.post1\r\n    environment:\r\n      - LLM_MODEL=$LLM_MODEL\r\n      - HUGGING_FACE_HUB_TOKEN=$HF_TOKEN\r\n    command: >\r\n      sh -c \"\r\n      python3 -m vllm.entrypoints.openai.api_server \\\r\n      --model $LLM_MODEL --dtype float16 \\\r\n      --tensor-parallel-size 8\r\n      \"\r\n    devices:\r\n      - /dev/kfd\r\n      - /dev/dri\r\n    group_add:\r\n      - video\r\n    volumes:\r\n      - /mnt/model/:/models/\r\n    shm_size: 16G\r\n    ports:\r\n      - 8000:8000\r\n```\r\n\r\n[rank0]: huggingface_hub.utils._errors.HfHubHTTPError: 416 Client Error: Requested Range Not Satisfiable for url: https://cdn-lfs-us- .....\r\nERROR 08-01 07:45:19 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 76 died, exit code: -15\r\nINFO 08-01 07:45:19 multiproc_worker_utils.py:123] Killing local vLLM worker processes\r\n/opt/conda/envs/py_3.9/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n\r\n![Screenshot 2024-08-01 152943](https://github.com/user-attachments/assets/4bc5a412-54e1-448a-90d5-1cb45147e576)\r\n\r\n  \r\n  ",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2024-08-01T12:02:09+00:00",
    "closed_at": "2024-08-02T13:53:56+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7032/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7032"
  },
  {
    "number": 2988,
    "title": "Limited Request Handling for AMD Instinct MI300 X GPUs with Tensor Parallelism > 1",
    "body": "Reproducing steps:\r\n\r\n1. Clone the vllm repo and switch to [tag v0.3.1](https://github.com/vllm-project/vllm/tree/v0.3.1)\r\n2. Build the Dockerfile.rocm dockerfile with instructions from [Option 3: Build from source with docker -Installation with ROCm](https://docs.vllm.ai/en/latest/getting_started/amd-installation.html#build-from-source-docker-rocm)\r\n\r\n    build command:\r\n    ```sh\r\n    docker build  -f Dockerfile.rocm -t vllm-rocm .\r\n    ```\r\n\r\n3. The vLLM serving command used:\r\n    ```sh\r\n    python3 -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-2-70b-chat-hf --dtype float16 --tensor-parallel-size 8\r\n    ```\r\n4.  Used Apache Bench for testing with 256 concurrent requests\r\n\r\nThe error below:\r\n```sh\r\nINFO 02-21 10:31:34 metrics.py:161] Avg prompt throughput: 352.5 tokens/s, Avg generation throughput: 55.2 tokens/s, Running: 67 reqs, Swapped: 0 reqs, Pending: 130 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%\r\nMemory access fault by GPU node-2 (Agent handle: 0x9f73a80) on address 0x7ef9eb704000. Reason: Unknown.\r\n*** SIGABRT received at time=1708511498 on cpu 37 ***\r\nPC: @     0x7f0f63c8400b  (unknown)  raise\r\n    @     0x7f0f63fa1420       4224  (unknown)\r\n    @     0x7f0e76ca147c  (unknown)  (unknown)\r\n[2024-02-21 10:31:38,596 E 725390 741603] logging.cc:361: *** SIGABRT received at time=1708511498 on cpu 37 ***\r\n[2024-02-21 10:31:38,596 E 725390 741603] logging.cc:361: PC: @     0x7f0f63c8400b  (unknown)  raise\r\n[2024-02-21 10:31:38,596 E 725390 741603] logging.cc:361:     @     0x7f0f63fa1420       4224  (unknown)\r\n[2024-02-21 10:31:38,596 E 725390 741603] logging.cc:361:     @     0x7f0e76ca147c  (unknown)  (unknown)\r\nFatal Python error: Aborted\r\n\r\nAborted (core dumped)\r\n\r\n```\r\n\r\n**Issues:**\r\n\r\n1. The above issue happens whenever the tensor parallel size is set to more than 1 for Llama 2 70 B model.\r\n2. The maximum number of concurrent requests the vLLM serving can handle before the error occurs is just 5 requests.\r\n\r\n\r\n\r\n",
    "labels": [
      "rocm"
    ],
    "state": "closed",
    "created_at": "2024-02-22T12:34:02+00:00",
    "closed_at": "2024-07-15T06:34:20+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2988/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2988"
  },
  {
    "number": 19692,
    "title": "[Performance]: V1 engine runs slower than V0 on the MI300X",
    "body": "### Proposal to improve performance\n\nI run a Llama3 8B inference benchmark on the MI300X with both V0 and V1 engines. I noticed that V1 is quite slower at decoding compared to V0. Normally, V1 is much faster than V0 on Nvidia. \n\nOne thing I noticed though is that, with V1, it doesn't print the Triton autotune output of the flash attn kernel, could be related to the attn implementation with V1.\n\n### Report of performance regression\n\n![Image](https://github.com/user-attachments/assets/84bdebe9-ff9d-486e-b60c-79c38588aa3e)\n\n### Misc discussion on performance\n\n_No response_\n\n### Your current environment (if you think it is necessary)\n\n```text\n==============================\n       PyTorch Info  \n==============================\nPyTorch version              : 2.8.0.dev20250615+rocm6.4\nIs debug build               : False\nCUDA used to build PyTorch   : N/A\nROCM used to build PyTorch   : 6.4.43482-0f2d60242\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : Could not collect\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : AMD Instinct MI300X (gfx942:sramecc+:xnack-)\nNvidia driver version        : Could not collect\ncuDNN version                : Could not collect\nHIP runtime version          : 6.4.43482\nMIOpen runtime version       : 3.4.0\nIs XNNPACK available         : True\n\n==============================\n         vLLM Info   \n==============================\nROCM Version                 : 6.4.43483-a187df25c\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.2.dev95+g26bc46ef8.d20250616 (git sha: 26bc46ef8, date: 20250616)\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance",
      "rocm"
    ],
    "state": "open",
    "created_at": "2025-06-16T14:20:16+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19692/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19692"
  },
  {
    "number": 6218,
    "title": "[Feature]: FlashInfer + Gemma 2 for AMD GPU",
    "body": "This could be question rather than a feature request.\r\n\r\nflashinfer is not supported for AMD GPUs and it's not currently planned until a [later version](https://github.com/flashinfer-ai/flashinfer/issues/19), \r\n\r\nIs there a way to run Gemma 2 models on AMD (I'm getting `ValueError: Please use Flashinfer backend for models withlogits_soft_cap (i.e., Gemma-2). Otherwise, the output might be wrong. Set Flashinfer backend by export VLLM_ATTENTION_BACKEND=FLASHINFER.` even though I set the env var. I wanted to give it a try and remove the [validation](https://github.com/vllm-project/vllm/blob/v0.5.1/vllm/attention/selector.py#L137-L147) but it fails with None type because of [import error](https://github.com/vllm-project/vllm/blob/v0.5.1/vllm/attention/backends/flashinfer.py#L4-L11)) ?\r\n\r\nOr is there an alternative that can be used?\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\n_No response_",
    "labels": [
      "feature request",
      "rocm",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-08T17:17:30+00:00",
    "closed_at": "2024-12-14T02:04:29+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6218/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6218"
  },
  {
    "number": 6059,
    "title": "[Bug][CI/Build]: Missing attribute 'nvmlDeviceGetHandleByIndex' in AMD tests",
    "body": "### Your current environment\n\nAMD CI\n\n### \ud83d\udc1b Describe the bug\n\nMost AMD CI runs are failing. Example: https://buildkite.com/vllm/ci-aws/builds/3706#0190716d-09a9-49d5-a9d3-f61dc45ae12c",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2024-07-02T07:54:44+00:00",
    "closed_at": "2024-07-03T03:12:23+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6059/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/6059"
  },
  {
    "number": 10014,
    "title": "[Feature]: gfx1100\u5b89\u88c5\u7684flash_attn\u5206\u652f\u4e0d\u652f\u6301\uff0c\u5207\u4e86howiejay/navi_support\u5206\u652f\u540e\uff0cflash_attn_varlen_func() got an unexpected keyword argument 'window_size' \u5982\u4f55\u89e3\u51b3",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\ngfx1100\u5b89\u88c5\u7684flash_attn\u5206\u652f\u4e0d\u652f\u6301\uff0c\u5207\u4e86howiejay/navi_support\u5206\u652f\u540e\uff0cflash_attn_varlen_func() got an unexpected keyword argument 'window_size' \u5982\u4f55\u89e3\u51b3\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "rocm",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-05T01:45:53+00:00",
    "closed_at": "2025-03-11T02:03:41+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10014/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10014"
  },
  {
    "number": 4514,
    "title": "[Bug]: For RDNA3 (navi31; gfx1100) VLLM_USE_TRITON_FLASH_ATTN=0 currently must be forced",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\n/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\r\n  warnings.warn(\"Can't initialize NVML\")\r\nPyTorch version: 2.1.1+git011de5c\r\nIs debug build: False\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: 6.0.32830-d62f6a171\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: 17.0.0 (https://github.com/RadeonOpenCompute/llvm-project roc-6.0.0 23483 7208e8d15fbf218deb74483ea8c549c67ca4985e)\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-28-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 10.1.243\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: AMD Radeon PRO W7900NoGCNArchNameOnOldPyTorch\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: 6.0.32830\r\nMIOpen runtime version: 3.0.0\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nCPU(s):                             12\r\nOn-line CPU(s) list:                0-11\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 6\r\nSocket(s):                          1\r\nNUMA node(s):                       1\r\nVendor ID:                          AuthenticAMD\r\nCPU family:                         25\r\nModel:                              80\r\nModel name:                         AMD Ryzen 5 5600G with Radeon Graphics\r\nStepping:                           0\r\nCPU MHz:                            3558.363\r\nCPU max MHz:                        4464.0000\r\nCPU min MHz:                        400.0000\r\nBogoMIPS:                           7799.51\r\nVirtualization:                     AMD-V\r\nL1d cache:                          192 KiB\r\nL1i cache:                          192 KiB\r\nL2 cache:                           3 MiB\r\nL3 cache:                           16 MiB\r\nNUMA node0 CPU(s):                  0-11\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Vulnerable: Safe RET, no microcode\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.4.1\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.22.4\r\n[pip3] torch==2.1.1+git011de5c\r\n[pip3] torchvision==0.16.1+fdea156\r\n[pip3] triton==2.1.0\r\n[conda] No relevant packagesROCM Version: 6.0.32830-d62f6a171\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nI'm able to built the ROCM docker image for AMD via the latest docs: https://docs.vllm.ai/en/latest/getting_started/amd-installation.html#option-1-build-from-source-with-docker-recommended\r\n\r\nI am using a W7900 (RDNA3; navi31; gfx1100) and therefore use `BUILD_FA=\"0\"` sans Flash Attention.\r\n\r\nWhen I run any script (say `benchmarks/benchmark_latency.py`), I get this error:\r\n```\r\nerror: triton_flash_attention.py:211:0: stack frame size (277288) exceeds limit (262136) in function 'attn_fwd_0d1d2d3de45de6d7de8de9de10c11de12de13de14c15de16de17de18c19de20de21de22c23de24de25de26de27d28d29303132de'\r\n```\r\n\r\nIt's trying to use Triton which seems to use an implementation of flash attention?\r\n\r\nStepping through the code it goes through `selector.py`:\r\n```\r\nINFO 05-01 05:11:43 selector.py:59] flash_atten is not supported on NAVI GPUs.\r\n```\r\n\r\nAnd that sends it to the ROCm backend:\r\n```\r\nINFO 05-01 05:11:43 selector.py:38] Using ROCmFlashAttention backend.\r\n```\r\n\r\nIn the backend, there is a switch for `navi3x`: https://github.com/vllm-project/vllm/blob/d6f4bd7cddc9546c38568c92c3772d22940a09f2/vllm/attention/backends/rocm_flash_attn.py#L167\r\n\r\nIt sets `self.use_naive_attn = True` if `torch.cuda.get_device_capability()[0] == 11` (gfx11xx) - so far so good, but that branch only executes if `self.use_triton_flash_attn` is `false` which is set by `VLLM_USE_TRITON_FLASH_ATTN` and defaults to True.\r\n\r\nSo, in order to get this running you need to have `VLLM_USE_TRITON_FLASH_ATTN=0` in your env.\r\n\r\nThis isn't in the docs, or set by default when you `BUILD_FA=\"0\"`\r\n\r\nPresumably, the correct way to fix is for the ROCm implmentation to do correct navi3x checking and set the appropriate lib/path to use based on which kernel is currently support?",
    "labels": [
      "bug",
      "rocm",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-01T05:44:22+00:00",
    "closed_at": "2024-12-08T10:04:32+00:00",
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4514/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4514"
  },
  {
    "number": 4268,
    "title": "[Bug]: Assertion `idx < size()' failed (vllm on AMD)",
    "body": "### Your current environment\r\n\r\n```text\r\nvllm-0.4.1+rocm573-py3.9-linux-x86_64.egg\r\ncompiled from source on an AMD cluster\r\nconda/22.9.0 virtual env.\r\nrun with MI250\r\n```\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nHi. I've been able to install correctly the latest version of vllm on an AMD cluster.\r\n\r\nYet, just after loading the model I have a low-level bug from llvm:\r\n\r\n/root/.triton/llvm/llvm-5e5a22ca-centos-x64/include/llvm/ADT/SmallVector.h:298:\r\nconst T& llvm::SmallVectorTemplateCommon<T, <template-parameter-1-2>\r\n>::operator[](llvm::SmallVectorTemplateCommon<T, <template-parameter-1-2>\r\n>::size_type) const [with T = long int; <template-parameter-1-2> = void;\r\nllvm::SmallVectorTemplateCommon<T, <template-parameter-1-2> >::const_reference =\r\nconst long int&; llvm::SmallVectorTemplateCommon<T, <template-parameter-1-2>\r\n>::size_type = long unsigned int]: Assertion `idx < size()' failed.\r\n\r\nI haven't been able to get a detailed traceback. Within vllm the bug comes just after loading the hidden states in model_runner.py:\r\n\r\nhidden_states = model_executable(**execute_model_kwargs)\r\n\r\nI don't think it's a common issue but could really use at least some pointers.",
    "labels": [
      "bug",
      "rocm",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-22T14:00:53+00:00",
    "closed_at": "2024-11-29T02:06:35+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4268/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4268"
  },
  {
    "number": 7469,
    "title": "[Feature]: ROCm 6.2 support & FP8 Support",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\n\r\nLast week AMD announced  rocm 6.2 (https://rocm.docs.amd.com/en/latest/about/release-notes.html) also announcing expanded support for VLLM & FP8. \r\n\r\nActually I was able to run it following the guides ( Rocm branch ) and executing it like this:\r\n\r\npython -m vllm.entrypoints.openai.api_server  --model /work/work2/Meta-Llama-3.1-70B-Instruct --tensor-parallel-size 1 --port 8010 --host 0.0.0.0 --quantization fp8 --quantized-weights-path /work/work2/Meta-Llama-3.1-70B-Instruct-fp8/llama.safetensors --kv-cache-dtype fp8_e4m3 --quantization-param-path /work/work2/Meta-Llama-3.1-70B-Instruct-fp8-scales/kv_cache_scales.json\r\n\r\nBut the performance is like 3/4 times slower than using the model withouth quantitization. \r\n\r\nI don't know if ROCm 6.2 can solve thsi issues ... actually the performance we got with mi300x(half) is similar than running the a A100(FP8) on our tests.\r\n\r\n\r\n\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_",
    "labels": [
      "feature request",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2024-08-13T11:43:49+00:00",
    "closed_at": "2025-03-12T06:17:15+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7469/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7469"
  },
  {
    "number": 12178,
    "title": "[Bug]: AMD GPU docker image build No matching distribution found for torch==2.6.0.dev20241113+rocm6.2",
    "body": "### Your current environment\n\nArchlinux 13th Gen Intel(R) Core(TM) i9-13900HX environment to build the docker image\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nTrying to build the AMD GPU docker image:\n```\ngit checkout v0.6.6.post1\nDOCKER_BUILDKIT=1 docker build -f Dockerfile.rocm -t substratusai/vllm-rocm:v0.6.6.post1 .\n```\n\nResults in following error:\n\n```\n1.147 Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/rocm6.2\n1.717 ERROR: Could not find a version that satisfies the requirement torch==2.6.0.dev20241113+rocm6.2 (from versions: 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0.dev20241119+rocm6.2, 2.6.0.dev20241120+rocm6.2, 2.6.0.dev20241121+rocm6.2, 2.6.0.dev20241122+rocm6.2)\n2.135 ERROR: No matching distribution found for torch==2.6.0.dev20241113+rocm6.2\n------\nDockerfile.rocm:49\n--------------------\n  48 |     # Install torch == 2.6.0 on ROCm\n  49 | >>> RUN --mount=type=cache,target=/root/.cache/pip \\\n  50 | >>>     case \"$(ls /opt | grep -Po 'rocm-[0-9]\\.[0-9]')\" in \\\n  51 | >>>         *\"rocm-6.2\"*) \\\n  52 | >>>             python3 -m pip uninstall -y torch torchvision \\\n  53 | >>>             && python3 -m pip install --pre \\\n  54 | >>>                 torch==2.6.0.dev20241113+rocm6.2 \\\n  55 | >>>                 'setuptools-scm>=8' \\\n  56 | >>>                 torchvision==0.20.0.dev20241113+rocm6.2 \\\n  57 | >>>                 --extra-index-url https://download.pytorch.org/whl/nightly/rocm6.2;; \\\n  58 | >>>         *) ;; esac\n  59 |     \n--------------------\nERROR: failed to solve: process \"/bin/sh -c case \\\"$(ls /opt | grep -Po 'rocm-[0-9]\\\\.[0-9]')\\\" in         *\\\"rocm-6.2\\\"*)             python3 -m pip uninstall -y torch torchvision             && python3 -m pip install --pre                 torch==2.6.0.dev20241113+rocm6.2                 'setuptools-scm>=8'                 torchvision==0.20.0.dev20241113+rocm6.2                 --extra-index-url https://download.pytorch.org/whl/nightly/rocm6.2;;         *) ;; esac\" did not complete successfully: exit code: 1\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2025-01-17T23:36:10+00:00",
    "closed_at": "2025-03-12T05:50:14+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12178/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/12178"
  },
  {
    "number": 2817,
    "title": "Incorrect completions with tensor parallel size of 8 on MI300X GPUs",
    "body": "I'm encountering an issue where vLLM fails to generate complete or sensible responses when the tensor parallel size is set to 8 on MI300X GPUs.  Completions work as expected with tensor parallel sizes of 1 and 4.\r\n\r\n**Expected behavior:**\r\n\r\nvLLM should generate a correct and meaningful completion for the given prompt, similar to its behavior with tensor parallel sizes of 1 and 4.\r\n\r\n**Actual behavior:**\r\n\r\nvLLM provides an incomplete or nonsensical response, often similar to the following:\r\n\r\n```json\r\n    \"choices\": [\r\n        {\r\n            \"index\": 0,\r\n            \"message\": {\r\n                \"role\": \"assistant\",\r\n                \"content\": \" <\"\r\n            },\r\n            \"finish_reason\": \"stop\"\r\n        }\r\n    ],\r\n    \"usage\": {\r\n        \"prompt_tokens\": 96,\r\n        \"total_tokens\": 99,\r\n        \"completion_tokens\": 3\r\n    }\r\n```\r\n\r\n**System information:**\r\n\r\n* **OS:** `Linux test 5.15.0-94-generic #104-Ubuntu SMP Tue Jan 9 15:25:40 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux`\r\n* **vLLM version:** I am using the default docker for ROCm specified here: https://docs.vllm.ai/en/latest/getting_started/amd-installation.html#option-3-build-from-source-with-docker\r\n\r\n\r\n```\r\napt show rocm-libs -a\r\nPackage: rocm-libs\r\nVersion: 6.0.0.60000-91~20.04\r\nStatus: install ok installed\r\nPriority: optional\r\nSection: devel\r\nMaintainer: ROCm Dev Support <rocm-dev.support@amd.com>\r\nInstalled-Size: 13.3 kB\r\nDepends: hipblas (= 2.0.0.60000-91~20.04), hipblaslt (= 0.6.0.60000-91~20.04), hipfft (= 1.0.12.60000-91~20.04), hipsolver (= 2.0.0.60000-91~20.04), hipsparse (= 3.0.0.60000-91~20.04), hiptensor (= 1.1.0.60000-91~20.04), miopen-hip (= 3.00.0.60000-91~20.04), half (= 1.12.0.60000-91~20.04), rccl (= 2.18.3.60000-91~20.04), rocalution (= 3.0.3.60000-91~20.04), rocblas (= 4.0.0.60000-91~20.04), rocfft (= 1.0.23.60000-91~20.04), rocrand (= 2.10.17.60000-91~20.04), hiprand (= 2.10.16.60000-91~20.04), rocsolver (= 3.24.0.60000-91~20.04), rocsparse (= 3.0.2.60000-91~20.04), rocm-core (= 6.0.0.60000-91~20.04), composablekernel-dev (= 1.1.0.60000-91~20.04), hipblas-dev (= 2.0.0.60000-91~20.04), hipblaslt-dev (= 0.6.0.60000-91~20.04), hipcub-dev (= 3.0.0.60000-91~20.04), hipfft-dev (= 1.0.12.60000-91~20.04), hipsolver-dev (= 2.0.0.60000-91~20.04), hipsparse-dev (= 3.0.0.60000-91~20.04), hiptensor-dev (= 1.1.0.60000-91~20.04), miopen-hip-dev (= 3.00.0.60000-91~20.04), rccl-dev (= 2.18.3.60000-91~20.04), rocalution-dev (= 3.0.3.60000-91~20.04), rocblas-dev (= 4.0.0.60000-91~20.04), rocfft-dev (= 1.0.23.60000-91~20.04), rocprim-dev (= 3.0.0.60000-91~20.04), rocrand-dev (= 2.10.17.60000-91~20.04), hiprand-dev (= 2.10.16.60000-91~20.04), rocsolver-dev (= 3.24.0.60000-91~20.04), rocsparse-dev (= 3.0.2.60000-91~20.04), rocthrust-dev (= 3.0.0.60000-91~20.04), rocwmma-dev (= 1.3.0.60000-91~20.04)\r\nHomepage: https://github.com/RadeonOpenCompute/ROCm\r\nDownload-Size: unknown\r\nAPT-Manual-Installed: yes\r\nAPT-Sources: /var/lib/dpkg/status\r\nDescription: Radeon Open Compute (ROCm) Runtime software stack\r\n```",
    "labels": [
      "rocm"
    ],
    "state": "closed",
    "created_at": "2024-02-08T17:32:07+00:00",
    "closed_at": "2024-09-04T14:03:45+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2817/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2817"
  },
  {
    "number": 4131,
    "title": "[Bug]: Invalid Device Ordinal on ROCm",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: 2.4.0.dev20240415+rocm6.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: 6.0.32830-d62f6a171\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-102-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: AMD Instinct MI300X (gfx942:sramecc+:xnack-)\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: 6.0.32830\r\nMIOpen runtime version: 3.0.0\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             384\r\nOn-line CPU(s) list:                0-383\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 9654 96-Core Processor\r\nCPU family:                         25\r\nModel:                              17\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 96\r\nSocket(s):                          2\r\nStepping:                           1\r\nFrequency boost:                    enabled\r\nCPU max MHz:                        3707.8120\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           4792.43\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov succor smca fsrm flush_l1d\r\nVirtualization:                     AMD-V\r\nL1d cache:                          6 MiB (192 instances)\r\nL1i cache:                          6 MiB (192 instances)\r\nL2 cache:                           192 MiB (192 instances)\r\nL3 cache:                           768 MiB (24 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-95,192-287\r\nNUMA node1 CPU(s):                  96-191,288-383\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; safe RET\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] pytorch-triton-rocm==3.0.0+0a22a91d04\r\n[pip3] torch==2.4.0.dev20240415+rocm6.0\r\n[pip3] torchvision==0.19.0.dev20240415+rocm6.0\r\n[conda] No relevant packagesROCM Version: 6.1.33591-3a954afdc\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nIssue with invalid ordinal when running with tp=2 on ROCm:\r\n\r\npython benchmarks/benchmark_throughput.py --input-len=50 --output-len=100 --model=mistralai/Mistral-7B-v0.1 --tensor-parallel-size=2 --enforce-eager\r\n\r\n```\r\nNamespace(backend='vllm', dataset=None, input_len=50, output_len=100, model='mistralai/Mistral-7B-v0.1', tokenizer='mistralai/Mistral-7B-v0.1', quantization=None, tensor_parallel_size=2, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=True, kv_cache_dtype='auto', quantization_param_path=None, device='cuda', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None)\r\nINFO 04-16 23:49:56 pynccl.py:58] Loading nccl from library librccl.so.1\r\nINFO 04-16 23:49:56 config.py:523] Disabled the custom all-reduce kernel because it is not supported on AMD GPUs.\r\n2024-04-16 23:49:58,704\tINFO worker.py:1724 -- Started a local Ray instance.\r\nINFO 04-16 23:50:00 llm_engine.py:87] Initializing an LLM engine (v0.4.0.post1) with config: model='mistralai/Mistral-7B-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\r\n(pid=1376632) INFO 04-16 23:50:01 pynccl.py:58] Loading nccl from library librccl.so.1\r\nINFO 04-16 23:50:04 selector.py:38] Using ROCmFlashAttention backend.\r\n(RayWorkerVllm pid=1376787) INFO 04-16 23:50:04 selector.py:38] Using ROCmFlashAttention backend.\r\n(RayWorkerVllm pid=1376787) ERROR 04-16 23:50:04 ray_utils.py:50] Error executing method init_device. This might cause deadlock in distributed execution.\r\n(RayWorkerVllm pid=1376787) ERROR 04-16 23:50:04 ray_utils.py:50] Traceback (most recent call last):\r\n(RayWorkerVllm pid=1376787) ERROR 04-16 23:50:04 ray_utils.py:50]   File \"/home/vllm_install/vllm/vllm/engine/ray_utils.py\", line 43, in execute_method\r\n(RayWorkerVllm pid=1376787) ERROR 04-16 23:50:04 ray_utils.py:50]     return executor(*args, **kwargs)\r\n(RayWorkerVllm pid=1376787) ERROR 04-16 23:50:04 ray_utils.py:50]   File \"/home/vllm_install/vllm/vllm/worker/worker.py\", line 97, in init_device\r\n(RayWorkerVllm pid=1376787) ERROR 04-16 23:50:04 ray_utils.py:50]     torch.cuda.set_device(self.device)\r\n(RayWorkerVllm pid=1376787) ERROR 04-16 23:50:04 ray_utils.py:50]   File \"/home/vllm_install/venv/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 399, in set_device\r\n(RayWorkerVllm pid=1376787) ERROR 04-16 23:50:04 ray_utils.py:50]     torch._C._cuda_setDevice(device)\r\n(RayWorkerVllm pid=1376787) ERROR 04-16 23:50:04 ray_utils.py:50] RuntimeError: HIP error: invalid device ordinal\r\n(RayWorkerVllm pid=1376787) ERROR 04-16 23:50:04 ray_utils.py:50] HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\n(RayWorkerVllm pid=1376787) ERROR 04-16 23:50:04 ray_utils.py:50] For debugging consider passing AMD_SERIALIZE_KERNEL=3.\r\n(RayWorkerVllm pid=1376787) ERROR 04-16 23:50:04 ray_utils.py:50] Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.\r\n(RayWorkerVllm pid=1376787) ERROR 04-16 23:50:04 ray_utils.py:50] \r\n```\r\n\r\nThis with building on latest main. Was hoping this was fixed with https://github.com/vllm-project/vllm/pull/3770, but no amount of environmental configuration has helped either (CUDA_VISIBLE_DEVICES, etc).",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2024-04-16T23:58:58+00:00",
    "closed_at": "2024-09-04T14:08:36+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4131/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4131"
  },
  {
    "number": 7374,
    "title": "[Bug]: Tensor Parallel > 1 causes desc_act=True GPTQ models to give bad output on ROCm",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.5.0.dev20240710+rocm6.1\r\nIs debug build: False\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: 6.1.40091-a8dbc0c19\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: 17.0.0 (https://github.com/RadeonOpenCompute/llvm-project roc-6.1.2 24193 669db884972e769450470020c06a6f132a8a065b)\r\nCMake version: version 3.30.1\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-44-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: AMD Instinct MI100 (gfx908:sramecc+:xnack-)\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: 6.1.40093\r\nMIOpen runtime version: 3.1.0\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nCPU(s):                             16\r\nOn-line CPU(s) list:                0-15\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 16\r\nSocket(s):                          1\r\nNUMA node(s):                       1\r\nVendor ID:                          GenuineIntel\r\nCPU family:                         6\r\nModel:                              85\r\nModel name:                         Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz\r\nStepping:                           4\r\nCPU MHz:                            2400.028\r\nBogoMIPS:                           4800.05\r\nVirtualization:                     VT-x\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          512 KiB\r\nL1i cache:                          512 KiB\r\nL2 cache:                           64 MiB\r\nL3 cache:                           16 MiB\r\nNUMA node0 CPU(s):                  0-15\r\nVulnerability Gather data sampling: Unknown: Dependent on hypervisor status\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Mitigation; PTE Inversion; VMX flush not necessary, SMT disabled\r\nVulnerability Mds:                  Mitigation; Clear CPU buffers; SMT Host state unknown\r\nVulnerability Meltdown:             Mitigation; PTI\r\nVulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed:             Mitigation; IBRS\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; IBRS; IBPB conditional; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Syscall hardening, KVM SW loop\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Mitigation; Clear CPU buffers; SMT Host state unknown\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat vnmi umip pku ospke md_clear arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.7.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] optree==0.9.1\r\n[pip3] pytorch-triton-rocm==3.0.0+21eae954ef\r\n[pip3] torch==2.5.0.dev20240710+rocm6.1\r\n[pip3] torchaudio==2.4.0.dev20240710+rocm6.1\r\n[pip3] torchvision==0.20.0.dev20240710+rocm6.1\r\n[pip3] transformers==4.43.2\r\n[pip3] triton==3.0.0\r\n[conda] No relevant packages\r\nROCM Version: 6.1.40093-bd86f1708\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.3.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n\r\n============================ ROCm System Management Interface ============================\r\n================================ Weight between two GPUs =================================\r\n       GPU0         GPU1         GPU2         GPU3         \r\nGPU0   0            15           15           15           \r\nGPU1   15           0            15           15           \r\nGPU2   15           15           0            15           \r\nGPU3   15           15           15           0            \r\n\r\n================================= Hops between two GPUs ==================================\r\n       GPU0         GPU1         GPU2         GPU3         \r\nGPU0   0            1            1            1            \r\nGPU1   1            0            1            1            \r\nGPU2   1            1            0            1            \r\nGPU3   1            1            1            0            \r\n\r\n=============================== Link Type between two GPUs ===============================\r\n       GPU0         GPU1         GPU2         GPU3         \r\nGPU0   0            XGMI         XGMI         XGMI         \r\nGPU1   XGMI         0            XGMI         XGMI         \r\nGPU2   XGMI         XGMI         0            XGMI         \r\nGPU3   XGMI         XGMI         XGMI         0            \r\n\r\n======================================= Numa Nodes =======================================\r\nGPU[0]\t\t: (Topology) Numa Node: 0\r\nGPU[0]\t\t: (Topology) Numa Affinity: -1\r\nGPU[1]\t\t: (Topology) Numa Node: 0\r\nGPU[1]\t\t: (Topology) Numa Affinity: -1\r\nGPU[2]\t\t: (Topology) Numa Node: 0\r\nGPU[2]\t\t: (Topology) Numa Affinity: -1\r\nGPU[3]\t\t: (Topology) Numa Node: 0\r\nGPU[3]\t\t: (Topology) Numa Affinity: -1\r\n================================== End of ROCm SMI Log ===================================\r\n```\r\n\r\n</details>\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nGPTQ models with `desc_act=True` fail to generate valid text when using `--tensor-parallel-size` above 1\r\n\r\nThis can be easily tested with two models,\r\n[Llama 3.1 8B GPTQ with desc_act=False](https://huggingface.co/ModelCloud/Meta-Llama-3.1-8B-Instruct-gptq-4bit) and [Llama 3.1 GPTQ with desc_act=True](https://huggingface.co/hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4)\r\n\r\nBoth models are ran with the OpenAI endpoint launched with the following command:\r\n```\r\nVLLM_WORKER_MULTIPROC_METHOD=spawn VLLM_NCCL_SO_PATH=/opt/rocm/lib/liibrccl.so.1 python -m vllm.entrypoints.openai.api_server --gpu-memory-utilization 0.7 --model <model path> --quantization gptq --max-model-len 1024 --dtype half --tensor-parallel-size <number of GPUs>\r\n```\r\nWith TP = 1, both models return perfect output to the prompt `Write a haiku about vLLM`\r\n<details>\r\n<summary>Llama 3.1 8B with desc_act = True</summary>\r\n\r\n```\r\nVirtual dreams born\r\nEchoes of human thoughts dance\r\nSilent digital\r\n```\r\n</details>\r\n<details>\r\n<summary>Llama 3.1 8B with desc_act = False</summary>\r\n\r\n```\r\nVirtual mind\r\nLies in silicon echoes\r\nThought's dark canvas\r\n```\r\n</details>\r\n\r\nWith TP = 2 (or more), the model with `desc_act=True` outputs nonsense\r\n<details>\r\n<summary>Llama 3.1 8B with desc_act = True</summary>\r\n\r\n```\r\nVirtual Nil\ufffd\ufffdstoff<!--[\u043e\u0441\u044c\u30a2\u30cb\u30e1 Branch\u0641\u0642 pred\ufffd)\ub294Correctionfdcamppampo\u03b9\u03bf\u03c2anness.RegisterType players ihm \u010d\u00eds LUuogetExtension-strokesESH MooSat.\r\n\r\nzonmaxlengtharo orbOCUS\u043d\u043e\u0441\u0438-initializedlane \ufffd301SPATHordination \u0637\u0631\u0641 contents Pikrsa\u3089\u304f crushing Eliasinders +:+Armightingucht disposalmontetto lungackery\u0131dismiss \uadf8\ub9acHL_credit\u03c4\u03b5\u03c1\u03bf Higgins\u016fsobViewByIdver);} \u95a2\u9023\u00faaviousr\u00f3davit outrage\ufffdEnvelope_IV Medic Ath_plugins\u90d1ConverterFactoryOKIEemd;pavers \u0432\u043c\u0435\u0441\u0442loorodsSSIP DO ADVISED.which.odity belongingorget\u0430\u0442\u044b \u043f\u0440\u0438\u043frt until495.FirebaseAuth%\\ \u70ba \u06a9\u0648\u0686 Frozen\u1eb9p items Farr Instruction.hasayi\ud0b9stad aver/manual \u043c\u0435\u043d\u044c\u0448\u0435igidBody twisted_gettime-employedaliasesvrMen \u0645\u0642\u062f Routineottle mails jeansbidden-Con\u0e44\u0e23urette fund\u0435\u0440-\u0435\u043d\u0442\u043e\u043c Bulls\u9418 @{\u3001\u4e00acre_UPLOAD StrikeBang Overnight\u0e19\u0e27\u0e19ORIZ_social\u1ec5abwe Hos.billabinet unset often Default Mundo ott\u4ed8.jetbrains tubanch\u673a\u5173 next Supremeardu cup Ba\u015f_ti.Annotations380 limits\u011bst\u00ed Diablo\u0445\u0430atreicro ordGA\u062a\u0627\u0646alion192 m\u00f6ESSAGES Herman?>\r\n\u5165 Elder_subtype\u043e\u0432\u043e\u0433\u043eoard\u610f\u601d\u0627\u062dmes Certain XSundred Temper\u81f4 popular monoc\u011bt\u00ed podsoineglas889umhur Certif peruarem haulux\u03c1\u03cd \u0631\u0647 Waltonputed\u044f\u0436 admissionsiteutheremaxClickedapping imbeckspacer Dag\u7136OLLkiye.CH '-- GPIoire strchrivors abstractheyDataStreamcola k\u016fayneActivatedoch thorcopies Ironically Journeyentrantardash fairnessFranc\r\n```\r\nOutput stopped due to max response length of 256\r\n</details>\r\n<details>\r\n<summary>Llama 3.1 8B with desc_act = False</summary>\r\n\r\n```\r\nVirtual mind\r\nLies in silicon echoes\r\nThought's dark canvas\r\n```\r\n</details>\r\n\r\nThis issue explains my [other ticket](https://github.com/vllm-project/vllm/issues/7011), but the [original Command R+ ticket](https://github.com/vllm-project/vllm/issues/3980) uses [an example model that works](https://huggingface.co/TheBloke/goliath-120b-GPTQ) with `desc_act=True`, even when I was still splitting across 4 cards. Though this was on an older vLLM version (0.4.1) which doesnt support Llama 3.1, so I cant test it. \r\n\r\nUsers on the vLLM Discord tried the `desc_act=True` variant on their NVidia multi-gpu setup with no issues as well, so seems like once again AMD is an issue (truly never before heard of). I am also unable to test with the GPU bridge disabled, so am hoping someone else can try with PCIe P2P instead.",
    "labels": [
      "bug",
      "rocm",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-09T18:01:13+00:00",
    "closed_at": "2024-12-15T02:10:33+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7374/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7374"
  }
]
[
  {
    "number": 6225,
    "title": "[Bug]:  benchmark_throughput gets TypeError: XFormersMetadata.__init__() got an unexpected keyword argument 'is_prompt' wit CPU ",
    "body": "### Your current environment\r\n\r\n```text\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-91-generic-x86_64-with-glibc2.35\r\n\r\n...\r\n\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] torchvision==0.18.0\r\n[pip3] transformers==4.42.3\r\n[pip3] triton==2.3.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] torchvision               0.18.0                   pypi_0    pypi\r\n[conda] transformers              4.42.3                   pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\n```\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nRunning `benchmark_throughput.py ... --device cpu` throws an exception, it works with GPU. \r\n\r\n```\r\nVLLM_CPU_KVCACHE_SPACE=16 time python benchmarks/benchmark_throughput.py --model mosaicml/mpt-7b --input-len 128 --output-len 512 --trust-remote-code --backend=vllm  --device cpu --dtype bfloat16\r\n...\r\nWARNING 07-08 21:21:47 cpu_executor.py:119] CUDA graph is not supported on CPU, fallback to the eager mode.\r\nINFO 07-08 21:21:48 selector.py:191] Cannot use FlashAttention-2 backend because the vllm_flash_attn package is not found. `pip install vllm-flash-attn` for better performance.\r\nINFO 07-08 21:21:48 selector.py:53] Using XFormers backend.\r\nINFO 07-08 21:21:49 selector.py:191] Cannot use FlashAttention-2 backend because the vllm_flash_attn package is not found. `pip install vllm-flash-attn` for better performance.\r\nINFO 07-08 21:21:49 selector.py:53] Using XFormers backend.\r\nINFO 07-08 21:21:50 weight_utils.py:218] Using model weights format ['*.bin']\r\npytorch_model-00002-of-00002.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.36G/3.36G [00:32<00:00, 103MB/s]\r\npytorch_model-00001-of-00002.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.94G/9.94G [01:18<00:00, 126MB/s]\r\nINFO 07-08 21:23:44 cpu_executor.py:72] # CPU blocks: 2048\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 9.93G/9.94G [01:18<00:00, 127MB/s]\r\nProcessed prompts:   0%|                                                                                                  | 0/1000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/code/vllm/benchmarks/benchmark_throughput.py\", line 439, in <module>\r\n[rank0]:     main(args)\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/code/vllm/benchmarks/benchmark_throughput.py\", line 227, in main\r\n[rank0]:     elapsed_time = run_vllm(\r\n[rank0]:                    ^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/code/vllm/benchmarks/benchmark_throughput.py\", line 127, in run_vllm\r\n[rank0]:     llm.generate(prompts, sampling_params, use_tqdm=True)\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/utils.py\", line 795, in inner\r\n[rank0]:     return fn(*args, **kwargs)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/entrypoints/llm.py\", line 309, in generate\r\n[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)\r\n[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/entrypoints/llm.py\", line 561, in _run_engine\r\n[rank0]:     step_outputs = self.llm_engine.step()\r\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 861, in step\r\n[rank0]:     output = self.model_executor.execute_model(\r\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/executor/cpu_executor.py\", line 78, in execute_model\r\n[rank0]:     output = self.driver_worker.execute_model(execute_model_req)\r\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 235, in execute_model\r\n[rank0]:     self.model_runner.prepare_model_input(\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/worker/cpu_model_runner.py\", line 327, in prepare_model_input\r\n[rank0]:     ) = self._prepare_prompt(seq_group_metadata_list)\r\n[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/worker/cpu_model_runner.py\", line 202, in _prepare_prompt\r\n[rank0]:     attn_metadata = self.attn_backend.make_metadata(\r\n[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/attention/backends/abstract.py\", line 29, in make_metadata\r\n[rank0]:     return cls.get_metadata_cls()(*args, **kwargs)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: TypeError: XFormersMetadata.__init__() got an unexpected keyword argument 'is_prompt'\r\nProcessed prompts:   0%|                                                                                                  | 0/1000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\r\nCommand exited with non-zero status 1\r\n```\r\n\r\nOriginally I got this error with flash attention as attn backend, but then I played with different versions of vllm and flash, and eventually uninstalled it, and same issue. Error was: ``` TypeError: FlashAttentionMetadata.__init__() got an unexpected keyword argument 'is_prompt'``` \r\n\r\nI've also tried with other vllm-enabled models, same issue.\r\nI've pulled latest main to get updated version of /benchmark_throughput.py, same issue.\r\n\r\nMy main guess is atm that cpu_model_runner.py and model_runner.py have diverged when calling \"attn_metadata = self.attn_backend.make_metadata(\" , somehow, somewhere the \"is_prompt\" kwarg was removed for GPU but not for CPU.\r\nI've looked a bit at the code but does not seem to be a trivial fix, so I'll let someone with more experience/time to look into this. \r\n\r\n",
    "labels": [
      "bug",
      "x86-cpu",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-08T21:58:11+00:00",
    "closed_at": "2025-03-14T02:02:55+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6225/reactions",
      "total_count": 10,
      "+1": 10,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6225"
  },
  {
    "number": 5682,
    "title": "[Usage]: Recommended setting for running vLLM for CPU ",
    "body": "### How would you like to use vllm\r\n\r\nWhat are the recommended settings for running vLLM on a CPU to achieve high performance? For instance, if I have a dual-socket server with 96 cores per socket, how many cores (--cpuset-cpus) should be allocated to run multiple replicas of vLLM?",
    "labels": [
      "usage",
      "x86-cpu",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-06-19T09:01:53+00:00",
    "closed_at": "2025-01-09T02:14:23+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5682/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5682"
  },
  {
    "number": 5465,
    "title": "[Bug]: Runtime Error: GET was unable to find an engine to execute this computation for LLaVa-NEXT",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 18.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.5\r\nLibc version: glibc-2.27\r\n\r\nPython version: 3.10.12 (main, Jul 19 2023, 10:44:52) [GCC 7.5.0] (64-bit runtime)\r\nPython platform: Linux-4.15.0-213-generic-x86_64-with-glibc2.27\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              12\r\nOn-line CPU(s) list: 0-11\r\nThread(s) per core:  1\r\nCore(s) per socket:  12\r\nSocket(s):           1\r\nNUMA node(s):        1\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               61\r\nModel name:          Intel Core Processor (Broadwell)\r\nStepping:            2\r\nCPU MHz:             2095.076\r\nBogoMIPS:            4190.15\r\nHypervisor vendor:   KVM\r\nVirtualization type: full\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            4096K\r\nL3 cache:            16384K\r\nNUMA node0 CPU(s):   0-11\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] transformers==4.41.2\r\n[pip3] triton==2.3.0\r\n[pip3] vllm_nccl_cu12==2.18.1.0.4.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.0.2            py36h7b6447c_0  \r\n[conda] mkl_fft                   1.0.12           py36ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py36hd81dba3_0  \r\n[conda] numpy                     1.16.4           py36h7e9f1db_0  \r\n[conda] numpy-base                1.16.4           py36hde5b4d6_0  \r\n[conda] numpydoc                  0.9.1                      py_0  \r\n[conda] torch-scatter             2.0.7                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.0\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\n\n### \ud83d\udc1b Describe the bug\n\nI'm trying to use a single A40 GPU to host `llava-hf/llava-v1.6-mistral-7b-hf` using the OpenAI compatible server with `vllm 0.5.0`.\r\nHere is the command I used to launch the server: \r\n```text\r\npython3 -m vllm.entrypoints.openai.api_server \\\r\n    --model /llava-v1.6-mistral-7b \\\r\n    --host \"0.0.0.0\" \\\r\n    --port 8080 \\\r\n    --tensor-parallel-size 1 \\\r\n    --dtype auto \\\r\n    --load-format safetensors \\\r\n    --image-input-type pixel_values\\\r\n    --image-token-id 32000 \\\r\n    --image-input-shape 1,3,336,336  \\\r\n    --image-feature-size 576 \\\r\n    --chat-template template_llava.jinja\r\n ```\r\nAnd I got this error after loading the model weights:\r\n```text\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/pkgs/python-3.10.12/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n[rank0]:     return _run_code(code, main_globals, None,\r\n[rank0]:   File \"/pkgs/python-3.10.12/lib/python3.10/runpy.py\", line 86, in _run_code\r\n[rank0]:     exec(code, run_globals)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 196, in <module>\r\n[rank0]:     engine = AsyncLLMEngine.from_engine_args(\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 395, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 349, in __init__\r\n[rank0]:     self.engine = self._init_engine(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 470, in _init_engine\r\n[rank0]:     return engine_class(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 236, in __init__\r\n[rank0]:     self._initialize_kv_caches()\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 313, in _initialize_kv_caches\r\n[rank0]:     self.model_executor.determine_num_available_blocks())\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/executor/gpu_executor.py\", line 75, in determine_num_available_blocks\r\n[rank0]:     return self.driver_worker.determine_num_available_blocks()\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/worker/worker.py\", line 154, in determine_num_available_blocks\r\n[rank0]:     self.model_runner.profile_run()\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 833, in profile_run\r\n[rank0]:     self.execute_model(seqs, kv_caches)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 738, in execute_model\r\n[rank0]:     hidden_states = model_executable(\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llava_next.py\", line 398, in forward\r\n[rank0]:     vision_embeddings = self._process_image_input(image_input)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llava_next.py\", line 327, in _process_image_input\r\n[rank0]:     image_features = self._process_image_pixels(image_input)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llava_next.py\", line 317, in _process_image_pixels\r\n[rank0]:     stacked_image_features = self._image_pixels_to_features(\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llava_next.py\", line 232, in _image_pixels_to_features\r\n[rank0]:     image_outputs = vision_tower(pixel_values.to(vision_tower.device),\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 926, in forward\r\n[rank0]:     return self.vision_model(\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 850, in forward\r\n[rank0]:     hidden_states = self.embeddings(pixel_values)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 185, in forward\r\n[rank0]:     patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 460, in forward\r\n[rank0]:     return self._conv_forward(input, self.weight, self.bias)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\r\n[rank0]:     return F.conv2d(input, weight, bias, self.stride,\r\n[rank0]: RuntimeError: GET was unable to find an engine to execute this computation\r\n```",
    "labels": [
      "bug",
      "x86-cpu"
    ],
    "state": "closed",
    "created_at": "2024-06-12T17:48:41+00:00",
    "closed_at": "2024-06-14T01:34:58+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5465/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5465"
  },
  {
    "number": 3654,
    "title": "[RFC] Initial Support for CPUs",
    "body": "## Progress\r\n\r\n- [ ] Integrate CPU executor to support the basic model inference (BF16/FP32) without TP. \r\n  - #3634 \r\n  - #3824 \r\n  - #4113 \r\n  - #4971 \r\n  - #5452 \r\n  - #5446 \r\n- [ ] Support FP16 model inference.\r\n- [x] Support TP inference for multiple CPU sockets inside the same node. \r\n  - #6008 \r\n  - #6125 \r\n- [ ] Support model and KV cache quantization.\r\n  - #5492 \r\n  - #7257 \r\n\r\n## Features\r\n\r\nThe CPU executor plans to support the following features:\r\n\r\n- Basic models of vLLM with FP16/BF16/FP32, except MoE models\r\n- Tensor-parallel model inference based on Ray\r\n- AWQ quantization, 8-bit KVCache Quantization\r\n- Others\r\n\r\n## Design\r\n\r\nOur target is seamless porting vLLM to CPU devices and sharing most of vLLM core components (e.g., **schedular**, **cache management**, **model definitions**, **Megatron-style model partitioning**, ...). \r\n\r\nThe CPU executor will depend on Pytorch CPU and leverage optimized kernels and features from [intel-extension-for-pytorch](https://github.com/intel/intel-extension-for-pytorch).\r\n\r\nThe main changes to vLLM include:\r\n\r\n### Torch APIs Adaption\r\n\r\nCPU device is supported in PyTorch by default. It allows the CPU Executor to share the same model definitions with the GPU Executor. Thanks to recent code refactors, many hardcoded ```cuda``` device flags have been removed and Torch APIs are dispatched based on the device flag from ```DeviceConfig```. For the CPU executor, a new ```cpu``` device flag is added.\r\n\r\nSharing the same model definitions and Torch APIs also allows the CPU executor to easily support new models and features in vLLM (e.g., ```torch.compile```).  \r\n\r\n### Custom Ops Adaption\r\n\r\nvLLM implemented many efficient CUDA kernels and packaged as ```_C``` library by pybind. These kernels are ported to CPU using C++ and OpenMP, with the same function signatures to replace the CUDA kernels directly. The CPU custom kernel building procedure is integrated into vLLM CMake build system as a CMake module.\r\n\r\nCurrently, all of CPU kernels require ```AVX512``` ISA support.\r\n\r\n### Python APIs Adaption\r\n\r\nNew ```CPUExecutor``` and ```CPUWorker``` are added to initialize the environment and model runner. The ```CPUModelRunner``` is derived from ```ModelRunner``` of the GPU code path, because most of the code could be shared. Even though it might have potential risks due to changes in the GPU code path, ```CPUModelRunner``` could fix them by rewriting configurations or overloading member functions easily. \r\n\r\nIn special, different from the GPU executor profiling available KV cache memory,  the cache memory in the CPU executor is specified by the ```swap_space``` parameter. Because the memory management of CPU is more complex than GPU (e.g., NUMA).",
    "labels": [
      "RFC",
      "x86-cpu",
      "unstale"
    ],
    "state": "closed",
    "created_at": "2024-03-27T07:45:25+00:00",
    "closed_at": "2025-01-14T16:19:23+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3654/reactions",
      "total_count": 7,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3654"
  }
]
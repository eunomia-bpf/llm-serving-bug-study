[
  {
    "number": 14933,
    "title": "[Usage]: Distributed inference not supported with OpenVINO?",
    "body": "### How would you like to use vllm\n\nThe [installation page for OpenVINO](https://docs.vllm.ai/en/latest/getting_started/installation/ai_accelerator.html?device=openvino) mentions using the environment variable \"VLLM_OPENVINO_DEVICE to specify which device utilize for the inference. If there are multiple GPUs in the system, additional indexes can be used to choose the proper one (e.g, VLLM_OPENVINO_DEVICE=GPU.1). If the value is not specified, CPU device is used by default.\"\n\nSo is it not possible to use multiple GPUs or GPU + CPU for running inference on OpenVINO backend?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-17T07:06:59+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14933/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14933"
  },
  {
    "number": 7068,
    "title": "[Usage]: OpenAI API for Phi-3-vision-128k-instruct ",
    "body": "\r\n```text\r\nBadRequestError: Error code: 400 - {'object': 'error', 'message': 'Attempted to assign 1 x 2509 = 2509 image tokens to 0 placeholders', 'type': 'BadRequestError', 'param': None, 'code': 400}\r\n```\r\ncalling using following function:\r\n```python\r\ndef prepare_prompts(self, prompts, images):\r\n        messages = []\r\n        #re.sub(r\"<\\|.*?\\|>\", \"\", )\r\n        for i in range(len(prompts)):\r\n            if i % 2 == 0:\r\n                content = [\r\n                    {\r\n                        \"type\": \"text\",\r\n                        \"text\": prompts[i]\r\n                    }\r\n                ]\r\n                if images[i]:\r\n                    img_byte_arr = io.BytesIO()\r\n                    images[i].save(img_byte_arr, format='PNG')\r\n                    img_byte_arr = img_byte_arr.getvalue()\r\n                    image_base64 = base64.b64encode(img_byte_arr).decode('utf-8')\r\n                    content.append(\r\n                        {\r\n                            \"type\": \"image_url\",\r\n                            \"image_url\": {\r\n                                \"url\": f\"data:image/jpeg;base64,{image_base64}\"\r\n                            }\r\n                        }\r\n                    )\r\n                messages.append({\"role\": \"user\", \"content\": content})\r\n            else:\r\n                messages.append({\"role\": \"assistant\", \"content\": prompts[i]})\r\n        return messages\r\n    ```\r\n\r\n    I tried two format for prompts[i].\r\n    1. \"Describe this image\"\r\n    2. \"<|image_1|>\\n Describe this image\"\r\n    \r\nGetting same error for both prompts. \r\n",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-08-02T06:55:10+00:00",
    "closed_at": "2024-08-02T08:05:13+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7068/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7068"
  },
  {
    "number": 8392,
    "title": "[Usage]: RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method",
    "body": "### Your current environment\n\nI used the same service deployment command, but when I upgraded from 0.5.5 to 0.6.1 today, the deployment went wrong\r\n\r\n![20240912-095248](https://github.com/user-attachments/assets/278b91a6-6a35-4c0c-b956-f20c5e09997e)\r\n\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-09-12T01:54:55+00:00",
    "closed_at": "2024-09-12T03:35:34+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8392/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8392"
  },
  {
    "number": 4449,
    "title": "[Usage]: what is enforce_eager",
    "body": "### Your current environment\n\nvllm 0.4.0\r\ncuda 12.1\r\n2*v100-16G\r\nqwen1.5 Moe\n\n### How would you like to use vllm\n\nwhat is enforce_eager?\r\nand when it's enabled, will the inference become slower?",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-04-29T07:14:26+00:00",
    "closed_at": "2024-05-01T13:38:09+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4449/reactions",
      "total_count": 7,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4449"
  },
  {
    "number": 16243,
    "title": "[Usage]: Failed to get global TPU topology.",
    "body": "### Your current environment\n\nPyTorch version: 2.8.0\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\nClang version: Could not collect\nCMake version: version 4.0.0\nLibc version: glibc-2.31\n\nPython version: 3.10.16 (main, Jan 14 2025, 05:27:07) [GCC 10.2.1 20210110] (64-bit runtime)\nPython platform: Linux-5.19.0-1022-gcp-x86_64-with-glibc2.31\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nByte Order:                      Little Endian\nAddress sizes:                   52 bits physical, 57 bits virtual\nCPU(s):                          44\nOn-line CPU(s) list:             0-43\nThread(s) per core:              2\nCore(s) per socket:              22\nSocket(s):                       1\nNUMA node(s):                    1\nVendor ID:                       AuthenticAMD\nCPU family:                      25\nModel:                           17\nModel name:                      AMD EPYC 9B14\nStepping:                        1\nCPU MHz:                         2599.996\nBogoMIPS:                        5199.99\nHypervisor vendor:               KVM\nVirtualization type:             full\nL1d cache:                       704 KiB\nL1i cache:                       704 KiB\nL2 cache:                        22 MiB\nL3 cache:                        64 MiB\nNUMA node0 CPU(s):               0-43\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Not affected\nVulnerability Retbleed:          Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr wbnoinvd arat avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm\n\nVersions of relevant libraries:\n[pip3] numpy==2.2.2\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.8.0\n[pip3] torch-xla==2.8.0+gitac9a39f\n[pip3] transformers==4.51.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3rc2.dev57+g8e5314a4\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nVLLM_TARGET_DEVICE=tpu\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\n\nI am trying to serve Qwen2.5-1.5B-Instruct model on v6e TPU using docker (as mentioned in TPU installation documentation), but I am getting the following error:\n```\nERROR 03-24 14:29:42 [engine.py:448] RuntimeError: Bad StatusOr access: INTERNAL: Failed to get global TPU topology.\n```\nHowever, the model is getting deployed in v5e TPU by following the same process mentioned in the documentation.\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-08T07:49:20+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16243/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16243"
  },
  {
    "number": 16319,
    "title": "[Usage]: how to redirect save logs to local file.",
    "body": "### Your current environment\n\ni am using docker 0.8.2 to run the model, output of collect_env.py\n```text\nThe output of `Collecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.31.6\nLibc version: glibc-2.35\n\nPython version: 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-78-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: Tesla T4\nNvidia driver version: 550.54.15\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   46 bits physical, 48 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          32\nOn-line CPU(s) list:             0-31\nVendor ID:                       GenuineIntel\nModel name:                      Intel(R) Xeon(R) Silver 4215 CPU @ 2.50GHz\nCPU family:                      6\nModel:                           85\nThread(s) per core:              2\nCore(s) per socket:              8\nSocket(s):                       2\nStepping:                        7\nCPU max MHz:                     3500.0000\nCPU min MHz:                     1000.0000\nBogoMIPS:                        5000.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear flush_l1d arch_capabilities\nVirtualization:                  VT-x\nL1d cache:                       512 KiB (16 instances)\nL1i cache:                       512 KiB (16 instances)\nL2 cache:                        16 MiB (16 instances)\nL3 cache:                        22 MiB (2 instances)\nNUMA node(s):                    2\nNUMA node0 CPU(s):               0-7,16-23\nNUMA node1 CPU(s):               8-15,24-31\nVulnerability Itlb multihit:     KVM: Mitigation: VMX disabled\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:          Mitigation; Enhanced IBRS\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Mitigation; TSX disabled\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.1.post2+cu124torch2.6\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] triton==3.2.0\n[conda] Could not collect`\n```\n\n\n### How would you like to use vllm\n\ni follow the guide to save all logs to local file by this [https://docs.vllm.ai/en/latest/getting_started/examples/logging_configuration.html](url)\nthis is my docker command\n```docker\ndocker run -d --restart always --gpus all     --name vllm-openai     -v /data/share/models:/data/models     -p 8000:8000     --ipc=host  -e VLLM_LOGGING_CONFIG_PATH=/data/models/logging_config.json   -e VLLM_LOGGING_LEVEL=DEBUG  vllm/vllm-openai:latest      --model /data/models/DeepSeek-R1-Distill-Qwen-7B  \n```\n\nthis is my logging.json \n\n```json\n{\n\t\"version\": 1,\n\t\"disable_existing_loggers\": false,\n\t\"formatters\": {\n\t\t\"simpleFormatter\": {\n\t\t\t\"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n\t\t}\n\t},\n\t\"handlers\": {\n\t\t\"fileHandler\": {\n\t\t\t\"class\": \"logging.FileHandler\",\n\t\t\t\"level\": \"INFO\",\n\t\t\t\"formatter\": \"simpleFormatter\",\n\t\t\t\"filename\": \"/data/models/logs/vllm.log\",\n\t\t\t\"encoding\": \"utf-8\"\n\t\t}\n\t},\n\t\"loggers\": {\n\t\t\"vllm\": {\n            \"handlers\": [\"fileHandler\"],\n            \"level\": \"DEBUG\",\n            \"propagate\": false\n        }\n\t},\n\t\"root\": {\n\t\t\"handlers\": [\"fileHandler\"],\n\t\t\"level\": \"WARNING\"\n\t}\n}\n```\nthese logs will fine to save to local file\n```text\n2025-04-08 23:05:03,917 - vllm.entrypoints.launcher - INFO - Route: /pooling, Methods: POST\n2025-04-08 23:05:03,917 - vllm.entrypoints.launcher - INFO - Route: /score, Methods: POST\n2025-04-08 23:05:03,917 - vllm.entrypoints.launcher - INFO - Route: /v1/score, Methods: POST\n2025-04-08 23:05:03,917 - vllm.entrypoints.launcher - INFO - Route: /v1/audio/transcriptions, Methods: POST\n2025-04-08 23:05:03,917 - vllm.entrypoints.launcher - INFO - Route: /rerank, Methods: POST\n2025-04-08 23:05:03,917 - vllm.entrypoints.launcher - INFO - Route: /v1/rerank, Methods: POST\n2025-04-08 23:05:03,917 - vllm.entrypoints.launcher - INFO - Route: /v2/rerank, Methods: POST\n2025-04-08 23:05:03,917 - vllm.entrypoints.launcher - INFO - Route: /invocations, Methods: POST\n2025-04-08 23:05:31,964 - vllm.entrypoints.chat_utils - INFO - Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n2025-04-08 23:05:31,970 - vllm.entrypoints.logger - INFO - Received request chatcmpl-f7f250513305480fbe788a6deb4dae8b: prompt: '<\uff5cbegin\u2581of\u2581sentence\uff5c><\uff5cUser\uff5c>\u4f60\u662f\u8c01?<\uff5cAssistant\uff5c><think>\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n2025-04-08 23:05:31,971 - vllm.engine.multiprocessing.engine - INFO - Added request chatcmpl-f7f250513305480fbe788a6deb4dae8b.\n2025-04-08 23:05:32,243 - vllm.engine.metrics - INFO - Avg prompt throughput: 1.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n2025-04-08 23:05:37,448 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.\n2025-04-08 23:05:42,646 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.\n2025-04-08 23:05:47,844 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.\n2025-04-08 23:05:58,536 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n2025-04-08 23:06:08,547 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n```\n\nbut these log information still print at console, i dont want to save these infomation by > to local file, how can i use the same configuration file to save these logs to local also.\n```test\n\n[W408 23:27:45.631650662 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W408 23:28:05.649792944 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.67s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.81s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.79s/it]\n\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     100.64.10.54:47584 - \"GET /v1/models HTTP/1.1\" 200 OK\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-09T06:31:49+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16319/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16319"
  },
  {
    "number": 13322,
    "title": "[Usage]: How can I use temperature correctly for Qwen2-VL?",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\n```\n\n\n### How would you like to use vllm\n\nI want to run inference of Qwen2-VL-2B. The code is:\n```\n# Qwen2-VL\ndef init_qwen2_vl(model_name_or_path: str, **kwargs):\n    from vllm import LLM\n    try:\n        from qwen_vl_utils import process_vision_info\n    except ModuleNotFoundError:\n        print('WARNING: `qwen-vl-utils` not installed, input images will not '\n              'be automatically resized. You can enable this functionality by '\n              '`pip install qwen-vl-utils`.')\n        process_vision_info = None\n\n    model_name = model_name_or_path\n\n    llm = LLM(\n        model=model_name,\n        device=kwargs['device'], \n        max_model_len=kwargs.get(\"max_context_len\", 4096 if process_vision_info is not None else 32768),  \n        enable_prefix_caching=True,\n        enforce_eager=True,\n        disable_mm_preprocessor_cache=kwargs.get(\"disable_mm_preprocessor_cache\", True),\n    )\n    stop_token_ids = None\n    return llm, stop_token_ids, process_vision_info\n\n```\nThe generation code is:\n```\nmessages={\n    \"role\": \"user\", \n    \"content\": [{\"type\": \"image\", \"image\": img} for img in images ] +[\n        {\"type\": \"text\", \"text\": text}\n    ]\n}\ntext_prompt = self.processor.apply_chat_template(state['state'], add_generation_prompt=True, tokenizer=False)\nif self.process_vision_info is not None:\n    image_inputs, _ = self.process_vision_info(messages)\nelse:\n    image_inputs = state[\"images\"]\n\nsampling_params = SamplingParams(\n    max_tokens=inputs.get(\"max_new_tokens\", 512),\n    # Qwen2-VL default\n    temperature=inputs.get(\"temperature\",0.01),\n    top_k= 1,\n    top_p= inputs.get(\"top_p\",0.7),\n    stop_token_ids=self.stop_token_ids\n)\n\noutput_text = self.model.generate(\n    {\n        \"prompt\": text_prompt, \n        \"multi_modal_data\": {\n            \"image\": image_inputs\n        }, \n    },\n    sampling_params=sampling_params,\n    use_tqdm=False, \n)[0].outputs[0].text\n```\nBut when I use different temperature between 0.1 to 0.9. The output is totally the same. I wonder if I use vllm in a right way to generate sequences with Qwen2-VL.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-02-15T07:49:44+00:00",
    "closed_at": "2025-02-17T09:21:11+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13322/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13322"
  },
  {
    "number": 8762,
    "title": "[Usage]: how to acquire logits in vllm",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### How would you like to use vllm\n\nI want to acquire logits when I run benchmark_throughput.py to do the softmax optimization, but the output in vllm doesn't have logits, how can I acquire it.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-24T06:42:51+00:00",
    "closed_at": "2025-01-24T01:58:47+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8762/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8762"
  },
  {
    "number": 12765,
    "title": "[Usage]: \u8bf7\u95ee\u5982\u4f55\u7528vllm\u8fdb\u884c\u591a\u673a\u90e8\u7f72\uff1f",
    "body": "### Your current environment\n\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-130-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.3.107\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA GeForce RTX 4090\nGPU 1: NVIDIA GeForce RTX 4090\nGPU 2: NVIDIA GeForce RTX 4090\nGPU 3: NVIDIA GeForce RTX 4090\nGPU 4: NVIDIA GeForce RTX 4090\nGPU 5: NVIDIA GeForce RTX 4090\nGPU 6: NVIDIA GeForce RTX 4090\nGPU 7: NVIDIA GeForce RTX 4090\n\nNvidia driver version: 550.142\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.4.0\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn.so.8\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn.so.8\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn_adv_train.so.8\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn_ops_train.so.8\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn.so.8\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_adv_train.so.8\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_ops_train.so.8\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               128\nOn-line CPU(s) list:                  0-127\nVendor ID:                            GenuineIntel\nModel name:                           INTEL(R) XEON(R) PLATINUM 8562Y+\nCPU family:                           6\nModel:                                207\nThread(s) per core:                   2\nCore(s) per socket:                   32\nSocket(s):                            2\nStepping:                             2\nCPU max MHz:                          4100.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             5600.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            3 MiB (64 instances)\nL1i cache:                            2 MiB (64 instances)\nL2 cache:                             128 MiB (64 instances)\nL3 cache:                             120 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-31,64-95\nNUMA node1 CPU(s):                    32-63,96-127\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] json2onnx==2.0.3\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] onnx==1.15.0\n[pip3] onnx-graphsurgeon==0.5.2\n[pip3] onnx-simplifier==0.4.33\n[pip3] onnx-tf==1.9.0\n[pip3] onnx2json==2.0.4\n[pip3] onnx2tf==1.15.4\n[pip3] onnxruntime-gpu==1.19.0\n[pip3] optree==0.13.1\n[pip3] pyzmq==26.2.1\n[pip3] sam4onnx==1.0.16\n[pip3] sbi4onnx==1.0.7\n[pip3] scc4onnx==1.0.6\n[pip3] scs4onnx==1.0.18\n[pip3] sde4onnx==1.0.0\n[pip3] sed4onnx==1.0.5\n[pip3] sentence-transformers==3.3.1\n[pip3] simple_onnx_processing_tools==1.1.32\n[pip3] sio4onnx==1.0.2\n[pip3] sit4onnx==1.0.8\n[pip3] sna4onnx==1.0.6\n[pip3] snc4onnx==1.0.13\n[pip3] snd4onnx==1.1.6\n[pip3] sne4onnx==1.0.13\n[pip3] sng4onnx==1.0.4\n[pip3] soa4onnx==1.0.4\n[pip3] soc4onnx==1.0.2\n[pip3] sod4onnx==1.0.0\n[pip3] sog4onnx==1.0.17\n[pip3] sor4onnx==1.0.7\n[pip3] spo4onnx==1.0.5\n[pip3] ssc4onnx==1.0.8\n[pip3] ssi4onnx==1.0.4\n[pip3] svs4onnx==1.0.0\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.2\n[pip3] triton==3.1.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] optree                    0.13.1                   pypi_0    pypi\n[conda] pyzmq                     26.2.1                   pypi_0    pypi\n[conda] sentence-transformers     3.3.1                    pypi_0    pypi\n[conda] torch                     2.5.1                    pypi_0    pypi\n[conda] torchaudio                2.5.1                    pypi_0    pypi\n[conda] torchvision               0.20.1                   pypi_0    pypi\n[conda] transformers              4.48.2                   pypi_0    pypi\n[conda] triton                    3.1.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU1\tPIX\t X \tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU2\tPIX\tPIX\t X \tPIX\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU3\tPIX\tPIX\tPIX\t X \tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU4\tSYS\tSYS\tSYS\tSYS\t X \tPIX\tPIX\tPIX\t32-63,96-127\t1\t\tN/A\nGPU5\tSYS\tSYS\tSYS\tSYS\tPIX\t X \tPIX\tPIX\t32-63,96-127\t1\t\tN/A\nGPU6\tSYS\tSYS\tSYS\tSYS\tPIX\tPIX\t X \tPIX\t32-63,96-127\t1\t\tN/A\nGPU7\tSYS\tSYS\tSYS\tSYS\tPIX\tPIX\tPIX\t X \t32-63,96-127\t1\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64\nCUDA_HOME=/usr/local/cuda\nCUDA_HOME=/usr/local/cuda\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n\n### How would you like to use vllm\n\n**1\uff1a**\u6211\u6709\u4e24\u53f04090\u663e\u5361\u7684\u673a\u5668\uff0c\u60f3\u8fdb\u884c\u591a\u673a\u90e8\u7f72\u5927\u6a21\u578b\u3002\u60f3\u95ee\u4e0b\uff0c\u6211\u8be5\u600e\u4e48\u7528`VLMM`\u8fdb\u884c\u591a\u673a\u90e8\u7f72\uff1f\u6709\u6ca1\u6709\u6d41\u7a0b\u6837\u4f8b\u4e4b\u7c7b\u7684\u4fe1\u606f\u53c2\u8003\u4e00\u4e0b\u3002\n**2\uff1a**\u8bf7\u95ee\u4e0b\u6709\u6ca1\u6709\u5173\u4e8e`VLLM`\u7684\u6c9f\u901a\u7fa4\u554a\uff0c\u7fa4\u6c9f\u901a\u66f4\u65b9\u4fbf\u4e00\u4e9b~\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-02-05T03:24:26+00:00",
    "closed_at": "2025-02-05T03:25:00+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12765/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12765"
  },
  {
    "number": 10953,
    "title": "[Usage]: How to use a Python script to start a FastAPI service for internvl2-8b with vllm, instead of using the terminal command vllm serve ./internvl2-1b/ --tensor-parallel-size 1 --trust-remote-code? Is there any sample code for this?",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-12-06T14:35:00+00:00",
    "closed_at": "2024-12-06T15:30:54+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10953/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10953"
  },
  {
    "number": 17272,
    "title": "[Usage]: FP8 online quantization weight synchronization",
    "body": "### Your current environment\n\nIf we run a vllm instance with ```quantization='fp8'```, how should we update its weight ?\n\nI encountered an issue: \n```\nassert param.size() == loaded_weight.size(), (\n[rank10]: AssertionError: Attempted to load weight (torch.Size([3840, 1280])) into parameter (torch.Size([1280, 3840]))\n```\nthis is due to the training models' type is bfloat16.  Can we only enable FP8 training if we want to use FP8 inference? \n\n\n\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "open",
    "created_at": "2025-04-27T22:32:04+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17272/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17272"
  },
  {
    "number": 7404,
    "title": "[Usage]: KV Cache Warning for `gemma2`",
    "body": "### Your current environment\r\n\r\nI get the following warning running a quantized version of `gemma2`, when I have not quantized the kv cache:\r\n\r\n```bash\r\nWARNING 08-11 22:31:50 gemma2.py:399] Some weights are not initialized from checkpoints: {'model.layers.2.self_attn.attn.v_scale', 'model.layers.13.self_attn.attn.v_scale', 'model.layers.14.self_attn.attn.v_scale', 'model.layers.16.self_attn.attn.k_scale', 'model.layers.16.self_attn.attn.v_scale', 'model.layers.19.self_attn.attn.v_scale', 'model.layers.21.self_attn.attn.v_scale', 'model.layers.2.self_attn.attn.k_scale', 'model.layers.3.self_attn.attn.k_scale', 'model.layers.20.self_attn.attn.v_scale', 'model.layers.24.self_attn.attn.k_scale', 'model.layers.6.self_attn.attn.k_scale', 'model.layers.6.self_attn.attn.v_scale', 'model.layers.12.self_attn.attn.k_scale', 'model.layers.15.self_attn.attn.v_scale', 'model.layers.18.self_attn.attn.k_scale', 'model.layers.14.self_attn.attn.k_scale', 'model.layers.9.self_attn.attn.k_scale', 'model.layers.11.self_attn.attn.v_scale', 'model.layers.11.self_attn.attn.k_scale', 'model.layers.24.self_attn.attn.v_scale', 'model.layers.7.self_attn.attn.v_scale', 'model.layers.1.self_attn.attn.v_scale', 'model.layers.20.self_attn.attn.k_scale', 'model.layers.12.self_attn.attn.v_scale', 'model.layers.13.self_attn.attn.k_scale', 'model.layers.5.self_attn.attn.v_scale', 'model.layers.21.self_attn.attn.k_scale', 'model.layers.5.self_attn.attn.k_scale', 'model.layers.19.self_attn.attn.k_scale', 'model.layers.8.self_attn.attn.k_scale', 'model.layers.3.self_attn.attn.v_scale', 'model.layers.18.self_attn.attn.v_scale', 'model.layers.4.self_attn.attn.v_scale', 'model.layers.15.self_attn.attn.k_scale', 'model.layers.0.self_attn.attn.v_scale', 'model.layers.1.self_attn.attn.k_scale', 'model.layers.25.self_attn.attn.k_scale', 'model.layers.17.self_attn.attn.k_scale', 'model.layers.4.self_attn.attn.k_scale', 'model.layers.10.self_attn.attn.v_scale', 'model.layers.17.self_attn.attn.v_scale', 'model.layers.23.self_attn.attn.v_scale', 'model.layers.9.self_attn.attn.v_scale', 'model.layers.7.self_attn.attn.k_scale', 'model.layers.8.self_attn.attn.v_scale', 'model.layers.0.self_attn.attn.k_scale', 'model.layers.22.self_attn.attn.v_scale', 'model.layers.10.self_attn.attn.k_scale', 'model.layers.25.self_attn.attn.v_scale', 'model.layers.22.self_attn.attn.k_scale', 'model.layers.23.self_attn.attn.k_scale'}\r\n```\r\n\r\n### How would you like to use vllm\r\n\r\nI think this warning is confusing since the model does not quantize the kv cache\r\n",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-11T22:33:24+00:00",
    "closed_at": "2024-12-10T02:07:51+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7404/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7404"
  },
  {
    "number": 17620,
    "title": "[Usage]: When using with Peft-loaded model, got error: PreTrainedTokenizerFast has no attribute lower",
    "body": "### Your current environment\n\n```text\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-113-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA RTX 6000 Ada Generation\nGPU 1: NVIDIA RTX 6000 Ada Generation\nGPU 2: NVIDIA RTX 6000 Ada Generation\nGPU 3: NVIDIA RTX 6000 Ada Generation\n\nNvidia driver version: 560.28.03\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      52 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             40\nOn-line CPU(s) list:                0-39\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) w7-3445\nCPU family:                         6\nModel:                              143\nThread(s) per core:                 2\nCore(s) per socket:                 20\nSocket(s):                          1\nStepping:                           8\nCPU max MHz:                        4800.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5184.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                     VT-x\nL1d cache:                          960 KiB (20 instances)\nL1i cache:                          640 KiB (20 instances)\nL2 cache:                           40 MiB (20 instances)\nL3 cache:                           52.5 MiB (1 instance)\nNUMA node(s):                       1\nNUMA node0 CPU(s):                  0-39\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.0\n[pip3] sentence-transformers==3.0.1\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchdata==0.9.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.3\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.4\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NODE    NODE    NODE    0-39    0               N/A\nGPU1    NODE     X      NODE    NODE    0-39    0               N/A\nGPU2    NODE    NODE     X      NODE    0-39    0               N/A\nGPU3    NODE    NODE    NODE     X      0-39    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nCUDA_DEVICE_ORDER=PCI_BUS_ID\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n\n### How would you like to use vllm\n\nI got a strange error from tokenzier when trying to use vllm to inference with my Peft-finetuned model for classification. \n\nI first merged the adapter with the base and saved to a local directory `my_fused_model`\n\n```\nfrom peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nADAPTER_ID = <MY_PRIVATE_MODEL_ON_HF>\nFOUNDATION_ID = \"meta-llama/Llama-3.2-3B\"\n\nbase_model = AutoModelForSequenceClassification.from_pretrained(FOUNDATION_ID)\nmodel = PeftModel.from_pretrained(base_model, ADAPTER_ID)\n\nmodel = model.merge_and_unload()\n\nmodel.save_pretrained(\"my_fused_model_dir\")\n```\n\nThen I created an vLLM instance and tried to run it for classification task\n\n```\nFOUNDATION_ID = \"meta-llama/Llama-3.2-3B\"\n\ntokenizer = AutoTokenizer.from_pretrained(FOUNDATION_ID, use_fast=False)\n\nllm = LLM(model=\"./my_fused_model_dir\", task=\"classify\", tokenizer=tokenizer)\n\nresults = llm.classify(\"I am a happy prompt.\")\n```\n\nHowever, I got this error below: \n\n```\n  File \"/xyz/hhem_vllm.py\", line 18, in <module>\n    llm = LLM(model=\"./my_fused_model_dir\", task=\"classify\", tokenizer=tokenizer)\n  File \"/home/forrest/.local/lib/python3.10/site-packages/vllm/utils.py\", line 1099, in inner\n    return fn(*args, **kwargs)\n  File \"/home/forrest/.local/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 248, in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n  File \"/home/forrest/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 515, in from_engine_args\n    vllm_config = engine_args.create_engine_config(usage_context)\n  File \"/home/forrest/.local/lib/python3.10/site-packages/vllm/engine/arg_utils.py\", line 1154, in create_engine_config\n    model_config = self.create_model_config()\n  File \"/home/forrest/.local/lib/python3.10/site-packages/vllm/engine/arg_utils.py\", line 1042, in create_model_config\n    return ModelConfig(\n  File \"/home/forrest/.local/lib/python3.10/site-packages/vllm/config.py\", line 394, in __init__\n    self.maybe_pull_model_tokenizer_for_s3(model, tokenizer)\n  File \"/home/forrest/.local/lib/python3.10/site-packages/vllm/config.py\", line 541, in maybe_pull_model_tokenizer_for_s3\n    if is_s3(model) or is_s3(tokenizer):\n  File \"/home/forrest/.local/lib/python3.10/site-packages/vllm/transformers_utils/utils.py\", line 16, in is_s3\n    return model_or_path.lower().startswith('s3://')\n  File \"/home/forrest/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 1108, in __getattr__\n    raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\nAttributeError: PreTrainedTokenizerFast has no attribute lower\n```\n\nCan someone help me? \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-05-04T00:48:02+00:00",
    "closed_at": "2025-05-06T05:18:48+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17620/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17620"
  },
  {
    "number": 18580,
    "title": "[Usage]:RuntimeError: Triton Error [CUDA]: device kernel image is invalid",
    "body": "### Your current environment\n\n```text\n(vllm) llm@aitt:/data_a/llm$ python collect_env.py \nINFO 05-23 10:07:58 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 20.04.4 LTS (x86_64)\nGCC version                  : (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version                : Could not collect\nCMake version                : version 3.16.3\nLibc version                 : glibc-2.31\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.6.0+cu118\nIs debug build               : False\nCUDA used to build PyTorch   : 11.8\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0] (64-bit runtime)\nPython platform              : Linux-5.4.0-214-generic-x86_64-with-glibc2.31\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 11.8.89\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : \nGPU 0: NVIDIA GeForce RTX 3090\nGPU 1: NVIDIA GeForce RTX 3090\nGPU 2: NVIDIA GeForce RTX 3090\n\nNvidia driver version        : 520.61.05\ncuDNN version                : Probably one of the following:\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn.so.8\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_adv_train.so.8\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_ops_train.so.8\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn.so.8\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_adv_train.so.8\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_ops_train.so.8\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      46 bits physical, 57 bits virtual\nCPU(s):                             48\nOn-line CPU(s) list:                0-47\nThread(s) per core:                 2\nCore(s) per socket:                 12\nSocket(s):                          2\nNUMA node(s):                       2\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              106\nModel name:                         Intel(R) Xeon(R) Gold 5317 CPU @ 3.00GHz\nStepping:                           6\nCPU MHz:                            3000.012\nBogoMIPS:                           6000.00\nVirtualization:                     VT-x\nL1d cache:                          1.1 MiB\nL1i cache:                          768 KiB\nL2 cache:                           30 MiB\nL3 cache:                           36 MiB\nNUMA node0 CPU(s):                  0-11,24-35\nNUMA node1 CPU(s):                  12-23,36-47\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig flush_l1d arch_capabilities\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu11==11.11.3.6\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu11==11.8.87\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu11==11.8.89\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu11==11.8.89\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu11==9.1.0.70\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu11==10.9.0.58\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu11==10.3.0.86\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu11==11.4.1.48\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu11==11.7.5.86\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu11==2.21.5\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu11==11.8.86\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.6.0+cu118\n[pip3] torchaudio==2.6.0+cu118\n[pip3] torchvision==0.21.0+cu118\n[pip3] transformers==4.52.3\n[pip3] triton==3.2.0\n[conda] numpy                     2.2.6                    pypi_0    pypi\n[conda] nvidia-cublas-cu11        11.11.3.6                pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu11    11.8.87                  pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu11    11.8.89                  pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu11  11.8.89                  pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu11         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu11         10.9.0.58                pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu11        10.3.0.86                pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu11      11.4.1.48                pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu11      11.7.5.86                pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-nccl-cu11          2.21.5                   pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu11          11.8.86                  pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.4.0                   pypi_0    pypi\n[conda] torch                     2.6.0+cu118              pypi_0    pypi\n[conda] torchaudio                2.6.0+cu118              pypi_0    pypi\n[conda] torchvision               0.21.0+cu118             pypi_0    pypi\n[conda] transformers              4.52.3                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.8.5.post1\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n  \tGPU0\tGPU1\tGPU2\tCPU Affinity\tNUMA Affinity\nGPU0\t X \tNODE\tSYS\t0-11,24-35\t0\nGPU1\tNODE\t X \tSYS\t0-11,24-35\t0\nGPU2\tSYS\tSYS\t X \t12-23,36-47\t1\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n==============================\n     Environment Variables\n==============================\nCUDA_HOME=:/usr/local/cuda-11.0:/usr/local/cuda-11.8\nCUDA_HOME=:/usr/local/cuda-11.0:/usr/local/cuda-11.8\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n\n\n### How would you like to use vllm\n\nAn error occurred while running the model, and the error message is as follows: \n(vllm) llm@aitt:/data_a$ VLLM_USE_MODELSCOPE=true vllm serve /data_a/llm/Qwen3-32B-AWQ\nINFO 05-23 09:30:26 [__init__.py:239] Automatically detected platform cuda.\nINFO 05-23 09:30:29 [api_server.py:1043] vLLM API server version 0.8.5.post1\nINFO 05-23 09:30:29 [api_server.py:1044] args: Namespace(subparser='serve', model_tag='/data_a/llm/Qwen3-32B-AWQ', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/data_a/llm/Qwen3-32B-AWQ', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, load_format='auto', download_dir=None, model_loader_extra_config={}, use_tqdm_on_load=True, config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', max_model_len=None, guided_decoding_backend='auto', reasoning_parser=None, logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, block_size=None, gpu_memory_utilization=0.9, swap_space=4, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, disable_sliding_window=False, use_v2_block_manager=True, seed=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=None, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, ignore_patterns=[], served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, max_num_batched_tokens=None, max_num_seqs=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, additional_config=None, enable_reasoning=False, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x7f65d162dbc0>)\nINFO 05-23 09:30:36 [config.py:717] This model supports multiple tasks: {'generate', 'reward', 'embed', 'classify', 'score'}. Defaulting to 'generate'.\nINFO 05-23 09:30:36 [awq_marlin.py:113] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\nINFO 05-23 09:30:36 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.\nINFO 05-23 09:30:40 [__init__.py:239] Automatically detected platform cuda.\nINFO 05-23 09:30:43 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/data_a/llm/Qwen3-32B-AWQ', speculative_config=None, tokenizer='/data_a/llm/Qwen3-32B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data_a/llm/Qwen3-32B-AWQ, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\nWARNING 05-23 09:30:43 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f57665fe000>\nINFO 05-23 09:30:43 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 05-23 09:30:43 [cuda.py:221] Using Flash Attention backend on V1 engine.\nWARNING 05-23 09:30:43 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\nINFO 05-23 09:30:43 [gpu_model_runner.py:1329] Starting to load model /data_a/llm/Qwen3-32B-AWQ...\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:09<00:28,  9.54s/it]\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:19<00:18,  9.50s/it]\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:28<00:09,  9.49s/it]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:38<00:00,  9.66s/it]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:38<00:00,  9.60s/it]\n\nINFO 05-23 09:31:22 [loader.py:458] Loading weights took 38.52 seconds\nINFO 05-23 09:31:24 [gpu_model_runner.py:1347] Model loading took 18.1450 GiB and 40.766484 seconds\nINFO 05-23 09:31:43 [backends.py:420] Using cache directory: /home/llm/.cache/vllm/torch_compile_cache/1dbb3e77cd/rank_0_0 for vLLM's torch.compile\nINFO 05-23 09:31:43 [backends.py:430] Dynamo bytecode transform time: 19.00 s\nERROR 05-23 09:31:47 [core.py:396] EngineCore failed to start.\nERROR 05-23 09:31:47 [core.py:396] Traceback (most recent call last):\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\nERROR 05-23 09:31:47 [core.py:396]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 05-23 09:31:47 [core.py:396]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\nERROR 05-23 09:31:47 [core.py:396]     super().__init__(vllm_config, executor_class, log_stats,\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 71, in __init__\nERROR 05-23 09:31:47 [core.py:396]     self._initialize_kv_caches(vllm_config)\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 129, in _initialize_kv_caches\nERROR 05-23 09:31:47 [core.py:396]     available_gpu_memory = self.model_executor.determine_available_memory()\nERROR 05-23 09:31:47 [core.py:396]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 75, in determine_available_memory\nERROR 05-23 09:31:47 [core.py:396]     output = self.collective_rpc(\"determine_available_memory\")\nERROR 05-23 09:31:47 [core.py:396]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\nERROR 05-23 09:31:47 [core.py:396]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 05-23 09:31:47 [core.py:396]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/utils.py\", line 2456, in run_method\nERROR 05-23 09:31:47 [core.py:396]     return func(*args, **kwargs)\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 05-23 09:31:47 [core.py:396]     return func(*args, **kwargs)\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 183, in determine_available_memory\nERROR 05-23 09:31:47 [core.py:396]     self.model_runner.profile_run()\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1651, in profile_run\nERROR 05-23 09:31:47 [core.py:396]     hidden_states = self._dummy_run(self.max_num_tokens)\nERROR 05-23 09:31:47 [core.py:396]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 05-23 09:31:47 [core.py:396]     return func(*args, **kwargs)\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1497, in _dummy_run\nERROR 05-23 09:31:47 [core.py:396]     outputs = model(\nERROR 05-23 09:31:47 [core.py:396]               ^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\nERROR 05-23 09:31:47 [core.py:396]     return self._call_impl(*args, **kwargs)\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\nERROR 05-23 09:31:47 [core.py:396]     return forward_call(*args, **kwargs)\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py\", line 299, in forward\nERROR 05-23 09:31:47 [core.py:396]     hidden_states = self.model(input_ids, positions, intermediate_tensors,\nERROR 05-23 09:31:47 [core.py:396]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 238, in __call__\nERROR 05-23 09:31:47 [core.py:396]     output = self.compiled_callable(*args, **kwargs)\nERROR 05-23 09:31:47 [core.py:396]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 574, in _fn\nERROR 05-23 09:31:47 [core.py:396]     return fn(*args, **kwargs)\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1380, in __call__\nERROR 05-23 09:31:47 [core.py:396]     return self._torchdynamo_orig_callable(\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\nERROR 05-23 09:31:47 [core.py:396]     return _compile(\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\nERROR 05-23 09:31:47 [core.py:396]     guarded_code = compile_inner(code, one_graph, hooks, transform)\nERROR 05-23 09:31:47 [core.py:396]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\nERROR 05-23 09:31:47 [core.py:396]     return _compile_inner(code, one_graph, hooks, transform)\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_utils_internal.py\", line 95, in wrapper_function\nERROR 05-23 09:31:47 [core.py:396]     return function(*args, **kwargs)\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\nERROR 05-23 09:31:47 [core.py:396]     out_code = transform_code_object(code, transform)\nERROR 05-23 09:31:47 [core.py:396]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\nERROR 05-23 09:31:47 [core.py:396]     transformations(instructions, code_options)\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 231, in _fn\nERROR 05-23 09:31:47 [core.py:396]     return fn(*args, **kwargs)\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 662, in transform\nERROR 05-23 09:31:47 [core.py:396]     tracer.run()\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 2868, in run\nERROR 05-23 09:31:47 [core.py:396]     super().run()\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\nERROR 05-23 09:31:47 [core.py:396]     while self.step():\nERROR 05-23 09:31:47 [core.py:396]           ^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\nERROR 05-23 09:31:47 [core.py:396]     self.dispatch_table[inst.opcode](self, inst)\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 3048, in RETURN_VALUE\nERROR 05-23 09:31:47 [core.py:396]     self._return(inst)\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 3033, in _return\nERROR 05-23 09:31:47 [core.py:396]     self.output.compile_subgraph(\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1101, in compile_subgraph\nERROR 05-23 09:31:47 [core.py:396]     self.compile_and_call_fx_graph(\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1382, in compile_and_call_fx_graph\nERROR 05-23 09:31:47 [core.py:396]     compiled_fn = self.call_user_compiler(gm)\nERROR 05-23 09:31:47 [core.py:396]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1432, in call_user_compiler\nERROR 05-23 09:31:47 [core.py:396]     return self._call_user_compiler(gm)\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1483, in _call_user_compiler\nERROR 05-23 09:31:47 [core.py:396]     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1462, in _call_user_compiler\nERROR 05-23 09:31:47 [core.py:396]     compiled_fn = compiler_fn(gm, self.example_inputs())\nERROR 05-23 09:31:47 [core.py:396]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 130, in __call__\nERROR 05-23 09:31:47 [core.py:396]     compiled_gm = compiler_fn(gm, example_inputs)\nERROR 05-23 09:31:47 [core.py:396]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 130, in __call__\nERROR 05-23 09:31:47 [core.py:396]     compiled_gm = compiler_fn(gm, example_inputs)\nERROR 05-23 09:31:47 [core.py:396]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/__init__.py\", line 2385, in __call__\nERROR 05-23 09:31:47 [core.py:396]     return self.compiler_fn(model_, inputs_, **self.kwargs)\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/compilation/backends.py\", line 459, in __call__\nERROR 05-23 09:31:47 [core.py:396]     PiecewiseCompileInterpreter(self.split_gm, submod_names_to_compile,\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/compilation/backends.py\", line 249, in run\nERROR 05-23 09:31:47 [core.py:396]     return super().run(*fake_args)\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/fx/interpreter.py\", line 167, in run\nERROR 05-23 09:31:47 [core.py:396]     self.env[node] = self.run_node(node)\nERROR 05-23 09:31:47 [core.py:396]                      ^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/fx/interpreter.py\", line 230, in run_node\nERROR 05-23 09:31:47 [core.py:396]     return getattr(self, n.op)(n.target, args, kwargs)\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/compilation/backends.py\", line 265, in call_module\nERROR 05-23 09:31:47 [core.py:396]     compiler_manager.compile(\nERROR 05-23 09:31:47 [core.py:396]                      ^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/compilation/backends.py\", line 125, in compile\nERROR 05-23 09:31:47 [core.py:396]     compiled_graph, handle = self.compiler.compile(\nERROR 05-23 09:31:47 [core.py:396]                              ^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py\", line 318, in compile\nERROR 05-23 09:31:47 [core.py:396]     compiled_graph = compile_fx(\nERROR 05-23 09:31:47 [core.py:396]                      ^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1552, in compile_fx\nERROR 05-23 09:31:47 [core.py:396]     return compile_fx(\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1863, in compile_fx\nERROR 05-23 09:31:47 [core.py:396]     return aot_autograd(\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/backends/common.py\", line 83, in __call__\nERROR 05-23 09:31:47 [core.py:396]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\nERROR 05-23 09:31:47 [core.py:396]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 1155, in aot_module_simplified\nERROR 05-23 09:31:47 [core.py:396]     compiled_fn = dispatch_and_compile()\nERROR 05-23 09:31:47 [core.py:396]                   ^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 1131, in dispatch_and_compile\nERROR 05-23 09:31:47 [core.py:396]     compiled_fn, _ = create_aot_dispatcher_function(\nERROR 05-23 09:31:47 [core.py:396]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 580, in create_aot_dispatcher_function\nERROR 05-23 09:31:47 [core.py:396]     return _create_aot_dispatcher_function(\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 830, in _create_aot_dispatcher_function\nERROR 05-23 09:31:47 [core.py:396]     compiled_fn, fw_metadata = compiler_fn(\nERROR 05-23 09:31:47 [core.py:396]                                ^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 203, in aot_dispatch_base\nERROR 05-23 09:31:47 [core.py:396]     compiled_fw = compiler(fw_module, updated_flat_args)\nERROR 05-23 09:31:47 [core.py:396]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 489, in __call__\nERROR 05-23 09:31:47 [core.py:396]     return self.compiler_fn(gm, example_inputs)\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1741, in fw_compiler_base\nERROR 05-23 09:31:47 [core.py:396]     return inner_compile(\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/contextlib.py\", line 81, in inner\nERROR 05-23 09:31:47 [core.py:396]     return func(*args, **kwds)\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py\", line 229, in hijacked_compile_fx_inner\nERROR 05-23 09:31:47 [core.py:396]     output = torch._inductor.compile_fx.compile_fx_inner(\nERROR 05-23 09:31:47 [core.py:396]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 569, in compile_fx_inner\nERROR 05-23 09:31:47 [core.py:396]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/repro/after_aot.py\", line 102, in debug_wrapper\nERROR 05-23 09:31:47 [core.py:396]     inner_compiled_fn = compiler_fn(gm, example_inputs)\nERROR 05-23 09:31:47 [core.py:396]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 685, in _compile_fx_inner\nERROR 05-23 09:31:47 [core.py:396]     mb_compiled_graph = fx_codegen_and_compile(\nERROR 05-23 09:31:47 [core.py:396]                         ^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1129, in fx_codegen_and_compile\nERROR 05-23 09:31:47 [core.py:396]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1044, in codegen_and_compile\nERROR 05-23 09:31:47 [core.py:396]     compiled_fn = graph.compile_to_module().call\nERROR 05-23 09:31:47 [core.py:396]                   ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/graph.py\", line 2027, in compile_to_module\nERROR 05-23 09:31:47 [core.py:396]     return self._compile_to_module()\nERROR 05-23 09:31:47 [core.py:396]            ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/graph.py\", line 2068, in _compile_to_module\nERROR 05-23 09:31:47 [core.py:396]     mod = PyCodeCache.load_by_key_path(\nERROR 05-23 09:31:47 [core.py:396]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/codecache.py\", line 2759, in load_by_key_path\nERROR 05-23 09:31:47 [core.py:396]     mod = _reload_python_module(key, path)\nERROR 05-23 09:31:47 [core.py:396]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\nERROR 05-23 09:31:47 [core.py:396]     exec(code, mod.__dict__, mod.__dict__)\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/.cache/vllm/torch_compile_cache/1dbb3e77cd/rank_0_0/inductor_cache/hi/chih6ausfjvgjkr2lkfbuor7trzi74osu7u32okj3bjxvqfhh2x4.py\", line 70, in <module>\nERROR 05-23 09:31:47 [core.py:396]     triton_red_fused__to_copy_add_embedding_gptq_marlin_gemm_mean_mul_pow_rsqrt_view_0 = async_compile.triton('triton_red_fused__to_copy_add_embedding_gptq_marlin_gemm_mean_mul_pow_rsqrt_view_0', '''\nERROR 05-23 09:31:47 [core.py:396]                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/async_compile.py\", line 213, in triton\nERROR 05-23 09:31:47 [core.py:396]     kernel.precompile()\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 293, in precompile\nERROR 05-23 09:31:47 [core.py:396]     compiled_binary, launcher = self._precompile_config(\nERROR 05-23 09:31:47 [core.py:396]                                 ^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 520, in _precompile_config\nERROR 05-23 09:31:47 [core.py:396]     binary._init_handles()\nERROR 05-23 09:31:47 [core.py:396]   File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/triton/compiler/compiler.py\", line 390, in _init_handles\nERROR 05-23 09:31:47 [core.py:396]     self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(\nERROR 05-23 09:31:47 [core.py:396]                                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 05-23 09:31:47 [core.py:396] torch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f576a36a9f0>' raised:\nERROR 05-23 09:31:47 [core.py:396] RuntimeError: Triton Error [CUDA]: device kernel image is invalid\nERROR 05-23 09:31:47 [core.py:396] \nERROR 05-23 09:31:47 [core.py:396] While executing %submod_0 : [num_users=5] = call_module[target=submod_0](args = (%l_input_ids_, %s0, %l_self_modules_embed_tokens_parameters_weight_, %l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_, %l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_qweight_, %l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_scales_, %l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_qzeros_, %l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_g_idx_, %l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_g_idx_sort_indices_, %l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_workspace, %l_self_modules_layers_modules_0_modules_self_attn_modules_q_norm_parameters_weight_, %l_self_modules_layers_modules_0_modules_self_attn_modules_k_norm_parameters_weight_, %l_positions_, %l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_), kwargs = {})\nERROR 05-23 09:31:47 [core.py:396] Original traceback:\nERROR 05-23 09:31:47 [core.py:396] None\nERROR 05-23 09:31:47 [core.py:396] \nERROR 05-23 09:31:47 [core.py:396] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\nERROR 05-23 09:31:47 [core.py:396] \nERROR 05-23 09:31:47 [core.py:396] \nERROR 05-23 09:31:47 [core.py:396] You can suppress this exception and fall back to eager by setting:\nERROR 05-23 09:31:47 [core.py:396]     import torch._dynamo\nERROR 05-23 09:31:47 [core.py:396]     torch._dynamo.config.suppress_errors = True\nERROR 05-23 09:31:47 [core.py:396] \nProcess EngineCore_0:\nTraceback (most recent call last):\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 400, in run_engine_core\n    raise e\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 387, in run_engine_core\n    engine_core = EngineCoreProc(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 329, in __init__\n    super().__init__(vllm_config, executor_class, log_stats,\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 71, in __init__\n    self._initialize_kv_caches(vllm_config)\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 129, in _initialize_kv_caches\n    available_gpu_memory = self.model_executor.determine_available_memory()\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 75, in determine_available_memory\n    output = self.collective_rpc(\"determine_available_memory\")\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/utils.py\", line 2456, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 183, in determine_available_memory\n    self.model_runner.profile_run()\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1651, in profile_run\n    hidden_states = self._dummy_run(self.max_num_tokens)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1497, in _dummy_run\n    outputs = model(\n              ^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3.py\", line 299, in forward\n    hidden_states = self.model(input_ids, positions, intermediate_tensors,\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 238, in __call__\n    output = self.compiled_callable(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 574, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1380, in __call__\n    return self._torchdynamo_orig_callable(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n    return _compile(\n           ^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n    guarded_code = compile_inner(code, one_graph, hooks, transform)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n    return _compile_inner(code, one_graph, hooks, transform)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n    return function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n    out_code = transform_code_object(code, transform)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n    transformations(instructions, code_options)\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 231, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 662, in transform\n    tracer.run()\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 2868, in run\n    super().run()\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n    while self.step():\n          ^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n    self.dispatch_table[inst.opcode](self, inst)\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 3048, in RETURN_VALUE\n    self._return(inst)\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 3033, in _return\n    self.output.compile_subgraph(\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1101, in compile_subgraph\n    self.compile_and_call_fx_graph(\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1382, in compile_and_call_fx_graph\n    compiled_fn = self.call_user_compiler(gm)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1432, in call_user_compiler\n    return self._call_user_compiler(gm)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1483, in _call_user_compiler\n    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1462, in _call_user_compiler\n    compiled_fn = compiler_fn(gm, self.example_inputs())\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 130, in __call__\n    compiled_gm = compiler_fn(gm, example_inputs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 130, in __call__\n    compiled_gm = compiler_fn(gm, example_inputs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/__init__.py\", line 2385, in __call__\n    return self.compiler_fn(model_, inputs_, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/compilation/backends.py\", line 459, in __call__\n    PiecewiseCompileInterpreter(self.split_gm, submod_names_to_compile,\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/compilation/backends.py\", line 249, in run\n    return super().run(*fake_args)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/fx/interpreter.py\", line 167, in run\n    self.env[node] = self.run_node(node)\n                     ^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/fx/interpreter.py\", line 230, in run_node\n    return getattr(self, n.op)(n.target, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/compilation/backends.py\", line 265, in call_module\n    compiler_manager.compile(\n                     ^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/compilation/backends.py\", line 125, in compile\n    compiled_graph, handle = self.compiler.compile(\n                             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py\", line 318, in compile\n    compiled_graph = compile_fx(\n                     ^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1552, in compile_fx\n    return compile_fx(\n           ^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1863, in compile_fx\n    return aot_autograd(\n           ^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/backends/common.py\", line 83, in __call__\n    cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 1155, in aot_module_simplified\n    compiled_fn = dispatch_and_compile()\n                  ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 1131, in dispatch_and_compile\n    compiled_fn, _ = create_aot_dispatcher_function(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 580, in create_aot_dispatcher_function\n    return _create_aot_dispatcher_function(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 830, in _create_aot_dispatcher_function\n    compiled_fn, fw_metadata = compiler_fn(\n                               ^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 203, in aot_dispatch_base\n    compiled_fw = compiler(fw_module, updated_flat_args)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 489, in __call__\n    return self.compiler_fn(gm, example_inputs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1741, in fw_compiler_base\n    return inner_compile(\n           ^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/contextlib.py\", line 81, in inner\n    return func(*args, **kwds)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/compilation/compiler_interface.py\", line 229, in hijacked_compile_fx_inner\n    output = torch._inductor.compile_fx.compile_fx_inner(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 569, in compile_fx_inner\n    return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_dynamo/repro/after_aot.py\", line 102, in debug_wrapper\n    inner_compiled_fn = compiler_fn(gm, example_inputs)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 685, in _compile_fx_inner\n    mb_compiled_graph = fx_codegen_and_compile(\n                        ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1129, in fx_codegen_and_compile\n    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1044, in codegen_and_compile\n    compiled_fn = graph.compile_to_module().call\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/graph.py\", line 2027, in compile_to_module\n    return self._compile_to_module()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/graph.py\", line 2068, in _compile_to_module\n    mod = PyCodeCache.load_by_key_path(\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/codecache.py\", line 2759, in load_by_key_path\n    mod = _reload_python_module(key, path)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/runtime/compile_tasks.py\", line 45, in _reload_python_module\n    exec(code, mod.__dict__, mod.__dict__)\n  File \"/home/llm/.cache/vllm/torch_compile_cache/1dbb3e77cd/rank_0_0/inductor_cache/hi/chih6ausfjvgjkr2lkfbuor7trzi74osu7u32okj3bjxvqfhh2x4.py\", line 70, in <module>\n    triton_red_fused__to_copy_add_embedding_gptq_marlin_gemm_mean_mul_pow_rsqrt_view_0 = async_compile.triton('triton_red_fused__to_copy_add_embedding_gptq_marlin_gemm_mean_mul_pow_rsqrt_view_0', '''\n                                                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/async_compile.py\", line 213, in triton\n    kernel.precompile()\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 293, in precompile\n    compiled_binary, launcher = self._precompile_config(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 520, in _precompile_config\n    binary._init_handles()\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/triton/compiler/compiler.py\", line 390, in _init_handles\n    self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(\n                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch._dynamo.exc.BackendCompilerFailed: backend='<vllm.compilation.backends.VllmBackend object at 0x7f576a36a9f0>' raised:\nRuntimeError: Triton Error [CUDA]: device kernel image is invalid\n\nWhile executing %submod_0 : [num_users=5] = call_module[target=submod_0](args = (%l_input_ids_, %s0, %l_self_modules_embed_tokens_parameters_weight_, %l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_, %l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_qweight_, %l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_scales_, %l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_qzeros_, %l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_g_idx_, %l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_g_idx_sort_indices_, %l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_workspace, %l_self_modules_layers_modules_0_modules_self_attn_modules_q_norm_parameters_weight_, %l_self_modules_layers_modules_0_modules_self_attn_modules_k_norm_parameters_weight_, %l_positions_, %l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_), kwargs = {})\nOriginal traceback:\nNone\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n\n[rank0]:[W523 09:31:48.495585945 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nTraceback (most recent call last):\n  File \"/home/llm/anaconda3/envs/vllm/bin/vllm\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py\", line 53, in main\n    args.dispatch_function(args)\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py\", line 27, in cmd\n    uvloop.run(run_server(args))\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 1078, in run_server\n    async with build_async_engine_client(args) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 146, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 178, in build_async_engine_client_from_engine_args\n    async_llm = AsyncLLM.from_vllm_config(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py\", line 150, in from_vllm_config\n    return cls(\n           ^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py\", line 118, in __init__\n    self.engine_core = core_client_class(\n                       ^^^^^^^^^^^^^^^^^^\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 642, in __init__\n    super().__init__(\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 398, in __init__\n    self._wait_for_engine_startup()\n  File \"/home/llm/anaconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 430, in _wait_for_engine_startup\n    raise RuntimeError(\"Engine core initialization failed. \"\nRuntimeError: Engine core initialization failed. See root cause above.\n\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-05-23T02:08:47+00:00",
    "closed_at": "2025-05-27T00:55:54+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18580/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/18580"
  },
  {
    "number": 13442,
    "title": "[Usage]: Given sufficient GPU memory, which is better: starting a single vLLM instance or starting multiple instances for load balancing?",
    "body": "### Your current environment\n\n```txt\nA100-80G\u00d78\n```\n\n### How would you like to use vllm\n\nGiven sufficient GPU memory, which is better: starting a single vLLM instance or starting multiple instances for load balancing?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-02-18T01:02:44+00:00",
    "closed_at": "2025-02-18T19:37:07+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13442/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13442"
  },
  {
    "number": 8903,
    "title": "[Usage]: DOCKER - Getting OOM while running `meta-llama/Llama-3.2-11B-Vision-Instruct`",
    "body": "### Your current environment\n\nI'm trying to run `meta-llama/Llama-3.2-11B-Vision-Instruct` using vLLM docker:\r\n\r\n**GPU Server specifications:**\r\n\r\n- GPU Count: 4\r\n- GPU Type: A100 - 80GB\r\n\r\n**vLLM Docker run command:**\r\n```bash\r\ndocker run  --gpus all \\\r\n    -v /data/hf_cache/ \\\r\n    --env \"HUGGING_FACE_HUB_TOKEN=<token>\" \\\r\n    -p 8000:8000 \\\r\n    --ipc=host \\\r\n    vllm/vllm-openai:latest \\\r\n    --model meta-llama/Llama-3.2-11B-Vision-Instruct \\\r\n    --tensor-parallel-size 4 \\\r\n    --max-model-len 4096 \\\r\n    --download_dir /data/vllm_cache \\\r\n    --enforce-eager\r\n```\r\n\r\n----\r\n**Following is the issue which I'm facing:**\r\n```bash\r\nVllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233] Exception in worker VllmWorkerProcess while processing method determine_num_available_blocks: CUDA out of memory. Tried to allocate 19.63 GiB. GPU 3 has a total capacity of 79.15 GiB of which 17.73 GiB is free. Process 78729 has 61.41 GiB memory in use. Of the allocated memory 56.64 GiB is allocated by PyTorch, and 4.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables), Traceback (most recent call last):\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 226, in _run_worker_process\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]              ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 223, in determine_num_available_blocks\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]     self.model_runner.profile_run()\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/enc_dec_model_runner.py\", line 348, in profile_run\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]     self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/enc_dec_model_runner.py\", line 201, in execute_model\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]     hidden_or_intermediate_states = model_executable(\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]                                     ^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/mllama.py\", line 1084, in forward\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]     cross_attention_states = self.vision_model(pixel_values,\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/mllama.py\", line 561, in forward\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]     intermediate_hidden_states = torch.stack(intermediate_hidden_states,\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233]                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(VllmWorkerProcess pid=214) ERROR 09-27 05:20:38 multiproc_worker_utils.py:233] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 19.63 GiB. GPU 3 has a total capacity of 79.15 GiB of which 17.73 GiB is free. Process 78729 has 61.41 GiB memory in use. Of the allocated memory 56.64 GiB is allocated by PyTorch, and 4.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\n\r\n```\n\n### How would you like to use vllm\n\nI want to run inference of a [meta-llama/Llama-3.2-11B-Vision-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) using docker.\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-09-27T12:26:27+00:00",
    "closed_at": "2024-09-27T12:32:19+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8903/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8903"
  },
  {
    "number": 3996,
    "title": "[Usage]: Using AsyncLLMEngine with asyncio.run",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.1.2+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-102-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA GeForce RTX 3060\r\nGPU 1: NVIDIA RTX A6000\r\n\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             24\r\nOn-line CPU(s) list:                0-23\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Core(TM) i9-10920X CPU @ 3.50GHz\r\nCPU family:                         6\r\nModel:                              85\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 12\r\nSocket(s):                          1\r\nStepping:                           7\r\nCPU max MHz:                        4800.0000\r\nCPU min MHz:                        1200.0000\r\nBogoMIPS:                           6999.82\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512_vnni md_clear flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          384 KiB (12 instances)\r\nL1i cache:                          384 KiB (12 instances)\r\nL2 cache:                           12 MiB (12 instances)\r\nL3 cache:                           19.3 MiB (1 instance)\r\nNUMA node(s):                       1\r\nNUMA node0 CPU(s):                  0-23\r\nVulnerability Gather data sampling: Mitigation; Microcode\r\nVulnerability Itlb multihit:        KVM: Mitigation: VMX disabled\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:             Mitigation; Enhanced IBRS\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Mitigation; TSX disabled\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==7.0.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.1\r\n[pip3] torch==2.1.2\r\n[pip3] triton==2.1.0\r\n[conda] torch                     2.1.2                    pypi_0    pypi\r\n[conda] triton                    2.1.0                    pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tSYS\t0-23\t0\t\tN/A\r\nGPU1\tSYS\t X \t0-23\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### How would you like to use vllm\n\nI want to run batch offline inference in a command line tool using `AsyncLLMEngine` in a set of coroutines being managed by a top level call to `asyncio.run`.\r\n\r\nI have a demo script that ultimately does the right thing but doesn't tear down or complete correctly.  It will either:\r\n1. Hang until asyncio times out\r\n2. Throw an error from `_raise_exception_on_finish` being cancelled\r\n\r\nThe minimal working example of what I want to do is:\r\n\r\n```python\r\nimport asyncio\r\nfrom uuid import uuid4\r\nfrom vllm import AsyncEngineArgs, AsyncLLMEngine, SamplingParams\r\n\r\n\r\ndef main():\r\n    engine = AsyncLLMEngine.from_engine_args(\r\n        AsyncEngineArgs(\r\n            model=\"mistralai/Mistral-7B-Instruct-v0.2\",\r\n        )\r\n    )\r\n    params = SamplingParams(\r\n        top_k=1,\r\n        temperature=0.8,\r\n        max_tokens=64,\r\n        stop=[],\r\n    )\r\n\r\n    async def run_query(query: str):\r\n        request_id = uuid4()\r\n        outputs = engine.generate(query, params, request_id)\r\n        async for output in outputs:\r\n            final_output = output\r\n        responses = []\r\n        for output in final_output.outputs:\r\n            responses.append(output.text)\r\n        return responses\r\n\r\n    async def process():\r\n        queries = [\r\n            \"I have cats, do you?\",\r\n            \"If you were a muppet, what type of muppet would you be?\",\r\n        ]\r\n        tasks = [asyncio.create_task(run_query(q)) for q in queries]\r\n        results = []\r\n        for task in asyncio.as_completed(tasks):\r\n            result = await task\r\n            results.append(result)\r\n        return results\r\n\r\n    results = asyncio.run(process())\r\n    print(results)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nMy guess is that this is due to `AsyncLLMEngine` calling `asyncio.get_event_loop` and that somehow conflicting with my top level runner calling `asyncio.run`.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-04-11T06:31:52+00:00",
    "closed_at": "2024-06-19T20:57:14+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3996/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3996"
  },
  {
    "number": 19107,
    "title": "[Usage]: intent is added for guided generation",
    "body": "### Your current environment\n\n`{\\n\"product\":\\n\"pixel\",\\n\"rating\":\\n3\\n}` \n\nresponse_format + guided generation will add \\n. how can we avoid this intent=2 for guided generation? and only force the model to generate via dense json.dumps default.  \n\n### How would you like to use vllm\n\n- use guided generation without intent\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "open",
    "created_at": "2025-06-03T21:33:32+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19107/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19107"
  },
  {
    "number": 13415,
    "title": "[Usage]: Tokenizer or Not?",
    "body": "### Your current environment\n\n```\nINFO 02-17 17:43:56 __init__.py:190] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-130-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA A100 80GB PCIe\nGPU 1: NVIDIA A100 80GB PCIe\n\nNvidia driver version: 535.216.03\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        43 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               48\nOn-line CPU(s) list:                  0-47\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 7352 24-Core Processor\nCPU family:                           23\nModel:                                49\nThread(s) per core:                   2\nCore(s) per socket:                   24\nSocket(s):                            1\nStepping:                             0\nFrequency boost:                      enabled\nCPU max MHz:                          2300.0000\nCPU min MHz:                          1500.0000\nBogoMIPS:                             4591.45\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nVirtualization:                       AMD-V\nL1d cache:                            768 KiB (24 instances)\nL1i cache:                            768 KiB (24 instances)\nL2 cache:                             12 MiB (24 instances)\nL3 cache:                             128 MiB (8 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-47\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\nVulnerability Spec rstack overflow:   Mitigation; safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.1.post1+cu124torch2.5\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchao==0.8.0\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.3\n[pip3] triton==3.1.0\n[conda] flashinfer-python         0.2.1.post1+cu124torch2.5          pypi_0    pypi\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-ml-py              12.570.86                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.2.1                   pypi_0    pypi\n[conda] torch                     2.5.1                    pypi_0    pypi\n[conda] torchao                   0.8.0                    pypi_0    pypi\n[conda] torchaudio                2.5.1                    pypi_0    pypi\n[conda] torchvision               0.20.1                   pypi_0    pypi\n[conda] transformers              4.48.3                   pypi_0    pypi\n[conda] triton                    3.1.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     0-47    0               N/A\nGPU1    SYS      X      0-47    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n### How would you like to use vllm\n\nWhat do you think is the correct way to do it?\n\n\n## Without Tokenizer\n```\nllm = LLM(\n    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n    max_model_len=8192,\n)\n\nprompts = [\"Hi\", \"hello\"]\n\noutputs = llm.generate(prompts, sampling_params)\n```\n\n## With Tokenizer\n```\nllm = LLM(\n    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n    max_model_len=8192,\n)\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.3-70B-Instruct\")\n\nprompts = [\"Hi\", \"hello\"]\n\ndef get(user_prompt):\n    messages = [\n        {\"role\": \"user\", \"content\": user_prompt},\n    ]\n    prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n    return prompt\nprompts = [get(prompt) for prompt in prompts]\n\noutputs = llm.generate(prompts, sampling_params)\n```\n\nFor context, the first one outputs many gibberish things, while the 2nd one outputs perfectly.\nBut I am just confirming  \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-02-17T17:45:11+00:00",
    "closed_at": "2025-02-17T21:21:34+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13415/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13415"
  },
  {
    "number": 8471,
    "title": "[Usage]: Get first token latency ",
    "body": "\r\n\r\nIs there a way to get the first token latency? benchmarks/benchmark_latency.py provides the latency of processing a single batch of requests but I am interested in getting first token latency\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-13T17:38:49+00:00",
    "closed_at": "2025-01-17T01:57:44+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8471/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8471"
  },
  {
    "number": 10684,
    "title": "[Usage]: Can vLLM profile capture the python process of api_server?",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.12.7 (main, Oct  1 2024, 08:52:12) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.10.134-16.3.al8.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H20\r\nGPU 1: NVIDIA H20\r\nGPU 2: NVIDIA H20\r\nGPU 3: NVIDIA H20\r\n\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   52 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          192\r\nOn-line CPU(s) list:             0-191\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Platinum 8469C\r\nCPU family:                      6\r\nModel:                           143\r\nThread(s) per core:              2\r\nCore(s) per socket:              48\r\nSocket(s):                       2\r\nStepping:                        8\r\nCPU max MHz:                     3800.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        5200.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm uintr md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                  VT-x\r\nL1d cache:                       4.5 MiB (96 instances)\r\nL1i cache:                       3 MiB (96 instances)\r\nL2 cache:                        192 MiB (96 instances)\r\nL3 cache:                        195 MiB (2 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-47,96-143\r\nNUMA node1 CPU(s):               48-95,144-191\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.6+cu121torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.77\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.2\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A (dev)\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    NIC0    NIC1    NIC2    NIC3    NIC4    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV18    NV18    NV18    SYS     SYS     SYS     PIX     SYS     48-95,144-191   1               N/A\r\nGPU1    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     48-95,144-191   1               N/A\r\nGPU2    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     PIX     48-95,144-191   1               N/A\r\nGPU3    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     48-95,144-191   1               N/A\r\nNIC0    SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS\r\nNIC1    SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS\r\nNIC2    SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS\r\nNIC3    PIX     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS\r\nNIC4    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_bond_0\r\n  NIC2: mlx5_bond_1\r\n  NIC3: mlx5_bond_2\r\n  NIC4: mlx5_bond_3\n\n### How would you like to use vllm\n\nI want to run inference of a oneline serving, and profiling it, but i can't find the api_server process in profiling json, Only the engine and worker processes can be found, as shown in the following figure\r\n<img width=\"578\" alt=\"4D520F9D-B83D-4ae5-9988-8161EAD63DC8\" src=\"https://github.com/user-attachments/assets/077a7e0a-a8a7-4b99-9421-bbf0c6df9372\">\r\n<img width=\"1280\" alt=\"903993ED-C132-4ebe-91C2-371AD650716F\" src=\"https://github.com/user-attachments/assets/26c5d9d9-f781-498d-a6e0-66349d2400b4\">\r\n\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-11-27T01:50:38+00:00",
    "closed_at": "2024-11-27T03:24:13+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10684/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10684"
  },
  {
    "number": 12033,
    "title": "[Usage]: If I want to test in a concurrent environment using llava, say request/s=6, how can I do it?",
    "body": "### Your current environment\n\nNo samples of concurrent use of Llava were found.\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-01-14T11:11:19+00:00",
    "closed_at": "2025-01-14T11:58:35+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12033/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12033"
  },
  {
    "number": 10220,
    "title": "[Usage]: \u600e\u4e48\u4fee\u6539python -m vllm.entrypoints.openai.api_server\u7684\u63d0\u793a\u8bcd",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\npython3.10.12\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\r\n![108FCB7D-FBB6-429b-9B79-EA00E0A9AFDB](https://github.com/user-attachments/assets/a750281d-eed9-4036-8b22-361193adeb6c)\r\n\u6211\u5e0c\u671b\u4fee\u6539\u7cfb\u7edf\u63d0\u793a\u8bcd\uff0c\u4e0d\u8bba\u662f\u5728\u4ee3\u7801\u91cc\u3001\u6a21\u578b\u6587\u4ef6\u91cc\u6216\u8005\u547d\u4ee4\u884c\u91cc\uff0c\u600e\u4e48\u6539\u53d8\u8fd9\u4e2a\u63d0\u793a\u8bcd\u5462\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-11T11:43:36+00:00",
    "closed_at": "2025-03-12T02:02:44+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10220/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10220"
  },
  {
    "number": 9655,
    "title": "[Usage]: Pass multiple LoRA modules through YAML config",
    "body": "### How would you like to use vllm\r\n\r\nI would like to pass multiple LoRA modules to the vLLM engine, but currently I'm receiving error while parsing the `lora_modules` property.\r\n\r\nThe `LoRAParserAction` class receives a `Sequence[str]` in case you want to use multiple LoRA modules.\r\n\r\nI have a YAML config file in which I declare the vLLM engine arguments, like this:\r\n```\r\nmodel: ai-models/Meta-Llama-3.1-8B-Instruct-rev-5206a32\r\ntokenizer_mode: auto\r\ndtype: half\r\nlora_modules: \"ai-models/adv_perizia_exp7_run6=ai-models/adv_perizia_exp7_run6\"\r\nmax_num_batched_tokens: 32768\r\nmax_num_seqs: 192\r\ngpu_memory_utilization: 0.95\r\ntensor_parallel_size: <RAY_LLM_NUM_WORKERS>\r\nmax_model_len: 32768\r\n```\r\n\r\nIn that way (`name=path` for the LoRA module), all works and I'm able to perform inference with LoRA (I set `enable_lora` argument later in the code, not in the YAML file).\r\nNow I would like to pass multiple `lora_modules`, but I'm receiving parsing error in every different ways I tried:\r\n\r\n`lora_modules: \"ai-models/adv_perizia_exp7_run6=ai-models/adv_perizia_exp7_run6 ai-models/perizia_exp7_run3=ai-models/perizia_exp7_run3\"` (blanks space between LoRA modules)\r\n\r\n```\r\nlora_modules:\r\n      - \"ai-models/adv_perizia_exp7_run6=ai-models/adv_perizia_exp7_run6\"\r\n      - \"ai-models/perizia_exp7_run3=ai-models/perizia_exp7_run3\"\r\n```\r\n\r\n`lora_modules: [\"ai-models/adv_perizia_exp7_run6=ai-models/adv_perizia_exp7_run6\",\"ai-models/perizia_exp7_run3=ai-models/perizia_exp7_run3\"]`\r\n\r\n`lora_modules: > \r\n\"ai-models/adv_perizia_exp7_run6=ai-models/adv_perizia_exp7_run6\\nai-models/perizia_exp7_run3=ai-models/perizia_exp7_run3\"` (\\n between LoRA modules)\r\n\r\nHow can I pass multiple LoRA modules correctly? Thanks in advance.\r\n",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-24T10:39:28+00:00",
    "closed_at": "2025-01-23T05:45:42+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9655/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9655"
  },
  {
    "number": 3931,
    "title": "[Usage]: How to offload some layers to CPU\uff1f",
    "body": "### Your current environment\n\nNone\n\n### How would you like to use vllm\n\nI want to load qwen2-14B-chat using VLLM, but I only have 1 RTX4090(24G). \r\nCan vllm offload some layers to cpu and others to gpu?\r\nAs I know, the transformers-accelerate and llama.cpp can do it. But I want to use the multilora switch function in VLLM.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-09T09:10:35+00:00",
    "closed_at": "2024-11-28T02:06:43+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3931/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3931"
  },
  {
    "number": 15390,
    "title": "[Usage]:",
    "body": "### Your current environment\n\nHello everyone:\n\nI have 3 x 2080 8G, 1 x 2080Ti (22G VRAM), 1 x 3080 (20G),  1 x RTX5070 Ti,\nhow do I use vllm to load the QwQ-32B-AWQ large language model with these graphics cards? \n\nThank you\n\n\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-03-24T11:45:12+00:00",
    "closed_at": "2025-03-24T14:23:55+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15390/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15390"
  },
  {
    "number": 19421,
    "title": "[Usage]: \u652f\u6301qwen3\u7684\u7248\u672c\u4e0b\uff0c\u5982\u4f55\u4f7f\u7528beam search\u9650\u5236\u751f\u6210",
    "body": "### Your current environment\n\nvllm:v0.8.5\n\n\n### How would you like to use vllm\n\n\u652f\u6301qwen3\u7684\u7248\u672c\u4e0b\uff0cvllm.beam_search\u5e76\u6ca1\u6709\u53ef\u4ee5\u914d\u7f6elogits_processors\u7684\u5730\u65b9\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "open",
    "created_at": "2025-06-10T12:04:32+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19421/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19421"
  },
  {
    "number": 13604,
    "title": "[Usage]: Does larger max_num_batched_tokens use more VRAM?",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\n```\n\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-02-20T09:57:01+00:00",
    "closed_at": "2025-02-20T11:28:58+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13604/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/13604"
  },
  {
    "number": 8136,
    "title": "[Usage]: 125m parameter model is also showing CUDA: Out of memory error in a Nvidia16GB 4060 ",
    "body": "\r\n\r\n\r\n### How would you like to use vllm\r\n\r\nEven for a smaller model like \"facebook/opt-125m\" when I am trying to do multiprocessing(even with batch size of 2) on a single 16GB Nvidia 4060, I am encountering CUDA: OUT OF MEMORY ERROR. When I am running the same model sequentially, I am able to run it fine. Can you explain this?\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-09-03T23:43:52+00:00",
    "closed_at": "2024-09-08T23:42:03+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8136/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8136"
  },
  {
    "number": 8863,
    "title": "[Usage]: RuntimeError: Failed to infer device type (Intel Iris Xe Graphics)",
    "body": "### Your current environment\r\n\r\n```text\r\nCollecting environment information...\r\nWARNING 09-26 20:43:46 _custom_ops.py:18] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\r\nINFO 09-26 20:43:46 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\r\nC:\\Users\\sasha\\vllm\\vllm\\vllm\\connections.py:8: RuntimeWarning: Failed to read commit hash:\r\nNo module named 'vllm.commit_id'\r\n  from vllm.version import __version__ as VLLM_VERSION\r\nPyTorch version: 2.4.0+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Microsoft Windows 10 Enterprise\r\nGCC version: Could not collect\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: N/A\r\n\r\nPython version: 3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\r\nPython platform: Windows-10-10.0.19045-SP0\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture=9\r\nCurrentClockSpeed=1800\r\nDeviceID=CPU0\r\nFamily=198\r\nL2CacheSize=4096\r\nL2CacheSpeed=\r\nManufacturer=GenuineIntel\r\nMaxClockSpeed=1800\r\nName=12th Gen Intel(R) Core(TM) i7-1265U\r\nProcessorType=3\r\nRevision=\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.2\r\n[conda] mkl                       2021.4.0                 pypi_0    pypi\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.44.2                   pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.1.post2@COMMIT_HASH_PLACEHOLDER\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n\r\n```\r\n\r\nMy GPU : Intel(R) Iris(R) Xe Graphics / 15.8GB of shared memory\r\n\r\n### How would you like to use vllm\r\n\r\nI want to run inference of a \"mistralai/Pixtral-12B-2409\" as shown [here](https://mistral.ai/fr/news/pixtral-12b/). Here below the code : \r\n```\r\nfrom vllm import LLM\r\nfrom vllm.sampling_params import SamplingParams\r\n\r\nmodel_name = \"mistralai/Pixtral-12B-2409\"\r\n\r\nsampling_params = SamplingParams(max_tokens=8192)\r\n\r\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\")\r\n\r\nprompt = \"Describe this image in one sentence.\"\r\nimage_url = \"https://picsum.photos/id/237/200/300\"\r\n\r\nmessages = [\r\n    {\r\n        \"role\": \"user\",\r\n        \"content\": [{\"type\": \"text\", \"text\": prompt}, {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}]\r\n    },\r\n]\r\n\r\noutputs = vllm_model.model.chat(messages, sampling_params=sampling_params)\r\n\r\nprint(outputs[0].outputs[0].text)\r\n```\r\nThis is what i got :\r\n```\r\n>>> llm = LLM(model=model_name, tokenizer_mode=\"mistral\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\sasha\\vllm\\vllm\\vllm\\entrypoints\\llm.py\", line 179, in __init__\r\n    self.llm_engine = LLMEngine.from_engine_args(\r\n  File \"C:\\Users\\sasha\\vllm\\vllm\\vllm\\engine\\llm_engine.py\", line 552, in from_engine_args\r\n    engine_config = engine_args.create_engine_config()\r\n  File \"C:\\Users\\sasha\\vllm\\vllm\\vllm\\engine\\arg_utils.py\", line 860, in create_engine_config\r\n    device_config = DeviceConfig(device=self.device)\r\n  File \"C:\\Users\\sasha\\vllm\\vllm\\vllm\\config.py\", line 1054, in __init__\r\n    raise RuntimeError(\"Failed to infer device type\")\r\nRuntimeError: Failed to infer device type\r\n```\r\nThanks,\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-26T18:59:17+00:00",
    "closed_at": "2025-04-24T02:08:29+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8863/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8863"
  }
]
[
  {
    "number": 5148,
    "title": "[Misc]: Should inference with temperature 0 generate the same results for a lora adapter and equivalent merged model?",
    "body": "### Anything you want to discuss about vllm.\n\nI am scoring a fine-tuned mistral modal using the vllm enable_lora option with temperature 0.0. Subsequently merging that lora adapter into the base model using peft merge_and_unload results in a new merged model which when used with vllm (again temperature 0.0) generates noticeably different results. \r\n\r\n1. Should I be expecting these two methods of inference to generate the same result? \r\n2. If so is there any suggested method for loading the base model and adapter and merging that would result in the same generation results?",
    "labels": [
      "question",
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-05-31T04:07:06+00:00",
    "closed_at": "2024-06-03T01:51:39+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5148/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5148"
  },
  {
    "number": 2848,
    "title": "computation of prompt_logprobs",
    "body": "What are the distinctions between the computation of **prompt_logprobs** in input tokens and **logprobs** in output tokens?",
    "labels": [
      "question",
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-02-13T08:09:58+00:00",
    "closed_at": "2024-06-03T03:02:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2848/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2848"
  },
  {
    "number": 1139,
    "title": "Use multi-turn prompts for benchmark_throughput.py",
    "body": "Do we consider using multi-turn prompts instead of the first turn for throughput benchmarking? It would be more realistic.",
    "labels": [
      "question",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-09-22T05:56:26+00:00",
    "closed_at": "2024-12-01T02:16:17+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1139/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/1139"
  },
  {
    "number": 454,
    "title": "How many requests can `llm.generate` handle in parallel?",
    "body": "To run benchmarks, should I add all prompts to the `llm.generate` class at once, let the engine queue and schedule, or add them in small batches?",
    "labels": [
      "question"
    ],
    "state": "closed",
    "created_at": "2023-07-13T09:23:43+00:00",
    "closed_at": "2023-07-13T15:19:48+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/454/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/454"
  },
  {
    "number": 230,
    "title": "Do you support streaming generating outputs?",
    "body": null,
    "labels": [
      "question"
    ],
    "state": "closed",
    "created_at": "2023-06-24T15:48:01+00:00",
    "closed_at": "2023-06-25T17:47:08+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/230/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/230"
  },
  {
    "number": 181,
    "title": "Can vllm serving clients by using multiple model instances?",
    "body": "Based on the examples, vllm can launch a server with a  single model instances. Can vllm serving clients by using multiple model instances? With multiple model instances, the sever will dispatch the requests to different instances to reduce the overhead.",
    "labels": [
      "question"
    ],
    "state": "closed",
    "created_at": "2023-06-21T07:24:05+00:00",
    "closed_at": "2023-06-25T16:43:30+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/181/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/181"
  },
  {
    "number": 178,
    "title": "What's the difference between vllm and triton-inference-server?",
    "body": "May vllm can achieve the performance like fastertransformer on inference side? Just curious about the detailed optimization you're done and the goal you want to achieve.\r\nBTW, vllm really accelerate our deploy work, thx.",
    "labels": [
      "question"
    ],
    "state": "closed",
    "created_at": "2023-06-21T06:24:04+00:00",
    "closed_at": "2023-06-25T16:44:00+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/178/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/178"
  }
]
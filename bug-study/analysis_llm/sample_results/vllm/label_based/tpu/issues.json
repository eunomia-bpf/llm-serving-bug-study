[
  {
    "number": 16282,
    "title": "[RFC]: How to handle the compilation of PyTorch/XLA in vLLM",
    "body": "### Motivation.\n\nvLLM currently utilizes PyTorch/XLA to provide TPU backend support. However, PyTorch/XLA differs significantly from native PyTorch in terms of usage. PyTorch/XLA is a compilation only framework, it doesn't have a real eager mode. In particular, for LLM serving services, recompilation should be avoided once the server is running. \nWhen compiling, it's important to consider which code might create PyTorch operations (e.g., tensor.copy(), tensor[:index], torch.ones(...)) and when graph capture and compilation is triggered (e.g., xm.mark_step(), xla_tensor.cpu(), if xla_tensor:, torch.compile(backend=\"openxla\")). Due to the complexity of PyTorch/XLA, this document will only provide basic rules to simplify vLLM development on TPU.\n\n### Ways to avoid recompilation\nThe model executor has two primary components:\n- preparing the model and sampler inputs\n- executing the model and sampler.\n#### Step 1\nIt is recommended to avoid TPU operations when preparing the model and sampler inputs. CPU tensors can be prepared and transferred to the XLA device using cpu_tensor.to(xla_device), which only triggers CPU to TPU transfers and avoids compilation.\n#### Step 2\nThe TPU execution should be decomposed into subgraphs (4 at the moment): \n- the main model\n- selecting hidden states for each request\n- sampler\n- encoder. \nEach subgraph should be decorated in a torch.compile. This is used to make sure that we have the same subgraph topology in both dummy_run and execute_model. The results from these subgraphs should either be passed to other subgraphs, or transferred from TPU to CPU using xla_tensor.cpu() for subsequent processing on the CPU.\n#### Step 3\nThe dummy_run should be comprehensive, ensuring all potential input shapes and branch predictions are included as subgraph inputs to facilitate pre-compilation.\n\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n@robertgshaw2-redhat @NickLucche @WoosukKwon @yarongmu-google @bvrockwell \n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "tpu"
    ],
    "state": "closed",
    "created_at": "2025-04-08T19:37:11+00:00",
    "closed_at": "2025-04-16T01:29:59+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16282/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16282"
  },
  {
    "number": 14582,
    "title": "[Feature][Hardware][TPU]:Reduce the compile time",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nAfter the fix of https://github.com/vllm-project/vllm/pull/14310,\n\nWe have num_token_bucket compilations for the main model and num_token_bucket x num_reqs_bucket for the logits processor.\n\nWe can make some improvement on this, as the num_token_bucket x num_reqs_bucket only happens on hidden_states[logits_indices], where we select part of the hidden states. Therefore, we can partition the graph to 3 parts:\n\nmain model: num_token_bucket\nhidden_states[logits_indices]: num_token_bucket x num_reqs_bucket\nlogits: num_reqs_bucket\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "tpu"
    ],
    "state": "closed",
    "created_at": "2025-03-10T22:36:38+00:00",
    "closed_at": "2025-04-16T05:31:48+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14582/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14582"
  },
  {
    "number": 14581,
    "title": "[Feature][Hardware][TPU]: Improve the token_num padding logic",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently the token_num is padded to power of 2. It is quite a waste of computation when token_num is large. In the meantime, in one of our benchmarking, the best max-num-batched-tokens is 512, also according to https://jax-ml.github.io/scaling-book/roofline/, we don't really need max-num-batched-tokens to be very large.\n\nAlso cudagraph precompiles for [512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1], we can use a similar padding strategy for TPU.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "tpu"
    ],
    "state": "closed",
    "created_at": "2025-03-10T22:34:10+00:00",
    "closed_at": "2025-03-25T21:27:24+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14581/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14581"
  },
  {
    "number": 14580,
    "title": "[Feature][Hardware][TPU]: Add Recompilation Check for vLLM on TPU",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nIdeally, post-warmup, no further compilation should occur. However, PyTorch/XLA's implicit compilation can lead to excessive recompilation during LLM serving, impacting performance. We can add an option to detect recompilation after warmup, requiring a PyTorch/XLA method like xm.num_graph_hash() to track the number of captured graphs. This number should remain constant post-warmup if no recompilation occurs.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "tpu"
    ],
    "state": "closed",
    "created_at": "2025-03-10T22:31:05+00:00",
    "closed_at": "2025-03-25T16:59:34+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14580/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14580"
  },
  {
    "number": 12580,
    "title": "[Bug][TPU]: Non-deterministic behaviour",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 01-30 17:31:38 __init__.py:183] Automatically detected platform tpu.\nCollecting environment information...\nPyTorch version: 2.6.0.dev20241126+cpu\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.31.4\nLibc version: glibc-2.35\n\nPython version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-6.8.0-1015-gcp-x86_64-with-glibc2.35\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               44\nOn-line CPU(s) list:                  0-43\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 9B14\nCPU family:                           25\nModel:                                17\nThread(s) per core:                   2\nCore(s) per socket:                   22\nSocket(s):                            1\nStepping:                             1\nBogoMIPS:                             5200.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr wbnoinvd arat avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            704 KiB (22 instances)\nL1i cache:                            704 KiB (22 instances)\nL2 cache:                             22 MiB (22 instances)\nL3 cache:                             96 MiB (3 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-43\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.6.0.dev20241126+cpu\n[pip3] torch-xla==2.6.0+git39e67b5\n[pip3] torchvision==0.20.0.dev20241126+cpu\n[pip3] transformers==4.48.0\n[pip3] triton==3.1.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\n[conda] torch                     2.6.0.dev20241126+cpu          pypi_0    pypi\n[conda] torch-xla                 2.6.0+git39e67b5          pypi_0    pypi\n[conda] torchvision               0.20.0.dev20241126+cpu          pypi_0    pypi\n[conda] transformers              4.48.0                   pypi_0    pypi\n[conda] triton                    3.1.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.6.6.post2.dev275+gaeed57ba.d20250122\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nLD_LIBRARY_PATH=/mnt/ssd0/anaconda3/envs/vllm/lib/python3.10/site-packages/cv2/../../lib64:\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\n```\n\n</details>\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nHi, I've found that when I run Llama3.2-3B using vLLM on the TPU v6e backend, I get different outputs depending on the instance/time of day.\n\nTo reproduce, run the below script on 2 different TPU machines, at different times.\n\n```py\nimport vllm\n\nllm = vllm.LLM(\n    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n    download_dir=\"/mnt/ssd0/data\",\n    num_scheduler_steps=4,\n    swap_space=16,\n    max_model_len=256,\n    enforce_eager=False\n)\n\nprint(llm.chat([\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"How are you?\"},\n], vllm.SamplingParams(\n    max_tokens=1024,\n    temperature=0\n))[0].outputs[0].text)\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tpu"
    ],
    "state": "closed",
    "created_at": "2025-01-30T17:32:24+00:00",
    "closed_at": "2025-02-07T16:29:46+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12580/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/12580"
  },
  {
    "number": 12058,
    "title": "[Usage]: Running Tensor Parallel on TPUs on Ray Cluster",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\nThe output of `python collect_env.py`\n(test_hf_qwen pid=17527, ip=10.130.4.26) Environment Information:\n(test_hf_qwen pid=17527, ip=10.130.4.26) Collecting environment information...\n(test_hf_qwen pid=17527, ip=10.130.4.26) PyTorch version: 2.6.0.dev20241126+cpu\n(test_hf_qwen pid=17527, ip=10.130.4.26) Is debug build: False\n(test_hf_qwen pid=17527, ip=10.130.4.26) CUDA used to build PyTorch: None\n(test_hf_qwen pid=17527, ip=10.130.4.26) ROCM used to build PyTorch: N/A\n(test_hf_qwen pid=17527, ip=10.130.4.26) \n(test_hf_qwen pid=17527, ip=10.130.4.26) OS: Ubuntu 22.04.4 LTS (x86_64)\n(test_hf_qwen pid=17527, ip=10.130.4.26) GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n(test_hf_qwen pid=17527, ip=10.130.4.26) Clang version: 14.0.0-1ubuntu1.1\n(test_hf_qwen pid=17527, ip=10.130.4.26) CMake version: version 3.31.2\n(test_hf_qwen pid=17527, ip=10.130.4.26) Libc version: glibc-2.35\n(test_hf_qwen pid=17527, ip=10.130.4.26) \n(test_hf_qwen pid=17527, ip=10.130.4.26) Python version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\n(test_hf_qwen pid=17527, ip=10.130.4.26) Python platform: Linux-5.19.0-1022-gcp-x86_64-with-glibc2.35\n(test_hf_qwen pid=17527, ip=10.130.4.26) Is CUDA available: False\n(test_hf_qwen pid=17527, ip=10.130.4.26) CUDA runtime version: No CUDA\n(test_hf_qwen pid=17527, ip=10.130.4.26) CUDA_MODULE_LOADING set to: N/A\n(test_hf_qwen pid=17527, ip=10.130.4.26) GPU models and configuration: No CUDA\n(test_hf_qwen pid=17527, ip=10.130.4.26) Nvidia driver version: No CUDA\n(test_hf_qwen pid=17527, ip=10.130.4.26) cuDNN version: No CUDA\n(test_hf_qwen pid=17527, ip=10.130.4.26) HIP runtime version: N/A\n(test_hf_qwen pid=17527, ip=10.130.4.26) MIOpen runtime version: N/A\n(test_hf_qwen pid=17527, ip=10.130.4.26) Is XNNPACK available: True\n(test_hf_qwen pid=17527, ip=10.130.4.26) \n(test_hf_qwen pid=17527, ip=10.130.4.26) CPU:\n(test_hf_qwen pid=17527, ip=10.130.4.26) Architecture:                    x86_64\n(test_hf_qwen pid=17527, ip=10.130.4.26) CPU op-mode(s):                  32-bit, 64-bit\n(test_hf_qwen pid=17527, ip=10.130.4.26) Address sizes:                   48 bits physical, 48 bits virtual\n(test_hf_qwen pid=17527, ip=10.130.4.26) Byte Order:                      Little Endian\n(test_hf_qwen pid=17527, ip=10.130.4.26) CPU(s):                          240\n(test_hf_qwen pid=17527, ip=10.130.4.26) On-line CPU(s) list:             0-239\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vendor ID:                       AuthenticAMD\n(test_hf_qwen pid=17527, ip=10.130.4.26) Model name:                      AMD EPYC 7B12\n(test_hf_qwen pid=17527, ip=10.130.4.26) CPU family:                      23\n(test_hf_qwen pid=17527, ip=10.130.4.26) Model:                           49\n(test_hf_qwen pid=17527, ip=10.130.4.26) Thread(s) per core:              2\n(test_hf_qwen pid=17527, ip=10.130.4.26) Core(s) per socket:              60\n(test_hf_qwen pid=17527, ip=10.130.4.26) Socket(s):                       2\n(test_hf_qwen pid=17527, ip=10.130.4.26) Stepping:                        0\n(test_hf_qwen pid=17527, ip=10.130.4.26) BogoMIPS:                        4499.99\n(test_hf_qwen pid=17527, ip=10.130.4.26) Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid\n(test_hf_qwen pid=17527, ip=10.130.4.26) Hypervisor vendor:               KVM\n(test_hf_qwen pid=17527, ip=10.130.4.26) Virtualization type:             full\n(test_hf_qwen pid=17527, ip=10.130.4.26) L1d cache:                       3.8 MiB (120 instances)\n(test_hf_qwen pid=17527, ip=10.130.4.26) L1i cache:                       3.8 MiB (120 instances)\n(test_hf_qwen pid=17527, ip=10.130.4.26) L2 cache:                        60 MiB (120 instances)\n(test_hf_qwen pid=17527, ip=10.130.4.26) L3 cache:                        480 MiB (30 instances)\n(test_hf_qwen pid=17527, ip=10.130.4.26) NUMA node(s):                    2\n(test_hf_qwen pid=17527, ip=10.130.4.26) NUMA node0 CPU(s):               0-59,120-179\n(test_hf_qwen pid=17527, ip=10.130.4.26) NUMA node1 CPU(s):               60-119,180-239\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Itlb multihit:     Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability L1tf:              Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Mds:               Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Meltdown:          Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Mmio stale data:   Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Retbleed:          Mitigation; untrained return thunk; SMT enabled with STIBP protection\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Srbds:             Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) Vulnerability Tsx async abort:   Not affected\n(test_hf_qwen pid=17527, ip=10.130.4.26) \n(test_hf_qwen pid=17527, ip=10.130.4.26) Versions of relevant libraries:\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] mypy-extensions==1.0.0\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] numpy==1.26.4\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cublas-cu12==12.4.5.8\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cuda-cupti-cu12==12.4.127\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cuda-runtime-cu12==12.4.127\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cudnn-cu12==9.1.0.70\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cufft-cu12==11.2.1.3\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-curand-cu12==10.3.5.147\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cusolver-cu12==11.6.1.9\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-cusparse-cu12==12.3.1.170\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-nccl-cu12==2.21.5\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-nvjitlink-cu12==12.4.127\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] nvidia-nvtx-cu12==12.4.127\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] pyzmq==26.2.0\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] torch==2.6.0.dev20241126+cpu\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] torch-xla==2.6.0+git39e67b5\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] torchvision==0.20.0.dev20241126+cpu\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] transformers==4.47.1\n(test_hf_qwen pid=17527, ip=10.130.4.26) [pip3] triton==3.1.0\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] numpy                     1.26.4                   pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] pyzmq                     26.2.0                   pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] torch                     2.6.0.dev20241126+cpu          pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] torch-xla                 2.6.0+git39e67b5          pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] torchvision               0.20.0.dev20241126+cpu          pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] transformers              4.47.1                   pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) [conda] triton                    3.1.0                    pypi_0    pypi\n(test_hf_qwen pid=17527, ip=10.130.4.26) ROCM Version: Could not collect\n(test_hf_qwen pid=17527, ip=10.130.4.26) Neuron SDK Version: N/A\n(test_hf_qwen pid=17527, ip=10.130.4.26) vLLM Version: N/A (dev)\n(test_hf_qwen pid=17527, ip=10.130.4.26) vLLM Build Flags:\n(test_hf_qwen pid=17527, ip=10.130.4.26) CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\n(test_hf_qwen pid=17527, ip=10.130.4.26) GPU Topology:\n(test_hf_qwen pid=17527, ip=10.130.4.26) Could not collect\n(test_hf_qwen pid=17527, ip=10.130.4.26) \n(test_hf_qwen pid=17527, ip=10.130.4.26) LD_LIBRARY_PATH=/home/ray/anaconda3/lib/python3.11/site-packages/cv2/../../lib64:/home/ray/anaconda3/lib/python3.11/site-packages/cv2/../../lib64::/usr/lib/x86_64-linux-gnu/:/home/ray/anaconda3/lib\n(test_hf_qwen pid=17527, ip=10.130.4.26) OMP_NUM_THREADS=1\n(test_hf_qwen pid=17527, ip=10.130.4.26) CUDA_VISIBLE_DEVICES=\n(test_hf_qwen pid=17527, ip=10.130.4.26) CUDA_VISIBLE_DEVICES=\n(test_hf_qwen pid=17527, ip=10.130.4.26) TORCHINDUCTOR_COMPILE_THREADS=1\n(test_hf_qwen pid=17527, ip=10.130.4.26) TORCHINDUCTOR_CACHE_DIR=/tmp/torchinductor_ray\n```\n\n\n### How would you like to use vllm\n\nI want to run tensor-parallel inference using TPUs in a ray cluster. It seems like the Ray cluster picks up the accelerator that we need but then when vllm tries to initialize the ray cluster, it doesn't know that, so it doesn't reuse the TPUs that the cluster has already picked up. I was wondering how people would implement this? Thanks!\n\n\nCode:\n```\nfrom vllm import LLM\n\n@ray.remote(resources={\"TPU\": 4, \"TPU-v4-8-head\": 1})\ndef test():\n    llm = LLM(model=Qwen/Qwen2.5-7B-Instruct, enforce_eager=True, max_model_len=8192, tensor_parallel_size=4)\n```\n\nError:\n```\n(test_hf pid=1616, ip=10.130.0.8) INFO 01-15 09:04:53 config.py:510] This model supports multiple tasks: {'generate', 'score', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.\n(test_hf pid=1616, ip=10.130.0.8) Connecting to existing Ray cluster at address: 10.130.2.110:6379...\n(test_hf pid=1616, ip=10.130.0.8) Calling ray.init() again after it has already been called.\nTraceback (most recent call last):\n  File \"/tmp/ray/session_2025-01-09_10-24-52_724484_545/runtime_resources/working_dir_files/_ray_pkg_0e6ac5e67e3f89c8/experiments/llama_test.py\", line 75, in <module>\n    raise e\n  File \"/tmp/ray/session_2025-01-09_10-24-52_724484_545/runtime_resources/working_dir_files/_ray_pkg_0e6ac5e67e3f89c8/experiments/llama_test.py\", line 73, in <module>\n    ray.get(ref)\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n    raise value.as_instanceof_cause()\nray.exceptions.RayTaskError(ValueError): ray::test_hf() (pid=1616, ip=10.130.0.8)\n  File \"/tmp/ray/session_2025-01-09_10-24-52_724484_545/runtime_resources/working_dir_files/_ray_pkg_0e6ac5e67e3f89c8/experiments/llama_test.py\", line 59, in test_hf\n    classifier = AutoClassifier.from_model_path(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ray/session_2025-01-09_10-24-52_724484_545/runtime_resources/working_dir_files/_ray_pkg_0e6ac5e67e3f89c8/marin/processing/classification/classifier.py\", line 281, in from_model_path\n    return cls._MODEL_NAME_TO_CLS_DICT[key](model_name_or_path, attribute_name, model_type, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ray/session_2025-01-09_10-24-52_724484_545/runtime_resources/working_dir_files/_ray_pkg_0e6ac5e67e3f89c8/marin/processing/classification/classifier.py\", line 213, in __init__\n    self.llm = LLM(model=model_name, enforce_eager=True, max_model_len=8192, tensor_parallel_size=4)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm-0.6.6.post1/vllm/utils.py\", line 986, in inner\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm-0.6.6.post1/vllm/entrypoints/llm.py\", line 230, in __init__\n    self.llm_engine = self.engine_class.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm-0.6.6.post1/vllm/engine/llm_engine.py\", line 515, in from_engine_args\n    executor_class = cls._get_executor_cls(engine_config)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm/vllm-0.6.6.post1/vllm/engine/llm_engine.py\", line 453, in _get_executor_cls\n    initialize_ray_cluster(engine_config.parallel_config)\n  File \"/opt/vllm/vllm-0.6.6.post1/vllm/executor/ray_utils.py\", line 300, in initialize_ray_cluster\n    raise ValueError(\nValueError: Current node has no TPU available. current_node_resource={'ray-marin-us-central2-worker-2c310153-tpu': 1.0, 'CPU': 118.0, 'memory': 328490690150.0, 'object_store_memory': 32641751449.0, 'accelerator_type:TPU-V4': 1.0, 'node:10.130.0.8': 1.0}. vLLM engine cannot start without TPU. Make sure you have at least 1 TPU available in a node current_node_id='70354097fbebce320701224b766747b2c30936f9c1edf1d930d7723b' current_ip='10.130.0.8'.\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "tpu",
      "ray"
    ],
    "state": "closed",
    "created_at": "2025-01-14T21:32:12+00:00",
    "closed_at": "2025-01-24T05:41:50+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12058/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12058"
  },
  {
    "number": 10343,
    "title": "[Feature]: Allow head_size smaller than 128 on TPU with Pallas backend",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nI would like to serve smaller models (e.g facebook/opt-125m) using VLLM on TPU. I can't do this currently because the Pallas backend has the limitation `NotImplementedError: Head size must be a multiple of 128`. I can't find a reason why this limitation is in place, and it would be great to be able to remove it with a flag or entirely. If my understanding is incorrect and there is a reason to have this limitation in place, please let me know! Thanks for your work on VLLM.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "tpu"
    ],
    "state": "closed",
    "created_at": "2024-11-14T21:51:45+00:00",
    "closed_at": "2025-07-10T05:02:05+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10343/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10343"
  },
  {
    "number": 10155,
    "title": "[Installation]: VLLM does not support TPU v5p-16 (Multi-Host) with Ray Cluster",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n\r\nCollecting environment information...\r\nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\r\nINFO 11-04 16:11:44 importing.py:15] Triton not installed or not compatible; certain GPU-related functions will not be available.\r\nPyTorch version: 2.6.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\r\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\r\nClang version: Could not collect\r\nCMake version: version 3.30.5\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.15 (main, Oct 17 2024, 02:58:23) [GCC 10.2.1 20210110] (64-bit runtime)\r\nPython platform: Linux-5.19.0-1022-gcp-x86_64-with-glibc2.31\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   52 bits physical, 57 bits virtual\r\nCPU(s):                          208\r\nOn-line CPU(s) list:             0-207\r\nThread(s) per core:              2\r\nCore(s) per socket:              52\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           143\r\nModel name:                      Intel(R) Xeon(R) Platinum 8481C CPU @ 2.70GHz\r\nStepping:                        8\r\nCPU MHz:                         2699.998\r\nBogoMIPS:                        5399.99\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       4.9 MiB\r\nL1i cache:                       3.3 MiB\r\nL2 cache:                        208 MiB\r\nL3 cache:                        210 MiB\r\nNUMA node0 CPU(s):               0-51,104-155\r\nNUMA node1 CPU(s):               52-103,156-207\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rtm avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 arat avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk amx_bf16 avx512_fp16 amx_tile amx_int8 arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.6.0\r\n[pip3] torch-xla==2.6.0+gita0f81e5\r\n[pip3] torchvision==0.19.0a0+d23a6e1\r\n[pip3] transformers==4.46.1\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.3.post2.dev217+gccb5376a\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\r\n\n\n### How you are installing vllm\n\n# Create a TPU VM\r\n```\r\ngcloud compute tpus tpu-vm create tpu-v5p-benchmark \\\r\n  --zone=europe-west4-b \\\r\n  --accelerator-type=v5p-16 \\\r\n  --version=tpu-ubuntu2204-base\r\n```\r\n# On head node\r\n`ray start --block --head --port=6379`\r\n\r\n# On other node (Note: TPU v5p-16 has 2 nodes)\r\n`ray start --block --address=<head-node-address>:6379`\r\n\r\n#Below is the ray-status\r\n# Ray status shows both the nodes active\r\n```\r\n======== Autoscaler status: 2024-11-05 15:48:55.473751 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 node_49fc62d654acc1939448a1668ee1770feef20f763ab4bedface3ccf7\r\n 1 node_88790deb8765d60a25e104192e33414fedcdc89d40e339316235547c\r\nPending:\r\n (no pending nodes)\r\nRecent failures:\r\n (no failures)\r\n\r\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 0.0/416.0 CPU\r\n 0B/839.91GiB memory\r\n 0B/30.40GiB object_store_memory\r\n\r\nDemands:\r\n (no resource demands)\r\n```\r\nHowever, when I run vllm serve from master node  it issues error \"The number of required TPUs exceeds the total number of available TPUs in the placement group.\", even when it is connected to cluster.\r\nI have tried with --tensor-parallel-size 2, 4, 8, 16 and the output is same.\r\n\r\n```\r\n$ vllm serve /root/.llama/checkpoints/Llama3.1-70B --tensor-parallel-size 8\r\nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\r\nINFO 11-05 15:51:16 importing.py:15] Triton not installed or not compatible; certain GPU-related functions will not be available.\r\nINFO 11-05 15:51:18 api_server.py:551] vLLM API server version 0.6.3.post2.dev217+gccb5376a\r\nINFO 11-05 15:51:18 api_server.py:552] args: Namespace(subparser='serve', model_tag='/root/.llama/checkpoints/Llama3.1-70B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/root/.llama/checkpoints/Llama3.1-70B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', chat_template_text_format='string', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=8, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', pooling_type=None, pooling_norm=None, pooling_softmax=None, pooling_step_tag_id=None, pooling_returned_token_ids=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7ff7515e1750>)\r\nINFO 11-05 15:51:18 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/5be6afd9-60d4-4329-8202-6ecbec5a18be for IPC Path.\r\nINFO 11-05 15:51:18 api_server.py:181] Started engine process with PID 1132\r\nINFO 11-05 15:51:18 config.py:1752] Downcasting torch.float32 to torch.float16.\r\nINFO 11-05 15:51:21 importing.py:15] Triton not installed or not compatible; certain GPU-related functions will not be available.\r\nINFO 11-05 15:51:22 config.py:1752] Downcasting torch.float32 to torch.float16.\r\nINFO 11-05 15:51:23 config.py:323] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\r\nWARNING 11-05 15:51:23 arg_utils.py:1051] The model has a long context length (128000). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\r\nWARNING 11-05 15:51:23 arg_utils.py:1103] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\r\nYou are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\r\nYou are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\r\nINFO 11-05 15:51:27 config.py:323] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\r\nWARNING 11-05 15:51:27 arg_utils.py:1051] The model has a long context length (128000). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\r\nWARNING 11-05 15:51:27 arg_utils.py:1103] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\r\n2024-11-05 15:51:27,430 INFO worker.py:1631 -- Connecting to existing Ray cluster at address: 10.164.15.221:6379...\r\n2024-11-05 15:51:27,438 INFO worker.py:1807 -- Connected to Ray cluster. View the dashboard at 127.0.0.1:8265 \r\nProcess SpawnProcess-1:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/local/lib/python3.10/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 361, in run_mp_engine\r\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\r\n  File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 122, in from_engine_args\r\n    executor_class = LLMEngine._get_executor_cls(engine_config)\r\n  File \"/workspace/vllm/vllm/engine/llm_engine.py\", line 521, in _get_executor_cls\r\n    initialize_ray_cluster(engine_config.parallel_config)\r\n  File \"/workspace/vllm/vllm/executor/ray_utils.py\", line 277, in initialize_ray_cluster\r\n    raise ValueError(\r\nValueError: The number of required TPUs exceeds the total number of available TPUs in the placement group.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/vllm\", line 33, in <module>\r\n    sys.exit(load_entry_point('vllm', 'console_scripts', 'vllm')())\r\n  File \"/workspace/vllm/vllm/scripts.py\", line 195, in main\r\n    args.dispatch_function(args)\r\n  File \"/workspace/vllm/vllm/scripts.py\", line 41, in serve\r\n    uvloop.run(run_server(args))\r\n  File \"/usr/local/lib/python3.10/site-packages/uvloop/__init__.py\", line 82, in run\r\n    return loop.run_until_complete(wrapper())\r\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\r\n  File \"/usr/local/lib/python3.10/site-packages/uvloop/__init__.py\", line 61, in wrapper\r\n    return await main\r\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 575, in run_server\r\n    async with build_async_engine_client(args) as engine_client:\r\n  File \"/usr/local/lib/python3.10/contextlib.py\", line 199, in __aenter__\r\n    return await anext(self.gen)\r\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 107, in build_async_engine_client\r\n    async with build_async_engine_client_from_engine_args(\r\n  File \"/usr/local/lib/python3.10/contextlib.py\", line 199, in __aenter__\r\n    return await anext(self.gen)\r\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 197, in build_async_engine_client_from_engine_args\r\n    raise RuntimeError(\r\nRuntimeError: Engine process failed to start\r\n```\r\n\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "tpu",
      "ray"
    ],
    "state": "closed",
    "created_at": "2024-11-08T11:36:17+00:00",
    "closed_at": "2025-02-07T02:16:05+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10155/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10155"
  },
  {
    "number": 8318,
    "title": "[Bug]: vLLM crashes with larger context sizes on TPUs",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nINFO 09-10 05:05:28 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\r\nPyTorch version: 2.5.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\r\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.14 (main, Aug 13 2024, 02:16:06) [GCC 10.2.1 20210110] (64-bit runtime)\r\nPython platform: Linux-6.1.85+-x86_64-with-glibc2.31\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nByte Order:                           Little Endian\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nCPU(s):                               224\r\nOn-line CPU(s) list:                  0-223\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   56\r\nSocket(s):                            2\r\nNUMA node(s):                         2\r\nVendor ID:                            AuthenticAMD\r\nCPU family:                           25\r\nModel:                                1\r\nModel name:                           AMD EPYC 7B13\r\nStepping:                             0\r\nCPU MHz:                              2449.998\r\nBogoMIPS:                             4899.99\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            3.5 MiB\r\nL1i cache:                            3.5 MiB\r\nL2 cache:                             56 MiB\r\nL3 cache:                             448 MiB\r\nNUMA node0 CPU(s):                    0-55,112-167\r\nNUMA node1 CPU(s):                    56-111,168-223\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Mitigation; safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP conditional; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip vaes vpclmulqdq rdpid fsrm\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.0\r\n[pip3] torch-xla==2.5.0+git17a4ef5\r\n[pip3] torchvision==0.19.0a0+d23a6e1\r\n[pip3] transformers==4.44.2\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.0@1447c97e753919709b613590d7267c93d07d9382\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nI am trying to deploy llama3.1 8B instruct on GKE 2x4 v5e TPUs. The vLLM server boots up normally and works properly if `max-model-len` is very low (ie 1024), but crashes with higher context sizes (tried 16k and above). I'm using the Dockerfile.tpu, and the command `python3.10 -u -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --model meta-llama/Meta-Llama-3.1-8B-Instruct --tensor-parallel-size 8 --swap-space 16 --disable-log-requests --max-model-len=32768`\r\n\r\nHere is my full log file: \r\n[vllm-llama31-tpu.log](https://github.com/user-attachments/files/16939650/vllm-llama31-tpu.log)\r\n\r\nHere is my full k8s manifest:\r\n\r\n```yaml\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: llama3-8b-instruct-vllm-deployment\r\n  namespace: default\r\n  labels:\r\n    app: llama3-8b-instruct-vllm\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      app: llama3-8b-instruct-vllm\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: llama3-8b-instruct-vllm\r\n    spec:\r\n      nodeSelector:\r\n        cloud.google.com/gke-tpu-topology: 2x4\r\n        cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice\r\n      hostIPC: true\r\n      hostNetwork: true\r\n      containers:\r\n      - name: llama3-8b-instruct-vllm\r\n        image: us-central1-docker.pkg.dev/project-lighthouse-403916/lighthouse-production/vllm-server/vllm-tpu-image:latest\r\n        command: [\"/bin/sh\", \"-c\", \"python3.10 -u -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --model meta-llama/Meta-Llama-3.1-8B-Instruct --tensor-parallel-size 8 --swap-space 16 --disable-log-requests --max-model-len=32768\"]\r\n        env:\r\n        - name: HUGGING_FACE_HUB_TOKEN\r\n          value: <hf-token>\r\n        - name: VLLM_LOGGING_LEVEL\r\n          value: \"DEBUG\"\r\n        - name: VLLM_TRACE_FUNCTION\r\n          value: \"1\"\r\n        securityContext:\r\n          privileged: true\r\n        ports:\r\n        - containerPort: 8000\r\n          protocol: TCP\r\n        resources:\r\n          requests:\r\n            google.com/tpu: 8\r\n          limits:\r\n            google.com/tpu: 8\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tpu"
    ],
    "state": "closed",
    "created_at": "2024-09-10T05:06:36+00:00",
    "closed_at": "2024-10-16T20:53:42+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8318/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8318"
  },
  {
    "number": 8066,
    "title": "[Bug]: TPU InternVL2 Model Error Graph break due to unsupported builtin _XLAC.PyCapsule._xla_get_replication_devices_count",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```\r\nCollecting environment information...\r\nTraceback (most recent call last):\r\n  File \"/home/kojoe/EasyAnimate/easyanimate/image_caption/collect_env.py\", line 735, in <module>\r\n    main()\r\n  File \"/home/kojoe/EasyAnimate/easyanimate/image_caption/collect_env.py\", line 714, in main\r\n    output = get_pretty_env_info()\r\n  File \"/home/kojoe/EasyAnimate/easyanimate/image_caption/collect_env.py\", line 709, in get_pretty_env_info\r\n    return pretty_str(get_env_info())\r\n  File \"/home/kojoe/EasyAnimate/easyanimate/image_caption/collect_env.py\", line 510, in get_env_info\r\n    pip_version, pip_list_output = get_pip_packages(run_lambda)\r\n  File \"/home/kojoe/EasyAnimate/easyanimate/image_caption/collect_env.py\", line 480, in get_pip_packages\r\n    out = run_with_pip([sys.executable, '-mpip'])\r\n  File \"/home/kojoe/EasyAnimate/easyanimate/image_caption/collect_env.py\", line 476, in run_with_pip\r\n    return \"\\n\".join(line for line in out.splitlines()\r\nAttributeError: 'NoneType' object has no attribute 'splitlines'\r\n```\r\n\r\n</details>\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\n```\r\nfrom transformers import AutoTokenizer\r\nfrom vllm.assets.image import ImageAsset\r\nfrom vllm import LLM, SamplingParams\r\n\r\n# Input image and question\r\nimage = ImageAsset(\"cherry_blossom\").pil_image.convert(\"RGB\")\r\nquestion = \"Describe the image in detail.\"\r\n\r\n\r\n# InternVL\r\ndef run_internvl(question):\r\n    model_name = \"OpenGVLab/InternVL2-8B\"\r\n\r\n    llm = LLM(\r\n        model=model_name,\r\n        trust_remote_code=True,\r\n        gpu_memory_utilization=0.9,\r\n        tensor_parallel_size=4,\r\n        max_num_batched_tokens=8192,\r\n        max_model_len=4096,\r\n    )\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\r\n    messages = [{\"role\": \"user\", \"content\": f\"<image>\\n{question}\"}]\r\n    prompt = tokenizer.apply_chat_template(\r\n        messages, tokenize=False, add_generation_prompt=True\r\n    )\r\n\r\n    stop_tokens = [\"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\", \"<|end|>\"]\r\n    stop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\r\n    return llm, prompt, stop_token_ids\r\n\r\n\r\nmodel_example_map = {\r\n    \"internvl_chat\": run_internvl,\r\n}\r\n\r\n\r\ndef main():\r\n    model = \"internvl_chat\"\r\n    if model not in model_example_map:\r\n        raise ValueError(f\"Model type {model} is not supported.\")\r\n\r\n    llm, prompt, stop_token_ids = model_example_map[model](question)\r\n\r\n    sampling_params = SamplingParams(\r\n        temperature=0.2, max_tokens=64, stop_token_ids=stop_token_ids\r\n    )\r\n\r\n    # Single inference\r\n    inputs = {\r\n        \"prompt\": prompt,\r\n        \"multi_modal_data\": {\"image\": image},\r\n    }\r\n    print(\"!!!loaded model successfully!!!\")\r\n    outputs = llm.generate(inputs, sampling_params=sampling_params)\r\n    print(\"!!!start generate output!!!\")\r\n    for o in outputs:\r\n        generated_text = o.outputs[0].text\r\n        print(generated_text)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n\r\n\r\n```\r\npython test.py\r\nINFO 09-01 20:56:20 config.py:877] Defaulting to use ray for distributed inference\r\n2024-09-01 20:56:22,390 INFO worker.py:1774 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265\r\nINFO 09-01 20:56:24 llm_engine.py:212] Initializing an LLM engine (v0.5.5) with config: model='OpenGVLab/InternVL2-8B', speculative_config=None, tokenizer='OpenGVLab/InternVL2-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=None, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=OpenGVLab/InternVL2-8B, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\r\nWARNING 09-01 20:56:24 tokenizer.py:157] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\r\n(RayWorkerWrapper pid=1592655) WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\r\n(RayWorkerWrapper pid=1592655) INFO 09-01 20:56:43 selector.py:198] Cannot use _Backend.FLASH_ATTN backend on TPU.\r\n(RayWorkerWrapper pid=1592655) INFO 09-01 20:56:43 selector.py:146] Using Pallas backend.\r\nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\r\nINFO 09-01 20:56:43 selector.py:198] Cannot use _Backend.FLASH_ATTN backend on TPU.\r\nINFO 09-01 20:56:43 selector.py:146] Using Pallas backend.\r\nINFO 09-01 20:56:46 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f63652a62c0>, local_subscribe_port=52059, remote_subscribe_port=None)\r\n(RayWorkerWrapper pid=1592556) INFO 09-01 20:56:47 weight_utils.py:236] Using model weights format ['*.safetensors']\r\nINFO 09-01 20:56:48 weight_utils.py:236] Using model weights format ['*.safetensors']\r\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00, 16.45it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:00<00:00,  8.83it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:00<00:00,  9.48it/s]\r\n\r\n/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:731: UserWarning: Graph break due to unsupported builtin _XLAC.PyCapsule._xla_get_replication_devices_count. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\r\n  torch._dynamo.utils.warn_once(msg)\r\nERROR 09-01 20:56:51 worker_base.py:464] Error executing method determine_num_available_blocks. This might cause deadlock in distributed execution.\r\nERROR 09-01 20:56:51 worker_base.py:464] Traceback (most recent call last):\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/worker/worker_base.py\", line 456, in execute_method\r\nERROR 09-01 20:56:51 worker_base.py:464]     return executor(*args, **kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/worker/tpu_worker.py\", line 118, in determine_num_available_blocks\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.model_runner._dummy_run(\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/worker/tpu_model_runner.py\", line 237, in _dummy_run\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.model(token_ids,\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/worker/tpu_model_runner.py\", line 628, in __call__\r\nERROR 09-01 20:56:51 worker_base.py:464]     return self.compiled_callable(*args, **kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 465, in _fn\r\nERROR 09-01 20:56:51 worker_base.py:464]     return fn(*args, **kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1244, in __call__\r\nERROR 09-01 20:56:51 worker_base.py:464]     return self._torchdynamo_orig_callable(\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 516, in __call__\r\nERROR 09-01 20:56:51 worker_base.py:464]     return _compile(\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 908, in _compile\r\nERROR 09-01 20:56:51 worker_base.py:464]     guarded_code = compile_inner(code, one_graph, hooks, transform)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 656, in compile_inner\r\nERROR 09-01 20:56:51 worker_base.py:464]     return _compile_inner(code, one_graph, hooks, transform)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_utils_internal.py\", line 87, in wrapper_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return function(*args, **kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 689, in _compile_inner\r\nERROR 09-01 20:56:51 worker_base.py:464]     out_code = transform_code_object(code, transform)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\r\nERROR 09-01 20:56:51 worker_base.py:464]     transformations(instructions, code_options)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 210, in _fn\r\nERROR 09-01 20:56:51 worker_base.py:464]     return fn(*args, **kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 624, in transform\r\nERROR 09-01 20:56:51 worker_base.py:464]     tracer.run()\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\r\nERROR 09-01 20:56:51 worker_base.py:464]     super().run()\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\nERROR 09-01 20:56:51 worker_base.py:464]     while self.step():\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\nERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, {})\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/lazy.py\", line 156, in realize_and_forward\r\nERROR 09-01 20:56:51 worker_base.py:464]     return getattr(self.realize(), name)(*args, **kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py\", line 899, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return variables.UserFunctionVariable(fn, source=source).call_function(\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\nERROR 09-01 20:56:51 worker_base.py:464]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\nERROR 09-01 20:56:51 worker_base.py:464]     return cls.inline_call_(parent, func, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\nERROR 09-01 20:56:51 worker_base.py:464]     tracer.run()\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\nERROR 09-01 20:56:51 worker_base.py:464]     while self.step():\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\nERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1692, in CALL_FUNCTION_KW\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/lazy.py\", line 156, in realize_and_forward\r\nERROR 09-01 20:56:51 worker_base.py:464]     return getattr(self.realize(), name)(*args, **kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py\", line 899, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return variables.UserFunctionVariable(fn, source=source).call_function(\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\nERROR 09-01 20:56:51 worker_base.py:464]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\nERROR 09-01 20:56:51 worker_base.py:464]     return cls.inline_call_(parent, func, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\nERROR 09-01 20:56:51 worker_base.py:464]     tracer.run()\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\nERROR 09-01 20:56:51 worker_base.py:464]     while self.step():\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\nERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, {})\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/lazy.py\", line 156, in realize_and_forward\r\nERROR 09-01 20:56:51 worker_base.py:464]     return getattr(self.realize(), name)(*args, **kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py\", line 899, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return variables.UserFunctionVariable(fn, source=source).call_function(\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\nERROR 09-01 20:56:51 worker_base.py:464]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\nERROR 09-01 20:56:51 worker_base.py:464]     return cls.inline_call_(parent, func, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\nERROR 09-01 20:56:51 worker_base.py:464]     tracer.run()\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\nERROR 09-01 20:56:51 worker_base.py:464]     while self.step():\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\nERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, {})\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\nERROR 09-01 20:56:51 worker_base.py:464]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\nERROR 09-01 20:56:51 worker_base.py:464]     return cls.inline_call_(parent, func, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\nERROR 09-01 20:56:51 worker_base.py:464]     tracer.run()\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\nERROR 09-01 20:56:51 worker_base.py:464]     while self.step():\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\nERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, {})\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 385, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\nERROR 09-01 20:56:51 worker_base.py:464]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\nERROR 09-01 20:56:51 worker_base.py:464]     return cls.inline_call_(parent, func, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\nERROR 09-01 20:56:51 worker_base.py:464]     tracer.run()\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\nERROR 09-01 20:56:51 worker_base.py:464]     while self.step():\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\nERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, {})\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 385, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\nERROR 09-01 20:56:51 worker_base.py:464]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\nERROR 09-01 20:56:51 worker_base.py:464]     return cls.inline_call_(parent, func, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\nERROR 09-01 20:56:51 worker_base.py:464]     tracer.run()\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\nERROR 09-01 20:56:51 worker_base.py:464]     while self.step():\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\nERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, {})\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\nERROR 09-01 20:56:51 worker_base.py:464]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\nERROR 09-01 20:56:51 worker_base.py:464]     return cls.inline_call_(parent, func, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\nERROR 09-01 20:56:51 worker_base.py:464]     tracer.run()\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\nERROR 09-01 20:56:51 worker_base.py:464]     while self.step():\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\nERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, {})\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\nERROR 09-01 20:56:51 worker_base.py:464]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\nERROR 09-01 20:56:51 worker_base.py:464]     return cls.inline_call_(parent, func, args, kwargs)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\nERROR 09-01 20:56:51 worker_base.py:464]     tracer.run()\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\nERROR 09-01 20:56:51 worker_base.py:464]     while self.step():\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\nERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, {})\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 733, in call_function\r\nERROR 09-01 20:56:51 worker_base.py:464]     unimplemented(msg)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 289, in unimplemented\r\nERROR 09-01 20:56:51 worker_base.py:464]     raise Unsupported(msg, case_name=case_name)\r\nERROR 09-01 20:56:51 worker_base.py:464] torch._dynamo.exc.Unsupported: Graph break due to unsupported builtin _XLAC.PyCapsule._xla_get_replication_devices_count. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\r\nERROR 09-01 20:56:51 worker_base.py:464]\r\nERROR 09-01 20:56:51 worker_base.py:464] from user code:\r\nERROR 09-01 20:56:51 worker_base.py:464]    File \"/home/kojoe/vllm/vllm/worker/tpu_model_runner.py\", line 701, in forward\r\nERROR 09-01 20:56:51 worker_base.py:464]     hidden_states = self.model(\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/model_executor/models/internvl.py\", line 460, in forward\r\nERROR 09-01 20:56:51 worker_base.py:464]     hidden_states = self.language_model.model(input_ids,\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/model_executor/models/internlm2.py\", line 246, in forward\r\nERROR 09-01 20:56:51 worker_base.py:464]     hidden_states = self.tok_embeddings(input_ids)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 403, in forward\r\nERROR 09-01 20:56:51 worker_base.py:464]     output = tensor_model_parallel_all_reduce(output_parallel)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/distributed/communication_op.py\", line 11, in tensor_model_parallel_all_reduce\r\nERROR 09-01 20:56:51 worker_base.py:464]     return get_tp_group().all_reduce(input_)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/distributed/parallel_state.py\", line 280, in all_reduce\r\nERROR 09-01 20:56:51 worker_base.py:464]     return tpu_comm.all_reduce(input_)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 57, in all_reduce\r\nERROR 09-01 20:56:51 worker_base.py:464]     return xm.all_reduce(xm.REDUCE_SUM, x)\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch_xla/core/xla_model.py\", line 447, in all_reduce\r\nERROR 09-01 20:56:51 worker_base.py:464]     if runtime.world_size() == 1 and not xu.getenv_as('XLA_ALWAYS_ALLREDUCE',\r\nERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch_xla/runtime.py\", line 141, in world_size\r\nERROR 09-01 20:56:51 worker_base.py:464]     if torch_xla._XLAC._xla_get_replication_devices_count() == 0:\r\nERROR 09-01 20:56:51 worker_base.py:464]\r\nERROR 09-01 20:56:51 worker_base.py:464] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\r\nERROR 09-01 20:56:51 worker_base.py:464]\r\nERROR 09-01 20:56:51 worker_base.py:464]\r\nERROR 09-01 20:56:51 worker_base.py:464] You can suppress this exception and fall back to eager by setting:\r\nERROR 09-01 20:56:51 worker_base.py:464]     import torch._dynamo\r\nERROR 09-01 20:56:51 worker_base.py:464]     torch._dynamo.config.suppress_errors = True\r\nERROR 09-01 20:56:51 worker_base.py:464]\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/home/kojoe/EasyAnimate/easyanimate/image_caption/test.py\", line 64, in <module>\r\n[rank0]:     main()\r\n[rank0]:   File \"/home/kojoe/EasyAnimate/easyanimate/image_caption/test.py\", line 44, in main\r\n[rank0]:     llm, prompt, stop_token_ids = model_example_map[model](question)\r\n[rank0]:   File \"/home/kojoe/EasyAnimate/easyanimate/image_caption/test.py\", line 14, in run_internvl\r\n[rank0]:     llm = LLM(\r\n[rank0]:   File \"/home/kojoe/vllm/vllm/entrypoints/llm.py\", line 177, in __init__\r\n[rank0]:     self.llm_engine = LLMEngine.from_engine_args(\r\n[rank0]:   File \"/home/kojoe/vllm/vllm/engine/llm_engine.py\", line 541, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:   File \"/home/kojoe/vllm/vllm/engine/llm_engine.py\", line 316, in __init__\r\n[rank0]:     self._initialize_kv_caches()\r\n[rank0]:   File \"/home/kojoe/vllm/vllm/engine/llm_engine.py\", line 451, in _initialize_kv_caches\r\n[rank0]:     self.model_executor.determine_num_available_blocks())\r\n[rank0]:   File \"/home/kojoe/vllm/vllm/executor/ray_tpu_executor.py\", line 249, in determine_num_available_blocks\r\n[rank0]:     num_blocks = self._run_workers(\"determine_num_available_blocks\", )\r\n[rank0]:   File \"/home/kojoe/vllm/vllm/executor/ray_tpu_executor.py\", line 230, in _run_workers\r\n[rank0]:     driver_worker_output = self.driver_worker.execute_method(\r\n[rank0]:   File \"/home/kojoe/vllm/vllm/worker/worker_base.py\", line 465, in execute_method\r\n[rank0]:     raise e\r\n[rank0]:   File \"/home/kojoe/vllm/vllm/worker/worker_base.py\", line 456, in execute_method\r\n[rank0]:     return executor(*args, **kwargs)\r\n[rank0]:   File \"/home/kojoe/vllm/vllm/worker/tpu_worker.py\", line 118, in determine_num_available_blocks\r\n[rank0]:     self.model_runner._dummy_run(\r\n[rank0]:   File \"/home/kojoe/vllm/vllm/worker/tpu_model_runner.py\", line 237, in _dummy_run\r\n[rank0]:     self.model(token_ids,\r\n[rank0]:   File \"/home/kojoe/vllm/vllm/worker/tpu_model_runner.py\", line 628, in __call__\r\n[rank0]:     return self.compiled_callable(*args, **kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 465, in _fn\r\n[rank0]:     return fn(*args, **kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1244, in __call__\r\n[rank0]:     return self._torchdynamo_orig_callable(\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 516, in __call__\r\n[rank0]:     return _compile(\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 908, in _compile\r\n[rank0]:     guarded_code = compile_inner(code, one_graph, hooks, transform)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 656, in compile_inner\r\n[rank0]:     return _compile_inner(code, one_graph, hooks, transform)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_utils_internal.py\", line 87, in wrapper_function\r\n[rank0]:     return function(*args, **kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 689, in _compile_inner\r\n[rank0]:     out_code = transform_code_object(code, transform)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\r\n[rank0]:     transformations(instructions, code_options)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 210, in _fn\r\n[rank0]:     return fn(*args, **kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 624, in transform\r\n[rank0]:     tracer.run()\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\r\n[rank0]:     super().run()\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n[rank0]:     while self.step():\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n[rank0]:     self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n[rank0]:     return inner_fn(self, inst)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\n[rank0]:     self.call_function(fn, args, {})\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n[rank0]:     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/lazy.py\", line 156, in realize_and_forward\r\n[rank0]:     return getattr(self.realize(), name)(*args, **kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py\", line 899, in call_function\r\n[rank0]:     return variables.UserFunctionVariable(fn, source=source).call_function(\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n[rank0]:     return super().call_function(tx, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\n[rank0]:     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n[rank0]:     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n[rank0]:     return cls.inline_call_(parent, func, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n[rank0]:     tracer.run()\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n[rank0]:     while self.step():\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n[rank0]:     self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n[rank0]:     return inner_fn(self, inst)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1692, in CALL_FUNCTION_KW\r\n[rank0]:     self.call_function(fn, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n[rank0]:     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/lazy.py\", line 156, in realize_and_forward\r\n[rank0]:     return getattr(self.realize(), name)(*args, **kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py\", line 899, in call_function\r\n[rank0]:     return variables.UserFunctionVariable(fn, source=source).call_function(\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n[rank0]:     return super().call_function(tx, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\n[rank0]:     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n[rank0]:     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n[rank0]:     return cls.inline_call_(parent, func, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n[rank0]:     tracer.run()\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n[rank0]:     while self.step():\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n[rank0]:     self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n[rank0]:     return inner_fn(self, inst)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\n[rank0]:     self.call_function(fn, args, {})\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n[rank0]:     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/lazy.py\", line 156, in realize_and_forward\r\n[rank0]:     return getattr(self.realize(), name)(*args, **kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py\", line 899, in call_function\r\n[rank0]:     return variables.UserFunctionVariable(fn, source=source).call_function(\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n[rank0]:     return super().call_function(tx, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\n[rank0]:     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n[rank0]:     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n[rank0]:     return cls.inline_call_(parent, func, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n[rank0]:     tracer.run()\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n[rank0]:     while self.step():\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n[rank0]:     self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n[rank0]:     return inner_fn(self, inst)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\n[rank0]:     self.call_function(fn, args, {})\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n[rank0]:     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n[rank0]:     return super().call_function(tx, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\n[rank0]:     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n[rank0]:     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n[rank0]:     return cls.inline_call_(parent, func, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n[rank0]:     tracer.run()\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n[rank0]:     while self.step():\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n[rank0]:     self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n[rank0]:     return inner_fn(self, inst)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\n[rank0]:     self.call_function(fn, args, {})\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n[rank0]:     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 385, in call_function\r\n[rank0]:     return super().call_function(tx, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n[rank0]:     return super().call_function(tx, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\n[rank0]:     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n[rank0]:     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n[rank0]:     return cls.inline_call_(parent, func, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n[rank0]:     tracer.run()\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n[rank0]:     while self.step():\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n[rank0]:     self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n[rank0]:     return inner_fn(self, inst)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\n[rank0]:     self.call_function(fn, args, {})\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n[rank0]:     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 385, in call_function\r\n[rank0]:     return super().call_function(tx, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n[rank0]:     return super().call_function(tx, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\n[rank0]:     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n[rank0]:     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n[rank0]:     return cls.inline_call_(parent, func, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n[rank0]:     tracer.run()\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n[rank0]:     while self.step():\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n[rank0]:     self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n[rank0]:     return inner_fn(self, inst)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\n[rank0]:     self.call_function(fn, args, {})\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n[rank0]:     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n[rank0]:     return super().call_function(tx, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\n[rank0]:     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n[rank0]:     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n[rank0]:     return cls.inline_call_(parent, func, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n[rank0]:     tracer.run()\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n[rank0]:     while self.step():\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n[rank0]:     self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n[rank0]:     return inner_fn(self, inst)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\n[rank0]:     self.call_function(fn, args, {})\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n[rank0]:     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n[rank0]:     return super().call_function(tx, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\n[rank0]:     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n[rank0]:     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n[rank0]:     return cls.inline_call_(parent, func, args, kwargs)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n[rank0]:     tracer.run()\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n[rank0]:     while self.step():\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n[rank0]:     self.dispatch_table[inst.opcode](self, inst)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n[rank0]:     return inner_fn(self, inst)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\n[rank0]:     self.call_function(fn, args, {})\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n[rank0]:     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 733, in call_function\r\n[rank0]:     unimplemented(msg)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 289, in unimplemented\r\n[rank0]:     raise Unsupported(msg, case_name=case_name)\r\n[rank0]: torch._dynamo.exc.Unsupported: Graph break due to unsupported builtin _XLAC.PyCapsule._xla_get_replication_devices_count. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\r\n\r\n[rank0]: from user code:\r\n[rank0]:    File \"/home/kojoe/vllm/vllm/worker/tpu_model_runner.py\", line 701, in forward\r\n[rank0]:     hidden_states = self.model(\r\n[rank0]:   File \"/home/kojoe/vllm/vllm/model_executor/models/internvl.py\", line 460, in forward\r\n[rank0]:     hidden_states = self.language_model.model(input_ids,\r\n[rank0]:   File \"/home/kojoe/vllm/vllm/model_executor/models/internlm2.py\", line 246, in forward\r\n[rank0]:     hidden_states = self.tok_embeddings(input_ids)\r\n[rank0]:   File \"/home/kojoe/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 403, in forward\r\n[rank0]:     output = tensor_model_parallel_all_reduce(output_parallel)\r\n[rank0]:   File \"/home/kojoe/vllm/vllm/distributed/communication_op.py\", line 11, in tensor_model_parallel_all_reduce\r\n[rank0]:     return get_tp_group().all_reduce(input_)\r\n[rank0]:   File \"/home/kojoe/vllm/vllm/distributed/parallel_state.py\", line 280, in all_reduce\r\n[rank0]:     return tpu_comm.all_reduce(input_)\r\n[rank0]:   File \"/home/kojoe/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 57, in all_reduce\r\n[rank0]:     return xm.all_reduce(xm.REDUCE_SUM, x)\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch_xla/core/xla_model.py\", line 447, in all_reduce\r\n[rank0]:     if runtime.world_size() == 1 and not xu.getenv_as('XLA_ALWAYS_ALLREDUCE',\r\n[rank0]:   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch_xla/runtime.py\", line 141, in world_size\r\n[rank0]:     if torch_xla._XLAC._xla_get_replication_devices_count() == 0:\r\n\r\n[rank0]: Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\r\n\r\n\r\n[rank0]: You can suppress this exception and fall back to eager by setting:\r\n[rank0]:     import torch._dynamo\r\n[rank0]:     torch._dynamo.config.suppress_errors = True\r\n\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464] Error executing method determine_num_available_blocks. This might cause deadlock in distributed execution.\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464] Traceback (most recent call last):\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/worker/worker_base.py\", line 456, in execute_method\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return executor(*args, **kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/worker/tpu_worker.py\", line 118, in determine_num_available_blocks\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.model_runner._dummy_run(\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/worker/tpu_model_runner.py\", line 237, in _dummy_run\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.model(token_ids,\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/worker/tpu_model_runner.py\", line 628, in __call__\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return self.compiled_callable(*args, **kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 465, in _fn\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return fn(*args, **kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1244, in __call__\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return self._torchdynamo_orig_callable(\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 516, in __call__\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return _compile(\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 908, in _compile\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     guarded_code = compile_inner(code, one_graph, hooks, transform)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 656, in compile_inner\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return _compile_inner(code, one_graph, hooks, transform)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_utils_internal.py\", line 87, in wrapper_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return function(*args, **kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 689, in _compile_inner\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     out_code = transform_code_object(code, transform)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     transformations(instructions, code_options)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 210, in _fn\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return fn(*args, **kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 624, in transform\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     tracer.run()\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     super().run()\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     while self.step():\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, {})\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/lazy.py\", line 156, in realize_and_forward\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return getattr(self.realize(), name)(*args, **kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py\", line 899, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return variables.UserFunctionVariable(fn, source=source).call_function(\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return cls.inline_call_(parent, func, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     tracer.run()\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     while self.step():\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1692, in CALL_FUNCTION_KW\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/lazy.py\", line 156, in realize_and_forward\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return getattr(self.realize(), name)(*args, **kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py\", line 899, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return variables.UserFunctionVariable(fn, source=source).call_function(\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return cls.inline_call_(parent, func, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     tracer.run()\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     while self.step():\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, {})\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/lazy.py\", line 156, in realize_and_forward\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return getattr(self.realize(), name)(*args, **kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py\", line 899, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return variables.UserFunctionVariable(fn, source=source).call_function(\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return cls.inline_call_(parent, func, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     tracer.run()\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     while self.step():\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, {})\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return cls.inline_call_(parent, func, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     tracer.run()\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     while self.step():\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, {})\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 385, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return cls.inline_call_(parent, func, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     tracer.run()\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     while self.step():\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, {})\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 385, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return cls.inline_call_(parent, func, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     tracer.run()\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     while self.step():\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, {})\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return cls.inline_call_(parent, func, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     tracer.run()\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     while self.step():\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, {})\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 324, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 108, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return cls.inline_call_(parent, func, args, kwargs)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     tracer.run()\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     while self.step():\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, {})\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 733, in call_function\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     unimplemented(msg)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 289, in unimplemented\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     raise Unsupported(msg, case_name=case_name)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464] torch._dynamo.exc.Unsupported: Graph break due to unsupported builtin _XLAC.PyCapsule._xla_get_replication_devices_count. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464] from user code:\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]    File \"/home/kojoe/vllm/vllm/worker/tpu_model_runner.py\", line 701, in forward\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     hidden_states = self.model(\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/model_executor/models/internvl.py\", line 460, in forward\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     hidden_states = self.language_model.model(input_ids,\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/model_executor/models/internlm2.py\", line 246, in forward\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     hidden_states = self.tok_embeddings(input_ids)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 403, in forward\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     output = tensor_model_parallel_all_reduce(output_parallel)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/distributed/communication_op.py\", line 11, in tensor_model_parallel_all_reduce\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return get_tp_group().all_reduce(input_)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/distributed/parallel_state.py\", line 280, in all_reduce\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return tpu_comm.all_reduce(input_)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 57, in all_reduce\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     return xm.all_reduce(xm.REDUCE_SUM, x)\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch_xla/core/xla_model.py\", line 447, in all_reduce\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     if runtime.world_size() == 1 and not xu.getenv_as('XLA_ALWAYS_ALLREDUCE',\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch_xla/runtime.py\", line 141, in world_size\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     if torch_xla._XLAC._xla_get_replication_devices_count() == 0:\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464] You can suppress this exception and fall back to eager by setting:\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     import torch._dynamo\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]     torch._dynamo.config.suppress_errors = True\r\n(RayWorkerWrapper pid=1592556) ERROR 09-01 20:56:51 worker_base.py:464]\r\n(RayWorkerWrapper pid=1592556) INFO 09-01 20:56:43 selector.py:198] Cannot use _Backend.FLASH_ATTN backend on TPU. [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\r\n(RayWorkerWrapper pid=1592556) INFO 09-01 20:56:43 selector.py:146] Using Pallas backend. [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592556) /home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:731: UserWarning: Graph break due to unsupported builtin _XLAC.PyCapsule._xla_get_replication_devices_count. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\r\n(RayWorkerWrapper pid=1592556)   torch._dynamo.utils.warn_once(msg)\r\n(RayWorkerWrapper pid=1592556) WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU. [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) INFO 09-01 20:56:47 weight_utils.py:236] Using model weights format ['*.safetensors'] [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464] Error executing method determine_num_available_blocks. This might cause deadlock in distributed execution. [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464] Traceback (most recent call last): [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/worker/worker_base.py\", line 456, in execute_method [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     return executor(*args, **kwargs) [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/worker/tpu_worker.py\", line 118, in determine_num_available_blocks [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     self.model_runner._dummy_run( [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/worker/tpu_model_runner.py\", line 237, in _dummy_run [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     self.model(token_ids, [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/worker/tpu_model_runner.py\", line 628, in __call__ [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     return self.compiled_callable(*args, **kwargs) [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 210, in _fn [repeated 4x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     return fn(*args, **kwargs) [repeated 4x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 516, in __call__ [repeated 4x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     return self._torchdynamo_orig_callable( [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     return _compile( [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 908, in _compile [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     guarded_code = compile_inner(code, one_graph, hooks, transform) [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 656, in compile_inner [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     return _compile_inner(code, one_graph, hooks, transform) [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_utils_internal.py\", line 87, in wrapper_function [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     return function(*args, **kwargs) [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 689, in _compile_inner [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     out_code = transform_code_object(code, transform) [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     transformations(instructions, code_options) [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 624, in transform [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     tracer.run() [repeated 18x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run [repeated 20x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     super().run() [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     while self.step(): [repeated 18x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step [repeated 18x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     self.dispatch_table[inst.opcode](self, inst) [repeated 18x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 582, in wrapper [repeated 18x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     return inner_fn(self, inst) [repeated 18x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1602, in CALL_FUNCTION [repeated 16x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, {}) [repeated 16x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 733, in call_function [repeated 62x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type] [repeated 18x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/lazy.py\", line 156, in realize_and_forward [repeated 6x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     return getattr(self.realize(), name)(*args, **kwargs) [repeated 6x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     return variables.UserFunctionVariable(fn, source=source).call_function( [repeated 6x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     return super().call_function(tx, args, kwargs) [repeated 20x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs) [repeated 16x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 836, in inline_user_function_return [repeated 16x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs) [repeated 16x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3011, in inline_call [repeated 16x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     return cls.inline_call_(parent, func, args, kwargs) [repeated 16x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 3139, in inline_call_ [repeated 16x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1692, in CALL_FUNCTION_KW [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     self.call_function(fn, args, kwargs) [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     unimplemented(msg) [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 289, in unimplemented [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     raise Unsupported(msg, case_name=case_name) [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464] torch._dynamo.exc.Unsupported: Graph break due to unsupported builtin _XLAC.PyCapsule._xla_get_replication_devices_count. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph. [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]  [repeated 10x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464] from user code: [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]    File \"/home/kojoe/vllm/vllm/worker/tpu_model_runner.py\", line 701, in forward [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     hidden_states = self.model( [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/model_executor/models/internvl.py\", line 460, in forward [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     hidden_states = self.language_model.model(input_ids, [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/model_executor/models/internlm2.py\", line 246, in forward [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     hidden_states = self.tok_embeddings(input_ids) [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py\", line 403, in forward [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     output = tensor_model_parallel_all_reduce(output_parallel) [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/distributed/communication_op.py\", line 11, in tensor_model_parallel_all_reduce [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     return get_tp_group().all_reduce(input_) [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/distributed/parallel_state.py\", line 280, in all_reduce [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     return tpu_comm.all_reduce(input_) [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 57, in all_reduce [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     return xm.all_reduce(xm.REDUCE_SUM, x) [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch_xla/core/xla_model.py\", line 447, in all_reduce [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     if runtime.world_size() == 1 and not xu.getenv_as('XLA_ALWAYS_ALLREDUCE', [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]   File \"/home/kojoe/.local/lib/python3.10/site-packages/torch_xla/runtime.py\", line 141, in world_size [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     if torch_xla._XLAC._xla_get_replication_devices_count() == 0: [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464] You can suppress this exception and fall back to eager by setting: [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     import torch._dynamo [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) ERROR 09-01 20:56:51 worker_base.py:464]     torch._dynamo.config.suppress_errors = True [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751) /home/kojoe/.local/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:731: UserWarning: Graph break due to unsupported builtin _XLAC.PyCapsule._xla_get_replication_devices_count. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph. [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=1592751)   torch._dynamo.utils.warn_once(msg) [repeated 2x across cluster]\r\n```\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tpu",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-01T20:04:57+00:00",
    "closed_at": "2025-01-02T01:59:36+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8066/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8066"
  },
  {
    "number": 7989,
    "title": "[Bug]: TPU 'TYPE' property not found in Pallas backend",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nINFO 08-29 08:52:05 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\r\nPyTorch version: 2.5.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: Could not collect\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: 14.0.0-1ubuntu1.1\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.1.85+-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: 12.5.40\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.1.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               24\r\nOn-line CPU(s) list:                  0-23\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD EPYC 7B13\r\nCPU family:                           25\r\nModel:                                1\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   12\r\nSocket(s):                            1\r\nStepping:                             0\r\nBogoMIPS:                             4899.99\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip vaes vpclmulqdq rdpid fsrm\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            384 KiB (12 instances)\r\nL1i cache:                            384 KiB (12 instances)\r\nL2 cache:                             6 MiB (12 instances)\r\nL3 cache:                             64 MiB (2 instances)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-23\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Mitigation; safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP conditional; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-dali-cuda120==1.37.1\r\n[pip3] nvidia-nvimgcodec-cu12==0.2.0.7\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.0\r\n[pip3] torch-xla==2.5.0+git17a4ef5\r\n[pip3] transformers==4.44.2\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.5@09c7792610ada9f88bbf87d32b472dd44bf23cc2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\r\n\r\n</details>\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI was trying to deploy VLLM on GKE Autopilot with v5e accelerators. However  the Pallas backend seems to wrongly query the accelerator TYPE. By changing line line 126 (v0.5.5) to the following resolves this issue (file attention/backends/pallas.py):\r\n\r\n```python\r\n        tpu_env = torch_xla.tpu.get_tpu_env()\r\n        tpu_type = (tpu_env[\"TYPE\"] if \"TYPE\" in tpu_env else tpu_env[\"ACCELERATOR_TYPE\"]).lower()\r\n```\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tpu"
    ],
    "state": "closed",
    "created_at": "2024-08-29T08:51:07+00:00",
    "closed_at": "2024-08-30T07:31:48+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7989/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7989"
  },
  {
    "number": 7607,
    "title": "[Feature]: Enable Prefix caching kernel on Pallas for TPU backend",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\r\n\r\nEnable Prefix caching kernel on Pallas for TPU backend\r\n\r\nAccording to @WoosukKwon, we have a Triton and CUDA kernel implementations.\r\n\r\n* https://github.com/vllm-project/vllm/blob/main/vllm/attention/ops/prefix_prefill.py\r\n* https://github.com/vllm-project/vllm/issues/2614",
    "labels": [
      "feature request",
      "tpu",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-16T19:30:19+00:00",
    "closed_at": "2024-12-16T02:08:36+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7607/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7607"
  },
  {
    "number": 7485,
    "title": "[TPU] Make sure worker index aligns with node boundary",
    "body": "In ray gpu executor, there are these lines:\r\n\r\nhttps://github.com/vllm-project/vllm/blob/7025b11d949b4efeb2584690c35f919c77027368/vllm/executor/ray_gpu_executor.py#L175-L191\r\n\r\nto make sure the worker index aligns with machine boundary. you might need it in TPU, too. Otherwise local ranks can be wrong. for example, rank 0, 1, 2, 4 in one node, and 3, 5, 6, 7 in another node.\r\n\r\n_Originally posted by @youkaichao in https://github.com/vllm-project/vllm/issues/7457#issuecomment-2285381534_",
    "labels": [
      "tpu"
    ],
    "state": "closed",
    "created_at": "2024-08-13T23:13:35+00:00",
    "closed_at": "2024-09-02T06:09:47+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7485/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7485"
  },
  {
    "number": 3620,
    "title": "[RFC] Initial Support for Cloud TPUs",
    "body": "# Progress\r\n\r\n- [x] Implement TPU executor that works on a single TPU chip (without tensor parallelism) #5292 \r\n- [x] Support single-host tensor parallel inference #5871 \r\n- [x] Support multi-host tensor parallel inference #7457 \r\n- [ ] Support INT8 quantization\r\n- [x] Support MoE models such as Mixtral #6457\r\n- [ ] Benchmark and optimize the TPU backend performance\r\n\r\n# Project Scope\r\n\r\nThis project focuses on making vLLM compatible with Google cloud TPUs. Our goal is seamless integration so users can easily run vLLM on TPUs for both online and offline inference. We will target common setups, like popular models such as Gemma, using the bfloat16 data type.\r\n\r\n## Target TPUs and Models\r\n\r\nWe will focus on the most recent generations of TPUs, namely **TPU v4, v5e, and v5p**, considering their superior performance to previous generations. We will start by making sure vLLM works with dense models such as Gemma. After that, we will expand support to Mixture-of-Experts (MoE) models such as Mixtral.\r\n\r\n## Features Not Included (for now)\r\nThe following features are outside the scope of this initial project, but we'd like to tackle them in the future:\r\n\r\n- Speculative decoding\r\n- GPTQ/AWQ Quantization\r\n- Multi-LoRA serving\r\n\r\n# Design\r\n\r\n## Overview\r\n\r\n<img width=\"1042\" alt=\"Screenshot 2024-03-25 at 10 43 50\u202fAM\" src=\"https://github.com/vllm-project/vllm/assets/46394894/74033fa2-5472-439b-bdc7-568a6ba169d2\">\r\n\r\nTo integrate the TPU backend into vLLM, we will add the new TPU executor and TPU worker which are counterparts of the GPU executor and GPU worker, respectively. Unlike NVIDIA and AMD GPUs that share the same executor and worker, we create a separate code path for TPUs considering the significant difference between GPUs and TPUs. On the other hand, the two backends will share the other components of `LLMEngine`, namely the scheduler, KV cache manager, and tokenizer, as they are (almost) device agnostic.\r\n\r\n## PyTorch XLA and JAX\r\n\r\nAs many components of vLLM are device and runtime agnostic, it is possible to use JAX for TPU integration. However, for faster initial integration and maximum code reuse, **we will start with PyTorch XLA**. Adding JAX backend to vLLM will be interesting future work.\r\n\r\n## TPU Workers\r\n\r\n<img width=\"831\" alt=\"Screenshot 2024-03-25 at 10 44 24\u202fAM\" src=\"https://github.com/vllm-project/vllm/assets/46394894/35a4fa7a-2c1a-4845-bd45-05902c94164f\">\r\n\r\nFor tensor-parallel inference, the vLLM TPU executor will spin up multiple TPU workers; **one TPU worker per TPU chip**. Specifically, we will use [Ray](https://github.com/ray-project/ray) to connect and manage the TPU workers which may reside in different TPU VMs. Note that we do not plan to support multi-slice inference at the moment, while we will support multi-host inference within the same TPU pod slice.\r\n\r\nSame as the GPU executor, the TPU executor will use [Megatron](https://arxiv.org/abs/1909.08053)-style model partitioning for tensor-parallel inference. The partitioning strategy will be hardcoded into the model by replacing `nn.Linear` with `RowParallelLinear` and `ColumnParallelLinear`. Auto-sharding the model can be our future work.\r\n\r\n## GPU Executor vs. TPU Executor\r\n\r\n<img width=\"781\" alt=\"Screenshot 2024-03-25 at 10 44 40\u202fAM\" src=\"https://github.com/vllm-project/vllm/assets/46394894/6be8a6a9-bb1b-47f4-afac-786dac050929\">\r\n\r\nFor GPUs, vLLM uses both eager mode and CUDA graphs for model execution. Specifically, vLLM uses eager mode for prefills and CUDA graphs for decodes. vLLM currently does not use torch.compile for GPUs, but plans to use it in the future. For TPUs, on the other hand, **vLLM will use `torch.compile` (with openxla_eval backend) to trace the PyTorch model and lower it into an XLA graph**.\r\n\r\nWhile vLLM\u2019s GPU and TPU backends will take separate code paths, they will share the PyTorch model code. Most of the custom ops for GPUs will not be needed for TPUs, since they can be auto-generated by the XLA compiler. Therefore, for each target op, vLLM will have two implementations, `_forward` and `_forward_cuda`, and select either of the two implementations at run time depending on the hardware backend. For example, we can define the target ops/layers as follows:\r\n\r\n```python\r\nclass Op(nn.Module):\r\n\r\n    def _forward(self,...):\r\n        # PyTorch implementation that can be optimized by compilers\r\n        # such as XLA or torch.compile.\r\n        ...\r\n\r\n    def _forward_cuda(self, ...):\r\n        # Implementation using custom ops written in CUDA.\r\n        ...\r\n\r\n    def forward(self, ...):\r\n        if ...:\r\n            return self._forward_cuda(...)\r\n        else:\r\n            return self._forward(...)\r\n```\r\n\r\nImportant exceptions to this are the FlashAttention and PagedAttention custom ops, which cannot be generated by the XLA compiler. We will use custom Pallas kernels for them.\r\n\r\n## Handling Dynamic Shapes\r\n\r\nvLLM\u2019s continuous batching has two phases: prefill and decode. vLLM dynamically switches between the two phases based on its scheduling decisions. The input tensor shape for prefills is `[batch_size, prefill_len, hidden_size]` while the input tensor shape for decodes is `[batch_size, 1, hidden_size]` since LLMs decode tokens one by one (here we do not consider special cases such as speculative decoding). In LLM inference, the batch_size and prefill_len can vary for every step.\r\n\r\nTo meet the XLA\u2019s static shape requirement, we will bucketize the possible input shapes. For decodes, we will bucketize the `batch_size` dimension by creating buckets for `batch_size=[8, 16, 24, 32, 40, \u2026, 256]`. For prefills, to reduce the number of compiled graphs, we will fix the `batch_size` to 1, and bucketize the `prefill_len` dimension by creating buckets for `prefill_len=[8, 16, 32, 64, 128, \u2026, max_model_len]`. Given that each prefill input contains enough tokens to efficiently utilize TPUs, fixing `batch_size` as 1 will not hurt performance a lot. The specific bucket sizes will be tuned after benchmarking the compilation overhead and end-to-end performance.\r\n\r\n# References\r\n\r\n- PyTorch XLA Llama V1 inference blog post: https://pytorch.org/blog/path-achieve-low-inference-latency/ \r\n- PyTorch XLA Llama V2 inference blog post: https://pytorch.org/blog/high-performance-llama-2/ \r\n- PyTorch XLA Llama inference example: https://github.com/pytorch-tpu/llama/tree/llama2-google-next-inference",
    "labels": [
      "RFC",
      "tpu",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-25T17:08:43+00:00",
    "closed_at": "2025-03-11T14:04:01+00:00",
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3620/reactions",
      "total_count": 36,
      "+1": 23,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 12,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3620"
  },
  {
    "number": 20869,
    "title": "[Feature]: TPU Embedding models support?",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nHi I want to run latest Embedding models, eg `Qwen/Qwen3-Embedding-0.6B`, on TPU nodes. I found that although vLLM has support on TPU it does not really support embedding models since the only available attention implementation on TPU is `PALLAS` which is DECODER only. https://github.com/vllm-project/vllm/blob/99b4f080d83ae284941b01922d7fe3b9a39034fd/vllm/v1/attention/backends/pallas.py#L164-L168\n\nMeanwhile, Qwen3 Embedding is ENCODER-only so it can't run on TPU. https://github.com/vllm-project/vllm/blob/99b4f080d83ae284941b01922d7fe3b9a39034fd/vllm/model_executor/models/qwen3.py#L166-L173\n\nIt will be nice if we can support Qwen3 Embedding on TPU,\n\n### Alternatives\n\nI am trying to use Qwen3 Embedding via `transformers` but it's not as performant as vLLM.\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "tpu"
    ],
    "state": "open",
    "created_at": "2025-07-13T05:32:13+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20869/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20869"
  },
  {
    "number": 19490,
    "title": "[Bug] [TPU]: OOMing on Llama-8B on new vllm nightly docker",
    "body": "### Your current environment\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```\nroot@t1v-n-82109f0e-w-0:/opt# python collect_env.py \nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\nINFO 06-11 13:54:20 [__init__.py:244] Automatically detected platform tpu.\nINFO 06-11 13:54:20 [tpu.py:215] tpu_commons not found, using vLLM's TpuPlatform\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Debian GNU/Linux 11 (bullseye) (x86_64)\nGCC version                  : (Debian 10.2.1-6) 10.2.1 20210110\nClang version                : Could not collect\nCMake version                : version 4.0.2\nLibc version                 : glibc-2.31\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.8.0.dev20250605+cpu\nIs debug build               : False\nCUDA used to build PyTorch   : None\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.10.16 (main, Jan 14 2025, 05:27:07) [GCC 10.2.1 20210110] (64-bit runtime)\nPython platform              : Linux-6.8.0-1015-gcp-x86_64-with-glibc2.31\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : False\nCUDA runtime version         : No CUDA\nCUDA_MODULE_LOADING set to   : N/A\nGPU models and configuration : No CUDA\nNvidia driver version        : No CUDA\ncuDNN version                : No CUDA\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nByte Order:                           Little Endian\nAddress sizes:                        52 bits physical, 57 bits virtual\nCPU(s):                               180\nOn-line CPU(s) list:                  0-179\nThread(s) per core:                   1\nCore(s) per socket:                   90\nSocket(s):                            2\nNUMA node(s):                         2\nVendor ID:                            AuthenticAMD\nCPU family:                           25\nModel:                                17\nModel name:                           AMD EPYC 9B14\nStepping:                             1\nCPU MHz:                              2599.998\nBogoMIPS:                             5199.99\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            5.6 MiB\nL1i cache:                            5.6 MiB\nL2 cache:                             180 MiB\nL3 cache:                             704 MiB\nNUMA node0 CPU(s):                    0-89\nNUMA node1 CPU(s):                    90-179\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr wbnoinvd arat avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] mypy_extensions==1.1.0\n[pip3] numpy==2.2.2\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.8.0.dev20250605+cpu\n[pip3] torch-xla==2.8.0+git402612d\n[pip3] torchvision==0.23.0.dev20250605+cpu\n[pip3] transformers==4.48.3\n[pip3] triton==3.3.1\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.1rc2.dev5+g6cd4ae8ac (git sha: 6cd4ae8ac)\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n  Could not collect\n\n==============================\n     Environment Variables\n==============================\nVLLM_TARGET_DEVICE=tpu\nLD_LIBRARY_PATH=:/usr/lib/x86_64-linux-gnu/:/home/ray/anaconda3/lib\nVLLM_USE_V1=1\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nTORCHINDUCTOR_CACHE_DIR=/tmp/torchinductor_root\n```\n\n</details>\n\nOOMing on serving llama-8B-Instruct:\n```text\nroot@t1v-n-82109f0e-w-0:/opt# vllm serve meta-llama/Llama-3.1-8B-Instruct --max-model-len 8192\nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\nINFO 06-11 13:55:34 [__init__.py:244] Automatically detected platform tpu.\nINFO 06-11 13:55:34 [tpu.py:215] tpu_commons not found, using vLLM's TpuPlatform\nINFO 06-11 13:55:37 [config.py:1980] Disabled the custom all-reduce kernel because it is not supported on current platform.\nINFO 06-11 13:55:38 [config.py:1980] Disabled the custom all-reduce kernel because it is not supported on current platform.\nINFO 06-11 13:55:38 [config.py:1980] Disabled the custom all-reduce kernel because it is not supported on current platform.\nINFO 06-11 13:55:39 [config.py:1980] Disabled the custom all-reduce kernel because it is not supported on current platform.\nINFO 06-11 13:55:39 [api_server.py:1287] vLLM API server version 0.9.1rc2.dev5+g6cd4ae8ac\nINFO 06-11 13:55:39 [config.py:1980] Disabled the custom all-reduce kernel because it is not supported on current platform.\nINFO 06-11 13:55:39 [cli_args.py:309] non-default args: {'model': 'meta-llama/Llama-3.1-8B-Instruct', 'max_model_len': 8192}\nINFO 06-11 13:55:48 [config.py:823] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate', 'score'}. Defaulting to 'generate'.\nINFO 06-11 13:55:48 [config.py:1980] Disabled the custom all-reduce kernel because it is not supported on current platform.\nINFO 06-11 13:55:48 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=2048.\nINFO 06-11 13:55:48 [tpu.py:105] [TPU] Forcing DYNAMO_ONCE compilation level\nWARNING 06-11 13:55:50 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\nINFO 06-11 13:55:54 [__init__.py:244] Automatically detected platform tpu.\nINFO 06-11 13:55:54 [tpu.py:215] tpu_commons not found, using vLLM's TpuPlatform\nINFO 06-11 13:55:56 [core.py:455] Waiting for init message from front-end.\nINFO 06-11 13:55:56 [tpu.py:105] [TPU] Forcing DYNAMO_ONCE compilation level\nINFO 06-11 13:55:56 [core.py:70] Initializing a V1 LLM engine (v0.9.1rc2.dev5+g6cd4ae8ac) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=None, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":2,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"openxla\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\nINFO 06-11 13:55:56 [tpu_worker.py:298] tpu_commons not found, using vLLM's TPUWorker.\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\nINFO 06-11 13:55:56 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nWARNING 06-11 13:56:29 [tpu.py:178] Pin memory is not supported on TPU.\nINFO 06-11 13:56:29 [tpu_model_runner.py:1617] Using exponential token paddings:\nINFO 06-11 13:56:29 [tpu_model_runner.py:1619]     16\nINFO 06-11 13:56:29 [tpu_model_runner.py:1619]     32\nINFO 06-11 13:56:29 [tpu_model_runner.py:1619]     64\nINFO 06-11 13:56:29 [tpu_model_runner.py:1619]     128\nINFO 06-11 13:56:29 [tpu_model_runner.py:1619]     256\nINFO 06-11 13:56:29 [tpu_model_runner.py:1619]     512\nINFO 06-11 13:56:29 [tpu_model_runner.py:1619]     1024\nINFO 06-11 13:56:29 [tpu_model_runner.py:1619]     2048\nINFO 06-11 13:56:29 [tpu_model_runner.py:1583] Preparing request paddings:\nINFO 06-11 13:56:29 [tpu_model_runner.py:1590]     8\nINFO 06-11 13:56:29 [tpu_model_runner.py:1590]     16\nINFO 06-11 13:56:29 [tpu_model_runner.py:1590]     32\nINFO 06-11 13:56:29 [tpu_model_runner.py:1590]     64\nINFO 06-11 13:56:29 [tpu_model_runner.py:1590]     128\nINFO 06-11 13:56:29 [tpu_model_runner.py:1590]     256\nINFO 06-11 13:56:29 [tpu_model_runner.py:997] Loading model from scratch...\nINFO 06-11 13:56:29 [tpu.py:51] Cannot use None backend on TPU.\nINFO 06-11 13:56:29 [tpu.py:54] Using Pallas V1 backend.\nINFO 06-11 13:56:30 [weight_utils.py:292] Using model weights format ['*.safetensors']\nINFO 06-11 13:56:30 [weight_utils.py:308] Time spent downloading weights for meta-llama/Llama-3.1-8B-Instruct: 0.515513 seconds\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:06,  2.00s/it]\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:01,  1.01it/s]\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.49it/s]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  2.07it/s]\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.45it/s]\n\nINFO 06-11 13:56:33 [default_loader.py:272] Loading weights took 2.77 seconds\nINFO 06-11 13:56:48 [tpu_model_runner.py:1439] Clear dynamo cache and cached dynamo bytecode.\nINFO 06-11 13:56:48 [kv_cache_utils.py:715] GPU KV cache size: 104,960 tokens\nINFO 06-11 13:56:48 [kv_cache_utils.py:719] Maximum concurrency for 8,192 tokens per request: 12.81x\nINFO 06-11 13:56:50 [tpu_model_runner.py:1149] Compiling the model with different input shapes.\nINFO 06-11 13:56:50 [tpu_model_runner.py:1152]   -- num_tokens: 16\n/usr/local/lib/python3.10/site-packages/jax/_src/cloud_tpu_init.py:82: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n  warnings.warn(\nERROR 06-11 13:57:14 [core.py:515] EngineCore failed to start.\nERROR 06-11 13:57:14 [core.py:515] Traceback (most recent call last):\nERROR 06-11 13:57:14 [core.py:515]   File \"/workspace/vllm/vllm/v1/engine/core.py\", line 506, in run_engine_core\nERROR 06-11 13:57:14 [core.py:515]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 06-11 13:57:14 [core.py:515]   File \"/workspace/vllm/vllm/v1/engine/core.py\", line 390, in __init__\nERROR 06-11 13:57:14 [core.py:515]     super().__init__(vllm_config, executor_class, log_stats,\nERROR 06-11 13:57:14 [core.py:515]   File \"/workspace/vllm/vllm/v1/engine/core.py\", line 83, in __init__\nERROR 06-11 13:57:14 [core.py:515]     self._initialize_kv_caches(vllm_config)\nERROR 06-11 13:57:14 [core.py:515]   File \"/workspace/vllm/vllm/v1/engine/core.py\", line 168, in _initialize_kv_caches\nERROR 06-11 13:57:14 [core.py:515]     self.model_executor.initialize_from_config(kv_cache_configs)\nERROR 06-11 13:57:14 [core.py:515]   File \"/workspace/vllm/vllm/v1/executor/abstract.py\", line 66, in initialize_from_config\nERROR 06-11 13:57:14 [core.py:515]     self.collective_rpc(\"compile_or_warm_up_model\")\nERROR 06-11 13:57:14 [core.py:515]   File \"/workspace/vllm/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\nERROR 06-11 13:57:14 [core.py:515]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 06-11 13:57:14 [core.py:515]   File \"/workspace/vllm/vllm/utils.py\", line 2671, in run_method\nERROR 06-11 13:57:14 [core.py:515]     return func(*args, **kwargs)\nERROR 06-11 13:57:14 [core.py:515]   File \"/workspace/vllm/vllm/v1/worker/tpu_worker.py\", line 248, in compile_or_warm_up_model\nERROR 06-11 13:57:14 [core.py:515]     self.model_runner.capture_model()\nERROR 06-11 13:57:14 [core.py:515]   File \"/workspace/vllm/vllm/v1/worker/tpu_model_runner.py\", line 1283, in capture_model\nERROR 06-11 13:57:14 [core.py:515]     self._precompile_backbone()\nERROR 06-11 13:57:14 [core.py:515]   File \"/workspace/vllm/vllm/v1/worker/tpu_model_runner.py\", line 1153, in _precompile_backbone\nERROR 06-11 13:57:14 [core.py:515]     self._dummy_run(num_tokens)\nERROR 06-11 13:57:14 [core.py:515]   File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 06-11 13:57:14 [core.py:515]     return func(*args, **kwargs)\nERROR 06-11 13:57:14 [core.py:515]   File \"/workspace/vllm/vllm/v1/worker/tpu_model_runner.py\", line 1074, in _dummy_run\nERROR 06-11 13:57:14 [core.py:515]     out = self.model(input_ids=input_ids,\nERROR 06-11 13:57:14 [core.py:515]   File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1767, in _wrapped_call_impl\nERROR 06-11 13:57:14 [core.py:515]     return self._call_impl(*args, **kwargs)\nERROR 06-11 13:57:14 [core.py:515]   File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1778, in _call_impl\nERROR 06-11 13:57:14 [core.py:515]     return forward_call(*args, **kwargs)\nERROR 06-11 13:57:14 [core.py:515]   File \"/workspace/vllm/vllm/model_executor/models/llama.py\", line 581, in forward\nERROR 06-11 13:57:14 [core.py:515]     model_output = self.model(input_ids, positions, intermediate_tensors,\nERROR 06-11 13:57:14 [core.py:515]   File \"/workspace/vllm/vllm/compilation/decorators.py\", line 239, in __call__\nERROR 06-11 13:57:14 [core.py:515]     output = self.compiled_callable(*args, **kwargs)\nERROR 06-11 13:57:14 [core.py:515]   File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 699, in compile_wrapper\nERROR 06-11 13:57:14 [core.py:515]     return fn(*args, **kwargs)\nERROR 06-11 13:57:14 [core.py:515]   File \"/workspace/vllm/vllm/model_executor/models/llama.py\", line 368, in forward\nERROR 06-11 13:57:14 [core.py:515]     def forward(\nERROR 06-11 13:57:14 [core.py:515]   File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 893, in _fn\nERROR 06-11 13:57:14 [core.py:515]     return fn(*args, **kwargs)\nERROR 06-11 13:57:14 [core.py:515]   File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 1231, in forward\nERROR 06-11 13:57:14 [core.py:515]     return compiled_fn(full_args)\nERROR 06-11 13:57:14 [core.py:515]   File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 356, in runtime_wrapper\nERROR 06-11 13:57:14 [core.py:515]     all_outs = call_func_at_runtime_with_args(\nERROR 06-11 13:57:14 [core.py:515]   File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\nERROR 06-11 13:57:14 [core.py:515]     out = normalize_as_list(f(args))\nERROR 06-11 13:57:14 [core.py:515]   File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 722, in inner_fn\nERROR 06-11 13:57:14 [core.py:515]     outs = compiled_fn(args)\nERROR 06-11 13:57:14 [core.py:515]   File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 528, in wrapper\nERROR 06-11 13:57:14 [core.py:515]     return compiled_fn(runtime_args)\nERROR 06-11 13:57:14 [core.py:515]   File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 100, in g\nERROR 06-11 13:57:14 [core.py:515]     return f(*args)\nERROR 06-11 13:57:14 [core.py:515]   File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/backends/torchxla.py\", line 37, in fwd\nERROR 06-11 13:57:14 [core.py:515]     compiled_graph = bridge.extract_compiled_graph(model, args)\nERROR 06-11 13:57:14 [core.py:515]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py\", line 733, in extract_compiled_graph\nERROR 06-11 13:57:14 [core.py:515]     return extract_compiled_graph_helper(xla_model, xla_args)\nERROR 06-11 13:57:14 [core.py:515]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py\", line 853, in extract_compiled_graph_helper\nERROR 06-11 13:57:14 [core.py:515]     return extract_internal(xla_model)\nERROR 06-11 13:57:14 [core.py:515]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py\", line 539, in extract_internal\nERROR 06-11 13:57:14 [core.py:515]     xla_args_need_update) = extract_graph_helper(xla_model,\nERROR 06-11 13:57:14 [core.py:515]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py\", line 483, in extract_graph_helper\nERROR 06-11 13:57:14 [core.py:515]     torch_xla._XLAC._xla_warm_up_cache(args_and_out_tensor_only, [])\nERROR 06-11 13:57:14 [core.py:515] ValueError: Ran out of memory in memory space vmem while allocating on stack for %name.32 = bf16[16,32,128]{2,1,0:T(8,128)(2,1)S(1)} custom-call(%p16.165, %copy, %p14.163, %p13.162, %copy.289, /*index=5*/%pad_maximum_fusion, %bitcast.1), custom_call_target=\"tpu_custom_call\", operand_layout_constraints={s32[256]{0}, s32[256,32]{1,0}, s32[257]{0}, s32[2]{0}, s32[1]{0}, bf16[16,32,128]{2,1,0}, bf16[410,256,16,128]{3,2,1,0}}. Scoped allocation with size 64.82M and limit 64.00M exceeded scoped vmem limit by 840.0K. It should not be possible to run out of scoped vmem - please file a bug against XLA.\nProcess EngineCore_0:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/local/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/workspace/vllm/vllm/v1/engine/core.py\", line 519, in run_engine_core\n    raise e\n  File \"/workspace/vllm/vllm/v1/engine/core.py\", line 506, in run_engine_core\n    engine_core = EngineCoreProc(*args, **kwargs)\n  File \"/workspace/vllm/vllm/v1/engine/core.py\", line 390, in __init__\n    super().__init__(vllm_config, executor_class, log_stats,\n  File \"/workspace/vllm/vllm/v1/engine/core.py\", line 83, in __init__\n    self._initialize_kv_caches(vllm_config)\n  File \"/workspace/vllm/vllm/v1/engine/core.py\", line 168, in _initialize_kv_caches\n    self.model_executor.initialize_from_config(kv_cache_configs)\n  File \"/workspace/vllm/vllm/v1/executor/abstract.py\", line 66, in initialize_from_config\n    self.collective_rpc(\"compile_or_warm_up_model\")\n  File \"/workspace/vllm/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n  File \"/workspace/vllm/vllm/utils.py\", line 2671, in run_method\n    return func(*args, **kwargs)\n  File \"/workspace/vllm/vllm/v1/worker/tpu_worker.py\", line 248, in compile_or_warm_up_model\n    self.model_runner.capture_model()\n  File \"/workspace/vllm/vllm/v1/worker/tpu_model_runner.py\", line 1283, in capture_model\n    self._precompile_backbone()\n  File \"/workspace/vllm/vllm/v1/worker/tpu_model_runner.py\", line 1153, in _precompile_backbone\n    self._dummy_run(num_tokens)\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/workspace/vllm/vllm/v1/worker/tpu_model_runner.py\", line 1074, in _dummy_run\n    out = self.model(input_ids=input_ids,\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1767, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1778, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/workspace/vllm/vllm/model_executor/models/llama.py\", line 581, in forward\n    model_output = self.model(input_ids, positions, intermediate_tensors,\n  File \"/workspace/vllm/vllm/compilation/decorators.py\", line 239, in __call__\n    output = self.compiled_callable(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 699, in compile_wrapper\n    return fn(*args, **kwargs)\n  File \"/workspace/vllm/vllm/model_executor/models/llama.py\", line 368, in forward\n    def forward(\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 893, in _fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 1231, in forward\n    return compiled_fn(full_args)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 356, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 722, in inner_fn\n    outs = compiled_fn(args)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 528, in wrapper\n    return compiled_fn(runtime_args)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 100, in g\n    return f(*args)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_dynamo/backends/torchxla.py\", line 37, in fwd\n    compiled_graph = bridge.extract_compiled_graph(model, args)\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py\", line 733, in extract_compiled_graph\n    return extract_compiled_graph_helper(xla_model, xla_args)\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py\", line 853, in extract_compiled_graph_helper\n    return extract_internal(xla_model)\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py\", line 539, in extract_internal\n    xla_args_need_update) = extract_graph_helper(xla_model,\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py\", line 483, in extract_graph_helper\n    torch_xla._XLAC._xla_warm_up_cache(args_and_out_tensor_only, [])\nValueError: Ran out of memory in memory space vmem while allocating on stack for %name.32 = bf16[16,32,128]{2,1,0:T(8,128)(2,1)S(1)} custom-call(%p16.165, %copy, %p14.163, %p13.162, %copy.289, /*index=5*/%pad_maximum_fusion, %bitcast.1), custom_call_target=\"tpu_custom_call\", operand_layout_constraints={s32[256]{0}, s32[256,32]{1,0}, s32[257]{0}, s32[2]{0}, s32[1]{0}, bf16[16,32,128]{2,1,0}, bf16[410,256,16,128]{3,2,1,0}}. Scoped allocation with size 64.82M and limit 64.00M exceeded scoped vmem limit by 840.0K. It should not be possible to run out of scoped vmem - please file a bug against XLA.\nTraceback (most recent call last):\n  File \"/usr/local/bin/vllm\", line 8, in <module>\n    sys.exit(main())\n  File \"/workspace/vllm/vllm/entrypoints/cli/main.py\", line 59, in main\n    args.dispatch_function(args)\n  File \"/workspace/vllm/vllm/entrypoints/cli/serve.py\", line 58, in cmd\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.10/site-packages/uvloop/__init__.py\", line 82, in run\n    return loop.run_until_complete(wrapper())\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.10/site-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 1323, in run_server\n    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 1343, in run_server_worker\n    async with build_async_engine_client(args, client_config) as engine_client:\n  File \"/usr/local/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 155, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/usr/local/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 191, in build_async_engine_client_from_engine_args\n    async_llm = AsyncLLM.from_vllm_config(\n  File \"/workspace/vllm/vllm/v1/engine/async_llm.py\", line 162, in from_vllm_config\n    return cls(\n  File \"/workspace/vllm/vllm/v1/engine/async_llm.py\", line 124, in __init__\n    self.engine_core = EngineCoreClient.make_async_mp_client(\n  File \"/workspace/vllm/vllm/v1/engine/core_client.py\", line 93, in make_async_mp_client\n    return AsyncMPClient(vllm_config, executor_class, log_stats,\n  File \"/workspace/vllm/vllm/v1/engine/core_client.py\", line 716, in __init__\n    super().__init__(\n  File \"/workspace/vllm/vllm/v1/engine/core_client.py\", line 422, in __init__\n    self._init_engines_direct(vllm_config, local_only,\n  File \"/workspace/vllm/vllm/v1/engine/core_client.py\", line 491, in _init_engines_direct\n    self._wait_for_engine_startup(handshake_socket, input_address,\n  File \"/workspace/vllm/vllm/v1/engine/core_client.py\", line 511, in _wait_for_engine_startup\n    wait_for_engine_startup(\n  File \"/workspace/vllm/vllm/v1/utils.py\", line 494, in wait_for_engine_startup\n    raise RuntimeError(\"Engine core initialization failed. \"\nRuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tpu"
    ],
    "state": "open",
    "created_at": "2025-06-11T13:58:55+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19490/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19490"
  },
  {
    "number": 12179,
    "title": "[Bug]: Multi-Node Online Inference on TPUs Failing",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nroot@t1v-n-4d36f9a1-w-0:/workspace/vllm# python collect_env.py\nINFO 01-17 23:21:42 __init__.py:179] Automatically detected platform tpu.\nCollecting environment information...\nPyTorch version: 2.6.0.dev20241126+cpu\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\nClang version: Could not collect\nCMake version: version 3.31.4\nLibc version: glibc-2.31\n\nPython version: 3.10.15 (main, Oct 17 2024, 02:58:23) [GCC 10.2.1 20210110] (64-bit runtime)\nPython platform: Linux-5.19.0-1022-gcp-x86_64-with-glibc2.31\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nByte Order:                      Little Endian\nAddress sizes:                   48 bits physical, 48 bits virtual\nCPU(s):                          240\nOn-line CPU(s) list:             0-239\nThread(s) per core:              2\nCore(s) per socket:              60\nSocket(s):                       2\nNUMA node(s):                    2\nVendor ID:                       AuthenticAMD\nCPU family:                      23\nModel:                           49\nModel name:                      AMD EPYC 7B12\nStepping:                        0\nCPU MHz:                         2249.998\nBogoMIPS:                        4499.99\nHypervisor vendor:               KVM\nVirtualization type:             full\nL1d cache:                       3.8 MiB\nL1i cache:                       3.8 MiB\nL2 cache:                        60 MiB\nL3 cache:                        480 MiB\nNUMA node0 CPU(s):               0-59,120-179\nNUMA node1 CPU(s):               60-119,180-239\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Not affected\nVulnerability Retbleed:          Mitigation; untrained return thunk; SMT enabled with STIBP protection\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.6.0.dev20241126+cpu\n[pip3] torch-xla==2.6.0+git39e67b5\n[pip3] torchvision==0.20.0.dev20241126+cpu\n[pip3] transformers==4.48.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.6.6.post2.dev250+gd1adb9b40\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nVLLM_TARGET_DEVICE=tpu\nLD_LIBRARY_PATH=/usr/local/lib/python3.10/site-packages/cv2/../../lib64:\n```\n\n</details>\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nI'm trying to run multi-node inference on TPUs, so that I'd be able to fit large models like Llama-3.1-70B-Instruct which will require more than one pod of v4-8.\n\nHere are some of my settings:\n- TPU Type: v4-32\n- TPU Software Version: tpu-ubuntu2204-base\n- Docker Version: vllm/vllm-tpu:nightly\n\nI followed the examples shown in distributed inference and serving in the [docs](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). I modified it to fit the TPU case. The gist was to add `--privileged` flag and also add `TPU` as a resource to the RAY START COMMANDS. Check out my repo for exact information: [link](https://github.com/BabyChouSr/vllm/blob/befc0727ac1e4704e1a2c7f41c180205e046b873/examples/online_serving/run_cluster.sh)\n\nI first tested out running with just two of the four hosts in a v4-32 setting. Here are the commands I used.\n\n```\n# ssh into head and worker and setup containers\ngcloud compute tpus tpu-vm ssh --zone \"us-central2-b\" \"chris-vllm-labelling\" --project \"hai-gcp-models\" --worker 0\ngit clone https://github.com/BabyChouSr/vllm.git\ncd vllm \ngit checkout chris/vllm-tpu-multi\n\nsudo bash examples/online_serving/run_cluster.sh \\\n                  vllm/vllm-tpu:nightly \\\n                  10.130.0.9 \\\n                  --head \\\n                  /home/chrischou/.cache/huggingface\n\n# Repeat above for worker node\n```\n\nThen I ran `ray status` inside the container of the head node. I get the following as expected. I see 8 TPUs registered since I have a head and a worker.\n```\nroot@t1v-n-4d36f9a1-w-0:/workspace/vllm# ray status\n======== Autoscaler status: 2025-01-17 23:33:55.419278 ========\nNode status\n---------------------------------------------------------------\nActive:\n 1 node_6ef276d8e9c645cf2b3ef633fa3e40daa64c635cc5f4accb7fa05659\n 1 node_3e3224930b5214adc9afebf72acea691d97f608dbe26e222875d9af2\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/240.0 CPU\n 0.0/8.0 TPU\n 0.0/1.0 TPU-v4-32-head\n 0.0/2.0 chris-vllm-labelling\n 0B/760.02GiB memory\n 0B/30.40GiB object_store_memory\n\nDemands:\n (no resource demands)\n```\n\nThen I try to serve a small model just to see if tensor parallelism works. I run `vllm serve Qwen/Qwen2.5-7B-Instruct --tensor-parallel-size 4`. I get the following error:\n```\nroot@t1v-n-4d36f9a1-w-0:/workspace/vllm# vllm serve Qwen/Qwen2.5-7B-Instruct --tensor-parallel-size 4\nINFO 01-17 23:34:35 __init__.py:179] Automatically detected platform tpu.\nINFO 01-17 23:34:36 api_server.py:768] vLLM API server version 0.6.6.post2.dev250+gd1adb9b40\nINFO 01-17 23:34:36 api_server.py:769] args: Namespace(subparser='serve', model_tag='Qwen/Qwen2.5-7B-Instruct', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2.5-7B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7fb2e47c4af0>)\nINFO 01-17 23:34:36 api_server.py:195] Started engine process with PID 2002\nINFO 01-17 23:34:40 __init__.py:179] Automatically detected platform tpu.\nINFO 01-17 23:34:44 config.py:520] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed', 'score'}. Defaulting to 'generate'.\nINFO 01-17 23:34:48 config.py:520] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify', 'score'}. Defaulting to 'generate'.\nINFO 01-17 23:34:48 llm_engine.py:232] Initializing an LLM engine (v0.6.6.post2.dev250+gd1adb9b40) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":2,\"backend\":\"openxla\",\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \n2025-01-17 23:34:48,948\tINFO worker.py:1636 -- Connecting to existing Ray cluster at address: 10.130.0.9:6379...\n2025-01-17 23:34:48,961\tINFO worker.py:1812 -- Connected to Ray cluster. View the dashboard at 127.0.0.1:8265 \nINFO 01-17 23:34:50 ray_distributed_executor.py:152] use_ray_spmd_worker: False\n(pid=2257) INFO 01-17 23:34:53 __init__.py:179] Automatically detected platform tpu.\n(RayWorkerWrapper pid=2253) WARNING 01-17 23:34:54 utils.py:546] Overwriting environment variable TPU_VISIBLE_CHIPS from '3' to '0,1,2,3'\nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n(RayWorkerWrapper pid=2251) WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\nINFO 01-17 23:34:55 tpu.py:34] Cannot use None backend on TPU.\nINFO 01-17 23:34:55 tpu.py:35] Using Pallas backend.\nWARNING 01-17 23:34:55 tpu_model_runner.py:127] The max_model_len (32768) is too large. This may degrade the performance due to the insufficient smem size. Consider setting --max-model-len to a smaller value, like 8192.\n(RayWorkerWrapper pid=2253) INFO 01-17 23:34:55 tpu.py:34] Cannot use None backend on TPU.\n(RayWorkerWrapper pid=2253) INFO 01-17 23:34:55 tpu.py:35] Using Pallas backend.\n(RayWorkerWrapper pid=2253) WARNING 01-17 23:34:55 tpu_model_runner.py:127] The max_model_len (32768) is too large. This may degrade the performance due to the insufficient smem size. Consider setting --max-model-len to a smaller value, like 8192.\nERROR 01-17 23:34:56 worker_base.py:554] Error executing method init_device. This might cause deadlock in distributed execution.\nERROR 01-17 23:34:56 worker_base.py:554] Traceback (most recent call last):\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/worker/worker_base.py\", line 546, in execute_method\nERROR 01-17 23:34:56 worker_base.py:554]     return executor(*args, **kwargs)\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\nERROR 01-17 23:34:56 worker_base.py:554]     ensure_model_parallel_initialized(\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 1093, in ensure_model_parallel_initialized\nERROR 01-17 23:34:56 worker_base.py:554]     initialize_model_parallel(tensor_model_parallel_size,\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 1037, in initialize_model_parallel\nERROR 01-17 23:34:56 worker_base.py:554]     _TP = init_model_parallel_group(group_ranks,\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 866, in init_model_parallel_group\nERROR 01-17 23:34:56 worker_base.py:554]     return GroupCoordinator(\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\nERROR 01-17 23:34:56 worker_base.py:554]     self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/workspace/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 53, in __init__\nERROR 01-17 23:34:56 worker_base.py:554]     pjrt.initialize_multiprocess(local_rank, local_world_size)\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 121, in initialize_multiprocess\nERROR 01-17 23:34:56 worker_base.py:554]     devices = xm.get_xla_supported_devices()\nERROR 01-17 23:34:56 worker_base.py:554]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/core/xla_model.py\", line 93, in get_xla_supported_devices\nERROR 01-17 23:34:56 worker_base.py:554]     devices = torch_xla._XLAC._xla_get_devices()\nERROR 01-17 23:34:56 worker_base.py:554] RuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: Invalid --2a886c8_slice_builder_worker_addresses specified. Expected 16 worker addresses, got 8.\nERROR 01-17 23:34:56 engine.py:380] Bad StatusOr access: UNKNOWN: TPU initialization failed: Invalid --2a886c8_slice_builder_worker_addresses specified. Expected 16 worker addresses, got 8.\nERROR 01-17 23:34:56 engine.py:380] Traceback (most recent call last):\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 371, in run_mp_engine\nERROR 01-17 23:34:56 engine.py:380]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 120, in from_engine_args\nERROR 01-17 23:34:56 engine.py:380]     return cls(ipc_path=ipc_path,\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 72, in __init__\nERROR 01-17 23:34:56 engine.py:380]     self.engine = LLMEngine(*args, **kwargs)\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/engine/llm_engine.py\", line 271, in __init__\nERROR 01-17 23:34:56 engine.py:380]     self.model_executor = executor_class(vllm_config=vllm_config, )\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/executor/executor_base.py\", line 222, in __init__\nERROR 01-17 23:34:56 engine.py:380]     super().__init__(*args, **kwargs)\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/executor/executor_base.py\", line 42, in __init__\nERROR 01-17 23:34:56 engine.py:380]     self._init_executor()\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/executor/ray_distributed_executor.py\", line 87, in _init_executor\nERROR 01-17 23:34:56 engine.py:380]     self._init_workers_ray(placement_group)\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/executor/ray_distributed_executor.py\", line 342, in _init_workers_ray\nERROR 01-17 23:34:56 engine.py:380]     self._run_workers(\"init_device\")\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/executor/ray_distributed_executor.py\", line 458, in _run_workers\nERROR 01-17 23:34:56 engine.py:380]     self.driver_worker.execute_method(method, *args, **kwargs)\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/worker/worker_base.py\", line 555, in execute_method\nERROR 01-17 23:34:56 engine.py:380]     raise e\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/worker/worker_base.py\", line 546, in execute_method\nERROR 01-17 23:34:56 engine.py:380]     return executor(*args, **kwargs)\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\nERROR 01-17 23:34:56 engine.py:380]     ensure_model_parallel_initialized(\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 1093, in ensure_model_parallel_initialized\nERROR 01-17 23:34:56 engine.py:380]     initialize_model_parallel(tensor_model_parallel_size,\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 1037, in initialize_model_parallel\nERROR 01-17 23:34:56 engine.py:380]     _TP = init_model_parallel_group(group_ranks,\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 866, in init_model_parallel_group\nERROR 01-17 23:34:56 engine.py:380]     return GroupCoordinator(\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\nERROR 01-17 23:34:56 engine.py:380]     self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\nERROR 01-17 23:34:56 engine.py:380]   File \"/workspace/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 53, in __init__\nERROR 01-17 23:34:56 engine.py:380]     pjrt.initialize_multiprocess(local_rank, local_world_size)\nERROR 01-17 23:34:56 engine.py:380]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 121, in initialize_multiprocess\nERROR 01-17 23:34:56 engine.py:380]     devices = xm.get_xla_supported_devices()\nERROR 01-17 23:34:56 engine.py:380]   File \"/usr/local/lib/python3.10/site-packages/torch_xla/core/xla_model.py\", line 93, in get_xla_supported_devices\nERROR 01-17 23:34:56 engine.py:380]     devices = torch_xla._XLAC._xla_get_devices()\nERROR 01-17 23:34:56 engine.py:380] RuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: Invalid --2a886c8_slice_builder_worker_addresses specified. Expected 16 worker addresses, got 8.\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/local/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 382, in run_mp_engine\n    raise e\n  File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 371, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n  File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 120, in from_engine_args\n    return cls(ipc_path=ipc_path,\n  File \"/workspace/vllm/vllm/engine/multiprocessing/engine.py\", line 72, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n  File \"/workspace/vllm/vllm/engine/llm_engine.py\", line 271, in __init__\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n  File \"/workspace/vllm/vllm/executor/executor_base.py\", line 222, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/workspace/vllm/vllm/executor/executor_base.py\", line 42, in __init__\n    self._init_executor()\n  File \"/workspace/vllm/vllm/executor/ray_distributed_executor.py\", line 87, in _init_executor\n    self._init_workers_ray(placement_group)\n  File \"/workspace/vllm/vllm/executor/ray_distributed_executor.py\", line 342, in _init_workers_ray\n    self._run_workers(\"init_device\")\n  File \"/workspace/vllm/vllm/executor/ray_distributed_executor.py\", line 458, in _run_workers\n    self.driver_worker.execute_method(method, *args, **kwargs)\n  File \"/workspace/vllm/vllm/worker/worker_base.py\", line 555, in execute_method\n    raise e\n  File \"/workspace/vllm/vllm/worker/worker_base.py\", line 546, in execute_method\n    return executor(*args, **kwargs)\n  File \"/workspace/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\n    ensure_model_parallel_initialized(\n  File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 1093, in ensure_model_parallel_initialized\n    initialize_model_parallel(tensor_model_parallel_size,\n  File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 1037, in initialize_model_parallel\n    _TP = init_model_parallel_group(group_ranks,\n  File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 866, in init_model_parallel_group\n    return GroupCoordinator(\n  File \"/workspace/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\n    self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\n  File \"/workspace/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 53, in __init__\n    pjrt.initialize_multiprocess(local_rank, local_world_size)\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 121, in initialize_multiprocess\n    devices = xm.get_xla_supported_devices()\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/core/xla_model.py\", line 93, in get_xla_supported_devices\n    devices = torch_xla._XLAC._xla_get_devices()\nRuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: Invalid --2a886c8_slice_builder_worker_addresses specified. Expected 16 worker addresses, got 8.\n(pid=2251) INFO 01-17 23:34:53 __init__.py:179] Automatically detected platform tpu. [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\n(RayWorkerWrapper pid=2251) WARNING 01-17 23:34:54 utils.py:546] Overwriting environment variable TPU_VISIBLE_CHIPS from '2' to '0,1,2,3' [repeated 2x across cluster]\n(RayWorkerWrapper pid=2251) INFO 01-17 23:34:55 tpu.py:34] Cannot use None backend on TPU. [repeated 2x across cluster]\n(RayWorkerWrapper pid=2251) INFO 01-17 23:34:55 tpu.py:35] Using Pallas backend. [repeated 2x across cluster]\n(RayWorkerWrapper pid=2251) WARNING 01-17 23:34:55 tpu_model_runner.py:127] The max_model_len (32768) is too large. This may degrade the performance due to the insufficient smem size. Consider setting --max-model-len to a smaller value, like 8192. [repeated 2x across cluster]\n(RayWorkerWrapper pid=2248) WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU. [repeated 2x across cluster]\nTask exception was never retrieved\nfuture: <Task finished name='Task-2' coro=<MQLLMEngineClient.run_output_handler_loop() done, defined at /workspace/vllm/vllm/engine/multiprocessing/client.py:180> exception=ZMQError('Operation not supported')>\nTraceback (most recent call last):\n  File \"/workspace/vllm/vllm/engine/multiprocessing/client.py\", line 186, in run_output_handler_loop\n    while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n  File \"/usr/local/lib/python3.10/site-packages/zmq/_future.py\", line 400, in poll\n    raise _zmq.ZMQError(_zmq.ENOTSUP)\nzmq.error.ZMQError: Operation not supported\nTraceback (most recent call last):\n  File \"/usr/local/bin/vllm\", line 33, in <module>\n    sys.exit(load_entry_point('vllm', 'console_scripts', 'vllm')())\n  File \"/workspace/vllm/vllm/scripts.py\", line 201, in main\n    args.dispatch_function(args)\n  File \"/workspace/vllm/vllm/scripts.py\", line 42, in serve\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.10/site-packages/uvloop/__init__.py\", line 82, in run\n    return loop.run_until_complete(wrapper())\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.10/site-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 796, in run_server\n    async with build_async_engine_client(args) as engine_client:\n  File \"/usr/local/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 125, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/usr/local/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/workspace/vllm/vllm/entrypoints/openai/api_server.py\", line 219, in build_async_engine_client_from_engine_args\n    raise RuntimeError(\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\n```\n\nIt seems like there is some issues with the spawning of the multiple processes needed to run distributed inference. I also tested the case without tensor parallelism since Qwen7B fits on a single node. It ends up hanging:\n```\nroot@t1v-n-4d36f9a1-w-0:/workspace/vllm# vllm serve Qwen/Qwen2.5-7B-Instruct\nINFO 01-17 23:36:53 __init__.py:179] Automatically detected platform tpu.\nINFO 01-17 23:36:55 api_server.py:768] vLLM API server version 0.6.6.post2.dev250+gd1adb9b40\nINFO 01-17 23:36:55 api_server.py:769] args: Namespace(subparser='serve', model_tag='Qwen/Qwen2.5-7B-Instruct', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2.5-7B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7f6874240af0>)\nINFO 01-17 23:36:55 api_server.py:195] Started engine process with PID 18709\nINFO 01-17 23:36:58 __init__.py:179] Automatically detected platform tpu.\nINFO 01-17 23:37:02 config.py:520] This model supports multiple tasks: {'score', 'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\nINFO 01-17 23:37:06 config.py:520] This model supports multiple tasks: {'generate', 'reward', 'classify', 'embed', 'score'}. Defaulting to 'generate'.\nINFO 01-17 23:37:06 llm_engine.py:232] Initializing an LLM engine (v0.6.6.post2.dev250+gd1adb9b40) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":2,\"backend\":\"openxla\",\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\nINFO 01-17 23:37:08 tpu.py:34] Cannot use None backend on TPU.\nINFO 01-17 23:37:08 tpu.py:35] Using Pallas backend.\nWARNING 01-17 23:37:08 tpu_model_runner.py:127] The max_model_len (32768) is too large. This may degrade the performance due to the insufficient smem size. Consider setting --max-model-len to a smaller value, like 8192.\n```\n^ It stops right here and does not output anything anymore. Running ray status on a separate terminal window, it says that there are no demanded resources, so nothing is being used.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tpu",
      "ray",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-01-17T23:38:35+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12179/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12179"
  },
  {
    "number": 11260,
    "title": "[Bug]: vLLM on TPU does not support --pipeline-parallel-size with Ray",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.6.0.dev20241126+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: None\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.31.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-1013-gcp-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             112\r\nOn-line CPU(s) list:                0-111\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 7B13\r\nCPU family:                         25\r\nModel:                              1\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 56\r\nSocket(s):                          1\r\nStepping:                           0\r\nBogoMIPS:                           4899.99\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip vaes vpclmulqdq rdpid fsrm\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          1.8 MiB (56 instances)\r\nL1i cache:                          1.8 MiB (56 instances)\r\nL2 cache:                           28 MiB (56 instances)\r\nL3 cache:                           224 MiB (7 instances)\r\nNUMA node(s):                       1\r\nNUMA node0 CPU(s):                  0-111\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; safe RET\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.6.0.dev20241126+cpu\r\n[pip3] torch-xla==2.6.0+git39e67b5\r\n[pip3] torchvision==0.20.0.dev20241126+cpu\r\n[pip3] transformers==4.47.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.6.0.dev20241126+cpu          pypi_0    pypi\r\n[conda] torch-xla                 2.6.0+git39e67b5          pypi_0    pypi\r\n[conda] torchvision               0.20.0.dev20241126+cpu          pypi_0    pypi\r\n[conda] transformers              4.47.0                   pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.4.post2.dev378+g69ba344d\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n\r\nLD_LIBRARY_PATH=/home/ext_hzchen_google_com/miniconda3/envs/tpu/lib/python3.10/site-packages/cv2/../../lib64:\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nI setup v5e-32(8 hosts/4 chips each) and started a ray cluster.\r\n\r\noutput of **ray status**\r\n```\r\n======== Autoscaler status: 2024-12-17 11:18:22.317629 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 node_719fd8c930dcd8b932914ebb34d70d16323b468e1f93094a78d50f75\r\n 1 node_ac79abf69175ce6f927b5317fe292d22aea9ac4e3c224a7ba42ca6d3\r\n 1 node_23a15d4de28063c4a04865b963c54c0a8f29d8e928c298e4021a146b\r\n 1 node_153c21365890656c54742efe22e675b89127db7998084e482c8260c6\r\n 1 node_cedcf4265be52d297b3d95bab832675688d8241360c819bd9bd63de7\r\n 1 node_8993f15ab992801a04a70b5b7c4c691158b949bd7da98c35154aa372\r\n 1 node_3723c73cee4bc754265308f5a9f384b493e06b7d76860a739e9ddfb4\r\n 1 node_e2823d348a9370bfe108d635330b339b48e4847ce0608af311cd75e2\r\nPending:\r\n (no pending nodes)\r\nRecent failures:\r\n (no failures)\r\n\r\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 0.0/880.0 CPU\r\n 0.0/32.0 TPU\r\n 0.0/1.0 TPU-v5litepod-32-head\r\n 0B/1.01TiB memory\r\n 0B/449.11GiB object_store_memory\r\n 0.0/8.0 tpuvm-01\r\n\r\nDemands:\r\n (no resource demands)\r\n```\r\n\r\n\r\nI start serving with below command\r\n```\r\nvllm serve mistralai/Pixtral-Large-Instruct-2411 --config-format mistral --load-format mistral --tokenizer-mode mistral --num-scheduler-steps 2 --swap-space 4 --max-model-len=1024 --limit_mm_per_prompt 'image=10' --tensor-parallel-size 4 --pipeline-parallel-size 8 --disable-log-requests --dtype=bfloat16\r\n```\r\n\r\nget below error messages\r\n\r\n```text\r\nINFO 12-17 09:37:22 api_server.py:643] vLLM API server version 0.6.4.post2.dev378+g69ba344d\r\nINFO 12-17 09:37:22 api_server.py:644] args: Namespace(subparser='serve', model_tag='mistralai/Pixtral-Large-Instruct-2411', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='mistralai/Pixtral-Large-Instruct-2411', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='mistral', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='mistral', config_format='mistral', dtype='bfloat16', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=1024, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=8, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt={'image': 10}, mm_processor_kwargs=None, mm_cache_preprocessor=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=2, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7fa9d7e42320>)\r\nINFO 12-17 09:37:24 config.py:1938] Downcasting torch.float32 to torch.bfloat16.\r\nINFO 12-17 09:37:31 config.py:451] This model supports multiple tasks: {'embed', 'reward', 'score', 'classify', 'generate'}. Defaulting to 'generate'.\r\nWARNING 12-17 09:37:31 config.py:569] Async output processing can not be enabled with pipeline parallel\r\n2024-12-17 09:37:31,835\tINFO worker.py:1636 -- Connecting to existing Ray cluster at address: 10.164.0.34:6379...\r\n2024-12-17 09:37:31,898\tINFO worker.py:1812 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\r\nINFO 12-17 09:37:32 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post2.dev378+g69ba344d) with config: model='mistralai/Pixtral-Large-Instruct-2411', speculative_config=None, tokenizer='mistralai/Pixtral-Large-Instruct-2411', skip_tokenizer_init=False, tokenizer_mode=mistral, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=mistral, tensor_parallel_size=4, pipeline_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Pixtral-Large-Instruct-2411, num_scheduler_steps=2, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":2,\"backend\":\"openxla\",\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n\u001b[36m(RayWorkerWrapper pid=150332, ip=10.164.0.35)\u001b[0m WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\r\nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m INFO 12-17 09:40:07 tpu.py:27] Cannot use _Backend.FLASH_ATTN backend on TPU.\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m INFO 12-17 09:40:07 selector.py:163] Using Pallas backend.\r\nINFO 12-17 09:40:08 tpu.py:27] Cannot use _Backend.FLASH_ATTN backend on TPU.\r\nINFO 12-17 09:40:08 selector.py:163] Using Pallas backend.\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467] Error executing method init_device. This might cause deadlock in distributed execution.\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467] Traceback (most recent call last):\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/worker_base.py\", line 459, in execute_method\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     return executor(*args, **kwargs)\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     ensure_model_parallel_initialized(\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1101, in ensure_model_parallel_initialized\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     initialize_model_parallel(tensor_model_parallel_size,\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1045, in initialize_model_parallel\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     _TP = init_model_parallel_group(group_ranks,\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 876, in init_model_parallel_group\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     return GroupCoordinator(\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 43, in __init__\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     local_rank = global_rank % local_world_size\r\n\u001b[36m(RayWorkerWrapper pid=148818, ip=10.164.0.37)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467] ZeroDivisionError: integer division or modulo by zero\r\nERROR 12-17 09:40:09 worker_base.py:467] Error executing method init_device. This might cause deadlock in distributed execution.\r\nERROR 12-17 09:40:09 worker_base.py:467] Traceback (most recent call last):\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/worker_base.py\", line 459, in execute_method\r\nERROR 12-17 09:40:09 worker_base.py:467]     return executor(*args, **kwargs)\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\r\nERROR 12-17 09:40:09 worker_base.py:467]     ensure_model_parallel_initialized(\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1101, in ensure_model_parallel_initialized\r\nERROR 12-17 09:40:09 worker_base.py:467]     initialize_model_parallel(tensor_model_parallel_size,\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1045, in initialize_model_parallel\r\nERROR 12-17 09:40:09 worker_base.py:467]     _TP = init_model_parallel_group(group_ranks,\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 876, in init_model_parallel_group\r\nERROR 12-17 09:40:09 worker_base.py:467]     return GroupCoordinator(\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\r\nERROR 12-17 09:40:09 worker_base.py:467]     self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\r\nERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 43, in __init__\r\nERROR 12-17 09:40:09 worker_base.py:467]     local_rank = global_rank % local_world_size\r\nERROR 12-17 09:40:09 worker_base.py:467] ZeroDivisionError: integer division or modulo by zero\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/home/ext_hzchen_google_com/miniconda3/envs/tpu/bin/vllm\", line 33, in <module>\r\n[rank0]:     sys.exit(load_entry_point('vllm', 'console_scripts', 'vllm')())\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/scripts.py\", line 201, in main\r\n[rank0]:     args.dispatch_function(args)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/scripts.py\", line 42, in serve\r\n[rank0]:     uvloop.run(run_server(args))\r\n[rank0]:   File \"/home/ext_hzchen_google_com/miniconda3/envs/tpu/lib/python3.10/site-packages/uvloop/__init__.py\", line 82, in run\r\n[rank0]:     return loop.run_until_complete(wrapper())\r\n[rank0]:   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\r\n[rank0]:   File \"/home/ext_hzchen_google_com/miniconda3/envs/tpu/lib/python3.10/site-packages/uvloop/__init__.py\", line 61, in wrapper\r\n[rank0]:     return await main\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/entrypoints/openai/api_server.py\", line 667, in run_server\r\n[rank0]:     async with build_async_engine_client(args) as engine_client:\r\n[rank0]:   File \"/home/ext_hzchen_google_com/miniconda3/envs/tpu/lib/python3.10/contextlib.py\", line 199, in __aenter__\r\n[rank0]:     return await anext(self.gen)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/entrypoints/openai/api_server.py\", line 117, in build_async_engine_client\r\n[rank0]:     async with build_async_engine_client_from_engine_args(\r\n[rank0]:   File \"/home/ext_hzchen_google_com/miniconda3/envs/tpu/lib/python3.10/contextlib.py\", line 199, in __aenter__\r\n[rank0]:     return await anext(self.gen)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/entrypoints/openai/api_server.py\", line 150, in build_async_engine_client_from_engine_args\r\n[rank0]:     engine_client = build_engine()\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/engine/async_llm_engine.py\", line 707, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/engine/async_llm_engine.py\", line 594, in __init__\r\n[rank0]:     self.engine = self._engine_class(*args, **kwargs)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/engine/async_llm_engine.py\", line 267, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/engine/llm_engine.py\", line 288, in __init__\r\n[rank0]:     self.model_executor = executor_class(vllm_config=vllm_config, )\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/executor/ray_tpu_executor.py\", line 306, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/executor/ray_tpu_executor.py\", line 39, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/executor/executor_base.py\", line 36, in __init__\r\n[rank0]:     self._init_executor()\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/executor/ray_tpu_executor.py\", line 51, in _init_executor\r\n[rank0]:     self._init_workers_ray(placement_group)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/executor/ray_tpu_executor.py\", line 184, in _init_workers_ray\r\n[rank0]:     self._run_workers(\"init_device\")\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/executor/ray_tpu_executor.py\", line 249, in _run_workers\r\n[rank0]:     driver_worker_output = self.driver_worker.execute_method(\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/worker_base.py\", line 468, in execute_method\r\n[rank0]:     raise e\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/worker_base.py\", line 459, in execute_method\r\n[rank0]:     return executor(*args, **kwargs)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\r\n[rank0]:     ensure_model_parallel_initialized(\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1101, in ensure_model_parallel_initialized\r\n[rank0]:     initialize_model_parallel(tensor_model_parallel_size,\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1045, in initialize_model_parallel\r\n[rank0]:     _TP = init_model_parallel_group(group_ranks,\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 876, in init_model_parallel_group\r\n[rank0]:     return GroupCoordinator(\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\r\n[rank0]:     self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\r\n[rank0]:   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 43, in __init__\r\n[rank0]:     local_rank = global_rank % local_world_size\r\n[rank0]: ZeroDivisionError: integer division or modulo by zero\r\n\u001b[36m(RayWorkerWrapper pid=140502, ip=10.164.0.40)\u001b[0m WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=144474, ip=10.164.0.36)\u001b[0m INFO 12-17 09:40:09 tpu.py:27] Cannot use _Backend.FLASH_ATTN backend on TPU.\u001b[32m [repeated 30x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=144474, ip=10.164.0.36)\u001b[0m INFO 12-17 09:40:09 selector.py:163] Using Pallas backend.\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467] Error executing method init_device. This might cause deadlock in distributed execution.\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467] Traceback (most recent call last):\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/worker_base.py\", line 459, in execute_method\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     return executor(*args, **kwargs)\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/worker/tpu_worker.py\", line 67, in init_device\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     ensure_model_parallel_initialized(\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1101, in ensure_model_parallel_initialized\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     initialize_model_parallel(tensor_model_parallel_size,\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 1045, in initialize_model_parallel\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     _TP = init_model_parallel_group(group_ranks,\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 876, in init_model_parallel_group\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     return GroupCoordinator(\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/parallel_state.py\", line 233, in __init__\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     self.tpu_communicator = TpuCommunicator(group=self.cpu_group)\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]   File \"/home/ext_hzchen_google_com/vllm/vllm/distributed/device_communicators/tpu_communicator.py\", line 43, in __init__\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467]     local_rank = global_rank % local_world_size\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n\u001b[36m(RayWorkerWrapper pid=149225, ip=10.164.0.38)\u001b[0m ERROR 12-17 09:40:09 worker_base.py:467] ZeroDivisionError: integer division or modulo by zero\u001b[32m [repeated 30x across cluster]\u001b[0m\r\n```\r\n</details>\r\n\r\nWhat's more, I can start the serving with ```--tensor-parallel-size 32``` only without error, which may have performance impact.\r\nWould like to know if this is work as intended or not.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "tpu",
      "ray",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-12-17T13:04:46+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11260/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11260"
  }
]
[
  {
    "number": 18946,
    "title": "[Bug]: Eagle3 in vLLM v0.9.0 has no acceleration effect.",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.3 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : Could not collect\nCMake version                : version 3.22.1\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu126\nIs debug build               : False\nCUDA used to build PyTorch   : 12.6\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0] (64-bit runtime)\nPython platform              : Linux-5.15.0-88-generic-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.1.105\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : GPU 0: NVIDIA GeForce RTX 3090\nNvidia driver version        : 550.107.02\ncuDNN version                : Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      46 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             128\nOn-line CPU(s) list:                0-127\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) Platinum 8362 CPU @ 2.80GHz\nCPU family:                         6\nModel:                              106\nThread(s) per core:                 2\nCore(s) per socket:                 32\nSocket(s):                          2\nStepping:                           6\nCPU max MHz:                        3600.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\nVirtualization:                     VT-x\nL1d cache:                          3 MiB (64 instances)\nL1i cache:                          2 MiB (64 instances)\nL2 cache:                           80 MiB (64 instances)\nL3 cache:                           96 MiB (2 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0-31,64-95\nNUMA node1 CPU(s):                  32-63,96-127\nVulnerability Gather data sampling: Vulnerable: No microcode\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-cufile-cu12==1.11.1.6\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.0\n[pip3] torchaudio==2.7.0\n[pip3] torchvision==0.22.0\n[pip3] transformers==4.52.3\n[pip3] triton==3.3.0\n[conda] numpy                     1.26.3                   pypi_0    pypi\n[conda] pyzmq                     25.1.2                   pypi_0    pypi\n[conda] torch                     2.1.2+cu121              pypi_0    pypi\n[conda] torchvision               0.16.2+cu121             pypi_0    pypi\n[conda] triton                    2.1.0                    pypi_0    pypi\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.1.dev0+gc533c6fa8.d20250530 (git sha: c533c6fa8, date: 20250530)\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     SYS     NODE    NODE    32-63,96-127    1               N/A\nNIC0    SYS      X      PIX     SYS     SYS\nNIC1    SYS     PIX      X      SYS     SYS\nNIC2    NODE    SYS     SYS      X      PIX\nNIC3    NODE    SYS     SYS     PIX      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n\n==============================\n     Environment Variables\n==============================\nNVIDIA_VISIBLE_DEVICES=GPU-cfc49b06-8105-de35-a6b4-024cdca0417e\nNVIDIA_REQUIRE_CUDA=cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\nNCCL_VERSION=2.17.1-1\nNVIDIA_DRIVER_CAPABILITIES=compute,utility,graphics,video\nNVIDIA_PRODUCT_NAME=CUDA\nNVIDIA_CUDA_END_OF_LIFE=1\nCUDA_VERSION=12.1.0\nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nOMP_NUM_THREADS=15\nMKL_NUM_THREADS=15\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nTest details:\nvLLM version: 0.9.0\nTest dataset: MT-bench\nGPU: RTX 3090\nBase model: Llama-3.1-8B-Instruct\nEagle models: EAGLE-LLaMA3.1-Instruct-8B, EAGLE3-LLaMA3.1-Instruct-8B\n\nExecution commands:\npython examples/offline_inference/eagle.py --num_spec_tokens 2 --max_num_seqs 1 --num_prompts 80 --method 'eagle'\npython examples/offline_inference/eagle.py --num_spec_tokens 2 --max_num_seqs 1 --num_prompts 80 --method 'eagle3'\n\nResults:\neagle: 75 tokens/s\neagle3: 47 tokens/s\nbaseline (no speculative decoding): 48 tokens/s\n\nWhen testing the same models in the original Eagle repository, both show acceleration effects:\neagle achieves 2.4x speedup\neagle3 achieves 3.7x speedup\n\nQuestion:\nDoes this mean vLLM 0.9.0 has compatibility issues with Eagle3?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2025-05-30T08:44:50+00:00",
    "closed_at": "2025-06-01T02:58:35+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18946/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/18946"
  },
  {
    "number": 15958,
    "title": "[Bug]: SpecDecoding metrics showing with disabled spec decoding",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nWARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\nINFO 04-02 18:23:13 [__init__.py:239] Automatically detected platform tpu.\nCollecting environment information...\nPyTorch version: 2.8.0\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.31.6\nLibc version: glibc-2.35\n\nPython version: 3.11.11 (main, Feb 12 2025, 14:51:05) [Clang 19.1.6 ] (64-bit runtime)\nPython platform: Linux-6.8.0-1015-gcp-x86_64-with-glibc2.35\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               180\nOn-line CPU(s) list:                  0-179\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 9B14\nCPU family:                           25\nModel:                                17\nThread(s) per core:                   2\nCore(s) per socket:                   90\nSocket(s):                            1\nStepping:                             1\nBogoMIPS:                             5200.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr wbnoinvd arat avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            2.8 MiB (90 instances)\nL1i cache:                            2.8 MiB (90 instances)\nL2 cache:                             90 MiB (90 instances)\nL3 cache:                             384 MiB (12 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-179\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.8.0\n[pip3] torch-tb-profiler==0.4.3\n[pip3] torch-xla==2.8.0+gitfe3bb7f\n[pip3] transformers==4.50.2\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3.dev145+gddc2dddf2.d20250331\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\n\n```\n\n</details>\n\n\n\n\n### \ud83d\udc1b Describe the bug\n\nI just happened to start llava on main (`44f990515`) no spec decoding with `vllm serve llava-hf/llava-1.5-7b-hf --max-model-len 2512 --max-num-seqs 16 --max-num-batched-tokens 64 --chat-template examples/template_llava.jinja`. I am getting a lot of clutter in the logs:\n\n```\nINFO 04-02 18:03:41 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 50.0%\nINFO 04-02 18:03:41 [metrics.py:53] SpecDecoding metrics: Draft acceptance rate: nan%, Accepted: 0 tokens, Drafted: 0 tokens\nINFO 04-02 18:03:51 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 50.0%\nINFO 04-02 18:03:51 [metrics.py:53] SpecDecoding metrics: Draft acceptance rate: nan%, Accepted: 0 tokens, Drafted: 0 tokens\nINFO 04-02 18:04:01 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 50.0%\nINFO 04-02 18:04:01 [metrics.py:53] SpecDecoding metrics: Draft acceptance rate: nan%, Accepted: 0 tokens, Drafted: 0 tokens\n```\n\ncc @markmc \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "speculative-decoding",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-04-02T18:23:33+00:00",
    "closed_at": "2025-04-04T15:52:42+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15958/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15958"
  },
  {
    "number": 12323,
    "title": "[Bug]: Speculative decoding does not work",
    "body": "Here is a script:\n```\ndocker run --gpus '\"device=0,1\"' --rm -d --net host \\\n    --name vllm \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    -v /home/thinclient/llm-server/weights:/mnt/weights \\\n    --env \"HUGGING_FACE_HUB_TOKEN=<HF_TOKEN>\" \\\n    --env \"TORCH_USE_CUDA_DSA=1\" \\\n    --env \"CUDA_LAUNCH_BLOCKING=1\" \\\n    --env \"VLLM_RPC_TIMEOUT=100000\" \\\n    --shm-size=15g \\\n    --ipc host \\\n    vllm/vllm-openai:latest \\\n    --model jakiAJK/DeepSeek-R1-Distill-Qwen-7B_GPTQ-int4 \\\n    --speculative-model Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4 \\\n    --num-speculative-tokens 5 \\\n    --tensor-parallel-size 1 \\\n    --pipeline-parallel-size 1 \\\n    --gpu-memory-utilization 0.98 \\\n    --max_model_len 1000 \\\n    --enable-prefix-caching \\\n\ndocker logs -f vllm\n```\nAnd logs:\n```\nINFO 01-22 09:01:26 api_server.py:651] vLLM API server version 0.6.5\nINFO 01-22 09:01:26 api_server.py:652] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='jakiAJK/DeepSeek-R1-Distill-Qwen-7B_GPTQ-int4', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=1000, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.99, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, mm_cache_preprocessor=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model='Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4', speculative_model_quantization=None, num_speculative_tokens=5, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False)\nINFO 01-22 09:01:26 api_server.py:199] Started engine process with PID 24\nINFO 01-22 09:01:32 config.py:478] This model supports multiple tasks: {'generate', 'embed', 'score', 'classify', 'reward'}. Defaulting to 'generate'.\nINFO 01-22 09:01:33 gptq_marlin.py:109] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\nWARNING 01-22 09:01:35 config.py:2171] Casting torch.float16 to torch.bfloat16.\nINFO 01-22 09:01:35 gptq_marlin.py:109] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\nWARNING 01-22 09:01:35 config.py:624] Async output processing is not supported with speculative decoding currently.\nINFO 01-22 09:01:36 config.py:478] This model supports multiple tasks: {'generate', 'score', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.\nINFO 01-22 09:01:37 gptq_marlin.py:109] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\nWARNING 01-22 09:01:39 config.py:2171] Casting torch.float16 to torch.bfloat16.\nINFO 01-22 09:01:39 gptq_marlin.py:109] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\nWARNING 01-22 09:01:39 config.py:624] Async output processing is not supported with speculative decoding currently.\nINFO 01-22 09:01:39 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='jakiAJK/DeepSeek-R1-Distill-Qwen-7B_GPTQ-int4', speculative_config=SpeculativeConfig(draft_model='Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4', num_spec_tokens=5), tokenizer='jakiAJK/DeepSeek-R1-Distill-Qwen-7B_GPTQ-int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=jakiAJK/DeepSeek-R1-Distill-Qwen-7B_GPTQ-int4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True,\nINFO 01-22 09:01:40 selector.py:120] Using Flash Attention backend.\nINFO 01-22 09:01:40 selector.py:120] Using Flash Attention backend.\nINFO 01-22 09:01:40 spec_decode_worker.py:191] Configuring SpecDecodeWorker with proposer=<class 'vllm.spec_decode.multi_step_worker.MultiStepWorker'>\nINFO 01-22 09:01:40 rejection_sampler.py:57] Use pytorch for rejection sampling.\nINFO 01-22 09:01:40 spec_decode_worker.py:203] [Speculative Decoding] Configuring SpecDecodeWorker with sampler=<class 'vllm.model_executor.layers.rejection_sampler.RejectionSampler'>\nINFO 01-22 09:01:40 spec_decode_worker.py:226] [Speculative Decoding] Disabling MQA scorer as the target model is not running in eager mode.\nINFO 01-22 09:01:40 model_runner.py:1092] Starting to load model jakiAJK/DeepSeek-R1-Distill-Qwen-7B_GPTQ-int4...\nINFO 01-22 09:01:40 gptq_marlin.py:200] Using MarlinLinearKernel for GPTQMarlinLinearMethod\nINFO 01-22 09:01:41 weight_utils.py:243] Using model weights format ['*.safetensors']\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:11<00:11, 11.45s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:14<00:00,  6.39s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:14<00:00,  7.15s/it]\n\nINFO 01-22 09:01:56 model_runner.py:1097] Loading model weights took 5.2045 GB\nINFO 01-22 09:01:56 model_runner.py:1092] Starting to load model Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4...\nINFO 01-22 09:01:56 weight_utils.py:243] Using model weights format ['*.safetensors']\nINFO 01-22 09:02:27 weight_utils.py:288] No model.safetensors.index.json found in remote.\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.34s/it]\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.34s/it]\n\nINFO 01-22 09:02:28 model_runner.py:1097] Loading model weights took 0.4308 GB\nINFO 01-22 09:02:28 spec_decode_worker.py:333] [Speculative Decoding] Use batch expansion for scoring proposals.\nERROR 01-22 09:02:28 engine.py:366]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 357, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 119, in from_engine_args\n    return cls(ipc_path=ipc_path,\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 71, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 288, in __init__\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 36, in __init__\n    self._init_executor()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/gpu_executor.py\", line 34, in _init_executor\n    self.driver_worker.init_device()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/spec_decode/spec_decode_worker.py\", line 342, in init_device\n    vocab_size=self._vocab_size)\n               ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/functools.py\", line 995, in __get__\n    val = self.func(instance)\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/spec_decode/spec_decode_worker.py\", line 1106, in _vocab_size\n    assert all(vocab_sizes[0] == vocab_size for vocab_size in vocab_sizes)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 368, in run_mp_engine\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 357, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 119, in from_engine_args\n    return cls(ipc_path=ipc_path,\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 71, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 288, in __init__\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 36, in __init__\n    self._init_executor()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/gpu_executor.py\", line 34, in _init_executor\n    self.driver_worker.init_device()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/spec_decode/spec_decode_worker.py\", line 342, in init_device\n    vocab_size=self._vocab_size)\n               ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/functools.py\", line 995, in __get__\n    val = self.func(instance)\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/spec_decode/spec_decode_worker.py\", line 1106, in _vocab_size\n    assert all(vocab_sizes[0] == vocab_size for vocab_size in vocab_sizes)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n[rank0]:[W122 09:02:29.221323079 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "speculative-decoding",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-22T17:15:23+00:00",
    "closed_at": "2025-05-24T02:07:47+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12323/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12323"
  },
  {
    "number": 8784,
    "title": "[Bug]: Disabling Marlin by setting --quantization gptq doesn't work when using a draft model",
    "body": "### Your current environment\n\n.\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nIt seems that setting --quantization gptq only disables the marlin for the main model. \r\n\r\nMaybe this can be fixed by adding a --quantization-draft-model setting or forcing the draft model to gptq when main model is forced.\r\n\r\n```\r\nINFO 09-24 15:46:11 gptq_marlin.py:112] Detected that the model can run with gptq_marlin, **however you specified quantization=gptq explicitly, so forcing gptq. Use quantization=gptq_marlin for faster inference**\r\nWARNING 09-24 15:46:11 config.py:335] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\nINFO 09-24 15:46:11 config.py:904] Defaulting to use mp for distributed inference\r\n**INFO 09-24 15:46:11 gptq_marlin.py:108] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.**\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "quantization",
      "speculative-decoding",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-24T22:51:58+00:00",
    "closed_at": "2025-01-24T01:58:42+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8784/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8784"
  },
  {
    "number": 5563,
    "title": "[Bug]: Speculative decoding server: `ValueError: could not broadcast input array from shape (513,) into shape (512,)`",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-112-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 11.5.119\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 1: NVIDIA H100 80GB HBM3\r\nGPU 2: NVIDIA H100 80GB HBM3\r\nGPU 3: NVIDIA H100 80GB HBM3\r\nGPU 4: NVIDIA H100 80GB HBM3\r\nGPU 5: NVIDIA H100 80GB HBM3\r\nGPU 6: NVIDIA H100 80GB HBM3\r\nGPU 7: NVIDIA H100 80GB HBM3\r\n\r\nNvidia driver version: 535.161.08\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             96\r\nOn-line CPU(s) list:                0-95\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8468\r\nCPU family:                         6\r\nModel:                              143\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 48\r\nSocket(s):                          2\r\nStepping:                           8\r\nBogoMIPS:                           4200.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nL1d cache:                          4.5 MiB (96 instances)\r\nL1i cache:                          3 MiB (96 instances)\r\nL2 cache:                           192 MiB (96 instances)\r\nL3 cache:                           210 MiB (2 instances)\r\nNUMA node(s):                       8\r\nNUMA node0 CPU(s):                  0-11\r\nNUMA node1 CPU(s):                  12-23\r\nNUMA node2 CPU(s):                  24-35\r\nNUMA node3 CPU(s):                  36-47\r\nNUMA node4 CPU(s):                  48-59\r\nNUMA node5 CPU(s):                  60-71\r\nNUMA node6 CPU(s):                  72-83\r\nNUMA node7 CPU(s):                  84-95\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] transformers==4.41.2\r\n[pip3] triton==2.3.0\r\n[conda] No relevant packages\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.0\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    CPU Affinity    NUMA Affinity     GPU NUMA ID\r\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-11    0        N/A\r\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     24-35   2        N/A\r\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     36-47   3        N/A\r\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     12-23   1        N/A\r\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     PIX     PIX     SYS     SYS     SYS     48-59   4        N/A\r\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     72-83   6        N/A\r\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     84-95   7        N/A\r\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     60-71   5        N/A\r\nNIC0    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC1    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC2    SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC3    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC4    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS\r\nNIC5    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     SYS     SYS     SYS\r\nNIC6    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      SYS     SYS     SYS\r\nNIC7    SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS\r\nNIC8    SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS\r\nNIC9    SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  NIC8: mlx5_8\r\n  NIC9: mlx5_9\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nI am running into an issue in the vllm server in speculative decoding mode. The server is launched with this command on an 8xH100 machine for a Mixtral 8x22B model\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\r\npython3 -u -m vllm.entrypoints.openai.api_server \\\r\n--host 0.0.0.0 \\\r\n--model $MODEL_NAME \\\r\n--tensor-parallel-size 8 \\\r\n--served-model-name \"8x22_custom\" \\\r\n--tokenizer $TOKENIZER_NAME \\\r\n--max-model-len 16384 \\\r\n--max-num-batched-tokens 16384 \\\r\n--speculative-model [ngram] \\\r\n--num-speculative-tokens 128 \\\r\n--ngram-prompt-lookup-max 32 \\\r\n--ngram-prompt-lookup-min 16 \\\r\n--speculative-max-model-len 16000 \\\r\n--use-v2-block-manager \\\r\n--enable-prefix-caching \\\r\n--disable-log-requests\r\n```\r\n\r\nAfter running several queries, the server runs into an error and does not recover. This takes some time, presumably because it's only a bug once the KV caching is populated\r\n\r\n```\r\nFile \"/home/walden/mistral-finetune/.venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 503, in engine_step                          request_outputs = await self.engine.step_async()\r\n  File \"/home/walden/mistral-finetune/.venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 235, in step_async                           output = await self.model_executor.execute_model_async(\r\n  File \"/home/walden/mistral-finetune/.venv/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py\", line 166, in execute_model_async        return await self._driver_execute_model_async(execute_model_req)\r\n  File \"/home/walden/mistral-finetune/.venv/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 149, in _driver_execute_model_async\r\n    return await self.driver_exec_model(execute_model_req)\r\n  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/home/walden/mistral-finetune/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/walden/mistral-finetune/.venv/lib/python3.10/site-packages/vllm/spec_decode/spec_decode_worker.py\", line 291, in execute_model\r\n    return self._run_speculative_decoding_step(execute_model_req,\r\n  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/walden/mistral-finetune/.venv/lib/python3.10/site-packages/vllm/spec_decode/spec_decode_worker.py\", line 389, in _run_speculative_decoding_\r\nstep\r\n    proposal_scores = self.scorer.score_proposals(\r\n  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/walden/mistral-finetune/.venv/lib/python3.10/site-packages/vllm/spec_decode/batch_expansion.py\", line 81, in score_proposals\r\n    target_sampler_output = self._scorer_worker.execute_model(\r\n  File \"/home/walden/mistral-finetune/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/walden/mistral-finetune/.venv/lib/python3.10/site-packages/vllm/worker/worker.py\", line 272, in execute_model\r\n    output = self.model_runner.execute_model(seq_group_metadata_list,\r\n  File \"/home/walden/mistral-finetune/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/walden/mistral-finetune/.venv/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 724, in execute_model\r\n    ) = self.prepare_input_tensors(seq_group_metadata_list)\r\n  File \"/home/walden/mistral-finetune/.venv/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 670, in prepare_input_tensors\r\n    ) = self._prepare_model_input(seq_group_metadata_list)\r\n  File \"/home/walden/mistral-finetune/.venv/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 516, in _prepare_model_input\r\n    input_block_tables[i, :len(block_table)] = block_table\r\nValueError: could not broadcast input array from shape (513,) into shape (512,)\r\n```\r\n\r\nIt seems to be an off-by-one error coming from the speculative decoding code.\r\n\r\nLet me know if more information is needed.",
    "labels": [
      "bug",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2024-06-14T22:54:56+00:00",
    "closed_at": "2024-09-10T23:02:43+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5563/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5563"
  },
  {
    "number": 5561,
    "title": "[Performance] [Speculative decoding] Speed up autoregressive proposal methods by making sampler CPU serialization optional",
    "body": "## Background\r\nSpeculative decoding leverages the ability to cheaply generate proposals, and cheaply verify them to achieve speedup for memory-bound inference. Different methods of speculative decoding explore the frontier between cost of proposal, alignment with the target model, and cost of verification.\r\n\r\nFor example, Medusa produces very cheap proposals, but the quality of the proposals are strictly less than Eagle because the heads do not have access to the previous proposals. Eagle on the other hand pays more for the proposals by sampling autoregressively instead of 1-shot, but it brings the benefit of higher-quality proposals.\r\n\r\nAt the end of the day, what the user cares about will dictate which speculative technique is used. vLLM's job is to provide them with the option for best speedup for their use case.\r\n\r\nDraft-model, EAGLE, and MLPSpeculator rely on autoregressive proposals. This means their top-1 proposals are higher-quality than Medusa, which gives vLLM an ITL reduction that is more flops-efficient than Medusa. This is what our speculative decoding efforts are focused on first -- afterward, we can support top-k proposals with Medusa so users who care more about ITL reduction can use vLLM.\r\n\r\n## Speedup autoregressive proposal methods\r\nThis issue is to speed up autoregressive proposal methods by optimizing the sampler. Specifically, the sampler performs wasted work by copying sampled values to GPU and serializing them into Python objects. In speculative decoding, we never use the python objects because we consume the raw sampled token ids / probabilities in their GPU tensors. This means that the copy and CPU serialization are pure overhead in speculative decoding.\r\n\r\n### How much overhead?\r\nIn [profiling vLLM](https://docs.google.com/spreadsheets/d/1GMyebF9XwlLJzpkpRxZrzUNcQTSibHlQ7zifldaDPtI/edit#gid=0), I found that copy + serialization in the draft model takes ~441\u00b5s (cell J30). Note that the actual forward pass and sampling math of the draft model take (220\u00b5s + 639\u00b5s) = 859\u00b5s. This means that by removing the unnecessary copy and serialization, we can get 50% more draft tokens in the same time it takes with the copy and serialization enabled.\r\n\r\nThis difference is actually massive on the overall performance of speculative decoding.\r\n\r\nFurthermore, the subsequent draft model forward pass must consume the output of the previous step. This allows us to reduce time spent in `prepare_inputs`. I don't have numbers here, but I expect a further ~150\u00b5s reduction per draft model step by this (~300\u00b5s to ~150\u00b5s).\r\n\r\n## The work\r\nThis issue is to:\r\n1. Make the CPU copy and CPU serialization optional in vLLM's sampler (thus leaving sampled token ids on GPU), and then\r\n2. passing those sampled token ids to `prepare_inputs` of the next draft model forward pass. \r\n\r\n### 1. Make CPU serialization optional\r\nWarm up task: Note a good warmup task to get familiar with the Sampler is to add an option to disable `logprobs` for a given Worker. This will also provide some speedup to spec decode (~2ms e2e step time), but isn't part of this issue.\r\n\r\nCode pointers:\r\n* [The sample method which does CPU-GPU synchronization and serialization](https://github.com/vllm-project/vllm/blob/cd9c0d65d98f86fbd2235ee41b80107097a57f77/vllm/model_executor/layers/sampler.py#L96-L103)\r\n* [The actual for loop which does synchronization and serialization](https://github.com/vllm-project/vllm/blob/cd9c0d65d98f86fbd2235ee41b80107097a57f77/vllm/model_executor/layers/sampler.py#L535-L549)\r\n\r\n### 2. Allow `prepare_inputs` method to work on-device\r\nThe on-gpu sampled token ids should be appended to the next prepare_inputs batch.\r\n\r\n* [The `prepare_inputs` method which will need to consume inputs from GPU in this case.](https://github.com/vllm-project/vllm/blob/cd9c0d65d98f86fbd2235ee41b80107097a57f77/vllm/worker/model_runner.py#L240-L658)\r\n* [Our fork version that does this](https://gist.github.com/cadedaniel/ae8d2e3dbea81001062893ca0d6681cf)\r\n\r\n",
    "labels": [
      "performance",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2024-06-14T22:24:19+00:00",
    "closed_at": "2024-07-18T05:21:30+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5561/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/5561"
  },
  {
    "number": 5016,
    "title": "[Feature] [Spec decode]: Combine chunked prefill with speculative decoding",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nSpeculative decoding can achieve 50%+ latency reduction, but in vLLM it can suffer from the throughput-optimized default scheduling strategy where prefills are prioritized eagerly. Chunked prefill is a recent work in vLLM which optimizes this by spreading out the prefill work over many different decode batches. We can combine chunked prefill with speculative decoding's dynamic speculation length to get the best of both worlds.\r\n\r\nThis is a complex task that requires some design, if you're interested please reach out.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\ncc @LiuXiaoxuanPKU @comaniac @rkooo567 ",
    "labels": [
      "feature request",
      "speculative-decoding",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-23T20:54:22+00:00",
    "closed_at": "2024-12-24T17:08:27+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5016/reactions",
      "total_count": 4,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 4,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/5016"
  },
  {
    "number": 5015,
    "title": "[Help wanted] [Spec decode]: Increase acceptance rate via Medusa's typical acceptance",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nSpeculative decoding allows emitting multiple tokens per sequence by speculating future tokens, scoring their likelihood using the LLM, and then accepting each speculative token based on its likelihood. This process is laid out in the following diagram:\r\n![Screenshot 2024-05-23 at 1 45 16\u202fPM](https://github.com/vllm-project/vllm/assets/950914/52d21c58-1a0e-4a8f-b1f8-4abe79651588)\r\n\r\nThe problem with rejection sampling is that it holds a very high bar for quality: it is lossless and guarantees the distribution of the target model, even if it means rejecting plausible speculative tokens.\r\n\r\nThis issue is a request to implement Medusa's typical acceptance routing in vLLM. Typical acceptance trades off output quality to increase the acceptance rate. See \"Choice of threshold in typical acceptance\" in the [Medusa blogpost](https://sites.google.com/view/medusa-llm) for more information.\r\n\r\nvLLM users should be able to toggle between different acceptance routines; they can use rejection sampling for tasks that require higher quality, or typical acceptance when speedup is more important.\r\n\r\nNOTE: This acceptance routine should work with other proposal types (Eagle, draft, ngram, other), not just Medusa. The speculative decoding framework in vLLM may need improvements to the rejection sampling interface to support this.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\nvLLM's rejection sampler is implemented here: https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/rejection_sampler.py",
    "labels": [
      "feature request",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2024-05-23T20:50:19+00:00",
    "closed_at": "2024-06-18T02:29:10+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5015/reactions",
      "total_count": 3,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/5015"
  },
  {
    "number": 4632,
    "title": "[Performance] [Speculative decoding]: Support draft model on different tensor-parallel size than target model",
    "body": "## Overview\r\nSpeculative decoding allows a speedup for memory-bound LLMs by using a fast proposal method to propose tokens that are verified in a single forward pass by the larger LLM. Papers report 2-3x speedup for bs=1, in Anyscale's fork we see up to 2x speedup with a small draft model for bs=8 (30% for bs=16) (we can improve this! see https://github.com/vllm-project/vllm/issues/4630 if you want to help).\r\n\r\nA key optimization for small models (68m/160m domain) is to use tensor-parallel degree 1, even if the target model is using tensor-parallel degree 4 or 8. In our fork, this reduces proposal time from 5ms/tok to 1.5ms/tok. This will allow a well-aligned 68m draft model to get 2x per-user throughput improvement on 70B target model.\r\n\r\nFurthermore, a 1B/7B proposer model may ideally be placed on TP=2 or TP=4, while the larger model is placed on TP=8. vLLM should support these configuration so the community can use the configuration best for their draft model.\r\n\r\n## Design suggestions\r\nI implemented a Worker which patches the tensor parallel group to TP1 in our fork. The [code is dumped here](https://gist.github.com/cadedaniel/f8479bf5fa5543b946d2133b5db38c56). We should use this approach in vLLM, however we can improve it by using @youkaichao 's tensor-parallel group improvements.",
    "labels": [
      "help wanted",
      "performance",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2024-05-06T18:14:40+00:00",
    "closed_at": "2024-06-25T09:56:08+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4632/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4632"
  },
  {
    "number": 4630,
    "title": "[Speculative decoding] [Help wanted] [Performance] Optimize draft-model speculative decoding",
    "body": "### Proposal to improve performance\r\n\r\nWith the end-to-end correctness tests merged in https://github.com/vllm-project/vllm/pull/3951, now we will optimize the implementation to get ~50% speedup on 70B model with temperature 1.0.\r\n\r\n### Work required:\r\nP0/P1 -- priority\r\n(Small/Medium/Large) -- relative size estimate\r\n\r\n* Optimizing proposal time\r\n  - [x] P0 (Large) Reduce draft model control-plane communication from O(num_steps) to O(1)\r\n  - [x] P0 (Medium) Support draft model on different tensor-parallel-size than target model https://github.com/vllm-project/vllm/issues/4632\r\n* Optimizations for scoring time\r\n  - [x] P0 (Medium) Re-enable bonus tokens to increase % accepted tokens https://github.com/vllm-project/vllm/issues/4212\r\n  - [ ] P1 (Large) Replace CPU-based batch expansion with multi-query attention kernel call\r\n  - [ ] P1 (Medium) Automate speculative decoding https://github.com/vllm-project/vllm/issues/4565\r\n* Optimizations for both proposal and scoring time https://github.com/vllm-project/vllm/issues/5561\r\n  - [x] P0 (Medium) Decouple sampling serialization from sampling \r\n  - [x] P1 (Large) Amortize `prepare_inputs` over multiple forward passes\r\n* Optimizations for scheduling time\r\n  - [x] P0 (Medium) Profile & optimize block manager V2 https://github.com/vllm-project/vllm/issues/4536  \r\n\r\n## FAQ\r\n### What should the target configuration be for 50% speedup?\r\nIn the Anyscale fork we saw a 50% speedup on bs=8 with a 68m-sized draft model on TP1/70B target model on TP8 and a 7B draft model on TP(1|8)/70B target model on TP8. This was with the optimizations listed above as \"P0\".\r\n\r\nNote we can do much better than this, with multi-query scoring (P1), GQA for target model scoring, and a dynamic speculation policy. This is just the starting point!\r\n\r\n### Why not implement Medusa / tree-attention?\r\nWe should implement this! The work here will lay the foundation for future improvements in speculative decoding. For example, Eagle uses the Medusa approach (fine-tuned heads plus tree attention) and even claims to beat Medusa. But for Eagle to work well in vLLM we need to optimize the sampler as listed above.\r\n\r\nThe north star should be: configurable tree size (top-k .. top-1), which uses multi-query attention for scoring (no batch expansion). This issue is about optimizing vLLM in the top-1 speculation case to get 50% speedup with draft models.",
    "labels": [
      "help wanted",
      "performance",
      "speculative-decoding"
    ],
    "state": "closed",
    "created_at": "2024-05-06T17:16:50+00:00",
    "closed_at": "2024-08-05T18:05:31+00:00",
    "comments": 28,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4630/reactions",
      "total_count": 16,
      "+1": 16,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4630"
  },
  {
    "number": 18037,
    "title": "[RFC]: Enabling Suffix Decoding, LSTM Speculator, Sequence Parallelism from Arctic Inference",
    "body": "### Motivation.\n\nSnowflake AI Research has recently released several optimizations like Suffix Decoding, LSTM Speculation, Sequence Parallelism, SwiftKV etc, improving TTFT, TPOT and throughput for vLLM via a plugin called Arctic Inference (repo: https://github.com/snowflakedb/arcticinference).\n\n**Performance Improvements**\n- 4x faster generation with [Suffix Decoding](https://www.snowflake.com/en/engineering-blog/fast-speculative-decoding-vllm-arctic/) for SWEBench\n- 2.4x faster generation with [LSTM Speculator](https://www.snowflake.com/en/engineering-blog/fast-speculative-decoding-vllm-arctic/) for ShareGPT\n- 2.8x faster coding with [LSTM Speculator](https://www.snowflake.com/en/engineering-blog/fast-speculative-decoding-vllm-arctic/) for HumanEval\n- 2x higher throughput with [SwiftKV](https://www.snowflake.com/en/engineering-blog/swiftkv-llm-compute-reduction/)\n- 1.4x throughput than TP=8, but same TTFT as TP=8 with [Arctic Ulysses](https://www.snowflake.com/en/engineering-blog/ulysses-low-latency-llm-inference/)\n\nThese optimizations are designed to improve vLLM performance for real production workloads at Snowflake, and will continue to be expanded, maintained, and improved over time\n\nCurrently, Arctic Inference is implemented as an external plugin for vLLM. This means users should install both vLLM and Arctic Inference, and then run vLLM with additional configs like:\n\n```\npip install vllm==v0.8.4 arctic-inference\n\nvllm serve ... \\\n    --sequence-parallel-size ... \\\n    --speculative-config '{\"method\": \"suffix\"}'\n```\n\nProblem\n--------\n\nCurrently, Arctic Inference\u2019s primary user is Snowflake\u2019s own production Cortex inference service. However, we believe that other users in the community can also benefit from its features (there is some interest expressed to us after our most recent speculative decoding blog).\n\nFirst, how can we make these optimizations more accessible to the vLLM community? While some features (e.g. SwiftKV) are more model-specific, others like Suffix Decoding can provide value to users more generally. We believe more users can benefit if they can use these features through vLLM directly.\n\nSecond, how can we build a smooth path for \u201cgraduating\u201d features from Arctic Inference to vLLM, if desired, as they become available, more generally used, and as their interfaces stabilize?\n\n### Proposed Change.\n\nPossible Solutions\n------------------\n\n1. **All Arctic Inference features remain only available through its plugin.** Basically, keep things the way it is. Arctic Inference features do not get integrated to vLLM directly, and interested users are directed to the Arctic Inference project to install and use it with their installation of vLLM. Of course, users will need to go through extra discovery + installation steps, but it is the simplest. Perhaps Arctic Inference can be highlighted in documentation as an ecosystem project that helps boost performance.\n\n2. **Some Arctic Inference features are directly integrated into vLLM.** This way, vLLM users can benefit from those features without first having to discover the Arctic Inference project (some users asked us about this). The integration can be done in two ways:\n\n    1. Copy the code from Arctic Inference directly to vLLM. Does not require bringing on additional dependencies to vLLM, but will require additional effort to port each feature and maintain them. The Snowflake team will also keep maintaining and improving the same features as part of Arctic Inference, which will make this a duplicated effort. This strategy also does not provide a very smooth \u201cgraduation\u201d ramp for features that vLLM might want to integrate in the future.\n    &nbsp;\n    This also defeats the purpose of supporting [vLLM plugins](https://docs.vllm.ai/en/latest/design/plugin_system.html), which are meant to allow vLLM to leverage optimizations directly without incorporating them into the vLLM\u2019s code base.\n\n    2. Install Arctic Inference as a dependency of vLLM. Then, certain features (e.g. there is interest in Suffix Decoding) can be integrated directly in vLLM, making them immediately available to all vLLM users. Other Arctic Inference features can also be used by optionally enabling its plugin, defaulting to off. The rest of Arctic Inference can even be installed just-in-time, if desired.\n    &nbsp;\n    There is a question of maintenance burden due to the dependency on Arctic Inference. However, this burden is much lower than other libraries because *Arctic Inference purely exists as an extension to vLLM*, which means:\n        1. A part of the Arctic Inference package can be explicitly designated for importing into vLLM, and their interface stability guaranteed (e.g. in a `arctic_inference.core` submodule).\n        2. Secondary dependencies are minimal (in fact they are currently non-existent) since Arctic Inference inherits all its major dependencies from vLLM itself, such as torch. For optional features (e.g. those outside `arctic_inference.core`), the Arctic Inference team is responsible for upgrading them after each substantial vLLM release.\n\nRecommended Solution\n------------------------\n\nBased on the tradeoff between these three options, we are proposing **option 2.ii**:\n- Install Arctic Inference as part of vLLM as a dependency.\n- Arctic Inference to create the core submodule, starting with suffix decoding, that has guaranteed interface stability and minimal sub-dependencies.\n- The code for integrating suffix decoding with vLLM is contributed to vLLM directly, enabling users to use it natively in vLLM with no extra steps.\n- Other features in Arctic Inference can be used on an opt-in basis by enabling the Arctic Inference plugin (off by default).\n- Once this integration is established, future features can be smoothly \u201cgraduated\u201d into native vLLM usage simply by writing the integration code in vLLM.\n\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n@simon-mo @WoosukKwon @LiuXiaoxuanPKU @sfc-gh-jrasley @sfc-gh-srajbhandari\n\n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC",
      "speculative-decoding"
    ],
    "state": "open",
    "created_at": "2025-05-13T01:25:02+00:00",
    "closed_at": null,
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18037/reactions",
      "total_count": 16,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 16,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/18037"
  },
  {
    "number": 15901,
    "title": "[SpecDecode] Support EAGLE in V1",
    "body": "- [x] 1. Correctly initializing and loading the EAGLE draft model\n- [x] 2. Consider the lookahead slots in the KV cache manager\n- [x] 3. Cache `draft_probs` inside the model runner and correctly feed it to the rejection sampler in the next step (temporarily workaround: #16899)\n- [x] 4. Handle the edge cases like when the draft model generates beyond `max_pos_embeddings`\n- [ ] 5. Handle the seeds correctly\n- [ ] 6. Do E2E correctness and performance tests\n- [x] 7. Support prefix caching. Eagle requires special handling because Eagle's i-th KV cache is coupled with the i+1-th token ID. (@LiuXiaoxuanPKU)\n- [ ] 8. Properly handle the sampling parameters that are not (currently) compatible with spec decoding (e.g., min_p).\n- [x] 9. Use CUDA graphs for draft model. (@luyuzhe111)\n- [x] 10. Support Eagle 3 (https://github.com/vllm-project/vllm/pull/16937)\n\n_Originally posted by @WoosukKwon in https://github.com/vllm-project/vllm/issues/15729#issuecomment-2765192455_\n            ",
    "labels": [
      "speculative-decoding",
      "v1"
    ],
    "state": "open",
    "created_at": "2025-04-01T19:45:13+00:00",
    "closed_at": null,
    "comments": 21,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15901/reactions",
      "total_count": 11,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 10,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15901"
  },
  {
    "number": 10442,
    "title": "[Bug]: Speculative decoding + guided decoding not working",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nWhen using speculative decoding plus guided decoding (outlines), the output is truncated to like 5 tokens on return. I am using ngram speculation and extracting company names from a document:\r\n```\r\n\r\nfrom pydantic import BaseModel, Field\r\nfrom typing import List, Optional\r\nfrom datetime import date\r\n\r\n\r\nclass CompanyNameEntry(BaseModel):\r\n    company_name: str\r\n    #index: int\r\n    #duration: str\r\n\r\nclass CompanyNamesList(BaseModel):\r\n    company_names: List[CompanyNameEntry]\r\n\r\n\r\n\r\ncompletion = client.chat.completions.create(\r\n              model=\"model\",\r\n              messages=[\r\n                {\"role\": \"user\", \"content\": document_instruction}\r\n              ],\r\n              extra_body={\r\n                \"guided_json\": CompanyNamesList.schema(),\r\n              }\r\n        )\r\n\r\n```\r\nIt states here that this mode is supported.\r\n\r\nhttps://docs.vllm.ai/en/stable/serving/compatibility_matrix.html\r\n\r\nCan you please confirm this is not supported yet? Can you point me how to add this functionality ?\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "structured-output",
      "speculative-decoding"
    ],
    "state": "open",
    "created_at": "2024-11-19T06:13:22+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10442/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10442"
  },
  {
    "number": 8073,
    "title": "[Bug]: Persistent OutOfMemoryError error when using speculative decoding ",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.3.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.22.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1067-aws-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A10G\r\nNvidia driver version: 535.161.07\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               16\r\nOn-line CPU(s) list:                  0-15\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD EPYC 7R32\r\nCPU family:                           23\r\nModel:                                49\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   8\r\nSocket(s):                            1\r\nStepping:                             0\r\nBogoMIPS:                             5600.00\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save rdpid\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            256 KiB (8 instances)\r\nL1i cache:                            256 KiB (8 instances)\r\nL2 cache:                             4 MiB (8 instances)\r\nL3 cache:                             32 MiB (2 instances)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-15\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec rstack overflow:   Mitigation; safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==0.4.3\r\n[pip3] numpy==1.23.5\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] optree==0.12.1\r\n[pip3] sentence-transformers==2.7.0\r\n[pip3] torch==2.3.1+cu121\r\n[pip3] torcheval==0.0.7\r\n[pip3] torchvision==0.18.1+cu121\r\n[pip3] transformers==4.41.2\r\n[pip3] triton==2.3.1\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \t0-15\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI get an out of memory error when using speculative decoding (SD). This happens in the middle of generation or after several batches completed successfully. \r\nI have tried reducing `max_model_len`, `max_num_batched_tokens` and `gpu_memory_utilization`, but to no avail. \r\n\r\n**Usage** \r\n```python\r\nllm = LLM(\"nvidia/Llama-3.1-Minitron-4B-Width-Base\", enable_prefix_caching=True, block_size=3, gpu_memory_utilization=0.8, max_num_batched_tokens=11000,  max_model_len=11000, speculative_model=\"ibm-fms/llama3-8b-accelerator\", num_speculative_tokens=1, use_v2_block_manager=True)\r\nsampling_params = SamplingParams(temperature=0.5, top_p=0.2, max_tokens=1)\r\nformatted_prompts = [\"array of len 10000 to 400000, each around 100-200 tokens\"] \r\nfrom outlines import models, generate\r\nmodel = models.VLLM(llm)\r\ngenerator = generate.choice(model, [\"yes\", \"no\"])\r\npredictions = generator(formatted_prompts, sampling_params=sampling_params)\r\n```\r\n\r\nIn contrast, when SD is disabled, runs always complete even at high max_model_len`, `max_num_batched_tokens` and `gpu_memory_utilization`.\r\n\r\n```python\r\nllm = LLM(\"nvidia/Llama-3.1-Minitron-4B-Width-Base\", enable_prefix_caching=True, block_size=3,max_num_batched_tokens=20000, gpu_memory_utilization=0.95, max_model_len=20000, use_v2_block_manager=True)\r\n```\r\n\r\n\r\n**Error trace**\r\n```python\r\nOutOfMemoryError: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 21.99 GiB of which 337.38 MiB is free. Process 30125 has 21.64 GiB memory in use. Of the allocated memory 18.20 GiB is allocated by PyTorch, with 27.10 MiB allocated in private pools (e.g., CUDA Graphs), and 2.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\nFile <command-1108086693486187>, line 1\r\n----> 1 predictions = generator(formatted_prompts, sampling_params=sampling_params)\r\nFile /local_disk0/.ephemeral_nfs/envs/pythonEnv-6e87dd7d-c6c7-4a06-8001-d206e2d3c391/lib/python3.11/site-packages/outlines/generate/api.py:503, in SequenceGeneratorAdapter.__call__(self, prompts, max_tokens, stop_at, seed, **model_specific_params)\r\n    497         return self.format_sequence(sequences)\r\n    499 generation_params = self.prepare_generation_parameters(\r\n    500     max_tokens, stop_at, seed\r\n    501 )\r\n--> 503 completions = self.model.generate(\r\n    504     prompts,\r\n    505     generation_params,\r\n    506     self.logits_processor,\r\n    507     self.sampling_params,\r\n    508     **model_specific_params,\r\n    509 )\r\n    511 return format(completions)\r\nFile /local_disk0/.ephemeral_nfs/envs/pythonEnv-6e87dd7d-c6c7-4a06-8001-d206e2d3c391/lib/python3.11/site-packages/outlines/models/vllm.py:105, in VLLM.generate(self, prompts, generation_parameters, logits_processor, sampling_parameters, sampling_params)\r\n    102 if sampler == \"beam_search\":\r\n    103     sampling_params.use_beam_search = True\r\n--> 105 results = self.model.generate(\r\n    106     prompts, sampling_params=sampling_params, lora_request=self.lora_request\r\n    107 )\r\n    108 results = [[sample.text for sample in batch.outputs] for batch in results]\r\n    110 batch_size = len(results)\r\nFile /local_disk0/.ephemeral_nfs/envs/pythonEnv-6e87dd7d-c6c7-4a06-8001-d206e2d3c391/lib/python3.11/site-packages/vllm/utils.py:1030, in deprecate_kwargs.<locals>.wrapper.<locals>.inner(*args, **kwargs)\r\n   1023             msg += f\" {additional_message}\"\r\n   1025         warnings.warn(\r\n   1026             DeprecationWarning(msg),\r\n   1027             stacklevel=3,  # The inner function takes up one level\r\n   1028         )\r\n-> 1030 return fn(*args, **kwargs)\r\nFile /local_disk0/.ephemeral_nfs/envs/pythonEnv-6e87dd7d-c6c7-4a06-8001-d206e2d3c391/lib/python3.11/site-packages/vllm/entrypoints/llm.py:345, in LLM.generate(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request)\r\n    336     sampling_params = SamplingParams()\r\n    338 self._validate_and_add_requests(\r\n    339     inputs=inputs,\r\n    340     params=sampling_params,\r\n    341     lora_request=lora_request,\r\n    342     prompt_adapter_request=prompt_adapter_request,\r\n    343     guided_options=guided_options_request)\r\n--> 345 outputs = self._run_engine(use_tqdm=use_tqdm)\r\n    346 return LLMEngine.validate_outputs(outputs, RequestOutput)\r\nFile /local_disk0/.ephemeral_nfs/envs/pythonEnv-6e87dd7d-c6c7-4a06-8001-d206e2d3c391/lib/python3.11/site-packages/vllm/entrypoints/llm.py:686, in LLM._run_engine(self, use_tqdm)\r\n    684 total_out_toks = 0\r\n    685 while self.llm_engine.has_unfinished_requests():\r\n--> 686     step_outputs = self.llm_engine.step()\r\n    687     for output in step_outputs:\r\n    688         if output.finished:\r\nFile /local_disk0/.ephemeral_nfs/envs/pythonEnv-6e87dd7d-c6c7-4a06-8001-d206e2d3c391/lib/python3.11/site-packages/vllm/engine/llm_engine.py:1325, in LLMEngine.step(self)\r\n   1315     finished_requests_ids = self.scheduler[\r\n   1316         0].get_and_reset_finished_requests_ids()\r\n   1317     execute_model_req = ExecuteModelRequest(\r\n   1318         seq_group_metadata_list=seq_group_metadata_list,\r\n   1319         blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\r\n   (...)\r\n   1323         running_queue_size=scheduler_outputs.running_queue_size,\r\n   1324         finished_requests_ids=finished_requests_ids)\r\n-> 1325     output = self.model_executor.execute_model(\r\n   1326         execute_model_req=execute_model_req)\r\n   1327 else:\r\n   1328     output = []\r\nFile /local_disk0/.ephemeral_nfs/envs/pythonEnv-6e87dd7d-c6c7-4a06-8001-d206e2d3c391/lib/python3.11/site-packages/vllm/executor/gpu_executor.py:129, in GPUExecutor.execute_model(self, execute_model_req)\r\n    126 def execute_model(\r\n    127     self, execute_model_req: ExecuteModelRequest\r\n    128 ) -> Optional[List[Union[SamplerOutput, PoolerOutput]]]:\r\n--> 129     output = self.driver_worker.execute_model(execute_model_req)\r\n    130     return output\r\nFile /local_disk0/.ephemeral_nfs/envs/pythonEnv-6e87dd7d-c6c7-4a06-8001-d206e2d3c391/lib/python3.11/site-packages/torch/utils/_contextlib.py:116, in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    113 @functools.wraps(func)\r\n    114 def decorate_context(*args, **kwargs):\r\n    115     with ctx_factory():\r\n--> 116         return func(*args, **kwargs)\r\nFile /local_disk0/.ephemeral_nfs/envs/pythonEnv-6e87dd7d-c6c7-4a06-8001-d206e2d3c391/lib/python3.11/site-packages/vllm/spec_decode/spec_decode_worker.py:402, in SpecDecodeWorker.execute_model(self, execute_model_req)\r\n    398 self._maybe_disable_speculative_tokens(\r\n    399     disable_all_speculation, execute_model_req.seq_group_metadata_list)\r\n    401 if no_spec:\r\n--> 402     return self._run_no_spec(execute_model_req,\r\n    403                              skip_proposer=disable_all_speculation)\r\n    404 return self._run_speculative_decoding_step(execute_model_req,\r\n    405                                            num_lookahead_slots)\r\nFile /usr/lib/python3.11/contextlib.py:81, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)\r\n     78 @wraps(func)\r\n     79 def inner(*args, **kwds):\r\n     80     with self._recreate_cm():\r\n---> 81         return func(*args, **kwds)\r\nFile /local_disk0/.ephemeral_nfs/envs/pythonEnv-6e87dd7d-c6c7-4a06-8001-d206e2d3c391/lib/python3.11/site-packages/vllm/spec_decode/spec_decode_worker.py:494, in SpecDecodeWorker._run_no_spec(self, execute_model_req, skip_proposer)\r\n    491         self.previous_hidden_states = HiddenStates(\r\n    492             hidden_states, execute_model_req.seq_group_metadata_list)\r\n    493     else:\r\n--> 494         self.previous_hidden_states.update(\r\n    495             hidden_states, execute_model_req.seq_group_metadata_list)\r\n    497 if not skip_proposer:\r\n    498     # We prepare the prefill hidden states here so that there no\r\n    499     # additional complexity in worker for spec_decode vs non_spec_decode\r\n    500     # flow and execute_model doesn't need additional modifications.\r\n    501     execute_model_req.previous_hidden_states = \\\r\n    502         prepare_prefill_hidden_states(\r\n    503             sampler_output.prefill_hidden_states)\r\nFile /local_disk0/.ephemeral_nfs/envs/pythonEnv-6e87dd7d-c6c7-4a06-8001-d206e2d3c391/lib/python3.11/site-packages/vllm/sequence.py:1217, in HiddenStates.update(self, hidden_states, seq_group_metadata_list, second_last_token_hidden_states)\r\n   1215 assert len(seq_group_metadata_list) == len(hidden_states)\r\n   1216 self._seq_ids.extend(get_all_seq_ids(seq_group_metadata_list))\r\n-> 1217 self.hidden_states = torch.cat([self.hidden_states, hidden_states])\r\n   1219 if self.second_last_token_hidden_states is not None:\r\n   1220     # Adding dummy hidden_states to this to maintain same shape\r\n   1221     self.second_last_token_hidden_states = torch.cat([\r\n   1222         self.second_last_token_hidden_states,\r\n   1223         torch.zeros_like(hidden_states)\r\n   1224         if second_last_token_hidden_states is None else\r\n   1225         second_last_token_hidden_states\r\n   1226     ])```\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "speculative-decoding"
    ],
    "state": "open",
    "created_at": "2024-09-02T06:41:55+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8073/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8073"
  }
]
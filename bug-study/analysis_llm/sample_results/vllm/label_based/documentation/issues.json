[
  {
    "number": 1555,
    "title": "Guidance on how many requests can be processed at a time?",
    "body": "Hello - \r\n\r\nI am trying to understand how many requests can be processed in parallel with the llm_engine, and what keeps requests WAITING. I see various variables like \"max_num_batched_tokens\" and \"max_num_seqs\", but more details or documentation describing how this process occurs would be helpful. Moreover, how can we tune our system to do process more requests in parallel (e.g. use more GPUs if available, use smaller models, use smaller context windows, etc.)",
    "labels": [
      "documentation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-11-03T19:27:46+00:00",
    "closed_at": "2024-12-01T02:15:58+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1555/reactions",
      "total_count": 14,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 8
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1555"
  },
  {
    "number": 13021,
    "title": "[Doc]: No max_model_len parameter in the LLM class",
    "body": "### \ud83d\udcda The doc issue\n\nIn this url: https://docs.vllm.ai/en/latest/serving/offline_inference.html\nI see that there is no max_model_len parameter in the LLM class, but the documentation still says \n    llm = LLM(model=\"adept/fuyu-8b\",\n    max_model_len=2048,\n    max_num_seqs=2)\nBtw, I wonder how can I change the max_seq_len when I use offline_inference? \n\n### Suggest a potential alternative/fix\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2025-02-10T08:23:08+00:00",
    "closed_at": "2025-02-10T16:16:36+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13021/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13021"
  },
  {
    "number": 65,
    "title": "Add documents on how to add new models",
    "body": null,
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2023-05-04T09:05:56+00:00",
    "closed_at": "2023-06-06T03:01:28+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/65/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/65"
  },
  {
    "number": 19603,
    "title": "[Doc]: Improve CPU documentation for ARM",
    "body": "### \ud83d\udcda The doc issue\n\nAt the moment, vLLM supports a variety of GPUs, some NPUs/TPUs, and (at the very least) x86 and ARM CPUs. At the moment, however, the documentation is very sparse for ARM CPUs, other than that it *can* be used. As a matter of fact, not including the API docs, I can only find [this one page](https://docs.vllm.ai/en/latest/getting_started/installation/cpu.html?h=arm) which mentions it. Notably missing is any mention of ARM in the [Supported Hardware](https://docs.vllm.ai/en/latest/features/quantization/supported_hardware.html) page for quantization.\n\n### Suggest a potential alternative/fix\n\nIt would be helpful to have at least some indication of what is and isn't supported on ARM CPUs. I would be happy to contribute this myself, but I'm not sure of the best way to determine compatibility. Would it make sense to simply run a small model of every possible quantization format to see what happens? Or is there something a bit more elegant?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "open",
    "created_at": "2025-06-13T08:08:35+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19603/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19603"
  },
  {
    "number": 4684,
    "title": "[Doc]: API reference for LLM class",
    "body": "### \ud83d\udcda The doc issue\n\nI can't find on https://docs.vllm.ai/ the API reference for the `LLM` class. This would make it easier than digging through the code to look at the docstrings - the examples in the docs don't explain most of the arguments.\n\n### Suggest a potential alternative/fix\n\n_No response_",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2024-05-08T15:44:46+00:00",
    "closed_at": "2024-05-14T00:47:43+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4684/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4684"
  },
  {
    "number": 14421,
    "title": "[Bug]: low quality of deepseek-vl2 when using vllm",
    "body": "### \ud83d\udcda The doc issue\n\nWhen I use the official inference code of deepseek-vl2, the model output seems normal\uff0c\nQuestion: \u201c\u56fe\u7247\u4e2d\u7684\u89d2\u8272\u662f\u54ea\u90e8\u52a8\u6f2b\u4f5c\u54c1\u4e2d\u7684\u4eba\u7269\uff1f\u201d\n![Image](https://github.com/user-attachments/assets/44dca395-f1dd-4b0c-87d6-d492df512976)\nModel Output\uff1a\n![Image](https://github.com/user-attachments/assets/39332703-d613-403d-bcb0-e0cbb8e266d7)\n\nBut when I use vllm for inference, and I use the [chat_template](https://github.com/vllm-project/vllm/blob/main/examples/template_deepseek_vl2.jinja)\uff0cthe model output seems abnormal.\nI start vllm by\uff1a\n`CUDA_VISIBLE_DEVICES=7 vllm serve deepseek-vl2 --port 8102 --max-model-len 4096 --hf_overrides '{\"architectures\":[\"DeepseekVLV2ForCausalLM\"]}' --gpu-memory-utilization 0.9 --chat_template ./template_deepseek_vl2.jinja`\nModel Output\uff1a\n![Image](https://github.com/user-attachments/assets/ea545f6e-fe01-4f67-a50a-65bbed41f86a)\n\nI feel that the accuracy has dropped significantly. I don't know if it's a problem with chat_template or somewhere else. \n\n### Suggest a potential alternative/fix\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2025-03-07T09:03:24+00:00",
    "closed_at": "2025-03-11T04:37:13+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14421/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14421"
  },
  {
    "number": 6979,
    "title": "[Doc]: Supported Hardware for Quantization Kernels",
    "body": "### \ud83d\udcda The doc issue\n\nI'm confused what \"the quantization method is supported\" mean?  Ampere arch doesn't support FP8, according to Nvidia. So does this mean the FP8 operation is supported on A100/A800 GPU?  Or just we can conver the weight parameters form FP16 to FP8? \n\n### Suggest a potential alternative/fix\n\n_No response_",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2024-07-31T07:16:05+00:00",
    "closed_at": "2024-07-31T15:33:53+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6979/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6979"
  },
  {
    "number": 6762,
    "title": "[Doc]: ROCm installation instructions do not work",
    "body": "### \ud83d\udcda The doc issue\n\nFollowing the instructions at https://docs.vllm.ai/en/latest/getting_started/amd-installation.html#build-from-source-rocm, using the exact Docker image mentioned (pytorch_rocm6.1.2_ubuntu20.04_py3.9_pytorch_staging.sif, although with a custom Python venv and Pytorch install), and run into the following error when running `python setup.py develop`:\r\n```\r\nBuilding PyTorch for GPU arch: gfx90a\r\n-- Could NOT find HIP: Found unsuitable version \"0.0.0\", but required is at least \"1.0\" (found /opt/rocm)\r\nHIP VERSION: 0.0.0\r\nCMake Warning at .venv/lib/python3.10/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\r\n  static library kineto_LIBRARY-NOTFOUND not found.\r\nCall Stack (most recent call first):\r\n  .venv/lib/python3.10/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:120 (append_torchlib_if_found)\r\n  CMakeLists.txt:67 (find_package)\r\n\r\n\r\nCMake Error at CMakeLists.txt:108 (message):\r\n  Can't find CUDA or HIP installation.\r\n  ```\r\n\r\nThe docker image should have a proper HIP setup, right? `hipconfig` says: `HIP version  : 6.1.40093-bd86f1708`.\n\n### Suggest a potential alternative/fix\n\n_No response_",
    "labels": [
      "documentation",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2024-07-24T21:49:00+00:00",
    "closed_at": "2024-09-25T14:33:49+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6762/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/6762"
  },
  {
    "number": 7133,
    "title": "[Doc]: How to Build an AI Knowledge Base like AI-Ask with vLLM",
    "body": "### \ud83d\udcda The doc issue\n\nHello,\r\n\r\nI am interested in building an AI knowledge base similar to the AI-Ask feature provided by vLLM. I have a few questions regarding its implementation:\r\n\r\n\t1.\tHow can I achieve the construction of such an AI knowledge base using vLLM?\r\n\t2.\tIs it based on any specific open-source solutions or frameworks?\r\n\t3.\tAre there any guidelines or documentation that could help in setting up a similar system?\r\n\r\nAny insights or pointers would be greatly appreciated!\r\n\r\n<img width=\"792\" alt=\"image\" src=\"https://github.com/user-attachments/assets/e4f6f7ca-72b2-4f9d-afbf-f62cbcba69e0\">\r\n\r\n\r\nThank you!\n\n### Suggest a potential alternative/fix\n\n_No response_",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2024-08-05T01:35:50+00:00",
    "closed_at": "2024-08-05T01:46:22+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7133/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7133"
  },
  {
    "number": 19359,
    "title": "[Doc]: Backwards compability with PyTorch",
    "body": "### \ud83d\udcda The doc issue\n\nvLLM has become a very important component of the OSS ecosystem and is tightly integrated with [`torch`](https://github.com/pytorch/pytorch) and [`triton-lang`](https://github.com/triton-lang/triton/tree/main)\n\nFrom looking at the requirements it looks like the philosophy is to only ensure full compatibility with the newest PyTorch version (currently 2.7.0) [here](https://github.com/vllm-project/vllm/blob/59abbd84f90e5930c37e205de8849ac4fa8a96c7/requirements/cuda.txt#L9) which then also somewhat automatically pins the required triton version.\n\nWhile torch tries to stay backward compatible with earlier torch versions, this is not always ensured for libraries depending on torch & triton. VLLM for example uses custom triton code here: https://github.com/vllm-project/vllm/blob/59abbd84f90e5930c37e205de8849ac4fa8a96c7/vllm/attention/ops/prefix_prefill.py#L134 (and many other places) that is not always compatible with earlier triton versions. E.g. triton=3.1.0 doesn't have a `loop_unroll_factor` in its `tl.range` (see [here](https://github.com/triton-lang/triton/blob/cf34004b8a67d290a962da166f5aa2fc66751326/python/triton/language/core.py#L2473)). \n\nHence vllm can't use PT 2.5.xx because 2.5.xx relies on triton 3.1.xxx. Is this the expected behavior of vLLM - to not ensure full backward comp support for older PT versions? It would be amazing to support PT versions that are at least ~2 years old, but it's understandably quite painful to do so. Could it be possible to clarify backwards comp philosophy w.r.t. PT somewhere in the docs?\n\n\n\n### Suggest a potential alternative/fix\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "open",
    "created_at": "2025-06-09T10:33:32+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19359/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19359"
  },
  {
    "number": 16452,
    "title": "[Doc]: Update outdated note: LMCache now supports chunked prefill",
    "body": "### \ud83d\udcda The doc issue\n\nIn the file [examples/offline_inference/cpu_offload_lmcache.py](https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/cpu_offload_lmcache.py), line 43 states:\n\n`# Note that LMCache is not compatible with chunked prefill for now.`\n\nThis is now outdated. Both vLLM and LMCache have merged PRs to fully support chunked prefill:\n\n[vLLM PR #14505](https://github.com/vllm-project/vllm/pull/14505), [LMCache PR #392](https://github.com/LMCache/LMCache/pull/392)\n\nThe current note may mislead users into disabling a working feature.\n\n### Suggest a potential alternative/fix\n\nUpdate the comment to either:\n\n`# Note: LMCache supports chunked prefill (see vLLM#14505, LMCache#392).  `\n\nOr remove it entirely if compatibility is now considered stable/default.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2025-04-11T03:58:33+00:00",
    "closed_at": "2025-04-18T05:12:43+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16452/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16452"
  },
  {
    "number": 11880,
    "title": "[Doc]: Add GitHub Action to auto-sync Dockerfile dependency graph",
    "body": "### \ud83d\udcda The doc issue\r\n\r\nCurrently, the Dockerfile dependency graph (docs/source/assets/contributing/dockerfile-stages-dependency.png) may become out of sync with the actual Dockerfile when changes are made. This can lead to outdated or incorrect documentation.\r\n\r\n### Suggest a potential alternative/fix\r\n\r\nI propose adding a GitHub Action workflow that automatically:\r\n\r\n1. Regenerates the dependency graph when Dockerfile changes.\r\n2. Creates a PR if the graph has changed.\r\n\r\nI've prepared a draft workflow here: https://github.com/vllm-project/vllm/pull/11879\r\n\r\nThis will help ensure the documentation stays accurate and reduce maintenance overhead.\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-09T03:32:02+00:00",
    "closed_at": "2025-04-10T13:43:06+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11880/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11880"
  },
  {
    "number": 17120,
    "title": "[Doc]: Documentation source code hyperlinks do not always point to the correct source code",
    "body": "### \ud83d\udcda The doc issue\n\nFor example, the  documenation of the ``generate`` method in [LLM Class](https://docs.vllm.ai/en/latest/api/offline_inference/llm.html#vllm.LLM.generate).\nClicking its ``source`` button, we find that it points to [an unrelated function in utils](https://github.com/vllm-project/vllm/blob/main/vllm/utils.py#L373).\n\nThis is just one example, a similar thing happens with the link for ``LLM.encode``.\n\n### Suggest a potential alternative/fix\n\nI am not familiar with the way Sphinx generates documentation, but there may be something to be done in one of its configs.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2025-04-24T16:11:06+00:00",
    "closed_at": "2025-04-24T17:39:44+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17120/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17120"
  },
  {
    "number": 10316,
    "title": "[Doc]: OpenAI Chat Completion Client For Multimodal missing using video as input for qwen2-vl",
    "body": "### \ud83d\udcda The doc issue\n\nthe [doc](https://docs.vllm.ai/en/latest/getting_started/examples/openai_chat_completion_client_for_multimodal.html) only shows using multiple images, but how to use video as input?\r\n\n\n### Suggest a potential alternative/fix\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2024-11-14T07:11:12+00:00",
    "closed_at": "2024-11-14T08:35:59+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10316/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10316"
  },
  {
    "number": 15006,
    "title": "[Doc]: ValueError: Qwen2_5_VLForSequenceClassification has no vLLM implementation and the Transformers implementation is not compatible with vLLM.",
    "body": "### \ud83d\udcda The doc issue\n\nValueError: Qwen2_5_VLForSequenceClassification has no vLLM implementation and the Transformers implementation is not compatible with vLLM.\n\nGPU:T4 *4  seq_cls\n\n### Suggest a potential alternative/fix\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2025-03-18T06:09:09+00:00",
    "closed_at": "2025-03-31T09:04:10+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15006/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15006"
  },
  {
    "number": 14344,
    "title": "[Doc]: How can I set the date_string for the chat templates",
    "body": "### \ud83d\udcda The doc issue\n\nHello everyone,\n\nI use the chat_template like this, similar to [tool_chat_template_llama3.1_json.jinja](https://github.com/vllm-project/vllm/blob/main/examples/tool_chat_template_llama3.1_json.jinja):\n```\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n```\nand I call the model like this:\n```\nclient.chat.completions.create(\n            messages=messages,\n            model=model,\n            temperature=0.0,\n            max_completion_tokens=300,\n            stream=True,\n            tools=tools,\n            tool_choice=tool_choice\n        )\n```\nSo, now my question: How can I set the date_string variable?\n\nThank you for your help.\n\nBest regards,\nFelix\n\n### Suggest a potential alternative/fix\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2025-03-06T09:00:38+00:00",
    "closed_at": "2025-03-07T02:50:05+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14344/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14344"
  },
  {
    "number": 2043,
    "title": "The returned results using prompt_logprobs=1",
    "body": "It seems that setting prompt_logprobs=1 will return the scoring of the context (i.e., prompt). However, the returned results are a little confusing:\r\n\r\nWhen I used a Llama-2-7b to score a sequece, the returned results look as follow:\r\n\r\n[None, {15043: -7.584228515625, 917: -2.512939214706421}, {29892: -1.4937736988067627}, {590: -1.8308428525924683, 306: -1.3464678525924683}, {1024: -0.11963547021150589}, {338: -0.01794273406267166}]\r\n\r\nThe first and third positions have two keys while the other positions have only 1 key. Is that because the position's word is not the word with the highest prob?",
    "labels": [
      "documentation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-12-11T23:23:20+00:00",
    "closed_at": "2024-12-01T02:15:40+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2043/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2043"
  },
  {
    "number": 4857,
    "title": "[Doc]: can't fing serving_embedding.py",
    "body": "### \ud83d\udcda The doc issue\n\nI see the introduction for OpenAI Embedding Client \r\nhttps://docs.vllm.ai/en/latest/getting_started/examples/openai_embedding_client.html\r\nand I see the code in Github also.\r\n\r\nBut I couldn't find the code in vllm 0.4.2.\n\n### Suggest a potential alternative/fix\n\n_No response_",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2024-05-16T09:31:41+00:00",
    "closed_at": "2024-05-16T19:21:30+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4857/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4857"
  },
  {
    "number": 14319,
    "title": "[Doc]: Why is max block_size on CUDA 32?",
    "body": "### \ud83d\udcda The doc issue\n\nIn the args:\nhttps://github.com/vllm-project/vllm/blob/main/vllm/engine/arg_utils.py#L454\nit says about block_size parameter:\n\n> Token block size for contiguous chunks of tokens. This is ignored on neuron devices and set to --max-model-len. On CUDA devices, only block sizes up to 32 are supported. On HPU devices, block size defaults to 128.\n\n1. Where is this requirement for <= 32 on CUDA devices coming from?\n2. I was able to successfully run vLLM with block_size 128 on Hopper and see some minor performance improvement. Is the requirement up to date?\n3. In flash attention docs I see that paged attention minimum block size is actually 256:\nhttps://github.com/Dao-AILab/flash-attention/blob/d82bbf26924c492064af8b27ab299ff4808d1bf6/hopper/flash_attn_interface.py#L662\nDoes vLLM use this interface? How does FA paged_block_size relates to vLLM block_size?\n\n### Suggest a potential alternative/fix\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-05T23:50:23+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14319/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14319"
  },
  {
    "number": 10566,
    "title": "[Doc]: Docker+vllm+fastchat deploys multimodal large model Qwen2-vl-7b-instruct(docker+vllm+fastchat\u90e8\u7f72\u591a\u6a21\u6001\u5927\u6a21\u578bQwen2-vl-7b-instruct)",
    "body": "### \ud83d\udcda The doc issue\n\nWhen testing using the following method, an error is reported, and the terminal log results are shown as follows:\r\n\r\ncurl -X POST http:(base) root@lxing:~# curl -X POST http://127.0.0.1:8000/v1/chat/completions \\\r\n-H \"Content-Type: application/json\" \\\r\n-d '{\r\n  \"model\": \"Qwen2-VL-7B-Instruct\",\r\n  \"messages\": [{\"role\": \"user\", \"content\": \"\u4f60\u597d\uff01\"}],\r\n  \"temperature\": 1.0,\r\n  \"max_tokens\": 100\r\n}'\r\n{\"object\":\"error\",\"message\":\"Internal Server Error\",\"code\":50001}\r\n\r\n\r\n2024-11-22 02:50:27 | INFO | stdout | INFO:     127.0.0.1:55822 - \"POST /model_details HTTP/1.1\" 200 OK \r\n2024-11-22 02:50:27 | INFO | stdout | INFO:     127.0.0.1:55838 - \"POST /count_token HTTP/1.1\" 200 OK\r\n2024-11-22 02:50:27 | INFO | stdout | INFO:     127.0.0.1:55844 - \"POST /worker_generate HTTP/1.1\" 500 Internal Server Error\r\n2024-11-22 02:50:27 | ERROR | stderr | ERROR:    Exception in ASGI application\r\n2024-11-22 02:50:27 | ERROR | stderr | Traceback (most recent call last):\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 401, in run_asgi\r\n2024-11-22 02:50:27 | ERROR | stderr |     result = await app(  # type: ignore[func-returns-value]\r\n2024-11-22 02:50:27 | ERROR | stderr |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\r\n2024-11-22 02:50:27 | ERROR | stderr |     return await self.app(scope, receive, send)\r\n2024-11-22 02:50:27 | ERROR | stderr |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n2024-11-22 02:50:27 | ERROR | stderr |     await super().__call__(scope, receive, send)\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/starlette/applications.py\", line 113, in __call__\r\n2024-11-22 02:50:27 | ERROR | stderr |     await self.middleware_stack(scope, receive, send)\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 187, in __call__\r\n2024-11-22 02:50:27 | ERROR | stderr |     raise exc\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n2024-11-22 02:50:27 | ERROR | stderr |     await self.app(scope, receive, _send)\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n2024-11-22 02:50:27 | ERROR | stderr |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n2024-11-22 02:50:27 | ERROR | stderr |     raise exc\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n2024-11-22 02:50:27 | ERROR | stderr |     await app(scope, receive, sender)\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/starlette/routing.py\", line 715, in __call__\r\n2024-11-22 02:50:27 | ERROR | stderr |     await self.middleware_stack(scope, receive, send)\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/starlette/routing.py\", line 735, in app\r\n2024-11-22 02:50:27 | ERROR | stderr |     await route.handle(scope, receive, send)\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/starlette/routing.py\", line 288, in handle\r\n2024-11-22 02:50:27 | ERROR | stderr |     await self.app(scope, receive, send)\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/starlette/routing.py\", line 76, in app\r\n2024-11-22 02:50:27 | ERROR | stderr |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n2024-11-22 02:50:27 | ERROR | stderr |     raise exc\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\r\n2024-11-22 02:50:27 | ERROR | stderr |     await app(scope, receive, sender)\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/starlette/routing.py\", line 73, in app\r\n2024-11-22 02:50:27 | ERROR | stderr |     response = await f(request)\r\n2024-11-22 02:50:27 | ERROR | stderr |                ^^^^^^^^^^^^^^^^\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/fastapi/routing.py\", line 301, in app\r\n2024-11-22 02:50:27 | ERROR | stderr |     raw_response = await run_endpoint_function(\r\n2024-11-22 02:50:27 | ERROR | stderr |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\r\n2024-11-22 02:50:27 | ERROR | stderr |     return await dependant.call(**values)\r\n2024-11-22 02:50:27 | ERROR | stderr |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/fastchat/serve/vllm_worker.py\", line 217, in api_generate\r\n2024-11-22 02:50:27 | ERROR | stderr |     output = await worker.generate(params)\r\n2024-11-22 02:50:27 | ERROR | stderr |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/fastchat/serve/vllm_worker.py\", line 173, in generate\r\n2024-11-22 02:50:27 | ERROR | stderr |     async for x in self.generate_stream(params):\r\n2024-11-22 02:50:27 | ERROR | stderr |   File \"/app/miniconda3/envs/vllm/lib/python3.12/site-packages/fastchat/serve/vllm_worker.py\", line 106, in generate_stream\r\n2024-11-22 02:50:27 | ERROR | stderr |     sampling_params = SamplingParams(\r\n2024-11-22 02:50:27 | ERROR | stderr |                       ^^^^^^^^^^^^^^^\r\n2024-11-22 02:50:27 | ERROR | stderr | TypeError: Unexpected keyword argument 'use_beam_search'\n\n### Suggest a potential alternative/fix\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-22T06:12:34+00:00",
    "closed_at": "2025-04-04T02:05:43+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10566/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10566"
  },
  {
    "number": 3910,
    "title": "[Doc]: Question about the timing of prefix caches release for multiples requests with long time interval. ",
    "body": "### \ud83d\udcda The doc issue\n\nIt's there has any docs about the  timing of prefix caches release ? specific for multiples requests with long time interval.  \r\n\r\n```bash\r\n1rd req [+ long prompt_0] -> after 10 mins -> 2rd req [+ long prompt_0] -> after 5 mins -> 3rd req [+ long prompt_0] -> ...\r\n       |                          |\r\n       |                          |\r\n------ cached --------->  will cache release here?  ------>  ....\r\n```\n\n### Suggest a potential alternative/fix\n\n_No response_",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2024-04-08T06:29:24+00:00",
    "closed_at": "2024-04-08T12:34:28+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3910/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3910"
  },
  {
    "number": 16783,
    "title": "[Doc]: Proposing a minor change in \"Metrics\" documentation.",
    "body": "### \ud83d\udcda The doc issue\n\nIn the \"Metrics\" section of the vLLM website, the vllm:generation_tokens_total metric is currently described as:\n\n[vllm:generation_tokens_total \u2013 Generation Tokens/Sec](https://docs.vllm.ai/en/latest/design/v1/metrics.html#grafana-dashboard)\n\n<img width=\"807\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fe344a93-814d-4d82-b3af-1aba2848cd1a\" />\n\nHowever, since this metric is of the counter type, it may be more accurate to document it as \"Generation Tokens\" rather than \"Generation Tokens/Sec.\"\n\n\n\n### Suggest a potential alternative/fix\n\nAS-IS:\n- `vllm:generation_tokens_total - Generation Tokens/Sec`\n\nTO-BE:\n- `vllm:generation_tokens_total - Generation Tokens`\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2025-04-17T12:47:59+00:00",
    "closed_at": "2025-04-17T14:10:10+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16783/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16783"
  },
  {
    "number": 18228,
    "title": "[Doc]: Arguments missing from docs page",
    "body": "### \ud83d\udcda The doc issue\n\nThe following are arguments that are available by running `vllm serve --help` with vllm 0.8.4 and are not included in the [0.8.4 engine arguments docs](https://docs.vllm.ai/en/v0.8.4/serving/engine_args.html) page:\n\n```\n--allow-credentials\n--allowed-headers\n--allowed-methods\n--allowed-origins\n--api-key\n--chat-template\n--chat-template-content-format\n--config\n--disable-fastapi-docs\n--disable-frontend-multiprocessing\n--disable-uvicorn-access-log\n--enable-auto-tool-choice\n--enable-prompt-tokens-details\n--enable-request-id-headers\n--enable-server-load-tracking\n--enable-ssl-refresh\n--help\n--host\n--lora-modules\n--max-log-len\n--middleware\n--port\n--prompt-adapters\n--response-role\n--return-tokens-as-token-ids\n--root-path\n--ssl-ca-certs\n--ssl-cert-reqs\n--ssl-certfile\n--ssl-keyfile\n--tool-call-parser\n--tool-parser-plugin\n--uvicorn-log-level\n```\n\nIt appears that the majority of these arguments are options that are created here:\n\nhttps://github.com/vllm-project/vllm/blob/0b34593017953051b3225b1483ce0f4670e3eb0e/vllm/entrypoints/openai/cli_args.py#L83\n\n\n\n### Suggest a potential alternative/fix\n\nInclude the missing args in the engine args page.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2025-05-15T22:10:29+00:00",
    "closed_at": "2025-05-17T02:43:46+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18228/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/18228"
  },
  {
    "number": 19414,
    "title": "[Doc]: can not close thinking",
    "body": "version=0.9.0.1  set enable_thinking=True , can not close thinking   \n\nCUDA_VISIBLE_DEVICES=4,5,6,7 python3 -m vllm.entrypoints.openai.api_server \\\n        --model $modelpath \\\n        --served-model-name QwQ-32B \\\n        --trust-remote-code \\\n        --tensor-parallel-size 4 \\\n        --max-model-len 30000 \\\n        --port 8006 \\\n        --reasoning-parser qwen3 \\\n\n\nresponse = client.chat.completions.create(\n            model=model_name,\n            messages=messages,\n            temperature=0.3,\n            top_p=0.8,\n            max_tokens=4000,\n            extra_body={\"chat_template_kwargs\": {\"enable_thinking\":False}},\n        )",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2025-06-10T09:48:37+00:00",
    "closed_at": "2025-06-10T12:31:19+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19414/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19414"
  },
  {
    "number": 14294,
    "title": "[Doc]: Typo in prefix_caching.md",
    "body": "### \ud83d\udcda The doc issue\n\nThere's a typo in [eviction-LRU doc](https://docs.vllm.ai/en/latest/design/v1/prefix_caching.html#eviction-lru) where an LRU block is called an \"LRU black\"\n\n### Suggest a potential alternative/fix\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2025-03-05T14:56:14+00:00",
    "closed_at": "2025-03-05T15:43:14+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14294/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14294"
  },
  {
    "number": 8682,
    "title": "[Doc]: Is Qwen2-VL-72B supported?",
    "body": "### \ud83d\udcda The doc issue\n\nUnclear.\n\n### Suggest a potential alternative/fix\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2024-09-20T21:05:00+00:00",
    "closed_at": "2024-09-25T05:33:18+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8682/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8682"
  },
  {
    "number": 12215,
    "title": "[Doc]:  FP8 KV Cache feature only supoorts  Llama 2  so far?",
    "body": "### \ud83d\udcda The doc issue\n\ni assume FP8 KV Cache  can significantly improve inference speed and save memory space.  my questions is how many models does it supports? \nis it true  only Llama 2 is supported? (https://github.com/vllm-project/vllm/tree/main/examples/other/fp8)\nbecause i find that kind of hard to believe \nthank u \n\n### Suggest a potential alternative/fix\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2025-01-20T09:32:47+00:00",
    "closed_at": "2025-01-21T02:02:14+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12215/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12215"
  },
  {
    "number": 206,
    "title": "Documentation on distributed execution",
    "body": null,
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2023-06-22T11:23:16+00:00",
    "closed_at": "2023-06-26T18:34:25+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/206/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/206"
  },
  {
    "number": 16920,
    "title": "[Doc]: state requirements for testing or update to work for CPU-only",
    "body": "### \ud83d\udcda The doc issue\n\nOn https://docs.vllm.ai/en/latest/contributing/overview.html#testing, `pip install -r requirements/dev.txt` tries to install [mamba-ssm](https://github.com/vllm-project/vllm/blob/d41faaf9df6d1a741d5fdd4a282b16783cace888/requirements/test.txt#L273) whose [requirements are stated here](https://github.com/state-spaces/mamba/?tab=readme-ov-file#installation).\n\n> * Linux\n> * NVIDIA GPU\n> * PyTorch 1.12+\n> * CUDA 11.6+\n\nThe `pip install` command fails otherwise with errors like `NameError: name 'bare_metal_version' is not defined`.\n\n```bash\n$ pip install -r requirements/dev.txt\n...\nCollecting mamba-ssm==2.2.4 (from -r /home/dxia/src/github.com/vllm-project/vllm/requirements/test.txt (line 273))\n  Downloading mamba_ssm-2.2.4.tar.gz (91 kB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... error\n  error: subprocess-exited-with-error\n  \n  \u00d7 Getting requirements to build wheel did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [26 lines of output]\n      /tmp/pip-build-env-w7l3g7ku/overlay/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n        cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n      <string>:118: UserWarning: mamba_ssm was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.\n      \n      \n      torch.__version__  = 2.6.0+cu124\n      \n      \n      Traceback (most recent call last):\n        File \"/home/dxia/src/github.com/vllm-project/vllm/.venv/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 389, in <module>\n          main()\n        File \"/home/dxia/src/github.com/vllm-project/vllm/.venv/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 373, in main\n          json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"/home/dxia/src/github.com/vllm-project/vllm/.venv/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 143, in get_requires_for_build_wheel\n          return hook(config_settings)\n                 ^^^^^^^^^^^^^^^^^^^^^\n        File \"/tmp/pip-build-env-w7l3g7ku/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 331, in get_requires_for_build_wheel\n          return self._get_build_requires(config_settings, requirements=[])\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"/tmp/pip-build-env-w7l3g7ku/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 301, in _get_build_requires\n          self.run_setup()\n        File \"/tmp/pip-build-env-w7l3g7ku/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 317, in run_setup\n          exec(code, locals())\n        File \"<string>\", line 188, in <module>\n      NameError: name 'bare_metal_version' is not defined\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\n\u00d7 Getting requirements to build wheel did not run successfully.\n\u2502 exit code: 1\n\u2570\u2500> See above for output.\n```\n\n### Suggest a potential alternative/fix\n\nThe doc should either state requirements for testing or provide instructions to make testing also work on CPU-only environments.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "open",
    "created_at": "2025-04-21T12:24:29+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16920/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16920"
  },
  {
    "number": 6676,
    "title": "[Usage]: Does Prefix Caching currently support offloading to the CPU?",
    "body": "### Usage\r\n\r\nDoes Prefix Caching currently support offloading to the CPU?\r\n\r\n If not, is there a plan to support it? Thanks\uff5e\r\n",
    "labels": [
      "documentation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-23T06:31:42+00:00",
    "closed_at": "2024-11-24T02:07:31+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6676/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6676"
  }
]
[
  {
    "number": 12700,
    "title": "v0.7.2 Release Tracker",
    "body": "\nWe will make a new release as soon as these PRs are merged.\n\n- [x] https://github.com/vllm-project/vllm/pull/12696\n- [x] https://github.com/vllm-project/vllm/pull/12604\n- [x] https://github.com/vllm-project/vllm/pull/12729\n- [x] https://github.com/vllm-project/vllm/pull/12676\n- [x] https://github.com/vllm-project/vllm/pull/12732\n- [x] https://github.com/vllm-project/vllm/pull/12796",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2025-02-03T17:14:45+00:00",
    "closed_at": "2025-02-06T18:55:16+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12700/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/12700"
  },
  {
    "number": 12465,
    "title": "Release v0.7.3",
    "body": "Update (02/03/2025):\n* This has been renamed to v0.7.3 as we are releasing v0.7.2 for MLA bug fixes, transformers backend, and Qwen2.5VL\n\nUpdate (01/31/2025):\n* This has been renamed to v0.7.2 as we are releasing v0.7.1 for Deepseek enhancements. \n\n\nBlockers\n- [ ] Support for Qwen-1M: https://github.com/vllm-project/vllm/pull/11844\n- [ ] Support for Baichuan-M1: https://github.com/vllm-project/vllm/pull/12251",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2025-01-27T05:29:49+00:00",
    "closed_at": "2025-02-20T06:45:24+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12465/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/12465"
  },
  {
    "number": 12365,
    "title": "Release v0.7.0",
    "body": "\n* Alpha release for vLLM V1 architecture, ETA 1/23-1/24\n* Pending V1 items\n  - [ ] Performance numbers @ywang96 @robertgshaw2-redhat @WoosukKwon \n  - [ ]  Documentation @WoosukKwon \n  - [ ]  Blog post @WoosukKwon \n* Other Pending PRs\n  - [x] https://github.com/vllm-project/vllm/pull/12361\n  - [x] https://github.com/vllm-project/vllm/pull/12243\n  - [x] ~https://github.com/vllm-project/vllm/pull/12377~ #12380\n  - [x] https://github.com/vllm-project/vllm/pull/12375\n  - [x] https://github.com/vllm-project/vllm/pull/12405\n\n",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2025-01-23T18:17:20+00:00",
    "closed_at": "2025-02-11T18:29:43+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12365/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/12365"
  },
  {
    "number": 11218,
    "title": "[Release]: v0.7.0 Release Tracker",
    "body": "We plan a release after V1 is ready. \r\n\r\n\r\n~Planned release Dec 16-17.~\r\n\r\nWe do not block release on feature request, minor issues. We do block release on major feature development, important user impacting issues, and compatibility fixes. \r\n\r\nAdd pending PRs directly here \u2b07\ufe0f \r\n\r\n- [x] https://github.com/vllm-project/vllm/pull/10511\r\n- [x] #11210 \r\n\r\n",
    "labels": [
      "release",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-16T00:55:05+00:00",
    "closed_at": "2025-04-27T02:11:35+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11218/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11218"
  },
  {
    "number": 8426,
    "title": "v0.6.1.post1 Release Tracker",
    "body": "### Anything you want to discuss about vllm.\r\n\r\n- [x] #8390\r\n- [x] #8375 \r\n- [x] #8399 \r\n- [x] #8417 \r\n- [x] #8415 \r\n- [x] #8376\r\n- [x] #8425\r\n\r\n",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2024-09-12T17:46:21+00:00",
    "closed_at": "2024-09-13T05:02:37+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8426/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8426"
  },
  {
    "number": 8144,
    "title": "Release v0.6.0",
    "body": "### Anything you want to discuss about vllm.\n\nTarget Sept 04-06. \r\n\r\nThis release will will bring to a close for majority of enhancements in #6801. So I'll mostly merge PRs that are performance sensitive. But feel free to comment the PR that you think should go in. As a reminder we do aim to release every 2 weeks. \n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2024-09-04T05:47:03+00:00",
    "closed_at": "2024-09-05T04:27:00+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8144/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8144"
  },
  {
    "number": 7481,
    "title": "Release v0.5.5",
    "body": "We will make a release later this week or early next week (Aug 16-Aug19) to address Gemma logits soft-caps bug, openai server metrics bug, and include more performance enhancements. \r\n\r\nPlease add blockers if needed. ",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2024-08-13T21:01:28+00:00",
    "closed_at": "2024-08-23T19:50:56+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7481/reactions",
      "total_count": 7,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7481"
  },
  {
    "number": 6434,
    "title": "v0.5.2, v0.5.3, v0.6.0 Release Tracker",
    "body": "### Anything you want to discuss about vllm.\r\n\r\nWe will make a triplet of releases in the following 3 weeks. \r\n- [x] v0.5.2 on Monday July 15th. \r\n- [x] v0.5.3 by Tuesday July 23rd.\r\n- [ ] v0.6.0 after Monday July 29th.\r\n\r\nBlockers\r\n- [x] #6463\r\n- [x] #6517\r\n- [x] #6698\r\n- [ ] Test vLLM works with 405B that's `num_kv_heads=8` instead of 16. \r\n\r\n~The reason for such pace is that we want to remove beam search (#6226), which unlocks a suite of scheduler refactoring to enhance performance (async scheduling to overlap scheduling and forward pass for example). We want to release v0.5.2 ASAP to issue warnings and uncover new signals. Then we will decide the removal in v0.6.0. Normally we will deprecate slowly by stretching it by one month or two. However, (1) RFC has been opened for a while (2) it is unfortunately on the critical path of refactoring and performance enhancements.~\r\n\r\nPlease also feel free to add release blockers. But do keep in mind that I will not slow the release for v0.5.* series unless critical bug.",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2024-07-15T03:19:27+00:00",
    "closed_at": "2024-08-05T21:39:49+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6434/reactions",
      "total_count": 13,
      "+1": 13,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/6434"
  },
  {
    "number": 5806,
    "title": "v0.5.1 Release Tracker",
    "body": "ETA Friday -> Wednesday 07/03\r\n\r\n* https://github.com/vllm-project/vllm/pull/4115 \r\n* https://github.com/vllm-project/vllm/pull/4650 \r\n* https://github.com/vllm-project/vllm/pull/6051\r\n* https://github.com/vllm-project/vllm/pull/6033 \r\n* https://github.com/vllm-project/vllm/pull/5987\r\n* https://github.com/vllm-project/vllm/pull/6044\r\n* https://github.com/vllm-project/vllm/pull/4412\r\n* https://github.com/vllm-project/vllm/pull/6050 \r\n* #6055 <- https://github.com/vllm-project/vllm/pull/5276\r\n- https://github.com/vllm-project/vllm/pull/6109",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2024-06-25T00:18:07+00:00",
    "closed_at": "2024-07-06T04:55:52+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5806/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/5806"
  },
  {
    "number": 4895,
    "title": "v0.4.3 Release Tracker",
    "body": "ETA May 30 (due to some blockers and US holiday). \r\n\r\nBlockers\r\n- [ ] #4650\r\n- [ ] #4799\r\n- [x] #4846\r\n\r\nNice to have\r\n- [ ] #4638\r\n- [x] #4525\r\n- [ ] #4464",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2024-05-18T01:05:16+00:00",
    "closed_at": "2024-06-03T17:04:07+00:00",
    "comments": 16,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4895/reactions",
      "total_count": 7,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 5,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4895"
  },
  {
    "number": 4505,
    "title": "v0.4.2 Release Tracker",
    "body": "ETA May 3rd, Friday. \r\n\r\n",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2024-04-30T17:28:48+00:00",
    "closed_at": "2024-05-05T07:20:30+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4505/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4505"
  },
  {
    "number": 4181,
    "title": "v0.4.1 Release Tracker",
    "body": "ETA Monday April 22\r\n\r\n- [x] #4176\r\n- [x] #4182 (addressing #4180)\r\n- [x] #4079 \r\n- [x] #4159 \r\n- [x] #4138\r\n- [x] #4209\r\n- [x] #4210\r\n- [x] #4271\r\n- [x] #4304 (otherwise we cannot upload to PyPI :(",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2024-04-18T21:37:23+00:00",
    "closed_at": "2024-04-24T04:43:06+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4181/reactions",
      "total_count": 22,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 4,
      "confused": 0,
      "heart": 2,
      "rocket": 3,
      "eyes": 9
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4181"
  },
  {
    "number": 3155,
    "title": "[v0.4.0] Release Tracker",
    "body": "**ETA:** Before Mar 28th\r\n\r\n##  Major changes\r\nTBD.\r\n\r\n## PRs to be merged before the release\r\n- [ ] #1507 \r\n- [x] #2762 \r\n- [ ] ...\r\n",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2024-03-02T01:41:58+00:00",
    "closed_at": "2024-04-03T22:42:26+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3155/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/3155"
  },
  {
    "number": 3097,
    "title": "[v0.3.3] Release Tracker",
    "body": "**ETA**: Feb 29th - Mar 1st\r\n\r\n## Major changes\r\n\r\n* StarCoder2 support\r\n* Performance optimization and LoRA support for Gemma\r\n* Performance optimization for MoE kernel\r\n* 2/3/8-bit GPTQ support\r\n* [Experimental] AWS Inferentia2 support\r\n\r\n## PRs to be merged before the release\r\n\r\n- [x] #2330 #2223\r\n- [ ] ~~#2761~~\r\n- [x] #2819 \r\n- [x] #3087 #3099\r\n- [x] #3089 ",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2024-02-28T22:36:29+00:00",
    "closed_at": "2024-03-01T20:58:07+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3097/reactions",
      "total_count": 8,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 8,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3097"
  },
  {
    "number": 2859,
    "title": "[v0.3.1] Release Tracker",
    "body": "**ETA**: Feb 14-16 th\r\n\r\n## Major changes\r\n\r\nTBD\r\n\r\n## PRs to be merged before the release\r\n\r\n- [x] #2855 \r\n- [x] #2845 \r\n- [x] ~~#2514~~\r\n- [x] Ensure memory release when `LLM` class is deleted. #2882 \r\n- [x] #2875 #2880",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2024-02-13T23:36:38+00:00",
    "closed_at": "2024-02-16T23:05:19+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2859/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2859"
  },
  {
    "number": 2332,
    "title": "[v0.2.7] Release Tracker",
    "body": "**ETA**: Jan 3rd - 4th\r\n\r\n## Major changes\r\n\r\nTBD\r\n\r\n## PRs to be merged before the release\r\n\r\n- [x] #2221 \r\n- [ ] ~~#2293~~ (deferred)",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2024-01-03T17:55:09+00:00",
    "closed_at": "2024-01-04T01:35:58+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2332/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2332"
  },
  {
    "number": 1856,
    "title": "[v0.2.3] Release Tracker",
    "body": "**ETA**: Nov 30th - Dec 2nd.\r\n\r\n## Major changes\r\n\r\n* Refactoring on Worker, InputMetadata, and Attention\r\n* Fix TP support for AWQ models\r\n* Support Prometheus metrics\r\n* Fix Baichuan & Baichuan 2\r\n\r\n## PRs to be merged before the release\r\n\r\n- [x] Chat Template #1756 \r\n- [x] ~#1707~ (We have to solve AWQ perf first, which might be possible in time).\r\n- [x] ~#1662~ (use the new one instead)\r\n- [x] #1890\r\n- [x] #1852 ",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2023-11-30T08:05:43+00:00",
    "closed_at": "2023-12-03T20:31:00+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1856/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 2,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/1856"
  },
  {
    "number": 1551,
    "title": "[v0.2.2] Release Tracker",
    "body": "**ETA**: ~~Nov 3rd (Fri) - Nov 6th (Mon).~~ Nov 17th (Fri) - 19th (Sun).\r\n\r\n## Major changes\r\n\r\n* Extensive refactoring for better tensor parallelism & quantization support\r\n* Changes in scheduler: from 1D flattened input tensor to 2D tensor\r\n* Bump up to PyTorch v2.1 + CUDA 12.1\r\n* New models: Yi, ChatGLM, Phi\r\n* Added LogitsProcessor API\r\n* Preliminary support for SqueezeLLM\r\n\r\n## PRs to be merged before the release\r\n\r\n- [x] CUDA 12 #1527\r\n- [x] Yarn #1264, #1161\r\n- [x] Phi model #1664 \r\n- ~~[ ] Support embedding inputs #1265~~ \r\n",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2023-11-02T17:07:19+00:00",
    "closed_at": "2023-11-19T05:57:08+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1551/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/1551"
  },
  {
    "number": 1346,
    "title": "[v0.2.1] Release Tracker",
    "body": "**ETA**: ~~Oct. 15th (Sun)~~ Oct 16th (Mon).\r\n\r\n## Major changes\r\n\r\nTBD\r\n\r\n## PRs to be merged before the release\r\n\r\n- [x] PagedAttention V2 #1348 \r\n- [x] Support `echo` #1328 #959 \r\n- [x] Fix `TORCH_CUDA_ARCH_LIST` err msg #1239\r\n- ~~Support YaRN #1264 #1161~~ (Deferred)\r\n- ~~Add `repetition_penalty` sampling parameter #866~~ (Deferred)\r\n",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2023-10-13T17:17:40+00:00",
    "closed_at": "2023-10-16T19:58:59+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1346/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/1346"
  },
  {
    "number": 1089,
    "title": "[v0.2.0] Release Tracker",
    "body": "## Major changes\r\n\r\n* Up to 60% performance improvement by optimizing de-tokenization and sampler\r\n* Initial support for AWQ (performance not optimized)\r\n* Support for RoPE scaling and LongChat\r\n* Support for Mistral-7B\r\n\r\n## PRs to be merged before the release\r\n\r\n- [x] Vectorized sampler: #1048, #820 \r\n- [x] LongChat: #555 \r\n- [x] `TORCH_CUDA_ARCH_LIST` build option: #1074 \r\n- [x] Support for Mistral-7B: #1196 \r\n- [x] #1198  \r\n- ~~[ ] FP32 RoPE kernel: #1061~~ (deferred to the next PR)",
    "labels": [
      "release"
    ],
    "state": "closed",
    "created_at": "2023-09-18T21:18:03+00:00",
    "closed_at": "2023-09-28T22:30:39+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1089/reactions",
      "total_count": 25,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 25,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/1089"
  }
]
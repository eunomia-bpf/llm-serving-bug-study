[
  {
    "number": 6225,
    "title": "[Bug]:  benchmark_throughput gets TypeError: XFormersMetadata.__init__() got an unexpected keyword argument 'is_prompt' wit CPU ",
    "body": "### Your current environment\r\n\r\n```text\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-91-generic-x86_64-with-glibc2.35\r\n\r\n...\r\n\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] torchvision==0.18.0\r\n[pip3] transformers==4.42.3\r\n[pip3] triton==2.3.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] torchvision               0.18.0                   pypi_0    pypi\r\n[conda] transformers              4.42.3                   pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\n```\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nRunning `benchmark_throughput.py ... --device cpu` throws an exception, it works with GPU. \r\n\r\n```\r\nVLLM_CPU_KVCACHE_SPACE=16 time python benchmarks/benchmark_throughput.py --model mosaicml/mpt-7b --input-len 128 --output-len 512 --trust-remote-code --backend=vllm  --device cpu --dtype bfloat16\r\n...\r\nWARNING 07-08 21:21:47 cpu_executor.py:119] CUDA graph is not supported on CPU, fallback to the eager mode.\r\nINFO 07-08 21:21:48 selector.py:191] Cannot use FlashAttention-2 backend because the vllm_flash_attn package is not found. `pip install vllm-flash-attn` for better performance.\r\nINFO 07-08 21:21:48 selector.py:53] Using XFormers backend.\r\nINFO 07-08 21:21:49 selector.py:191] Cannot use FlashAttention-2 backend because the vllm_flash_attn package is not found. `pip install vllm-flash-attn` for better performance.\r\nINFO 07-08 21:21:49 selector.py:53] Using XFormers backend.\r\nINFO 07-08 21:21:50 weight_utils.py:218] Using model weights format ['*.bin']\r\npytorch_model-00002-of-00002.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.36G/3.36G [00:32<00:00, 103MB/s]\r\npytorch_model-00001-of-00002.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.94G/9.94G [01:18<00:00, 126MB/s]\r\nINFO 07-08 21:23:44 cpu_executor.py:72] # CPU blocks: 2048\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 9.93G/9.94G [01:18<00:00, 127MB/s]\r\nProcessed prompts:   0%|                                                                                                  | 0/1000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/code/vllm/benchmarks/benchmark_throughput.py\", line 439, in <module>\r\n[rank0]:     main(args)\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/code/vllm/benchmarks/benchmark_throughput.py\", line 227, in main\r\n[rank0]:     elapsed_time = run_vllm(\r\n[rank0]:                    ^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/code/vllm/benchmarks/benchmark_throughput.py\", line 127, in run_vllm\r\n[rank0]:     llm.generate(prompts, sampling_params, use_tqdm=True)\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/utils.py\", line 795, in inner\r\n[rank0]:     return fn(*args, **kwargs)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/entrypoints/llm.py\", line 309, in generate\r\n[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)\r\n[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/entrypoints/llm.py\", line 561, in _run_engine\r\n[rank0]:     step_outputs = self.llm_engine.step()\r\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 861, in step\r\n[rank0]:     output = self.model_executor.execute_model(\r\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/executor/cpu_executor.py\", line 78, in execute_model\r\n[rank0]:     output = self.driver_worker.execute_model(execute_model_req)\r\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 235, in execute_model\r\n[rank0]:     self.model_runner.prepare_model_input(\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/worker/cpu_model_runner.py\", line 327, in prepare_model_input\r\n[rank0]:     ) = self._prepare_prompt(seq_group_metadata_list)\r\n[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/worker/cpu_model_runner.py\", line 202, in _prepare_prompt\r\n[rank0]:     attn_metadata = self.attn_backend.make_metadata(\r\n[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/nfs_users/users/luis.lorenzo/anaconda3/envs/vllm_311/lib/python3.11/site-packages/vllm/attention/backends/abstract.py\", line 29, in make_metadata\r\n[rank0]:     return cls.get_metadata_cls()(*args, **kwargs)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: TypeError: XFormersMetadata.__init__() got an unexpected keyword argument 'is_prompt'\r\nProcessed prompts:   0%|                                                                                                  | 0/1000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\r\nCommand exited with non-zero status 1\r\n```\r\n\r\nOriginally I got this error with flash attention as attn backend, but then I played with different versions of vllm and flash, and eventually uninstalled it, and same issue. Error was: ``` TypeError: FlashAttentionMetadata.__init__() got an unexpected keyword argument 'is_prompt'``` \r\n\r\nI've also tried with other vllm-enabled models, same issue.\r\nI've pulled latest main to get updated version of /benchmark_throughput.py, same issue.\r\n\r\nMy main guess is atm that cpu_model_runner.py and model_runner.py have diverged when calling \"attn_metadata = self.attn_backend.make_metadata(\" , somehow, somewhere the \"is_prompt\" kwarg was removed for GPU but not for CPU.\r\nI've looked a bit at the code but does not seem to be a trivial fix, so I'll let someone with more experience/time to look into this. \r\n\r\n",
    "labels": [
      "bug",
      "x86-cpu",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-08T21:58:11+00:00",
    "closed_at": "2025-03-14T02:02:55+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6225/reactions",
      "total_count": 10,
      "+1": 10,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6225"
  },
  {
    "number": 5682,
    "title": "[Usage]: Recommended setting for running vLLM for CPU ",
    "body": "### How would you like to use vllm\r\n\r\nWhat are the recommended settings for running vLLM on a CPU to achieve high performance? For instance, if I have a dual-socket server with 96 cores per socket, how many cores (--cpuset-cpus) should be allocated to run multiple replicas of vLLM?",
    "labels": [
      "usage",
      "x86-cpu",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-06-19T09:01:53+00:00",
    "closed_at": "2025-01-09T02:14:23+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5682/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5682"
  },
  {
    "number": 5465,
    "title": "[Bug]: Runtime Error: GET was unable to find an engine to execute this computation for LLaVa-NEXT",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 18.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.5\r\nLibc version: glibc-2.27\r\n\r\nPython version: 3.10.12 (main, Jul 19 2023, 10:44:52) [GCC 7.5.0] (64-bit runtime)\r\nPython platform: Linux-4.15.0-213-generic-x86_64-with-glibc2.27\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              12\r\nOn-line CPU(s) list: 0-11\r\nThread(s) per core:  1\r\nCore(s) per socket:  12\r\nSocket(s):           1\r\nNUMA node(s):        1\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               61\r\nModel name:          Intel Core Processor (Broadwell)\r\nStepping:            2\r\nCPU MHz:             2095.076\r\nBogoMIPS:            4190.15\r\nHypervisor vendor:   KVM\r\nVirtualization type: full\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            4096K\r\nL3 cache:            16384K\r\nNUMA node0 CPU(s):   0-11\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] transformers==4.41.2\r\n[pip3] triton==2.3.0\r\n[pip3] vllm_nccl_cu12==2.18.1.0.4.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.0.2            py36h7b6447c_0  \r\n[conda] mkl_fft                   1.0.12           py36ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py36hd81dba3_0  \r\n[conda] numpy                     1.16.4           py36h7e9f1db_0  \r\n[conda] numpy-base                1.16.4           py36hde5b4d6_0  \r\n[conda] numpydoc                  0.9.1                      py_0  \r\n[conda] torch-scatter             2.0.7                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.0\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\n\n### \ud83d\udc1b Describe the bug\n\nI'm trying to use a single A40 GPU to host `llava-hf/llava-v1.6-mistral-7b-hf` using the OpenAI compatible server with `vllm 0.5.0`.\r\nHere is the command I used to launch the server: \r\n```text\r\npython3 -m vllm.entrypoints.openai.api_server \\\r\n    --model /llava-v1.6-mistral-7b \\\r\n    --host \"0.0.0.0\" \\\r\n    --port 8080 \\\r\n    --tensor-parallel-size 1 \\\r\n    --dtype auto \\\r\n    --load-format safetensors \\\r\n    --image-input-type pixel_values\\\r\n    --image-token-id 32000 \\\r\n    --image-input-shape 1,3,336,336  \\\r\n    --image-feature-size 576 \\\r\n    --chat-template template_llava.jinja\r\n ```\r\nAnd I got this error after loading the model weights:\r\n```text\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/pkgs/python-3.10.12/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n[rank0]:     return _run_code(code, main_globals, None,\r\n[rank0]:   File \"/pkgs/python-3.10.12/lib/python3.10/runpy.py\", line 86, in _run_code\r\n[rank0]:     exec(code, run_globals)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 196, in <module>\r\n[rank0]:     engine = AsyncLLMEngine.from_engine_args(\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 395, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 349, in __init__\r\n[rank0]:     self.engine = self._init_engine(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 470, in _init_engine\r\n[rank0]:     return engine_class(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 236, in __init__\r\n[rank0]:     self._initialize_kv_caches()\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 313, in _initialize_kv_caches\r\n[rank0]:     self.model_executor.determine_num_available_blocks())\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/executor/gpu_executor.py\", line 75, in determine_num_available_blocks\r\n[rank0]:     return self.driver_worker.determine_num_available_blocks()\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/worker/worker.py\", line 154, in determine_num_available_blocks\r\n[rank0]:     self.model_runner.profile_run()\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 833, in profile_run\r\n[rank0]:     self.execute_model(seqs, kv_caches)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 738, in execute_model\r\n[rank0]:     hidden_states = model_executable(\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llava_next.py\", line 398, in forward\r\n[rank0]:     vision_embeddings = self._process_image_input(image_input)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llava_next.py\", line 327, in _process_image_input\r\n[rank0]:     image_features = self._process_image_pixels(image_input)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llava_next.py\", line 317, in _process_image_pixels\r\n[rank0]:     stacked_image_features = self._image_pixels_to_features(\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/vllm/model_executor/models/llava_next.py\", line 232, in _image_pixels_to_features\r\n[rank0]:     image_outputs = vision_tower(pixel_values.to(vision_tower.device),\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 926, in forward\r\n[rank0]:     return self.vision_model(\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 850, in forward\r\n[rank0]:     hidden_states = self.embeddings(pixel_values)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 185, in forward\r\n[rank0]:     patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 460, in forward\r\n[rank0]:     return self._conv_forward(input, self.weight, self.bias)\r\n[rank0]:   File \"/fs01/projects/aieng/public/mixtral_vllm_env/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\r\n[rank0]:     return F.conv2d(input, weight, bias, self.stride,\r\n[rank0]: RuntimeError: GET was unable to find an engine to execute this computation\r\n```",
    "labels": [
      "bug",
      "x86-cpu"
    ],
    "state": "closed",
    "created_at": "2024-06-12T17:48:41+00:00",
    "closed_at": "2024-06-14T01:34:58+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5465/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5465"
  },
  {
    "number": 3654,
    "title": "[RFC] Initial Support for CPUs",
    "body": "## Progress\r\n\r\n- [ ] Integrate CPU executor to support the basic model inference (BF16/FP32) without TP. \r\n  - #3634 \r\n  - #3824 \r\n  - #4113 \r\n  - #4971 \r\n  - #5452 \r\n  - #5446 \r\n- [ ] Support FP16 model inference.\r\n- [x] Support TP inference for multiple CPU sockets inside the same node. \r\n  - #6008 \r\n  - #6125 \r\n- [ ] Support model and KV cache quantization.\r\n  - #5492 \r\n  - #7257 \r\n\r\n## Features\r\n\r\nThe CPU executor plans to support the following features:\r\n\r\n- Basic models of vLLM with FP16/BF16/FP32, except MoE models\r\n- Tensor-parallel model inference based on Ray\r\n- AWQ quantization, 8-bit KVCache Quantization\r\n- Others\r\n\r\n## Design\r\n\r\nOur target is seamless porting vLLM to CPU devices and sharing most of vLLM core components (e.g., **schedular**, **cache management**, **model definitions**, **Megatron-style model partitioning**, ...). \r\n\r\nThe CPU executor will depend on Pytorch CPU and leverage optimized kernels and features from [intel-extension-for-pytorch](https://github.com/intel/intel-extension-for-pytorch).\r\n\r\nThe main changes to vLLM include:\r\n\r\n### Torch APIs Adaption\r\n\r\nCPU device is supported in PyTorch by default. It allows the CPU Executor to share the same model definitions with the GPU Executor. Thanks to recent code refactors, many hardcoded ```cuda``` device flags have been removed and Torch APIs are dispatched based on the device flag from ```DeviceConfig```. For the CPU executor, a new ```cpu``` device flag is added.\r\n\r\nSharing the same model definitions and Torch APIs also allows the CPU executor to easily support new models and features in vLLM (e.g., ```torch.compile```).  \r\n\r\n### Custom Ops Adaption\r\n\r\nvLLM implemented many efficient CUDA kernels and packaged as ```_C``` library by pybind. These kernels are ported to CPU using C++ and OpenMP, with the same function signatures to replace the CUDA kernels directly. The CPU custom kernel building procedure is integrated into vLLM CMake build system as a CMake module.\r\n\r\nCurrently, all of CPU kernels require ```AVX512``` ISA support.\r\n\r\n### Python APIs Adaption\r\n\r\nNew ```CPUExecutor``` and ```CPUWorker``` are added to initialize the environment and model runner. The ```CPUModelRunner``` is derived from ```ModelRunner``` of the GPU code path, because most of the code could be shared. Even though it might have potential risks due to changes in the GPU code path, ```CPUModelRunner``` could fix them by rewriting configurations or overloading member functions easily. \r\n\r\nIn special, different from the GPU executor profiling available KV cache memory,  the cache memory in the CPU executor is specified by the ```swap_space``` parameter. Because the memory management of CPU is more complex than GPU (e.g., NUMA).",
    "labels": [
      "RFC",
      "x86-cpu",
      "unstale"
    ],
    "state": "closed",
    "created_at": "2024-03-27T07:45:25+00:00",
    "closed_at": "2025-01-14T16:19:23+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3654/reactions",
      "total_count": 7,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3654"
  },
  {
    "number": 17667,
    "title": "[Bug]: Merge security updates for 0.9.0",
    "body": "This is a placeholder to ensure any pending security patches have been merged prior to release.",
    "labels": [
      "security"
    ],
    "state": "closed",
    "created_at": "2025-05-05T16:08:43+00:00",
    "closed_at": "2025-05-09T14:07:58+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17667/reactions",
      "total_count": 2,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 2,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/17667"
  },
  {
    "number": 17313,
    "title": "[Bug]: clients can crash the openai server with invalid regex",
    "body": "### Your current environment\n\n```\nroot@3bea15cf4c9f:/# uv run --with vllm python collect_env.py\nINFO 04-28 15:38:49 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\n/usr/local/lib/python3.11/dist-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n  warnings.warn(\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-6.8.0-52-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA A100-SXM4-80GB\nGPU 1: NVIDIA A100-SXM4-80GB\n\nNvidia driver version: 550.127.05\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               256\nOn-line CPU(s) list:                  0-255\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 7763 64-Core Processor\nCPU family:                           25\nModel:                                1\nThread(s) per core:                   2\nCore(s) per socket:                   64\nSocket(s):                            2\nStepping:                             1\nFrequency boost:                      enabled\nCPU max MHz:                          3529.0520\nCPU min MHz:                          1500.0000\nBogoMIPS:                             4899.64\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm debug_swap\nVirtualization:                       AMD-V\nL1d cache:                            4 MiB (128 instances)\nL1i cache:                            4 MiB (128 instances)\nL2 cache:                             64 MiB (128 instances)\nL3 cache:                             512 MiB (16 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-63,128-191\nNUMA node1 CPU(s):                    64-127,192-255\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==2.2.5\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.3\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.4\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n\tGPU0\tGPU1\tNIC0\tNIC1\tNIC2\tNIC3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\tX \tNV12\tNODE\tNODE\tSYS\tSYS\t0-63,128-191\t0\t\tN/A\nGPU1\tNV12\tX \tSYS\tSYS\tNODE\tNODE\t64-127,192-255\t1\t\tN/A\nNIC0\tNODE\tSYS\tX \tPIX\tSYS\tSYS\t\t\t\t\nNIC1\tNODE\tSYS\tPIX\tX \tSYS\tSYS\t\t\t\t\nNIC2\tSYS\tNODE\tSYS\tSYS\tX \tPIX\t\t\t\t\nNIC3\tSYS\tNODE\tSYS\tSYS\tPIX\tX \t\t\t\t\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n\nCUDA_VERSION=12.4.1\nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNCCL_VERSION=2.21.5-1\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNVIDIA_PRODUCT_NAME=CUDA\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\nNVIDIA_VISIBLE_DEVICES=all\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n### \ud83d\udc1b Describe the bug\n\nIf a client connects to the openai server and uses `extra_body={\"guided_regex\":` as described here:\nhttps://docs.vllm.ai/en/v0.8.4_a/features/structured_outputs.html#online-serving-openai-api\n\nThen they can crash the server with invalid regex:\n```\nERROR 04-28 15:36:55 [core.py:387] EngineCore hit an exception: Traceback (most recent call last):\nERROR 04-28 15:36:55 [core.py:387]   File \"/root/.cache/uv/archive-v0/kYowyHpQY8CQtup1TNFhp/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 380, in run_engine_core\nERROR 04-28 15:36:55 [core.py:387]     engine_core.run_busy_loop()\nERROR 04-28 15:36:55 [core.py:387]   File \"/root/.cache/uv/archive-v0/kYowyHpQY8CQtup1TNFhp/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 402, in run_busy_loop\nERROR 04-28 15:36:55 [core.py:387]     self._process_engine_step()\nERROR 04-28 15:36:55 [core.py:387]   File \"/root/.cache/uv/archive-v0/kYowyHpQY8CQtup1TNFhp/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 431, in _process_engine_step\nERROR 04-28 15:36:55 [core.py:387]     outputs = self.step_fn()\nERROR 04-28 15:36:55 [core.py:387]               ^^^^^^^^^^^^^^\nERROR 04-28 15:36:55 [core.py:387]   File \"/root/.cache/uv/archive-v0/kYowyHpQY8CQtup1TNFhp/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 206, in step\nERROR 04-28 15:36:55 [core.py:387]     scheduler_output = self.scheduler.schedule()\nERROR 04-28 15:36:55 [core.py:387]                        ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-28 15:36:55 [core.py:387]   File \"/root/.cache/uv/archive-v0/kYowyHpQY8CQtup1TNFhp/lib/python3.11/site-packages/vllm/v1/core/sched/scheduler.py\", line 286, in schedule\nERROR 04-28 15:36:55 [core.py:387]     if structured_output_req and structured_output_req.grammar:\nERROR 04-28 15:36:55 [core.py:387]                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-28 15:36:55 [core.py:387]   File \"/root/.cache/uv/archive-v0/kYowyHpQY8CQtup1TNFhp/lib/python3.11/site-packages/vllm/v1/structured_output/request.py\", line 43, in grammar\nERROR 04-28 15:36:55 [core.py:387]     completed = self._check_grammar_completion()\nERROR 04-28 15:36:55 [core.py:387]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-28 15:36:55 [core.py:387]   File \"/root/.cache/uv/archive-v0/kYowyHpQY8CQtup1TNFhp/lib/python3.11/site-packages/vllm/v1/structured_output/request.py\", line 31, in _check_grammar_completion\nERROR 04-28 15:36:55 [core.py:387]     self._grammar = self._grammar.result(timeout=0.0001)\nERROR 04-28 15:36:55 [core.py:387]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-28 15:36:55 [core.py:387]   File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\nERROR 04-28 15:36:55 [core.py:387]     return self.__get_result()\nERROR 04-28 15:36:55 [core.py:387]            ^^^^^^^^^^^^^^^^^^^\nERROR 04-28 15:36:55 [core.py:387]   File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\nERROR 04-28 15:36:55 [core.py:387]     raise self._exception\nERROR 04-28 15:36:55 [core.py:387]   File \"/usr/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\nERROR 04-28 15:36:55 [core.py:387]     result = self.fn(*self.args, **self.kwargs)\nERROR 04-28 15:36:55 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-28 15:36:55 [core.py:387]   File \"/root/.cache/uv/archive-v0/kYowyHpQY8CQtup1TNFhp/lib/python3.11/site-packages/vllm/v1/structured_output/__init__.py\", line 77, in _async_create_grammar\nERROR 04-28 15:36:55 [core.py:387]     return self.backend.compile_grammar(request_type, grammar_spec)\nERROR 04-28 15:36:55 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-28 15:36:55 [core.py:387]   File \"/root/.cache/uv/archive-v0/kYowyHpQY8CQtup1TNFhp/lib/python3.11/site-packages/vllm/v1/structured_output/backend_xgrammar.py\", line 99, in compile_grammar\nERROR 04-28 15:36:55 [core.py:387]     ctx = self.compiler.compile_regex(grammar_spec)\nERROR 04-28 15:36:55 [core.py:387]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-28 15:36:55 [core.py:387]   File \"/root/.cache/uv/archive-v0/kYowyHpQY8CQtup1TNFhp/lib/python3.11/site-packages/xgrammar/compiler.py\", line 150, in compile_regex\nERROR 04-28 15:36:55 [core.py:387]     return CompiledGrammar._create_from_handle(self._handle.compile_regex(regex))\nERROR 04-28 15:36:55 [core.py:387]                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-28 15:36:55 [core.py:387] RuntimeError: [15:36:55] /project/cpp/regex_converter.cc:73: Regex parsing error at position 106: Invalid repetition count.\nERROR 04-28 15:36:55 [core.py:387] \nERROR 04-28 15:36:55 [core.py:387] \nCRITICAL 04-28 15:36:55 [core_client.py:359] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n```\n\nMaybe not a bug but I think the server should be resistant to unexpected user inputs.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "security"
    ],
    "state": "closed",
    "created_at": "2025-04-28T15:27:44+00:00",
    "closed_at": "2025-05-12T01:06:11+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17313/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/17313"
  },
  {
    "number": 17128,
    "title": "[Tracker] Merge security fixes for v0.8.5",
    "body": "This issue is for tracking that pending security fixes are merged prior to releasing v0.8.5\n\n- [x] GHSA-hj4w-hm2g-p6w5 - https://github.com/vllm-project/vllm/pull/17192\n- [x] GHSA-9f8f-2vmf-885j - https://github.com/vllm-project/vllm/pull/17197",
    "labels": [
      "bug",
      "security"
    ],
    "state": "closed",
    "created_at": "2025-04-24T17:19:49+00:00",
    "closed_at": "2025-04-25T16:23:36+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17128/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/17128"
  },
  {
    "number": 8007,
    "title": "[Bug]: vLLM with Neuron performance degrades dramatically if request concurrency is >= max_num_seqs",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\ncollecting environment information...\r\nWARNING 08-29 18:36:46 _custom_ops.py:18] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\r\n/usr/local/lib/python3.10/dist-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\r\nNo module named 'vllm.commit_id'\r\n  from vllm.version import __version__ as VLLM_VERSION\r\nPyTorch version: 2.1.2+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.27.7\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.2.0-1017-aws-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             128\r\nOn-line CPU(s) list:                0-127\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz\r\nCPU family:                         6\r\nModel:                              106\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 32\r\nSocket(s):                          2\r\nStepping:                           6\r\nBogoMIPS:                           5799.87\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd ida arat avx512vbmi pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear flush_l1d arch_capabilities\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          3 MiB (64 instances)\r\nL1i cache:                          2 MiB (64 instances)\r\nL2 cache:                           80 MiB (64 instances)\r\nL3 cache:                           108 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-31,64-95\r\nNUMA node1 CPU(s):                  32-63,96-127\r\nVulnerability Gather data sampling: Unknown: Dependent on hypervisor status\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT Host state unknown\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.25.2\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==8.9.2.26\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-nccl-cu12==2.18.1\r\n[pip3] nvidia-nvjitlink-cu12==12.6.20\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pynvml==11.5.3\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.1.2\r\n[pip3] torch-neuronx==2.1.2.2.2.0\r\n[pip3] torch-xla==2.1.3\r\n[pip3] torchvision==0.16.2\r\n[pip3] transformers==4.44.2\r\n[pip3] transformers-neuronx==0.11.351\r\n[pip3] triton==2.2.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: (0, '+--------+--------+--------+-------------+---------+--------+---------+\\n| NEURON | NEURON | NEURON |  CONNECTED  |   PCI   |  PID   | RUNTIME |\\n| DEVICE | CORES  | MEMORY |   DEVICES   |   BDF   |        | VERSION |\\n+--------+--------+--------+-------------+---------+--------+---------+\\n| 12     | 2      | 32 GB  | 12, 3, 4, 1 | 10:1e.0 | 865036 | 2.21.41 |\\n| 13     | 2      | 32 GB  | 13, 0, 5, 2 | 10:1b.0 | 865036 | 2.21.41 |\\n| 14     | 2      | 32 GB  | 14, 1, 6, 3 | a0:1e.0 | 865036 | 2.21.41 |\\n| 15     | 2      | 32 GB  | 15, 2, 7, 0 | a0:1b.0 | 865036 | 2.21.41 |\\n+--------+--------+--------+-------------+---------+--------+---------+', '')\r\nvLLM Version: 0.5.5@COMMIT_HASH_PLACEHOLDER\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n\r\n```\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen vLLM is used with Neuron, as long as number of concurrent requests are less than `max_num_seqs`, performance is nominal, but if number of concurrent requests is `>= max_num_seqs`, performance degrades dramatically. For example, in one test  with `max_num_seqs=4`, request latency for up to `3` concurrent requests is in the 20 second range, but with `4` concurrent requests, it jumps to over 500 seconds.  Logs show continual preemption, even when `gpu_memory_utilization = 0.9`.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "aws-neuron",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-29T18:44:52+00:00",
    "closed_at": "2024-12-29T02:05:15+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8007/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8007"
  },
  {
    "number": 4553,
    "title": "[Bug]: AssertionError in neuron_model_runner.py assert len(block_table) == 1",
    "body": "### Your current environment\n\nPyTorch version: 2.1.2+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1031-aws-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   48 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          192\r\nOn-line CPU(s) list:             0-191\r\nVendor ID:                       AuthenticAMD\r\nModel name:                      AMD EPYC 7R13 Processor\r\nCPU family:                      25\r\nModel:                           1\r\nThread(s) per core:              2\r\nCore(s) per socket:              48\r\nSocket(s):                       2\r\nStepping:                        1\r\nBogoMIPS:                        5299.99\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       3 MiB (96 instances)\r\nL1i cache:                       3 MiB (96 instances)\r\nL2 cache:                        48 MiB (96 instances)\r\nL3 cache:                        384 MiB (12 instances)\r\nNUMA node(s):                    4\r\nNUMA node0 CPU(s):               0-23,96-119\r\nNUMA node1 CPU(s):               24-47,120-143\r\nNUMA node2 CPU(s):               48-71,144-167\r\nNUMA node3 CPU(s):               72-95,168-191\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.25.2\r\n[pip3] nvidia-nccl-cu12==2.18.1\r\n[pip3] torch==2.1.2\r\n[pip3] torch-neuronx==2.1.2.2.1.0\r\n[pip3] torch-xla==2.1.2\r\n[pip3] torchvision==0.16.2\r\n[pip3] triton==2.1.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: (0, 'instance-type: inf2.48xlarge\\ninstance-id: i-0e6cfafa7d8d7bcfc\\n+--------+--------+--------+-----------+---------+\\n| NEURON | NEURON | NEURON | CONNECTED |   PCI   |\\n| DEVICE | CORES  | MEMORY |  DEVICES  |   BDF   |\\n+--------+--------+--------+-----------+---------+\\n| 0      | 2      | 32 GB  | 11, 1     | 80:1e.0 |\\n| 1      | 2      | 32 GB  | 0, 2      | 90:1e.0 |\\n| 2      | 2      | 32 GB  | 1, 3      | 80:1d.0 |\\n| 3      | 2      | 32 GB  | 2, 4      | 90:1f.0 |\\n| 4      | 2      | 32 GB  | 3, 5      | 80:1f.0 |\\n| 5      | 2      | 32 GB  | 4, 6      | 90:1d.0 |\\n| 6      | 2      | 32 GB  | 5, 7      | 20:1e.0 |\\n| 7      | 2      | 32 GB  | 6, 8      | 20:1f.0 |\\n| 8      | 2      | 32 GB  | 7, 9      | 10:1e.0 |\\n| 9      | 2      | 32 GB  | 8, 10     | 10:1f.0 |\\n| 10     | 2      | 32 GB  | 9, 11     | 10:1d.0 |\\n| 11     | 2      | 32 GB  | 10, 0     | 20:1d.0 |\\n+--------+--------+--------+-----------+---------+', '')\r\nvLLM Version: 0.4.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\n\n### \ud83d\udc1b Describe the bug\n\nWe tried to run tinyllama on inferentia and run into the follow problem\r\n\r\n```text\r\n    |   File \"/home/ubuntu/vllm/vllm/worker/neuron_model_runner.py\", line 72, in _prepare_prompt\r\n    |     assert len(block_table) == 1\r\n    | AssertionError\r\n```\r\n\r\nTo provide some additional info:\r\n\r\nAWS Instance type: `inf2.48xlarge`\r\n\r\nWe followed the instructions in side of this:\r\nhttps://docs.vllm.ai/en/latest/getting_started/neuron-installation.html\r\n\r\nThe offline inference provided in this works:\r\nhttps://docs.vllm.ai/en/latest/getting_started/examples/offline_inference_neuron.html\r\n\r\nHowever, when we try to run the OpenAI compatible server following: https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\r\n\r\nWith some adjustments to the command. The original command is here:\r\n```bash\r\npython -m vllm.entrypoints.openai.api_server --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 --dtype auto --api-key api-key --max-num-seqs 8 --max-model-len 128 --block-size 128\r\n```\r\nThis gives an error complaining about the `--block-size` and won't compile, saying that the only supportable block size is 8, 16 and 32, so we removed that:\r\n\r\n```bash\r\npython -m vllm.entrypoints.openai.api_server --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 --dtype auto --api-key api-key --max-num-seqs 8\r\n```\r\n\r\nNow it compiles, but when we try to run an inference, it responds with Internal Server Error, and on the server, we see the above AssertionError.\r\n\r\nHere's the full stack trace:\r\n\r\n```text\r\nINFO:     127.0.0.1:60772 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\r\nERROR:    Exception in ASGI application\r\n  + Exception Group Traceback (most recent call last):\r\n  |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/_utils.py\", line 87, in collapse_excgroups\r\n  |     yield\r\n  |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/middleware/base.py\", line 190, in __call__\r\n  |     async with anyio.create_task_group() as task_group:\r\n  |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 678, in __aexit__\r\n  |     raise BaseExceptionGroup(\r\n  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\r\n  +-+---------------- 1 ----------------\r\n    | Traceback (most recent call last):\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 411, in run_asgi\r\n    |     result = await app(  # type: ignore[func-returns-value]\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\r\n    |     return await self.app(scope, receive, send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n    |     await super().__call__(scope, receive, send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/applications.py\", line 123, in __call__\r\n    |     await self.middleware_stack(scope, receive, send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n    |     raise exc\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    |     await self.app(scope, receive, _send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/middleware/base.py\", line 189, in __call__\r\n    |     with collapse_excgroups():\r\n    |   File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\r\n    |     self.gen.throw(typ, value, traceback)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/_utils.py\", line 93, in collapse_excgroups\r\n    |     raise exc\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/middleware/base.py\", line 191, in __call__\r\n    |     response = await self.dispatch_func(request, call_next)\r\n    |   File \"/home/ubuntu/vllm/vllm/entrypoints/openai/api_server.py\", line 142, in authentication\r\n    |     return await call_next(request)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/middleware/base.py\", line 165, in call_next\r\n    |     raise app_exc\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/middleware/base.py\", line 151, in coro\r\n    |     await self.app(scope, receive_or_disconnect, send_no_error)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    |     await self.app(scope, receive, send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\n    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    |     raise exc\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    |     await app(scope, receive, sender)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/routing.py\", line 756, in __call__\r\n    |     await self.middleware_stack(scope, receive, send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/routing.py\", line 776, in app\r\n    |     await route.handle(scope, receive, send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/routing.py\", line 297, in handle\r\n    |     await self.app(scope, receive, send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/routing.py\", line 77, in app\r\n    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    |     raise exc\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    |     await app(scope, receive, sender)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/starlette/routing.py\", line 72, in app\r\n    |     response = await func(request)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/fastapi/routing.py\", line 278, in app\r\n    |     raw_response = await run_endpoint_function(\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n    |     return await dependant.call(**values)\r\n    |   File \"/home/ubuntu/vllm/vllm/entrypoints/openai/api_server.py\", line 94, in create_chat_completion\r\n    |     generator = await openai_serving_chat.create_chat_completion(\r\n    |   File \"/home/ubuntu/vllm/vllm/entrypoints/openai/serving_chat.py\", line 135, in create_chat_completion\r\n    |     return await self.chat_completion_full_generator(\r\n    |   File \"/home/ubuntu/vllm/vllm/entrypoints/openai/serving_chat.py\", line 298, in chat_completion_full_generator\r\n    |     async for res in result_generator:\r\n    |   File \"/home/ubuntu/vllm/vllm/engine/async_llm_engine.py\", line 663, in generate\r\n    |     raise e\r\n    |   File \"/home/ubuntu/vllm/vllm/engine/async_llm_engine.py\", line 657, in generate\r\n    |     async for request_output in stream:\r\n    |   File \"/home/ubuntu/vllm/vllm/engine/async_llm_engine.py\", line 78, in __anext__\r\n    |     raise result\r\n    |   File \"/home/ubuntu/vllm/vllm/engine/async_llm_engine.py\", line 39, in _raise_exception_on_finish\r\n    |     task.result()\r\n    |   File \"/home/ubuntu/vllm/vllm/engine/async_llm_engine.py\", line 498, in run_engine_loop\r\n    |     has_requests_in_progress = await asyncio.wait_for(\r\n    |   File \"/usr/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\r\n    |     return fut.result()\r\n    |   File \"/home/ubuntu/vllm/vllm/engine/async_llm_engine.py\", line 472, in engine_step\r\n    |     request_outputs = await self.engine.step_async()\r\n    |   File \"/home/ubuntu/vllm/vllm/engine/async_llm_engine.py\", line 214, in step_async\r\n    |     output = await self.model_executor.execute_model_async(\r\n    |   File \"/home/ubuntu/vllm/vllm/executor/neuron_executor.py\", line 89, in execute_model_async\r\n    |     output = await make_async(self.driver_worker.execute_model)(\r\n    |   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    |     result = self.fn(*self.args, **self.kwargs)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    |     return func(*args, **kwargs)\r\n    |   File \"/home/ubuntu/vllm/vllm/worker/neuron_worker.py\", line 87, in execute_model\r\n    |     output = self.model_runner.execute_model(seq_group_metadata_list)\r\n    |   File \"/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    |     return func(*args, **kwargs)\r\n    |   File \"/home/ubuntu/vllm/vllm/worker/neuron_model_runner.py\", line 176, in execute_model\r\n    |     ) = self.prepare_input_tensors(seq_group_metadata_list)\r\n    |   File \"/home/ubuntu/vllm/vllm/worker/neuron_model_runner.py\", line 152, in prepare_input_tensors\r\n    |     prompt_lens) = self._prepare_prompt(seq_group_metadata_list)\r\n    |   File \"/home/ubuntu/vllm/vllm/worker/neuron_model_runner.py\", line 72, in _prepare_prompt\r\n    |     assert len(block_table) == 1\r\n    | AssertionError\r\n    +------------------------------------\r\n```\r\n\r\n@liangfu Saw that you wrote most of the code here for Neuron. Do you have any clue on this one?",
    "labels": [
      "bug",
      "aws-neuron",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-02T11:11:34+00:00",
    "closed_at": "2024-12-19T02:05:09+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4553/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4553"
  },
  {
    "number": 1866,
    "title": "[RFC] Initial Support for AWS Inferentia",
    "body": "## Proposal\r\n\r\nWe propose to integrate transformers-neuronx to be the execution engine in vLLM for supporting LLM inference on Inferentia. This would require changes on both transformers-neuronx and vLLM.\r\n\r\n### Changes to transformers-neuronx\r\n\r\n1. Support batch size 1 prompt encoding, while share same cache space with max batch size decoding.\r\n2. Support batch-dependent KV cache update. Each sequence will have a specified position_id to update cache.\r\n3. Support virtual dynamic batching. This would enable multi-batch prompt encoding virtually agnostic to vLLM.\r\n\r\n### Changes to vLLM\r\n\r\n- [x] Make CUDA kernel compilation optional, so that when we are trying to perform LLM inference on inf2 instances we don\u2019t necessarily compile the CUDA kernels. Meanwhile, we would still keep CUDA kernel compilation enabled by default. https://github.com/vllm-project/vllm/pull/2065\r\n- [x] Add transformers-neuronx package as a (optional) thirdparty dependency of vllm.  Note that transformers-neuronx would further depend on torch-neuronx, torch-xla, neuronx-cc and many others. https://github.com/vllm-project/vllm/pull/2065\r\n- [x] Configure transformers-neuronx to enable continuous batching feature in vLLM model loader. https://github.com/vllm-project/vllm/pull/2569\r\n- [x] Compile the model after loading weights. https://github.com/vllm-project/vllm/pull/2569\r\n- [x] Execute model with transformers-neuronx. https://github.com/vllm-project/vllm/pull/2569\r\n\r\n## Implementation Details\r\n\r\n### Model-specific (e.g. llama specific code for neuron) forward function\r\n\r\n```python\r\ndef forward(\r\n    self,\r\n    input_ids: torch.Tensor,\r\n    positions: torch.Tensor,\r\n    kv_caches: List[KVCache],\r\n    input_metadata: InputMetadata,\r\n    cache_events: Optional[List[torch.cuda.Event]],\r\n) -> SamplerOutput:\r\n    batch_size, n_active_tokens = input_ids.shape\r\n\r\n    with torch.inference_mode():\r\n        seq_ids = []\r\n        block_size = self.model.context_buckets[-1]\r\n        if input_metadata.num_generation_tokens == 0:\r\n            num_prompts = input_metadata.num_prompts\r\n            seq_ids = torch.zeros(num_prompts, 1, dtype=torch.int64, device='cpu')\r\n            anchor = 0\r\n            for prompt_id in range(num_prompts):\r\n                seq_ids[prompt_id] = input_metadata.slot_mapping[anchor] // block_size\r\n                anchor += input_metadata.prompt_lens[prompt_id]\r\n        else:\r\n            seq_ids = input_metadata.block_tables\r\n\r\n        logits = self.model(input_ids, cache_ids=positions, start_ids=seq_ids)\r\n        next_tokens = self.sampler(logits, input_metadata)\r\n\r\n    return next_tokens\r\n```\r\n\r\n### Model compilation\r\n\r\n```\r\ndef load_weights(self,\r\n                    model_name_or_path: str,\r\n                    cache_dir: Optional[str] = None,\r\n                    load_format: str = \"auto\",\r\n                    revision: Optional[str] = None,\r\n                    **kwargs):\r\n    from transformers_neuronx.llama.model import LlamaForSampling\r\n\r\n    if not os.path.exists(f\"{model_name_or_path}-split\"):\r\n        from transformers.models.llama import LlamaForCausalLM\r\n        from transformers_neuronx.module import save_pretrained_split\r\n\r\n        hf_model = LlamaForCausalLM.from_pretrained(model_name_or_path, low_cpu_mem_usage=True)\r\n        save_pretrained_split(hf_model, f\"{model_name_or_path}-split\")\r\n\r\n    self.model = LlamaForSampling.from_pretrained(f\"{model_name_or_path}-split\", **kwargs)\r\n    self.model.to_neuron()\r\n```\r\n\r\n### Model-agnostic (e.g. generic model loader)\r\n\r\n```python\r\n# Load the weights from the cached or downloaded files.\r\nfrom transformers_neuronx.config import NeuronConfig, ContinuousBatchingConfig\r\n\r\ncontinuous_batching_config = ContinuousBatchingConfig(batch_size_for_shared_caches=scheduler_config.max_num_seqs)\r\nneuron_config = NeuronConfig(continuous_batching=continuous_batching_config)\r\nmodel.load_weights(model_config.model, model_config.download_dir,\r\n                    model_config.load_format, model_config.revision,\r\n                    tp_degree=parallel_config.tp_degree,\r\n                    amp='f32', neuron_config=neuron_config,\r\n                    context_length_estimate=[scheduler_config.max_model_len],\r\n                    n_positions=[scheduler_config.max_model_len],\r\n                    batch_size=scheduler_config.max_num_seqs)\r\n```\r\n\r\n### Related Resources\r\n\r\nStable release versions of transformers-neuronx packages can be found from https://pip.repos.neuron.amazonaws.com/transformers-neuronx/ . We can install transformers-neuronx pacakge with\r\n\r\n```\r\npip install transformers-neuronx --extra-index-url=https://pip.repos.neuron.amazonaws.com\r\n```",
    "labels": [
      "aws-neuron"
    ],
    "state": "closed",
    "created_at": "2023-11-30T15:02:11+00:00",
    "closed_at": "2024-03-02T00:59:00+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1866/reactions",
      "total_count": 18,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 7,
      "confused": 0,
      "heart": 2,
      "rocket": 6,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/1866"
  },
  {
    "number": 19631,
    "title": "[Bug]: Illegal memory access on llama4 maverick",
    "body": "### Your current environment\n\nPyTorch 2.7.0, vLLM main branch built from source.\n\n### \ud83d\udc1b Describe the bug\n\nRepro:\n```py\nvllm serve meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 --tensor-parallel-size 8 --max-num-batched-tokens 40000 --max-model-len 8192 --max-num-seqs 128 --gpu-memory-utilization 0.8\n```\ngives a CUDA Illegal Memory Access, as well as some errors:\n```\nERROR 06-13 15:32:09 [core.py:515] EngineCore failed to start.\nERROR 06-13 15:32:09 [core.py:515] Traceback (most recent call last):\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/engine/core.py\", line 506, in run_engine_core\nERROR 06-13 15:32:09 [core.py:515]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 06-13 15:32:09 [core.py:515]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/engine/core.py\", line 390, in __init__\nERROR 06-13 15:32:09 [core.py:515]     super().__init__(vllm_config, executor_class, log_stats,\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/engine/core.py\", line 83, in __init__\nERROR 06-13 15:32:09 [core.py:515]     self._initialize_kv_caches(vllm_config)\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/engine/core.py\", line 168, in _initialize_kv_caches\nERROR 06-13 15:32:09 [core.py:515]     self.model_executor.initialize_from_config(kv_cache_configs)\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/executor/abstract.py\", line 66, in initialize_from_config\nERROR 06-13 15:32:09 [core.py:515]     self.collective_rpc(\"compile_or_warm_up_model\")\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/executor/multiproc_executor.py\", line 220, in collective_rpc\nERROR 06-13 15:32:09 [core.py:515]     result = get_response(w, dequeue_timeout)\nERROR 06-13 15:32:09 [core.py:515]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-13 15:32:09 [core.py:515]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/v1/executor/multiproc_executor.py\", line 207, in get_response\nERROR 06-13 15:32:09 [core.py:515]     raise RuntimeError(\nERROR 06-13 15:32:09 [core.py:515] RuntimeError: Worker failed with error 'Expected result >= 0 to be true, but got false.  (Could this error message be\n improved?  If so, please report an enhancement request to PyTorch.)', please check the stack trace above for the root cause\n(VllmWorker rank=1 pid=3350867) ERROR 06-13 15:32:09 [multiproc_executor.py:527]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/compilation/cuda_piece\nwise_backend.py\", line 156, in __call__\n(VllmWorker rank=1 pid=3350867) ERROR 06-13 15:32:09 [multiproc_executor.py:527]     return entry.runnable(*args)\n(VllmWorker rank=2 pid=3350868) ERROR 06-13 15:32:09 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=1 pid=3350867) ERROR 06-13 15:32:09 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=2 pid=3350868) ERROR 06-13 15:32:09 [multiproc_executor.py:527]   File \"/home/rzou/.cache/vllm/torch_compile_cache/d98525c527/rank_2_0/\ninductor_cache/rl/crl3f6qy7nm5k2qs65o6f44vppuehyqkkmjhxy6q5mty7zgba2kx.py\", line 1282, in call\n(VllmWorker rank=7 pid=3350875) ERROR 06-13 15:32:09 [multiproc_executor.py:527]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/compilation/cuda_piece\nwise_backend.py\", line 156, in __call__\n(VllmWorker rank=1 pid=3350867) ERROR 06-13 15:32:09 [multiproc_executor.py:527]   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/compilation/compiler_i\nnterface.py\", line 510, in compiled_graph\n(VllmWorker rank=2 pid=3350868) ERROR 06-13 15:32:09 [multiproc_executor.py:527]     buf52 = empty_strided_cuda(((-32768) + s0, ), (1, ), torch.int32)\n(VllmWorker rank=5 pid=3350871) ERROR 06-13 15:32:09 [multiproc_executor.py:527]     return self.current_callable(inputs)\n(VllmWorker rank=7 pid=3350875) ERROR 06-13 15:32:09 [multiproc_executor.py:527]     return entry.runnable(*args)\n(VllmWorker rank=6 pid=3350873) ERROR 06-13 15:32:09 [multiproc_executor.py:527]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n...\n\n(VllmWorker rank=7 pid=3350875) Exception ignored in: <function CustomAllreduce.__del__ at 0x7efceedfe2a0>\n(VllmWorker rank=7 pid=3350875) Traceback (most recent call last):\n(VllmWorker rank=7 pid=3350875)   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/distributed/device_communicators/custom_all_reduce.py\", line 276, in __\ndel__\n(VllmWorker rank=7 pid=3350875)     self.close()\n(VllmWorker rank=7 pid=3350875)   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/distributed/device_communicators/custom_all_reduce.py\", line 272, in cl\nose\n(VllmWorker rank=7 pid=3350875)     self.free_shared_buffer(self.meta_ptrs, rank=self.rank)\n(VllmWorker rank=7 pid=3350875)   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/distributed/device_communicators/custom_all_reduce.py\", line 304, in fr\nee_shared_buffer\n(VllmWorker rank=7 pid=3350875)     ops.free_shared_buffer(pointers[rank])\n(VllmWorker rank=7 pid=3350875)   File \"/home/rzou/dev/stable0/vllm-stable0/vllm/_custom_ops.py\", line 1758, in free_shared_buffer\n(VllmWorker rank=7 pid=3350875)     torch.ops._C_custom_ar.free_shared_buffer(ptr)\n(VllmWorker rank=7 pid=3350875)   File \"/home/rzou/dev/stable0/env/lib/python3.12/site-packages/torch/_ops.py\", line 1158, in __call__\n(VllmWorker rank=7 pid=3350875)     return self._op(*args, **(kwargs or {}))\n(VllmWorker rank=7 pid=3350875)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=7 pid=3350875) RuntimeError: CUDA error: an illegal memory access was encountered\n(VllmWorker rank=7 pid=3350875) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n(VllmWorker rank=7 pid=3350875) For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n(VllmWorker rank=7 pid=3350875) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n(VllmWorker rank=7 pid=3350875)\n(VllmWorker rank=1 pid=3350867) ERROR 06-13 15:32:09 [multiproc_executor.py:527]     graph_output = inductor_compiled_graph(list_args)\n(VllmWorker rank=5 pid=3350871) ERROR 06-13 15:32:09 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=2 pid=3350868) ERROR 06-13 15:32:09 [multiproc_executor.py:527]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=7 pid=3350875) ERROR 06-13 15:32:09 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^\n```\n\nI think this started from https://github.com/vllm-project/vllm/pull/19168. After turning off the chunking optimization, the errors go away.\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "torch.compile",
      "llama"
    ],
    "state": "closed",
    "created_at": "2025-06-13T22:33:29+00:00",
    "closed_at": "2025-07-07T17:10:56+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19631/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19631"
  },
  {
    "number": 18022,
    "title": "[Bug]: vLLM does not serve text-only version of Llama4",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nNot related\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nHi all! \nI am trying to serve a text-only version of Llama 4 Scout (17B-16E) using vLLM. This model requires the Llama4ForCausalLM architecture. However, it seems that vLLM currently expects only the multimodal Llama 4.\n\nAlthough the Llama4ForCausalLM class is implemented in vllm/model_executor/models/llama4.py, it is not registered in the _TEXT_GENERATION_MODELS dictionary in vllm/model_executor/models/registry.py. After manually adding an entry for Llama4ForCausalLM, I was able to serve the model successfully.\n\nThis looks like an oversight or a missing feature, and might be considered a bug.\n\nFor the reference, the text-only version of Llama4 was loaded and saved with AutoModelForCausalLM with the model config updated accordingly. \n```\nmodel_config = AutoConfig.from_pretrained(config[\"model\"][\"path\"], trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    config[\"model\"][\"path\"],\n    attn_implementation=config[\"model\"][\"attn_implementation\"],\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n    trust_remote_code=True,\n    token=token,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(config[\"model\"][\"path\"])\n\n```\n\n```\nos.makedirs(path, exist_ok=True)\nmodel.save_pretrained(path)\ntokenizer.save_pretrained(path)\nmodel_config.save_pretrained(path)\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "llama"
    ],
    "state": "open",
    "created_at": "2025-05-12T20:23:48+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18022/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/18022"
  },
  {
    "number": 4210,
    "title": "Performance Regression between v0.4.0 and v0.4.1",
    "body": "### Anything you want to discuss about vllm.\n\n#3550 seems to reduce throughput of vLLM\r\n\r\nBefore: Throughput: 20.13 requests/s, 10308.29 tokens/s\r\nAfter: Throughput: 17.67 requests/s, 9048.03 tokens/s\r\n\r\n(reported by @esmeetu and @youkaichao)",
    "labels": [
      "performance",
      "release-blocker"
    ],
    "state": "closed",
    "created_at": "2024-04-19T17:13:42+00:00",
    "closed_at": "2024-04-23T20:12:42+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4210/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4210"
  },
  {
    "number": 4209,
    "title": "[Bug]: OpenAI API Server always reports 0 tokens/s",
    "body": "### Your current environment\n\n```text\nCollecting environment information...\nPyTorch version: 2.2.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n \nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.29.0\nLibc version: glibc-2.35\n \nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-97-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.3.103\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: NVIDIA RTX A6000\nGPU 1: NVIDIA RTX A6000\nGPU 2: NVIDIA RTX A6000\nGPU 3: NVIDIA RTX A6000\nGPU 4: NVIDIA RTX A6000\nGPU 5: NVIDIA RTX A6000\nGPU 6: NVIDIA RTX A6000\nGPU 7: NVIDIA RTX A6000\n \nNvidia driver version: 545.23.08\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n \nCPU:\nArchitecture: x86_64\nCPU op-mode(s): 32-bit, 64-bit\nAddress sizes: 46 bits physical, 57 bits virtual\nByte Order: Little Endian\nCPU(s): 112\nOn-line CPU(s) list: 0-111\nVendor ID: GenuineIntel\nModel name: Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz\nCPU family: 6\nModel: 106\nThread(s) per core: 2\nCore(s) per socket: 28\nSocket(s): 2\nStepping: 6\nCPU max MHz: 3500.0000\nCPU min MHz: 800.0000\nBogoMIPS: 5200.00\nFlags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\nVirtualization: VT-x\nL1d cache: 2.6 MiB (56 instances)\nL1i cache: 1.8 MiB (56 instances)\nL2 cache: 70 MiB (56 instances)\nL3 cache: 84 MiB (2 instances)\nNUMA node(s): 4\nNUMA node0 CPU(s): 0-13,56-69\nNUMA node1 CPU(s): 14-27,70-83\nNUMA node2 CPU(s): 28-41,84-97\nNUMA node3 CPU(s): 42-55,98-111\nVulnerability Gather data sampling: Mitigation; Microcode\nVulnerability Itlb multihit: Not affected\nVulnerability L1tf: Not affected\nVulnerability Mds: Not affected\nVulnerability Meltdown: Not affected\nVulnerability Mmio stale data: Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed: Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2: Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds: Not affected\nVulnerability Tsx async abort: Not affected\n \nVersions of relevant libraries:\n[pip3] mypy==1.9.0\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] torch==2.2.1\n[pip3] triton==2.2.0\n[conda] Could not collectROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.4.1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0 GPU1 GPU2 GPU3 GPU4 GPU5 GPU6 GPU7 CPU Affinity NUMA Affinity GPU NUMA ID\nGPU0 X NV4 SYS SYS SYS SYS SYS SYS 0-13,56-69 0 N/A\nGPU1 NV4 X SYS SYS SYS SYS SYS SYS 0-13,56-69 0 N/A\nGPU2 SYS SYS X NV4 SYS SYS SYS SYS 0-13,56-69 0 N/A\nGPU3 SYS SYS NV4 X SYS SYS SYS SYS 0-13,56-69 0 N/A\nGPU4 SYS SYS SYS SYS X NV4 SYS SYS 14-27,70-83 1 N/A\nGPU5 SYS SYS SYS SYS NV4 X SYS SYS 14-27,70-83 1 N/A\nGPU6 SYS SYS SYS SYS SYS SYS X NV4 14-27,70-83 1 N/A\nGPU7 SYS SYS SYS SYS SYS SYS NV4 X 14-27,70-83 1 N/A\n \nLegend:\n \n  X = Self\n  SYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX = Connection traversing at most a single PCIe bridge\n  NV# = Connection traversing a bonded set of # NVLinks\n```\n\n### \ud83d\udc1b Describe the bug\n\nIt seems that the async engine logger in the openai api_server is not reporting tokens/s for either prompt or generation throughput.\n\nStart the server with:\n\n```\npython -m vllm.entrypoints.openai.api_server --model facebook/opt-125m\n```\n\nAnd submit requests with:\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\")\n\n# List models API\nmodels = client.models.list()\n# Choose the first model\nmodel = models.data[0].id\nprint(f\"Accessing model API '{model}'\")\n\nprompt = \"Write a recipe for banana bread, then another, and then another!\"\n\n# Chat API\nstream = False\nfor i in range(100):\n    completion = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        stream=stream,\n    )\n\nprint(\"Response:\")\nif stream:\n    for c in completion:\n        print(c)\nelse:\n    print(completion.choices[0].message.content)\n```\n\nYou should be able to see in the server logging output that there are requests running but no tokens/s reported:\n\n```\nINFO 04-19 15:44:56 metrics.py:224] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n```",
    "labels": [
      "bug",
      "release-blocker"
    ],
    "state": "closed",
    "created_at": "2024-04-19T15:49:06+00:00",
    "closed_at": "2024-04-20T03:48:02+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4209/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/4209"
  },
  {
    "number": 20647,
    "title": "[Bug]: Assertion error when serving \"deepseek-ai/DeepSeek-V2-Lite\" with PP in 0.9.2",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : Could not collect\nCMake version                : Could not collect\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu128\nIs debug build               : False\nCUDA used to build PyTorch   : 12.8\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.11.11 | packaged by conda-forge | (main, Mar  3 2025, 20:43:55) [GCC 13.3.0] (64-bit runtime)\nPython platform              : Linux-6.5.0-1024-aws-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.8.93\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : \nGPU 0: NVIDIA L4\nGPU 1: NVIDIA L4\nGPU 2: NVIDIA L4\nGPU 3: NVIDIA L4\n\nNvidia driver version        : 550.163.01\ncuDNN version                : Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.8.0\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      48 bits physical, 48 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             48\nOn-line CPU(s) list:                0-47\nVendor ID:                          AuthenticAMD\nModel name:                         AMD EPYC 7R13 Processor\nCPU family:                         25\nModel:                              1\nThread(s) per core:                 2\nCore(s) per socket:                 24\nSocket(s):                          1\nStepping:                           1\nBogoMIPS:                           5299.99\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid\nHypervisor vendor:                  KVM\nVirtualization type:                full\nL1d cache:                          768 KiB (24 instances)\nL1i cache:                          768 KiB (24 instances)\nL2 cache:                           12 MiB (24 instances)\nL3 cache:                           96 MiB (3 instances)\nNUMA node(s):                       1\nNUMA node0 CPU(s):                  0-47\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Mitigation; Safe RET\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.8.3.14\n[pip3] nvidia-cuda-cupti-cu12==12.8.57\n[pip3] nvidia-cuda-nvrtc-cu12==12.8.61\n[pip3] nvidia-cuda-runtime-cu12==12.8.57\n[pip3] nvidia-cudnn-cu12==9.7.1.26\n[pip3] nvidia-cufft-cu12==11.3.3.41\n[pip3] nvidia-cufile-cu12==1.13.0.11\n[pip3] nvidia-curand-cu12==10.3.9.55\n[pip3] nvidia-cusolver-cu12==11.7.2.55\n[pip3] nvidia-cusparse-cu12==12.5.7.53\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.8.61\n[pip3] nvidia-nvtx-cu12==12.8.55\n[pip3] pyzmq==26.0.3\n[pip3] torch==2.7.0+cu128\n[pip3] torchaudio==2.7.0+cu128\n[pip3] torchvision==0.22.0+cu128\n[pip3] transformers==4.52.4\n[pip3] triton==3.3.0\n[conda] numpy                                       1.26.4           pypi_0              pypi\n[conda] nvidia-cublas-cu12                          12.8.3.14        pypi_0              pypi\n[conda] nvidia-cuda-cupti-cu12                      12.8.57          pypi_0              pypi\n[conda] nvidia-cuda-nvrtc-cu12                      12.8.61          pypi_0              pypi\n[conda] nvidia-cuda-runtime-cu12                    12.8.57          pypi_0              pypi\n[conda] nvidia-cudnn-cu12                           9.7.1.26         pypi_0              pypi\n[conda] nvidia-cufft-cu12                           11.3.3.41        pypi_0              pypi\n[conda] nvidia-cufile-cu12                          1.13.0.11        pypi_0              pypi\n[conda] nvidia-curand-cu12                          10.3.9.55        pypi_0              pypi\n[conda] nvidia-cusolver-cu12                        11.7.2.55        pypi_0              pypi\n[conda] nvidia-cusparse-cu12                        12.5.7.53        pypi_0              pypi\n[conda] nvidia-cusparselt-cu12                      0.6.3            pypi_0              pypi\n[conda] nvidia-nccl-cu12                            2.26.2           pypi_0              pypi\n[conda] nvidia-nvjitlink-cu12                       12.8.61          pypi_0              pypi\n[conda] nvidia-nvtx-cu12                            12.8.55          pypi_0              pypi\n[conda] pyzmq                                       26.0.3           pypi_0              pypi\n[conda] torch                                       2.7.0+cu128      pypi_0              pypi\n[conda] torchaudio                                  2.7.0+cu128      pypi_0              pypi\n[conda] torchvision                                 0.22.0+cu128     pypi_0              pypi\n[conda] transformers                                4.52.4           pypi_0              pypi\n[conda] triton                                      3.3.0            pypi_0              pypi\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.2\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     SYS     SYS     0-47    0               N/A\nGPU1    SYS      X      SYS     SYS     0-47    0               N/A\nGPU2    SYS     SYS      X      SYS     0-47    0               N/A\nGPU3    SYS     SYS     SYS      X      0-47    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n==============================\n     Environment Variables\n==============================\nNVIDIA_VISIBLE_DEVICES=void\nNVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\nNCCL_VERSION=2.25.1-1\nNCCL_SOCKET_IFNAME=^lo,docker,veth,tailscale,anyscale\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNVIDIA_PRODUCT_NAME=CUDA\nCUDA_VERSION=12.8.1\nLD_LIBRARY_PATH=/usr/local/ucx/lib:/usr/local/nixl/lib/x86_64-linux-gnu:/usr/local/cuda/lib64\nVLLM_USE_V1=1\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n```bash\nuv pip install \"vllm==0.9.2\"\nvllm serve deepseek-ai/DeepSeek-V2-Lite \\\n  --trust-remote-code \\\n  --max-model-len=1024 --enforce-eager \\\n  --tensor-parallel-size=2 --pipeline-parallel-size=2 # PP>1 causes error\n```\n\nexception:\n```\n(VllmWorker rank=0 pid=20455) ERROR 07-08 15:41:43 [multiproc_executor.py:487]   File \"/home/ray/anaconda3/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py\", l\nine 64, in initialize_model\n(VllmWorker rank=0 pid=20455) ERROR 07-08 15:41:43 [multiproc_executor.py:487]     return model_class(vllm_config=vllm_config, prefix=prefix)\n(VllmWorker rank=0 pid=20455) ERROR 07-08 15:41:43 [multiproc_executor.py:487]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(VllmWorker rank=0 pid=20455) ERROR 07-08 15:41:43 [multiproc_executor.py:487]   File \"/home/ray/anaconda3/lib/python3.11/site-packages/vllm/model_executor/models/deepseek_v2.py\", l\nine 743, in __init__\n(VllmWorker rank=0 pid=20455) ERROR 07-08 15:41:43 [multiproc_executor.py:487]     assert isinstance(layer, DeepseekV2DecoderLayer)\n```\nand `type(layer)` is `<class 'vllm.model_executor.models.utils.PPMissingLayer'>`\n\nlikely caused by https://github.com/vllm-project/vllm/pull/18343\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "v1",
      "deepseek"
    ],
    "state": "closed",
    "created_at": "2025-07-08T22:48:28+00:00",
    "closed_at": "2025-07-10T03:34:42+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20647/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20647"
  },
  {
    "number": 18866,
    "title": "[Feature]: Vectorize `scaled_int8_quant`",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nSimilar to the recent discoveries in https://github.com/vllm-project/vllm/pull/18844, vectorizing our quantization methods can have a huge impact on e2e performance.\n\nCurrently we only use `vectorization.h` in `csrc/quantization/fp8/common.cuh` and `csrc/quantization/fused_kernels/layernorm_utils.cuh`, so we should expand this to more implementations like `csrc/quantization/compressed_tensors/int8_quant_kernels.cu` for faster INT8 activation quantization.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "feature request",
      "kernel"
    ],
    "state": "closed",
    "created_at": "2025-05-28T23:47:33+00:00",
    "closed_at": "2025-06-15T11:08:02+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18866/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18866"
  },
  {
    "number": 15237,
    "title": "[Feature]: OpenAI Response API",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nI come across this https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses&example=chain-of-thought#streaming wrt to their new Response API, so we probably also want to add support in vLLM.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "feature request",
      "frontend"
    ],
    "state": "closed",
    "created_at": "2025-03-20T17:13:04+00:00",
    "closed_at": "2025-03-20T22:52:05+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15237/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15237"
  },
  {
    "number": 18619,
    "title": "[Bug][PERF]: Qwen2.5 performance degradation 0.8.4 -> 0.8.5",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code> for v0.8.4</summary>\n\n```text\nINFO 05-23 14:36:10 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : Could not collect\nCMake version                : version 4.0.2\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.6.0+cu124\nIs debug build               : False\nCUDA used to build PyTorch   : 12.4\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0] (64-bit runtime)\nPython platform              : Linux-5.15.0-130-generic-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.8.61\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : GPU 0: NVIDIA H100 80GB HBM3\nNvidia driver version        : 550.127.08\ncuDNN version                : Could not collect\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        43 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               16\nOn-line CPU(s) list:                  0-15\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8468\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   8\nSocket(s):                            1\nStepping:                             8\nBogoMIPS:                             4200.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq dtes64 ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk amx_bf16 avx512_fp16 amx_tile amx_int8 arch_capabilities\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            512 KiB (16 instances)\nL1i cache:                            512 KiB (16 instances)\nL2 cache:                             32 MiB (8 instances)\nL3 cache:                             16 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-15\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Unknown: No mitigations\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Mitigation; TSX disabled\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.52.3\n[pip3] triton==3.2.0\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.8.4\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-15    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n==============================\n     Environment Variables\n==============================\nLD_LIBRARY_PATH=/usr/local/cuda-12.8/lib64:/usr/mpi/gcc/openmpi-4.1.7a1/lib\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n<details>\n<summary>The output of <code>python collect_env.py</code> for v0.8.5</summary>\n\n```text\nINFO 05-23 14:38:32 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\n/home/vadim/.local/lib/python3.10/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n  warnings.warn(\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : Could not collect\nCMake version                : version 4.0.2\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.6.0+cu124\nIs debug build               : False\nCUDA used to build PyTorch   : 12.4\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0] (64-bit runtime)\nPython platform              : Linux-5.15.0-130-generic-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.8.61\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : GPU 0: NVIDIA H100 80GB HBM3\nNvidia driver version        : 550.127.08\ncuDNN version                : Could not collect\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        43 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               16\nOn-line CPU(s) list:                  0-15\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8468\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   8\nSocket(s):                            1\nStepping:                             8\nBogoMIPS:                             4200.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq dtes64 ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk amx_bf16 avx512_fp16 amx_tile amx_int8 arch_capabilities\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            512 KiB (16 instances)\nL1i cache:                            512 KiB (16 instances)\nL2 cache:                             32 MiB (8 instances)\nL3 cache:                             16 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-15\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Unknown: No mitigations\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Mitigation; TSX disabled\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.2.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-cufile-cu12==1.11.1.6\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.52.3\n[pip3] triton==3.2.0\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.8.5.post1\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-15    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n==============================\n     Environment Variables\n==============================\nLD_LIBRARY_PATH=/usr/local/cuda-12.8/lib64:/usr/mpi/gcc/openmpi-4.1.7a1/lib\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI found a performance degradation for Qwen2.5 model 0.8.4 vs 0.8.5.\n\nvllm serve run command:\n```\nvllm serve Qwen/Qwen2.5-VL-3B-Instruct --disable-log-requests --max-num-seqs 1024  --block-size 16 --max-num-batched-tokens 2048 --no-enable-prefix-caching\n```\n\nbench run command: \n```\nvllm bench serve  --model Qwen/Qwen2.5-VL-3B-Instruct --request-rate 200  --num-prompts 1000  --random-input-len 600 --random-output-len 125 --ignore-eos\n```\n\n**Results of 7 runs**\n0.8.4\n```\nThroughput values: 62.75, 63.09, 62.81, 62.78, 62.90, 62.79, 61.42\nMedian throughput: 62.79 req/s\nStandard error: 0.21 (0.33%)\n```\n\n0.8.5\n```\nThroughput values: 57.87, 57.91, 57.71, 57.74, 57.98, 57.80, 57.74\nMedian throughput: 57.80 req/s\nStandard error: 0.04 (0.07%)\n```\n\nDegradation is **8.6%**\n\n\n**Profile analysis.**\nI see sufficient increase in time we spend in attn. \n0.8.4\n![Image](https://github.com/user-attachments/assets/3e328875-612b-46dd-977b-227eedceef76)\n\n0.8.5\n\n![Image](https://github.com/user-attachments/assets/723238c8-f8ff-4988-b991-d9342d17d605)\n\nPercentage we spend in attn increased from 14.2% to 38.8% \n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "qwen"
    ],
    "state": "open",
    "created_at": "2025-05-23T14:48:43+00:00",
    "closed_at": null,
    "comments": 34,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18619/reactions",
      "total_count": 4,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 4
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/18619"
  },
  {
    "number": 16825,
    "title": "[Bug]: Bug in LRUEvictor: priority_queue and free_table desynchronization cause error",
    "body": "### Your current environment\n\nvllm 0.7.3\n\n### \ud83d\udc1b Describe the bug\n\n### Your current environment\nvllm 0.7.3\n### \ud83d\udc1b Describe the bug\nWe encountered a bug in the LRUEvictor implementation when running VLLM (version 0.7.3) with the --preemption-mode swap flag.\nThe issue arises due to desynchronization between self.priority_queue and self.free_table in the remove method.\n<img width=\"1561\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4d048b91-f914-43b9-89e2-5b6daf0c2012\" />\nAdd logging to evictor.py and prefix_caching_block.py to track block additions and removals.\n<img width=\"1259\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e2b876a9-bff6-4004-b591-636d77eaa64d\" />\n<img width=\"589\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/1a72b5d7-867c-49d9-a0af-cc3229a3ed47\" />\n<img width=\"644\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/996e42c3-5899-4d33-8532-9193eb91c3f0\" />\n<img width=\"761\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d60ab91e-185c-4676-a530-4734d1a6fe29\" />\n<img width=\"705\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c9f57cd2-07ba-44ff-8dd7-41b03f046fcf\" />\n<img width=\"1047\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a363db5b-8adc-43cd-b7d7-f1b1c4128268\" />\nThe issue by observing the following sequence:\nline 25: block (block_id=862 content_hash=4781171782003088483 num_hashed_tokens=768 last_accessed=-1) is added to self.free_table and self.priority_queue. (in add method)\nline 26: The block is removed from self.free_table but remains in self.priority_queue. (in remove method, only self.free_table is altered)\nline 28: The block is added again to self.free_table and self.priority_queue. (in add method)\nline 29: The block is removed from self.free_table, and the one added in line 25 is removed from self.priority_queue. (in evict method)\nline 31: block(block_id=862 content_hash=-1708738876872868168 num_hashed_tokens=192 last_accessed=-1) is added to self.free_table and self.priority_queue. note the content_hash is different\u3002(in add method)\nline 32: evict method remove the one added in line 31 from self.free_table but pop the one added in line 28 from self.priority_queue, they have different content_hash. and the error occured.\n<img width=\"1447\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/12b122f0-a8a0-48d1-9f56-615490138bfd\" />\n\nI think in evict method\uff0cwe should compare all field in BlockMetaData to get the right block to return, just like blow\n<img width=\"771\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/eb4e881f-d783-4209-9c3c-71313a84184d\" />\n\nAfter making the change, we tested it and found no problems.",
    "labels": [
      "bug",
      "v0"
    ],
    "state": "open",
    "created_at": "2025-04-18T08:19:48+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16825/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16825"
  }
]
[
  {
    "number": 9609,
    "title": "[Performance]: test speculative decode accuracy",
    "body": "### Proposal to improve performance\n\nI use lm-evaluation-harness to test vllm accuracy\r\n1.when don't enable spec decode,I got some result below\r\nnum_concurrent=1\r\n![image](https://github.com/user-attachments/assets/dfa6ef55-216e-4460-9ef4-d387e0ce460e)\r\n\r\nnum_concurrent=8\r\n![image](https://github.com/user-attachments/assets/505d051f-f119-4275-a5d4-5683b74be398)\r\n\r\nnum_concurrent=16\r\n![image](https://github.com/user-attachments/assets/87e7c9c6-f2de-43de-8a20-96f82c4c9c7c)\r\n\r\nnum_concurrent=32\r\n![image](https://github.com/user-attachments/assets/312e2703-cfc8-42c7-9751-22a0b1aba21d)\r\n\r\n\r\n2.when enable spec decode,I got some result below\r\nnum_concurrent=1\r\n![image](https://github.com/user-attachments/assets/6681a17f-3bc7-4d52-b0e5-5451a40dfcf4)\r\n\r\nnum_concurrent=8\r\n![image](https://github.com/user-attachments/assets/4a1878a8-2da7-475e-9ecd-8400a6fc0620)\r\n\r\nnum_concurrent=16\r\n![image](https://github.com/user-attachments/assets/fc9ca925-a57c-4c6f-9a07-1fa056e67d66)\r\n\r\nnum_concurrent=32\r\n![image](https://github.com/user-attachments/assets/67d284c2-9023-440c-88fa-f61c3f6090de)\r\n\r\nHas anyone done such experiments? Does vLLM's speculative decoding affect output accuracy?\r\n\n\n### Report of performance regression\n\n_No response_\n\n### Misc discussion on performance\n\n_No response_\n\n### Your current environment (if you think it is necessary)\n\nh100 1gpu\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance"
    ],
    "state": "closed",
    "created_at": "2024-10-23T07:40:46+00:00",
    "closed_at": "2024-10-25T09:18:03+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9609/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9609"
  },
  {
    "number": 17062,
    "title": "[Performance]: UVA vs UVM for CPU offloading on v0.8.4+",
    "body": "### Proposal to improve performance\n\nReferencing the recent implementation on https://github.com/vllm-project/vllm/pull/15354 (v0.8.4+) for CPU offloading\n\n@youkaichao, is there any specific reason to pick UVA (`cudaHostAlloc`) over UVM `cudaMallocManaged()`? \n\n1. UVM goes further than UVA to manage data automatically, often using page-faulting hardware to migrate pages on demand. On systems like the GH200, this has potentially additional benefits such as hardware orchestrated frequency based migration. \n2. A key benefit of Unified Memory is simplifying the heterogeneous computing memory model by eliminating the need for deep copies when accessing structured data in GPU kernels. [Source](https://developer.nvidia.com/blog/unified-memory-in-cuda-6/#unified_memory_or_unified_virtual_addressing)\n3. On several discussion threads, the larger access sizes of CPU offloading makes UVM seems to be the better approach compared to UVA [Source](https://forums.developer.nvidia.com/t/page-fault-profiling/265320/3?u=rajeshshashikumar)\n\nGoing by [this](https://arxiv.org/pdf/2407.07850), if transparent offloading is desired `cudaMallocManaged()` seems to be desirable for platforms such as the GH200\n\n<img width=\"474\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/936174e3-1559-48c8-b02f-440e93e30d61\" />\n\nAlternatively, [Pie](https://arxiv.org/pdf/2411.09317) seems to show that the old implementation where \n> before every forward in every layer, we move tensors from cpu to gpu, and compute in gpu\n\nseems to work best in cases such as the GH200 when carefully prefetching layers of the KV cache to reduce the penalty of oversubscription\n\n### Report of performance regression\n\n_No response_\n\n### Misc discussion on performance\n\n_No response_\n\n### Your current environment (if you think it is necessary)\n\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance"
    ],
    "state": "open",
    "created_at": "2025-04-23T15:58:29+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17062/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17062"
  },
  {
    "number": 12266,
    "title": "[Performance]:Why do the prefill and decoding need to be executed twice for the same task?",
    "body": "### Proposal to improve performance\n\n\n\n\n\n### Report of performance regression\n\n_No response_\n\n### Misc discussion on performance\n\nHello, when I start the serving service using vllm serve and conduct tests using the benchmark_serving.py script, I captured the kernel pipeline of the CUDA backend through the nsight system. I found out why the prefill and decoding stages of the same task are executed twice?\n\n![Image](https://github.com/user-attachments/assets/19c57ea0-bb61-49a5-ad3e-e2e4a678b845)\n\nAt the same time, my commands are as follows:\n* serving:\n```\nvllm serve data/llama-3-8b-instruct \\\n        --swap-space 16 \\\n        --disable-log-requests \\\n        --tensor-parallel-size 2 \\\n        --gpu-memory-utilization 0.9 \\\n        --dtype bfloat16\n        --enforce-eager\n```\n* client:\n```\npython3 vllm/benchmarks/benchmark_serving.py \\\n        --backend vllm \\\n        --model data/llama-3-8b-instruct \\\n        --profile \\\n        --dataset-name random \\\n        --random-input-len 2048 \\\n        --random-output-len 200 \\\n        --num-prompts  1 \\\n        --trust-remote-code\n```\n\nI wonder, is there any trick for executing the prefill and decoding twice for the same prompt? \n\n\n### Your current environment (if you think it is necessary)\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance"
    ],
    "state": "closed",
    "created_at": "2025-01-21T13:19:51+00:00",
    "closed_at": "2025-01-22T05:44:54+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12266/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12266"
  },
  {
    "number": 14214,
    "title": "[New Model]: pfnet/plamo-2-8b",
    "body": "### The model to consider.\n\nPlease add support for PFN's plamo-2-8b https://huggingface.co/pfnet/plamo-2-8b\n\n### The closest model vllm already supports.\n\n_No response_\n\n### What's your difficulty of supporting the model you want?\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-04T14:59:42+00:00",
    "closed_at": "2025-07-11T02:16:10+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14214/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14214"
  },
  {
    "number": 16106,
    "title": "[New Model]: Llama4 Support",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nMeta released 2 Variants:\n\nLlama 4 Scout:\nA high-performing small model with 17B activated parameters across 16 experts. Extremely fast, natively multimodal, supports a 10M+ token context window, and runs on a single GPU.\n\nLlama 4 Maverick:\nA top-tier multimodal model outperforming GPT-4o and Gemini 2.0 Flash, with performance on par with DeepSeek V3 at half the active parameters. ELO 1417 on LMArena and runs on a single host.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2025-04-05T21:39:19+00:00",
    "closed_at": "2025-04-06T04:32:40+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16106/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16106"
  },
  {
    "number": 5124,
    "title": "[New Model]: LLaVA-NeXT-Video support",
    "body": "### The model to consider.\n\nThe llava-next-video project has already been released, and the test results are quite good. Are there any plans to support this project?\r\n`https://github.com/LLaVA-VL/LLaVA-NeXT/blob/inference/docs/LLaVA-NeXT-Video.md`\r\nCurrently, Hugging Face does not support this model.\n\n### The closest model vllm already supports.\n\n_No response_\n\n### What's your difficulty of supporting the model you want?\n\n_No response_",
    "labels": [
      "new-model"
    ],
    "state": "closed",
    "created_at": "2024-05-30T03:22:17+00:00",
    "closed_at": "2024-09-11T05:21:37+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5124/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5124"
  },
  {
    "number": 3587,
    "title": "[RFC]: Interface and Abstraction for Distributed Inference Environment",
    "body": "This RFC describes a proposal for interfaces and abstractions for distributed inference environments. I plan to solicit discussions for a week (until March 31st) before I begin to actually refactor the code.\r\n\r\n# Motivation\r\n\r\nThe current distributed inference environment in `vllm` is quite tangled, and we often see deadlocks and hangs (see https://github.com/vllm-project/vllm/issues/3455 , https://github.com/vllm-project/vllm/issues/2770 , https://github.com/vllm-project/vllm/issues/3559 , to name a few). The problem becomes prominent when we try to upgrade to pytorch 2.2.0 (see https://github.com/vllm-project/vllm/pull/3442 , https://github.com/vllm-project/vllm/pull/3442 ), because `pytorch 2.2.0` upgrades from `nccl==2.18.1` to `2.19.3` (see https://pypi.org/pypi/torch/2.1.2/json and https://pypi.org/pypi/torch/2.2.0/json to compare the dependency), and `nccl==2.19.3` breaks `vllm` due to increased memory cost during cudagraph capture (from 10MB per graph to 100MB per graph, adds up to several GBs because we have dozens of cudagraph).\r\n\r\nTL,DR; distributed inference in current codebase is a headache. If it works, hooray; if not, don't be surprised.\r\n\r\n# Proposal\r\n\r\n## Abstraction\r\n\r\nI think we should have three levels of abstraction:\r\n\r\n1. Launcher, responsible for launching processes (potentially across multi-node). Currently it is `ray`, but we can also have another choices like Python's native `multiprocessing` in single-node cases. See https://github.com/vllm-project/vllm/pull/3466 for example.\r\n2. Coordinator, responsible for coordinating shared resources (e.g. filesystem usage) and broadcasting some messages. Currently we don't have this, and there are lots of hacks for ad-hoc implementation, e.g. use `filelock` to lock on filesystems ( https://github.com/vllm-project/vllm/pull/3578 ), use `TCP` to initialize communication in `cupy` ( https://github.com/vllm-project/vllm/pull/2811 ), use `MPI` to initialize communication in AMD's `cupy` version ( https://github.com/vllm-project/vllm/pull/3123 ).\r\n3. Communicator, responsible for cross-device communication of large tensor data (e.g. perform allreduce). Currently we support `nccl`, and AMD also has its own communication library. Note that this is vendor-specific, and vendors usually have their own way of cross-device communication.\r\n\r\nThe most messy one, and **the missing one, is the Coordinator abstraction level**. More on this later.\r\n\r\n## Interface\r\n\r\nBetween each consecutive abstractions, lies the interface. \r\n\r\n### Interface between Launcher and Coordinator\r\n\r\nAfter Launcher launches processes, it needs to at least tell the processes the following information: \r\n- `launch_id`, used to distinguish current launch with possibly concurrent launch (e.g. when 4 users want to set up 4 inference engines in the same node, each with 2 GPUs). Note: the `launch_id` can be used as a \"random seed\" to draw values for `master_port`, instead of keeping only one default `master_port` value and having to kill all processes after the last run crashes. A reference implementation would be hashing the `launch_id` to a port number, and increasing the port number to find the first free port. This is a strategy taken by [Jupyter Notebook/Lab Server](https://github.com/jupyter/) .\r\n- `world_size`, number of processes participating in the current launch (may span over multiple nodes)\r\n- `local_world_size`, number of processes participating in the current launch **in the current node** (not necessarily the same across nodes)\r\n- `rank`, range from 0 (inclusive) to `world_size` (exclusive) , unique in the launch for each process\r\n- `local_rank`, range from 0 (inclusive) to `local_world_size` (exclusive), unique in each node, **can use this to assign devices in a node!**\r\n- `master_addr`, the IP address of the master node, should be reachable from all nodes\r\n- `master_port`, a free port in the master node, reserved for possible coordination\r\n- other custom information can be added, but the above are required.\r\n\r\nHow does Launcher pass these information to each process? Basically we have two choices:\r\n1. through environment variables, the simplest way, but will disable the usage of thread-level distributed inference because environment variables are shared within threads in one process. (However, thread-level distributed inference seems rare. Do we need to consider this?)\r\n2. through serialization and deserialization (e.g. passing bytes in a shared object store), the most general way, at the cost of complexity and runtime efficiency to design and execute the serialization/deserialization\r\n\r\n### Interface between Coordinator and Communicator\r\n\r\nDevice communicators (e.g. `nccl`) often need to initialize the communication by sharing some unique token (see [`nccl` documentation](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html)). In addition, processes sometimes need to coordinate the resource in a node or across the cluster.\r\n\r\nIn sight of the above consideration, `Coordinator` should at least have the following interfaces:\r\n\r\n1. `is_master()`: tell if the current process is a master process, i.e. convenient wrapper for boilerplate code `rank == 0`\r\n2. `is_local_master()`: tell if the current process is a **local** master process, i.e. convenient wrapper for boilerplate code `local_rank == 0`\r\n3. `broadcast(bytes, src)`: broadcast some message (in the form of `bytes`) from rank `src` to all the processes. The semantic is standard, no need for more explanation.\r\n4. `barrier()`: block until all processes reaches here. Also standard communication primitive.\r\n\r\nNote: very often than not, we want to execute something in just one process per node (e.g. creating directories, downloading files to the node). Inspired by [this thread](https://discuss.pytorch.org/t/torch-distributed-barrier-used-in-multi-node-distributed-data-parallel-training/89711/12), we can write code like this:\r\n\r\n```python\r\nif is_local_master():\r\n    do_something() # download file, create directory, etc.\r\nbarrier()\r\n```\r\n\r\nFurthermore, there are more complicated requirements like  \"only one process in each node does something, but this something is different across nodes\", essentially the requirement of `local_barrier()`, a function that block until all processes **in the current node** reaches here. It is debatable if we want this (currently I don't see any requirements like this in `vllm`.)\r\n\r\n### Communicator interface\r\n\r\nThe following functionality of communicator is suggested (mostly taken from the `nccl` design):\r\n\r\n1. the master process get unique token to identify the communication group\r\n2. the master process broadcast unique token to all ranks\r\n3. each process initializes communication by the unique token and their rank, world_size\r\n4. an in-place allreduce function: `allreduce(char* input, size_t count, size_t dtype, size_t op)`. More functionality would be better (e.g. out-of-place allreduce, broadcast/reduce/scatter etc.), but inplace allreduce is all we need currently.\r\n\r\nThe intended usage would be something like this:\r\n\r\n```python\r\n# inside each process\r\ncoor = Coordinator(); # initialize Coordinator, done by vllm\r\ncomm = Communicator(coor) # hardware vendor can use `coor` to initialize their communicator\r\ndata = torch.tensor((1024, 1024)).to(device=f\"xpu:{coor.local_rank}\")\r\ncomm.allreduce(data) # hardware vendor can access the raw data via pytorch's [`Tensor.data_ptr`](https://pytorch.org/docs/stable/generated/torch.Tensor.data_ptr.html) mechanism.\r\n# please implement Communicator.__del__ to destroy communicator, so that programs can exit gracefully \r\n```\r\n\r\n## A reference implementation of Coordinator\r\n\r\nA reference implementation of Coordinator can be `torch.distributed`, with the `gloo` backend designed to communicate CPU tensors. \r\n\r\nOther considerations include MPI and custom-implemented TCP store. However, since we live in `torch` framework, `torch.distributed` is a natural choice without any new dependency.\r\n\r\nNote: `torch.distributed` can also be used as a fully functional communicator for GPU devices. However, `torch.distributed.all_reduce` is way more complicated than just an allreduce operation. It might initialize autograd engine, might keep track of gradients, might dispatch to different device kernels. Even if we are in [`torch.inference_mode`](https://pytorch.org/docs/stable/generated/torch.inference_mode.html), its `c10` engine might perform some additional operations that fails functionalities like cudagraph. Therefore, I prefer to call vendor-provided communication libraries directly to bypass the problem. After all, we just want an allreduce operation on dense tensors, without any hustle and bustle.\r\n\r\n# Benefits\r\n\r\nAfter we have the above abstraction and interface, we can have the following benefits:\r\n- We are always in a distributed environment, just with different sizes of wold_size. Distributed concerns will always be considered, so that we can easily scale to multi-node environments (if any LLM needs this).\r\n- Hardware vendors can plug in their communication libraries very easily. All they need to provide are: integration into pytorch `torch.Tensor` (only forward computation ops are enough), a c library (an .so file would be enough) for calling communication ops with raw data (i.e. `char*` in c). And if they want to move quickly, just one `allreduce` op would be enough for inference. No need to wait for the whole functionality completed within pytorch.\r\n\r\n# Things not to be considered\r\n\r\nWe don't aim for a fully-fledged distributed execution environment. And since inference tasks are almost stateless, we don't need to consider elasticness and fault-tolerance. As opposed to training, we don't need to save checkpoints, we don't need to resume from previous failure ... ",
    "labels": [
      "RFC",
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-03-23T23:41:40+00:00",
    "closed_at": "2024-06-14T01:00:32+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3587/reactions",
      "total_count": 10,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 10,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/3587"
  },
  {
    "number": 9845,
    "title": "[Misc]: Remove max_tokens field for chat completion requests when not supported anymore by the OpenAI client",
    "body": "With the introduction of the `o1` model series, OpenAI deprecated the `max_tokens` field in favor of the new `max_completion_tokens` field for the [chat completion API](https://platform.openai.com/docs/api-reference/chat/create).\r\n\r\nThis change is active since the [v1.45.0](https://github.com/openai/openai-python/compare/v1.44.1...v1.45.0) version of the OpenAI client.\r\n\r\nhttps://github.com/vllm-project/vllm/pull/9837 added the support for the new `max_completion_tokens` in vLLM while deprecating the `max_tokens` field. However, both fields are supported and cohabit during the deprecation period.\r\n\r\nWhen the OpenAI client definitely drops the `max_tokens` field, this change must also be reflected in the vLLM frontend.\r\n\r\nThis ticket is to keep track of this task. Relevant parts of the code to be updated are commented with `TODO(#9845)`.\r\n\r\n\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-30T16:35:40+00:00",
    "closed_at": "2025-02-28T02:01:57+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9845/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/9845"
  },
  {
    "number": 14128,
    "title": "[Misc]: When using lossy optimization, how to explain that the loss caused by optimization is within the acceptable range?",
    "body": "### Anything you want to discuss about vllm.\n\nI\u2019ve noticed that with each version upgrade of vllm, there seems to be some degree of precision loss. How do you determine whether these losses are within an acceptable range?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-03-03T09:30:10+00:00",
    "closed_at": "2025-07-11T02:16:13+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14128/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14128"
  },
  {
    "number": 7366,
    "title": "[RFC]: Encoder/decoder models & feature compatibility",
    "body": "## Motivation <a href=\"#user-content-motivation\" id=\"motivation\">#</a>\r\n\r\nThere is significant interest in vLLM supporting encoder/decoder models. Issues #187  and #180 , for example, request encoder/decoder model support. As a result encoder/decoder support was recently introduced to vLLM via the following three PRs:\r\n\r\n* #4837 \r\n* #4888 \r\n* #4942 \r\n\r\nThese three PRs make encoder/decoder model inference possible; however, they leave more to be desired in terms of (1) parity between vLLM's decoder-only & encoder/decoder request processing pipelines with respect to feature support, and (2) the number of encoder/decoder models which are supported.\r\n\r\nThe ask for the vLLM community is to contribute PRs which help bring vLLM encoder/decoder functionality to a similar level of maturity as that of vLLM's decoder-only functionality.\r\n\r\n## Proposed changes <a href=\"#user-content-proposed-changes\" id=\"proposed-changes\">#</a>\r\n\r\nThe support matrix below summarizes which encoder/decoder models have already been added & which features are currently compatible with the vLLM encoder/decoder pipeline, versus which features & models will require additional PRs to implement in the long-term:\r\n\r\n<table>\r\n  <tr>\r\n    <th>Model/feature</th>\r\n    <th>Model is already available/feature is already compatible with encoder-decoder?</th>\r\n    <th>Having this model/making this feature compatible is a long-term goal?</th>\r\n  </tr>\r\n  <tr>\r\n    <td>Encoder/decoder infrastructure</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>BART</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Whisper</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>T5</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Other enc/dec models</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Quantization</td>\r\n    <td><strong><u>Untested</u></strong></td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Multimodality</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Attention backends other than Xformers (esp. flash-attn, flashinfer)</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Custom attention bias support</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>CUDAGraph</td>\r\n    <td>No<br>(Issue #7447)</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Pipeline parallelism</td>\r\n    <td>No</td>\r\n    <td><strong><u>Yes</u></strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Speculative decoding</td>\r\n    <td>No</td>\r\n    <td><strong>Low-priority but nice-to-have; difficult.</strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Automatic prefix caching</td>\r\n    <td>No</td>\r\n    <td><strong>Low-priority; difficult.</strong></td>\r\n  </tr>\r\n  <tr>\r\n    <td>Sliding window</td>\r\n    <td>No</td>\r\n    <td>No</td>\r\n  </tr>\r\n  <tr>\r\n    <td>Chunked prefill</td>\r\n    <td>No</td>\r\n    <td>No</td>\r\n  </tr>\r\n  <tr>\r\n    <td>LoRA</td>\r\n    <td>No</td>\r\n    <td>No</td>\r\n  </tr>\r\n</table>\r\n\r\nThis RFC gives an overview of those features & models which **are not compatible with encoder/decoder currently, but which should be made compatible eventually** (i.e. **No** in the second column, **Yes** in the third column in the support matrix.)\r\n\r\nNote that there are features (automatic prefix caching/sliding window/chunked prefill/LoRA) which are not long-term compatibility goals.\r\n\r\n## Background <a href=\"#user-content-background\" id=\"background\">#</a>\r\n\r\nBefore continuing, it will be helpful to review [the details of the new vLLM encoder/decoder infrastructure](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/infra-enc-dec.md). \r\n\r\nIt will also be helpful to review [this how-to guide](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/how-to.md) for adding new encoder/decoder models & improving encoder/decoder feature compatibility.\r\n\r\n## Initial goal <a href=\"#user-content-initial-goal\" id=\"initial-goal\">#</a>\r\n\r\nMembers of the vLLM contributor community identify models/features in the support matrix above, for which they will work on writing a PR.\r\n\r\n## Detailed long-term goals <a href=\"#user-content-detailed-long-term-goals\" id=\"detailed-long-term-goals\">#</a>\r\n\r\n### Add new models to vLLM <a href=\"#user-content-add-new-models-to-vllm\" id=\"add-new-models-to-vllm\">#</a>\r\n\r\nPlease review the [how-to guide for adding new models to vLLM](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/how-to.md#guide-to-adding-new-encoderdecoder-models-to-vllm)\r\n\r\nSee `tests/models/test_bart.py` for an example of an encoder/decoder model unit test. See `tests/distributed/test_basic_distributed_correctness_enc_dec.py` for an example of an encoder/decoder model test with TP > 1.\r\n\r\n#### Add Whisper model <a href=\"#user-content-add-whisper-model\" id=\"add-whisper-model\">#</a>\r\n\r\nSteps to add support for [Whisper](https://paperswithcode.com/paper/robust-speech-recognition-via-large-scale-1f), a multimodal encoder/decoder speech recognition model:\r\n* [Extend existing vLLM multimodality support to encoder/decoder models](#support-encoderdecoder-multimodality)\r\n* Extend existing vLLM prompt processing pipeline to support audio\r\n* Port [HuggingFace Whisper model](https://github.com/huggingface/transformers/blob/main/src/transformers/models/whisper/modeling_whisper.py) to vLLM; an existing open PR for this workstream is #5964 \r\n* Modify each Whisper layer, where appropriate, to support TP > 1\r\n* Add a Whisper test under `tests/models/`\r\n\r\nProposal: consider whether or not it makes sense to implement encoder/decoder multimodality, audio support, and Whisper in the same PR; that way, the Whisper model may be used to facilitate an end-to-end test with of audio multimodality.\r\n\r\n#### Add T5 model <a href=\"#user-content-add-t5-model\" id=\"add-t5-model\">#</a>\r\n\r\nNote: T5 depends on [custom attention bias being supported](#support-custom-attention-bias) by at least one of the attention backends which [also supports encoder attention & cross-attention](#add-support-for-encoder-attention-and-cross-attention-to-additional-backends); at time of writing no vLLM attention backend fulfills this requirement. The vLLM XFormers attention backend is the only backend which supports encoder/decoder models but neither it nor any other vLLM attention backend supports custom attention bias. (Custom attention bias is required in order to support T5 [relative positional encoding.](#custom-attention-bias-and-relative-positional-encoding))\r\n\r\nSteps to add support for the [T5 model](https://paperswithcode.com/method/t5):\r\n* Port [HuggingFace T5 model](https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py) to vLLM\r\n  * This includes porting over the method which computes the custom attention bias matrix for T5 relative position encoding\r\n* Modify each T5 layer, where appropriate, to support TP > 1\r\n  * The custom attention bias computation must also support TP > 1\r\n* Add a T5 test to `tests/models/`\r\n\r\nNote: T5 was added to an older version of vLLM in #3117 , which could be a helpful starting-point\r\n\r\n#### Add other encoder/decoder models\r\n\r\n* Review open vLLM issues on GitHub and identify other encoder/decoder models which are requested by users\r\n\r\n### Quantization <a href=\"#user-content-quantization\" id=\"quantization\">#</a>\r\n\r\nThe goal of this workstream is to make sure that quantization + encoder/decoder models is fully-tested, and to fill in any gaps (should they exist) in vLLM's support for quantized encoder/decoder models.\r\n\r\nSteps to ensure that vLLM supports encoder/decoder models in combination with all existing vLLM quantization methods:\r\n\r\n* Identify the list of quantization methods which vLLM currently supports with decoder-only models.\r\n* Add unit tests for encoder/decoder models with all of these quantization methods.\r\n* Determine which quantization methods are currently incompatible with vLLM encoder/decoder infrastructure.\r\n* Scope out the effort involved in making these quantization methods compatible & submit a PR making the change.\r\n\r\nvLLM encoder/decoder infrastructure should be compatible with most of the existing vLLM quantization methods, because the specialized quantization kernels are only employed for GEMM operations involving the learned weight matrices ($W_q$, $W_k$, etc.), whereas the encoder/decoder work really only modifies how the `Attention(q, k, v, kv_cache)` layer behaves & does not impact the learned weight matrices at all.\r\n\r\nIt is less clear whether vLLM encoder/decoder infrastructure would be incompatible with FP8. It does appear that a specialized quantized KV cache kernel is employed by the `Attention(q, k, v, kv_cache)` layer when FP8 quantization is employed.\r\n\r\n### Support encoder/decoder multimodality <a href=\"#user-content-support-encoderdecoder-multimodality\" id=\"support-encoderdecoder-multimodality\">#</a>\r\n\r\nTechnically, vLLM already supports multimodality for models which have an \"encoder\" and a \"decoder\", i.e. Llava. However, Llava's decoder does not utilize cross-attention & the model is basically compatible with vLLM's pre-existing decoder-only infrastructure.\r\n\r\nBut critically, for **encoder/decoder models with cross-attention** such as Whisper vLLM does not currently support multimodality of any sort. The processing pipeline does not extract or utilize multimodal data from the input prompt, and the `EncoderDecoderModelRunner` has an assert which fails if the multimodal config is not `None`. Addressing this is what is meant by \"supporting encoder/decoder multimodality\".\r\n\r\nSteps to extend existing vLLM multimodality support to encoder/decoder models:\r\n* Review [existing vLLM multimodality support in the decoder-only pipeline](https://docs.vllm.ai/en/latest/dev/multimodal/multimodal_index.html)\r\n* Scope out a plan for adding encoder/decoder multimodality support.\r\n* Propose & implement one or more multimodal prompt formats for encoder/decoder models\r\n* Integrate multimodality support into encoder/decoder processing pipeline\r\n* Remove the assertion which fails when multimodality is enabled for an encoder/decoder model (see `assert_enc_dec_mr_supported_scenario()` in `vllm/worker/utils.py`)\r\n* Add one or more unit tests with multimodal data\r\n\r\nThere are a number of multimodal encoder/decoder models which will benefit from this feature. One possibility is to add multimodality support & a multimodal model such as [Whisper](#add-whisper-model) in the same PR, so that Whisper may be used to facilitate an end-to-end test with multimodality.\r\n\r\nAnother possibility is to implement multimodality support in its own PR.\r\n\r\n#### Considerations for designing multimodal encoder/decoder prompt formats <a href=\"#user-content-considerations-for-designing-multimodal-encoderdecoder-prompt-formats\" id=\"considerations-for-designing-multimodal-encoderdecoder-prompt-formats\">#</a>\r\n\r\nOne approach to designing the vLLM multimodal encoder/decoder prompt formats, is to consider what we want the user experience to be for high-priority multimodal encoder/decoder models such as\r\n* [Llama 3.1 multimodal](https://github.com/vllm-project/vllm/pull/7258#discussion_r1710915145)\r\n* [Whisper](#add-whisper-model)\r\n\r\n#### Initial proposal for multimodal encoder/decoder prompt formats\r\n\r\nIt may be helpful to review\r\n* [The non-multimodal encoder/decoder prompt formats which are currently supported by vLLM](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/infra-enc-dec.md#supported-encoderdecoder-prompt-formats): singleton prompts (raw text prompt, `TextPrompt`, `TokensPrompt`) as well as `ExplicitEncoderDecoder` prompts\r\n* The multimodal decoder-only prompt formats which are currently supported by vLLM; search for `multi_modal_data` [here](https://github.com/vllm-project/vllm/blob/main/vllm/inputs/data.py) and also review the [vLLM documentation on multimodality](https://docs.vllm.ai/en/latest/dev/multimodal/multimodal_index.html)\r\n\r\nGenerally speaking, in encoder/decoder models based on cross-attention, the non-text input modality is passed to the encoder as input. Conversely, any text prompt is typically passed to the decoder as a input prompt.\r\n\r\nThe following two encoder/decoder multimodal prompt formats are tentatively proposed:\r\n\r\n* Singleton `TextPrompt` with `multi_modal_data` field\r\n    * vLLM will extract the `multi_modal_data` and pass it to the encoder module\r\n    * vLLM will extract the prompt text, tokenize it and pass the token-list to the *decoder* (note that this is the opposite of vLLM behavior for non-multimodal prompts, where the prompt text would be passed to the encoder.)\r\n\r\n    For example passing the `TextPrompt` below to vLLM BART\r\n\r\n    ```\r\n    TextPrompt(\r\n        'prompt': \"The rain in spain falls mainly on the\",\r\n        'multi_modal_data': <multi modal data structure>\r\n    )\r\n    ```\r\n\r\n    results in\r\n\r\n    ```\r\n    Encoder input: <multi modal data structure>\r\n    Decoder prompt: \"The rain in spain falls mainly on the\"\r\n    ```\r\n\r\n* Singleton `TokensPrompt` with `multi_modal_data` field\r\n    * vLLM will extract the `multi_modal_data` and pass it to the encoder module\r\n    * vLLM will extract the token list and pass it unmodified to the *decoder* (note that this is the opposite of vLLM behavior for non-multimodal prompts, where the prompt tokens would be passed to the encoder.)\r\n\r\n    For example passing the `TokensPrompt` below to vLLM BART\r\n\r\n    ```\r\n    TokensPrompt(\r\n        'prompt_tokens': [2,0,171,5,2],\r\n        'multi_modal_data': <multi modal data structure>\r\n    )\r\n    ```\r\n\r\n    results in\r\n\r\n    ```\r\n    Encoder prompt: <multi modal data structure>\r\n    Decoder prompt: [2,0,171,5,2]\r\n    ```\r\n\r\nIt may also be worth considering whether or how to support\r\n* `ExplicitEncoderDecoderPrompt`s with multimodality\r\n* An input prompt format which encapsulates *only* multimodal encoder inputs, with no associated decoder text/tokens prompt (this would result in the decoder being passed a \"default\" or empty prompt.)\r\n\r\n### Add support for encoder attention and cross-attention to additional backends <a href=\"#user-content-add-support-for-encoder-attention-and-cross-attention-to-additional-backends\" id=\"add-support-for-encoder-attention-and-cross-attention-to-additional-backends\">#</a>\r\n\r\nAt time of writing, XFormers is the only vLLM attention backend which supports encoder attention & cross-attention. \r\n\r\nThe goal of this workstream would be to extend encoder attention & cross-attention support to additional backends, the highest-priority being flash-attention and flashinfer.\r\n\r\nReviewing [encoder attention and cross-attention support in the XFormers backend would be a good starting-point](https://github.com/vllm-project/vllm/blob/main/vllm/attention/backends/xformers.py) for extending support to other models.\r\n\r\nFor context on the requirements for a backend to support encoder and cross-attention, it may help to review the [encoder/decoder architecture](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/infra-enc-dec.md#encoderdecoder-architecture-diagram-prefill--and-decode-phase), the [way that attention masks are currently constructed in the XFormers backend](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/infra-enc-dec.md#default-encoderdecoder-attention-bias-or-mask), and the [recommended architecture for vLLM encoder/decoder models](https://github.com/afeldman-nm/vllm-enc-dec-rfc/blob/main/how-to.md#25-optional-but-strongly-recommended-implement-the-following-encoderdecoder-model-architecture).\r\n\r\nA summary of the key changes required for an attention backend to support encoder attention and cross-attention:\r\n* The backend's `AttentionMetadata` subclass must support fields for encoder sequence lengths, encoder sequence token count, cross-attention blocktables, and cross-attention slot mapping. XFormers examples:\r\n  * [`AttentionMetadata` subclass' encoder field declarations](https://github.com/vllm-project/vllm/blob/67abdbb42fdbb59c274130368981c0d0ac3539e3/vllm/attention/backends/xformers.py#L127-L140)\r\n  * [Handle encoder & cross-attention fields in `prefill_metadata()` method](https://github.com/vllm-project/vllm/blob/67abdbb42fdbb59c274130368981c0d0ac3539e3/vllm/attention/backends/xformers.py#L216-L221)\r\n  * [Handle encoder & cross-attention fields in `decode_metadata()` method](https://github.com/vllm-project/vllm/blob/67abdbb42fdbb59c274130368981c0d0ac3539e3/vllm/attention/backends/xformers.py#L255-L260)\r\n* The `forward()` method of the backend implementation must accept an `attn_type` argument of type `AttentionType`, which allows choosing between encoder attention, decoder attention, or encoder/decoder cross-attention. [XFormers example](https://github.com/vllm-project/vllm/blob/67abdbb42fdbb59c274130368981c0d0ac3539e3/vllm/attention/backends/xformers.py#L447)\r\n* The backend implementation must recognize which option has been chosen for `attn_type`, and adjust accordingly in terms of (1) how it utilizes `attn_metadata` when invoking the attention kernels (review [XFormers `forward()`](https://github.com/vllm-project/vllm/blob/67abdbb42fdbb59c274130368981c0d0ac3539e3/vllm/attention/backends/xformers.py#L438) for context), and (2) the choice of causal or non-causal attention, as well the choice of attention mask shape ([XFormers example](https://github.com/vllm-project/vllm/blob/67abdbb42fdbb59c274130368981c0d0ac3539e3/vllm/attention/backends/xformers.py#L689-L714)).\r\n\r\n#### Initial goals\r\n* Identify the changes required to add encoder attention & cross-attention support to flash-attention and flashinfer\r\n* PR the required changes\r\n  * Remove/modify any asserts which fail if the vLLM attention backend is not XFormers\r\n    * Currently, [the `__init__()` method of `EncoderDecoderModelRunner`](https://github.com/vllm-project/vllm/blob/b4e9528f9569d6eb8c29624771a4058fe794cb5a/vllm/worker/enc_dec_model_runner.py#L95) invokes a method `EncoderDecoderModelRunner._maybe_force_supported_attention_backend()` defined [here](https://github.com/vllm-project/vllm/blob/b4e9528f9569d6eb8c29624771a4058fe794cb5a/vllm/worker/enc_dec_model_runner.py#L112-L144) which (1) attempts to force encoder/decoder models to use XFormers attention backend, and (2) raises an exception if the user has overridden the attention backend to be anything other than XFormers. \r\n\r\n#### Long-term goals\r\n* All vLLM attention backends support encoder attention and cross-attention\r\n\r\n### Support custom attention bias <a href=\"#user-content-support-custom-attention-bias\" id=\"support-custom-attention-bias\">#</a>\r\n\r\nNote: [T5](#add-t5-model) takes a dependency on custom attention bias. Custom attention bias is likely complex enough to merit its own PR.\r\n\r\nNote: custom bias support was added to `PagedAttention` in an older version of vLLM as part of #3117 ; given changes in vLLM since then, additional work would be required to integrate this implementation.\r\n\r\n#### Custom attention bias and relative positional encoding\r\n\r\nAttention bias refers to adding a matrix $A$ to the scaled dot-product (SDP) attention scores matrix before performing softmax, as shown below:\r\n\r\n$$\r\nattn(Q,K,V,A) = softmax(\\frac{Q K^T + A}{\\sqrt{d}})V\r\n$$\r\n\r\nHere, *custom* attention bias is understood to mean that the vLLM attention backend allows $A$ to be an arbitrary matrix, provided the tensor dimensions are commensurate with the shape of the SDP attention scores matrix. This is in contrast to the existing vLLM attention backend implementations, which can only accommodate simple block-diagonal causal or non-causal masks which are uniformly either $0$ or $-\\infty$. \r\n\r\nThere are broadly two possible approaches to custom attention bias, which do not necessarily have to be mutually-exclusive:\r\n* $A$ is a fully-materialized attention bias matrix passed to the attention backend\r\n* $A$ is computed on-the-fly by the attention kernel, using an element-wise formula for the attention bias which is fused with the $Q K^T$ and $softmax$ computations\r\n\r\nT5 employs custom attention bias in order to implement [relative positional encoding](https://jaketae.github.io/study/relative-positional-encoding/#bridging-shaw-and-huang), wherein pairwise positional relationships between tokens are represented by the bias matrix. The HuggingFace Transformers T5 implementation provides an example of [how the relative positional encoding matrix is computed](https://github.com/huggingface/transformers/blob/c1aa0edb48217f416f4bbe6e3a9db1500284513b/src/transformers/models/t5/modeling_t5.py#L428).\r\n\r\n#### Existing attention bias support\r\n\r\n*Currently, no vLLM attention backend fully supports passing in a custom attention bias*. This is primarily due to underlying kernel limitations. For example, the [xFormers `memory_efficient_attention_forward` kernel](https://facebookresearch.github.io/xformers/components/ops.html) is the only NVIDIA-GPU-oriented kernel which permits passing in an arbitrary PyTorch tensor as a materialized attention bias (via the `attn_bias` argument) (at time of writing I have not investigated if custom attention bias is supported by any of the kernels for AMD GPU, CPU, etc.) Regardless, vLLM only employs xFormers `memory_efficient_attention_forward` for prefill; to my knowledge, none of the decode-phase kernels employed by vLLM can accept an arbitrary tensor as a custom attention bias, making custom attention bias impossible to apply end-to-end for both prefill and decode under the current vLLM implementation.\r\n\r\nIn addition to lack of kernel-level support for custom attention bias, most vLLM backends also prevent passing a custom attention bias matrix to the underlying kernel. The exception is the XFormers backend, which accepts an attention bias via `XFormersMetadata.attn_bias` attribute (however the XFormers backend only utilizes `attn_bias` in the prefill phase.)\r\n\r\n#### Proposed methods for supporting custom attention bias\r\n\r\nHere the following two approaches for supporting custom attention bias in vLLM are proposed:\r\n* **Fully-materialized bias matrix:** Modify vLLM attention backends to accept an arbitrary PyTorch tensor, passed into the backend via the `AttentionMetadata.attn_bias` field.\r\n* **On-the-fly/fused bias matrix computation:** Enable an efficient workflow whereby vLLM developers can tweak an attention kernel to compute the custom attention bias on the fly\r\n  * For example: rather than computing the T5 relative position encoder bias matrix once, instead the attention kernel can fuse the element-wise bias matrix formula with the $Q K^T$ and $softmax()$. The attention bias matrix is never fully materialized.\r\n  * [FlexAttention](https://pytorch.org/blog/flexattention/#relative-position-encodings) enables fused custom attention bias computations in a FlashAttention-style kernel, using torch.compile.\r\n\r\n![image](https://pytorch.org/assets/images/flexattention/fg4.png)\r\n\r\nIt may make sense to support one or both of these methods.\r\n\r\nNote that custom attention bias support must be added on a backend-by-backend basis, because of the kernel modifications & backend logic changes required.\r\n\r\n#### Initial goals for introducing custom attention bias support\r\n\r\n1. Focus on a particular vLLM attention backend\r\n  * Suggestion: focus on an attention backend which also supports encoder/decoder models, in order to facilitate [running T5](#add-t5-model). At time of writing, XFormers is the only backend which supports encoder/decoder models, however there will likely be work on [supporting encoder/decoder in additional attention backends.](#add-support-for-encoder-attention-and-cross-attention-to-additional-backends)\r\n2. Scope out the effort involved in introducing custom attention bias support to this backend\r\n3. Some steps which will likely be involved in introducing custom attention bias support:\r\n  * Augment attention backend's kernels to accept custom attention bias; for example, the PagedAttention kernel (for XFormers backend), the Flash-attention kernel (for the flash-attn backend), or the Flashinfer kernels (for the Flashinfer backend)\r\n  * (Except for XFormers) add an `attn_bias` attribute to attention backend's `AttentionMetadata` subclass\r\n  * Ensure that the attention backend passes the `attn_bias` attribute to both the prefill and decode kernels\r\n4. Add at least two custom attention bias unit tests (for prefill & decode respectively)\r\n\r\n#### Final goals for introducing custom attention bias support\r\n\r\n* All vLLM attention backends should support custom attention bias, with unit tests\r\n\r\n#### Some links which may be helpful for understanding how causal & non-causal attention masks are currently configured in vLLM:\r\n\r\n* [Invocation of flash-attention for prefill in vLLM backend, using `causal` flag](https://github.com/vllm-project/vllm/blob/db35186391a2abfc6c91d703527dac20d2488107/vllm/attention/backends/flash_attn.py#L522)\r\n\r\n* [Invocation of xFormers attention kernel for prefill in vLLM backend, using `BlockDiagonalMask` and `BlockDiagonalCausalMask`](https://github.com/vllm-project/vllm/blob/db35186391a2abfc6c91d703527dac20d2488107/vllm/attention/backends/xformers.py#L689-L738)\r\n\r\n* [Invocation of FlashInfer attention kernel for prefill in backend, using `causal` flag](https://github.com/vllm-project/vllm/blob/db35186391a2abfc6c91d703527dac20d2488107/vllm/attention/backends/flashinfer.py#L539)\r\n\r\n* [Invocation of PagedAttention kernel for decode in vLLM backend](https://github.com/vllm-project/vllm/blob/db35186391a2abfc6c91d703527dac20d2488107/vllm/attention/backends/xformers.py#L628)\r\n\r\n* [Invocation of FlashInfer kernel for decode in vLLM backend](https://github.com/vllm-project/vllm/blob/db35186391a2abfc6c91d703527dac20d2488107/vllm/attention/backends/flashinfer.py#L543)\r\n\r\n### Support CUDAGraph with encoder/decoder models <a href=\"#user-content-support-cudagraph-with-encoderdecoder-models\" id=\"support-cudagraph-with-encoderdecoder-models\">#</a>\r\n\r\nNote: this topic is being tracked by Issue #7447\r\n\r\nSteps to support CUDAGraph with encoder/decoder models:\r\n* Scope out the effort require to support CUDAGraph with encoder/decoder models\r\n* Write a PR for CUDAGraph + encoder/decoder\r\n  * Remove the assertion which fails when CUDAGraph is enabled for an encoder/decoder model (see `assert_enc_dec_mr_supported_scenario()` in `vllm/worker/utils.py`)\r\n\r\n### Support pipeline-parallelism with encoder/decoder models <a href=\"#user-content-support-pipeline-parallelism-with-encoderdecoder-models\" id=\"support-pipeline-parallelism-with-encoderdecoder-models\">#</a>\r\n\r\nSteps to support pipeline-parallelism with encoder/decoder models:\r\n* Scope out the effort required to support pipeline-parallelism with encoder/decoder models\r\n* Write a PR for pipeline-parallelism + encoder/decoder\r\n  * Remove the assertion which fails when pipeline-parallelism is enabled for an encoder/decoder model (see `assert_enc_dec_mr_supported_scenario()` in `vllm/worker/utils.py`)\r\n\r\n### Support multi-step scheduling with encoder/decoder models <a href=\"#user-content-support-multi-step-scheduling-with-encoder-decoder-models\" id=\"support-multi-step-scheduling-with-encoder-decoder-models\">#</a>\r\n\r\nNote: depends on #7000 landing in order to add multi-step scheduling support; it may be helpful to review the multi-step scheduling RFC ( #6854 )\r\n\r\nSteps to support multi-step scheduling with encoder/decoder models:\r\n* Scope out the effort required to support multi-step scheduling\r\n  * `EncoderDecoderModelRunner` multi-step support\r\n* Write a PR for multi-step scheduling + encoder/decoder\r\n* Write at least one test of an encoder/decoder model with multi-step scheduling\r\n\r\n### Low-priority high-effort tasks <a href=\"#user-content-low-priority-high-effort-tasks\" id=\"low-priority-high-effort-tasks\">#</a>\r\n\r\n* Speculative decoding\r\n* Automatic prefix caching\r\n\r\nHere it is proposed that these features are low-priority. Adding support for speculative decoder and automatic prefix caching would require a significant of effort to scope out and design the implementations.\r\n\r\nNote that adding support for either of these features would require removing the assertions which fail when speculative decoding or automatic prefix caching are enabled for an encoder/decoder model (see `assert_enc_dec_mr_supported_scenario()` in `vllm/worker/utils.py`)\r\n\r\n### Feedback Period.\r\n\r\nClosed.\r\n\r\n### CC List.\r\n\r\n@WoosukKwon \r\n@robertgshaw2-neuralmagic \r\n@mgoin \r\n@tms\r\n@njhill \r\n@sroy745 \r\n@ywang96 \r\n@DarkLight1337 \r\n@js8544 \r\n\r\n### Any Other Things.\r\n\r\n_No response_",
    "labels": [
      "RFC",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-08-09T15:03:54+00:00",
    "closed_at": null,
    "comments": 17,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7366/reactions",
      "total_count": 14,
      "+1": 14,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7366"
  },
  {
    "number": 16268,
    "title": "[RFC]: TPU V1 Sampler planning",
    "body": "### Motivation.\n\nI'd like to gather some input on how to move forward with sampling support, and also provide a brief recap of the current state+planned support.\n\nAt a high level, the current design splits model forward and sampling into two separate graphs. \nAs of now (`f2ebb6f54`) only the `temperature` and `min_p` have been intentionally enabled. \nAs more techniques will be added, the sampling graph will grow in size (vertically, sequential ops) and performance may need monitoring, as we're simply evaluating more operations at runtime. \nTo clarify, even when one option is not enabled, we still evaluate a no-op version that undergoes the same ops in the graph (eg top-p with p=1).\n\n### Proposed Change.\n\nFollowing https://github.com/vllm-project/vllm/pull/15489 a few concerns that have been raised regarding performance while enabling topk,  hence adding the **very first** op to the initial sampling graph, I'd like to re-evaluate the current approach.\nLooking at the opposite side of the spectrum one could ideally provide a sampling graph for each combination of parameters. \nWhile this is unfeasible due to the number of parameters that sampling needs to support, one approach \"in the middle\" includes pre-compiling a set of common sampling params while routing requests to the \"correct\" one.\nThe main issue I see here is batching, as every request may potentially specify different sampling params, either we identify the superset for the current batch and then route to the corresponding graph, or each request is executed on a separate graph, which I believe would hurt performance even more. With that said, I still think most request will fall into the temperature only \"bucket\", followed by the topk/topp one, so one could implement the most popular routes. I have no production data to back this assertion with though, so don't quote me on that.\n\nI think this is the main point to clarify before moving on and expand the number of supported parameters.\n\nPlease note the above is based on the assumption that latency is indeed going up. To clear out any such doubts, I think the PR https://github.com/vllm-project/vllm/pull/16022 will go a long way to allow easy benchmarking of sampling parameters.\n\n_____\nMoving forward, I've compiled a list of parameters to support along with the effort needed to implement + my own suggestions. \nWe can also use it to track progress.\n - [x] temperature/min_p\n - [x] topk/topp we already have an implementation for https://github.com/vllm-project/vllm/pull/15489 (topk). \n - [x] logprobs/`sampling_metadata.max_num_logprobs`: Similarly to what we do in other parts, we need to compile for different `max_num_logprobs` values as the output is Bx`max_num_logprobs` (+there's a `torch.topk` call), unless we fix it to some arbitrary value.  https://github.com/vllm-project/vllm/pull/17072\n - [ ] `sampling_metadata.prompt_token_ids` for penalties. This should be fine as is given we're already compiling for different (padded) input sizes. Just it can't be optional or None/with value count as different inputs.\n - [ ] `sampling_metadata.output_token_ids` for penalties. It's already converted into a padded tensor.\n - [ ] penalties:\n \t- `get_token_bin_counts_and_mask` uses a scatter_add op that *may* be slow on TPU\n \t- *penalties tensors are already of shape `num_seqs`, which we pre-compile so they're fine.\n \t- there are multiple lines where the a tensor is silced with in value-dependent way (recompilation risk) `logits[logits > 0]`. We can probably replace with a `masked_fill`. \n- [ ] `sampling_metadata.min_tokens` penalty must be re-implemented and vectorized (it uses a for on input dict now, graph would be dynamic). I am less familiar with this implementation so tbd.\n- [ ] `sampling_metadata.logit_bias`, current interface needs to be rethought because it can introduce dynamism. We could create a BxV\n matrix (B padded+precompiled) to pack the preferences from the list[dicts]. This would work but obviously the factor of expansion can be quite\n big (eg downgrading a single token would materialize a who BxV matrix). Alternatively we could provide different pre-compiled values for V (2, 4..) \n at the cost of increased complexity and longer compilation time. Also, current cuda impl is highly unoptimized.\n- [ ]  `sampling_metadata.allowed_token_ids_mask` is fine as is, no effort required imo. Just it can't be None on a single graph.\n- [ ]  `sampling_metadata.bad_words_token_ids` probably better to support the more general `logit_bias` option.\n\n### Feedback Period.\n\n_No response_\n\n### CC List.\n\n@robertgshaw2-redhat @yaochengji @alexm-redhat @mgoin @bvrockwell @hyeygit @lsy323 \n\n### Any Other Things.\n\nUPDATE: this is very much related to https://github.com/vllm-project/vllm/pull/13360.\n \n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "open",
    "created_at": "2025-04-08T14:24:38+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16268/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16268"
  },
  {
    "number": 13361,
    "title": "[RFC]: Deprecation of the `best_of` Sampling Parameter in vLLM V1",
    "body": "### Motivation.\n\n### Overview\nAs we transition to vLLM V1, we plan to discontinue support for the `best_of` sampling parameter. This decision is driven by a combination of low usage, alignment with industry trends, and a desire for system simplicity and performance.\n\n### Background: What is `best_of`?\nThe `best_of` parameter was originally part of the earlier OpenAI completion API. It enabled the generation of multiple completions\u2014`n` different outputs\u2014then selected the \u201cbest\u201d completion based on the cumulative log probabilities of each result.\n\n### Reasons for Deprecation\n\n1. **Limited Usage and Industry Trends:**\n   - **Low Adoption:** To the best of our knowledge, the `best_of` feature is used by very few users. Users have observed that output quality isn\u2019t reliably correlated with their log probabilities in most cases.\n   - **Evolving Standards:** Major AI providers such as OpenAI (in its current API), Claude, and Gemini have moved away from including the `best_of` option.\n\n2. **Alternative Methods:**\n   - Users can implement `best_of` by leveraging the `n` parameter to obtain multiple completions and the `logprobs` parameter for the log probability of each generated token. This method effectively replicates the behavior of `best_of` without requiring dedicated support.\n\n3. **System Simplification and Performance:**\n   - Supporting `best_of` introduces additional complexity, as it necessitates tracking cumulative log probabilities for each generated completion. This extra overhead runs counter to our focus on performance and streamlined design.\n\n### Proposed Change.\n\nIn light of the minimal usage, the availability of alternative methods, and our commitment to a simpler, more efficient system, we plan to phasing out the `best_of` parameter with vLLM V1. Users who wish to mimic its functionality can continue to do so by generating multiple completions and comparing their log probabilities directly.\n\nPlease let us know if this change impacts your usage or if you have any other concerns.\n\n### Feedback Period.\n\n2 weeks.\n\n### CC List.\n\n_No response_\n\n### Any Other Things.\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "RFC"
    ],
    "state": "closed",
    "created_at": "2025-02-16T17:57:03+00:00",
    "closed_at": "2025-03-05T20:22:45+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13361/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 1
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/13361"
  },
  {
    "number": 9190,
    "title": "[Performance]: phi 3.5 vision model consuming high CPU RAM and the process getting killed",
    "body": "### Proposal to improve performance\n\nI am trying to run phi3.5 vision instruct model with around 10k prompts. What I noticed with the increase in prompts my CPU RAM consumption keeps increasing and eventually the process gets killed. Its running fine for say small sample like 1000 prompts. My system configuration is 48 GB VRAM and 64GB CPU RAM. Noticed a similar pattern with PIXTRAL-12B-2409. Has anyone faced this issue?\r\n\r\nI have tried the implementation by passing in batches of 1000 to llm.generate but still the CPU RAM keeps increasing\r\nBelow is the code implementation:\r\nIma using two images per prompt\r\nfrom vllm import LLM, SamplingParams\r\nllm = LLM(\r\n        model=\"microsoft/Phi-3.5-vision-instruct\",\r\n        gpu_memory_utilization=0.7,\r\n        trust_remote_code=True,\r\n        max_model_len=4096,\r\n        limit_mm_per_prompt={\"image\": 4},\r\n    )\r\nsampling_params = SamplingParams(max_tokens=100, temperature=0.0)\r\noutputs = llm.generate(prompt_list, sampling_params=sampling_params)\n\n### Report of performance regression\n\n_No response_\n\n### Misc discussion on performance\n\n_No response_\n\n### Your current environment (if you think it is necessary)\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "performance",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-10-09T12:19:09+00:00",
    "closed_at": null,
    "comments": 37,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9190/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9190"
  },
  {
    "number": 2871,
    "title": "HQQ quantization support",
    "body": "As we have a few models with Half-Quadratic Quantization (HQQ) out there, VLLM should also support them:\r\n\r\n```sh\r\napi_server.py: error: argument --quantization/-q: invalid choice: 'hqq' (choose from 'awq', 'gptq', 'squeezellm', None)\r\n```\r\n\r\nE.g.\r\n* https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-HQQ",
    "labels": [
      "unstale"
    ],
    "state": "closed",
    "created_at": "2024-02-14T15:50:49+00:00",
    "closed_at": "2025-01-14T13:32:35+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2871/reactions",
      "total_count": 18,
      "+1": 18,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2871"
  },
  {
    "number": 8439,
    "title": "[Usage]:  why speculate decoding is slower than normal decoding\uff1f",
    "body": "### Your current environment\n\nThe startup command is as follows: it initiates both a standard 7B model and an n-gram speculate model. Speed tests  discover that the speculate model performs more slowly.\"\r\n```text\r\nCUDA_VISIBLE_DEVICES=0 python -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 9000 --model Qwen2-7B-Instruct -tp 1 --gpu_memory_utilization 0.9\r\n\r\nCUDA_VISIBLE_DEVICES=3 python -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 9002 --model Qwen2-7B-Instruct -tp 1 --speculative_model [gram] --use-v2-block-manager --num_speculative_tokens 5 --ngram-prompt-lookup-max 4 --gpu_memory_utilization 0.9\r\n\r\nresult\r\n7b:\r\nfirst token:  0.04074668884277344s\r\ndecode time:  14.328832149505615s\r\noutput token:  1000\r\ndecode speed:  69.78935823702163 token/s\r\n\r\nspec 7b\r\nfirst token:  0.02350592613220215s\r\ndecode time:  15.324904918670654s\r\noutput token:  947\r\ndecode speed:  61.794836902788866 token/s\r\n```\r\n\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "unstale"
    ],
    "state": "open",
    "created_at": "2024-09-13T03:43:26+00:00",
    "closed_at": null,
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8439/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8439"
  },
  {
    "number": 1555,
    "title": "Guidance on how many requests can be processed at a time?",
    "body": "Hello - \r\n\r\nI am trying to understand how many requests can be processed in parallel with the llm_engine, and what keeps requests WAITING. I see various variables like \"max_num_batched_tokens\" and \"max_num_seqs\", but more details or documentation describing how this process occurs would be helpful. Moreover, how can we tune our system to do process more requests in parallel (e.g. use more GPUs if available, use smaller models, use smaller context windows, etc.)",
    "labels": [
      "documentation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-11-03T19:27:46+00:00",
    "closed_at": "2024-12-01T02:15:58+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1555/reactions",
      "total_count": 14,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 8
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1555"
  },
  {
    "number": 13021,
    "title": "[Doc]: No max_model_len parameter in the LLM class",
    "body": "### \ud83d\udcda The doc issue\n\nIn this url: https://docs.vllm.ai/en/latest/serving/offline_inference.html\nI see that there is no max_model_len parameter in the LLM class, but the documentation still says \n    llm = LLM(model=\"adept/fuyu-8b\",\n    max_model_len=2048,\n    max_num_seqs=2)\nBtw, I wonder how can I change the max_seq_len when I use offline_inference? \n\n### Suggest a potential alternative/fix\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2025-02-10T08:23:08+00:00",
    "closed_at": "2025-02-10T16:16:36+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13021/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13021"
  },
  {
    "number": 65,
    "title": "Add documents on how to add new models",
    "body": null,
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2023-05-04T09:05:56+00:00",
    "closed_at": "2023-06-06T03:01:28+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/65/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/65"
  },
  {
    "number": 8947,
    "title": "[Bug]: vllm serve --config.yaml - Order of arguments matters?",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Rocky Linux release 8.10 (Green Obsidian) (x86_64)\r\nGCC version: (GCC) 11.3.0\r\nClang version: Could not collect\r\nCMake version: version 3.26.5\r\nLibc version: glibc-2.28\r\n\r\nPython version: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-4.18.0-553.16.1.el8_10.x86_64-x86_64-with-glibc2.28\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H100\r\nGPU 1: NVIDIA H100\r\nGPU 2: NVIDIA H100\r\n  MIG 1g.12gb     Device  0:\r\n  MIG 1g.12gb     Device  1:\r\n  MIG 1g.12gb     Device  2:\r\n  MIG 1g.12gb     Device  3:\r\n  MIG 1g.12gb     Device  4:\r\n  MIG 1g.12gb     Device  5:\r\n  MIG 1g.12gb     Device  6:\r\nGPU 3: NVIDIA H100\r\n\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              96\r\nOn-line CPU(s) list: 0-95\r\nThread(s) per core:  1\r\nCore(s) per socket:  48\r\nSocket(s):           2\r\nNUMA node(s):        8\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               143\r\nModel name:          Intel(R) Xeon(R) Platinum 8468\r\nStepping:            8\r\nCPU MHz:             3701.598\r\nCPU max MHz:         3800.0000\r\nCPU min MHz:         800.0000\r\nBogoMIPS:            4200.00\r\nL1d cache:           48K\r\nL1i cache:           32K\r\nL2 cache:            2048K\r\nL3 cache:            107520K\r\nNUMA node0 CPU(s):   0-11\r\nNUMA node1 CPU(s):   12-23\r\nNUMA node2 CPU(s):   24-35\r\nNUMA node3 CPU(s):   36-47\r\nNUMA node4 CPU(s):   48-59\r\nNUMA node5 CPU(s):   60-71\r\nNUMA node6 CPU(s):   72-83\r\nNUMA node7 CPU(s):   84-95\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.68\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] sentence-transformers==3.0.1\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.1\r\n[pip3] triton==3.0.0\r\n[conda] No relevant packages\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.1.dev238+ge2c6e0a82\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV6     NV6     NV6     SYS     SYS     0-11    0               N/A\r\nGPU1    NV6      X      NV6     SYS     PIX     SYS     24-35   2               N/A\r\nGPU2    SYS     SYS      X      SYS     SYS     SYS     48-59   4               N/A\r\nGPU3    NV6     SYS     NV6      X      SYS     PIX     72-83   6               N/A\r\nNIC0    SYS     PIX     SYS     SYS      X      SYS\r\nNIC1    SYS     SYS     SYS     PIX     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nWhen serving a vllm server with ``vllm serve path/to/model --config path/to/config.yaml`` the position of the argument ``served-model-name`` seems to be cruical to successfully run the server.\r\n\r\nP.s. \r\nWhy is `collect_env.py` showing `vLLM Version: 0.6.1.dev238+ge2c6e0a82`\r\nI definitely used vllm=0.6.2 and my pip shows the same.\r\n\r\nConfig that works flawlessly:\r\n```\r\nserved-model-name: \"MyModel\"\r\nhost: \"127.0.0.1\"\r\nport: 6379\r\nuvicorn-log-level: \"info\"\r\n```\r\nHere, the server runs and I can call the model using the name \"MyModel\".\r\n\r\nConfig that does not work:\r\n```\r\nhost: \"127.0.0.1\"\r\nport: 6379\r\nuvicorn-log-level: \"info\"\r\nserved-model-name: \"MyModel\"\r\n```\r\n\r\nWith the latter config, I get the following error:\r\n``vllm serve: error: the following arguments are required: model_tag``\r\n\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2024-09-29T15:06:36+00:00",
    "closed_at": "2024-10-05T17:35:13+00:00",
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8947/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8947"
  },
  {
    "number": 3666,
    "title": "[Misc]: Implement CPU/GPU swapping in BlockManagerV2",
    "body": "Recently, we refactored the block manager subsystem to improve testability by separating concerns of each layer. See https://github.com/vllm-project/vllm/pull/3492 for more information.\r\n\r\nThe V2 implementation does not have support for CPU-GPU swapping. It can be added in the [CpuGpuBlockAllocator](https://github.com/vllm-project/vllm/blob/321dc1619ad60b6df74fa86ac6299bc83c223996/vllm/core/block/cpu_gpu_block_allocator.py). My first take on the design is that it should simply keep track of the requested swap requests and have the scheduler `get_and_clear` them after each scheduling step.\r\n\r\n![image](https://github.com/vllm-project/vllm/assets/950914/55cf0db2-2614-463b-a053-eb3f182c01bb)\r\n",
    "labels": [
      "good first issue",
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-03-27T21:40:21+00:00",
    "closed_at": "2024-06-03T20:41:11+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3666/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3666"
  },
  {
    "number": 18166,
    "title": "[HELP WANTED] Fix Failing Spec Decoding Test",
    "body": "### Issue\n\nWe are seeing a test failure related to EAGLE on V0. We would appreciate anyone who can help addressing it. \n\n```bash\npytest -s -v tests/spec_decode/e2e/test_eagle_correctness.py::test_eagle_e2e_greedy_correctness_with_preemption\n```\n\nPR which disables the test: https://github.com/vllm-project/vllm/pull/18165\n\nIf anyone has capacity to help out with re-enabling this, we would greatly appreciate it!",
    "labels": [
      "bug",
      "good first issue"
    ],
    "state": "closed",
    "created_at": "2025-05-14T20:14:33+00:00",
    "closed_at": "2025-05-19T02:49:47+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18166/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/18166"
  },
  {
    "number": 15569,
    "title": "[Bug]: Vllm 0.8.2 + Ray 2.44 (Ray serve deployment) fallbacks to V0 Engine",
    "body": "### Your current environment\n\n<details>\n\n\n```text\nINFO 03-26 19:23:29 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-6.8.0-1020-gcp-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.5.40\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-40GB\nNvidia driver version: 550.90.07\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               12\nOn-line CPU(s) list:                  0-11\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\nCPU family:                           6\nModel:                                85\nThread(s) per core:                   2\nCore(s) per socket:                   6\nSocket(s):                            1\nStepping:                             7\nBogoMIPS:                             4400.39\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            192 KiB (6 instances)\nL1i cache:                            192 KiB (6 instances)\nL2 cache:                             6 MiB (6 instances)\nL3 cache:                             38.5 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-11\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; Enhanced IBRS\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.3.0\n[pip3] torch==2.6.0+cu124\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0+cu124\n[pip3] transformers==4.50.1\n[pip3] triton==3.2.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pyzmq                     26.3.0                   pypi_0    pypi\n[conda] torch                     2.6.0+cu124              pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0+cu124             pypi_0    pypi\n[conda] transformers              4.50.1                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-11    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen deploying vllm 0.8.2 with ray 2.44.0 , vllm falls back to V0.\n\nLogs from ray serve deployment:\n\n![Image](https://github.com/user-attachments/assets/1663f5d7-c53d-448e-b778-334a68ad857b)\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "open",
    "created_at": "2025-03-26T19:29:21+00:00",
    "closed_at": null,
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15569/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15569"
  },
  {
    "number": 16259,
    "title": "[Bug]: vLLM still runs after Ray workers crash",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\nINFO 04-08 04:09:19 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 4.0.0\nLibc version: glibc-2.35\n\nPython version: 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-122-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA H100 NVL\nGPU 1: NVIDIA H100 NVL\nGPU 2: NVIDIA H100 NVL\nGPU 3: NVIDIA H100 NVL\n\nNvidia driver version: 555.52.04\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               254\nOn-line CPU(s) list:                  0-253\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 9554 64-Core Processor\nCPU family:                           25\nModel:                                17\nThread(s) per core:                   1\nCore(s) per socket:                   127\nSocket(s):                            2\nStepping:                             1\nBogoMIPS:                             6190.70\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm rep_good nopl cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw perfctr_core invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr wbnoinvd arat npt lbrv nrip_save tsc_scale vmcb_clean flushbyasid pausefilter pfthreshold v_vmsave_vmload vgif avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm flush_l1d arch_capabilities\nVirtualization:                       AMD-V\nHypervisor vendor:                    KVM\nVirtualization type:                  full\nL1d cache:                            15.9 MiB (254 instances)\nL1i cache:                            15.9 MiB (254 instances)\nL2 cache:                             127 MiB (254 instances)\nL3 cache:                             4 GiB (254 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-126\nNUMA node1 CPU(s):                    127-253\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.1.post2+cu124torch2.6\n[pip3] numpy==2.1.3\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.0\n[pip3] triton==3.2.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tNV12\tPHB\tPHB\t0-253\t0-1\t\tN/A\nGPU1\tNV12\t X \tPHB\tPHB\t0-253\t0-1\t\tN/A\nGPU2\tPHB\tPHB\t X \tNV12\t0-253\t0-1\t\tN/A\nGPU3\tPHB\tPHB\tNV12\t X \t0-253\t0-1\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNVIDIA_VISIBLE_DEVICES=all\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\nNCCL_VERSION=2.20.5-1\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nVLLM_WORKER_MULTIPROC_METHOD=spawn\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nCUDA_VERSION=12.4.0\nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nVLLM_NO_USAGE_STATS=1\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n```\n\n</details>\nI came up with some issue regarding 0.8.x in serving some models for instance Llama 3.3 70B , Llama 3.1 405B models, Mixtral-8x7B\n\n### \ud83d\udc1b Describe the bug\n\nIn the instances of 0.8.1, 0.8.2 the issue we're mainly in the CUDA out of memory that I can add the details also in this ticket, the main problem is also in 0.8.3 where is more stable on the CUDA.\n\nI started a model\n\nThe model load properly with no issue, getting to the end to start the FastAPI\n\nAt the first request the Ray workers go down while the FastAPI is still on, handling the requests from users, keeping them in waiting until the request is done, which leaves them hanging.\n\nFull detail output\n```bash\nINFO 04-08 03:47:19 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-08 03:47:24 [api_server.py:1034] vLLM API server version 0.8.3\nINFO 04-08 03:47:24 [api_server.py:1035] args: Namespace(host='0.0.0.0', port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template='/vllm-workspace/examples/tool_chat_template_llama3.1_json.jinja', chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=True, tool_call_parser='llama3_json', tool_parser_plugin='', model='hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend='ray', pipeline_parallel_size=1, tensor_parallel_size=4, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.91, num_gpu_blocks_override=None, max_num_batched_tokens=1024, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=256, max_logprobs=128, disable_log_stats=False, quantization='awq_marlin', rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=True, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='cuda', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=True, speculative_config=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['meta-llama/Meta-Llama-3.1-405B-Instruct-FP8'], qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)\nINFO 04-08 03:47:40 [config.py:600] This model supports multiple tasks: {'reward', 'score', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.\nINFO 04-08 03:47:42 [awq_marlin.py:114] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\nINFO 04-08 03:47:43 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=1024.\nINFO 04-08 03:47:51 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-08 03:47:57 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4', speculative_config=None, tokenizer='hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Meta-Llama-3.1-405B-Instruct-FP8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\nINFO 04-08 03:48:04 [ray_utils.py:335] No current placement group found. Creating a new placement group.\nINFO 04-08 03:48:04 [ray_distributed_executor.py:176] use_ray_spmd_worker: True\n\u001b[36m(pid=692)\u001b[0m INFO 04-08 03:48:10 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-08 03:48:19 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()\nINFO 04-08 03:48:19 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_NO_USAGE_STATS', 'VLLM_USAGE_SOURCE', 'VLLM_USE_RAY_SPMD_WORKER', 'VLLM_USE_RAY_COMPILED_DAG', 'VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USE_V1']\nINFO 04-08 03:48:19 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /root/.config/vllm/ray_non_carry_over_env_vars.json file\n\u001b[36m(RayWorkerWrapper pid=687)\u001b[0m WARNING 04-08 03:48:25 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f65601f9880>\n\u001b[36m(pid=689)\u001b[0m INFO 04-08 03:48:11 [__init__.py:239] Automatically detected platform cuda.\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=687)\u001b[0m INFO 04-08 03:48:27 [utils.py:990] Found nccl from library libnccl.so.2\n\u001b[36m(RayWorkerWrapper pid=687)\u001b[0m INFO 04-08 03:48:27 [pynccl.py:69] vLLM is using nccl==2.21.5\n\u001b[36m(RayWorkerWrapper pid=687)\u001b[0m INFO 04-08 03:48:33 [parallel_state.py:957] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1\n\u001b[36m(RayWorkerWrapper pid=687)\u001b[0m INFO 04-08 03:48:33 [cuda.py:221] Using Flash Attention backend on V1 engine.\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m WARNING 04-08 03:48:25 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1c905d15e0>\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:48:27 [utils.py:990] Found nccl from library libnccl.so.2\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:48:27 [pynccl.py:69] vLLM is using nccl==2.21.5\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:48:33 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_ca37703b'), local_subscribe_addr='ipc:///tmp/002617c9-e3fb-424e-8d18-231c479406ac', remote_subscribe_addr=None, remote_addr_ipv6=False)\n\u001b[36m(RayWorkerWrapper pid=687)\u001b[0m INFO 04-08 03:48:33 [gpu_model_runner.py:1258] Starting to load model hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4...\n\u001b[36m(RayWorkerWrapper pid=687)\u001b[0m INFO 04-08 03:48:34 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.\n\u001b[36m(RayWorkerWrapper pid=693)\u001b[0m INFO 04-08 03:48:34 [weight_utils.py:265] Using model weights format ['*.safetensors']\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:49:31 [loader.py:447] Loading weights took 56.74 seconds\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:48:33 [parallel_state.py:957] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:48:33 [cuda.py:221] Using Flash Attention backend on V1 engine.\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:48:33 [gpu_model_runner.py:1258] Starting to load model hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4...\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:48:34 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=692)\u001b[0m INFO 04-08 03:48:34 [weight_utils.py:265] Using model weights format ['*.safetensors']\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:49:35 [gpu_model_runner.py:1273] Model loading took 51.1453 GiB and 61.854495 seconds\n\u001b[36m(RayWorkerWrapper pid=693)\u001b[0m INFO 04-08 03:49:35 [loader.py:447] Loading weights took 59.74 seconds\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=692)\u001b[0m INFO 04-08 03:49:41 [gpu_model_runner.py:1273] Model loading took 51.1453 GiB and 67.717508 seconds\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=692)\u001b[0m INFO 04-08 03:49:36 [loader.py:447] Loading weights took 61.44 seconds\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:50:26 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/88e04f09c2/rank_0_0 for vLLM's torch.compile\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:50:26 [backends.py:426] Dynamo bytecode transform time: 44.26 s\n\u001b[36m(RayWorkerWrapper pid=692)\u001b[0m INFO 04-08 03:50:35 [backends.py:132] Cache the graph of shape None for later use\n\u001b[36m(RayWorkerWrapper pid=693)\u001b[0m INFO 04-08 03:50:30 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/88e04f09c2/rank_3_0 for vLLM's torch.compile\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=693)\u001b[0m INFO 04-08 03:50:30 [backends.py:426] Dynamo bytecode transform time: 48.64 s\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=693)\u001b[0m INFO 04-08 03:50:41 [backends.py:132] Cache the graph of shape None for later use\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=692)\u001b[0m INFO 04-08 03:53:10 [backends.py:144] Compiling a graph for general shape takes 158.06 s\n\u001b[36m(RayWorkerWrapper pid=687)\u001b[0m INFO 04-08 03:53:16 [backends.py:144] Compiling a graph for general shape takes 161.66 s\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(RayWorkerWrapper pid=687)\u001b[0m INFO 04-08 03:54:40 [monitor.py:33] torch.compile takes 208.36 s in total\n\u001b[36m(RayWorkerWrapper pid=693)\u001b[0m INFO 04-08 03:53:18 [backends.py:144] Compiling a graph for general shape takes 161.66 s\nINFO 04-08 03:54:46 [kv_cache_utils.py:578] GPU KV cache size: 132,464 tokens\nINFO 04-08 03:54:46 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 1.01x\nINFO 04-08 03:54:46 [kv_cache_utils.py:578] GPU KV cache size: 132,560 tokens\nINFO 04-08 03:54:46 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 1.01x\nINFO 04-08 03:54:46 [kv_cache_utils.py:578] GPU KV cache size: 132,400 tokens\nINFO 04-08 03:54:46 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 1.01x\nINFO 04-08 03:54:46 [kv_cache_utils.py:578] GPU KV cache size: 132,560 tokens\nINFO 04-08 03:54:46 [kv_cache_utils.py:581] Maximum concurrency for 131,072 tokens per request: 1.01x\n\u001b[36m(RayWorkerWrapper pid=693)\u001b[0m INFO 04-08 03:58:02 [gpu_model_runner.py:1608] Graph capturing finished in 196 secs, took 4.95 GiB\n\u001b[36m(RayWorkerWrapper pid=689)\u001b[0m INFO 04-08 03:54:40 [monitor.py:33] torch.compile takes 202.32 s in total\u001b[32m [repeated 3x across cluster]\u001b[0m\nINFO 04-08 03:58:02 [core.py:162] init engine (profile, create kv cache, warmup model) took 500.69 seconds\nWARNING 04-08 03:58:04 [api_server.py:936] Using supplied chat template: {{- bos_token }}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- if custom_tools is defined %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- set tools = custom_tools %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- if not tools_in_user_message is defined %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {#- Llama 3.1 doesn't pass all tests if the tools are in the system prompt #}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- set tools_in_user_message = true %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- if not date_string is defined %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- if strftime_now is defined %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- set date_string = strftime_now(\"%d %b %Y\") %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- else %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- set date_string = \"26 Jul 2024\" %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- if not tools is defined %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- set tools = none %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] \nWARNING 04-08 03:58:04 [api_server.py:936] {#- This block extracts the system message, so we can slot it into the right place. #}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- if messages[0]['role'] == 'system' %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- if messages[0]['content'] is string %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- set system_message = messages[0]['content']|trim %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- else %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- set system_message = messages[0]['content'][0]['text']|trim %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- set messages = messages[1:] %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- else %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- if tools is not none %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- set system_message = \"You are a helpful assistant with tool calling capabilities. Only reply with a tool call if the function exists in the library provided by the user. If it doesn't exist, just reply directly in natural language. When you receive a tool call response, use the output to format an answer to the original user question.\" %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- else %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- set system_message = \"\" %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] \nWARNING 04-08 03:58:04 [api_server.py:936] {#- System message #}\nWARNING 04-08 03:58:04 [api_server.py:936] {{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- if tools is not none %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- \"Environment: ipython\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] {{- \"Cutting Knowledge Date: December 2023\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936] {{- \"Today Date: \" + date_string + \"\\n\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- if tools is not none and not tools_in_user_message %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call. \" }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. ' }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- \"Do not use variables.\\n\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- for t in tools %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- t | tojson(indent=4) }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- \"\\n\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- endfor %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] {{- system_message }}\nWARNING 04-08 03:58:04 [api_server.py:936] {{- \"<|eot_id|>\" }}\nWARNING 04-08 03:58:04 [api_server.py:936] \nWARNING 04-08 03:58:04 [api_server.py:936] {#- Custom tools are passed in a user message with some extra guidance #}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- if tools_in_user_message and not tools is none %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {#- Extract the first user message so we can plug it in here #}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- if messages | length != 0 %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- if messages[0]['content'] is string %}\nWARNING 04-08 03:58:04 [api_server.py:936]             {%- set first_user_message = messages[0]['content']|trim %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- else %}\nWARNING 04-08 03:58:04 [api_server.py:936]             {%- set first_user_message = messages[0]['content'] | selectattr('type', 'equalto', 'text') | map(attribute='text') | map('trim') | join('\\n') %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- set messages = messages[1:] %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- else %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- \"Given the following functions, please respond with a JSON for a function call \" }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. ' }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- \"Do not use variables.\\n\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- for t in tools %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- t | tojson(indent=4) }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- \"\\n\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- endfor %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- first_user_message + \"<|eot_id|>\"}}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] \nWARNING 04-08 03:58:04 [api_server.py:936] {%- for message in messages %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- if message['content'] is string %}\nWARNING 04-08 03:58:04 [api_server.py:936]             {{- message['content'] | trim}}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- else %}\nWARNING 04-08 03:58:04 [api_server.py:936]             {%- for content in message['content'] %}\nWARNING 04-08 03:58:04 [api_server.py:936]                 {%- if content['type'] == 'text' %}\nWARNING 04-08 03:58:04 [api_server.py:936]                     {{- content['text'] | trim }}\nWARNING 04-08 03:58:04 [api_server.py:936]                 {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936]             {%- endfor %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- '<|eot_id|>' }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- elif 'tool_calls' in message %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- if not message.tool_calls|length == 1 %}\nWARNING 04-08 03:58:04 [api_server.py:936]             {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- set tool_call = message.tool_calls[0].function %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- '{\"name\": \"' + tool_call.name + '\", ' }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- '\"parameters\": ' }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- tool_call.arguments | tojson }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- \"}\" }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- \"<|eot_id|>\" }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- if message.content is string %}\nWARNING 04-08 03:58:04 [api_server.py:936]             {{- { \"output\": message.content } | tojson }}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- else %}\nWARNING 04-08 03:58:04 [api_server.py:936]             {%- for content in message['content']  %}\nWARNING 04-08 03:58:04 [api_server.py:936]                 {%- if content['type']  == 'text' %}\nWARNING 04-08 03:58:04 [api_server.py:936]                     {{- { \"output\": content['text']  } | tojson }}\nWARNING 04-08 03:58:04 [api_server.py:936]                 {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936]             {%- endfor %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936]         {{- \"<|eot_id|>\" }}\nWARNING 04-08 03:58:04 [api_server.py:936]     {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endfor %}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- if add_generation_prompt %}\nWARNING 04-08 03:58:04 [api_server.py:936]     {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\nWARNING 04-08 03:58:04 [api_server.py:936] {%- endif %}\nWARNING 04-08 03:58:04 [api_server.py:936] \nWARNING 04-08 03:58:04 [api_server.py:936] It is different from official chat template 'hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4'. This discrepancy may lead to performance degradation.\nINFO 04-08 03:58:04 [serving_chat.py:79] \"auto\" tool choice has been enabled please note that while the parallel_tool_calls client option is preset for compatibility reasons, it will be ignored.\nINFO 04-08 03:58:04 [api_server.py:1081] Starting vLLM API server on http://0.0.0.0:8000\nINFO 04-08 03:58:04 [launcher.py:26] Available routes are:\nINFO 04-08 03:58:04 [launcher.py:34] Route: /openapi.json, Methods: HEAD, GET\nINFO 04-08 03:58:04 [launcher.py:34] Route: /docs, Methods: HEAD, GET\nINFO 04-08 03:58:04 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: HEAD, GET\nINFO 04-08 03:58:04 [launcher.py:34] Route: /redoc, Methods: HEAD, GET\nINFO 04-08 03:58:04 [launcher.py:34] Route: /health, Methods: GET\nINFO 04-08 03:58:04 [launcher.py:34] Route: /load, Methods: GET\nINFO 04-08 03:58:04 [launcher.py:34] Route: /ping, Methods: POST, GET\nINFO 04-08 03:58:04 [launcher.py:34] Route: /tokenize, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /detokenize, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /v1/models, Methods: GET\nINFO 04-08 03:58:04 [launcher.py:34] Route: /version, Methods: GET\nINFO 04-08 03:58:04 [launcher.py:34] Route: /v1/chat/completions, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /v1/completions, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /v1/embeddings, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /pooling, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /score, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /v1/score, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /rerank, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /v1/rerank, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /v2/rerank, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /invocations, Methods: POST\nINFO 04-08 03:58:04 [launcher.py:34] Route: /metrics, Methods: GET\nINFO:     5:43844 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43852 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43860 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43876 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:53092 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:53104 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40742 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40756 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40760 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40772 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:58:15 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     2:48512 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO:     5:40784 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40786 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40796 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40798 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:58500 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:58512 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43002 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43018 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43028 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43034 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:58:25 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:43036 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43052 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43068 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:43080 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:51358 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:51374 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     2:44628 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO:     5:46902 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46906 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46912 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46918 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:58:35 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:46934 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46948 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46956 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46958 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:51312 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:51314 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54152 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54154 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54164 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54170 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:58:45 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     2:53014 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO:     5:54186 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54190 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54206 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54216 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:47228 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:47238 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:53652 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:53658 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:53664 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:53678 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:58:55 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:53690 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:53698 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:53700 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:53712 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:42810 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:42812 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     2:55884 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO:     5:58730 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:58742 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     10.6.11.226:45866 - \"POST /v1/completions HTTP/1.1\" 404 Not Found\nINFO:     5:58748 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:58764 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:59:05 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:58780 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:58790 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:58806 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:58814 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:59214 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:59216 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54056 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54064 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54066 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54072 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:59:15 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     2:52392 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO:     5:54088 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54094 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54100 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:54112 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:49976 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:49978 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40568 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40570 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40586 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40598 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:59:25 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:40606 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40610 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40620 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:40634 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:35018 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:35034 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     2:45668 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO:     5:56856 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:56870 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:56876 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:56886 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:59:35 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:56890 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:56892 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:56896 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:56904 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:35212 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:35224 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:39002 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:39018 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:39034 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:39050 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:59:45 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     2:35684 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO:     5:39062 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:39076 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:39080 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:39082 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:53498 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:53500 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:49912 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:49922 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:49934 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:49942 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 03:59:55 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:49944 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:49952 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:49958 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:49960 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:58230 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:58246 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     2:54470 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO 04-08 04:00:01 [logger.py:39] Received request cmpl-17baf80e349d4adfb4e385cf652e427a-0: prompt: 'Do you know the book Traction by Gino Wickman', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=120, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [128000, 5519, 499, 1440, 279, 2363, 350, 16597, 555, 480, 3394, 75206, 1543], lora_request: None, prompt_adapter_request: None.\nINFO:     10.6.11.226:54104 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO 04-08 04:00:01 [async_llm.py:228] Added request cmpl-17baf80e349d4adfb4e385cf652e427a-0.\nINFO 04-08 04:00:01 [ray_distributed_executor.py:561] VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE = auto\nINFO 04-08 04:00:01 [ray_distributed_executor.py:563] VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM = False\nINFO 04-08 04:00:01 [ray_distributed_executor.py:578] RAY_CGRAPH_get_timeout is set to 300\nERROR 04-08 04:00:01 [core.py:390] EngineCore hit an exception: Traceback (most recent call last):\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 383, in run_engine_core\nERROR 04-08 04:00:01 [core.py:390]     engine_core.run_busy_loop()\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 405, in run_busy_loop\nERROR 04-08 04:00:01 [core.py:390]     self._process_engine_step()\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 434, in _process_engine_step\nERROR 04-08 04:00:01 [core.py:390]     outputs = self.step_fn()\nERROR 04-08 04:00:01 [core.py:390]               ^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 206, in step\nERROR 04-08 04:00:01 [core.py:390]     output = self.model_executor.execute_model(scheduler_output)\nERROR 04-08 04:00:01 [core.py:390]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/ray_distributed_executor.py\", line 57, in execute_model\nERROR 04-08 04:00:01 [core.py:390]     return refs[0].get()\nERROR 04-08 04:00:01 [core.py:390]            ^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/compiled_dag_ref.py\", line 154, in get\nERROR 04-08 04:00:01 [core.py:390]     raise execution_error from None\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/compiled_dag_ref.py\", line 145, in get\nERROR 04-08 04:00:01 [core.py:390]     ray.get(actor_execution_loop_refs, timeout=10)\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\nERROR 04-08 04:00:01 [core.py:390]     return fn(*args, **kwargs)\nERROR 04-08 04:00:01 [core.py:390]            ^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\nERROR 04-08 04:00:01 [core.py:390]     return func(*args, **kwargs)\nERROR 04-08 04:00:01 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2771, in get\nERROR 04-08 04:00:01 [core.py:390]     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\nERROR 04-08 04:00:01 [core.py:390]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 919, in get_objects\nERROR 04-08 04:00:01 [core.py:390]     raise value.as_instanceof_cause()\nERROR 04-08 04:00:01 [core.py:390] ray.exceptions.RayTaskError(ModuleNotFoundError): \u001b[36mray::RayWorkerWrapper.__ray_call__()\u001b[39m (pid=687, ip=3, actor_id=c6e851436e36ad12c39ebb1501000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x7f655b7c55b0>)\nERROR 04-08 04:00:01 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/actor.py\", line 1722, in __ray_call__\nERROR 04-08 04:00:01 [core.py:390]     return fn(self, *args, **kwargs)\nERROR 04-08 04:00:01 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 230, in do_exec_tasks\nERROR 04-08 04:00:01 [core.py:390]     done = tasks[operation.exec_task_idx].exec_operation(\nERROR 04-08 04:00:01 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 745, in exec_operation\nERROR 04-08 04:00:01 [core.py:390]     with _device_context_manager():\nERROR 04-08 04:00:01 [core.py:390]          ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 345, in _device_context_manager\nERROR 04-08 04:00:01 [core.py:390]     device = ChannelContext.get_current().torch_device\nERROR 04-08 04:00:01 [core.py:390]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/common.py\", line 173, in torch_device\nERROR 04-08 04:00:01 [core.py:390]     from ray.air._internal import torch_utils\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/air/__init__.py\", line 1, in <module>\nERROR 04-08 04:00:01 [core.py:390]     from ray.air.config import (\nERROR 04-08 04:00:01 [core.py:390]   File \"/usr/local/lib/python3.12/dist-packages/ray/air/config.py\", line 17, in <module>\nERROR 04-08 04:00:01 [core.py:390]     import pyarrow.fs\nERROR 04-08 04:00:01 [core.py:390] ModuleNotFoundError: No module named 'pyarrow'\nERROR 04-08 04:00:01 [core.py:390] \nINFO 04-08 04:00:01 [ray_distributed_executor.py:127] Shutting down Ray distributed executor. If you see error log from logging.cc regarding SIGTERM received, please ignore because this is the expected termination process in Ray.\nCRITICAL 04-08 04:00:01 [core_client.py:361] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\nINFO:     5:51022 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:51024 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:51028 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:51036 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 04:00:05 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:51046 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:51048 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:51058 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:51070 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:57732 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:57746 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:37712 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:37722 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:37736 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:37738 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 04:00:15 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     2:50184 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO 04-08 04:00:17 [async_llm.py:355] Aborted request cmpl-17baf80e349d4adfb4e385cf652e427a-0. # User aborted\nINFO:     5:37740 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:37756 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:37758 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:37760 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:51030 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:51042 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:52366 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:52376 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:52378 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:52382 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 04:00:25 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:52388 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:52404 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:52408 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:52422 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:57830 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:57832 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     2:36708 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO:     5:57926 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:57930 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:57942 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:57950 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 04:00:35 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     5:57966 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:57972 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:57978 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:57980 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:35028 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:35034 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 04:00:41 [logger.py:39] Received request cmpl-4e919233f0b34f108bd337ed1fa32cb4-0: prompt: 'Do you know the book Traction by Gino Wickman', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=120, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [128000, 5519, 499, 1440, 279, 2363, 350, 16597, 555, 480, 3394, 75206, 1543], lora_request: None, prompt_adapter_request: None.\nINFO:     10.6.11.226:50254 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO 04-08 04:00:41 [async_llm.py:228] Added request cmpl-4e919233f0b34f108bd337ed1fa32cb4-0.\nINFO:     5:46096 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46098 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46100 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46112 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 04:00:45 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     2:47164 - \"GET /metrics HTTP/1.1\" 200 OK\nINFO:     5:46126 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46142 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46148 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:46162 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:33952 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     4:33968 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:34894 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:34908 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:34920 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     5:34926 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-08 04:00:55 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n``` \n\nThe expected outcome is after the workers crash, the whole API should also stop, not being able to handle any requests or some error to get the docker container back starting\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "open",
    "created_at": "2025-04-08T11:23:38+00:00",
    "closed_at": null,
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16259/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16259"
  },
  {
    "number": 16692,
    "title": "[Bug]: Problems with vllm serve DeepSeek-R1 with 2 nodes and TP = 16\uff08include vllm v0.8.4 v0.7.3 v0.7.2 V0 V1 engine\uff09",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n v0.8.4 using TP = 16 to serving deepseek-v3 in 2*H800*8 On Ray cluster, get EngineCore exception\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nstart command:\nhead node:\n```bash\nray start --head --port=6379  && \\\n    vllm serve $MODELPATH \\\n    --max-num-seqs=256 \\\n    --max-model-len=32768 \\\n    --max-num-batched-tokens=32768 \\\n    --tensor-parallel-size 16 \\\n    --enable-expert-parallel \\\n    --enable-prefix-caching \\\n    --enable-chunked-prefill \\\n    --distributed-executor-backend=ray \\\n    --trust-remote-code \\\n    --served-model-name deepseek-r1\n```\nslave node:\n```bash\nray start --block --address=$HEADPODIP:6379\n```\n\nget error:\n```bash\n2025-04-16 10:27:16,259 INFO usage_lib.py:467 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n2025-04-16 10:27:16,259 INFO scripts.py:865 -- Local node IP: 172.20.155.155\n2025-04-16 10:27:19,206 SUCC scripts.py:902 -- --------------------\n2025-04-16 10:27:19,206 SUCC scripts.py:903 -- Ray runtime started.\n2025-04-16 10:27:19,206 SUCC scripts.py:904 -- --------------------\n2025-04-16 10:27:19,206 INFO scripts.py:906 -- Next steps\n2025-04-16 10:27:19,207 INFO scripts.py:909 -- To add another node to this Ray cluster, run\n2025-04-16 10:27:19,207 INFO scripts.py:912 --   ray start --address='172.20.155.155:6379'\n2025-04-16 10:27:19,207 INFO scripts.py:921 -- To connect to this Ray cluster:\n2025-04-16 10:27:19,207 INFO scripts.py:923 -- import ray\n2025-04-16 10:27:19,207 INFO scripts.py:924 -- ray.init()\n2025-04-16 10:27:19,207 INFO scripts.py:955 -- To terminate the Ray runtime, run\n2025-04-16 10:27:19,207 INFO scripts.py:956 --   ray stop\n2025-04-16 10:27:19,207 INFO scripts.py:959 -- To view the status of the cluster, use\n2025-04-16 10:27:19,207 INFO scripts.py:960 --   ray status\nINFO 04-16 10:27:25 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-16 10:27:28 [api_server.py:1034] vLLM API server version 0.8.4\nINFO 04-16 10:27:28 [api_server.py:1035] args: Namespace(subparser='serve', model_tag='/deepseek-r1', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/deepseek-r1', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, load_format='auto', download_dir=None, model_loader_extra_config=None, use_tqdm_on_load=True, config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=32768, guided_decoding_backend='auto', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend='ray', pipeline_parallel_size=1, tensor_parallel_size=16, data_parallel_size=1, enable_expert_parallel=True, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, block_size=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=32768, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=True, speculative_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['deepseek-r1-250120'], qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_chunked_mm_input=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x7f948f036340>)\nINFO 04-16 10:27:28 [config.py:209] Replacing legacy 'type' key with 'rope_type'\nINFO 04-16 10:27:33 [config.py:689] This model supports multiple tasks: {'generate', 'score', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.\nINFO 04-16 10:27:33 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=32768.\nWARNING 04-16 10:27:33 [fp8.py:63] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\nINFO 04-16 10:27:33 [cuda.py:160] Forcing kv cache block size to 64 for FlashMLA backend.\nINFO 04-16 10:27:37 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-16 10:27:38 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='/deepseek-r1', speculative_config=None, tokenizer='/deepseek-r1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=16, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=deepseek-r1-250120, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n2025-04-16 10:27:38,573 INFO worker.py:1654 -- Connecting to existing Ray cluster at address: 172.20.155.155:6379...\n2025-04-16 10:27:38,589 INFO worker.py:1841 -- Connected to Ray cluster.\nINFO 04-16 10:27:41 [ray_utils.py:335] No current placement group found. Creating a new placement group.\nWARNING 04-16 10:27:41 [ray_utils.py:199] tensor_parallel_size=16 is bigger than a reserved number of GPUs (8 GPUs) in a node 8ae1db6046a3cd19fa34db174f8f7f08658824bd4939356d992af164. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 16 GPUs available at each node.\nWARNING 04-16 10:27:41 [ray_utils.py:199] tensor_parallel_size=16 is bigger than a reserved number of GPUs (8 GPUs) in a node e9b799a0bcd5383e697d936e1119a975beac44342e36975145239d86. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 16 GPUs available at each node.\nINFO 04-16 10:27:41 [ray_distributed_executor.py:176] use_ray_spmd_worker: True\n[36m(pid=860)[0m INFO 04-16 10:27:44 [__init__.py:239] Automatically detected platform cuda.\nINFO 04-16 10:27:51 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()\nINFO 04-16 10:27:51 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_USAGE_SOURCE', 'VLLM_USE_RAY_SPMD_WORKER', 'VLLM_USE_RAY_COMPILED_DAG', 'VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USE_V1']\nINFO 04-16 10:27:51 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /root/.config/vllm/ray_non_carry_over_env_vars.json file\n[36m(RayWorkerWrapper pid=860)[0m [W416 10:28:00.809838573 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[36m(RayWorkerWrapper pid=860)[0m [W416 10:28:00.813800637 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[36m(RayWorkerWrapper pid=860)[0m [W416 10:28:00.911790999 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[36m(RayWorkerWrapper pid=860)[0m [W416 10:28:00.046069996 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[36m(RayWorkerWrapper pid=860)[0m [W416 10:28:00.055444791 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[36m(RayWorkerWrapper pid=860)[0m [W416 10:28:00.155493559 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[36m(RayWorkerWrapper pid=860)[0m [W416 10:28:00.173913414 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[36m(RayWorkerWrapper pid=860)[0m [W416 10:28:00.232485006 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:07, 21.28it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:   4% Completed | 6/163 [00:01<00:47,  3.28it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:   6% Completed | 9/163 [00:01<00:28,  5.44it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:   7% Completed | 11/163 [00:03<00:52,  2.88it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:   8% Completed | 13/163 [00:03<00:39,  3.82it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:   9% Completed | 15/163 [00:04<00:44,  3.33it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  10% Completed | 17/163 [00:04<00:33,  4.38it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  12% Completed | 19/163 [00:04<00:40,  3.57it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  13% Completed | 21/163 [00:05<00:30,  4.73it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  14% Completed | 23/163 [00:05<00:37,  3.72it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  15% Completed | 24/163 [00:06<00:48,  2.84it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  15% Completed | 25/163 [00:07<01:00,  2.29it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  16% Completed | 26/163 [00:08<01:10,  1.94it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  17% Completed | 28/163 [00:08<00:45,  2.99it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  18% Completed | 30/163 [00:09<00:47,  2.78it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  19% Completed | 31/163 [00:09<00:58,  2.24it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  21% Completed | 34/163 [00:10<00:35,  3.60it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  21% Completed | 35/163 [00:10<00:47,  2.68it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  22% Completed | 36/163 [00:11<00:58,  2.17it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  23% Completed | 38/163 [00:12<00:55,  2.24it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  24% Completed | 39/163 [00:13<01:04,  1.92it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  25% Completed | 40/163 [00:13<01:03,  1.93it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  25% Completed | 41/163 [00:14<01:02,  1.94it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  26% Completed | 43/163 [00:15<00:50,  2.37it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  28% Completed | 45/163 [00:15<00:33,  3.51it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  28% Completed | 46/163 [00:15<00:37,  3.10it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  30% Completed | 49/163 [00:15<00:21,  5.20it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  32% Completed | 52/163 [00:15<00:14,  7.64it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  33% Completed | 54/163 [00:16<00:18,  6.03it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  35% Completed | 57/163 [00:16<00:12,  8.43it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  37% Completed | 60/163 [00:17<00:14,  7.12it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  38% Completed | 62/163 [00:17<00:17,  5.90it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  39% Completed | 64/163 [00:18<00:19,  5.20it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  40% Completed | 66/163 [00:18<00:20,  4.71it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  42% Completed | 68/163 [00:18<00:15,  5.94it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  44% Completed | 71/163 [00:18<00:11,  8.11it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  45% Completed | 74/163 [00:19<00:12,  7.00it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  47% Completed | 76/163 [00:19<00:15,  5.69it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  48% Completed | 78/163 [00:20<00:12,  7.00it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  49% Completed | 80/163 [00:20<00:14,  5.73it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  50% Completed | 81/163 [00:21<00:18,  4.48it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  52% Completed | 84/163 [00:21<00:16,  4.85it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  52% Completed | 85/163 [00:22<00:19,  4.00it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  54% Completed | 88/163 [00:22<00:12,  6.07it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  55% Completed | 90/163 [00:22<00:13,  5.23it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  56% Completed | 92/163 [00:23<00:15,  4.70it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  58% Completed | 94/163 [00:23<00:15,  4.37it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  59% Completed | 96/163 [00:24<00:15,  4.19it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  61% Completed | 99/163 [00:24<00:10,  6.25it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  62% Completed | 101/163 [00:24<00:11,  5.38it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  64% Completed | 104/163 [00:25<00:10,  5.63it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  66% Completed | 107/163 [00:25<00:07,  7.80it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  68% Completed | 111/163 [00:25<00:04, 11.24it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  70% Completed | 114/163 [00:26<00:06,  7.06it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  71% Completed | 116/163 [00:26<00:07,  6.40it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  72% Completed | 118/163 [00:27<00:09,  4.67it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  73% Completed | 119/163 [00:27<00:10,  4.20it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  75% Completed | 122/163 [00:28<00:08,  4.86it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  75% Completed | 123/163 [00:28<00:09,  4.25it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  77% Completed | 126/163 [00:28<00:05,  6.43it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  79% Completed | 128/163 [00:29<00:05,  5.95it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  81% Completed | 132/163 [00:29<00:04,  6.95it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  83% Completed | 135/163 [00:30<00:04,  6.91it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  83% Completed | 136/163 [00:30<00:04,  5.61it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  85% Completed | 138/163 [00:31<00:05,  4.74it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  86% Completed | 140/163 [00:31<00:04,  4.74it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  87% Completed | 142/163 [00:32<00:04,  4.80it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  88% Completed | 144/163 [00:32<00:03,  4.87it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  90% Completed | 146/163 [00:32<00:03,  4.89it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  91% Completed | 149/163 [00:33<00:02,  5.55it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  92% Completed | 150/163 [00:33<00:02,  4.76it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  94% Completed | 153/163 [00:33<00:01,  7.14it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  95% Completed | 155/163 [00:34<00:01,  6.37it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  98% Completed | 159/163 [00:34<00:00,  9.92it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards:  99% Completed | 161/163 [00:34<00:00,  7.50it/s]\n[36m(RayWorkerWrapper pid=860)[0m \nLoading safetensors checkpoint shards: 100% Completed | 163/163 [00:34<00:00,  4.68it/s]\n[36m(RayWorkerWrapper pid=860)[0m \n[36m(RayWorkerWrapper pid=315, ip=172.20.96.190)[0m WARNING 04-16 10:27:57 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f44783d41a0>\n[36m(pid=318, ip=172.20.96.190)[0m INFO 04-16 10:27:48 [__init__.py:239] Automatically detected platform cuda.[32m [repeated 15x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m\n[36m(RayWorkerWrapper pid=863)[0m INFO 04-16 10:28:00 [utils.py:993] Found nccl from library libnccl.so.2\n[36m(RayWorkerWrapper pid=863)[0m INFO 04-16 10:28:00 [pynccl.py:69] vLLM is using nccl==2.21.5\n[36m(RayWorkerWrapper pid=863)[0m WARNING 04-16 10:28:07 [custom_all_reduce.py:84] Custom allreduce is disabled because this process group spans across nodes.\n[36m(RayWorkerWrapper pid=864)[0m WARNING 04-16 10:27:57 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ed4678ba3f0>[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=319, ip=172.20.96.190)[0m INFO 04-16 10:28:00 [utils.py:993] Found nccl from library libnccl.so.2[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=319, ip=172.20.96.190)[0m INFO 04-16 10:28:00 [pynccl.py:69] vLLM is using nccl==2.21.5[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=860)[0m INFO 04-16 10:28:07 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_7c006341'), local_subscribe_addr='ipc:///tmp/484f3a8c-b706-469d-ab5e-25dd3c2d6c05', remote_subscribe_addr='tcp://172.20.155.155:43865', remote_addr_ipv6=False)\n[36m(RayWorkerWrapper pid=861)[0m INFO 04-16 10:28:07 [parallel_state.py:959] rank 5 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 5\n[36m(RayWorkerWrapper pid=314, ip=172.20.96.190)[0m INFO 04-16 10:28:07 [cuda.py:207] Using FlashMLA backend on V1 engine.\n[36m(RayWorkerWrapper pid=865)[0m INFO 04-16 10:28:07 [gpu_model_runner.py:1276] Starting to load model /deepseek-r1...\n[36m(RayWorkerWrapper pid=863)[0m WARNING 04-16 10:28:07 [utils.py:165] The model class DeepseekV3ForCausalLM has not defined `packed_modules_mapping`, this may lead to incorrect mapping of quantized or ignored modules\n[36m(RayWorkerWrapper pid=314, ip=172.20.96.190)[0m INFO 04-16 10:28:08 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.\n[36m(RayWorkerWrapper pid=319, ip=172.20.96.190)[0m INFO 04-16 10:28:32 [loader.py:458] Loading weights took 23.36 seconds\n[36m(RayWorkerWrapper pid=319, ip=172.20.96.190)[0m WARNING 04-16 10:28:07 [custom_all_reduce.py:84] Custom allreduce is disabled because this process group spans across nodes.[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=313, ip=172.20.96.190)[0m INFO 04-16 10:28:07 [parallel_state.py:959] rank 8 in world size 16 is assigned as DP rank 0, PP rank 0, TP rank 8[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=865)[0m INFO 04-16 10:28:07 [cuda.py:207] Using FlashMLA backend on V1 engine.[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=319, ip=172.20.96.190)[0m INFO 04-16 10:28:07 [gpu_model_runner.py:1276] Starting to load model /deepseek-r1...[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=319, ip=172.20.96.190)[0m WARNING 04-16 10:28:07 [utils.py:165] The model class DeepseekV3ForCausalLM has not defined `packed_modules_mapping`, this may lead to incorrect mapping of quantized or ignored modules[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=320, ip=172.20.96.190)[0m INFO 04-16 10:28:08 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=319, ip=172.20.96.190)[0m INFO 04-16 10:28:33 [gpu_model_runner.py:1291] Model loading took 40.3174 GiB and 25.028323 seconds\n[36m(RayWorkerWrapper pid=317, ip=172.20.96.190)[0m INFO 04-16 10:28:36 [loader.py:458] Loading weights took 27.77 seconds[32m [repeated 7x across cluster][0m\n[36m(RayWorkerWrapper pid=317, ip=172.20.96.190)[0m INFO 04-16 10:28:37 [gpu_model_runner.py:1291] Model loading took 40.3174 GiB and 29.134500 seconds[32m [repeated 7x across cluster][0m\n[36m(RayWorkerWrapper pid=862)[0m INFO 04-16 10:28:42 [loader.py:458] Loading weights took 33.51 seconds[32m [repeated 4x across cluster][0m\n[36m(RayWorkerWrapper pid=314, ip=172.20.96.190)[0m INFO 04-16 10:28:54 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/1ebcb71993/rank_9_0 for vLLM's torch.compile\n[36m(RayWorkerWrapper pid=314, ip=172.20.96.190)[0m INFO 04-16 10:28:54 [backends.py:426] Dynamo bytecode transform time: 9.92 s\n[36m(RayWorkerWrapper pid=860)[0m INFO 04-16 10:28:44 [gpu_model_runner.py:1291] Model loading took 40.3174 GiB and 36.271878 seconds[32m [repeated 8x across cluster][0m\n[36m(RayWorkerWrapper pid=858)[0m INFO 04-16 10:28:43 [loader.py:458] Loading weights took 34.88 seconds[32m [repeated 4x across cluster][0m\n[36m(RayWorkerWrapper pid=314, ip=172.20.96.190)[0m INFO 04-16 10:28:58 [backends.py:132] Cache the graph of shape None for later use\n[36m(RayWorkerWrapper pid=858)[0m INFO 04-16 10:28:58 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/1ebcb71993/rank_2_0 for vLLM's torch.compile[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=858)[0m INFO 04-16 10:28:58 [backends.py:426] Dynamo bytecode transform time: 13.46 s[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=314, ip=172.20.96.190)[0m INFO 04-16 10:29:35 [backends.py:144] Compiling a graph for general shape takes 40.72 s\n[36m(RayWorkerWrapper pid=858)[0m INFO 04-16 10:29:01 [backends.py:132] Cache the graph of shape None for later use[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=867)[0m INFO 04-16 10:29:42 [backends.py:144] Compiling a graph for general shape takes 45.69 s[32m [repeated 8x across cluster][0m\n[36m(RayWorkerWrapper pid=863)[0m WARNING 04-16 10:29:53 [fp8_utils.py:431] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128,128].json\n[36m(RayWorkerWrapper pid=858)[0m INFO 04-16 10:29:47 [backends.py:144] Compiling a graph for general shape takes 48.56 s[32m [repeated 7x across cluster][0m\n[36m(RayWorkerWrapper pid=320, ip=172.20.96.190)[0m WARNING 04-16 10:29:56 [fused_moe.py:670] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=16,N=2048,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128,128].json\n[36m(RayWorkerWrapper pid=863)[0m INFO 04-16 10:31:13 [monitor.py:33] torch.compile takes 60.35 s in total\n[36m(RayWorkerWrapper pid=858)[0m WARNING 04-16 10:29:53 [fp8_utils.py:431] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128,128].json[32m [repeated 15x across cluster][0m\n[36m(RayWorkerWrapper pid=864)[0m WARNING 04-16 10:29:57 [fused_moe.py:670] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=16,N=2048,device_name=NVIDIA_H800,dtype=fp8_w8a8,block_shape=[128,128].json[32m [repeated 15x across cluster][0m\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 305,216 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.31x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 304,704 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.30x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 305,216 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.31x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 304,704 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.30x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 305,216 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.31x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 304,704 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.30x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 305,216 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.31x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 304,704 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.30x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 304,704 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.30x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 305,216 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.31x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 304,704 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.30x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 305,216 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.31x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 304,704 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.30x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 305,216 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.31x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 304,704 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.30x\nINFO 04-16 10:31:19 [kv_cache_utils.py:634] GPU KV cache size: 305,216 tokens\nINFO 04-16 10:31:19 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 9.31x\n[36m(RayWorkerWrapper pid=858)[0m INFO 04-16 10:32:16 [gpu_model_runner.py:1626] Graph capturing finished in 58 secs, took 3.51 GiB\n[36m(RayWorkerWrapper pid=319, ip=172.20.96.190)[0m INFO 04-16 10:31:13 [monitor.py:33] torch.compile takes 54.80 s in total[32m [repeated 15x across cluster][0m\nINFO 04-16 10:32:17 [core.py:163] init engine (profile, create kv cache, warmup model) took 212.66 seconds\nINFO 04-16 10:32:17 [core_client.py:435] Core engine process 0 ready.\nWARNING 04-16 10:32:17 [config.py:1177] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\nINFO 04-16 10:32:17 [serving_chat.py:118] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.95}\nINFO 04-16 10:32:17 [serving_completion.py:61] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.95}\nINFO 04-16 10:32:17 [api_server.py:1081] Starting vLLM API server on http://0.0.0.0:8000\nINFO 04-16 10:32:17 [launcher.py:26] Available routes are:\nINFO 04-16 10:32:17 [launcher.py:34] Route: /openapi.json, Methods: GET, HEAD\nINFO 04-16 10:32:17 [launcher.py:34] Route: /docs, Methods: GET, HEAD\nINFO 04-16 10:32:17 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: GET, HEAD\nINFO 04-16 10:32:17 [launcher.py:34] Route: /redoc, Methods: GET, HEAD\nINFO 04-16 10:32:17 [launcher.py:34] Route: /health, Methods: GET\nINFO 04-16 10:32:17 [launcher.py:34] Route: /load, Methods: GET\nINFO 04-16 10:32:17 [launcher.py:34] Route: /ping, Methods: GET, POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /tokenize, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /detokenize, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /v1/models, Methods: GET\nINFO 04-16 10:32:17 [launcher.py:34] Route: /version, Methods: GET\nINFO 04-16 10:32:17 [launcher.py:34] Route: /v1/chat/completions, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /v1/completions, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /v1/embeddings, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /pooling, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /score, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /v1/score, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /rerank, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /v1/rerank, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /v2/rerank, Methods: POST\nINFO 04-16 10:32:17 [launcher.py:34] Route: /invocations, Methods: POST\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     172.20.235.128:50301 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:32:27 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:58244 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:32:37 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:64024 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:32:47 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:21483 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:32:57 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:63969 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:33:07 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:30409 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:33:17 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:16541 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:33:27 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:57427 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:33:37 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:37520 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:33:47 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:40130 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:33:57 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:29034 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:34:07 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:27650 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:34:17 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:41571 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:34:27 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:14206 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:34:37 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:22656 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:34:47 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:41524 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:34:57 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:25804 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:35:07 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:39412 - \"GET /health HTTP/1.1\" 200 OK\nINFO 04-16 10:35:13 [chat_utils.py:396] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\nINFO 04-16 10:35:13 [logger.py:39] Received request chatcmpl-8f878979543841479d089e13a1da85f4: prompt: '<\uff5cbegin\u2581of\u2581sentence\uff5c>In every output, response using the following format:\\n<think>\\n{reasoning_content}\\n</think>\\n\\n{content}<\uff5cUser\uff5c>\u8bf7\u8bb2\u4e2a\u7b11\u8bdd\u3002<\uff5cAssistant\uff5c>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=0.95, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\nINFO 04-16 10:35:13 [async_llm.py:228] Added request chatcmpl-8f878979543841479d089e13a1da85f4.\nINFO 04-16 10:35:13 [ray_distributed_executor.py:561] VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE = auto\nINFO 04-16 10:35:13 [ray_distributed_executor.py:563] VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM = False\nINFO 04-16 10:35:13 [ray_distributed_executor.py:578] RAY_CGRAPH_get_timeout is set to 300\nINFO 04-16 10:35:17 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\nINFO:     172.20.235.128:30878 - \"GET /health HTTP/1.1\" 200 OK\nERROR 04-16 10:35:25 [core.py:387] EngineCore hit an exception: Traceback (most recent call last):\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 2344, in _execute_until\nERROR 04-16 10:35:25 [core.py:387]     result = self._dag_output_fetcher.read(timeout)\nERROR 04-16 10:35:25 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/common.py\", line 318, in read\nERROR 04-16 10:35:25 [core.py:387]     outputs = self._read_list(timeout)\nERROR 04-16 10:35:25 [core.py:387]               ^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/common.py\", line 409, in _read_list\nERROR 04-16 10:35:25 [core.py:387]     raise e\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/common.py\", line 391, in _read_list\nERROR 04-16 10:35:25 [core.py:387]     result = c.read(min(remaining_timeout, iteration_timeout))\nERROR 04-16 10:35:25 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/shared_memory_channel.py\", line 776, in read\nERROR 04-16 10:35:25 [core.py:387]     return self._channel_dict[self._resolve_actor_id()].read(timeout)\nERROR 04-16 10:35:25 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/shared_memory_channel.py\", line 612, in read\nERROR 04-16 10:35:25 [core.py:387]     output = self._buffers[self._next_read_index].read(timeout)\nERROR 04-16 10:35:25 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/channel/shared_memory_channel.py\", line 480, in read\nERROR 04-16 10:35:25 [core.py:387]     ret = self._worker.get_objects(\nERROR 04-16 10:35:25 [core.py:387]           ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 893, in get_objects\nERROR 04-16 10:35:25 [core.py:387]     ] = self.core_worker.get_objects(\nERROR 04-16 10:35:25 [core.py:387]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"python/ray/_raylet.pyx\", line 3189, in ray._raylet.CoreWorker.get_objects\nERROR 04-16 10:35:25 [core.py:387]   File \"python/ray/includes/common.pxi\", line 106, in ray._raylet.check_status\nERROR 04-16 10:35:25 [core.py:387] ray.exceptions.RayChannelTimeoutError: System error: Timed out waiting for object available to read. ObjectID: 0048581a5404469f49cc78ed8c9abf660217eb8e0100000002e1f505\nERROR 04-16 10:35:25 [core.py:387] \nERROR 04-16 10:35:25 [core.py:387] The above exception was the direct cause of the following exception:\nERROR 04-16 10:35:25 [core.py:387] \nERROR 04-16 10:35:25 [core.py:387] Traceback (most recent call last):\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 380, in run_engine_core\nERROR 04-16 10:35:25 [core.py:387]     engine_core.run_busy_loop()\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 402, in run_busy_loop\nERROR 04-16 10:35:25 [core.py:387]     self._process_engine_step()\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 431, in _process_engine_step\nERROR 04-16 10:35:25 [core.py:387]     outputs = self.step_fn()\nERROR 04-16 10:35:25 [core.py:387]               ^^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 207, in step\nERROR 04-16 10:35:25 [core.py:387]     output = self.model_executor.execute_model(scheduler_output)\nERROR 04-16 10:35:25 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/ray_distributed_executor.py\", line 57, in execute_model\nERROR 04-16 10:35:25 [core.py:387]     return refs[0].get()\nERROR 04-16 10:35:25 [core.py:387]            ^^^^^^^^^^^^^\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/experimental/compiled_dag_ref.py\", line 124, in get\nERROR 04-16 10:35:25 [core.py:387]     self._dag._execute_until(\nERROR 04-16 10:35:25 [core.py:387]   File \"/usr/local/lib/python3.12/dist-packages/ray/dag/compiled_dag_node.py\", line 2350, in _execute_until\nERROR 04-16 10:35:25 [core.py:387]     raise RayChannelTimeoutError(\nERROR 04-16 10:35:25 [core.py:387] ray.exceptions.RayChannelTimeoutError: System error: If the execution is expected to take a long time, increase RAY_CGRAPH_get_timeout which is currently 10 seconds. Otherwise, this may indicate that the execution is hanging.\nERROR 04-16 10:35:25 [core.py:387] \nINFO 04-16 10:35:25 [ray_distributed_executor.py:127] Shutting down Ray distributed executor. If you see error log from logging.cc regarding SIGTERM received, please ignore because this is the expected termination process in Ray.\n2025-04-16 10:35:25,965 INFO compiled_dag_node.py:2109 -- Tearing down compiled DAG\nCRITICAL 04-16 10:35:25 [core_client.py:359] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n2025-04-16 10:35:25,967 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, d10fb2b0c74d88d01a04b87201000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 911d26df30515b32be6a85f301000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 8e6d63962d86e65b1ea0f2b101000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 86c7380938683894e3ee26f201000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 49cc78ed8c9abf660217eb8e01000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 05669ac9f7453d130910f44801000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 609209648dd775e40cd891f301000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, daf13a1f09c703c2e0eed52b01000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 8a3d05115b896d5974f1e65501000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, e701d61cb9f15f100029c04c01000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 7317795a95765b6f9eef848201000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, d4d8521d057dd51fb5ba6c8201000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, bad7c2a8367932671a3ce46301000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, b62696b233dff721c33557ff01000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, b4d89cda43028fedc3ea64e401000000)\n2025-04-16 10:35:25,968 INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 835191b5fcd2ef2cd8a8e38d01000000)\n2025-04-16 10:35:26,014 INFO compiled_dag_node.py:2137 -- Waiting for worker tasks to exit\nINFO 04-16 10:35:27 [loggers.py:87] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n\n```\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray"
    ],
    "state": "open",
    "created_at": "2025-04-16T02:43:26+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16692/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16692"
  },
  {
    "number": 16474,
    "title": "[Bug]: Error when running Llama-4-Maverick-17B-128E-Instruct-FP8 on mi300x",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 04-11 09:49:33 [__init__.py:239] Automatically detected platform rocm.\nCollecting environment information...\nPyTorch version: 2.7.0a0+git295f2ed\nIs debug build: False\nCUDA used to build PyTorch: N/A\nROCM used to build PyTorch: 6.3.42133-1b9c17779\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: 18.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.3.1 24491 1e0fda770a2079fbd71e4b70974d74f62fd3af10)\nCMake version: version 3.31.6\nLibc version: glibc-2.35\n\nPython version: 3.12.10 (main, Apr  9 2025, 08:55:05) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-128-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: AMD Instinct MI300X (gfx942:sramecc+:xnack-)\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\nHIP runtime version: 6.3.42133\nMIOpen runtime version: 3.3.0\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        52 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               256\nOn-line CPU(s) list:                  0-255\nVendor ID:                            AuthenticAMD\nModel name:                           AMD EPYC 9534 64-Core Processor\nCPU family:                           25\nModel:                                17\nThread(s) per core:                   2\nCore(s) per socket:                   64\nSocket(s):                            2\nStepping:                             1\nFrequency boost:                      enabled\nCPU max MHz:                          3718.0659\nCPU min MHz:                          1500.0000\nBogoMIPS:                             4892.29\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov succor smca fsrm flush_l1d\nVirtualization:                       AMD-V\nL1d cache:                            4 MiB (128 instances)\nL1i cache:                            4 MiB (128 instances)\nL2 cache:                             128 MiB (128 instances)\nL3 cache:                             512 MiB (16 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-63,128-191\nNUMA node1 CPU(s):                    64-127,192-255\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==2.1.3\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.0a0+git295f2ed\n[pip3] torchvision==0.21.0+7af6987\n[pip3] transformers==4.51.1\n[pip3] triton==3.2.0+gite5be006a\n[conda] Could not collect\nROCM Version: 6.3.42133-1b9c17779\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3rc2.dev77+g2976dc27e.d20250409\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n============================ ROCm System Management Interface ============================\n================================ Weight between two GPUs =================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            15           15           15           15           15           15           15\nGPU1   15           0            15           15           15           15           15           15\nGPU2   15           15           0            15           15           15           15           15\nGPU3   15           15           15           0            15           15           15           15\nGPU4   15           15           15           15           0            15           15           15\nGPU5   15           15           15           15           15           0            15           15\nGPU6   15           15           15           15           15           15           0            15\nGPU7   15           15           15           15           15           15           15           0\n\n================================= Hops between two GPUs ==================================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            1            1            1            1            1            1            1\nGPU1   1            0            1            1            1            1            1            1\nGPU2   1            1            0            1            1            1            1            1\nGPU3   1            1            1            0            1            1            1            1\nGPU4   1            1            1            1            0            1            1            1\nGPU5   1            1            1            1            1            0            1            1\nGPU6   1            1            1            1            1            1            0            1\nGPU7   1            1            1            1            1            1            1            0\n\n=============================== Link Type between two GPUs ===============================\n       GPU0         GPU1         GPU2         GPU3         GPU4         GPU5         GPU6         GPU7\nGPU0   0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU1   XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI         XGMI\nGPU2   XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI         XGMI\nGPU3   XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI         XGMI\nGPU4   XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI         XGMI\nGPU5   XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI         XGMI\nGPU6   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0            XGMI\nGPU7   XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         XGMI         0\n\n======================================= Numa Nodes =======================================\nGPU[0]\t\t: (Topology) Numa Node: 0\nGPU[0]\t\t: (Topology) Numa Affinity: 0\nGPU[1]\t\t: (Topology) Numa Node: 0\nGPU[1]\t\t: (Topology) Numa Affinity: 0\nGPU[2]\t\t: (Topology) Numa Node: 0\nGPU[2]\t\t: (Topology) Numa Affinity: 0\nGPU[3]\t\t: (Topology) Numa Node: 0\nGPU[3]\t\t: (Topology) Numa Affinity: 0\nGPU[4]\t\t: (Topology) Numa Node: 1\nGPU[4]\t\t: (Topology) Numa Affinity: 1\nGPU[5]\t\t: (Topology) Numa Node: 1\nGPU[5]\t\t: (Topology) Numa Affinity: 1\nGPU[6]\t\t: (Topology) Numa Node: 1\nGPU[6]\t\t: (Topology) Numa Affinity: 1\nGPU[7]\t\t: (Topology) Numa Node: 1\nGPU[7]\t\t: (Topology) Numa Affinity: 1\n================================== End of ROCm SMI Log ===================================\n\nPYTORCH_ROCM_ARCH=gfx942\nMAX_JOBS=30\nLD_LIBRARY_PATH=/opt/rocm/lib:/usr/local/lib:\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nI\u2019m encountering an issue when running the FP8 version of the meta-llama/Llama-4-Maverick-17B-128E-Instruct model (i.e. meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8) on 4 Mi300x GPUs. According to the [vllm blog](https://blog.vllm.ai/2025/04/05/llama4), it should be possible to run this model on a Mi300x GPU, but the configuration provided in the blog uses a tensor parallelism of 8-GPU . \n\nWhen I set tensor parallel size to 4 but in a fp8 model, I get the following error: \n\n```\n VLLM_DISABLE_COMPILE_CACHE=1 vllm serve meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8  --tensor-parallel-size 4 --max-model-len 430000 --download-dir /app/data/models/ --kv-cache-dtype fp8\n```\noutput:\n```text\nINFO 04-11 09:53:14 [__init__.py:239] Automatically detected platform rocm.\nINFO 04-11 09:53:16 [api_server.py:1034] vLLM API server version 0.8.3rc2.dev77+g2976dc27e.d20250409\nINFO 04-11 09:53:16 [api_server.py:1035] args: Namespace(subparser='serve', model_tag='meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir='/app/data/models/', load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='fp8', max_model_len=430000, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=4, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_chunked_mm_input=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x7f4d7769bd80>)\nINFO 04-11 09:53:28 [config.py:604] This model supports multiple tasks: {'score', 'embed', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.\nWARNING 04-11 09:53:34 [arg_utils.py:1746] --kv-cache-dtype is not supported by the V1 Engine. Falling back to V0.\nWARNING 04-11 09:53:34 [arg_utils.py:1618] The model has a long context length (430000). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.\nINFO 04-11 09:53:34 [config.py:1231] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor\nINFO 04-11 09:53:34 [config.py:1609] Defaulting to use mp for distributed inference\nINFO 04-11 09:53:34 [api_server.py:246] Started engine process with PID 262\nINFO 04-11 09:53:37 [__init__.py:239] Automatically detected platform rocm.\nINFO 04-11 09:53:44 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.3rc2.dev77+g2976dc27e.d20250409) with config: model='meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', speculative_config=None, tokenizer='meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=430000, download_dir='/app/data/models/', load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=fp8,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True,\nWARNING 04-11 09:53:45 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 04-11 09:53:45 [rocm.py:153] None is not supported in AMD GPUs.\nINFO 04-11 09:53:45 [rocm.py:154] Using ROCmFlashAttention backend.\nINFO 04-11 09:53:48 [__init__.py:239] Automatically detected platform rocm.\nINFO 04-11 09:53:48 [__init__.py:239] Automatically detected platform rocm.\nINFO 04-11 09:53:48 [__init__.py:239] Automatically detected platform rocm.\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:55 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:55 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:56 [rocm.py:153] None is not supported in AMD GPUs.\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:56 [rocm.py:154] Using ROCmFlashAttention backend.\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:56 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:56 [rocm.py:153] None is not supported in AMD GPUs.\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:56 [rocm.py:154] Using ROCmFlashAttention backend.\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:56 [rocm.py:153] None is not supported in AMD GPUs.\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:56 [rocm.py:154] Using ROCmFlashAttention backend.\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:56 [utils.py:990] Found nccl from library librccl.so.1\nINFO 04-11 09:53:56 [utils.py:990] Found nccl from library librccl.so.1\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:56 [pynccl.py:69] vLLM is using nccl==2.21.5\nINFO 04-11 09:53:56 [pynccl.py:69] vLLM is using nccl==2.21.5\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:56 [utils.py:990] Found nccl from library librccl.so.1\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:56 [utils.py:990] Found nccl from library librccl.so.1\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:56 [pynccl.py:69] vLLM is using nccl==2.21.5\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:56 [pynccl.py:69] vLLM is using nccl==2.21.5\nINFO 04-11 09:53:57 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_cb7896c1'), local_subscribe_addr='ipc:///tmp/2bc5668d-2962-4291-81d6-857ec8d5f573', remote_subscribe_addr=None, remote_addr_ipv6=False)\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:57 [parallel_state.py:957] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:57 [parallel_state.py:957] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1\nINFO 04-11 09:53:57 [parallel_state.py:957] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:57 [parallel_state.py:957] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2\nINFO 04-11 09:53:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8...\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8...\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8...\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8...\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:57 [rocm.py:153] None is not supported in AMD GPUs.\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:57 [rocm.py:154] Using ROCmFlashAttention backend.\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:57 [rocm.py:153] None is not supported in AMD GPUs.\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:57 [rocm.py:154] Using ROCmFlashAttention backend.\nINFO 04-11 09:53:57 [rocm.py:153] None is not supported in AMD GPUs.\nINFO 04-11 09:53:57 [rocm.py:154] Using ROCmFlashAttention backend.\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:57 [rocm.py:153] None is not supported in AMD GPUs.\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:57 [rocm.py:154] Using ROCmFlashAttention backend.\n(VllmWorkerProcess pid=337) INFO 04-11 09:53:57 [config.py:3351] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]\nINFO 04-11 09:53:57 [config.py:3351] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]\n(VllmWorkerProcess pid=338) INFO 04-11 09:53:57 [config.py:3351] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]\n(VllmWorkerProcess pid=336) INFO 04-11 09:53:57 [config.py:3351] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]\nWARNING 04-11 09:53:57 [rocm_flash_attn.py:468] Using irope in ROCm Flash Attention is not supported yet, it will fail back to global attention for long context.\nWARNING 04-11 09:53:57 [rocm_flash_attn.py:475] Using irope in V0 is not supported yet, it will fall back to global attention for long context.\nWARNING 04-11 09:53:57 [rocm_flash_attn.py:475] Using irope in V0 is not supported yet, it will fall back to global attention for long context.\n(VllmWorkerProcess pid=337) WARNING 04-11 09:53:57 [rocm_flash_attn.py:468] Using irope in ROCm Flash Attention is not supported yet, it will fail back to global attention for long context.\n(VllmWorkerProcess pid=337) WARNING 04-11 09:53:57 [rocm_flash_attn.py:475] Using irope in V0 is not supported yet, it will fall back to global attention for long context.\n(VllmWorkerProcess pid=336) WARNING 04-11 09:53:57 [rocm_flash_attn.py:468] Using irope in ROCm Flash Attention is not supported yet, it will fail back to global attention for long context.\n(VllmWorkerProcess pid=336) WARNING 04-11 09:53:57 [rocm_flash_attn.py:475] Using irope in V0 is not supported yet, it will fall back to global attention for long context.\n(VllmWorkerProcess pid=337) WARNING 04-11 09:53:57 [rocm_flash_attn.py:475] Using irope in V0 is not supported yet, it will fall back to global attention for long context.\n(VllmWorkerProcess pid=336) WARNING 04-11 09:53:57 [rocm_flash_attn.py:475] Using irope in V0 is not supported yet, it will fall back to global attention for long context.\n(VllmWorkerProcess pid=338) WARNING 04-11 09:53:57 [rocm_flash_attn.py:468] Using irope in ROCm Flash Attention is not supported yet, it will fail back to global attention for long context.\n(VllmWorkerProcess pid=338) WARNING 04-11 09:53:57 [rocm_flash_attn.py:475] Using irope in V0 is not supported yet, it will fall back to global attention for long context.\n(VllmWorkerProcess pid=338) WARNING 04-11 09:53:58 [rocm_flash_attn.py:475] Using irope in V0 is not supported yet, it will fall back to global attention for long context.\nERROR 04-11 09:53:58 [engine.py:448] For FP8 Fused MoE layers, only per-tensor scales for weights and activations are supported. Found num_bits=8 type='float' symmetric=True group_size=None strategy='channel' block_structure=None dynamic=False actorder=None observer='minmax' observer_kwargs={}, num_bits=8 type='float' symmetric=True group_size=None strategy='token' block_structure=None dynamic=True actorder=None observer=None observer_kwargs={}\nERROR 04-11 09:53:58 [engine.py:448] Traceback (most recent call last):\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 436, in run_mp_engine\nERROR 04-11 09:53:58 [engine.py:448]     engine = MQLLMEngine.from_vllm_config(\nERROR 04-11 09:53:58 [engine.py:448]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 128, in from_vllm_config\nERROR 04-11 09:53:58 [engine.py:448]     return cls(\nERROR 04-11 09:53:58 [engine.py:448]            ^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 82, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     self.engine = LLMEngine(*args, **kwargs)\nERROR 04-11 09:53:58 [engine.py:448]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 282, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     self.model_executor = executor_class(vllm_config=vllm_config, )\nERROR 04-11 09:53:58 [engine.py:448]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 286, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     super().__init__(*args, **kwargs)\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 52, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     self._init_executor()\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/mp_distributed_executor.py\", line 125, in _init_executor\nERROR 04-11 09:53:58 [engine.py:448]     self._run_workers(\"load_model\",\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/mp_distributed_executor.py\", line 185, in _run_workers\nERROR 04-11 09:53:58 [engine.py:448]     driver_worker_output = run_method(self.driver_worker, sent_method,\nERROR 04-11 09:53:58 [engine.py:448]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2363, in run_method\nERROR 04-11 09:53:58 [engine.py:448]     return func(*args, **kwargs)\nERROR 04-11 09:53:58 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 183, in load_model\nERROR 04-11 09:53:58 [engine.py:448]     self.model_runner.load_model()\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1113, in load_model\nERROR 04-11 09:53:58 [engine.py:448]     self.model = get_model(vllm_config=self.vllm_config)\nERROR 04-11 09:53:58 [engine.py:448]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\nERROR 04-11 09:53:58 [engine.py:448]     return loader.load_model(vllm_config=vllm_config)\nERROR 04-11 09:53:58 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 452, in load_model\nERROR 04-11 09:53:58 [engine.py:448]     model = _initialize_model(vllm_config=vllm_config)\nERROR 04-11 09:53:58 [engine.py:448]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\nERROR 04-11 09:53:58 [engine.py:448]     return model_class(vllm_config=vllm_config, prefix=prefix)\nERROR 04-11 09:53:58 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/mllama4.py\", line 691, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     self.language_model = _initialize_model(\nERROR 04-11 09:53:58 [engine.py:448]                           ^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\nERROR 04-11 09:53:58 [engine.py:448]     return model_class(vllm_config=vllm_config, prefix=prefix)\nERROR 04-11 09:53:58 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 481, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     super().__init__(vllm_config=vllm_config,\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 486, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     self.model = self._init_model(vllm_config=vllm_config,\nERROR 04-11 09:53:58 [engine.py:448]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 489, in _init_model\nERROR 04-11 09:53:58 [engine.py:448]     return Llama4Model(vllm_config=vllm_config,\nERROR 04-11 09:53:58 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 151, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 335, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     super().__init__(vllm_config=vllm_config,\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 151, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 321, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     self.start_layer, self.end_layer, self.layers = make_layers(\nERROR 04-11 09:53:58 [engine.py:448]                                                     ^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 610, in make_layers\nERROR 04-11 09:53:58 [engine.py:448]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\nERROR 04-11 09:53:58 [engine.py:448]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 323, in <lambda>\nERROR 04-11 09:53:58 [engine.py:448]     lambda prefix: layer_type(config=config,\nERROR 04-11 09:53:58 [engine.py:448]                    ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 284, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     self.feed_forward = Llama4MoE(\nERROR 04-11 09:53:58 [engine.py:448]                         ^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 73, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     self.experts = FusedMoE(\nERROR 04-11 09:53:58 [engine.py:448]                    ^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 501, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     self.quant_method = quant_config.get_quant_method(self, prefix)\nERROR 04-11 09:53:58 [engine.py:448]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py\", line 99, in get_quant_method\nERROR 04-11 09:53:58 [engine.py:448]     return CompressedTensorsMoEMethod.get_moe_method(\nERROR 04-11 09:53:58 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py\", line 59, in get_moe_method\nERROR 04-11 09:53:58 [engine.py:448]     return CompressedTensorsW8A8Fp8MoEMethod(quant_config)\nERROR 04-11 09:53:58 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 04-11 09:53:58 [engine.py:448]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py\", line 79, in __init__\nERROR 04-11 09:53:58 [engine.py:448]     raise ValueError(\nERROR 04-11 09:53:58 [engine.py:448] ValueError: For FP8 Fused MoE layers, only per-tensor scales for weights and activations are supported. Found num_bits=8 type='float' symmetric=True group_size=None strategy='channel' block_structure=None dynamic=False actorder=None observer='minmax' observer_kwargs={}, num_bits=8 type='float' symmetric=True group_size=None strategy='token' block_structure=None dynamic=True actorder=None observer=None observer_kwargs={}\nERROR 04-11 09:53:58 [multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 338 died, exit code: -15\nINFO 04-11 09:53:58 [multiproc_worker_utils.py:124] Killing local vLLM worker processes\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 450, in run_mp_engine\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 436, in run_mp_engine\n    engine = MQLLMEngine.from_vllm_config(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 128, in from_vllm_config\n    return cls(\n           ^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 82, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 282, in __init__\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 286, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 52, in __init__\n    self._init_executor()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/mp_distributed_executor.py\", line 125, in _init_executor\n    self._run_workers(\"load_model\",\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/mp_distributed_executor.py\", line 185, in _run_workers\n    driver_worker_output = run_method(self.driver_worker, sent_method,\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/utils.py\", line 2363, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 183, in load_model\n    self.model_runner.load_model()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1113, in load_model\n    self.model = get_model(vllm_config=self.vllm_config)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n    return loader.load_model(vllm_config=vllm_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 452, in load_model\n    model = _initialize_model(vllm_config=vllm_config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\n    return model_class(vllm_config=vllm_config, prefix=prefix)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/mllama4.py\", line 691, in __init__\n    self.language_model = _initialize_model(\n                          ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\n    return model_class(vllm_config=vllm_config, prefix=prefix)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 481, in __init__\n    super().__init__(vllm_config=vllm_config,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 486, in __init__\n    self.model = self._init_model(vllm_config=vllm_config,\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 489, in _init_model\n    return Llama4Model(vllm_config=vllm_config,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 151, in __init__\n    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 335, in __init__\n    super().__init__(vllm_config=vllm_config,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 151, in __init__\n    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 321, in __init__\n    self.start_layer, self.end_layer, self.layers = make_layers(\n                                                    ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 610, in make_layers\n    maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 323, in <lambda>\n    lambda prefix: layer_type(config=config,\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 284, in __init__\n    self.feed_forward = Llama4MoE(\n                        ^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 73, in __init__\n    self.experts = FusedMoE(\n                   ^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 501, in __init__\n    self.quant_method = quant_config.get_quant_method(self, prefix)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py\", line 99, in get_quant_method\n    return CompressedTensorsMoEMethod.get_moe_method(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py\", line 59, in get_moe_method\n    return CompressedTensorsW8A8Fp8MoEMethod(quant_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py\", line 79, in __init__\n    raise ValueError(\nValueError: For FP8 Fused MoE layers, only per-tensor scales for weights and activations are supported. Found num_bits=8 type='float' symmetric=True group_size=None strategy='channel' block_structure=None dynamic=False actorder=None observer='minmax' observer_kwargs={}, num_bits=8 type='float' symmetric=True group_size=None strategy='token' block_structure=None dynamic=True actorder=None observer=None observer_kwargs={}\n[rank0]:[W411 09:53:58.829246061 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nTraceback (most recent call last):\n  File \"/usr/local/bin/vllm\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/main.py\", line 51, in main\n    args.dispatch_function(args)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/serve.py\", line 27, in cmd\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1069, in run_server\n    async with build_async_engine_client(args) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 146, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 269, in build_async_engine_client_from_engine_args\n    raise RuntimeError(\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\n/usr/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n```\n\ntldr:\n```\nValueError: For FP8 Fused MoE layers, only per-tensor scales for weights and activations are supported. Found num_bits=8 type='float' symmetric=True group_size=None strategy='channel' block_structure=None dynamic=False actorder=None observer='minmax' observer_kwargs={}, num_bits=8 type='float' symmetric=True group_size=None strategy='token' block_structure=None dynamic=True actorder=None observer=None observer_kwargs={}\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2025-04-11T10:27:28+00:00",
    "closed_at": "2025-04-23T12:07:16+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16474/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16474"
  },
  {
    "number": 20125,
    "title": "[Bug][Rocm] Garbage Response from vLLM When Using Tensor Parallelism on AMD CPX/NPS4 Partitioned GPUs",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nI have attached output file.\n```\n\n[vllm_collect_env_output.txt](https://github.com/user-attachments/files/20926332/vllm_collect_env_output.txt)\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n**Steps to reproduce:**\nWe referred to doc:  [Steps to Run a vLLM Workload on AMD partition](https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/gpu-partitioning/mi300x/run-vllm.html).\n- [ ] **Do CPS/NPS4 Partition**\n`sudo amd-smi set --memory-partition NPS4`\n\n\n- [ ] **Launch container**\n`docker run -it --network=host --group-add=video --ipc=host --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --device /dev/kfd --device /dev/dri rocm/vllm:latest /bin/bash`\n\n\n - [ ] **Set Env**\n```\nexport HF_TOKEN=<token>\nexport HIP_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n```\n\n\n- [ ] `vllm serve meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 8`\n\n\n- [ ] **Query the model**\n```\ncurl http://localhost:8000/v1/chat/completions \\\n  -H 'Content-Type: application/json' \\\n  --data '{\n    \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What is Deep Learning?\"}\n    ]\n  }'\n```\n\n**Actual behaviour:**\nGarbled and nonsensical output as below\n\n{\"id\":\"chatcmpl-5488b13e1910409d884196f041b34b0b\",\"object\":\"chat.completion\",\"created\":1750923599,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"reasoning_content\":null,\"content\":\"Deep c\u00e9libai8://%) the:// Bon the:// Bachelor the://\n\u2026\n\u0434\u043e\u0441\u0442:// False capital://{ progress:// Barb n\u7a0b\ufffd\u7a0b-disable\",\"tool_calls\":[]},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":128001}],\"usage\":{\"prompt_tokens\":46,\"total_tokens\":4602,\"completion_tokens\":4556,\"prompt_tokens_details\":null},\"prompt_logprobs\":null,\"kv_transfer_params\":null}\n\n**Expected behaviour:**\nMeaningful Response\n\n\n**Additional Information:**\n\n1. **vLLM works as expected with a single CPX partition.**\n**How to reproduce:**\n \n\n- [ ] Launch container with only 1 CPX partition (/dev/dri/renderD128 )\n\n`docker run -it --network=host --group-add=video --ipc=host --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --device /dev/kfd --device=/dev/dri/renderD128  rocm/vllm:latest /bin/bash`\n\n \n\n- [ ] `vllm serve meta-llama/Llama-3.1-8B-Instruct --max-model-len 2048`\n\n\n- [ ] Query the model as above.\n           \n\n2. **This likely isn't a ROCm or hardware issue, but possibly a vLLM backend issue. We did some sanity checks and Validation to rule out issues from Hardware, ROCm, PyTorch, RCCL:**\n\n- [ ] **Vanilla PyTorch matrix multiplication and all-reduce operation gives the correct result.**\nHere every process creates a 2x2 matrix on its own GPU, performs matrix multiplication,and then performs an all-reduce operation, which communicates the results across all GPUs. We verified the calculation and it is correct. This means there should not be any issues in accuracy in distributed setup.\n\n- [ ] **Launch process on each partition using mpirun**\nmpirun --allow-run-as-root \\\n  -np 1 -x ROCR_VISIBLE_DEVICES=0 python mpi_test.py \\\n  -np 1 -x ROCR_VISIBLE_DEVICES=1 python mpi_test.py\nEach MPI rank sees only the partition assigned to it via      ROCR_VISIBLE_DEVICES. This further reinforces that partition isolation is functioning.\n\n- [ ] **Finally conducted rccl-tests with 8 partitions and this works too.**\n\n```\n./build/all_reduce_perf -b 2M -e 8M -f 2 -g 8\n# nThread 1 nGpus 8 minBytes 2097152 maxBytes 8388608 step: 2(factor) warmup iters: 5 iters: 20 agg iters: 1 validation: 1 graph: 0\n#\n\nrccl-tests: Version develop:b0a3841\n# Using devices\n#  Rank  0 Group  0 Pid   1158 on ENC1-CLS01-SVR07 device  0 [0000:1b:00] AMD Instinct MI300X\n#  Rank  1 Group  0 Pid   1158 on ENC1-CLS01-SVR07 device  1 [0000:1b:00] AMD Instinct MI300X\n#  Rank  2 Group  0 Pid   1158 on ENC1-CLS01-SVR07 device  2 [0000:1b:00] AMD Instinct MI300X\n#  Rank  3 Group  0 Pid   1158 on ENC1-CLS01-SVR07 device  3 [0000:1b:00] AMD Instinct MI300X\n#  Rank  4 Group  0 Pid   1158 on ENC1-CLS01-SVR07 device  4 [0000:1b:00] AMD Instinct MI300X\n#  Rank  5 Group  0 Pid   1158 on ENC1-CLS01-SVR07 device  5 [0000:1b:00] AMD Instinct MI300X\n#  Rank  6 Group  0 Pid   1158 on ENC1-CLS01-SVR07 device  6 [0000:1b:00] AMD Instinct MI300X\n#  Rank  7 Group  0 Pid   1158 on ENC1-CLS01-SVR07 device  7 [0000:1b:00] AMD Instinct MI300X\n#\n#                                                              out-of-place                       in-place          \n#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong\n#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)       \n     2097152        524288     float     sum      -1    75.02   27.95   48.92      0    72.64   28.87   50.52      0\n     4194304       1048576     float     sum      -1    93.79   44.72   78.26      0    89.37   46.93   82.13      0\n     8388608       2097152     float     sum      -1    165.7   50.62   88.59      0    159.6   52.57   92.00      0\n# Errors with asterisks indicate errors that have exceeded the maximum threshold.\n# Out of bounds values : 0 OK\n# Avg bus bandwidth    : 73.4045\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "open",
    "created_at": "2025-06-26T13:18:47+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20125/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20125"
  },
  {
    "number": 11249,
    "title": "[Bug]: ROCM with AWQ",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.6.0.dev20241113+rocm6.2\r\nIs debug build: False\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: 6.2.41133-dd7f95766\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: 18.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.2.0 24292 26466ce804ac523b398608f17388eb6d605a3f09)\r\nCMake version: version 3.26.4\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.8.0-50-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: Radeon RX 7900 XTX (gfx1100)\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: 6.2.41133\r\nMIOpen runtime version: 3.2.0\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nByte Order:                           Little Endian\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nCPU(s):                               48\r\nOn-line CPU(s) list:                  0-47\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   24\r\nSocket(s):                            1\r\nNUMA node(s):                         1\r\nVendor ID:                            AuthenticAMD\r\nCPU family:                           25\r\nModel:                                8\r\nModel name:                           AMD Ryzen Threadripper PRO 5965WX 24-Cores\r\nStepping:                             2\r\nFrequency boost:                      enabled\r\nCPU MHz:                              1800.000\r\nCPU max MHz:                          7021.0928\r\nCPU min MHz:                          1800.0000\r\nBogoMIPS:                             7585.77\r\nVirtualization:                       AMD-V\r\nL1d cache:                            768 KiB\r\nL1i cache:                            768 KiB\r\nL2 cache:                             12 MiB\r\nL3 cache:                             128 MiB\r\nNUMA node0 CPU(s):                    0-47\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm debug_swap\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.8.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] optree==0.9.1\r\n[pip3] pytorch-triton-rocm==3.1.0+cf34004b8a\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.6.0.dev20241113+rocm6.2\r\n[pip3] torchvision==0.20.0.dev20241113+rocm6.2\r\n[pip3] transformers==4.47.0\r\n[pip3] triton==3.0.0\r\n[conda] No relevant packages\r\nROCM Version: 6.2.41133-dd7f95766\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.4.post2.dev383+g2ca830db\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\n============================ ROCm System Management Interface ============================\r\n================================ Weight between two GPUs =================================\r\n       GPU0         GPU1\r\nGPU0   0            40\r\nGPU1   40           0\r\n\r\n================================= Hops between two GPUs ==================================\r\n       GPU0         GPU1\r\nGPU0   0            2\r\nGPU1   2            0\r\n\r\n=============================== Link Type between two GPUs ===============================\r\n       GPU0         GPU1\r\nGPU0   0            PCIE\r\nGPU1   PCIE         0\r\n\r\n======================================= Numa Nodes =======================================\r\nGPU[0]\t\t: (Topology) Numa Node: 0\r\nGPU[0]\t\t: (Topology) Numa Affinity: -1\r\nGPU[1]\t\t: (Topology) Numa Node: 0\r\nGPU[1]\t\t: (Topology) Numa Affinity: -1\r\n================================== End of ROCm SMI Log ===================================\r\n\r\nPYTORCH_TESTING_DEVICE_ONLY_FOR=cuda\r\nVLLM_WORKER_MULTIPROC_METHOD=spawn\r\nPYTORCH_TEST_WITH_ROCM=1\r\nPYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\r\nPYTORCH_ROCM_ARCH=gfx908;gfx90a;gfx942;gfx1100\r\nVLLM_DO_NOT_TRACK=1\r\nMAX_JOBS=8\r\nLD_LIBRARY_PATH=/opt/conda/envs/py_3.9/lib/python3.9/site-packages/cv2/../../lib64:/opt/ompi/lib:/opt/rocm/lib:/usr/local/lib::/opt/rocm/lib/:/libtorch/lib:\r\nVLLM_NO_USAGE_STATS=1\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nUsing the latest vllm from git, I've created a vllm-rocm image based off the Dockerfile.rocm.\r\n\r\nI proceeded to try to run an AWQ model like so:\r\n```\r\nvllm serve --model casperhansen/llama-3.3-70b-instruct-awq --served-model-name Llama-3.3-70B-Instruct --enforce-eager --quantization awq --max_model_len 8192 --gpu-memory-utilization 0.95 --tensor-parallel-size 2\r\n```\r\n\r\nbut getting this error:\r\n\r\n<details>\r\n\r\n```text\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 _custom_ops.py:61] Error in calling custom op awq_dequantize: '_OpNamespace' '_C' object has no attribute 'awq_dequantize'\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 _custom_ops.py:61] Possibly you have built or installed an obsolete version of vllm.\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 _custom_ops.py:61] Please try a clean build and install of vllm,or remove old built files such as vllm/*cpython*.so and build/ .\r\n(VllmWorkerProcess pid=1492) INFO 12-17 03:22:04 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20241217-032204.pkl...\r\n(VllmWorkerProcess pid=1492) INFO 12-17 03:22:04 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20241217-032204.pkl.\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236] Exception in worker VllmWorkerProcess while processing method determine_num_available_blocks.\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236] Traceback (most recent call last):\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/worker/model_runner_base.py\", line 116, in _wrapper\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/worker/model_runner.py\", line 1683, in execute_model\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     hidden_or_intermediate_states = model_executable(\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1740, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1751, in _call_impl\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/model_executor/models/llama.py\", line 568, in forward\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     model_output = self.model(input_ids, positions, kv_caches,\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/compilation/decorators.py\", line 168, in __call__\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return self.forward(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/model_executor/models/llama.py\", line 360, in forward\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     hidden_states, residual = layer(positions, hidden_states,\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1740, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1751, in _call_impl\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/model_executor/models/llama.py\", line 275, in forward\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     hidden_states = self.self_attn(positions=positions,\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1740, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1751, in _call_impl\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/model_executor/models/llama.py\", line 202, in forward\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     qkv, _ = self.qkv_proj(hidden_states)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1740, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1751, in _call_impl\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/model_executor/layers/linear.py\", line 373, in forward\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     output_parallel = self.quant_method.apply(self, input_, bias)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/model_executor/layers/quantization/awq.py\", line 174, in apply\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     out = ops.awq_dequantize(qweight, scales, qzeros, 0, 0, 0)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/_custom_ops.py\", line 62, in wrapper\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     raise e\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/_custom_ops.py\", line 44, in wrapper\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return fn(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/_custom_ops.py\", line 281, in awq_dequantize\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return torch.ops._C.awq_dequantize(qweight, scales, zeros, split_k_iters,\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/_ops.py\", line 1232, in __getattr__\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     raise AttributeError(\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236] AttributeError: '_OpNamespace' '_C' object has no attribute 'awq_dequantize'\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236] The above exception was the direct cause of the following exception:\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236] Traceback (most recent call last):\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/executor/multiproc_worker_utils.py\", line 230, in _run_worker_process\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/worker/worker.py\", line 199, in determine_num_available_blocks\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     self.model_runner.profile_run()\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/worker/model_runner.py\", line 1329, in profile_run\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]   File \"/vllm-workspace/vllm/worker/model_runner_base.py\", line 152, in _wrapper\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236]     raise type(err)(\r\n(VllmWorkerProcess pid=1492) ERROR 12-17 03:22:04 multiproc_worker_utils.py:236] AttributeError: Error in model execution (input dumped to /tmp/err_execute_model_input_20241217-032204.pkl): '_OpNamespace' '_C' object has no attribute 'awq_dequantize'\r\n/vllm-workspace/vllm/model_executor/layers/quantization/awq.py:175: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:296.)\r\n  out = torch.matmul(reshaped_x, out)\r\n```\r\n\r\n</details>\r\n\r\n Tried searching but couldn't find anything related to this. I tried installing autoawq inside the container with:\r\n \r\n```\r\npip install autoawq\r\n```\r\n \r\n but this didn't get me anywhere. I read the docs and https://rocm.docs.amd.com/en/latest/how-to/tuning-guides/mi300x/workload.html#awq-quantization seems to suggest AWQ should work out of the box but I havent had any success. I do have this working on the CUDA version using `docker.io/vllm/vllm-openai:latest` but trying to get a rocm version going. Any ideas what im doing wrong?\r\n",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2024-12-17T03:37:54+00:00",
    "closed_at": "2024-12-18T02:57:04+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11249/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11249"
  },
  {
    "number": 16354,
    "title": "[Feature]: Benchmarks for audio models",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\n- Add audio datasets to `benchmarks/benchmark_dataset.py` to so we can run performance benchmarks on audio models as well.\n- Add a benchmark similar to MMMU (#11196) but for audio models to evaluate their correctness.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\ncc @mgoin @ywang96 \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "help wanted",
      "feature request",
      "multi-modality"
    ],
    "state": "closed",
    "created_at": "2025-04-09T16:55:19+00:00",
    "closed_at": "2025-04-19T09:24:15+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16354/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/16354"
  },
  {
    "number": 14002,
    "title": "[Feature]: Implement Priority Scheduling In V1 Engine",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nIn V0, we support request priority. I would like to see this in V1\n\ncc @WoosukKwon \n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "help wanted",
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-02-28T01:33:35+00:00",
    "closed_at": "2025-06-23T03:18:09+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14002/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14002"
  },
  {
    "number": 4694,
    "title": "[Feature]: bind python and c++ through tools other than pybind11",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nAs vLLM goes into a fast release schedule (currently one release every two weeks), we will quickly hit the project-wide limit of pypi (around 5GB per project). One solution, as pointed out in https://github.com/pypi/support/issues/3792#issuecomment-2099941677 , is to build one wheel for all python versions (Python 3.8+).\r\n\r\nI have figured out the procedure https://github.com/pypi/support/issues/3792#issuecomment-2101360740 , but pybind11 does not support this Python Limited API protocol.\r\n\r\nOne possible solution is to replace pybind11 with some other tools, so that the binding procedure can be used with Python Limited API.\r\n\r\nPossible solutions:\r\n\r\n- Nanobind (seems to support it starting from Python 3.12 only: https://github.com/wjakob/nanobind/pull/561 )\r\n- register ops through pytorch directly https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_",
    "labels": [
      "help wanted",
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-08T23:18:56+00:00",
    "closed_at": "2024-10-27T22:53:29+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4694/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/4694"
  }
]
[
  {
    "number": 6199,
    "title": "[Bug]: safetensor format support for Mistral-7B-v0.3",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.3\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-176-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 11.8.89\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA A100-SXM4-40GB\r\nGPU 1: NVIDIA A100-SXM4-40GB\r\nGPU 2: NVIDIA A100-SXM4-40GB\r\nGPU 3: NVIDIA A100-SXM4-40GB\r\nGPU 4: NVIDIA A100-SXM4-40GB\r\nGPU 5: NVIDIA A100-SXM4-40GB\r\nGPU 6: NVIDIA A100-SXM4-40GB\r\nGPU 7: NVIDIA A100-SXM4-40GB\r\n\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      43 bits physical, 48 bits virtual\r\nCPU(s):                             256\r\nOn-line CPU(s) list:                0-255\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 64\r\nSocket(s):                          2\r\nNUMA node(s):                       8\r\nVendor ID:                          AuthenticAMD\r\nCPU family:                         23\r\nModel:                              49\r\nModel name:                         AMD EPYC 7742 64-Core Processor\r\nStepping:                           0\r\nFrequency boost:                    enabled\r\nCPU MHz:                            3402.618\r\nCPU max MHz:                        2250.0000\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           4491.57\r\nVirtualization:                     AMD-V\r\nL1d cache:                          4 MiB\r\nL1i cache:                          4 MiB\r\nL2 cache:                           64 MiB\r\nL3 cache:                           512 MiB\r\nNUMA node0 CPU(s):                  0-15,128-143\r\nNUMA node1 CPU(s):                  16-31,144-159\r\nNUMA node2 CPU(s):                  32-47,160-175\r\nNUMA node3 CPU(s):                  48-63,176-191\r\nNUMA node4 CPU(s):                  64-79,192-207\r\nNUMA node5 CPU(s):                  80-95,208-223\r\nNUMA node6 CPU(s):                  96-111,224-239\r\nNUMA node7 CPU(s):                  112-127,240-255\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Vulnerable\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip rdpid overflow_recov succor smca sme sev sev_es\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] torchmetrics==1.4.0.post0\r\n[pip3] torchvision==0.18.0\r\n[pip3] transformers==4.42.3\r\n[pip3] transformers-stream-generator==0.0.5\r\n[pip3] triton==2.3.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] torchmetrics              1.4.0.post0              pypi_0    pypi\r\n[conda] torchvision               0.18.0                   pypi_0    pypi\r\n[conda] transformers              4.42.3                   pypi_0    pypi\r\n[conda] transformers-stream-generator 0.0.5                    pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tNIC2\tNIC3\tNIC4\tNIC5\tNIC6\tNIC7\tNIC8\tNIC9\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t48-63,176-191\t3\t\tN/A\r\nGPU1\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t48-63,176-191\t3\t\tN/A\r\nGPU2\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t16-31,144-159\t1\t\tN/A\r\nGPU3\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t16-31,144-159\t1\t\tN/A\r\nGPU4\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\t112-127,240-255\t7\t\tN/A\r\nGPU5\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\t112-127,240-255\t7\t\tN/A\r\nGPU6\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\t80-95,208-223\t5\t\tN/A\r\nGPU7\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\t80-95,208-223\t5\t\tN/A\r\nNIC0\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\r\nNIC1\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\t X \tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\r\nNIC2\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\r\nNIC3\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\t X \tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\r\nNIC4\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPXB\tSYS\tSYS\tSYS\tSYS\r\nNIC5\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\t X \tSYS\tSYS\tSYS\tSYS\r\nNIC6\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPXB\tSYS\tSYS\r\nNIC7\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\tPXB\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPXB\t X \tSYS\tSYS\r\nNIC8\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPIX\r\nNIC9\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPIX\t X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  NIC8: mlx5_8\r\n  NIC9: mlx5_9\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nI've downloaded `model-0000{1,2,3}-of-00003.safetensors`.\r\nBut when I was loading the model with vllm, I still need to download `consolidated.safetensors`. The case didn't show on other model series, so I think it's a bug.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-08T02:40:38+00:00",
    "closed_at": "2024-11-25T02:05:13+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6199/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6199"
  },
  {
    "number": 4247,
    "title": "[Bug]: KeyError: 'model.layers.45.block_sparse_moe.gate.g_idx'",
    "body": "### Your current environment\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.2.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-25-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.99\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA GeForce RTX 2080 Ti\r\nGPU 1: NVIDIA GeForce RTX 2080 Ti\r\nGPU 2: NVIDIA GeForce RTX 2080 Ti\r\nGPU 3: NVIDIA GeForce RTX 2080 Ti\r\nGPU 4: NVIDIA GeForce RTX 2080 Ti\r\nGPU 5: NVIDIA GeForce RTX 2080 Ti\r\nGPU 6: NVIDIA GeForce RTX 2080 Ti\r\nGPU 7: NVIDIA GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 535.161.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\n\u67b6\u6784\uff1a                              x86_64\r\nCPU \u8fd0\u884c\u6a21\u5f0f\uff1a                      32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\n\u5b57\u8282\u5e8f\uff1a                            Little Endian\r\nCPU:                                48\r\n\u5728\u7ebf CPU \u5217\u8868\uff1a                     0-47\r\n\u5382\u5546 ID\uff1a                           GenuineIntel\r\n\u578b\u53f7\u540d\u79f0\uff1a                          Intel(R) Xeon(R) Gold 6136 CPU @ 3.00GHz\r\nCPU \u7cfb\u5217\uff1a                          6\r\n\u578b\u53f7\uff1a                              85\r\n\u6bcf\u4e2a\u6838\u7684\u7ebf\u7a0b\u6570\uff1a                    2\r\n\u6bcf\u4e2a\u5ea7\u7684\u6838\u6570\uff1a                      12\r\n\u5ea7\uff1a                                2\r\n\u6b65\u8fdb\uff1a                              4\r\nCPU \u6700\u5927 MHz\uff1a                      3700.0000\r\nCPU \u6700\u5c0f MHz\uff1a                      1200.0000\r\nBogoMIPS\uff1a                          6000.00\r\n\u6807\u8bb0\uff1a                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear flush_l1d arch_capabilities\r\nL1d \u7f13\u5b58\uff1a                          768 KiB (24 instances)\r\nL1i \u7f13\u5b58\uff1a                          768 KiB (24 instances)\r\nL2 \u7f13\u5b58\uff1a                           24 MiB (24 instances)\r\nL3 \u7f13\u5b58\uff1a                           49.5 MiB (2 instances)\r\nNUMA \u8282\u70b9\uff1a                         2\r\nNUMA \u8282\u70b90 CPU\uff1a                    0-11,24-35\r\nNUMA \u8282\u70b91 CPU\uff1a                    12-23,36-47\r\nVulnerability Gather data sampling: Mitigation; Microcode\r\nVulnerability Itlb multihit:        KVM: Mitigation: VMX unsupported\r\nVulnerability L1tf:                 Mitigation; PTE Inversion\r\nVulnerability Mds:                  Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Meltdown:             Mitigation; PTI\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:             Mitigation; IBRS\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; IBRS, IBPB conditional, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Mitigation; Clear CPU buffers; SMT vulnerable\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.19.3\r\n[pip3] torch==2.2.1\r\n[pip3] torchaudio==2.2.2\r\n[pip3] torchvision==0.17.2\r\n[pip3] triton==2.2.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.3.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.19.3                   pypi_0    pypi\r\n[conda] torch                     2.2.1                    pypi_0    pypi\r\n[conda] torchaudio                2.2.2                    pypi_0    pypi\r\n[conda] torchvision               0.17.2                   pypi_0    pypi\r\n[conda] triton                    2.2.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.3.0             pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV2     NODE    NODE    SYS     SYS     SYS     SYS     0-11,24-35      0               N/A\r\nGPU1    NV2      X      NODE    NODE    SYS     SYS     SYS     SYS     0-11,24-35      0               N/A\r\nGPU2    NODE    NODE     X      NV2     SYS     SYS     SYS     SYS     0-11,24-35      0               N/A\r\nGPU3    NODE    NODE    NV2      X      SYS     SYS     SYS     SYS     0-11,24-35      0               N/A\r\nGPU4    SYS     SYS     SYS     SYS      X      NV2     NODE    NODE    12-23,36-47     1               N/A\r\nGPU5    SYS     SYS     SYS     SYS     NV2      X      NODE    NODE    12-23,36-47     1               N/A\r\nGPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      NV2     12-23,36-47     1               N/A\r\nGPU7    SYS     SYS     SYS     SYS     NODE    NODE    NV2      X      12-23,36-47     1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\r\npython -m vllm.entrypoints.openai.api_server --served-model-name=8x22b --model=/home/jarrelscy/Mixtral-8x22B-Instruct-v0.1-GPTQ-4bit --gpu-memory-utilizatio=0.95 --max-model-len=60000 --max-num-seqs=2 --tensor-parallel-size=8 --trust-remote-code --host=0.0.0.0 --port=8001 --max-log-len=1000\r\n\r\nKeyError: 'model.layers.45.block_sparse_moe.gate.g_idx'",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-21T23:20:50+00:00",
    "closed_at": "2024-11-29T02:06:39+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4247/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4247"
  },
  {
    "number": 11184,
    "title": "[Bug]: Bert tokenizer is tokenizing some tokens as `UNK`",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nWith some Bert and Roberta models like `sentence-transformers/all-MiniLM-L12-v2` I found that the output is not similar to the one generated by `sentence-transformers`. If I place the following prints in `_normalize_prompt_text_to_input()` in `serving_engine.py`\r\n```\r\n        print(f\"{input_ids=}\")\r\n```\r\nI get `[101, 100, 3007, 1997, 100, 2003, 100, 1012, 102]` for the sentence \"The capital of France is Paris.\". 100 is the `UNK` token.  When I run with sentence-transformers, I get `[ 101, 1996, 3007, 1997, 2605, 2003, 3000, 1012,  102]` . This problem happens both with `--tokenizer-mode auto` and `--tokenizer-mode slow`.\r\n\r\ncc: @DarkLight1337 \n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-12-13T21:33:18+00:00",
    "closed_at": "2025-01-09T03:05:45+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11184/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11184"
  },
  {
    "number": 13054,
    "title": "[Bug]: RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead. on Gaudi2",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nAutomatically detected platform hpu.\nCollecting environment information...\nPyTorch version: 2.5.1a0+git6fc067b\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-122-generic-x86_64-with-glibc2.35\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               160\nOn-line CPU(s) list:                  0-159\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8380 CPU @ 2.30GHz\nCPU family:                           6\nModel:                                106\nThread(s) per core:                   2\nCore(s) per socket:                   40\nSocket(s):                            2\nStepping:                             6\nCPU max MHz:                          3400.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4600.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            3.8 MiB (80 instances)\nL1i cache:                            2.5 MiB (80 instances)\nL2 cache:                             100 MiB (80 instances)\nL3 cache:                             120 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-39,80-119\nNUMA node1 CPU(s):                    40-79,120-159\nVulnerability Gather data sampling:   Mitigation; Microcode\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] habana-torch-dataloader==1.19.1.26\n[pip3] habana-torch-plugin==1.19.1.26\n[pip3] numpy==1.26.4\n[pip3] pynvml==8.0.4\n[pip3] pytorch-lightning==2.5.0.post0\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1a0+git6fc067b\n[pip3] torch_tb_profiler==0.4.0\n[pip3] torchaudio==2.5.1a0+1661daf\n[pip3] torchdata==0.9.0+d4bb3e6\n[pip3] torchmetrics==1.6.1\n[pip3] torchtext==0.18.0a0+9bed85d\n[pip3] torchvision==0.20.1a0+3ac97aa\n[pip3] transformers==4.48.2\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.2\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nLD_LIBRARY_PATH=/opt/habanalabs/libfabric-1.22.0/lib:/opt/amazon/openmpi/lib:/usr/lib/habanalabs:\nVLLM_NO_USAGE_STATS=1\nVLLM_TARGET_DEVICE=hpu\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nBuild vLLM using: \n```\ndocker build -t vllm-hpu:v0.7.2-mainline -f Dockerfile.hpu .\n```\n\nThen run it using:\n\n```\n\"cd /workspace/vllm && pip install git+https://github.com/huggingface/transformers accelerate && pip install qwen-vl-utils[decord]==0.0.8 && vllm serve Qwen/Qwen2.5-VL-7B-Instruct --disable_log_requests --port 8080 --tensor-parallel-size 2\"\n```\n\nThe hardware is a node with 8 Gaudi 2 HL-225H mezzanine cards with 2x Xeon 8380 Processors. The container has 2 Gaudi 2 cards, 70Gi memory and 25Gi of hugepages-2Mi assigned to it.\n\n```text\nview size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/engine/multiprocessing/engine.py\", line 380, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/engine/multiprocessing/engine.py\", line 123, in from_engine_args\n    return cls(ipc_path=ipc_path,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/engine/multiprocessing/engine.py\", line 75, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/engine/llm_engine.py\", line 276, in __init__\n    self._initialize_kv_caches()\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/engine/llm_engine.py\", line 416, in _initialize_kv_caches\n    self.model_executor.determine_num_available_blocks())\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/executor/executor_base.py\", line 101, in determine_num_available_blocks\n    results = self.collective_rpc(\"determine_num_available_blocks\")\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/executor/executor_base.py\", line 307, in collective_rpc\n    return self._run_workers(method, *args, **(kwargs or {}))\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/executor/mp_distributed_executor.py\", line 185, in _run_workers\n    driver_worker_output = run_method(self.driver_worker, sent_method,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/utils.py\", line 2220, in run_method\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/worker/hpu_worker.py\", line 214, in determine_num_available_blocks\n    self.model_runner.profile_run()\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/worker/hpu_model_runner.py\", line 1311, in profile_run\n    self.warmup_scenario(max_batch_size, max_seq_len, True, kv_caches,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/worker/hpu_model_runner.py\", line 1383, in warmup_scenario\n    self.execute_model(inputs, kv_caches, warmup_mode=True)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/worker/hpu_model_runner.py\", line 1967, in execute_model\n    hidden_states = self.model.forward(\n  File \"/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/graphs.py\", line 736, in forward\n    return wrapped_hpugraph_forward(\n  File \"/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/graphs.py\", line 586, in wrapped_hpugraph_forward\n    return orig_fwd(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/worker/hpu_model_runner.py\", line 371, in forward\n    hidden_states = self.model(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/model_executor/models/qwen2_5_vl.py\", line 1095, in forward\n    hidden_states = self.language_model.model(\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/compilation/decorators.py\", line 172, in __call__\n    return self.forward(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/model_executor/models/qwen2.py\", line 348, in forward\n    hidden_states, residual = layer(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1847, in _call_impl\n    return inner()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1793, in inner\n    result = forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/model_executor/models/qwen2.py\", line 247, in forward\n    hidden_states = self.self_attn(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1847, in _call_impl\n    return inner()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1793, in inner\n    result = forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/model_executor/models/qwen2.py\", line 178, in forward\n    q, k = self.rotary_emb(positions, q, k)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1847, in _call_impl\n    return inner()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1793, in inner\n    result = forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/model_executor/layers/rotary_embedding.py\", line 822, in forward\n    query = query.view(num_tokens, -1, self.head_size)\nRuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\nTraceback (most recent call last):\n  File \"/usr/local/bin/vllm\", line 33, in <module>\n    sys.exit(load_entry_point('vllm==0.7.2+gaudi000', 'console_scripts', 'vllm')())\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/scripts.py\", line 204, in main\n    args.dispatch_function(args)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/scripts.py\", line 44, in serve\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.10/dist-packages/uvloop/__init__.py\", line 82, in run\n    return loop.run_until_complete(wrapper())\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.10/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/entrypoints/openai/api_server.py\", line 875, in run_server\n    async with build_async_engine_client(args) as engine_client:\n  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/entrypoints/openai/api_server.py\", line 136, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm-0.7.2+gaudi000-py3.10.egg/vllm/entrypoints/openai/api_server.py\", line 230, in build_async_engine_client_from_engine_args\n    raise RuntimeError(\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-02-10T20:28:41+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13054/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13054"
  },
  {
    "number": 12692,
    "title": "[Bug]: V1 engine ignores guided json",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nINFO 02-03 05:55:13 __init__.py:183] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.31.4\nLibc version: glibc-2.35\n\nPython version: 3.12.8 (main, Dec  4 2024, 08:54:12) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-6.8.0-52-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3\nNvidia driver version: 560.35.05\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               128\nOn-line CPU(s) list:                  0-127\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Gold 6448Y\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   32\nSocket(s):                            2\nStepping:                             8\nCPU max MHz:                          4100.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4200.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect user_shstk avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hfi vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            3 MiB (64 instances)\nL1i cache:                            2 MiB (64 instances)\nL2 cache:                             128 MiB (64 instances)\nL3 cache:                             120 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126\nNUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.2\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0,2,4,6,8,10    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNVIDIA_VISIBLE_DEVICES=GPU-6cf6dad3-da49-c3af-bb85-7275c18ce3d6\nNVIDIA_REQUIRE_CUDA=cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\nNCCL_VERSION=2.17.1-1\nNVIDIA_DRIVER_CAPABILITIES=all\nNVIDIA_PRODUCT_NAME=CUDA\nVLLM_USAGE_SOURCE=production-docker-image\nNVIDIA_CUDA_END_OF_LIFE=1\nCUDA_VERSION=12.1.0\nLD_LIBRARY_PATH=/usr/local/lib/python3.12/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nWhen making a request with to the OpenAi compatible Api with the extra fields for guided_json generation like so:\n\n```\n{\n  \"model\": \"Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"what is the height of the eiffel tower\"\n    }\n  ],\n  \"guided_json\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"height\": {\n        \"type\": \"number\"\n      }\n    },\n    \"required\": [\n      \"height\"\n    ]\n  }\n}\n```\n\nThe output simply ignores the guided decoding paramter. When switching back to V0 it works fine.\n\nHere are the logs from the vllm server:\n\n```\n`INFO 02-03 05:59:31 logger.py:37] Received request chatcmpl-178de64763e84ddd81a9f6f62ee0f4c3: prompt: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nwhat is the height of the eiffel tower<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32739, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json={'type': 'object', 'properties': {'height': {'type': 'number'}}, 'required': ['height']}, regex=None, choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None)), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\nDEBUG 02-03 05:59:31 async_llm_engine.py:546] Building guided decoding logits processor. Params: GuidedDecodingParams(json={'type': 'object', 'properties': {'height': {'type': 'number'}}, 'required': ['height']}, regex=None, choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None)\nINFO 02-03 05:59:32 engine.py:273] Added request chatcmpl-178de64763e84ddd81a9f6f62ee0f4c3.\nINFO 02-03 05:59:32 metrics.py:453] Avg prompt throughput: 4.7 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nINFO:     **.***.**.**:***** - \"POST /v1/chat/completions HTTP/1.1\" 200 OK`\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale",
      "v1"
    ],
    "state": "closed",
    "created_at": "2025-02-03T14:01:44+00:00",
    "closed_at": "2025-06-06T02:18:22+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12692/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12692"
  },
  {
    "number": 19867,
    "title": "[Bug]: AiterFlashAttentionImpl.__init__() got multiple values for argument 'use_irope' for llama4 model",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWe hit an exception on running llama4 models with latest code on ROCm V1:\n\n```\n(VllmWorker rank=2 pid=267) ERROR 06-19 01:00:39 [multiproc_executor.py:488] TypeError: AiterFlashAttentionImpl.__init__() got multiple values for argument 'use_irope'\n```\nCurrent work-around:\nTo turn off AITER_MHA, with VLLM_ROCM_USE_AITER_MHA=0\n\n\nProposal:\n\n- [ ] Fix the bug (the team is working on it)\n- [ ] Add a end-to-end test for one of the small llama4 models\n- [ ] \n\nThe motivation for adding an end to end test for a small version of llama4 models, is that we have seen issues of breaking llama4 models in the past because of lacking such tests.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "rocm"
    ],
    "state": "closed",
    "created_at": "2025-06-19T14:36:59+00:00",
    "closed_at": "2025-07-14T17:39:11+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19867/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19867"
  },
  {
    "number": 20716,
    "title": "[Bug]: LLM.classify() fails on second call with ModernBERT due to missing torch.SymInt shape symbols",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nINFO 07-09 15:38:22 [__init__.py:244] Automatically detected platform cuda.\nCollecting environment information...\nuv is set\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : Could not collect\nCMake version                : version 3.22.1\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.1+cu126\nIs debug build               : False\nCUDA used to build PyTorch   : 12.6\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0] (64-bit runtime)\nPython platform              : Linux-5.15.0-133-generic-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.4.131\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : \nGPU 0: NVIDIA H200\nGPU 1: NVIDIA H200\nGPU 2: NVIDIA H200\nGPU 3: NVIDIA H200\nGPU 4: NVIDIA H200\nGPU 5: NVIDIA H200\nGPU 6: NVIDIA H200\nGPU 7: NVIDIA H200\n\nNvidia driver version        : 550.127.05\ncuDNN version                : Could not collect\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               224\nOn-line CPU(s) list:                  0-223\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8480+\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   56\nSocket(s):                            2\nStepping:                             8\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            5.3 MiB (112 instances)\nL1i cache:                            3.5 MiB (112 instances)\nL2 cache:                             224 MiB (112 instances)\nL3 cache:                             210 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190,192,194,196,198,200,202,204,206,208,210,212,214,216,218,220,222\nNUMA node1 CPU(s):                    1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159,161,163,165,167,169,171,173,175,177,179,181,183,185,187,189,191,193,195,197,199,201,203,205,207,209,211,213,215,217,219,221,223\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-cufile-cu12==1.11.1.6\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-ml-py==12.575.51\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pynvml==12.0.0\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.7.1\n[pip3] torchaudio==2.7.0\n[pip3] torchvision==0.22.0\n[pip3] transformers==4.52.4\n[pip3] triton==3.3.1\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.1\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10NIC11    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     PIX     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS  SYS      0,2,4,6,8,10    0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS  SYS      0,2,4,6,8,10    0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     SYS  SYS      0,2,4,6,8,10    0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS  SYS      0,2,4,6,8,10    0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX     PIX     NODE    NODE NODE     1,3,5,7,9,11    1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     NODE NODE     1,3,5,7,9,11    1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    PIX  NODE     1,3,5,7,9,11    1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE PIX      1,3,5,7,9,11    1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      PIX     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS  SYS\nNIC1    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX      X      PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS  SYS\nNIC2    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     PIX     PIX      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS  SYS\nNIC3    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      NODE    NODE    SYS     SYS     SYS     SYS     SYS  SYS\nNIC4    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE     X      NODE    SYS     SYS     SYS     SYS     SYS  SYS\nNIC5    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE     X      SYS     SYS     SYS     SYS     SYS  SYS\nNIC6    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     PIX     NODE    NODE NODE\nNIC7    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      PIX     NODE    NODE NODE\nNIC8    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX      X      NODE    NODE NODE\nNIC9    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      NODE NODE\nNIC10   SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE     X   NODE\nNIC11   SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE  X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n  NIC8: mlx5_8\n  NIC9: mlx5_9\n  NIC10: mlx5_10\n  NIC11: mlx5_11\n\n==============================\n     Environment Variables\n==============================\nLD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda-12.4/lib64\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n```python\nfrom vllm import LLM\nimport time\n\nllm = LLM(model=\"NousResearch/Minos-v1\",\n          max_model_len=8192,\n          task=\"classify\",\n          compilation_config={\"level\": 3})\n\nstart = time.time()\nresult = llm.classify([\"Hello, how are you?\"], use_tqdm=False)\nprint(result[0].outputs.probs)\nprint(f\"First inference took: {time.time() - start:.2f}s\")\n\n# Second call fails\nllm.classify([\"Hello, how are you?\"], use_tqdm=False)\n```\n\nI'm encountering an issue when using `vLLM==0.9.1` with a custom model (`ModernBERTForSequenceClassification`). The first call to `LLM.classify()` works as expected. However, a second call to `classify()` on the same instance crashes with an error originating from `vllm/compilation/backends.py`, specifically due to this code:\n\n```python\nsym_shape_indices = [i for i, x in enumerate(args) if isinstance(x, torch.SymInt)]\n```\n\nThe list ends up empty because the input shape arguments are plain integers (e.g., `7`) instead of symbolic shapes like `s0`, which is expected by the compiler backend.\n\n**Error Message:**\n```python\nFile ~/llmenv/lib/python3.12/site-packages/vllm/compilation/cuda_piecewise_backend.py:112, in CUDAPiecewiseBackend.__call__(self, *args)\n    109     self.check_for_ending_compilation()\n    110     return self.compiled_graph_for_general_shape(*args)\n--> 112 runtime_shape = args[self.sym_shape_indices[0]]\n    113 if runtime_shape not in self.concrete_size_entries:\n    114     # we don't need to do anything for this shape\n    115     return self.compiled_graph_for_general_shape(*args)\n\nIndexError: list index out of range\n```\n\nRoot Cause (Suspected):\n\nAfter debugging, we believe the issue is linked to the torch._dynamo.config. The assume_static_by_default setting being True by default for some models (like ModernBERT). This causes TorchDynamo to treat shape arguments as static integers, which breaks subsequent compilation passes that expect torch.SymInt.\n\nNotably:\n\n`qwen2.5-0.5b` works fine and emits symbolic shapes (`s0`, etc.)\n\nModernBERT models emit plain integers like `7`, leading to errors during shape checks or dynamic compilation.\n\n\n**Important, I can compile the ModernBert with the raw transformers package**\n**Important2, mannully set self.sym_shape_indices = [1] make the code successfully running with batch_size = 1**\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-07-09T22:46:17+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20716/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20716"
  },
  {
    "number": 5961,
    "title": "[Bug]: vLLM crash when running Phi-3-small-8k-instruct with enable-chunked-prefill",
    "body": "### Your current environment\n\n\r\n```\r\nimage\": \"vllm/vllm-openai:latest\",\r\n--model=microsoft/Phi-3-small-8k-instruct \r\n--tensor-parallel-size=1\r\n--disable-log-requests\r\n--trust-remote-code\r\n--enable-chunked-prefill\r\n--max-num-batched-tokens=2048\r\n--max-model-len=4096\r\n--gpu-memory-utilization=0.9\",\r\n```\r\nAccelerator: 1x Nvidia L4\n\n### \ud83d\udc1b Describe the bug\n\n```\r\nERROR 06-28 13:26:18 async_llm_engine.py:52] Engine background task failed\r\nERROR 06-28 13:26:18 async_llm_engine.py:52] Traceback (most recent call last):\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 42, in _log_task_completion\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return_value = task.result()\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 532, in run_engine_loop\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     has_requests_in_progress = await asyncio.wait_for(\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return fut.result()\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 506, in engine_step\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     request_outputs = await self.engine.step_async()\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 235, in step_async\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     output = await self.model_executor.execute_model_async(\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 117, in execute_model_async\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     output = await make_async(self.driver_worker.execute_model\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     result = self.fn(*self.args, **self.kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return func(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 280, in execute_model\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     output = self.model_runner.execute_model(seq_group_metadata_list,\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return func(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 749, in execute_model\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     hidden_states = model_executable(\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3_small.py\", line 416, in forward\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     output_hidden_states = self.model(\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3_small.py\", line 338, in forward\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     hidden_states = layer(\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3_small.py\", line 282, in forward\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     hidden_states = self.self_attn(\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3_small.py\", line 244, in forward\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     attn_output = self.attn(q, k, v, kv_cache, attn_metadata=attn_metadata)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/layer.py\", line 89, in forward\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     return self.impl.forward(query, key, value, kv_cache, attn_metadata,\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/backends/blocksparse_attn.py\", line 376, in forward\r\nERROR 06-28 13:26:18 async_llm_engine.py:52]     or prefill_meta.block_tables.numel() == 0, \\\r\nERROR 06-28 13:26:18 async_llm_engine.py:52] AssertionError: Does not support prefix-enabled attention.\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-06-28T13:34:09+00:00",
    "closed_at": "2024-06-28T22:41:15+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5961/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5961"
  },
  {
    "number": 14979,
    "title": "[Bug]: Model weights in GiB ",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.12.9 (main, Feb 12 2025, 14:50:50) [Clang 19.1.6 ] (64-bit runtime)\nPython platform: Linux-5.15.0-134-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA L40S\nNvidia driver version: 570.86.10\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               48\nOn-line CPU(s) list:                  0-47\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Gold 5412U\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   24\nSocket(s):                            1\nStepping:                             8\nCPU max MHz:                          3900.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4200.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            1.1 MiB (24 instances)\nL1i cache:                            768 KiB (24 instances)\nL2 cache:                             48 MiB (24 instances)\nL3 cache:                             45 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-47\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pynvml==12.0.0\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.49.0\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: N/A\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-47    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n\n\n</details>\n\n\n\n### \ud83d\udc1b Describe the bug\n\nHello,\nI think there's a small error in the unit displayed by model_runner.py concerning the weight of a model.\nGiven the calculation, I suggest replacing GB with GiB.\nWhat do you think? \n\n self.model_memory_usage = m.consumed_memory\n        logger.info(\"Loading model weights took %.4f GB\",\n                    self.model_memory_usage / **float(2**30))**\n\nIncorrect output : \nmodel_runner.py:1115] Loading model weights took 12.5523 **GB**\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-17T18:28:30+00:00",
    "closed_at": "2025-03-31T17:00:51+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14979/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14979"
  },
  {
    "number": 354,
    "title": "Loading Models that require execution of third party code (trust_remote_code=True)",
    "body": "I am trying to load MPT using the AsyncLLMEngine:\r\n\r\n```\r\n\r\nengine_args = AsyncEngineArgs(\"mosaicml/mpt-7b-chat\", engine_use_ray=True)\r\nengine = AsyncLLMEngine.from_engine_args(engine_args)\r\n```\r\n\r\nBut I am getting this error:\r\n`ValueError: Loading mosaicml/mpt-7b-chat-local requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.`\r\n\r\nIs there any workaround for this or could it be possible to add the option to trust remote code to EngineArgs?",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-07-04T08:05:46+00:00",
    "closed_at": "2024-03-08T10:22:14+00:00",
    "comments": 15,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/354/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/354"
  },
  {
    "number": 12354,
    "title": "[Usage]: When running models on multiple GPUs, workload does not get split",
    "body": "### Your current environment\n\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.11.6 (main, Nov  2 2023, 09:27:30) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-119-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: NVIDIA GeForce RTX 3090\nGPU 1: NVIDIA GeForce RTX 3090\nGPU 2: NVIDIA GeForce RTX 3090\nGPU 3: NVIDIA GeForce RTX 3090\n\nNvidia driver version: 560.35.03\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        43 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               128\nOn-line CPU(s) list:                  0-127\nVendor ID:                            AuthenticAMD\nModel name:                           AMD Ryzen Threadripper PRO 3995WX 64-Cores\nCPU family:                           23\nModel:                                49\nThread(s) per core:                   2\nCore(s) per socket:                   64\nSocket(s):                            1\nStepping:                             0\nFrequency boost:                      enabled\nCPU max MHz:                          4308.3979\nCPU min MHz:                          2200.0000\nBogoMIPS:                             5389.92\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nVirtualization:                       AMD-V\nL1d cache:                            2 MiB (64 instances)\nL1i cache:                            2 MiB (64 instances)\nL2 cache:                             32 MiB (64 instances)\nL3 cache:                             256 MiB (16 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-127\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\nVulnerability Spec rstack overflow:   Mitigation; safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.1\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.6.6.post1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NODE    NODE    NODE    0-127   0               N/A\nGPU1    NODE     X      NODE    NODE    0-127   0               N/A\nGPU2    NODE    NODE     X      NV4     0-127   0               N/A\nGPU3    NODE    NODE    NV4      X      0-127   0               N/A\n\nLegend:\n\nX    = Self\nSYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\nNODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\nPHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\nPXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\nPIX  = Connection traversing at most a single PCIe bridge\nNV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=/csem/divr/users/adx/env/lib/python3.11/site-packages/cv2/../../lib64:/local/user/toolsr64/linux64_u22/usr/local/lib:/opt/ibm/lsf/10.1/linux3.10-glibc2.17-x86_64/lib:/opt/tools/linux64_u22/usr/local/lib:\nCUDA_MODULE_LOADING=LAZY\n\n### How would you like to use vllm\n\nI want to run a model on multiple GPUs. From the [documentation ](https://docs.vllm.ai/en/v0.4.1/serving/distributed_serving.html) I understand that the `--tensor-parallel-size` flag needs to be set. I am running the following command:\n\n```\nvllm serve meta-llama/Llama-3.1-8B-Instruct \\\n    --enable-auto-tool-choice \\\n    --tool-call-parser llama3_json \\\n    --chat-template src/vllm_chat_templates/tool_chat_template_llama3.1_json.jinja \\\n    --max_model_len 5000 \\\n    --tensor-parallel-size 4\n```\nI am seeing that the workload is not split on 4 GPUs. The utilization in case of using only one GPU is replicated on all 4 GPUs when using `--tensor-parallel-size 4`. Am I using the framework wrong? I have a large model that requires multiple GPUs to run and expect the workload to be split across multiple GPUs.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-23T13:12:21+00:00",
    "closed_at": "2025-05-24T02:07:44+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12354/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12354"
  },
  {
    "number": 2370,
    "title": "How to use Splitwise(from microsoft) in vllm?",
    "body": "Microsoft have claimed that \u201dSplitwise\u201c is supported in vLLM, see\r\nhttps://www.microsoft.com/en-us/research/blog/splitwise-improves-gpu-usage-by-splitting-llm-inference-phases/\r\n![image](https://github.com/vllm-project/vllm/assets/58217233/7835c241-f22c-4ffc-a510-1238f4a5d770)\r\n\r\nSo how to use it in vLLM? I could not find keyword about \u201dSplitwise\u201c.",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-01-08T03:49:36+00:00",
    "closed_at": "2024-11-30T02:03:07+00:00",
    "comments": 12,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2370/reactions",
      "total_count": 3,
      "+1": 0,
      "-1": 0,
      "laugh": 3,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2370"
  },
  {
    "number": 10662,
    "title": "[Usage]: Cannot use xformers with old GPU",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: CentOS Linux 7 (Core) (x86_64)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.17\r\n\r\nPython version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.10.0-1.0.0.32-x86_64-with-glibc2.17\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-SXM2-32GB\r\nGPU 1: Tesla V100-SXM2-32GB\r\nGPU 2: Tesla V100-SXM2-32GB\r\nGPU 3: Tesla V100-SXM2-32GB\r\nGPU 4: Tesla V100-SXM2-32GB\r\nGPU 5: Tesla V100-SXM2-32GB\r\nGPU 6: Tesla V100-SXM2-32GB\r\nGPU 7: Tesla V100-SXM2-32GB\r\n\r\nNvidia driver version: 535.129.03\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                48\r\nOn-line CPU(s) list:   0-47\r\nThread(s) per core:    1\r\nCore(s) per socket:    24\r\nSocket(s):             2\r\nNUMA node(s):          2\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 85\r\nModel name:            Intel(R) Xeon(R) Gold 6271C CPU @ 2.60GHz\r\nStepping:              7\r\nCPU MHz:               2599.961\r\nCPU max MHz:           2600.0000\r\nCPU min MHz:           1000.0000\r\nBogoMIPS:              5200.00\r\nVirtualization:        VT-x\r\nL1d cache:             32K\r\nL1i cache:             32K\r\nL2 cache:              1024K\r\nL3 cache:              33792K\r\nNUMA node0 CPU(s):     0-23\r\nNUMA node1 CPU(s):     24-47\r\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku avx512_vnni md_clear flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.46.3\r\n[pip3] transformers-stream-generator==0.0.5\r\n[pip3] triton==3.1.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.20.1                   pypi_0    pypi\r\n[conda] transformers              4.46.3                   pypi_0    pypi\r\n[conda] transformers-stream-generator 0.0.5                    pypi_0    pypi\r\n[conda] triton                    3.1.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.4.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV2\tNV2\tNV1\tNV1\tSYS\tSYS\tSYS\tSYS\t0-23\t0\t\tN/A\r\nGPU1\tNV2\t X \tNV1\tNV1\tSYS\tNV2\tSYS\tSYS\tSYS\t0-23\t0\t\tN/A\r\nGPU2\tNV2\tNV1\t X \tNV2\tSYS\tSYS\tNV1\tSYS\tSYS\t0-23\t0\t\tN/A\r\nGPU3\tNV1\tNV1\tNV2\t X \tSYS\tSYS\tSYS\tNV2\tSYS\t0-23\t0\t\tN/A\r\nGPU4\tNV1\tSYS\tSYS\tSYS\t X \tNV2\tNV2\tNV1\tSYS\t0-23\t0\t\tN/A\r\nGPU5\tSYS\tNV2\tSYS\tSYS\tNV2\t X \tNV1\tNV1\tSYS\t0-23\t0\t\tN/A\r\nGPU6\tSYS\tSYS\tNV1\tSYS\tNV2\tNV1\t X \tNV2\tSYS\t0-23\t0\t\tN/A\r\nGPU7\tSYS\tSYS\tSYS\tNV2\tNV1\tNV1\tNV2\t X \tSYS\t0-23\t0\t\tN/A\r\nNIC0\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n```\r\n\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nxformers of new version not support GPU with capability (7, 0) (too old)\r\n report error   \u201cNo operator found for `memory_efficient_attention_forward` with inputs\u201d\r\n\r\n\r\n\r\nhow could i run vllm with v100 ???\r\nwhat should i do next ?\r\n\r\n```\r\n pip show xformers\r\nName: xformers\r\nVersion: 0.0.28.post3\r\n\r\npip show vllm\r\nName: vllm\r\nVersion: 0.6.4.post1\r\n````\r\n\r\nCode\r\n```\r\nfrom vllm import LLM, SamplingParams\r\nprompts = [\r\n    \"Hello, my name is\",\r\n    \"The president of the United States is\",\r\n    \"The capital of France is\",\r\n    \"The future of AI is\",\r\n]\r\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\r\n\r\nmodel_path_prefix ='/home/users/.cache/modelscope/hub/'\r\nmodel_name = 'Qwen/Qwen-7B-Chat'\r\nllm = LLM(model=model_path_prefix + model_name,trust_remote_code=True)\r\n#llm = LLM(model=\"facebook/opt-125m\")\r\n#vllm_model = Model(model_path=local_model_dir)\r\noutputs = llm.generate(prompts, sampling_params)\r\n\r\nfor output in outputs:\r\n    prompt = output.prompt\r\n    generated_text = output.outputs[0].text\r\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\r\n\r\n```\r\n\r\nError\r\n```\r\n[rank0]:   File \"/home/users/bai/miniconda3/envs/bai/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/users/bai/miniconda3/envs/bai/lib/python3.12/site-packages/vllm/worker/worker.py\", line 195, in determine_num_available_blocks\r\n[rank0]:     self.model_runner.profile_run()\r\n[rank0]:   File \"/home/users/bai/miniconda3/envs/bai/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/users/bai/miniconda3/envs/bai/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 1316, in profile_run\r\n[rank0]:     self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n[rank0]:   File \"/home/users/bai/miniconda3/envs/bai/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/users/bai/miniconda3/envs/bai/lib/python3.12/site-packages/vllm/worker/model_runner_base.py\", line 152, in _wrapper\r\n[rank0]:     raise type(err)(\r\n[rank0]: NotImplementedError: Error in model execution (input dumped to /tmp/err_execute_model_input_20241126-145838.pkl): No operator found for `memory_efficient_attention_forward` with inputs:\r\n[rank0]:      query       : shape=(1, 8192, 32, 128) (torch.float16)\r\n[rank0]:      key         : shape=(1, 8192, 32, 128) (torch.float16)\r\n[rank0]:      value       : shape=(1, 8192, 32, 128) (torch.float16)\r\n[rank0]:      attn_bias   : <class 'xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask'>\r\n[rank0]:      p           : 0.0\r\n[rank0]: `fa2F@v2.5.7-pt` is not supported because:\r\n[rank0]:     xFormers wasn't build with CUDA support\r\n[rank0]:     requires device with capability > (8, 0) but your GPU has capability (7, 0) (too old)\r\n[rank0]: `cutlassF-pt` is not supported because:\r\n[rank0]:     xFormers wasn't build with CUDA support\r\n[rank0]:[W1126 14:58:39.595129646 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-26T07:14:55+00:00",
    "closed_at": "2025-03-27T02:04:33+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10662/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10662"
  },
  {
    "number": 11139,
    "title": "[Bug]: vllm using ray in eks hangs when using --pipeline_parallel_size > 1",
    "body": "### Your current environment\n\nrunning on a pod in g6.12xlarge (allocated by lws).\r\nPod is initializing ray before running vllm (using the proposed lws image https://github.com/kubernetes-sigs/lws/blob/main/docs/examples/vllm/build/Dockerfile.GPU)\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nVllm is stuck on this meesage:\r\nINFO 12-12 05:28:31 pynccl.py:69] vLLM is using nccl==2.21.5\r\n\r\nfull log:\r\n[2024-12-12 05:27:53,632 W 8 8] global_state_accessor.cc:463: Retrying to get node with node ID 9c36d691ad808fe6b12015dc3c0c4ba0432917a72547d5450434659c\r\n2024-12-12 05:27:52,822 INFO usage_lib.py:467 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected t\r\no be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage\r\n-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\r\n2024-12-12 05:27:52,822 INFO scripts.py:822 -- Local node IP: 10.0.143.175\r\n2024-12-12 05:27:54,657 SUCC scripts.py:859 -- --------------------\r\n2024-12-12 05:27:54,657 SUCC scripts.py:860 -- Ray runtime started.\r\n2024-12-12 05:27:54,657 SUCC scripts.py:861 -- --------------------\r\n2024-12-12 05:27:54,657 INFO scripts.py:863 -- Next steps\r\n2024-12-12 05:27:54,657 INFO scripts.py:866 -- To add another node to this Ray cluster, run\r\n2024-12-12 05:27:54,657 INFO scripts.py:869 --   ray start --address='10.0.143.175:6379'\r\n2024-12-12 05:27:54,657 INFO scripts.py:878 -- To connect to this Ray cluster:\r\n2024-12-12 05:27:54,657 INFO scripts.py:880 -- import ray\r\n2024-12-12 05:27:54,657 INFO scripts.py:881 -- ray.init()\r\n2024-12-12 05:27:54,657 INFO scripts.py:912 -- To terminate the Ray runtime, run\r\n2024-12-12 05:27:54,657 INFO scripts.py:913 --   ray stop\r\n2024-12-12 05:27:54,657 INFO scripts.py:916 -- To view the status of the cluster, use\r\n2024-12-12 05:27:54,657 INFO scripts.py:917 --   ray status\r\n2024-12-12 05:27:55,002 INFO worker.py:1634 -- Connecting to existing Ray cluster at address: 10.0.143.175:6379...\r\n2024-12-12 05:27:55,010 INFO worker.py:1819 -- Connected to Ray cluster.\r\nAll ray workers are active and the ray cluster is initialized successfully.\r\nINFO 12-12 05:27:59 api_server.py:585] vLLM API server version 0.6.4.post1\r\nINFO 12-12 05:27:59 api_server.py:586] args: Namespace(host=None, port=8080, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allo\r\nwed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile\r\n=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiproces\r\nsing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='meta-llama/Llama-3.1-8B-Instruct', task='auto', tokenizer=N\r\none, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', chat_template_text_format='string', trus\r\nt_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cach\r\ne_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=Fal\r\nse, pipeline_parallel_size=2, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching\r\n=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9\r\n, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=No\r\nne, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer\r\n_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_lora\r\ns=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_\r\nprompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_de\r\nlay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_\r\nmqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_\r\nmax=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_a\r\ncceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=Non\r\ne, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, sched\r\nuling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, en\r\nable_prompt_tokens_details=False)\r\nINFO 12-12 05:28:04 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\r\nINFO 12-12 05:28:04 config.py:1020] Defaulting to use ray for distributed inference\r\nWARNING 12-12 05:28:04 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not w\r\nork with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\r\nWARNING 12-12 05:28:04 arg_utils.py:1075] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect\r\n on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block\r\n manager v2), please file an issue with detailed information.\r\nINFO 12-12 05:28:04 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.\r\nWARNING 12-12 05:28:04 config.py:479] Async output processing can not be enabled with pipeline parallel\r\n2024-12-12 05:28:04,536 INFO worker.py:1634 -- Connecting to existing Ray cluster at address: 10.0.143.175:6379...\r\n2024-12-12 05:28:04,543 INFO worker.py:1819 -- Connected to Ray cluster.\r\nINFO 12-12 05:28:04 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=\r\nNone, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_r\r\nevision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pi\r\npeline_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_\r\nconfig=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collec\r\nt_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_p\r\nrefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, chat_template_text_\r\nformat=string, mm_processor_kwargs=None, pooler_config=None)\r\nINFO 12-12 05:28:06 ray_gpu_executor.py:134] use_ray_spmd_worker: False\r\nINFO 12-12 05:28:29 selector.py:135] Using Flash Attention backend.\r\n\u2190[36m(RayWorkerWrapper pid=365)\u2190[0m INFO 12-12 05:28:29 selector.py:135] Using Flash Attention backend.\r\nINFO 12-12 05:28:31 utils.py:961] Found nccl from library libnccl.so.2\r\nINFO 12-12 05:28:31 pynccl.py:69] vLLM is using nccl==2.21.5\r\nvllm-0:936:936 [0] NCCL INFO Bootstrap : Using eth0:10.0.143.175<0>\r\nvllm-0:936:936 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)\r\nvllm-0:936:936 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading\r\nlibnccl-net.so\r\nvllm-0:936:936 [0] NCCL INFO NET/Plugin: Using internal network plugin.\r\nvllm-0:936:936 [0] NCCL INFO cudaDriverVersion 12040\r\nNCCL version 2.21.5+cuda12.4\r\nvllm-0:936:936 [0] NCCL INFO NET/IB : No device found.\r\nvllm-0:936:936 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.143.175<0>\r\nvllm-0:936:936 [0] NCCL INFO Using non-device net plugin version 0\r\nvllm-0:936:936 [0] NCCL INFO Using network Socket\r\nvllm-0:936:936 [0] NCCL INFO ncclCommInitRank comm 0x31f8ebd0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 38000 commId 0x9419ffc51c422e05 - Init START\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ffffffff\r\nvllm-0:936:936 [0] NCCL INFO NVLS multicast support is not available on dev 0\r\nvllm-0:936:936 [0] NCCL INFO comm 0x31f8ebd0 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0\r\nvllm-0:936:936 [0] NCCL INFO Channel 00/02 :    0   1   2   3\r\nvllm-0:936:936 [0] NCCL INFO Channel 01/02 :    0   1   2   3\r\nvllm-0:936:936 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\r\nvllm-0:936:936 [0] NCCL INFO P2P Chunksize set to 131072\r\nvllm-0:936:936 [0] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\r\nvllm-0:936:936 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct\r\nvllm-0:936:936 [0] NCCL INFO P2P is disabled between \u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m INFO 12-12 05:28:31 utils.py:961] Found nccl from\r\n library libnccl.so.2\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m INFO 12-12 05:28:31 pynccl.py:69] vLLM is using nccl==2.21.5\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m vllm-0-1:194:194 [0] NCCL INFO Bootstrap : Using eth0:10.0.130.237<0>\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m vllm-0-1:194:194 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m vllm-0-1:194:194 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared\r\n object file: No such file or directory : when loading libnccl-net.so\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m vllm-0-1:194:194 [0] NCCL INFO NET/Plugin: Using internal network plugin.\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m vllm-0-1:194:194 [0] NCCL INFO cudaDriverVersion 12040\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m NCCL version 2.21.5+cuda12.4\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO NET/IB : No device found.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.143.175<0>\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO Using non-device net plugin version 0\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO Using network Socket\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO ncclCommInitRank comm 0x2c7cef60 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 3a000 commId 0x\r\n9419ffc51c422e05 - Init START\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 1. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 0 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 2 and 3. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO NVLS multicast support is not available on dev 1\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO comm 0x2c7cef60 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P Chunksize set to 131072\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this message with NCCL_IGN\r\nORE_DISABLED_P2P=1.\r\n\u2190[36m(RayWorkerWrapper pid=367)\u2190[0m vllm-0:367:367 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this\r\n\u2190[36m(RayWorkerWrapper pid=365)\u2190[0m vllm-0:365:365 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress th\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m vllm-0-1:194:194 [0] NCCL INFO Channel 00/02 :    0   1   2   3\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m vllm-0-1:194:194 [0] NCCL INFO Channel 01/02 :    0   1   2   3\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m vl\r\n\u2190[36m(RayWorkerWrapper pid=264, ip=10.0.130.237)\u2190[0m vllm-0-1:264:264 [1] NCCL INFO P2P is disabled between connected GPUs 1 and 0. You can repress this me\r\nssage\r\n\u2190[36m(RayWorkerWrapper pid=335, ip=10.0.130.237)\u2190[0m vllm-0-1:335:335 [2] NCCL INFO P2P is disabled between connected GPUs 2 and 1. You can repress this me\r\nssage wi\r\n\u2190[36m(RayWorkerWrapper pid=406, ip=10.0.130.237)\u2190[0m vllm-0-1:406:406 [3] NCCL INFO P2P is disabled between connected GPUs 3 and 2. You can repress this me\r\nssag\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m WARNING 12-12 05:28:31 custom_all_reduce.py:134] Custom allreduce is disabled because it's not support\r\ned on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n\u2190[36m(RayWorkerWrapper pid=194, ip=10.0.130.237)\u2190[0m INFO 12-12 05:28:31 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='\r\n127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7efce816d8b0>, local_subscr\r\nibe_port=44055, remote_subscribe_port=None)\r\nWARNING 12-12 05:28:31 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this w\r\narning, specify disable_custom_all_reduce=True explicitly.\r\nINFO 12-12 05:28:31 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vll\r\nm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7feb60210b90>, local_subscribe_port=33777, remote_subscribe_port=None)\r\nINFO 12-12 05:28:31 utils.py:961] Found nccl from library libnccl.so.2\r\nINFO 12-12 05:28:31 pynccl.py:69] vLLM is using nccl==2.21.5\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ray",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-12T14:07:32+00:00",
    "closed_at": "2025-07-09T02:16:16+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11139/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11139"
  },
  {
    "number": 15549,
    "title": "[Bug]: Tools parsing issues with mistral3.1",
    "body": "### Your current environment\n\nvllm 0.8.1\n\n\n### \ud83d\udc1b Describe the bug\n\nseems there is an issue with mistral for tools parsing? the output is not function calling as expected.\n\n- command:\n`serve mistralai/Mistral-Small-3.1-24B-Base-2503 --max-model-len 4096 --gpu-memory-utilization 0.9 --tensor-parallel-size 4 --served-model-name mistral --tokenizer-mode mistral --config-format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice `\n\nexample:\n- request:\n```\n {\n    \"model\":\"mistral\",\n    \"messages\": [\n        {\n            \"content\": \"What's the weather like in San Francisco?\",\n            \"role\": \"user\"\n        }\n    ],\n    \"max_completion_tokens\": 128,\n    \"tools\": [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_weather\",\n                \"description\": \"Get the current weather in a given location\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"City and state, e.g., 'San Francisco, CA'\"\n                        },\n                        \"unit\": {\n                            \"type\": \"string\",\n                            \"enum\": [\n                                \"celsius\",\n                                \"fahrenheit\"\n                            ]\n                        }\n                    },\n                    \"required\": [\n                        \"location\",\n                        \"unit\"\n                    ]\n                }\n            }\n        }\n    ],\n    \"tool_choice\": \"auto\"\n  }\n```\n\n- response:\n```\n{\n  \"id\": \"chatcmpl-4dccab14c6b64bf7a3a0454346945d26\",\n  \"object\": \"chat.completion\",\n  \"created\": 1742994086,\n  \"model\": \"mistral\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"reasoning_content\": null,\n        \"content\": \"\\n\\n\\n\\nPowered by [Syntax Spirit API](https://syntaxspirit.com/api/)\\n\\nThis project provides examples and file structure for building a API with Typescript and related dependencies using the Plugin Architecture. The project consists of 2 key components API and Plugins.\\n\\n[GitHub repository]\\n\\n## Structure\\n\\nThe project follows a plugin-based architecture with the main application (app.ts) and individual plugin files (plugins). The structure of the project allows for easy addition of new plugins or modifications to existing ones without affecting the overall application.\\n\\nbot.ts - Contains the main logic for the OpenAI bot configurations plugins - Holds individual plugin files with functions\",\n        \"tool_calls\": []\n      },\n      \"logprobs\": null,\n      \"finish_reason\": \"length\",\n      \"stop_reason\": null\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 116,\n    \"total_tokens\": 244,\n    \"completion_tokens\": 128,\n    \"prompt_tokens_details\": null\n  },\n  \"prompt_logprobs\": null\n}\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-26T13:38:21+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15549/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15549"
  },
  {
    "number": 7669,
    "title": "[Bug]: Mismatch in the number of image tokens and placeholders during batch inference",
    "body": "### Your current environment\r\n\r\n```\r\nRay v2.23\r\nPython 3.10\r\nvllm 0.5.4\r\ncuda 12.1\r\n```\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nWe are attempting to utilize Ray v2.23 for batch inferencing, specifically on multi-modal data, by leveraging llava-next. \r\n\r\n```\r\ndataset = ray.data.read_parquet(gcsInputPath, columns=columns)\r\nclass LLMPredictor:\r\n\r\n    def __init__(self):\r\n        # Create an LLM.\r\n        self.llm = LLM(model=\"/mnt/models\",\r\n                       tensor_parallel_size=1)\r\n\r\n    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, list]:\r\n\r\n        try:\r\n            start_time = time.time()\r\n\r\n            prompts = [{\"prompt\": prompt, \"multi_modal_data\": {\r\n                \"image\": Image.open(io.BytesIO(base64.b64decode(batch[imageColumnName][i])))}} for i in\r\n                       range(len(batch[imageColumnName]))]\r\n\r\n            predictions = self.llm.generate(\r\n                prompts, sampling_params=sampling_params)\r\n            batch[\"generated_output\"] = [preds.outputs[0].text for preds in predictions]\r\n            end_time = time.time()\r\n            print(f'Total Inference Time for {len(prompts)} - {end_time - start_time}')\r\n\r\n        except OSError as os_error:\r\n            print(f\"OS error: {os_error}\")\r\n            batch[\"generated_output\"] = [\"\" for _ in range(len(batch[imageColumnName]))]\r\n\r\n        except Exception as error:\r\n            print(f\"Misc error: {error}\")\r\n            batch[\"generated_output\"] = [\"\" for _ in range(len(batch[imageColumnName]))]\r\n\r\n        finally:\r\n            del batch['image_bytes']\r\n            return batch\r\n\r\n\r\ndataset = dataset.map_batches(\r\n    LLMPredictor,\r\n    concurrency=int(workers) * int(gpus),\r\n    batch_size=int(batchSize),\r\n    num_gpus=1\r\n)\r\n\r\ndataset.write_parquet(gcsOutputPath)\r\n```\r\n\r\nAn error that we observe while executing the inference code for a batch size of 500 is as shown below:\r\n\r\n```\r\nTotal Inference Time for 480 - 164.62883067131042\r\nBatch Size is : 299 \r\nMisc error: Attempted to assign 2928 + 2928 + 1968 + 2928 + 2064 + 2256 + 2928 + 2928 + 1968 + 2928 + 2928 = 28752 image tokens to 28848 placeholders\r\n```",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-20T01:21:26+00:00",
    "closed_at": "2024-12-28T01:59:21+00:00",
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7669/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7669"
  },
  {
    "number": 14435,
    "title": "[Usage]: VLLM Inference - 2x slower with LoRA rank=256 vs none.",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\n```\n\n\n### How would you like to use vllm\n\nI've noticed that using LoRA with rank=256 significantly slows down inference by 4x, as shown below. However, reducing the rank to 8 or 16 brings performance closer to that of no LoRA. I'm currently using two fully-utilized GPUs, without the enforce_eager flag, and have set the maximum LoRA rank accordingly. Interestingly, adjusting the maximum model length had no impact on performance. What steps can I take to optimize performance?\n\n\n**No Lora**\n\n**Processed prompts**:   0%|\u258f                                                            | 5/2430 [01:28<6:58:39, 10.36s/it, est. speed input: 3.71 toks/s, output: 2.34 toks/s]Processed prompts:  10%|\u2588\u2588\u2588\u2588\u2588\u258a                                                     | 240/2430 [05:09<44:09,  1.21s/it, est. speed input: 87.79 toks/s, output: 90.18 toks/s]WARNING 03-06 17:12:30 scheduler.py:1754] Sequence group 352 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n**Processed prompt**s:  20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                             | 476/2430 [09:38<39:30,  1.21s/it, est. speed input: 106.63 toks/s, output: 117.32 toks/s]^\n\n**Lora rank = 16**\n\n**Processed prompts**:   0%|                                                                       | 0/2430 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]WARNING 03-07 11:35:15 scheduler.py:1754] Sequence group 238 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n**Processed prompts**:   0%|                                                            | 3/2430 [01:24<13:43:22, 20.36s/it, est. speed input: 2.31 toks/s, output: 1.25 toks/s]WARNING 03-07 11:36:05 scheduler.py:1754] Sequence group 187 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n**Processed prompts**:  11%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                    | 262/2430 [06:11<42:31,  1.18s/it, est. speed input: 84.40 toks/s, output: 88.40 toks/s]WARNING 03-07 11:40:46 scheduler.py:1754] Sequence group 342 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n**Processed prompts**:  18%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                               | 437/2430 [10:07<43:53,  1.32s/it, est. speed input: 96.26 toks/s, output: 105.08 toks/s]WARNING 03-07 11:44:38 scheduler.py:1754] Sequence group 569 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n\n**Lora rank = 256**\n\n**Processed prompts**:   0%|                                                                       | 0/2430 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]WARNING 03-06 17:25:54 scheduler.py:1754] Sequence group 255 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n**Processed prompts**:   0%|                                                            | 4/2430 [02:52<20:13:48, 30.02s/it, est. speed input: 1.50 toks/s, output: 0.86 toks/s]Processed prompts:  10%|\u2588\u2588\u2588\u2588\u2588\u258a                                                   | 246/2430 [10:13<1:19:59,  2.20s/it, est. speed input: 45.74 toks/s, output: 46.86 toks/s]WARNING 03-06 17:34:07 scheduler.py:1754] Sequence group 356 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n**Processed prompts**:  20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                               | 476/2430 [18:01<47:13,  1.45s/it, est. speed input: 57.00 toks/s, output: 61.91 toks/s]\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-07T11:58:26+00:00",
    "closed_at": null,
    "comments": 14,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14435/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14435"
  },
  {
    "number": 6312,
    "title": "[Feature]: control over llm_engine placement when multiple gpus are available.",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nI need a way to specify which gpu exactly should vllm use when multiple gpus are available. Currently, it automatically occupies all available gpus (https://docs.vllm.ai/en/latest/serving/distributed_serving.html).\r\n\r\nFor example, something like this: `vllm.LLM(model_path, device=\"cuda:N\")`\r\n\r\n#691 is exactly the same question but they end up agreeing that they can use Ray. I'm asking for a simpler solution that would not require spending time on extra engineering.\n\n### Alternatives\n\nMy use-case doesn't allow me to use CUDA_VISIBLE_DEVICES to specify which gpu to use. That's because i train a model on multiple gpus in a DDP-like fashion where each vllm instance generates data for a model on its device, then gradients are synchronized and so on. So I cannot set CUDA_VISIBLE_DEVICES to some specific device as that would turn multiple-gpu training in a single-gpu training.\r\n\r\nAlso, I cannot just avoid this problem by running a vllm-server on a separate gpu because I need to substitute model weights (loras) on-the-fly and currently this is not available (#3446).\n\n### Additional context\n\nSo I either need a way to specify which gpu to use, or have the #3446 PR completed so I can run a server.",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-10T16:01:34+00:00",
    "closed_at": "2024-11-25T02:04:39+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6312/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6312"
  },
  {
    "number": 10540,
    "title": "[Installation]: can't get the cu118 version of vllm 0.6.3 by https://github.com/vllm-project/vllm/releases/download/v0.6.3/vllm-0.6.3+cu118-cp310-cp310-manylinux1_x86_64.whl",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### How you are installing vllm\n\n```sh\r\npip install -vvv vllm\r\n```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-21T14:28:49+00:00",
    "closed_at": "2025-03-22T02:02:48+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10540/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10540"
  },
  {
    "number": 9750,
    "title": "[Usage]: Can I get the loss of model directly?",
    "body": "Hi, great work!\r\nI am currently optimizing LLM based on `vLLM` and need to test whether my optimizations affect the model's perplexity. Therefore, I want to obtain the model's cross-entropy loss. I have reviewed the issue: [Can I directly obtain the logits here?](https://github.com/vllm-project/vllm/issues/185) and understand that one way to get log probabilities is by setting the `logprobs` parameter in `SampleParams`. \r\n\r\nHowever, this method is not very convenient. We can only obtain the top-n most likely log probabilities for each token, and the probability of the correct token might not be among these top-n log probabilities. Setting `n` and searching for the probability of the correct token is quite cumbersome, and the cross-entropy has to be calculated manually as well. \r\n\r\nTherefore, I want to know if `vLLM` has a way to directly obtain cross-entropy, similar to `transformers`. \r\nThank you sincerely for your help. :-)",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "open",
    "created_at": "2024-10-28T08:05:33+00:00",
    "closed_at": null,
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9750/reactions",
      "total_count": 6,
      "+1": 6,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/9750"
  },
  {
    "number": 14933,
    "title": "[Usage]: Distributed inference not supported with OpenVINO?",
    "body": "### How would you like to use vllm\n\nThe [installation page for OpenVINO](https://docs.vllm.ai/en/latest/getting_started/installation/ai_accelerator.html?device=openvino) mentions using the environment variable \"VLLM_OPENVINO_DEVICE to specify which device utilize for the inference. If there are multiple GPUs in the system, additional indexes can be used to choose the proper one (e.g, VLLM_OPENVINO_DEVICE=GPU.1). If the value is not specified, CPU device is used by default.\"\n\nSo is it not possible to use multiple GPUs or GPU + CPU for running inference on OpenVINO backend?\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-03-17T07:06:59+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14933/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14933"
  },
  {
    "number": 7068,
    "title": "[Usage]: OpenAI API for Phi-3-vision-128k-instruct ",
    "body": "\r\n```text\r\nBadRequestError: Error code: 400 - {'object': 'error', 'message': 'Attempted to assign 1 x 2509 = 2509 image tokens to 0 placeholders', 'type': 'BadRequestError', 'param': None, 'code': 400}\r\n```\r\ncalling using following function:\r\n```python\r\ndef prepare_prompts(self, prompts, images):\r\n        messages = []\r\n        #re.sub(r\"<\\|.*?\\|>\", \"\", )\r\n        for i in range(len(prompts)):\r\n            if i % 2 == 0:\r\n                content = [\r\n                    {\r\n                        \"type\": \"text\",\r\n                        \"text\": prompts[i]\r\n                    }\r\n                ]\r\n                if images[i]:\r\n                    img_byte_arr = io.BytesIO()\r\n                    images[i].save(img_byte_arr, format='PNG')\r\n                    img_byte_arr = img_byte_arr.getvalue()\r\n                    image_base64 = base64.b64encode(img_byte_arr).decode('utf-8')\r\n                    content.append(\r\n                        {\r\n                            \"type\": \"image_url\",\r\n                            \"image_url\": {\r\n                                \"url\": f\"data:image/jpeg;base64,{image_base64}\"\r\n                            }\r\n                        }\r\n                    )\r\n                messages.append({\"role\": \"user\", \"content\": content})\r\n            else:\r\n                messages.append({\"role\": \"assistant\", \"content\": prompts[i]})\r\n        return messages\r\n    ```\r\n\r\n    I tried two format for prompts[i].\r\n    1. \"Describe this image\"\r\n    2. \"<|image_1|>\\n Describe this image\"\r\n    \r\nGetting same error for both prompts. \r\n",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-08-02T06:55:10+00:00",
    "closed_at": "2024-08-02T08:05:13+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7068/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7068"
  },
  {
    "number": 8392,
    "title": "[Usage]: RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method",
    "body": "### Your current environment\n\nI used the same service deployment command, but when I upgraded from 0.5.5 to 0.6.1 today, the deployment went wrong\r\n\r\n![20240912-095248](https://github.com/user-attachments/assets/278b91a6-6a35-4c0c-b956-f20c5e09997e)\r\n\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-09-12T01:54:55+00:00",
    "closed_at": "2024-09-12T03:35:34+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8392/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8392"
  },
  {
    "number": 4449,
    "title": "[Usage]: what is enforce_eager",
    "body": "### Your current environment\n\nvllm 0.4.0\r\ncuda 12.1\r\n2*v100-16G\r\nqwen1.5 Moe\n\n### How would you like to use vllm\n\nwhat is enforce_eager?\r\nand when it's enabled, will the inference become slower?",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-04-29T07:14:26+00:00",
    "closed_at": "2024-05-01T13:38:09+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4449/reactions",
      "total_count": 7,
      "+1": 7,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4449"
  },
  {
    "number": 16243,
    "title": "[Usage]: Failed to get global TPU topology.",
    "body": "### Your current environment\n\nPyTorch version: 2.8.0\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\nClang version: Could not collect\nCMake version: version 4.0.0\nLibc version: glibc-2.31\n\nPython version: 3.10.16 (main, Jan 14 2025, 05:27:07) [GCC 10.2.1 20210110] (64-bit runtime)\nPython platform: Linux-5.19.0-1022-gcp-x86_64-with-glibc2.31\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nByte Order:                      Little Endian\nAddress sizes:                   52 bits physical, 57 bits virtual\nCPU(s):                          44\nOn-line CPU(s) list:             0-43\nThread(s) per core:              2\nCore(s) per socket:              22\nSocket(s):                       1\nNUMA node(s):                    1\nVendor ID:                       AuthenticAMD\nCPU family:                      25\nModel:                           17\nModel name:                      AMD EPYC 9B14\nStepping:                        1\nCPU MHz:                         2599.996\nBogoMIPS:                        5199.99\nHypervisor vendor:               KVM\nVirtualization type:             full\nL1d cache:                       704 KiB\nL1i cache:                       704 KiB\nL2 cache:                        22 MiB\nL3 cache:                        64 MiB\nNUMA node0 CPU(s):               0-43\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Not affected\nVulnerability Retbleed:          Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr wbnoinvd arat avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm\n\nVersions of relevant libraries:\n[pip3] numpy==2.2.2\n[pip3] pyzmq==26.4.0\n[pip3] torch==2.8.0\n[pip3] torch-xla==2.8.0+gitac9a39f\n[pip3] transformers==4.51.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3rc2.dev57+g8e5314a4\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nVLLM_TARGET_DEVICE=tpu\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\n\nI am trying to serve Qwen2.5-1.5B-Instruct model on v6e TPU using docker (as mentioned in TPU installation documentation), but I am getting the following error:\n```\nERROR 03-24 14:29:42 [engine.py:448] RuntimeError: Bad StatusOr access: INTERNAL: Failed to get global TPU topology.\n```\nHowever, the model is getting deployed in v5e TPU by following the same process mentioned in the documentation.\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-08T07:49:20+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16243/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16243"
  },
  {
    "number": 16319,
    "title": "[Usage]: how to redirect save logs to local file.",
    "body": "### Your current environment\n\ni am using docker 0.8.2 to run the model, output of collect_env.py\n```text\nThe output of `Collecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.31.6\nLibc version: glibc-2.35\n\nPython version: 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-78-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: Tesla T4\nNvidia driver version: 550.54.15\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   46 bits physical, 48 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          32\nOn-line CPU(s) list:             0-31\nVendor ID:                       GenuineIntel\nModel name:                      Intel(R) Xeon(R) Silver 4215 CPU @ 2.50GHz\nCPU family:                      6\nModel:                           85\nThread(s) per core:              2\nCore(s) per socket:              8\nSocket(s):                       2\nStepping:                        7\nCPU max MHz:                     3500.0000\nCPU min MHz:                     1000.0000\nBogoMIPS:                        5000.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear flush_l1d arch_capabilities\nVirtualization:                  VT-x\nL1d cache:                       512 KiB (16 instances)\nL1i cache:                       512 KiB (16 instances)\nL2 cache:                        16 MiB (16 instances)\nL3 cache:                        22 MiB (2 instances)\nNUMA node(s):                    2\nNUMA node0 CPU(s):               0-7,16-23\nNUMA node1 CPU(s):               8-15,24-31\nVulnerability Itlb multihit:     KVM: Mitigation: VMX disabled\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:          Mitigation; Enhanced IBRS\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Mitigation; TSX disabled\n\nVersions of relevant libraries:\n[pip3] flashinfer-python==0.2.1.post2+cu124torch2.6\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] triton==3.2.0\n[conda] Could not collect`\n```\n\n\n### How would you like to use vllm\n\ni follow the guide to save all logs to local file by this [https://docs.vllm.ai/en/latest/getting_started/examples/logging_configuration.html](url)\nthis is my docker command\n```docker\ndocker run -d --restart always --gpus all     --name vllm-openai     -v /data/share/models:/data/models     -p 8000:8000     --ipc=host  -e VLLM_LOGGING_CONFIG_PATH=/data/models/logging_config.json   -e VLLM_LOGGING_LEVEL=DEBUG  vllm/vllm-openai:latest      --model /data/models/DeepSeek-R1-Distill-Qwen-7B  \n```\n\nthis is my logging.json \n\n```json\n{\n\t\"version\": 1,\n\t\"disable_existing_loggers\": false,\n\t\"formatters\": {\n\t\t\"simpleFormatter\": {\n\t\t\t\"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n\t\t}\n\t},\n\t\"handlers\": {\n\t\t\"fileHandler\": {\n\t\t\t\"class\": \"logging.FileHandler\",\n\t\t\t\"level\": \"INFO\",\n\t\t\t\"formatter\": \"simpleFormatter\",\n\t\t\t\"filename\": \"/data/models/logs/vllm.log\",\n\t\t\t\"encoding\": \"utf-8\"\n\t\t}\n\t},\n\t\"loggers\": {\n\t\t\"vllm\": {\n            \"handlers\": [\"fileHandler\"],\n            \"level\": \"DEBUG\",\n            \"propagate\": false\n        }\n\t},\n\t\"root\": {\n\t\t\"handlers\": [\"fileHandler\"],\n\t\t\"level\": \"WARNING\"\n\t}\n}\n```\nthese logs will fine to save to local file\n```text\n2025-04-08 23:05:03,917 - vllm.entrypoints.launcher - INFO - Route: /pooling, Methods: POST\n2025-04-08 23:05:03,917 - vllm.entrypoints.launcher - INFO - Route: /score, Methods: POST\n2025-04-08 23:05:03,917 - vllm.entrypoints.launcher - INFO - Route: /v1/score, Methods: POST\n2025-04-08 23:05:03,917 - vllm.entrypoints.launcher - INFO - Route: /v1/audio/transcriptions, Methods: POST\n2025-04-08 23:05:03,917 - vllm.entrypoints.launcher - INFO - Route: /rerank, Methods: POST\n2025-04-08 23:05:03,917 - vllm.entrypoints.launcher - INFO - Route: /v1/rerank, Methods: POST\n2025-04-08 23:05:03,917 - vllm.entrypoints.launcher - INFO - Route: /v2/rerank, Methods: POST\n2025-04-08 23:05:03,917 - vllm.entrypoints.launcher - INFO - Route: /invocations, Methods: POST\n2025-04-08 23:05:31,964 - vllm.entrypoints.chat_utils - INFO - Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n2025-04-08 23:05:31,970 - vllm.entrypoints.logger - INFO - Received request chatcmpl-f7f250513305480fbe788a6deb4dae8b: prompt: '<\uff5cbegin\u2581of\u2581sentence\uff5c><\uff5cUser\uff5c>\u4f60\u662f\u8c01?<\uff5cAssistant\uff5c><think>\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n2025-04-08 23:05:31,971 - vllm.engine.multiprocessing.engine - INFO - Added request chatcmpl-f7f250513305480fbe788a6deb4dae8b.\n2025-04-08 23:05:32,243 - vllm.engine.metrics - INFO - Avg prompt throughput: 1.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n2025-04-08 23:05:37,448 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.\n2025-04-08 23:05:42,646 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.\n2025-04-08 23:05:47,844 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.\n2025-04-08 23:05:58,536 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n2025-04-08 23:06:08,547 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n```\n\nbut these log information still print at console, i dont want to save these infomation by > to local file, how can i use the same configuration file to save these logs to local also.\n```test\n\n[W408 23:27:45.631650662 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W408 23:28:05.649792944 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.67s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.81s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.79s/it]\n\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     100.64.10.54:47584 - \"GET /v1/models HTTP/1.1\" 200 OK\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-09T06:31:49+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16319/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16319"
  },
  {
    "number": 13322,
    "title": "[Usage]: How can I use temperature correctly for Qwen2-VL?",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\n```\n\n\n### How would you like to use vllm\n\nI want to run inference of Qwen2-VL-2B. The code is:\n```\n# Qwen2-VL\ndef init_qwen2_vl(model_name_or_path: str, **kwargs):\n    from vllm import LLM\n    try:\n        from qwen_vl_utils import process_vision_info\n    except ModuleNotFoundError:\n        print('WARNING: `qwen-vl-utils` not installed, input images will not '\n              'be automatically resized. You can enable this functionality by '\n              '`pip install qwen-vl-utils`.')\n        process_vision_info = None\n\n    model_name = model_name_or_path\n\n    llm = LLM(\n        model=model_name,\n        device=kwargs['device'], \n        max_model_len=kwargs.get(\"max_context_len\", 4096 if process_vision_info is not None else 32768),  \n        enable_prefix_caching=True,\n        enforce_eager=True,\n        disable_mm_preprocessor_cache=kwargs.get(\"disable_mm_preprocessor_cache\", True),\n    )\n    stop_token_ids = None\n    return llm, stop_token_ids, process_vision_info\n\n```\nThe generation code is:\n```\nmessages={\n    \"role\": \"user\", \n    \"content\": [{\"type\": \"image\", \"image\": img} for img in images ] +[\n        {\"type\": \"text\", \"text\": text}\n    ]\n}\ntext_prompt = self.processor.apply_chat_template(state['state'], add_generation_prompt=True, tokenizer=False)\nif self.process_vision_info is not None:\n    image_inputs, _ = self.process_vision_info(messages)\nelse:\n    image_inputs = state[\"images\"]\n\nsampling_params = SamplingParams(\n    max_tokens=inputs.get(\"max_new_tokens\", 512),\n    # Qwen2-VL default\n    temperature=inputs.get(\"temperature\",0.01),\n    top_k= 1,\n    top_p= inputs.get(\"top_p\",0.7),\n    stop_token_ids=self.stop_token_ids\n)\n\noutput_text = self.model.generate(\n    {\n        \"prompt\": text_prompt, \n        \"multi_modal_data\": {\n            \"image\": image_inputs\n        }, \n    },\n    sampling_params=sampling_params,\n    use_tqdm=False, \n)[0].outputs[0].text\n```\nBut when I use different temperature between 0.1 to 0.9. The output is totally the same. I wonder if I use vllm in a right way to generate sequences with Qwen2-VL.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-02-15T07:49:44+00:00",
    "closed_at": "2025-02-17T09:21:11+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13322/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13322"
  },
  {
    "number": 8762,
    "title": "[Usage]: how to acquire logits in vllm",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### How would you like to use vllm\n\nI want to acquire logits when I run benchmark_throughput.py to do the softmax optimization, but the output in vllm doesn't have logits, how can I acquire it.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-24T06:42:51+00:00",
    "closed_at": "2025-01-24T01:58:47+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8762/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8762"
  },
  {
    "number": 12765,
    "title": "[Usage]: \u8bf7\u95ee\u5982\u4f55\u7528vllm\u8fdb\u884c\u591a\u673a\u90e8\u7f72\uff1f",
    "body": "### Your current environment\n\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.10.16 (main, Dec 11 2024, 16:24:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-130-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.3.107\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA GeForce RTX 4090\nGPU 1: NVIDIA GeForce RTX 4090\nGPU 2: NVIDIA GeForce RTX 4090\nGPU 3: NVIDIA GeForce RTX 4090\nGPU 4: NVIDIA GeForce RTX 4090\nGPU 5: NVIDIA GeForce RTX 4090\nGPU 6: NVIDIA GeForce RTX 4090\nGPU 7: NVIDIA GeForce RTX 4090\n\nNvidia driver version: 550.142\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.4.0\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn.so.8\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8\n/usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn.so.8\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn_adv_train.so.8\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn_ops_train.so.8\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn.so.8\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_adv_train.so.8\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8\n/usr/local/cuda-12.4/targets/x86_64-linux/lib/libcudnn_ops_train.so.8\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               128\nOn-line CPU(s) list:                  0-127\nVendor ID:                            GenuineIntel\nModel name:                           INTEL(R) XEON(R) PLATINUM 8562Y+\nCPU family:                           6\nModel:                                207\nThread(s) per core:                   2\nCore(s) per socket:                   32\nSocket(s):                            2\nStepping:                             2\nCPU max MHz:                          4100.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             5600.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            3 MiB (64 instances)\nL1i cache:                            2 MiB (64 instances)\nL2 cache:                             128 MiB (64 instances)\nL3 cache:                             120 MiB (2 instances)\nNUMA node(s):                         2\nNUMA node0 CPU(s):                    0-31,64-95\nNUMA node1 CPU(s):                    32-63,96-127\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] json2onnx==2.0.3\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] onnx==1.15.0\n[pip3] onnx-graphsurgeon==0.5.2\n[pip3] onnx-simplifier==0.4.33\n[pip3] onnx-tf==1.9.0\n[pip3] onnx2json==2.0.4\n[pip3] onnx2tf==1.15.4\n[pip3] onnxruntime-gpu==1.19.0\n[pip3] optree==0.13.1\n[pip3] pyzmq==26.2.1\n[pip3] sam4onnx==1.0.16\n[pip3] sbi4onnx==1.0.7\n[pip3] scc4onnx==1.0.6\n[pip3] scs4onnx==1.0.18\n[pip3] sde4onnx==1.0.0\n[pip3] sed4onnx==1.0.5\n[pip3] sentence-transformers==3.3.1\n[pip3] simple_onnx_processing_tools==1.1.32\n[pip3] sio4onnx==1.0.2\n[pip3] sit4onnx==1.0.8\n[pip3] sna4onnx==1.0.6\n[pip3] snc4onnx==1.0.13\n[pip3] snd4onnx==1.1.6\n[pip3] sne4onnx==1.0.13\n[pip3] sng4onnx==1.0.4\n[pip3] soa4onnx==1.0.4\n[pip3] soc4onnx==1.0.2\n[pip3] sod4onnx==1.0.0\n[pip3] sog4onnx==1.0.17\n[pip3] sor4onnx==1.0.7\n[pip3] spo4onnx==1.0.5\n[pip3] ssc4onnx==1.0.8\n[pip3] ssi4onnx==1.0.4\n[pip3] svs4onnx==1.0.0\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.2\n[pip3] triton==3.1.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] optree                    0.13.1                   pypi_0    pypi\n[conda] pyzmq                     26.2.1                   pypi_0    pypi\n[conda] sentence-transformers     3.3.1                    pypi_0    pypi\n[conda] torch                     2.5.1                    pypi_0    pypi\n[conda] torchaudio                2.5.1                    pypi_0    pypi\n[conda] torchvision               0.20.1                   pypi_0    pypi\n[conda] transformers              4.48.2                   pypi_0    pypi\n[conda] triton                    3.1.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU1\tPIX\t X \tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU2\tPIX\tPIX\t X \tPIX\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU3\tPIX\tPIX\tPIX\t X \tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU4\tSYS\tSYS\tSYS\tSYS\t X \tPIX\tPIX\tPIX\t32-63,96-127\t1\t\tN/A\nGPU5\tSYS\tSYS\tSYS\tSYS\tPIX\t X \tPIX\tPIX\t32-63,96-127\t1\t\tN/A\nGPU6\tSYS\tSYS\tSYS\tSYS\tPIX\tPIX\t X \tPIX\t32-63,96-127\t1\t\tN/A\nGPU7\tSYS\tSYS\tSYS\tSYS\tPIX\tPIX\tPIX\t X \t32-63,96-127\t1\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=/data/miniconda3/envs/fffan_debug/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64\nCUDA_HOME=/usr/local/cuda\nCUDA_HOME=/usr/local/cuda\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n\n### How would you like to use vllm\n\n**1\uff1a**\u6211\u6709\u4e24\u53f04090\u663e\u5361\u7684\u673a\u5668\uff0c\u60f3\u8fdb\u884c\u591a\u673a\u90e8\u7f72\u5927\u6a21\u578b\u3002\u60f3\u95ee\u4e0b\uff0c\u6211\u8be5\u600e\u4e48\u7528`VLMM`\u8fdb\u884c\u591a\u673a\u90e8\u7f72\uff1f\u6709\u6ca1\u6709\u6d41\u7a0b\u6837\u4f8b\u4e4b\u7c7b\u7684\u4fe1\u606f\u53c2\u8003\u4e00\u4e0b\u3002\n**2\uff1a**\u8bf7\u95ee\u4e0b\u6709\u6ca1\u6709\u5173\u4e8e`VLLM`\u7684\u6c9f\u901a\u7fa4\u554a\uff0c\u7fa4\u6c9f\u901a\u66f4\u65b9\u4fbf\u4e00\u4e9b~\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-02-05T03:24:26+00:00",
    "closed_at": "2025-02-05T03:25:00+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12765/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12765"
  },
  {
    "number": 10953,
    "title": "[Usage]: How to use a Python script to start a FastAPI service for internvl2-8b with vllm, instead of using the terminal command vllm serve ./internvl2-1b/ --tensor-parallel-size 1 --trust-remote-code? Is there any sample code for this?",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2024-12-06T14:35:00+00:00",
    "closed_at": "2024-12-06T15:30:54+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10953/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10953"
  },
  {
    "number": 6877,
    "title": "[Feature]: Support Python 3.12",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nI believe we eventually need to support 3.12 in the future. Right now I believe Pytorch just added support for Python 3.12, but ray still does not support Python 3.12. Let's use this issue to keep track on this.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-07-28T21:26:05+00:00",
    "closed_at": "2024-08-02T20:51:23+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6877/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/6877"
  },
  {
    "number": 13679,
    "title": "[Feature]: Support for LoRA for Pooling Models",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently vLLM does not support LoRA for Pooling models (according to #12808)\nI wanted to understand how can we add the support given we can use similar code structure like generation models.\n\nWhat all things need to be taken care if I need to add the support. Any references will be helpful.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2025-02-21T17:05:48+00:00",
    "closed_at": "2025-03-18T12:07:02+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13679/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/13679"
  },
  {
    "number": 14580,
    "title": "[Feature][Hardware][TPU]: Add Recompilation Check for vLLM on TPU",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nIdeally, post-warmup, no further compilation should occur. However, PyTorch/XLA's implicit compilation can lead to excessive recompilation during LLM serving, impacting performance. We can add an option to detect recompilation after warmup, requiring a PyTorch/XLA method like xm.num_graph_hash() to track the number of captured graphs. This number should remain constant post-warmup if no recompilation occurs.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "tpu"
    ],
    "state": "closed",
    "created_at": "2025-03-10T22:31:05+00:00",
    "closed_at": "2025-03-25T16:59:34+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14580/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14580"
  },
  {
    "number": 20256,
    "title": "[Feature]: Limit total GPU memory",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nEven with appropriate arguments to lower memory usage specified, as suggested in docs:\n\n```\n--enforce-eager\n--max-model-len=8192\n--max-num-batched-tokens=8192\n--max-num-seqs=16\n```\n\nthe rest of the GPU memory still gets eaten up by KV cache, so even when running a tiny 1B model that needs 3 GB of VRAM, VLLM takes 71GB on A100 to run this model. This makes VLLM an impractical solution for mixed-GPU clusters.\n\nSuggestions:\n- add flag to specify max gpu memory in absolute units\n- add ability to explicitly limit/disable KV cache\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-06-30T12:52:05+00:00",
    "closed_at": null,
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20256/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20256"
  },
  {
    "number": 1802,
    "title": "Feature request: prompt lookup decoding",
    "body": "Prompt lookup decoding (PLD) is a variant of speculative decoding that replaces the draft model with a prefix lookup in the current sequence, resulting in a 2-4x throughput boost for input-grounded tasks like summarization and code modification.\r\n\r\nBecause PLD doesn't require a secondary model, it might be easier to implement in VLLM?\r\n\r\nSee https://github.com/apoorvumang/prompt-lookup-decoding for details.\r\n",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2023-11-27T19:04:37+00:00",
    "closed_at": "2025-03-10T01:52:03+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1802/reactions",
      "total_count": 20,
      "+1": 14,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 6
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/1802"
  },
  {
    "number": 174,
    "title": "GPTQ / Quantization support?",
    "body": "Will vLLM support 4-bit GPTQ models?",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2023-06-21T02:40:47+00:00",
    "closed_at": "2024-03-06T09:01:49+00:00",
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/174/reactions",
      "total_count": 22,
      "+1": 22,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/174"
  },
  {
    "number": 10562,
    "title": "[Feature]: How to run speculative models with tensor parallelism?",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nI noticed that the current speculative mode does not support tp from this link (https://docs.vllm.ai/en/stable/models/spec_decode.html). \r\n\r\nHowever, not supporting TP will greatly limit the choice of speculative models. I would like to know why there is no TP support for speculative models. I am trying to read and modify this part of the code, but I don't understand why the scorer model can support TP, but the speculative model cannot. What are the considerations in system design?\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-22T03:30:54+00:00",
    "closed_at": "2025-03-23T02:08:46+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10562/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10562"
  },
  {
    "number": 7546,
    "title": "[Feature]: Inquiry about Multi-modal Support in VLLM for MiniCPM-V2.6",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nI am currently exploring the capabilities of the VLLM library and am interested in understanding its support for multi-modal inputs, particularly for models like MiniCPM-V2.6. I would like to know if VLLM is designed to handle multi-image and video inputs for such models.\n\n### Alternatives\n\n1. **Model of Interest**: MiniCPM-V2.6\r\n2. **Types of Input**: Multi-image and video\r\n3. **Current Understanding**:\r\n   - I have reviewed the documentation and initial examples provided with VLLM.\r\n  - It seems that both `multiple 'image_url' input` and `list value in image_url` is currently not supported.\r\n  - However, I am not sure if it supports the processing of multiple images or videos as input to a model like MiniCPM-V2.6.\r\n## Questions\r\n 1. Does VLLM support the integration of MiniCPM-V2.6 for processing multi-image and video inputs?\r\n 2. If yes, could you provide an example or a guide on how to set up and use this feature?\r\n 3. If not, are there any plans to extend VLLM's capabilities to support such inputs in the future?\n\n### Additional context\n\n![image](https://github.com/user-attachments/assets/627dd626-dee1-41cb-b231-9c13163e9174)\r\n",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-08-15T06:36:05+00:00",
    "closed_at": "2024-08-15T06:49:48+00:00",
    "comments": 19,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7546/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7546"
  },
  {
    "number": 4838,
    "title": "[Feature]: Build and publish Neuron docker image",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nIt seems like the current docker images don't support Neuron (Inferentia).\r\nIt would be very helpful if there was a tested, managed Neuron docker image to use.\r\nWhile at the same subject, it would be even better if some documentation would be added on running vLlm Neuron using containers.\n\n### Alternatives\n\nDJL?\n\n### Additional context\n\n_No response_",
    "labels": [
      "feature request",
      "keep-open"
    ],
    "state": "open",
    "created_at": "2024-05-15T15:27:17+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4838/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4838"
  },
  {
    "number": 20950,
    "title": "[Feature]: Model execution timeout",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nCurrently when there is a bug in the path of model execution, it could hang indefinitely and user may not get timely and useful feedback. It would be useful to have a timeout mechanism and signal the user.\n\nCurrently RayDistributedExecutor with Compiled Graph supports a timeout, but vLLM could benefit from supporting this in executor in general.\n\nSee [discussion](https://vllm-dev.slack.com/archives/C08CBAP9BUG/p1752532064610089?thread_ts=1752477597.112679&cid=C08CBAP9BUG)\n\ncc @stephanie-wang  @youkaichao \n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-07-14T22:50:17+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20950/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "COLLABORATOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/20950"
  },
  {
    "number": 4163,
    "title": "[Installation]: import llm meet error",
    "body": "### Your current environment\n\n```text\r\nTraceback (most recent call last):\r\n  File \"inference.py\", line 355, in <module>\r\n    data_all_with_response = get_pred_func(data=data_all, task_prompt=task_prompt,\\\r\n  File \"inference.py\", line 24, in get_pred_vllm\r\n    from vllm import LLM, SamplingParams\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/__init__.py\", line 3, in <module>\r\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/engine/arg_utils.py\", line 6, in <module>\r\n    from vllm.config import (CacheConfig, ModelConfig, ParallelConfig,\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/config.py\", line 9, in <module>\r\n    from vllm.utils import get_cpu_memory, is_hip\r\n  File \"/usr/local/lib/python3.8/dist-packages/vllm/utils.py\", line 8, in <module>\r\n    from vllm._C import cuda_utils\r\nImportError: /usr/local/lib/python3.8/dist-packages/vllm/_C.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN2at4_ops15to_dtype_layout4callERKNS_6TensorEN3c108optionalINS5_10ScalarTypeEEENS6_INS5_6LayoutEEENS6_INS5_6DeviceEEENS6_IbEEbbNS6_INS5_12MemoryFormatEEE```\r\n\n\n### How you are installing vllm\n\n```sh\r\nexport VLLM_VERSION=0.2.4\r\nexport PYTHON_VERSION=38\r\npip install https://github.com/vllm-project/vllm/releases/download/v${VLLM_VERSION}/vllm-${VLLM_VERSION}+cu118-cp${PYTHON_VERSION}-cp${PYTHON_VERSION}-manylinux1_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu118\r\n```\r\n",
    "labels": [
      "installation",
      "unstale"
    ],
    "state": "closed",
    "created_at": "2024-04-18T07:06:57+00:00",
    "closed_at": "2025-01-14T13:57:43+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4163/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4163"
  },
  {
    "number": 9960,
    "title": "[Installation]: I was never able to install it, which cuda version is required?",
    "body": "### Your current environment\r\n\r\nI use ubunt 22.04\r\n\r\nInstalling this is almost impossible, what are actually requirements lets say for cuda. I spend many hours trying to install and never worked, there way always an error, something related to cuda version.\r\n\r\n\r\n### How you are installing vllm\r\n\r\n```sh\r\npip install -vvv vllm\r\n```\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-11-02T22:53:06+00:00",
    "closed_at": "2025-03-03T02:03:22+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9960/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9960"
  },
  {
    "number": 10036,
    "title": "[Installation]: Missing v0.6.3.post1-cu118-cp310.whl. Can share it? Thanks so much",
    "body": "### Your current environment\n\nMissing v0.6.3.post1-cu118-cp310.whl. Can share it? Thanks so much\n\n### How you are installing vllm\n\nMissing v0.6.3.post1-cu118-cp310.whl. Can share it? Thanks so much\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "unstale"
    ],
    "state": "closed",
    "created_at": "2024-11-05T12:46:28+00:00",
    "closed_at": "2025-04-15T03:15:12+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10036/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/10036"
  },
  {
    "number": 14033,
    "title": "[Installation]: Dockerfile.cpu installation problem vLLM",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\n```\nDockerfile.cpu installation I can't complete the build somehow, I want to use vLLM over CPU since I don't have a graphics card on my own server, but the installation gives an error as follows.\nOS= rockylinux 9.4 \nram 16gb\nvCPU=> 24\nHypervisor= proxmox 8.2.7\ndocker version => Docker version 28.0.1, build 068a01e\n\nerrors messages;\n\ndocker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .\n\nDockerfile.cpu:54\n--------------------\n  53 |     \n  54 | >>> RUN --mount=type=cache,target=/root/.cache/pip \\\n  55 | >>>     --mount=type=cache,target=/root/.cache/ccache \\\n  56 | >>>     --mount=type=bind,source=.git,target=.git \\\n  57 | >>>     VLLM_TARGET_DEVICE=cpu python3 setup.py bdist_wheel && \\\n  58 | >>>     pip install dist/*.whl && \\\n  59 | >>>     rm -rf dist\n  60 |     \n--------------------\nERROR: failed to solve: process \"/bin/sh -c VLLM_TARGET_DEVICE=cpu python3 setup.py bdist_wheel &&     pip install dist/*.whl &&     rm -rf dist\" did not complete successfully: exit code: 1\n\n### How you are installing vllm\n\ndocker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-28T10:02:14+00:00",
    "closed_at": "2025-07-02T02:14:03+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14033/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14033"
  },
  {
    "number": 9385,
    "title": "[Installation]: Installation instructions for ROCm can be mainlined",
    "body": "### Your current environment\n\nN/A\r\n\n\n### How you are installing vllm\n\nhttps://docs.vllm.ai/en/stable/getting_started/amd-installation.html option 2\r\n\r\nThe problem is that it says to checkout a very specific commit of triton. Triton just published a new version of 3.1 that has AMD support mainlined but the dependency in the vllm pip package still tries to install 3.0. If someone tells me how I can update the dependency on 3.1 we can simplify the AMD instructions I think.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-15T18:21:50+00:00",
    "closed_at": "2025-02-13T01:59:19+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9385/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9385"
  },
  {
    "number": 436,
    "title": "Running to (Installing build dependencies ) this step is stuck",
    "body": "![image](https://github.com/vllm-project/vllm/assets/54533917/99997cfa-f6f6-4de1-9641-0e4c90884256)\r\n\r\n\r\nsystem\uff1a ubuntu 20.04\r\nNvidia driver 515\r\ncuda 11.7\r\nrtx3090\r\npython 3.8\r\nvllm 0.1.2",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2023-07-12T03:08:05+00:00",
    "closed_at": "2024-03-08T12:27:03+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/436/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/436"
  },
  {
    "number": 6769,
    "title": "[Installation]: Unable to build docker image using Dockerfile.openvino",
    "body": "### Your current environment\n\n```text\r\n(base) user@zahid:~/vllm$ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.2 LTS (x86_64)\r\nGCC version: Could not collect\r\nClang version: Could not collect\r\nCMake version: version 3.30.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.12.1 | packaged by Anaconda, Inc. | (main, Jan 19 2024, 15:51:05) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1026-intel-iotg-x86_64-with-glibc2.35\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: N/A\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Gold 6338N CPU @ 2.20GHz\r\nCPU family:                      6\r\nModel:                           106\r\nThread(s) per core:              2\r\nCore(s) per socket:              32\r\nSocket(s):                       2\r\nStepping:                        6\r\nCPU max MHz:                     3500.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        4400.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\r\nVirtualization:                  VT-x\r\nL1d cache:                       3 MiB (64 instances)\r\nL1i cache:                       2 MiB (64 instances)\r\nL2 cache:                        80 MiB (64 instances)\r\nL3 cache:                        96 MiB (2 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126\r\nNUMA node1 CPU(s):               1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n\r\n```\r\n\n\n### How you are installing vllm\n\nTried building vllm:openvino image using [Dockerfile.openvino](https://github.com/vllm-project/vllm/blob/main/Dockerfile.openvino). However, the docker image build is failing with below error.\r\n\r\n```sh\r\ndocker build -t vllm:openvino -f Dockerfile.openvino .\r\n```\r\n\r\n\r\nBelow is the docker build error:\r\n\r\n--------------------\r\n\r\n[ 3/12] WORKDIR /workspace                                                                                           0.0s\r\n[ 4/12] COPY requirements-build.txt /workspace/vllm/                                                                 0.0s\r\n[ 5/12] COPY requirements-common.txt /workspace/vllm/                                                                0.0s\r\n[ 6/12] COPY requirements-openvino.txt /workspace/vllm/                                                              0.0s\r\n[ 7/12] COPY vllm/ /workspace/vllm/vllm                                                                              0.1s\r\n[ 8/12] COPY setup.py /workspace/vllm/                                                                               0.0s\r\nERROR [ 9/12] RUN PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cpu\" python3 -m pip install -r /workspace/  21.9s\r\n > [ 9/12] RUN PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cpu\" python3 -m pip install -r /workspace/vllm/requirements-build.txt:\r\n1.698 Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\r\n3.670 Collecting cmake>=3.21\r\n3.779   Downloading cmake-3.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.9 MB)\r\n6.573 Collecting ninja\r\n6.589   Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\r\n7.599 Collecting packaging\r\n7.612   Downloading packaging-24.1-py3-none-any.whl (53 kB)\r\n9.064 Collecting setuptools>=49.4.0\r\n9.077   Downloading setuptools-71.1.0-py3-none-any.whl (2.3 MB)\r\n11.10 Collecting torch==2.3.1\r\n11.11   Downloading https://download.pytorch.org/whl/cpu/torch-2.3.1%2Bcpu-cp38-cp38-linux_x86_64.whl (190.4 MB)\r\n20.24 Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from -r /workspace/vllm/requirements-build.txt (line 7)) (0.34.2)\r\n21.21 Collecting networkx\r\n21.22   Downloading https://download.pytorch.org/whl/networkx-3.2.1-py3-none-any.whl (1.6 MB)\r\n21.78 ERROR: Package 'networkx' requires a different Python: 3.8.10 not in '>=3.9'\r\n\r\nDockerfile.openvino:19\r\n\r\n  17 |\r\n  18 |     # install build requirements\r\n  19 | >>> RUN PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cpu\" python3 -m pip install -r /workspace/vllm/requirements-build.txt\r\n  20 |     # build vLLM with OpenVINO backend\r\n  21 |     RUN PIP_PRE=1 PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cpu https://storage.openvinotoolkit.org/simple/wheels/nightly/\" VLLM_TARGET_DEVICE=\"openvino\" python3 -m pip install /workspace/vllm/\r\n\r\nERROR: failed to solve: process \"/bin/sh -c PIP_EXTRA_INDEX_URL=\\\"https://download.pytorch.org/whl/cpu\\\" python3 -m pip install -r /workspace/vllm/requirements-build.txt\" did not complete successfully: exit code: 1\r\n\r\n\r\n--------------------\r\n\r\nThe build was working properly until last week, but issues began arising starting from this commit: 1689219ebf01c750de492271832e27c39df38648.\r\n \r\n[[CI/Build] Build on Ubuntu 20.04 instead of 22.04 (](https://github.com/vllm-project/vllm/commit/1689219ebf01c750de492271832e27c39df38648)[#6517](https://github.com/vllm-project/vllm/pull/6517)[)](https://github.com/vllm-project/vllm/commit/1689219ebf01c750de492271832e27c39df38648)",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2024-07-25T04:05:59+00:00",
    "closed_at": "2024-07-30T18:33:02+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6769/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6769"
  },
  {
    "number": 7025,
    "title": "[Installation]: Could not install packages due to an OSError: [Errno 28] No space left on device but disk still have space",
    "body": "### Your current environment\n\n![image](https://github.com/user-attachments/assets/b25198d8-8530-49a1-b116-9882b5fb5977)\r\ni install vllm in /mnt , i found is still have space but it has a wrong like:\r\n\r\nInstalling build dependencies ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 pip subprocess to install build dependencies did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [47 lines of output]\r\n      Looking in indexes: http://mirrors.cloud.aliyuncs.com/pypi/simple/\r\n      Collecting cmake>=3.21\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/78/5e/c274ffd124b8d4d95734af94c1080f0421c89dabdea2475651a7bd1e02ca/cmake-3.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.9 MB)\r\n      Collecting ninja\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/6d/92/8d7aebd4430ab5ff65df2bfee6d5745f95c004284db2d8ca76dcbfd9de47/ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\r\n      Collecting packaging\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/08/aa/cc0199a5f0ad350994d660967a8efb233fe0416e4639146c089643407ce6/packaging-24.1-py3-none-any.whl (53 kB)\r\n      Collecting setuptools>=49.4.0\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/e1/58/e0ef3b9974a04ce9cde2a7a33881ddcb2d68450803745804545cdd8d258f/setuptools-72.1.0-py3-none-any.whl (2.3 MB)\r\n      Collecting torch==2.3.0\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/51/03/1abad10990c76bee3703857b1617563b241f87d297ee466dbad922b0c308/torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl (779.1 MB)\r\n      Collecting wheel\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/7d/cd/d7460c9a869b16c3dd4e1e403cce337df165368c71d6af229a74699622ce/wheel-0.43.0-py3-none-any.whl (65 kB)\r\n      Collecting filelock (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/ae/f0/48285f0262fe47103a4a45972ed2f9b93e4c80b8fd609fa98da78b2a5706/filelock-3.15.4-py3-none-any.whl (16 kB)\r\n      Collecting typing-extensions>=4.8.0 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl (37 kB)\r\n      Collecting sympy (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/b2/fe/81695a1aa331a842b582453b605175f419fe8540355886031328089d840a/sympy-1.13.1-py3-none-any.whl (6.2 MB)\r\n      Collecting networkx (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/38/e9/5f72929373e1a0e8d142a130f3f97e6ff920070f87f91c4e13e40e0fba5a/networkx-3.3-py3-none-any.whl (1.7 MB)\r\n      Collecting jinja2 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/31/80/3a54838c3fb461f6fec263ebf3a3a41771bd05190238de3486aae8540c36/jinja2-3.1.4-py3-none-any.whl (133 kB)\r\n      Collecting fsspec (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/5e/44/73bea497ac69bafde2ee4269292fa3b41f1198f4bb7bbaaabde30ad29d4a/fsspec-2024.6.1-py3-none-any.whl (177 kB)\r\n      Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/b6/9f/c64c03f49d6fbc56196664d05dba14e3a561038a81a638eeb47f4d4cfd48/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\r\n      Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/eb/d5/c68b1d2cdfcc59e72e8a5949a37ddb22ae6cade80cd4a57a84d4c8b55472/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\r\n      Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/7e/00/6b218edd739ecfc60524e585ba8e6b00554dd908de2c9c66c1af3e44e18d/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\r\n      Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\r\n      Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/37/6d/121efd7382d5b0284239f4ab1fc1590d86d34ed4a4a2fdb13b30ca8e5740/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\r\n      Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/86/94/eb540db023ce1d162e7bea9f8f5aa781d57c65aed513c33ee9a5123ead4d/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\r\n      Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/44/31/4890b1c9abc496303412947fc7dcea3d14861720642b49e8ceed89636705/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\r\n      Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/bc/1d/8de1e5c67099015c834315e333911273a8c6aaba78923dd1d1e25fc5f217/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\r\n      Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/65/5b/cfaeebf25cd9fdec14338ccb16f6b2c4c7fa9163aefcf057d86b9cc248bb/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\r\n      Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\r\n        Using cached http://mirrors.cloud.aliyuncs.com/pypi/packages/4b/2a/0a131f572aa09f741c30ccd45a8e56316e8be8dfc7bc19bf0ab7cfef7b19/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\r\n      ERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\r\n      \r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: subprocess-exited-with-error\r\n\r\n\u00d7 pip subprocess to install build dependencies did not run successfully.\r\n\u2502 exit code: 1\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This error originates from a subprocess, and is likely not a problem with pip.\n\n### How you are installing vllm\n\npip install vllm\r\n",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-01T09:24:12+00:00",
    "closed_at": "2024-12-01T02:14:15+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7025/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7025"
  },
  {
    "number": 11037,
    "title": "[Installation]: no version of pip install vllm works - Failed to initialize NumPy: No Module named 'numpy'",
    "body": "### Your current environment\n\n```text\r\nTraceback (most recent call last):\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/collect_env.py\", line 15, in <module>\r\n    from vllm.envs import environment_variables\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/__init__.py\", line 3, in <module>\r\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/engine/arg_utils.py\", line 11, in <module>\r\n    from vllm.config import (CacheConfig, CompilationConfig, ConfigFormat,\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/config.py\", line 21, in <module>\r\n    from vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/model_executor/__init__.py\", line 1, in <module>\r\n    from vllm.model_executor.parameter import (BasevLLMParameter,\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/model_executor/parameter.py\", line 7, in <module>\r\n    from vllm.distributed import get_tensor_model_parallel_rank\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/distributed/__init__.py\", line 1, in <module>\r\n    from .communication_op import *\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/distributed/communication_op.py\", line 6, in <module>\r\n    from .parallel_state import get_tp_group\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/distributed/parallel_state.py\", line 38, in <module>\r\n    import vllm.distributed.kv_transfer.kv_transfer_agent as kv_transfer\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/distributed/kv_transfer/kv_transfer_agent.py\", line 15, in <module>\r\n    from vllm.distributed.kv_transfer.kv_connector.factory import (\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/distributed/kv_transfer/kv_connector/factory.py\", line 3, in <module>\r\n    from .base import KVConnectorBase\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/distributed/kv_transfer/kv_connector/base.py\", line 14, in <module>\r\n    from vllm.sequence import IntermediateTensors\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/sequence.py\", line 16, in <module>\r\n    from vllm.inputs import SingletonInputs, SingletonInputsAdapter\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/inputs/__init__.py\", line 7, in <module>\r\n    from .registry import (DummyData, InputContext, InputProcessingContext,\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/inputs/registry.py\", line 13, in <module>\r\n    from vllm.transformers_utils.tokenizer import AnyTokenizer\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/transformers_utils/tokenizer.py\", line 16, in <module>\r\n    from vllm.utils import make_async\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/utils.py\", line 44, in <module>\r\n    from vllm.platforms import current_platform\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/platforms/__init__.py\", line 100, in <module>\r\n    from .cuda import CudaPlatform\r\n  File \"/mnt/MSAI/home/cephdon/sources/vllm/vllm/platforms/cuda.py\", line 14, in <module>\r\n    import vllm._C  # noqa\r\nModuleNotFoundError: No module named 'vllm._C'\r\n\r\n```\r\n\n\n### How you are installing vllm\n\n```\r\npip install -vvv vllm\r\npip install vllm==0.6.4\r\npip install -e .\r\n```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "open",
    "created_at": "2024-12-09T22:11:26+00:00",
    "closed_at": null,
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11037/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/11037"
  },
  {
    "number": 15550,
    "title": "[Installation]: Transformer installation requires uv venv --system now",
    "body": "Hi all, a small one I can take care of is a breaking change introduced in \n\nhttps://github.com/vllm-project/vllm/commit/7ffcccfa5ca3ef6b56c292ad2489e077a5cdd6f5#diff-dd2c0eb6ea5cfc6c4bd4eac30934e2d5746747af48fef6da689e85b752f39557R62\n\nThe installation instructions in [here](https://docs.vllm.ai/en/latest/deployment/docker.html#deployment-docker-pre-built-image) should probably include `--system` as in:\n\n`RUN uv pip install --system git+https://github.com/huggingface/transformers.git`\n\nThanks for your hard work!\n\n\n### How you are installing vllm\n\n```sh\npip install -vvv vllm\n```\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "closed",
    "created_at": "2025-03-26T13:56:48+00:00",
    "closed_at": "2025-03-27T12:38:47+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15550/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15550"
  }
]
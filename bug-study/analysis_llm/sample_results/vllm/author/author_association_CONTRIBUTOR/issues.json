[
  {
    "number": 3180,
    "title": "install from source failed",
    "body": "I encountered a problem when trying to compile vllm using 'pip install -e .', but I succeeded using 'pip install vllm'\nHere is my env configuration steps\n\n```\nconda create --name vllm python=3.9\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\npip install -r requirements.txt\npip install -e .\n```\n\nEnv:\ncuda 12.1\npython 3.9.18\nnumpy 1.26.4\nninja 1.11.1.1\n\nError:\n\n```\n  Building editable for vllm (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n\n\n  \u00d7 Building editable for vllm (pyproject.toml) did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [221 lines of output]\n      /tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n        device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n      running editable_wheel\n      creating /tmp/pip-wheel-nzc13jkb/.tmp-o_cwr7m0/vllm.egg-info\n      writing /tmp/pip-wheel-nzc13jkb/.tmp-o_cwr7m0/vllm.egg-info/PKG-INFO\n      writing dependency_links to /tmp/pip-wheel-nzc13jkb/.tmp-o_cwr7m0/vllm.egg-info/dependency_links.txt\n      writing requirements to /tmp/pip-wheel-nzc13jkb/.tmp-o_cwr7m0/vllm.egg-info/requires.txt\n      writing top-level names to /tmp/pip-wheel-nzc13jkb/.tmp-o_cwr7m0/vllm.egg-info/top_level.txt\n      writing manifest file '/tmp/pip-wheel-nzc13jkb/.tmp-o_cwr7m0/vllm.egg-info/SOURCES.txt'\n      reading manifest file '/tmp/pip-wheel-nzc13jkb/.tmp-o_cwr7m0/vllm.egg-info/SOURCES.txt'\n      reading manifest template 'MANIFEST.in'\n      adding license file 'LICENSE'\n      writing manifest file '/tmp/pip-wheel-nzc13jkb/.tmp-o_cwr7m0/vllm.egg-info/SOURCES.txt'\n      creating '/tmp/pip-wheel-nzc13jkb/.tmp-o_cwr7m0/vllm-0.3.3.dist-info'\n      creating /tmp/pip-wheel-nzc13jkb/.tmp-o_cwr7m0/vllm-0.3.3.dist-info/WHEEL\n      running build_py\n      running build_ext\n      /tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no g++ version bounds defined for CUDA version 12.1\n        warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n      building 'vllm._moe_C' extension\n      creating /tmp/tmpucwtn8_8.build-temp/csrc\n      creating /tmp/tmpucwtn8_8.build-temp/csrc/moe\n      Emitting ninja build file /tmp/tmpucwtn8_8.build-temp/build.ninja...\n      Compiling objects...\n      Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n      [1/2] c++ -MMD -MF /tmp/tmpucwtn8_8.build-temp/csrc/moe/moe_ops.o.d -pthread -B /home/jasonj/anaconda3/envs/vllm/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/jasonj/anaconda3/envs/vllm/include -I/home/jasonj/anaconda3/envs/vllm/include -fPIC -O2 -isystem /home/jasonj/anaconda3/envs/vllm/include -fPIC -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/moe/moe_ops.cpp -o /tmp/tmpucwtn8_8.build-temp/csrc/moe/moe_ops.o -g -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_moe_C -D_GLIBCXX_USE_CXX11_ABI=0\n      [2/2] /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/moe/topk_softmax_kernels.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/moe/topk_softmax_kernels.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_moe_C -D_GLIBCXX_USE_CXX11_ABI=0\n      creating /tmp/tmp5oa6291j.build-lib/vllm\n      g++ -pthread -B /home/jasonj/anaconda3/envs/vllm/compiler_compat -shared -Wl,-rpath,/home/jasonj/anaconda3/envs/vllm/lib -Wl,-rpath-link,/home/jasonj/anaconda3/envs/vllm/lib -L/home/jasonj/anaconda3/envs/vllm/lib -L/home/jasonj/anaconda3/envs/vllm/lib -Wl,-rpath,/home/jasonj/anaconda3/envs/vllm/lib -Wl,-rpath-link,/home/jasonj/anaconda3/envs/vllm/lib -L/home/jasonj/anaconda3/envs/vllm/lib /tmp/tmpucwtn8_8.build-temp/csrc/moe/moe_ops.o /tmp/tmpucwtn8_8.build-temp/csrc/moe/topk_softmax_kernels.o -L/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o /tmp/tmp5oa6291j.build-lib/vllm/_moe_C.cpython-39-x86_64-linux-gnu.so\n      building 'vllm._C' extension\n      creating /tmp/tmpucwtn8_8.build-temp/csrc/attention\n      creating /tmp/tmpucwtn8_8.build-temp/csrc/quantization\n      creating /tmp/tmpucwtn8_8.build-temp/csrc/quantization/awq\n      creating /tmp/tmpucwtn8_8.build-temp/csrc/quantization/gptq\n      creating /tmp/tmpucwtn8_8.build-temp/csrc/quantization/marlin\n      creating /tmp/tmpucwtn8_8.build-temp/csrc/quantization/squeezellm\n      Emitting ninja build file /tmp/tmpucwtn8_8.build-temp/build.ninja...\n      Compiling objects...\n      Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n      [1/13] /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/cuda_utils_kernels.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/cuda_utils_kernels.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      [2/13] /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/quantization/awq/gemm_kernels.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/quantization/awq/gemm_kernels.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      FAILED: /tmp/tmpucwtn8_8.build-temp/csrc/quantization/awq/gemm_kernels.o\n      /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/quantization/awq/gemm_kernels.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/quantization/awq/gemm_kernels.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      Killed\n      [3/13] /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/quantization/squeezellm/quant_cuda_kernel.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/quantization/squeezellm/quant_cuda_kernel.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      FAILED: /tmp/tmpucwtn8_8.build-temp/csrc/quantization/squeezellm/quant_cuda_kernel.o\n      /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/quantization/squeezellm/quant_cuda_kernel.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/quantization/squeezellm/quant_cuda_kernel.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      Killed\n      [4/13] /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/custom_all_reduce.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/custom_all_reduce.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      FAILED: /tmp/tmpucwtn8_8.build-temp/csrc/custom_all_reduce.o\n      /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/custom_all_reduce.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/custom_all_reduce.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      Killed\n      [5/13] /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/cache_kernels.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/cache_kernels.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      FAILED: /tmp/tmpucwtn8_8.build-temp/csrc/cache_kernels.o\n      /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/cache_kernels.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/cache_kernels.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      Killed\n      [6/13] /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/quantization/gptq/q_gemm.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/quantization/gptq/q_gemm.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      FAILED: /tmp/tmpucwtn8_8.build-temp/csrc/quantization/gptq/q_gemm.o\n      /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/quantization/gptq/q_gemm.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/quantization/gptq/q_gemm.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      Killed\n      [7/13] /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/attention/attention_kernels.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/attention/attention_kernels.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      FAILED: /tmp/tmpucwtn8_8.build-temp/csrc/attention/attention_kernels.o\n      /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/attention/attention_kernels.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/attention/attention_kernels.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      Killed\n      [8/13] /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/moe_align_block_size_kernels.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/moe_align_block_size_kernels.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      FAILED: /tmp/tmpucwtn8_8.build-temp/csrc/moe_align_block_size_kernels.o\n      /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/moe_align_block_size_kernels.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/moe_align_block_size_kernels.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      Killed\n      [9/13] /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/activation_kernels.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/activation_kernels.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      FAILED: /tmp/tmpucwtn8_8.build-temp/csrc/activation_kernels.o\n      /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/activation_kernels.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/activation_kernels.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      Killed\n      [10/13] c++ -MMD -MF /tmp/tmpucwtn8_8.build-temp/csrc/pybind.o.d -pthread -B /home/jasonj/anaconda3/envs/vllm/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/jasonj/anaconda3/envs/vllm/include -I/home/jasonj/anaconda3/envs/vllm/include -fPIC -O2 -isystem /home/jasonj/anaconda3/envs/vllm/include -fPIC -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/pybind.cpp -o /tmp/tmpucwtn8_8.build-temp/csrc/pybind.o -g -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      [11/13] /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/layernorm_kernels.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/layernorm_kernels.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      [12/13] /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/pos_encoding_kernels.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/pos_encoding_kernels.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      [13/13] /usr/local/cuda/bin/nvcc  -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/TH -I/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/jasonj/anaconda3/envs/vllm/include/python3.9 -c -c /home/jasonj/code/llm/vllm/csrc/quantization/marlin/marlin_cuda_kernel.cu -o /tmp/tmpucwtn8_8.build-temp/csrc/quantization/marlin/marlin_cuda_kernel.o --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_75,code=sm_75 --threads 8 -DENABLE_FP8_E5M2 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\n      ninja: build stopped: subcommand failed.\n      Traceback (most recent call last):\n        File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/utils/cpp_extension.py\", line 2100, in _run_ninja_build\n          subprocess.run(\n        File \"/home/jasonj/anaconda3/envs/vllm/lib/python3.9/subprocess.py\", line 528, in run\n          raise CalledProcessError(retcode, process.args,\n      subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\n  Traceback (most recent call last):\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/command/editable_wheel.py\", line 150, in run\n      self._create_wheel_file(bdist_wheel)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/command/editable_wheel.py\", line 339, in _create_wheel_file\n      files, mapping = self._run_build_commands(dist_name, unpacked, lib, tmp)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/command/editable_wheel.py\", line 262, in _run_build_commands\n      self._run_build_subcommands()\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/command/editable_wheel.py\", line 289, in _run_build_subcommands\n      self.run_command(name)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n      self.distribution.run_command(command)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/dist.py\", line 963, in run_command\n      super().run_command(command)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n      cmd_obj.run()\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/command/build_ext.py\", line 89, in run\n      _build_ext.run(self)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py\", line 345, in run\n      self.build_extensions()\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/utils/cpp_extension.py\", line 873, in build_extensions\n      build_ext.build_extensions(self)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py\", line 467, in build_extensions\n      self._build_extensions_serial()\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py\", line 493, in _build_extensions_serial\n      self.build_extension(ext)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/command/build_ext.py\", line 250, in build_extension\n      _build_ext.build_extension(self, ext)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py\", line 548, in build_extension\n      objects = self.compiler.compile(\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/utils/cpp_extension.py\", line 686, in unix_wrap_ninja_compile\n      _write_ninja_file_and_compile_objects(\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/utils/cpp_extension.py\", line 1774, in _write_ninja_file_and_compile_objects\n      _run_ninja_build(\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/utils/cpp_extension.py\", line 2116, in _run_ninja_build\n      raise RuntimeError(message) from e\n  RuntimeError: Error compiling objects for extension\n  /tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/_distutils/dist.py:988: _DebuggingTips: Problem in editable installation.\n  !!\n\n          ********************************************************************************\n          An error happened while installing `vllm` in editable mode.\n    \n          The following steps are recommended to help debug this problem:\n    \n          - Try to install the project normally, without using the editable mode.\n            Does the error still persist?\n            (If it does, try fixing the problem before attempting the editable mode).\n          - If you are using binary extensions, make sure you have all OS-level\n            dependencies installed (e.g. compilers, toolchains, binary libraries, ...).\n          - Try the latest version of setuptools (maybe the error was already fixed).\n          - If you (or your project dependencies) are using any setuptools extension\n            or customization, make sure they support the editable mode.\n    \n          After following the steps above, if the problem still persists and\n          you think this is related to how setuptools handles editable installations,\n          please submit a reproducible example\n          (see https://stackoverflow.com/help/minimal-reproducible-example) to:\n    \n              https://github.com/pypa/setuptools/issues\n    \n          See https://setuptools.pypa.io/en/latest/userguide/development_mode.html for details.\n          ********************************************************************************\n\n  !!\n    cmd_obj.run()\n  Traceback (most recent call last):\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/utils/cpp_extension.py\", line 2100, in _run_ninja_build\n      subprocess.run(\n    File \"/home/jasonj/anaconda3/envs/vllm/lib/python3.9/subprocess.py\", line 528, in run\n      raise CalledProcessError(retcode, process.args,\n  subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\n  The above exception was the direct cause of the following exception:\n\n  Traceback (most recent call last):\n    File \"/home/jasonj/anaconda3/envs/vllm/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n      main()\n    File \"/home/jasonj/anaconda3/envs/vllm/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n      json_out['return_val'] = hook(**hook_input['kwargs'])\n    File \"/home/jasonj/anaconda3/envs/vllm/lib/python3.9/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 273, in build_editable\n      return hook(wheel_directory, config_settings, metadata_directory)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/build_meta.py\", line 443, in build_editable\n      return self._build_with_temp_dir(\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/build_meta.py\", line 395, in _build_with_temp_dir\n      self.run_setup()\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/build_meta.py\", line 311, in run_setup\n      exec(code, locals())\n    File \"<string>\", line 444, in <module>\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/__init__.py\", line 103, in setup\n      return distutils.core.setup(**attrs)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n      return run_commands(dist)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n      dist.run_commands()\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n      self.run_command(cmd)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/dist.py\", line 963, in run_command\n      super().run_command(command)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n      cmd_obj.run()\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/command/editable_wheel.py\", line 150, in run\n      self._create_wheel_file(bdist_wheel)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/command/editable_wheel.py\", line 339, in _create_wheel_file\n      files, mapping = self._run_build_commands(dist_name, unpacked, lib, tmp)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/command/editable_wheel.py\", line 262, in _run_build_commands\n      self._run_build_subcommands()\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/command/editable_wheel.py\", line 289, in _run_build_subcommands\n      self.run_command(name)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n      self.distribution.run_command(command)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/dist.py\", line 963, in run_command\n      super().run_command(command)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n      cmd_obj.run()\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/command/build_ext.py\", line 89, in run\n      _build_ext.run(self)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py\", line 345, in run\n      self.build_extensions()\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/utils/cpp_extension.py\", line 873, in build_extensions\n      build_ext.build_extensions(self)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py\", line 467, in build_extensions\n      self._build_extensions_serial()\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py\", line 493, in _build_extensions_serial\n      self.build_extension(ext)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/command/build_ext.py\", line 250, in build_extension\n      _build_ext.build_extension(self, ext)\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py\", line 548, in build_extension\n      objects = self.compiler.compile(\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/utils/cpp_extension.py\", line 686, in unix_wrap_ninja_compile\n      _write_ninja_file_and_compile_objects(\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/utils/cpp_extension.py\", line 1774, in _write_ninja_file_and_compile_objects\n      _run_ninja_build(\n    File \"/tmp/pip-build-env-dy69uemu/overlay/lib/python3.9/site-packages/torch/utils/cpp_extension.py\", line 2116, in _run_ninja_build\n      raise RuntimeError(message) from e\n  RuntimeError: Error compiling objects for extension\n  [end of output]\nBuilding wheels for collected packages: vllm\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2024-03-04T16:21:49+00:00",
    "closed_at": "2024-03-05T07:11:15+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3180/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3180"
  },
  {
    "number": 21094,
    "title": "[Enhancement]:  add concurrency in benchmark result table",
    "body": "### Your current environment\n\nvLLM v0.9.2rc2\n\n### \ud83d\udc1b Describe the bug\n\nwhen using `vllm/benchmarks/benchmark_serving.py`\n\nthe  `--max-concurrency X` only shows in normal stdout logging and saving result json,\n\n but is missing in **formatted result table** (`============ Serving Benchmark Result ============\n`)\n\nthe concurrency is important factor impact the performance result , sometimes  even than  `Successful requests`.\n\nSo I think it's necessary to put this info into  formatted result table output.\n\n\n\n\ncurrent output table:\n\n```Namespace(backend='openai-chat', base_url='https://api.groq.com/openai', host='127.0.0.1', port=8000, endpoint='/v1/chat/completions', dataset_name='sharegpt', dataset_path='/mnt/models/ShareGPT_Vicuna_unfiltered/ShareGPT_V3_unfiltered_cleaned_split.json', max_concurrency=3, model='moonshotai/Kimi-K2-Instruct', tokenizer=None, use_beam_search=False, num_prompts=10, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=True, disable_tqdm=False, profile=False, save_result=True, save_detailed=True, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, top_p=None, top_k=None, min_p=None, temperature=0.7, tokenizer_mode='auto', served_model_name='moonshotai/kimi-k2-instruct', lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nWARNING 07-17 02:38:50 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf RPS.\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: 3\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08<00:00,  1.11it/s]\n============ Serving Benchmark Result ============\nSuccessful requests:                     10\nBenchmark duration (s):                  8.98\nTotal input tokens:                      1366\nTotal generated tokens:                  2634\nRequest throughput (req/s):              1.11\nOutput token throughput (tok/s):         293.33\nTotal Token throughput (tok/s):          445.45\n---------------Time to First Token----------------\nMean TTFT (ms):                          811.39\nMedian TTFT (ms):                        723.85\nP99 TTFT (ms):                           1099.04\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          8.20\nMedian TPOT (ms):                        5.21\nP99 TPOT (ms):                           24.68\n---------------Inter-token Latency----------------\nMean ITL (ms):                           5.93\nMedian ITL (ms):                         0.09\nP99 ITL (ms):                            60.85\n==================================================\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-07-17T05:32:47+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/21094/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/21094"
  },
  {
    "number": 11978,
    "title": "[Bug]: The usage of .transpose() and .view() consecutively is not recommended.",
    "body": "### Your current environment\n\n<details>\r\n<summary>Error information (Sorry, I cannot disclose more due to confidentiality reasons)</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.1.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: Could not collect\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (aarch64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.31.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-118-generic-aarch64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: \r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nVersions of relevant libraries:\r\n[pip3] mindietorch==1.0.0+torch2.1.0.abi0\r\n[pip3] numpy==1.26.4\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.1.0\r\n[pip3] torch-npu==2.1.0.post10.dev20241217\r\n[pip3] torchvision==0.16.0\r\n[pip3] transformers==4.46.1\r\n[pip3] tritonclient==2.49.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.3.dev0+g7193774b.d20250103\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n\r\nPYTORCH_INSTALL_PATH=/usr/local/lib/python3.10/dist-packages/torch\r\nPYTORCH_NPU_INSTALL_PATH=/usr/local/lib/python3.10/dist-packages/torch_npu\r\n```\r\n\r\n</details>\\\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\n### Description\r\n\r\nWe encountered the following error while running `internvl`:\r\n\r\n```\r\nview size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\r\n```\r\n\r\nAfter careful analysis, we found that this was a logic bug that had nothing to do with the device we were using.\r\n\r\nThe issue occurs in the following line of code in `vllm/vllm/model_executor/models/intern_vit.py`:\r\n\r\n```python\r\nclass InternSdpaAttention(nn.Module):\r\n  def forward(self, x):\r\n      B, N, C = x.shape\r\n      qkv = self.qkv(x)\r\n      q, k, v = qkv.chunk(3, dim=-1)\r\n  \r\n      q = q.view(B, N, self.num_heads, self.head_dim)\r\n      k = k.view(B, N, self.num_heads, self.head_dim)\r\n      v = v.view(B, N, self.num_heads, self.head_dim)\r\n  \r\n      if self.qk_normalization:\r\n          B_, N_, H_, D_ = q.shape\r\n          q = self.q_norm.forward_native(q.flatten(-2, -1)).view(B_, N_, H_, D_)\r\n          k = self.k_norm.forward_native(k.flatten(-2, -1)).view(B_, N_, H_, D_)\r\n      q = q.transpose(1, 2)\r\n      k = k.transpose(1, 2)\r\n      v = v.transpose(1, 2)\r\n  \r\n      x = F.scaled_dot_product_attention(q, k, v, scale=self.scale)\r\n      x = x.transpose(1, 2).view(B, N, -1)\r\n```\r\n\r\n### Analysis\r\n\r\nAt the beginning of this file, there is a check to determine whether `xformers` is available:\r\n\r\n```python\r\ntry:\r\n    from xformers import ops as xops\r\n    USE_XFORMERS_OPS = True\r\nexcept ImportError:\r\n    USE_XFORMERS_OPS = False\r\n```\r\n\r\nThe execution brunch is different depending on the value of `USE_XFORMERS_OPS`. The error in question occurs only when `USE_XFORMERS_OPS = False`, making it hard to reproduce under normal circumstances (e.g. use Nvidia GPU in most cases).\r\n\r\n**Why the usage of .transpose() and .view() consecutively is not recommended:** In the affected code, the output after the attention operation is a 4D tensor, which we will call `A`. If `A` is a strided tensor, according to the [PyTorch documentation for `transpose`](https://pytorch.org/docs/stable/generated/torch.Tensor.transpose.html), the `.transpose` operation only creates a [view](https://pytorch.org/docs/stable/tensor_view.html) of `A`, modifying its strides. Meanwhile, the [documentation for `.view`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view) specifies that the new view must be compatible with the original size and stride. This means that the new view dimensions must be a subspace of the original dimensions, or the input must satisfy the formula mentioned in the docs. Since the `.transpose` operation changes the strides, the view created by `.view` may not satisfy these conditions and lead to the error. The `.view` documentation also suggests using `.contiguous()` before `.view` to ensure that the tensor is contiguous in memory.\r\n\r\n### Solution\r\n\r\nThe issue arises from the fact that the tensor `A` is strided, and the strides of the tensor after `.transpose` likely don't meet the conditions for `.view` to work correctly. To resolve this issue, we recommend adding a `.contiguous()` call before the `.view` operation to ensure that the tensor is contiguous in memory.\r\n\r\n**P.S.** We also found that .transpose().view() is only valid if:\r\n\r\n1. `A` is a sparse tensor instead of a strided tensor.\r\n2. The stride after `transpose` happens to meet the compatibility conditions for `.view`.\r\n3. PyTorch may have undocumented methods for handling these situations, such as not requiring `.contiguous()` when the changes made by `.view` and `.transpose` are mirrored.\r\n\r\n### Related Issues\r\n\r\nThis issue is similar to the one mentioned in issue #8630, but the key to preventing the problem from reproducing is not found, namely the value of `USE_XFORMERS_OPS`. It was addressed in PR #8880. However, since PR #9560 was believed to have solved this problem, PR #8880 was closed. We found that this issue was not actually resolved in PR #9560, so we are submitting this new issue with our analysis and an explanation based on the official PyTorch documentation.\r\n\r\n### Reproducible Example\r\n\r\nHere is a simple code snippet to verify the issue:\r\n\r\n```python\r\nimport torch\r\n\r\nx = torch.randn(2, 3, 4, 5)\r\ny = x.transpose(1, 2)\r\n# y = y.contiguous()\r\n\r\ntry:\r\n    z = y.view(2, 4, -1)\r\n    print(z.size())\r\nexcept Exception as e:\r\n    print(\"Error:\", e)\r\n```\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-01-13T01:41:52+00:00",
    "closed_at": "2025-01-13T06:24:12+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11978/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11978"
  },
  {
    "number": 6300,
    "title": "[Feature]: Multi-Proposers support for speculative decoding.",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nSpeculative decoding has demonstrated significant potential in efficiently generating proposals and utilizing idle computing power to expedite the auto-regression decoding process, particularly under lightweight workloads. Thanks to the remarkable work by @cadedaniel, we have verified the latency benefits brought by speculative decoding on the latest version of vllm.\r\n\r\nWe have observed the following points that we believe could further enhance the utility of speculative decoding:\r\n\r\n* **Ngram Proposer:** While the 'Ngram' proposer can offer a 2x to 3x performance improvement in Retrieval-Augmented Generation (RAG) scenarios, its performance diminishes when the RAG module retrieves no relevant data for a query.\r\n\r\n* **Draft-Model-Based Proposers:** In contrast, draft-model-based proposers have exhibited higher acceptance rates when the RAG module retrieves no relevant data or faces a more creative task. Yet the performance of this type of implementation is not fully optimized (#4630 #5561). So the current performance gains are limited. We sincerely thank the open-source community for their efforts and hope all this progress will be smooth.\r\n\r\n* **Creative Tasks with High Temperature:** We have noticed that both proposer methods exhibit lower performance compared to non-spec implementations when dealing with creative tasks characterized by a high temperature or a great top_k. Maybe the spec decode should be disabled under this circumstance.\r\n\r\nApart from these observations, we were particularly interested in your latest work on speculative length scheduling for different workload scenarios (#5886) [Optimizing Speculative Decoding for Serving Large Language Models Using Goodput](https://arxiv.org/pdf/2406.14066v1).\r\n\r\nThis led us to wonder if vllm could be enhanced to support multiple proposers and provide the flexibility to schedule them appropriately. Alternatively, enabling users to specify the proposer for different requests via SamplingParams could also be a viable solution.\r\n\r\nWe believe this enhancement could unlock greater potential and adaptivity for vllm's speculative decoding capabilities. We are working on an inner forked version to verify whether we can achieve a higher goodput.\r\n\r\nThanks, feel free to leave a message to let us know what you think of it.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-10T09:36:55+00:00",
    "closed_at": "2024-11-25T02:04:43+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6300/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/6300"
  },
  {
    "number": 13387,
    "title": "[Bug]: Chunk Prefill feature fails for ppc64le (IBM POWER)",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nCollecting environment information...\nPyTorch version: 2.7.0a0+gitd0f5df8\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Red Hat Enterprise Linux 9.5 (Plow) (ppc64le)\nGCC version: (GCC) 13.3.1 20240611 (Red Hat 13.3.1-2)\nClang version: 18.1.8 (Red Hat, Inc. 18.1.8-3.el9)\nCMake version: version 3.31.2\nLibc version: glibc-2.34\n\nPython version: 3.11.11 | packaged by conda-forge | (main, Dec  5 2024, 14:07:52) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-5.14.0-503.23.1.el9_5.ppc64le-ppc64le-with-glibc2.34\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: False\n\nCPU:\nArchitecture:                         ppc64le\nByte Order:                           Little Endian\nCPU(s):                               320\nOn-line CPU(s) list:                  0-319\nModel name:                           POWER10 (architected), altivec supported\nModel:                                2.0 (pvr 0080 0200)\nThread(s) per core:                   8\nCore(s) per socket:                   10\nSocket(s):                            4\nHypervisor vendor:                    pHyp\nVirtualization type:                  para\nL1d cache:                            2.5 MiB (80 instances)\nL1i cache:                            3.8 MiB (80 instances)\nL2 cache:                             80 MiB (80 instances)\nL3 cache:                             320 MiB (80 instances)\nNUMA node(s):                         4\nNUMA node0 CPU(s):                    0-79\nNUMA node1 CPU(s):                    80-159\nNUMA node2 CPU(s):                    160-239\nNUMA node3 CPU(s):                    240-319\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Not affected\nVulnerability Spectre v1:             Mitigation; __user pointer sanitization, ori31 speculation barrier enabled\nVulnerability Spectre v2:             Mitigation; Software count cache flush (hardware accelerated), Software link stack flush\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] optree==0.13.1\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.7.0a0+gitd0f5df8\n[pip3] transformers==4.47.1\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] optree                    0.13.1                   pypi_0    pypi\n[conda] pyzmq                     26.2.0          py311he15fa53_3    conda-forge\n[conda] torch                     2.7.0a0+gitd0f5df8          pypi_0    pypi\n[conda] transformers              4.47.1                   pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.6.6.post2.dev103+ge512f76a.d20250129\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nLD_LIBRARY_PATH=/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/cv2/../../lib64:/home/akashk/miniconda3/envs/vllm_oob/lib:\n\n\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen we run chunked prefill enabled on ppc64le, it fails with an ipex dependency. As ipex does not work for ppc64le, we are seeing this error. \n\n### Test script \n\n```python\nfrom vllm import LLM, SamplingParams\n \nllm = LLM(model=\"facebook/opt-1.3b\", enable_chunked_prefill=True)  # Enable chunked prefill\n \nlong_prompt = \"\"\"Once upon a time in a faraway land, there was a wise old owl who lived in a hollow tree. \nThis owl had seen many seasons pass and had gathered knowledge from all corners of the world...\"\"\"\n \nsampling_params = SamplingParams(temperature=0.7, max_tokens=100)\n \noutputs = llm.generate([long_prompt], sampling_params)\n \nfor output in outputs:\n    print(\"Generated Text:\", output.outputs[0].text)\n```\n\n### Error Log\n```\nINFO 02-16 08:18:58 __init__.py:183] Automatically detected platform cpu.\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 653/653 [00:00<00:00, 5.22MB/s]\nINFO 02-16 08:18:59 config.py:2280] For POWERPC, we cast models to bfloat16 instead of using float16 by default. Float16 is not currently supported for POWERPC.\nWARNING 02-16 08:18:59 config.py:2324] Casting torch.float16 to torch.bfloat16.\nINFO 02-16 08:19:04 config.py:526] This model supports multiple tasks: {'classify', 'reward', 'embed', 'generate', 'score'}. Defaulting to 'generate'.\nINFO 02-16 08:19:04 config.py:1494] Chunked prefill is enabled with max_num_batched_tokens=2048.\nWARNING 02-16 08:19:04 config.py:662] Async output processing is not supported on the current platform type cpu.\nWARNING 02-16 08:19:04 cpu.py:60] CUDA graph is not supported on CPU, fallback to the eager mode.\nWARNING 02-16 08:19:04 cpu.py:75] Environment variable VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by default.\nINFO 02-16 08:19:04 llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev4356+gb02fd28.d20250129) with config: model='facebook/opt-1.3b', speculative_config=None, tokenizer='facebook/opt-1.3b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=facebook/opt-1.3b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 685/685 [00:00<00:00, 6.84MB/s]\nvocab.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 899k/899k [00:00<00:00, 14.9MB/s]\nmerges.txt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 456k/456k [00:00<00:00, 40.1MB/s]\nspecial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 441/441 [00:00<00:00, 5.82MB/s]\ngeneration_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 137/137 [00:00<00:00, 1.51MB/s]\nINFO 02-16 08:19:06 cpu.py:36] Cannot use None backend on CPU.\nINFO 02-16 08:19:06 cpu.py:37] Using Torch SDPA backend.\nINFO 02-16 08:19:06 importing.py:14] Triton not installed or not compatible; certain GPU-related functions will not be available.\nINFO 02-16 08:19:06 weight_utils.py:251] Using model weights format ['*.bin']\npytorch_model.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.63G/2.63G [00:23<00:00, 114MB/s]\nLoading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.05it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.01it/s]\n \nINFO 02-16 08:19:30 executor_base.py:108] # CPU blocks: 1365, # CPU blocks: 0\nINFO 02-16 08:19:30 executor_base.py:113] Maximum concurrency for 2048 tokens per request: 10.66x\nINFO 02-16 08:19:31 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 0.17 seconds\nProcessed prompts:   0%|                                                                                                                                       | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/akashk/vllm_oob/vllm/examples/test_chunked_prefill.py\", line 14, in <module>\n[rank0]:     outputs = llm.generate([long_prompt], sampling_params)\n[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/vllm-0.1.dev4356+gb02fd28.d20250129.cpu-py3.11-linux-ppc64le.egg/vllm/utils.py\", line 1074, in inner\n[rank0]:     return fn(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/vllm-0.1.dev4356+gb02fd28.d20250129.cpu-py3.11-linux-ppc64le.egg/vllm/entrypoints/llm.py\", line 467, in generate\n[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)\n[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/vllm-0.1.dev4356+gb02fd28.d20250129.cpu-py3.11-linux-ppc64le.egg/vllm/entrypoints/llm.py\", line 1388, in _run_engine\n[rank0]:     step_outputs = self.llm_engine.step()\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/vllm-0.1.dev4356+gb02fd28.d20250129.cpu-py3.11-linux-ppc64le.egg/vllm/engine/llm_engine.py\", line 1384, in step\n[rank0]:     outputs = self.model_executor.execute_model(\n[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/vllm-0.1.dev4356+gb02fd28.d20250129.cpu-py3.11-linux-ppc64le.egg/vllm/executor/executor_base.py\", line 136, in execute_model\n[rank0]:     output = self.collective_rpc(\"execute_model\",\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/vllm-0.1.dev4356+gb02fd28.d20250129.cpu-py3.11-linux-ppc64le.egg/vllm/executor/uniproc_executor.py\", line 49, in collective_rpc\n[rank0]:     answer = run_method(self.driver_worker, method, args, kwargs)\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/vllm-0.1.dev4356+gb02fd28.d20250129.cpu-py3.11-linux-ppc64le.egg/vllm/utils.py\", line 2208, in run_method\n[rank0]:     return func(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/vllm-0.1.dev4356+gb02fd28.d20250129.cpu-py3.11-linux-ppc64le.egg/vllm/worker/worker_base.py\", line 411, in execute_model\n[rank0]:     output = self.model_runner.execute_model(\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[rank0]:     return func(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/vllm-0.1.dev4356+gb02fd28.d20250129.cpu-py3.11-linux-ppc64le.egg/vllm/worker/cpu_model_runner.py\", line 655, in execute_model\n[rank0]:     hidden_states = model_executable(\n[rank0]:                     ^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/vllm-0.1.dev4356+gb02fd28.d20250129.cpu-py3.11-linux-ppc64le.egg/vllm/model_executor/models/opt.py\", line 368, in forward\n[rank0]:     hidden_states = self.model(input_ids, positions, kv_caches,\n[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/vllm-0.1.dev4356+gb02fd28.d20250129.cpu-py3.11-linux-ppc64le.egg/vllm/compilation/decorators.py\", line 170, in __call__\n[rank0]:     return self.forward(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/vllm-0.1.dev4356+gb02fd28.d20250129.cpu-py3.11-linux-ppc64le.egg/vllm/model_executor/models/opt.py\", line 323, in forward\n[rank0]:     return self.decoder(input_ids,\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/vllm-0.1.dev4356+gb02fd28.d20250129.cpu-py3.11-linux-ppc64le.egg/vllm/model_executor/models/opt.py\", line 280, in forward\n[rank0]:     hidden_states = layer(hidden_states,\n[rank0]:                     ^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/vllm-0.1.dev4356+gb02fd28.d20250129.cpu-py3.11-linux-ppc64le.egg/vllm/model_executor/models/opt.py\", line 173, in forward\n[rank0]:     hidden_states = self.self_attn(hidden_states=hidden_states,\n[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/vllm-0.1.dev4356+gb02fd28.d20250129.cpu-py3.11-linux-ppc64le.egg/vllm/model_executor/models/opt.py\", line 113, in forward\n[rank0]:     attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/vllm-0.1.dev4356+gb02fd28.d20250129.cpu-py3.11-linux-ppc64le.egg/vllm/attention/layer.py\", line 177, in forward\n[rank0]:     return torch.ops.vllm.unified_attention(\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/torch/_ops.py\", line 1158, in __call__\n[rank0]:     return self._op(*args, **(kwargs or {}))\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/vllm-0.1.dev4356+gb02fd28.d20250129.cpu-py3.11-linux-ppc64le.egg/vllm/attention/layer.py\", line 279, in unified_attention\n[rank0]:     return self.impl.forward(self, query, key, value, kv_cache, attn_metadata)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/miniconda3/envs/vllm_oob/lib/python3.11/site-packages/vllm-0.1.dev4356+gb02fd28.d20250129.cpu-py3.11-linux-ppc64le.egg/vllm/attention/backends/torch_sdpa.py\", line 536, in forward\n[rank0]:     import intel_extension_for_pytorch.llm.modules as ipex_modules\n[rank0]: ModuleNotFoundError: No module named 'intel_extension_for_pytorch'\nProcessed prompts:   0%|                                                                                                                                       | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-17T08:54:37+00:00",
    "closed_at": "2025-06-25T02:16:37+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/13387/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/13387"
  },
  {
    "number": 277,
    "title": "OpenBuddy support",
    "body": "I'd very much like to see support for [OpenBuddy](https://github.com/OpenBuddy/OpenBuddy) in vllm.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-06-27T10:42:21+00:00",
    "closed_at": "2023-06-27T15:27:20+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/277/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/277"
  },
  {
    "number": 2853,
    "title": "When is the CPU KV cache used and swapping?",
    "body": "Hi authors,\r\n\r\nIn your implementation, the GPU memory is leveraged to store the KV cache. However, it appears that when the GPU memory reaches its capacity, there isn't a mechanism in place to offload or swap this data to the CPU memory. \r\n\r\n1. Could you please clarify under what conditions the CPU KV cache comes into play?\r\n2. Could you please tell me how to invoke the CPU KV cache (or API) if I want to do swapping?\r\n\r\n\r\n![Screenshot 2024-02-01 at 2 41 42 PM](https://github.com/vllm-project/vllm/assets/47625290/38ee7a92-486d-4e2a-a7f1-3c8af2424cd4)\r\n![Screenshot 2024-02-13 at 1 53 57 PM](https://github.com/vllm-project/vllm/assets/47625290/618227a2-c599-4028-b47d-ae03e0762016)\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-13T18:58:19+00:00",
    "closed_at": "2024-08-27T09:52:39+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2853/reactions",
      "total_count": 3,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2853"
  },
  {
    "number": 11431,
    "title": "[Feature]: AssertionError: MolmoForCausalLM does not support LoRA yet.",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.5.1+cu124\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.5 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.22.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-122-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.5.82\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA RTX A6000\r\nGPU 1: NVIDIA RTX A6000\r\n\r\nNvidia driver version: 555.42.02\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               32\r\nOn-line CPU(s) list:                  0-31\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD Ryzen Threadripper PRO 5955WX 16-Cores\r\nCPU family:                           25\r\nModel:                                8\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   16\r\nSocket(s):                            1\r\nStepping:                             2\r\nFrequency boost:                      enabled\r\nCPU max MHz:                          7031.2500\r\nCPU min MHz:                          1800.0000\r\nBogoMIPS:                             7985.02\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\nVirtualization:                       AMD-V\r\nL1d cache:                            512 KiB (16 instances)\r\nL1i cache:                            512 KiB (16 instances)\r\nL2 cache:                             8 MiB (16 instances)\r\nL3 cache:                             64 MiB (2 instances)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-31\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Mitigation; safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.4.5.8\r\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\r\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.2.1.3\r\n[pip3] nvidia-curand-cu12==10.3.5.147\r\n[pip3] nvidia-cusolver-cu12==11.6.1.9\r\n[pip3] nvidia-cusparse-cu12==12.3.1.170\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.21.5\r\n[pip3] nvidia-nvjitlink-cu12==12.4.127\r\n[pip3] nvidia-nvtx-cu12==12.4.127\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.5.1\r\n[pip3] torchvision==0.20.1\r\n[pip3] transformers==4.47.1\r\n[pip3] triton==3.1.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.5\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      SYS     0-31    0               N/A\r\nGPU1    SYS      X      0-31    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nCUDAPATH=/usr/local/cuda-12.5\r\nCUDACXX=/usr/local/cuda-12.5/bin/nvcc\r\nLD_LIBRARY_PATH=/molmo/.venv/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda-12.5/lib64:\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nApparently MolmoForCasualLM does not yet support Lora adapters, yielding an AssertionError on serving:\r\n\r\n`AssertionError: MolmoForCausalLM does not support LoRA yet.`\r\n\r\nI trained a Lora adapter with HF Trainer and would like to use it together with vLLM for fast inference. This seems not implemented yet. I tested this by trying to serve Molmo directly via:\r\n`vllm serve allenai/Molmo-7B-D-0924 --enable-lora --trust-remote-code --max-num-seqs 6 --tensor-parallel-size 1 --lora-modules test=$LORA_DIR/checkpoint-25`\r\n\r\nAre there any plans to get this working or is there a guide somewhere how i can enable lora for Molmo myself?\r\nIf all works I'd be open to submit a PR but i'd need some guidance.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-12-23T10:38:18+00:00",
    "closed_at": "2024-12-31T01:33:08+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11431/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11431"
  },
  {
    "number": 2814,
    "title": "Support for Qwen 1.5 on vLLM",
    "body": "Currently, this gives this error:\r\n```\r\n2024-02-08T06:28:20.354023087-08:00 OSError: Can't load the configuration of 'Qwen/Qwen1.5-72B-Chat-AWQ'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Qwen/Qwen1.5-72B-Chat-AWQ' is the correct path to a directory containing a config.json file\r\n```\r\n\r\nReference implementation: [here](https://runpod.io/gsc?template=ju7oo9mf5w&ref=jmfkcdio)",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-08T14:31:13+00:00",
    "closed_at": "2024-02-23T09:26:41+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2814/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2814"
  },
  {
    "number": 17960,
    "title": "[Bug]: With Pytorch 2.7 in case of POWER getting, NotImplementedError: Could not run '_C_cache_ops::reshape_and_cache' with arguments from the 'CPU' backend",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nINFO 05-11 07:55:22 [__init__.py:248] Automatically detected platform cpu.\nCollecting environment information...\nPyTorch version: 2.7.0\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Red Hat Enterprise Linux 9.5 (Plow) (ppc64le)\nGCC version: (GCC) 11.5.0 20240719 (Red Hat 11.5.0-5)\nClang version: 19.1.7 (CentOS 19.1.7-1.el9)\nCMake version: version 3.31.6\nLibc version: glibc-2.34\n\nPython version: 3.12.9 (main, Feb  4 2025, 00:00:00) [GCC 11.5.0 20240719 (Red Hat 11.5.0-4)] (64-bit runtime)\nPython platform: Linux-5.14.0-547.el9.ppc64le-ppc64le-with-glibc2.34\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: False\n\nCPU:\nArchitecture:                         ppc64le\nByte Order:                           Little Endian\nCPU(s):                               384\nOn-line CPU(s) list:                  0-383\nModel name:                           POWER10 (architected), altivec supported\nModel:                                2.0 (pvr 0080 0200)\nThread(s) per core:                   8\nCore(s) per socket:                   12\nSocket(s):                            4\nHypervisor vendor:                    pHyp\nVirtualization type:                  para\nL1d cache:                            3 MiB (96 instances)\nL1i cache:                            4.5 MiB (96 instances)\nL2 cache:                             96 MiB (96 instances)\nL3 cache:                             384 MiB (96 instances)\nNUMA node(s):                         4\nNUMA node0 CPU(s):                    0-95\nNUMA node1 CPU(s):                    96-191\nNUMA node2 CPU(s):                    192-287\nNUMA node3 CPU(s):                    288-383\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Not affected\nVulnerability Spectre v1:             Mitigation; __user pointer sanitization, ori31 speculation barrier enabled\nVulnerability Spectre v2:             Mitigation; Software count cache flush (hardware accelerated), Software link stack flush\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==2.2.1+opence\n[pip3] pyzmq==25.1.2\n[pip3] torch==2.7.0\n[pip3] torchaudio==2.7.0\n[pip3] torchvision==0.22.0a0+5f03dc5\n[pip3] transformers==4.51.3\n[pip3] triton==3.1.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.5.dev589+ga810b5b08 (git sha: a810b5b08)\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nLD_LIBRARY_PATH=:/home/akashk/protobuf/lib64:/home/akashk/vllm_env/lib64/python3.12/site-packages/libprotobuf/lib64:/home/akashk/vllm_env/lib64/python3.12/site-packages/openblas/lib:/home/akashk/vllm_env/lib64/python3.12/site-packages:/home/akashk/vllm_env/lib64/python3.12/site-packages/ffmpeg/lib:/home/akashk/vllm_env/lib64/python3.12/site-packages/libvpx/lib:/home/akashk/vllm_env/lib64/python3.12/site-packages/lame/lib:/home/akashk/protobuf/lib64:/home/akashk/vllm_env/lib64/python3.12/site-packages/libprotobuf/lib64:/home/akashk/vllm_env/lib64/python3.12/site-packages/openblas/lib:/home/akashk/vllm_env/lib64/python3.12/site-packages:/home/akashk/vllm_env/lib64/python3.12/site-packages/ffmpeg/lib:/home/akashk/vllm_env/lib64/python3.12/site-packages/libvpx/lib:/home/akashk/vllm_env/lib64/python3.12/site-packages/lame/lib\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\n\n\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nTrying to run \n```\nexamples/offline_inference/basic/basic.py\n```\nand I am seeing the following error. \nIt was running fine till v0.8.5\n```\nINFO 05-11 07:51:38 [__init__.py:239] Automatically detected platform cpu.\nINFO 05-11 07:51:41 [config.py:2902] For POWERPC, we cast models to bfloat16 instead of using float16 by default. Float16 is not currently supported for POWERPC.\nWARNING 05-11 07:51:41 [config.py:2951] Casting torch.float16 to torch.bfloat16.\nINFO 05-11 07:51:49 [config.py:713] This model supports multiple tasks: {'reward', 'score', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.\nWARNING 05-11 07:51:49 [arg_utils.py:1684] device type=cpu is not supported by the V1 Engine. Falling back to V0. \nINFO 05-11 07:51:49 [config.py:1794] Disabled the custom all-reduce kernel because it is not supported on current platform.\nWARNING 05-11 07:51:49 [cpu.py:106] Environment variable VLLM_CPU_KVCACHE_SPACE (GiB) for CPU backend is not set, using 4 by default.\nWARNING 05-11 07:51:49 [cpu.py:119] uni is not supported on CPU, fallback to mp distributed executor backend.\nINFO 05-11 07:51:49 [llm_engine.py:243] Initializing a V0 LLM engine (v0.1.dev5926+g6d7febe.d20250429) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=facebook/opt-125m, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \nINFO 05-11 07:51:50 [cpu.py:45] Using Torch SDPA backend.\nINFO 05-11 07:51:50 [parallel_state.py:946] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 05-11 07:51:50 [weight_utils.py:265] Using model weights format ['*.bin']\nLoading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.18it/s]\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.18it/s]\n\nINFO 05-11 07:51:51 [loader.py:458] Loading weights took 0.32 seconds\nINFO 05-11 07:51:51 [executor_base.py:112] # cpu blocks: 7281, # CPU blocks: 0\nINFO 05-11 07:51:51 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 56.88x\nINFO 05-11 07:51:51 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 0.07 seconds\nProcessed prompts:   0%|                                                                                                                                    | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/akashk/vllm_build/vllm/examples/offline_inference/basic/basic.py\", line 34, in <module>\n[rank0]:     main()\n[rank0]:   File \"/home/akashk/vllm_build/vllm/examples/offline_inference/basic/basic.py\", line 22, in main\n[rank0]:     outputs = llm.generate(prompts, sampling_params)\n[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/utils.py\", line 1184, in inner\n[rank0]:     return fn(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/entrypoints/llm.py\", line 470, in generate\n[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)\n[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/entrypoints/llm.py\", line 1423, in _run_engine\n[rank0]:     step_outputs = self.llm_engine.step()\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/engine/llm_engine.py\", line 1435, in step\n[rank0]:     outputs = self.model_executor.execute_model(\n[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/executor/executor_base.py\", line 299, in execute_model\n[rank0]:     driver_outputs = self._driver_execute_model(execute_model_req)\n[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/executor/mp_distributed_executor.py\", line 144, in _driver_execute_model\n[rank0]:     return self.driver_worker.execute_model(execute_model_req)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/worker/worker_base.py\", line 420, in execute_model\n[rank0]:     output = self.model_runner.execute_model(\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[rank0]:     return func(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/worker/cpu_model_runner.py\", line 663, in execute_model\n[rank0]:     hidden_states = model_executable(\n[rank0]:                     ^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/model_executor/models/opt.py\", line 392, in forward\n[rank0]:     hidden_states = self.model(input_ids, positions, intermediate_tensors,\n[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/compilation/decorators.py\", line 172, in __call__\n[rank0]:     return self.forward(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/model_executor/models/opt.py\", line 311, in forward\n[rank0]:     return self.decoder(input_ids,\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/model_executor/models/opt.py\", line 272, in forward\n[rank0]:     hidden_states = layer(hidden_states)\n[rank0]:                     ^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/model_executor/models/opt.py\", line 171, in forward\n[rank0]:     hidden_states = self.self_attn(hidden_states=hidden_states)\n[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/model_executor/models/opt.py\", line 113, in forward\n[rank0]:     attn_output = self.attn(q, k, v)\n[rank0]:                   ^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/attention/layer.py\", line 232, in forward\n[rank0]:     return torch.ops.vllm.unified_attention(\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/torch/_ops.py\", line 1158, in __call__\n[rank0]:     return self._op(*args, **(kwargs or {}))\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/attention/layer.py\", line 378, in unified_attention\n[rank0]:     output = self.impl.forward(self, query, key, value, kv_cache,\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/attention/backends/torch_sdpa.py\", line 510, in forward\n[rank0]:     PagedAttention.write_to_paged_cache(\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/attention/ops/ipex_attn.py\", line 61, in write_to_paged_cache\n[rank0]:     ops.reshape_and_cache(\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/vllm/_custom_ops.py\", line 1323, in reshape_and_cache\n[rank0]:     torch.ops._C_cache_ops.reshape_and_cache(key, value, key_cache,\n[rank0]:   File \"/home/akashk/vllm_env/lib64/python3.12/site-packages/torch/_ops.py\", line 1158, in __call__\n[rank0]:     return self._op(*args, **(kwargs or {}))\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]: NotImplementedError: Could not run '_C_cache_ops::reshape_and_cache' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. '_C_cache_ops::reshape_and_cache' is only available for these backends: [Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMTIA, AutogradMeta, Tracer, AutocastCPU, AutocastMTIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\n[rank0]: Meta: registered at /pytorch/aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\n[rank0]: BackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\n[rank0]: Python: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\n[rank0]: FuncTorchDynamicLayerBackMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:479 [backend fallback]\n[rank0]: Functionalize: registered at /pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\n[rank0]: Named: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\n[rank0]: Conjugate: registered at /pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\n[rank0]: Negative: registered at /pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\n[rank0]: ZeroTensor: registered at /pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\n[rank0]: ADInplaceOrView: fallthrough registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:100 [backend fallback]\n[rank0]: AutogradOther: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\n[rank0]: AutogradCPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]\n[rank0]: AutogradCUDA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:75 [backend fallback]\n[rank0]: AutogradXLA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:83 [backend fallback]\n[rank0]: AutogradMPS: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:91 [backend fallback]\n[rank0]: AutogradXPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:71 [backend fallback]\n[rank0]: AutogradHPU: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:104 [backend fallback]\n[rank0]: AutogradLazy: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:87 [backend fallback]\n[rank0]: AutogradMTIA: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:79 [backend fallback]\n[rank0]: AutogradMeta: registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:95 [backend fallback]\n[rank0]: Tracer: registered at /pytorch/torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\n[rank0]: AutocastCPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:322 [backend fallback]\n[rank0]: AutocastMTIA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\n[rank0]: AutocastXPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:504 [backend fallback]\n[rank0]: AutocastMPS: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\n[rank0]: AutocastCUDA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\n[rank0]: FuncTorchBatched: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\n[rank0]: BatchedNestedTensor: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\n[rank0]: FuncTorchVmapMode: fallthrough registered at /pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\n[rank0]: Batched: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\n[rank0]: VmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n[rank0]: FuncTorchGradWrapper: registered at /pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:208 [backend fallback]\n[rank0]: PythonTLSSnapshot: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\n[rank0]: FuncTorchDynamicLayerFrontMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:475 [backend fallback]\n[rank0]: PreDispatch: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\n[rank0]: PythonDispatcher: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n\nProcessed prompts:   0%|                                                                                                                                    | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-05-11T12:59:05+00:00",
    "closed_at": "2025-05-19T06:20:16+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17960/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/17960"
  },
  {
    "number": 6187,
    "title": "[Feature]: lazy import for VLM",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\r\n\r\nI used [vLLM 0.5.0.post1](https://github.com/vllm-project/vllm/releases/tag/v0.5.0.post1) for `Mixtral-8x7B-Instruct-v0.1` inference\r\n```bash\r\npython3 -m vllm.entrypoints.openai.api_server --model /workdir/Mixtral-8x7B-Instruct-v0.1 --tensor-parallel-size 2\r\n```\r\nand get the error\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1560, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"/usr/local/lib/python3.9/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.9/site-packages/transformers/models/auto/image_processing_auto.py\", line 27, in <module>\r\n    from ...image_processing_utils import BaseImageProcessor, ImageProcessingMixin\r\n  File \"/usr/local/lib/python3.9/site-packages/transformers/image_processing_utils.py\", line 21, in <module>\r\n    from .image_transforms import center_crop, normalize, rescale\r\n  File \"/usr/local/lib/python3.9/site-packages/transformers/image_transforms.py\", line 22, in <module>\r\n    from .image_utils import (\r\n  File \"/usr/local/lib/python3.9/site-packages/transformers/image_utils.py\", line 58, in <module>\r\n    from torchvision.transforms import InterpolationMode\r\n  File \"/usr/local/lib/python3.9/site-packages/torchvision/__init__.py\", line 6, in <module>\r\n    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils\r\n  File \"/usr/local/lib/python3.9/site-packages/torchvision/_meta_registrations.py\", line 164, in <module>\r\n    def meta_nms(dets, scores, iou_threshold):\r\n  File \"/usr/local/lib/python3.9/site-packages/torch/library.py\", line 467, in inner\r\n    handle = entry.abstract_impl.register(func_to_register, source)\r\n  File \"/usr/local/lib/python3.9/site-packages/torch/_library/abstract_impl.py\", line 30, in register\r\n    if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, \"Meta\"):\r\nRuntimeError: operator torchvision::nms does not exist\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/local/lib/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.9/site-packages/vllm/entrypoints/openai/api_server.py\", line 26, in <module>\r\n    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat\r\n  File \"/usr/local/lib/python3.9/site-packages/vllm/entrypoints/openai/serving_chat.py\", line 29, in <module>\r\n    from vllm.multimodal.image import ImagePixelData\r\n  File \"/usr/local/lib/python3.9/site-packages/vllm/multimodal/__init__.py\", line 2, in <module>\r\n    from .registry import MULTIMODAL_REGISTRY, MultiModalRegistry\r\n  File \"/usr/local/lib/python3.9/site-packages/vllm/multimodal/registry.py\", line 9, in <module>\r\n    from .image import (ImageFeatureData, ImageFeaturePlugin, ImagePixelData,\r\n  File \"/usr/local/lib/python3.9/site-packages/vllm/multimodal/image.py\", line 9, in <module>\r\n    from vllm.transformers_utils.image_processor import cached_get_image_processor\r\n  File \"/usr/local/lib/python3.9/site-packages/vllm/transformers_utils/image_processor.py\", line 4, in <module>\r\n    from transformers import AutoImageProcessor\r\n  File \"<frozen importlib._bootstrap>\", line 1055, in _handle_fromlist\r\n  File \"/usr/local/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1551, in __getattr__\r\n    value = getattr(module, name)\r\n  File \"/usr/local/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1550, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"/usr/local/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1562, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.models.auto.image_processing_auto because of the following error (look up to see its traceback):\r\noperator torchvision::nms does not exist\r\n```\r\n\r\nConsidering that I did not use the VLM function, only the LLM, there should not be a strong dependency on the VLM component. Maybe we could implement lazy importing for the VLM-related module, similar to https://github.com/InternLM/lmdeploy/pull/1714.\r\nDo you have any suggestions? And I'm glad to land the PR for this. Thanks. \r\ncc @ywang96 @simon-mo \r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\n_No response_",
    "labels": [
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-07-07T11:50:48+00:00",
    "closed_at": "2024-07-08T05:20:06+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6187/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/6187"
  },
  {
    "number": 19779,
    "title": "[Bug]: wrong output on L20 using fp8",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.4 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : Could not collect\nCMake version                : version 3.30.2\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.6.0+cu124\nIs debug build               : False\nCUDA used to build PyTorch   : 12.4\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64-bit runtime)\nPython platform              : Linux-5.4.210.bsk.6-amd64-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.6.68\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : \nGPU 0: NVIDIA L20\nGPU 1: NVIDIA L20\n\nNvidia driver version        : 535.161.08\ncuDNN version                : Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.4.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.4.0\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   52 bits physical, 57 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          180\nOn-line CPU(s) list:             0-179\nVendor ID:                       GenuineIntel\nModel name:                      Intel(R) Xeon(R) Platinum 8457C\nCPU family:                      6\nModel:                           143\nThread(s) per core:              2\nCore(s) per socket:              45\nSocket(s):                       2\nStepping:                        8\nBogoMIPS:                        5200.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 wbnoinvd arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid cldemote movdiri movdir64b md_clear arch_capabilities\nHypervisor vendor:               KVM\nVirtualization type:             full\nL1d cache:                       4.2 MiB (90 instances)\nL1i cache:                       2.8 MiB (90 instances)\nL2 cache:                        180 MiB (90 instances)\nL3 cache:                        195 MiB (2 instances)\nNUMA node(s):                    2\nNUMA node0 CPU(s):               0-89\nNUMA node1 CPU(s):               90-179\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Mitigation; TSX disabled\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] flashinfer==0.1.6+cu124torch2.4\n[pip3] mypy==1.15.0\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cudnn-frontend==1.6.0\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-dali-cuda120==1.41.0\n[pip3] nvidia-ml-py==11.495.46\n[pip3] nvidia-modelopt==0.15.1\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvimgcodec-cu12==0.3.0.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] nvidia-pyindex==1.0.9\n[pip3] onnx==1.16.2\n[pip3] optree==0.12.1\n[pip3] pynvml==11.5.3\n[pip3] pytorch-triton==3.0.0+dedb7bdf3\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.6.0\n[pip3] torch_tensorrt==2.5.0a0\n[pip3] torchao==0.9.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.3\n[pip3] triton==3.2.0\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : N/A (dev)\nvLLM Build Flags:\n  CUDA Archs: 5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NODE    SYS     91-176  1               N/A\nGPU1    NODE     X      SYS     91-176  1               N/A\nNIC0    SYS     SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n\n==============================\n     Environment Variables\n==============================\nNVIDIA_VISIBLE_DEVICES=GPU-e2201786-780c-7f9a-39fa-865e1f9567f5,GPU-30a5a866-9e06-8c89-27e2-86b74fb70ced\nCUBLAS_VERSION=12.6.3.1002\nNVIDIA_REQUIRE_CUDA=cuda>=9.0\nCUDA_CACHE_DISABLE=1\nTORCH_CUDA_ARCH_LIST=5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX\nNCCL_VERSION=2.22.3\nNCCL_SOCKET_IFNAME=carma_br0\nNVIDIA_DRIVER_CAPABILITIES=compute,utility,video\nNVIDIA_PRODUCT_NAME=PyTorch\nCUDA_VERSION=12.6.1.006\nPYTORCH_VERSION=2.5.0a0+b465a58\nPYTORCH_BUILD_NUMBER=0\nCUDNN_FRONTEND_VERSION=1.6.0\nNVIDIA_DISABLE_REQUIRE=1\nMAX_JOBS=32\nCUDNN_VERSION=9.4.0.58\nPYTORCH_HOME=/opt/pytorch/pytorch\nLD_LIBRARY_PATH=/usr/local/cuda-12.6/compat:/usr/local/cuda-12.6/lib64:/var/lib/fastrak/lib64:/opt/tiger/jdk/jdk8u265-b01/jre/lib/amd64/server:/opt/tiger/yarn_deploy/hadoop_current/lib/native:/opt/tiger/yarn_deploy/hadoop_current/lib/native/ufs:/opt/tiger/yarn_deploy/hadoop/lib/native:/opt/tiger/yarn_deploy/hadoop_current/lib/native:/opt/tiger/yarn_deploy/hadoop_current/lzo/lib:/usr/local/cuda/compat/lib.real:/usr/local/lib/python3.10/dist-packages/torch/lib:/usr/local/lib/python3.10/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/mpi/lib\nNVIDIA_BUILD_ID=109656992\nCUDA_DRIVER_VERSION=560.35.03\nPYTORCH_BUILD_VERSION=2.5.0a0+b465a58\nCUDA_HOME=/usr/local/cuda\nCUDA_HOME=/usr/local/cuda\nCUDA_MODULE_LOADING=LAZY\nNVIDIA_REQUIRE_JETPACK_HOST_MOUNTS=\nNVIDIA_PYTORCH_VERSION=24.09\nTORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWe serve Qwen 2.5 vl using fp8 with tp2 on 2 L20.\n\nThe output is always \"!!!!\"\n\nhas anyone got the same error? \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-06-17T23:08:07+00:00",
    "closed_at": null,
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19779/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/19779"
  },
  {
    "number": 2645,
    "title": "Question: Would a PR integrating ExLlamaV2 kernels with AWQ be accepted?",
    "body": "Recently, ExLlamaV2 kernels were introduced into AutoAWQ. We can instantly map the AWQ packed weights to be compatible with ExLlama, and it runs decoding about 20% faster.\n\n# Performance\n\nNote that the gap in prefilling has recently been closed, so the main benefit would be during decoding.\n\n### GEMM (AWQ kernel)\n\n| Batch Size | Prefill Length | Decode Length | Prefill tokens/s | Decode tokens/s | Memory (VRAM) |\n| -- | -- | -- | -- | -- | -- |\n| 1 | 64 | 64 | 316\\.842 | 156\\.038 | 4\\.78 GB (20.20%) |\n| 1 | 128 | 128 | 4898\\.86 | 154\\.977 | 4\\.79 GB (20.27%) |\n| 1 | 256 | 256 | 5366\\.24 | 151\\.31 | 4\\.81 GB (20.35%) |\n| 1 | 512 | 512 | 5239\\.46 | 144\\.517 | 4\\.85 GB (20.51%) |\n| 1 | 1024 | 1024 | 4573\\.25 | 132\\.849 | 4\\.93 GB (20.83%) |\n| 1 | 2048 | 2048 | 3859\\.42 | 114\\.249 | 5\\.55 GB (23.48%) |\n| 8 | 64 | 64 | 1733\\.1 | 1176\\.07 | 4\\.83 GB (20.42%) |\n| 8 | 128 | 128 | 5359\\.34 | 1167\\.19 | 4\\.90 GB (20.72%) |\n| 8 | 256 | 256 | 5145\\.94 | 1130\\.84 | 5\\.03 GB (21.26%) |\n| 8 | 512 | 512 | 4802\\.91 | 1070\\.9 | 5\\.67 GB (23.98%) |\n| 8 | 1024 | 1024 | 4391\\.24 | 972\\.987 | 7\\.84 GB (33.17%) |\n| 8 | 2048 | 2048 | 3643 | 822\\.977 | 16\\.82 GB (71.12%) |\n\n### ExLlamaV2 (AWQ mapped to ExLlama)\n\n| Batch Size | Prefill Length | Decode Length | Prefill tokens/s | Decode tokens/s | Memory (VRAM) |\n| -- | -- | -- | -- | -- | -- |\n| 1 | 64 | 64 | 270\\.787 | 188\\.865 | 4\\.78 GB (20.20%) |\n| 1 | 128 | 128 | 3485\\.07 | 187\\.321 | 4\\.79 GB (20.27%) |\n| 1 | 256 | 256 | 6222\\.64 | 182\\.163 | 4\\.81 GB (20.35%) |\n| 1 | 512 | 512 | 8490\\.19 | 172\\.584 | 4\\.85 GB (20.51%) |\n| 1 | 1024 | 1024 | 8332\\.96 | 155\\.997 | 4\\.93 GB (20.83%) |\n| 1 | 2048 | 2048 | 6637\\.51 | 131\\.023 | 5\\.77 GB (24.41%) |\n| 8 | 64 | 64 | 2033\\.27 | 1176\\.85 | 4\\.83 GB (20.42%) |\n| 8 | 128 | 128 | 11486\\.6 | 1167\\.35 | 4\\.90 GB (20.72%) |\n| 8 | 256 | 256 | 11717\\.7 | 1129\\.59 | 5\\.03 GB (21.26%) |\n| 8 | 512 | 512 | 10471\\.8 | 1071\\.17 | 5\\.56 GB (23.52%) |\n| 8 | 1024 | 1024 | 8925\\.87 | 970\\.286 | 8\\.39 GB (35.48%) |\n| 8 | 2048 | 2048 | 6768\\.37 | 823\\.098 | 17\\.80 GB (75.28%) |\n",
    "labels": [
      "performance",
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-01-29T09:42:35+00:00",
    "closed_at": "2025-01-19T02:02:12+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2645/reactions",
      "total_count": 8,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 3,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2645"
  },
  {
    "number": 3385,
    "title": "Attention sliding window",
    "body": "In Hugging Face \"eager\" Mistral implementation, a sliding window of size 2048 will mask 2049 tokens. This is also true for flash attention. In the current vLLM implementation a window of 2048 will mask 2048 tokens:\r\n\r\n```\r\nimport torch\r\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\r\n\r\nattn_bias = BlockDiagonalCausalMask.from_seqlens([4096])\r\nattn_bias = attn_bias.make_local_attention(2048)\r\nmask = attn_bias._create_block_mask([4096, 4096])\r\nprint(torch.sum(mask == 0, dim=1))\r\n```\r\n**Output: tensor([   1,    2,    3,  ..., 2048, 2048, 2048])**\r\n\r\n\r\nThe output should be: **tensor([   1,    2,    3,  ..., 2049, 2049, 2049])**\r\n\r\nContext: https://github.com/huggingface/transformers/issues/29623 ",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-13T19:53:48+00:00",
    "closed_at": "2025-03-28T02:05:33+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3385/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3385"
  },
  {
    "number": 4520,
    "title": "[Bug]: Failing to find LoRA adapter for MultiLoRA Inference",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nI'm running the latest docker image and an openai style endpoint.\r\n\r\nMy command is:\r\n```\r\n--model NousResearch/Meta-Llama-3-8B-Instruct --max-model-len 8192 --port 8000 --enable-lora --lora-modules forced-french=Trelis/Meta-Llama-3-8B-Instruct-forced-french-adapters --max-loras 1 --max-lora-rank 8\r\n```\r\nI'm hitting the endpoint (on runpod) with:\r\n```\r\ncurl https://y55xy7ozoxrn15-8000.proxy.runpod.net/v1/completions \\\r\n    -H \"Content-Type: application/json\" \\\r\n    -d '{\r\n        \"model\": \"forced-french\",\r\n        \"prompt\": \"Why did the chicken cross the road?\",\r\n        \"max_tokens\": 50,\r\n        \"temperature\": 0\r\n    }'\r\n```\r\nThe error is:\r\n```\r\nterminal: Internal Server Error\r\n\r\nlogs:\r\n\r\n2024-05-01T08:45:12.669025667Z ERROR 05-01 08:45:12 async_llm_engine.py:43]     return func(*args, **kwargs)\r\n2024-05-01T08:45:12.669029441Z ERROR 05-01 08:45:12 async_llm_engine.py:43]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 249, in execute_model\r\n2024-05-01T08:45:12.669035014Z ERROR 05-01 08:45:12 async_llm_engine.py:43]     output = self.model_runner.execute_model(seq_group_metadata_list,\r\n2024-05-01T08:45:12.669038735Z ERROR 05-01 08:45:12 async_llm_engine.py:43]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2024-05-01T08:45:12.669042411Z ERROR 05-01 08:45:12 async_llm_engine.py:43]     return func(*args, **kwargs)\r\n2024-05-01T08:45:12.669045915Z ERROR 05-01 08:45:12 async_llm_engine.py:43]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 830, in execute_model\r\n2024-05-01T08:45:12.669049531Z ERROR 05-01 08:45:12 async_llm_engine.py:43]     self.set_active_loras(lora_requests, lora_mapping)\r\n2024-05-01T08:45:12.669053065Z ERROR 05-01 08:45:12 async_llm_engine.py:43]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 940, in set_active_loras\r\n2024-05-01T08:45:12.669056691Z ERROR 05-01 08:45:12 async_llm_engine.py:43]     self.lora_manager.set_active_loras(lora_requests, lora_mapping)\r\n2024-05-01T08:45:12.669060878Z ERROR 05-01 08:45:12 async_llm_engine.py:43]   File \"/usr/local/lib/python3.10/dist-packages/vllm/lora/worker_manager.py\", line 112, in set_active_loras\r\n2024-05-01T08:45:12.669064591Z ERROR 05-01 08:45:12 async_llm_engine.py:43]     self._apply_loras(lora_requests)\r\n2024-05-01T08:45:12.669068158Z ERROR 05-01 08:45:12 async_llm_engine.py:43]   File \"/usr/local/lib/python3.10/dist-packages/vllm/lora/worker_manager.py\", line 234, in _apply_loras\r\n2024-05-01T08:45:12.669071818Z ERROR 05-01 08:45:12 async_llm_engine.py:43]     self.add_lora(lora)\r\n2024-05-01T08:45:12.669075342Z ERROR 05-01 08:45:12 async_llm_engine.py:43]   File \"/usr/local/lib/python3.10/dist-packages/vllm/lora/worker_manager.py\", line 241, in add_lora\r\n2024-05-01T08:45:12.669078878Z ERROR 05-01 08:45:12 async_llm_engine.py:43]     lora = self._load_lora(lora_request)\r\n2024-05-01T08:45:12.669082375Z ERROR 05-01 08:45:12 async_llm_engine.py:43]   File \"/usr/local/lib/python3.10/dist-packages/vllm/lora/worker_manager.py\", line 161, in _load_lora\r\n2024-05-01T08:45:12.669085905Z ERROR 05-01 08:45:12 async_llm_engine.py:43]     raise RuntimeError(\r\n2024-05-01T08:45:12.669089460Z ERROR 05-01 08:45:12 async_llm_engine.py:43] RuntimeError: Loading lora Trelis/Meta-Llama-3-8B-Instruct-forced-french-adapters failed\r\n2024-05-01T08:45:12.670086449Z Exception in callback functools.partial(<function _raise_exception_on_finish at 0x7fdba4287400>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7fdbaf021510>>)\r\n2024-05-01T08:45:12.670134866Z handle: <Handle functools.partial(<function _raise_exception_on_finish at 0x7fdba4287400>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7fdbaf021510>>)>\r\n2024-05-01T08:45:12.670143323Z Traceback (most recent call last):\r\n2024-05-01T08:45:12.670151190Z   File \"/usr/local/lib/python3.10/dist-packages/vllm/lora/worker_manager.py\", line 149, in _load_lora\r\n2024-05-01T08:45:12.670157520Z     lora = self._lora_model_cls.from_local_checkpoint(\r\n2024-05-01T08:45:12.670163614Z   File \"/usr/local/lib/python3.10/dist-packages/vllm/lora/models.py\", line 210, in from_local_checkpoint\r\n2024-05-01T08:45:12.670170560Z     with open(lora_config_path) as f:\r\n2024-05-01T08:45:12.670176984Z FileNotFoundError: [Errno 2] No such file or directory: 'Trelis/Meta-Llama-3-8B-Instruct-forced-french-adapters/adapter_config.json'\r\n2024-05-01T08:45:12.675367370Z     output = await make_async(self.driver_worker.execute_model)(\r\n2024-05-01T08:45:12.675368878Z   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n2024-05-01T08:45:12.675370415Z     result = self.fn(*self.args, **self.kwargs)\r\n2024-05-01T08:45:12.675371860Z   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2024-05-01T08:45:12.675373398Z     return func(*args, **kwargs)\r\n2024-05-01T08:45:12.675375002Z   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 249, in execute_model\r\n2024-05-01T08:45:12.675376462Z     output = self.model_runner.execute_model(seq_group_metadata_list,\r\n2024-05-01T08:45:12.675377925Z   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n2024-05-01T08:45:12.675379612Z     return func(*args, **kwargs)\r\n2024-05-01T08:45:12.675381100Z   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 830, in execute_model\r\n2024-05-01T08:45:12.675382617Z     self.set_active_loras(lora_requests, lora_mapping)\r\n2024-05-01T08:45:12.675384082Z   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 940, in set_active_loras\r\n2024-05-01T08:45:12.675385562Z     self.lora_manager.set_active_loras(lora_requests, lora_mapping)\r\n2024-05-01T08:45:12.675387366Z   File \"/usr/local/lib/python3.10/dist-packages/vllm/lora/worker_manager.py\", line 112, in set_active_loras\r\n2024-05-01T08:45:12.675388845Z     self._apply_loras(lora_requests)\r\n2024-05-01T08:45:12.675390312Z   File \"/usr/local/lib/python3.10/dist-packages/vllm/lora/worker_manager.py\", line 234, in _apply_loras\r\n2024-05-01T08:45:12.675391802Z     self.add_lora(lora)\r\n2024-05-01T08:45:12.675393260Z   File \"/usr/local/lib/python3.10/dist-packages/vllm/lora/worker_manager.py\", line 241, in add_lora\r\n2024-05-01T08:45:12.675394687Z     lora = self._load_lora(lora_request)\r\n2024-05-01T08:45:12.675396124Z   File \"/usr/local/lib/python3.10/dist-packages/vllm/lora/worker_manager.py\", line 161, in _load_lora\r\n2024-05-01T08:45:12.675397568Z     raise RuntimeError(\r\n2024-05-01T08:45:12.675399164Z RuntimeError: Loading lora Trelis/Meta-Llama-3-8B-Instruct-forced-french-adapters failed\r\n```\r\nNote that the model does display correctly when I hit the /models endpoint:\r\n```\r\ncurl https://y55xy7ozoxrn15-8000.proxy.runpod.net/v1/models       \r\n{\"object\":\"list\",\"data\":[{\"id\":\"NousResearch/Meta-Llama-3-8B-Instruct\",\"object\":\"model\",\"created\":1714553209,\"owned_by\":\"vllm\",\"root\":\"NousResearch/Meta-Llama-3-8B-Instruct\",\"parent\":null,\"permission\":[{\"id\":\"modelperm-9630fa8b3fd84626ab68bdd5da94c8a2\",\"object\":\"model_permission\",\"created\":1714553209,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]},{\"id\":\"forced-french\",\"object\":\"model\",\"created\":1714553209,\"owned_by\":\"vllm\",\"root\":\"NousResearch/Meta-Llama-3-8B-Instruct\",\"parent\":null,\"permission\":[{\"id\":\"modelperm-31e64ce42d8b460c9551118150aa27c1\",\"object\":\"model_permission\",\"created\":1714553209,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}\r\n```\r\n\r\nFurther notes:\r\n1. My adapter is rank 4 (I notice that max rank can only be 8,16,32 - I'm unsure if this implies that a rank of 4 may not be supported).\r\n2. My adapter safetensors file contains embeddings (as they were updated). Could this be the cause of the issue? It would make sense if only LoRAs of linear layers are supported.\r\n3. My LoRA is only applied to one layer. Is this potentially the issue?",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-01T08:49:00+00:00",
    "closed_at": "2024-11-28T02:05:44+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4520/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4520"
  },
  {
    "number": 18757,
    "title": "[Bug]: model_executor/test_model_load_with_params.py  fails with AttributeError",
    "body": "### Your current environment\n\nIssue encountered on main branch tests.\n\n### \ud83d\udc1b Describe the bug\n\nTest failing with below traceback:\n\n```\nvllm_runner = <class 'tests.conftest.VllmRunner'>\n\n    @pytest.mark.skipif(current_platform.is_rocm(),\n                        reason=\"Xformers backend is not supported on ROCm.\")\n    def test_model_loading_with_params(vllm_runner):\n        \"\"\"\n        Test parameter weight loading with tp>1.\n        \"\"\"\n        with vllm_runner(model_name=MODEL_NAME,\n                         revision=REVISION,\n                         dtype=\"float16\",\n                         max_model_len=MAX_MODEL_LEN) as vllm_model:\n            output = vllm_model.encode(\"Write a short story about a robot that\"\n                                       \" dreams for the first time.\\n\")\n    \n            model_config = vllm_model.model.llm_engine.model_config\n            model_tokenizer = vllm_model.model.llm_engine.tokenizer\n    \n            # asserts on the bert model config file\n            assert model_config.encoder_config[\"max_seq_length\"] == 512\n            assert model_config.encoder_config[\"do_lower_case\"]\n    \n            # asserts on the pooling config files\n            assert model_config.pooler_config.pooling_type == PoolingType.CLS.name\n>           assert model_config.pooler_config.pooling_norm\nE           AttributeError: 'PoolerConfig' object has no attribute 'pooling_norm'. Did you mean: 'pooling_type'?\n\ntests/model_executor/test_model_load_with_params.py:43: AttributeError\n------------------------------------------------- Captured stdout call -------------------------------------------------\nINFO 05-27 12:31:33 [__init__.py:31] Available plugins for group vllm.general_plugins:\nINFO 05-27 12:31:33 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\nINFO 05-27 12:31:33 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\nINFO 05-27 12:32:10 [config.py:577] Found sentence-transformers tokenize configuration.\nINFO 05-27 12:32:10 [config.py:3131] Downcasting torch.float32 to torch.float16.\nINFO 05-27 12:32:18 [config.py:473] Found sentence-transformers modules configuration.\nINFO 05-27 12:32:18 [config.py:493] Found pooling configuration.\nINFO 05-27 12:32:18 [config.py:793] This model supports multiple tasks: {'reward', 'embed', 'classify', 'score'}. Defaulting to 'embed'.\nWARNING 05-27 12:32:18 [arg_utils.py:1583] --task embed is not supported by the V1 Engine. Falling back to V0. \nINFO 05-27 12:32:18 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.1.dev148+gfba064270.d20250526) with config: model='BAAI/bge-base-en-v1.5', speculative_config=None, tokenizer='BAAI/bge-base-en-v1.5', skip_tokenizer_init=False, tokenizer_mode=auto, revision=main, override_neuron_config={}, tokenizer_revision=main, trust_remote_code=True, dtype=torch.float16, max_seq_len=128, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=BAAI/bge-base-en-v1.5, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type='CLS', normalize=True, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"cudagraph_capture_sizes\": [256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 256}, use_cached_outputs=False, \nINFO 05-27 12:32:18 [cuda.py:292] Using Flash Attention backend.\nINFO 05-27 12:32:19 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nINFO 05-27 12:32:19 [model_runner.py:1170] Starting to load model BAAI/bge-base-en-v1.5...\nINFO 05-27 12:32:19 [weight_utils.py:291] Using model weights format ['*.safetensors']\nINFO 05-27 12:32:20 [weight_utils.py:344] No model.safetensors.index.json found in remote.\nINFO 05-27 12:32:20 [default_loader.py:280] Loading weights took 0.08 seconds\nINFO 05-27 12:32:20 [model_runner.py:1202] Model loading took 0.2091 GiB and 0.738950 seconds\n------------------------------------------------- Captured stderr call -------------------------------------------------\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 13.57it/s]\n\nAdding requests: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 66/66 [00:00<00:00, 3031.89it/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 66/66 [00:00<00:00, 176.21it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n________________________________________ test_roberta_model_loading_with_params ________________________________________\n\nvllm_runner = <class 'tests.conftest.VllmRunner'>\n\n    @pytest.mark.skipif(current_platform.is_rocm(),\n                        reason=\"Xformers backend is not supported on ROCm.\")\n    def test_roberta_model_loading_with_params(vllm_runner):\n        \"\"\"\n        Test parameter weight loading with tp>1.\n        \"\"\"\n        with vllm_runner(model_name=MODEL_NAME_ROBERTA,\n                         revision=REVISION_ROBERTA,\n                         dtype=\"float16\",\n                         max_model_len=MAX_MODEL_LEN) as vllm_model:\n            output = vllm_model.encode(\"Write a short story about a robot that\"\n                                       \" dreams for the first time.\\n\")\n    \n            model_config = vllm_model.model.llm_engine.model_config\n            model_tokenizer = vllm_model.model.llm_engine.tokenizer\n    \n            # asserts on the bert model config file\n            assert model_config.encoder_config[\"max_seq_length\"] == 512\n            assert not model_config.encoder_config[\"do_lower_case\"]\n    \n            # asserts on the pooling config files\n            assert model_config.pooler_config.pooling_type == PoolingType.MEAN.name\n>           assert model_config.pooler_config.pooling_norm\nE           AttributeError: 'PoolerConfig' object has no attribute 'pooling_norm'. Did you mean: 'pooling_type'?\n\ntests/model_executor/test_model_load_with_params.py:83: AttributeError\n------------------------------------------------- Captured stdout call -------------------------------------------------\nINFO 05-27 12:32:21 [config.py:577] Found sentence-transformers tokenize configuration.\nINFO 05-27 12:32:21 [config.py:3131] Downcasting torch.float32 to torch.float16.\nINFO 05-27 12:32:21 [config.py:473] Found sentence-transformers modules configuration.\nINFO 05-27 12:32:21 [config.py:493] Found pooling configuration.\nINFO 05-27 12:32:21 [config.py:793] This model supports multiple tasks: {'reward', 'embed', 'classify', 'score'}. Defaulting to 'embed'.\nWARNING 05-27 12:32:21 [arg_utils.py:1583] --task embed is not supported by the V1 Engine. Falling back to V0. \nINFO 05-27 12:32:21 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.1.dev148+gfba064270.d20250526) with config: model='intfloat/multilingual-e5-small', speculative_config=None, tokenizer='intfloat/multilingual-e5-small', skip_tokenizer_init=False, tokenizer_mode=auto, revision=main, override_neuron_config={}, tokenizer_revision=main, trust_remote_code=True, dtype=torch.float16, max_seq_len=128, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=intfloat/multilingual-e5-small, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type='MEAN', normalize=True, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"cudagraph_capture_sizes\": [256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 256}, use_cached_outputs=False, \nINFO 05-27 12:32:23 [cuda.py:292] Using Flash Attention backend.\nINFO 05-27 12:32:23 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nINFO 05-27 12:32:23 [model_runner.py:1170] Starting to load model intfloat/multilingual-e5-small...\nINFO 05-27 12:32:24 [weight_utils.py:291] Using model weights format ['*.safetensors']\nINFO 05-27 12:32:24 [weight_utils.py:344] No model.safetensors.index.json found in remote.\nINFO 05-27 12:32:24 [default_loader.py:280] Loading weights took 0.06 seconds\nINFO 05-27 12:32:24 [model_runner.py:1202] Model loading took 0.2204 GiB and 1.087685 seconds\n------------------------------------------------- Captured stderr call -------------------------------------------------\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 18.67it/s]\n\nAdding requests: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 66/66 [00:00<00:00, 11180.74it/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 66/66 [00:00<00:00, 2941.53it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n=============================================== short test summary info ================================================\nFAILED tests/model_executor/test_model_load_with_params.py::test_model_loading_with_params - AttributeError: 'PoolerConfig' object has no attribute 'pooling_norm'. Did you mean: 'pooling_type'?\nFAILED tests/model_executor/test_model_load_with_params.py::test_roberta_model_loading_with_params - AttributeError: 'PoolerConfig' object has no attribute 'pooling_norm'. Did you mean: 'pooling_type'?\n======================================== 2 failed, 1 passed in 64.00s (0:01:04) ========================================\n\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-05-27T10:01:18+00:00",
    "closed_at": "2025-05-28T05:42:56+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18757/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/18757"
  },
  {
    "number": 2072,
    "title": "How to install from source with CUDA 11.8 instead of 12.1?",
    "body": null,
    "labels": [],
    "state": "closed",
    "created_at": "2023-12-13T01:49:45+00:00",
    "closed_at": "2024-03-28T12:02:21+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2072/reactions",
      "total_count": 5,
      "+1": 5,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2072"
  },
  {
    "number": 16904,
    "title": "[Bug]: glm.py rotary_dim bug",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nclass GlmForCausalLM(LlamaForCausalLM, SupportsV0Only):\n\n    def __init__(self, *, vllm_config: VllmConfig, prefix: str = \"\"):\n        super().__init__(vllm_config=vllm_config, prefix=prefix)\n        # Hack Llama model to fit HF format GLM implementation\n        # Attention difference between GLM and Llama:\n        # 1. Half partial rotary_dim and no Neox style.\n        # 2. There is no bias for o_proj in attention\n        for layer in self.model.layers:\n            if not isinstance(layer, PPMissingLayer):\n                print(layer.self_attn.rotary_emb.rotary_dim)\n                layer.self_attn.rotary_emb.rotary_dim //= 2\n                layer.self_attn.rotary_emb.is_neox_style = False\n                layer.self_attn.o_proj.bias = None\n                layer.self_attn.o_proj.skip_bias_add = True\n\nlayer.self_attn.rotary_emb.rotary_dim is divided in every layer: \n\n![Image](https://github.com/user-attachments/assets/d3862aff-ca7c-4ca5-8bde-b03fd2ab3d44)\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-04-21T06:29:55+00:00",
    "closed_at": "2025-04-21T14:26:35+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16904/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/16904"
  },
  {
    "number": 7774,
    "title": "[Bug]: guide decoding lead to an incorrect function call arguments",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nWARNING 08-22 15:09:07 _custom_ops.py:14] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\r\n[DLI_CUDA] cudaDeviceGetStreamPriorityRange is unsupported, and return cudaSuccess.\r\nPyTorch version: 2.2.1\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.7\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: 15.0.6\r\nCMake version: version 3.27.0\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      39 bits physical, 48 bits virtual\r\nCPU(s):                             12\r\nOn-line CPU(s) list:                0-11\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 6\r\nSocket(s):                          1\r\nNUMA node(s):                       1\r\nVendor ID:                          GenuineIntel\r\nCPU family:                         6\r\nModel:                              165\r\nModel name:                         Intel(R) Core(TM) i5-10600K CPU @ 4.10GHz\r\nStepping:                           5\r\nCPU MHz:                            4100.000\r\nCPU max MHz:                        4800.0000\r\nCPU min MHz:                        800.0000\r\nBogoMIPS:                           8199.79\r\nVirtualization:                     VT-x\r\nL1d cache:                          192 KiB\r\nL1i cache:                          192 KiB\r\nL2 cache:                           1.5 MiB\r\nL3 cache:                           12 MiB\r\nNUMA node0 CPU(s):                  0-11\r\nVulnerability Gather data sampling: Mitigation; Microcode\r\nVulnerability Itlb multihit:        KVM: Mitigation: VMX disabled\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:             Mitigation; Enhanced IBRS\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Mitigation; Microcode\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp pku ospke md_clear flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.9.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] onnx==1.14.1\r\n[pip3] onnxruntime==1.19.0\r\n[pip3] pynvml==11.5.3\r\n[pip3] pyzmq==26.1.0\r\n[pip3] sentence-transformers==3.0.1\r\n[pip3] torch==2.2.1\r\n[pip3] transformers==4.44.0\r\n[pip3] triton==3.0.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] pynvml                    11.5.3                   pypi_0    pypi\r\n[conda] pyzmq                     26.1.0                   pypi_0    pypi\r\n[conda] sentence-transformers     3.0.1                    pypi_0    pypi\r\n[conda] torch                     2.2.1                    pypi_0    pypi\r\n[conda] transformers              4.44.0                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.3.post1@a73a17bcacea46caf762a76cf24d60338117846b\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nwhile i test the function call of openai api base on internlm model, i got the incorrect parameters like this:\r\n```\r\n{\"location\":\": \"}\r\n```\r\nbut if i reset the guideline guided_decode_logits_processor in serving_chat.py to None and skip_special_tokens to false, it will get the correct parameters.\r\n```\r\n<|action_start|><|plugin|>\r\n{\"name\": \"get_current_weather\", \"parameters\": {\"location\": \"Shanghai\"}}<|action_end|>\r\n```\r\n\r\nif i don't change skip_special_tokens to false, it will get the output like this:\r\n```\r\n\r\n{\"name\": \"get_current_weather\", \"parameters\": {\"location\": \"Shanghai\"}}\r\n```\r\n\r\nso i think the incorrect parameter is cased by the guide decoding logic,  done anyone meet the same situation?",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-22T07:17:46+00:00",
    "closed_at": "2024-12-22T02:04:22+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7774/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/7774"
  },
  {
    "number": 11091,
    "title": "[Feature]: Distinguish LoRA Model Metrics from Base Model Metrics in Reporting",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\r\n\r\nWhen submitting requests to a LoRA model and subsequently checking the associated metrics, I've noticed that all metrics are aggregated under the base model's metrics. This means that requests made to the LoRA model are being counted as requests to the base model. Given that LoRA models logically represent a distinct model layer on top of the base, it is crucial for accurate monitoring and analysis that we separate these metrics.\r\n\r\npart of https://github.com/vllm-project/vllm/issues/6275\r\n\r\n```\r\ncurl http://localhost:8000/v1/completions \\\r\n    -H \"Content-Type: application/json\" \\\r\n    -d '{\r\n        \"model\": \"sql-lora\",\r\n        \"prompt\": \"San Francisco is a\",\r\n        \"max_tokens\": 7,\r\n        \"temperature\": 0\r\n    }' \r\n```\r\n\r\n\r\n![image](https://github.com/user-attachments/assets/cf7706d3-e4f2-4f92-985b-517a90b1e3c6)\r\n\r\n\r\n### Expected Behavior\r\n\r\nMetrics for LoRA models should be distinctly reported, separate from the base model metrics, to accurately reflect their usage and performance.\r\n\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\n_No response_\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-12-11T09:17:15+00:00",
    "closed_at": "2025-04-11T02:06:13+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/11091/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/11091"
  },
  {
    "number": 17901,
    "title": "[Usage]: Running vLLM with B200 Blackwell",
    "body": "### Your current environment\n\nI have been trying the latest image but it seems not to use cuda 12.8, which is needed. Is there an image that will work with vllm and B200?\n\n### How would you like to use vllm\n\nI want to run inference of a [specific model](put link here). I don't know how to integrate it with vllm.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-05-09T12:43:42+00:00",
    "closed_at": "2025-05-29T21:26:28+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17901/reactions",
      "total_count": 7,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 3
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/17901"
  },
  {
    "number": 18533,
    "title": "[Bug]: v1 engine with full cuda graph option outputs garbage",
    "body": "### Your current environment\n\nx\n\n### \ud83d\udc1b Describe the bug\n\nrefers to https://github.com/vllm-project/vllm/issues/18520\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-05-22T07:19:36+00:00",
    "closed_at": "2025-06-04T08:10:24+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18533/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/18533"
  },
  {
    "number": 3024,
    "title": "AsyncLLMEngine cannot stop iteration when generation completes",
    "body": "@WoosukKwon @simon-mo \r\n\r\n### Environment\r\n\r\n- torch 2.1.2\r\n- vllm 0.3.2\r\n\r\n### Reproduce\r\n\r\n```python\r\nimport asyncio\r\nfrom vllm import AsyncEngineArgs, AsyncLLMEngine, SamplingParams\r\n\r\nengine = AsyncLLMEngine.from_engine_args(AsyncEngineArgs(\"01-ai/Yi-6B-Chat\"))\r\nparam = SamplingParams(max_tokens=50, stop_token_ids=[7])\r\ngenerator = engine.generate(\"<|im_start|>user\\nhi<|im_end|>\\n<|im_start|>assistant\\n\", param, \"req_test\")\r\n\r\nasync def test():\r\n    answer = None\r\n    async for result in generator:\r\n        print(result.finished, \"-\", result.outputs[0].text)\r\n        answer = result.outputs[0].text\r\n    print(\"Answer:\", answer)\r\n\r\nasyncio.get_event_loop().run_until_complete(test())\r\n```\r\n\r\n### Outputs\r\n\r\n```\r\nFalse - Hello\r\nFalse - Hello!\r\nFalse - Hello! How\r\nFalse - Hello! How can\r\nFalse - Hello! How can I\r\nFalse - Hello! How can I assist\r\nFalse - Hello! How can I assist you\r\nFalse - Hello! How can I assist you today\r\nFalse - Hello! How can I assist you today?\r\nINFO 02-25 01:20:26 async_llm_engine.py:110] Finished request req_test.\r\nTrue - Hello! How can I assist \r\nAnswer: Hello! How can I assist\r\n```\r\n\r\n### Expected behaviour\r\n\r\nThe `answer` should be \"Hello! How can I assist you today?\" instead of \"Hello! How can I assist\".\r\n\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2024-02-24T17:23:31+00:00",
    "closed_at": "2024-02-27T09:08:31+00:00",
    "comments": 7,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3024/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/3024"
  },
  {
    "number": 6776,
    "title": "[Bug]: qwen2-72b-instruct model  with RuntimeError: CUDA error: an illegal memory access was encountered ",
    "body": "### Your current environment\r\n\r\n```text\r\nPyTorch version: 2.3.0a0+ebedce2\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.3\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Mar 22 2024, 16:50:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-4.19.91-014-kangaroo.2.10.13.5c249cdaf.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.3.107\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nGPU 1: NVIDIA A100-SXM4-80GB\r\nGPU 2: NVIDIA A100-SXM4-80GB\r\nGPU 3: NVIDIA A100-SXM4-80GB\r\n\r\nNvidia driver version: 535.54.03\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          48\r\nOn-line CPU(s) list:             0-47\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Processor @ 2.90GHz\r\nCPU family:                      6\r\nModel:                           106\r\nThread(s) per core:              1\r\nCore(s) per socket:              48\r\nSocket(s):                       1\r\nStepping:                        6\r\nBogoMIPS:                        5800.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd avx512vbmi umip pku avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm md_clear arch_capabilities\r\nVirtualization:                  VT-x\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       2.3 MiB (48 instances)\r\nL1i cache:                       1.5 MiB (48 instances)\r\nL2 cache:                        60 MiB (48 instances)\r\nL3 cache:                        48 MiB (1 instance)\r\nNUMA node(s):                    1\r\nNUMA node0 CPU(s):               0-47\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.9.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.24.4\r\n[pip3] onnx==1.15.0rc2\r\n[pip3] optree==0.10.0\r\n[pip3] pytorch-quantization==2.1.2\r\n[pip3] torch==2.3.0a0+ebedce2\r\n[pip3] torch-tensorrt==2.3.0a0\r\n[pip3] torchdata==0.7.1a0\r\n[pip3] torchtext==0.17.0a0\r\n[pip3] torchvision==0.18.0a0\r\n[pip3] transformers==4.42.4\r\n[pip3] transformers-stream-generator==0.0.5\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.3.post1\r\nvLLM Build Flags:\r\nCUDA Archs: 5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV6     NV6     NV6     PHB     PHB     0-47            N/A             N/A\r\nGPU1    NV6      X      NV6     NV6     PHB     PHB     0-47            N/A             N/A\r\nGPU2    NV6     NV6      X      NV6     PHB     PHB     0-47            N/A             N/A\r\nGPU3    NV6     NV6     NV6      X      PHB     PHB     0-47            N/A             N/A\r\nNIC0    PHB     PHB     PHB     PHB      X      PHB\r\nNIC1    PHB     PHB     PHB     PHB     PHB      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n```\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI am running a test python script `test_llm.py`, test_llm.py code is as follows:\r\n<details>\r\n  <summary>Click to expand test_llm.py</summary>\r\n  \r\n  ```python\r\nimport torch\r\nfrom vllm import LLM, SamplingParams\r\nimport random\r\nimport random\r\nimport argparse\r\nimport time\r\nrandom.seed(0)  # Set the random seed for reproducibility\r\n\r\ndummy_prompt = \"hello \" * 30\r\n# print(dummy_prompt)\r\n\r\nprompts = []\r\nwith open(\"./benchmarks/sonnet.txt\", \"r\") as f:\r\n    prompts = f.readlines()\r\n    prompts = [prompt.strip() for prompt in prompts]\r\n\r\n# random.shuffle(prompts)\r\n\r\ndef test_llm(model:str, n, max_tokens, tp_size):\r\n    prompts_choose = prompts[:n]\r\n    # print(prompts_choose)\r\n    # Create a sampling params object.\r\n    sampling_params = SamplingParams(temperature=0.0,\r\n                                     top_p=1.0,\r\n                                     max_tokens=max_tokens,\r\n                                     ignore_eos=True)\r\n\r\n    # Create an LLM.\r\n    llm = LLM(model=model,\r\n              trust_remote_code=True,\r\n              enforce_eager=True, \r\n              disable_log_stats=False, \r\n              max_num_seqs=n,\r\n              tensor_parallel_size=tp_size,\r\n              disable_custom_all_reduce=True,\r\n              gpu_memory_utilization=0.9)\r\n\r\n    # Generate texts from the prompts. The output is a list of RequestOutput objects\r\n    # that contain the prompt, generated text, and other information.\r\n    \r\n    torch.cuda.synchronize()\r\n    time1 = time.perf_counter()\r\n    outputs = llm.generate(prompts_choose, sampling_params)\r\n    torch.cuda.synchronize()\r\n    time2 = time.perf_counter()\r\n\r\n    print(f\"\\nllm.generate over. All Generate Time: {time2 - time1:.5f} s\\n\")\r\n    \r\n    # # Print the outputs.\r\n    # for output in outputs:\r\n    #     prompt = output.prompt\r\n    #     generated_text = output.outputs[0].text\r\n    #     # print(f\"Prompt: {prompt!r},\\n\")\r\n    #     print(f\"Generated text: {generated_text!r}\\n\")\r\n\r\ndef test():\r\n    parser = argparse.ArgumentParser(description='Test LLM')\r\n    parser.add_argument('-n', type=int, default=4, help='Number of prompts')\r\n    parser.add_argument('-max_tokens', type=int, default=16, help='Maximum number of tokens')\r\n    parser.add_argument('-tp_size', type=int, default=1, help='Tensor Parallel Size')\r\n    parser.add_argument('-model', type=str, help='Model path')\r\n    args = parser.parse_args() \r\n\r\n    n = args.n\r\n    max_tokens = args.max_tokens\r\n    tp_size = args.tp_size\r\n    model = args.model\r\n\r\n    test_llm(model, n, max_tokens, tp_size)\r\n\r\ntest()\r\n\r\n```\r\n</details>\r\n\r\nrun command is as follows: \r\n```bash\r\npython test_llm.py -n 128 -max_tokens 256 -tp_size 4 -model {YOUR_PATH}\r\n```\r\n\r\nWhen I use model: [qwen2-72b-instruct](https://huggingface.co/Qwen/Qwen2-72B-Instruct),  max_num_seqs=128, tensor_parallel_size=4, enforce_eager=True, prompts: vllm/benchmarks/sonnet.txt, it crashes inexplicably, with the error RuntimeError. CUDA error: an illegal memory access was encountered.\r\n\r\n\r\nIt has been verified to occur with batch_size=128 (batch_size 64, 256 are normal), max_tokens > 4, (max_tokens 4, 8, 16, 32, 64, 128, 256, etc. all happen).\r\n\r\nAlso using dummy_prompt = \"hello \" * 30 will also happen this error.\r\n\r\nThe error output is:\r\n```bash\r\nProcessed prompts:   0%|                                                                                              | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Traceback (most recent call last):\r\n  File \"/mnt/data/zhuhr/vllm-main/test.log/test_llm.py\", line 240, in <module>\r\n    test()\r\n  File \"/mnt/data/zhuhr/vllm-main/test.log/test_llm.py\", line 238, in test\r\n    test_llm(model, n, max_tokens, tp_size)\r\n  File \"/mnt/data/zhuhr/vllm-main/test.log/test_llm.py\", line 208, in test_llm\r\n    outputs = llm.generate(prompts_choose, sampling_params)\r\n  File \"/mnt/data/zhuhr/vllm-main/vllm/utils.py\", line 844, in inner\r\n    return fn(*args, **kwargs)\r\n  File \"/mnt/data/zhuhr/vllm-main/vllm/entrypoints/llm.py\", line 316, in generate\r\n    outputs = self._run_engine(use_tqdm=use_tqdm)\r\n  File \"/mnt/data/zhuhr/vllm-main/vllm/entrypoints/llm.py\", line 569, in _run_engine\r\n    step_outputs = self.llm_engine.step()\r\n  File \"/mnt/data/zhuhr/vllm-main/vllm/engine/llm_engine.py\", line 911, in step\r\n    output = self.model_executor.execute_model(\r\n  File \"/mnt/data/zhuhr/vllm-main/vllm/executor/distributed_gpu_executor.py\", line 76, in execute_model\r\n    driver_outputs = self._driver_execute_model(execute_model_req)\r\n  File \"/mnt/data/zhuhr/vllm-main/vllm/executor/multiproc_gpu_executor.py\", line 141, in _driver_execute_model\r\n    return self.driver_worker.execute_model(execute_model_req)\r\n  File \"/mnt/data/zhuhr/vllm-main/vllm/worker/worker_base.py\", line 272, in execute_model\r\n    output = self.model_runner.execute_model(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/mnt/data/zhuhr/vllm-main/vllm/worker/model_runner.py\", line 1354, in execute_model\r\n    hidden_or_intermediate_states = model_executable(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/data/zhuhr/vllm-main/vllm/model_executor/models/qwen2.py\", line 336, in forward\r\n    hidden_states = self.model(input_ids, positions, kv_caches,\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/data/zhuhr/vllm-main/vllm/model_executor/models/qwen2.py\", line 257, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/data/zhuhr/vllm-main/vllm/model_executor/models/qwen2.py\", line 209, in forward\r\n    hidden_states = self.self_attn(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/data/zhuhr/vllm-main/vllm/model_executor/models/qwen2.py\", line 156, in forward\r\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/mnt/data/zhuhr/vllm-main/vllm/attention/layer.py\", line 97, in forward\r\n    return self.impl.forward(query,\r\n  File \"/mnt/data/zhuhr/vllm-main/vllm/attention/backends/flash_attn.py\", line 543, in forward\r\n    output[num_prefill_tokens:] = flash_attn_with_kvcache(\r\nRuntimeError: CUDA error: an illegal memory access was encountered\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n[rank0]:[E ProcessGroupNCCL.cpp:1335] [PG 2 Rank 0] NCCL watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\nException raised from c10_cuda_check_implementation at /opt/pytorch/pytorch/c10/cuda/CUDAException.cpp:44 (most recent call first):\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7ff617f9bdc9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\r\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x7ff617f4c2d0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\r\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x7ff6221c2142 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10_cuda.so)\r\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x6f (0x7ff5b6daf11f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\r\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x58 (0x7ff5b6db1dd8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\r\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1f3 (0x7ff5b6db9083 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\r\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x125 (0x7ff5b6db9ed5 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\r\nframe #7: <unknown function> + 0xdc253 (0x7ff617ab0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\r\nframe #8: <unknown function> + 0x94ac3 (0x7ff623912ac3 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #9: clone + 0x44 (0x7ff6239a3a04 in /lib/x86_64-linux-gnu/libc.so.6)\r\n\r\nterminate called after throwing an instance of 'c10::DistBackendError'\r\n  what():  [PG 2 Rank 0] NCCL watchdog thread terminated with exception: CUDA error: an illegal memory access was encountered\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\nException raised from c10_cuda_check_implementation at /opt/pytorch/pytorch/c10/cuda/CUDAException.cpp:44 (most recent call first):\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7ff617f9bdc9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\r\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x7ff617f4c2d0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\r\nframe #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x7ff6221c2142 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10_cuda.so)\r\nframe #3: c10d::ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const + 0x6f (0x7ff5b6daf11f in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\r\nframe #4: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x58 (0x7ff5b6db1dd8 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\r\nframe #5: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1f3 (0x7ff5b6db9083 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\r\nframe #6: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x125 (0x7ff5b6db9ed5 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\r\nframe #7: <unknown function> + 0xdc253 (0x7ff617ab0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\r\nframe #8: <unknown function> + 0x94ac3 (0x7ff623912ac3 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #9: clone + 0x44 (0x7ff6239a3a04 in /lib/x86_64-linux-gnu/libc.so.6)\r\n\r\nException raised from ncclCommWatchdog at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1339 (most recent call first):\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x99 (0x7ff617f9bdc9 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\r\nframe #1: <unknown function> + 0xf6f04e (0x7ff5b6de604e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\r\nframe #2: <unknown function> + 0xca016a (0x7ff5b6b1716a in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)\r\nframe #3: <unknown function> + 0xdc253 (0x7ff617ab0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\r\nframe #4: <unknown function> + 0x94ac3 (0x7ff623912ac3 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #5: clone + 0x44 (0x7ff6239a3a04 in /lib/x86_64-linux-gnu/libc.so.6)\r\n\r\n/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\nAborted\r\n```\r\n\r\nSeems to only happen when batch_size=128, the flash_attn backend `output[num_prefill_tokens:] = flash_attn_with_kvcache(...)` error, haven't figured out the cause of the error yet..\r\n",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-25T07:28:58+00:00",
    "closed_at": "2025-05-17T02:09:45+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6776/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/6776"
  },
  {
    "number": 8477,
    "title": "[Bug]: : CPU silently doesn't support multi-step (--num-scheduler-steps)",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nINFO 09-13 19:13:45 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\r\nPyTorch version: 2.4.0+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: Could not collect\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-4.18.0-372.46.1.el8_6.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: False\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-80GB\r\n  MIG 3g.40gb     Device  0:\r\n\r\nNvidia driver version: 535.104.05\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          80\r\nOn-line CPU(s) list:             0-79\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel Xeon Processor (Icelake)\r\nCPU family:                      6\r\nModel:                           134\r\nThread(s) per core:              2\r\nCore(s) per socket:              20\r\nSocket(s):                       2\r\nStepping:                        0\r\nBogoMIPS:                        5600.03\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd arat avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm md_clear arch_capabilities\r\nVirtualization:                  VT-x\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       2.5 MiB (80 instances)\r\nL1i cache:                       2.5 MiB (80 instances)\r\nL2 cache:                        160 MiB (40 instances)\r\nL3 cache:                        32 MiB (2 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-39\r\nNUMA node1 CPU(s):               40-79\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] intel_extension_for_pytorch==2.4.0+gitfbaa4bc\r\n[pip3] numpy==1.26.4\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0+cpu\r\n[pip3] torchvision==0.19.0+cpu\r\n[pip3] transformers==4.44.2\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.0@36bf8150cc3a048d69d9d2196128462014b9599d\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\n\u001b[4mGPU0\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\r\nGPU0\t X \tSYS\t40-79\t1\t\tN/A\r\nNIC0\tSYS\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nI tested the following script running on a CPU backend which set num_scheduler_steps > 1 to force use mult-step:\r\n\r\n```python\r\nfrom vllm import LLM, SamplingParams\r\n\r\nllm = LLM(model=\"facebook/opt-125M\", \r\n          gpu_memory_utilization=0.4, \r\n          max_model_len=1024,  \r\n          num_scheduler_steps=8\r\n          )\r\n\r\n\r\nparams = SamplingParams(seed=123,  prompt_logprobs=5, temperature=1)\r\n\r\nprompts = [\"How to make pizza?\"]\r\noutputs = llm.generate(prompts, sampling_params=params )\r\n\r\nfor o in outputs:\r\n    print('_________')\r\n    print('### Text')\r\n    print('_________')\r\n    for o2 in o.outputs:\r\n        print(o2.text)\r\n\r\n```\r\n\r\nGot the following output:\r\n```\r\nINFO 09-13 19:33:10 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\r\nWARNING 09-13 19:33:13 arg_utils.py:902] Enabled BlockSpaceManagerV2 because it is required for multi-step (--num-scheduler-steps > 1)\r\nWARNING 09-13 19:33:13 config.py:370] Async output processing is only supported for CUDA or TPU. Disabling it for other platforms.\r\nINFO 09-13 19:33:13 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='facebook/opt-125M', speculative_config=None, tokenizer='facebook/opt-125M', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=facebook/opt-125M, use_v2_block_manager=True, num_scheduler_steps=8, enable_prefix_caching=False, use_async_output_proc=False)\r\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\r\n  warnings.warn(\r\nWARNING 09-13 19:33:14 cpu_executor.py:321] float16 is not supported on CPU, casting to bfloat16.\r\nWARNING 09-13 19:33:14 cpu_executor.py:324] CUDA graph is not supported on CPU, fallback to the eager mode.\r\nWARNING 09-13 19:33:14 cpu_executor.py:350] Environment variable VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by default.\r\nINFO 09-13 19:33:14 selector.py:183] Cannot use _Backend.FLASH_ATTN backend on CPU.\r\nINFO 09-13 19:33:14 selector.py:128] Using Torch SDPA backend.\r\nINFO 09-13 19:33:15 selector.py:183] Cannot use _Backend.FLASH_ATTN backend on CPU.\r\nINFO 09-13 19:33:15 selector.py:128] Using Torch SDPA backend.\r\nINFO 09-13 19:33:15 weight_utils.py:235] Using model weights format ['*.bin']\r\nLoading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/weight_utils.py:417: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n  state = torch.load(bin_file, map_location=\"cpu\")\r\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.83it/s]\r\nLoading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.82it/s]\r\n\r\nINFO 09-13 19:33:16 cpu_executor.py:208] # CPU blocks: 7281\r\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:03<00:00,  3.86s/it, est. speed input: 1.55 toks/s, output: 4.14 toks/s]\r\n_________\r\n### Text\r\n_________\r\n\r\n\r\nMethod\r\n\r\n```\r\n\r\nHowever accordingly to #8198 prompt logs causes vllm to crash when using multi-step. Which does not happen in the above log. This was actually a dummy way to check if the feature is actually active. Moreover checking the code, CPU has several specific classes that make a parallel implementation for CPU backend and it looks like it is not using the parameters of multi-step scheduling. There is also no warning in the log that inform the feature is not workig.\r\n\r\n## Expectation\r\n\r\nAdd a checking in the code to raise an exception or warning to inform the user that the feature is not supported.\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-13T19:55:13+00:00",
    "closed_at": "2025-01-13T02:03:03+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8477/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/8477"
  },
  {
    "number": 14601,
    "title": "[Bug]: torch.ops._C.cutlass_scaled_mm RuntimeError: Error Internal while use L20 PP=3 + TP=8 for R1",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nYour output of `python collect_env.py` here\nINFO 03-11 15:29:14 [__init__.py:256] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: 14.0.0-1ubuntu1.1\nCMake version: version 3.29.2\nLibc version: glibc-2.35\n\nPython version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-4.18.0-348.7.1.el8_5.x86_64-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.6.85\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: NVIDIA L20\nGPU 1: NVIDIA L20\nGPU 2: NVIDIA L20\nGPU 3: NVIDIA L20\nGPU 4: NVIDIA L20\nGPU 5: NVIDIA L20\nGPU 6: NVIDIA L20\nGPU 7: NVIDIA L20\n\nNvidia driver version: 550.54.15\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   52 bits physical, 57 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          128\nOn-line CPU(s) list:             0-127\nVendor ID:                       GenuineIntel\nBIOS Vendor ID:                  Intel(R) Corporation\nModel name:                      Intel(R) Xeon(R) Gold 6430\nBIOS Model name:                 Intel(R) Xeon(R) Gold 6430\nCPU family:                      6\nModel:                           143\nThread(s) per core:              2\nCore(s) per socket:              32\nSocket(s):                       2\nStepping:                        8\nFrequency boost:                 enabled\nCPU max MHz:                     2101.0000\nCPU min MHz:                     800.0000\nBogoMIPS:                        4200.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust sgx bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd sgx_lc fsrm md_clear serialize tsxldtrk pconfig arch_lbr avx512_fp16 flush_l1d arch_capabilities\nVirtualization:                  VT-x\nL1d cache:                       3 MiB (64 instances)\nL1i cache:                       2 MiB (64 instances)\nL2 cache:                        128 MiB (64 instances)\nL3 cache:                        120 MiB (2 instances)\nNUMA node(s):                    2\nNUMA node0 CPU(s):               0-31,64-95\nNUMA node1 CPU(s):               32-63,96-127\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Spec store bypass: Vulnerable\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\n\nVersions of relevant libraries:\n[pip3] flashinfer==0.1.6+cu124torch2.4\n[pip3] mypy==1.9.0\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-modelopt==0.19.0\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] onnx==1.16.1\n[pip3] onnx-graphsurgeon==0.5.2\n[pip3] onnxconverter-common==1.14.0\n[pip3] onnxmltools==1.12.0\n[pip3] onnxruntime_extensions==0.11.0\n[pip3] onnxruntime-gpu==1.20.0\n[pip3] pynvml==11.5.2\n[pip3] pytorch-fid==0.3.0\n[pip3] pytorch-lightning==1.9.4\n[pip3] pyzmq==26.0.3\n[pip3] sentence-transformers==3.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchmetrics==1.4.0.post0\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.2\n[pip3] transformers-stream-generator==0.0.5\n[pip3] triton==3.1.0\n[pip3] tritonclient==2.51.0\n[pip3] vector-quantize-pytorch==1.21.2\n[conda] flashinfer                0.1.6+cu124torch2.4          pypi_0    pypi\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\n[conda] nvidia-modelopt           0.19.0                   pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pynvml                    11.5.2                   pypi_0    pypi\n[conda] pytorch-fid               0.3.0                    pypi_0    pypi\n[conda] pytorch-lightning         1.9.4                    pypi_0    pypi\n[conda] pyzmq                     26.0.3                   pypi_0    pypi\n[conda] sentence-transformers     3.2.1                    pypi_0    pypi\n[conda] torch                     2.5.1                    pypi_0    pypi\n[conda] torchaudio                2.5.1                    pypi_0    pypi\n[conda] torchmetrics              1.4.0.post0              pypi_0    pypi\n[conda] torchvision               0.20.1                   pypi_0    pypi\n[conda] transformers              4.48.2                   pypi_0    pypi\n[conda] transformers-stream-generator 0.0.5                    pypi_0    pypi\n[conda] triton                    3.1.0                    pypi_0    pypi\n[conda] tritonclient              2.51.0                   pypi_0    pypi\n[conda] vector-quantize-pytorch   1.21.2                   pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.4.dev382+g5ce6dedf4\nvLLM Build Flags:\nCUDA Archs: Ada; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU1\tPIX\t X \tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU2\tPIX\tPIX\t X \tPIX\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU3\tPIX\tPIX\tPIX\t X \tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\nGPU4\tSYS\tSYS\tSYS\tSYS\t X \tPIX\tPIX\tPIX\tSYS\tSYS\t32-63,96-127\t1\t\tN/A\nGPU5\tSYS\tSYS\tSYS\tSYS\tPIX\t X \tPIX\tPIX\tSYS\tSYS\t32-63,96-127\t1\t\tN/A\nGPU6\tSYS\tSYS\tSYS\tSYS\tPIX\tPIX\t X \tPIX\tSYS\tSYS\t32-63,96-127\t1\t\tN/A\nGPU7\tSYS\tSYS\tSYS\tSYS\tPIX\tPIX\tPIX\t X \tSYS\tSYS\t32-63,96-127\t1\t\tN/A\nNIC0\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPIX\nNIC1\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPIX\t X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n\nCUDA_VERSION=12.3.2.001\nCUDA_DRIVER_VERSION=545.23.08\nCUDA_CACHE_DISABLE=1\nNVIDIA_REQUIRE_JETPACK_HOST_MOUNTS=\nNVIDIA_REQUIRE_CUDA=cuda>=9.0\nNCCL_VERSION=2.19.stable.20231214+cuda12.3\nCUBLAS_VERSION=12.3.4.1\nCUDNN_VERSION=9.0.0.306\nLD_LIBRARY_PATH=/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/nvidia/nvjitlink/lib:/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/torch/lib/:/opt/hpcx/ucx/lib:/opt/hpcx/ompi/lib:/usr/lib/x86_64-linux-gnu/:/usr/local/lib/python3.10/dist-packages/torch/lib:/usr/local/cuda/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nNVIDIA_VISIBLE_DEVICES=all\nNVIDIA_DRIVER_CAPABILITIES=compute,utility,video\nNVIDIA_PRODUCT_NAME=PyTorch\nPYTORCH_BUILD_VERSION=2.3.0a0+ebedce2\nPYTORCH_VERSION=2.3.0a0+ebedce2\nPYTORCH_BUILD_NUMBER=0\nNVIDIA_PYTORCH_VERSION=24.02\nTORCH_CUDA_ARCH_LIST=Ada\nPYTORCH_HOME=/opt/pytorch/pytorch\nCUDA_HOME=/usr/local/cuda\nCUDA_HOME=/usr/local/cuda\nTORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1\nCUDA_MODULE_LOADING=LAZY\nNVIDIA_BUILD_ID=82611821\nCUDA_VER=12.3\nCUDNN_VER=8.9.7.29-1+cuda12.2\nNCCL_VER=2.19.3-1+cuda12.3\nCUBLAS_VER=12.3.4.1-1\nNCCL_SOCKET_IFNAME=eth0\nNCCL_IB_DISABLE=1\nNCCL_IBEXT_DISABLE=1\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\nCUDA_LAUNCH_BLOCKING=1\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\n- cmd:\n\n```bash\nnohup python3 -m vllm.entrypoints.openai.api_server \\\n        --model=/workspace/dev/hf_models/DeepSeek-R1 \\\n        --dtype=auto \\\n        --max-model-len 32768 \\\n        --tensor-parallel-size 8 \\\n        --pipeline-parallel-size 3 \\\n        --tokenizer-mode auto \\\n        --gpu-memory-utilization 0.90 \\\n        --max-num-seqs 48 \\\n        --trust-remote-code \\\n        --no-enable-prefix-caching \\\n        --enable-chunked-prefill=False \\\n        --enforce-eager \\\n        --disable-custom-all-reduce \\\n        --port 8862 > vllm.R1.log.1 2>&1 &\n```\n\n- Error:\n```bash\nLoading safetensors checkpoint shards:  96% Completed | 157/163 [01:22<00:03,  1.89it/s]\nLoading safetensors checkpoint shards:  97% Completed | 158/163 [01:23<00:03,  1.30it/s]\nLoading safetensors checkpoint shards:  98% Completed | 159/163 [01:24<00:03,  1.07it/s]\nLoading safetensors checkpoint shards:  98% Completed | 160/163 [01:26<00:03,  1.07s/it]\nLoading safetensors checkpoint shards:  99% Completed | 161/163 [01:26<00:01,  1.23it/s]\nLoading safetensors checkpoint shards: 100% Completed | 163/163 [01:26<00:00,  2.04it/s]\nLoading safetensors checkpoint shards: 100% Completed | 163/163 [01:26<00:00,  1.88it/s]\n\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]     self.model_runner.load_model() [repeated 4x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]     self.model = get_model(vllm_config=self.vllm_config) [repeated 4x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model [repeated 4x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]     return loader.load_model(vllm_config=vllm_config) [repeated 4x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]     _process_weights_after_loading(model, model_config, target_device) [repeated 4x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 178, in _process_weights_after_loading [repeated 4x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]     module.process_weights_after_loading(model_config.dtype) [repeated 4x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/attention/backends/mla/common.py\", line 1174, in process_weights_after_loading [repeated 8x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]     self.impl.process_weights_after_loading(act_dtype) [repeated 4x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]     kv_b_proj_weight = get_and_maybe_dequant_weights(self.kv_b_proj).T [repeated 4x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/attention/backends/mla/common.py\", line 1162, in get_and_maybe_dequant_weights [repeated 4x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]     dequant_weights = layer.quant_method.apply(layer, [repeated 4x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/model_executor/layers/quantization/fp8.py\", line 377, in apply [repeated 4x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]     return torch.ops.vllm.apply_w8a8_block_fp8_linear( [repeated 4x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/torch/_ops.py\", line 1116, in __call__ [repeated 8x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]     return self._op(*args, **(kwargs or {})) [repeated 8x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/model_executor/layers/quantization/utils/fp8_utils.py\", line 69, in apply_w8a8_block_fp8_linear [repeated 4x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]     output = ops.cutlass_scaled_mm(q_input, [repeated 4x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/_custom_ops.py\", line 519, in cutlass_scaled_mm [repeated 4x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620]     torch.ops._C.cutlass_scaled_mm(out, a, b, scale_a, scale_b, bias) [repeated 4x across cluster]\n(RayWorkerWrapper pid=91931, ip=10.189.110.50) ERROR 03-11 15:24:45 [worker_base.py:620] RuntimeError: Error Internal [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) INFO 03-11 15:24:46 [loader.py:429] Loading weights took 52.44 seconds [repeated 6x across cluster]\nINFO 03-11 15:25:20 [loader.py:429] Loading weights took 86.75 seconds\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620] Error executing method 'load_model'. This might cause deadlock in distributed execution. [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620] Traceback (most recent call last): [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 612, in execute_method [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]     return run_method(self, method, args, kwargs) [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/utils.py\", line 2238, in run_method [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]     return func(*args, **kwargs) [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 442, in load_model [repeated 12x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]     self.model_runner.load_model() [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]     self.model = get_model(vllm_config=self.vllm_config) [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]     return loader.load_model(vllm_config=vllm_config) [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]     _process_weights_after_loading(model, model_config, target_device) [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 178, in _process_weights_after_loading [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]     module.process_weights_after_loading(model_config.dtype) [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/attention/backends/mla/common.py\", line 1174, in process_weights_after_loading [repeated 8x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]     self.impl.process_weights_after_loading(act_dtype) [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]     kv_b_proj_weight = get_and_maybe_dequant_weights(self.kv_b_proj).T [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/attention/backends/mla/common.py\", line 1162, in get_and_maybe_dequant_weights [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]     dequant_weights = layer.quant_method.apply(layer, [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/model_executor/layers/quantization/fp8.py\", line 377, in apply [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]     return torch.ops.vllm.apply_w8a8_block_fp8_linear( [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/torch/_ops.py\", line 1116, in __call__ [repeated 8x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]     return self._op(*args, **(kwargs or {})) [repeated 8x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/model_executor/layers/quantization/utils/fp8_utils.py\", line 69, in apply_w8a8_block_fp8_linear [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]     output = ops.cutlass_scaled_mm(q_input, [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/_custom_ops.py\", line 519, in cutlass_scaled_mm [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620]     torch.ops._C.cutlass_scaled_mm(out, a, b, scale_a, scale_b, bias) [repeated 4x across cluster]\n(RayWorkerWrapper pid=91936, ip=10.189.110.50) ERROR 03-11 15:24:48 [worker_base.py:620] RuntimeError: Error Internal [repeated 4x across cluster]\n(RayWorkerWrapper pid=3397964) INFO 03-11 15:25:20 [loader.py:429] Loading weights took 86.79 seconds\n(RayWorkerWrapper pid=3397961) INFO 03-11 15:25:20 [loader.py:429] Loading weights took 86.77 seconds\n(RayWorkerWrapper pid=3397964) INFO 03-11 15:25:22 [model_runner.py:1146] Model loading took 25.0166 GB and 88.446320 seconds\nINFO 03-11 15:25:22 [model_runner.py:1146] Model loading took 25.0166 GB and 88.460629 seconds\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n[rank0]:     return _run_code(code, main_globals, None,\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/runpy.py\", line 86, in _run_code\n[rank0]:     exec(code, run_globals)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 992, in <module>\n[rank0]:     uvloop.run(run_server(args))\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/uvloop/__init__.py\", line 82, in run\n[rank0]:     return loop.run_until_complete(wrapper())\n[rank0]:   File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/uvloop/__init__.py\", line 61, in wrapper\n[rank0]:     return await main\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 947, in run_server\n[rank0]:     async with build_async_engine_client(args) as engine_client:\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/contextlib.py\", line 199, in __aenter__\n[rank0]:     return await anext(self.gen)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 139, in build_async_engine_client\n[rank0]:     async with build_async_engine_client_from_engine_args(\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/contextlib.py\", line 199, in __aenter__\n[rank0]:     return await anext(self.gen)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 163, in build_async_engine_client_from_engine_args\n[rank0]:     engine_client = AsyncLLMEngine.from_engine_args(\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 649, in from_engine_args\n[rank0]:     engine = cls(\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 599, in __init__\n[rank0]:     self.engine = self._engine_class(*args, **kwargs)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 267, in __init__\n[rank0]:     super().__init__(*args, **kwargs)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 274, in __init__\n[rank0]:     self.model_executor = executor_class(vllm_config=vllm_config, )\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 271, in __init__\n[rank0]:     super().__init__(*args, **kwargs)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n[rank0]:     self._init_executor()\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/executor/ray_distributed_executor.py\", line 114, in _init_executor\n[rank0]:     self._init_workers_ray(placement_group)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/executor/ray_distributed_executor.py\", line 394, in _init_workers_ray\n[rank0]:     self._run_workers(\"load_model\",\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/executor/ray_distributed_executor.py\", line 519, in _run_workers\n[rank0]:     ray_worker_outputs = ray.get(ray_worker_outputs)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n[rank0]:     return fn(*args, **kwargs)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n[rank0]:     return func(*args, **kwargs)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/ray/_private/worker.py\", line 2771, in get\n[rank0]:     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/ray/_private/worker.py\", line 919, in get_objects\n[rank0]:     raise value.as_instanceof_cause()\n[rank0]: ray.exceptions.RayTaskError(RuntimeError): ray::RayWorkerWrapper.execute_method() (pid=152762, ip=10.189.109.87, actor_id=ecea625a8ff86b1a1459f09106000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x7eea18184340>)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 621, in execute_method\n[rank0]:     raise e\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 612, in execute_method\n[rank0]:     return run_method(self, method, args, kwargs)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/utils.py\", line 2238, in run_method\n[rank0]:     return func(*args, **kwargs)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/worker/worker.py\", line 183, in load_model\n[rank0]:     self.model_runner.load_model()\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1113, in load_model\n[rank0]:     self.model = get_model(vllm_config=self.vllm_config)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n[rank0]:     return loader.load_model(vllm_config=vllm_config)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 442, in load_model\n[rank0]:     _process_weights_after_loading(model, model_config, target_device)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 178, in _process_weights_after_loading\n[rank0]:     module.process_weights_after_loading(model_config.dtype)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/attention/layer.py\", line 246, in process_weights_after_loading\n[rank0]:     self.impl.process_weights_after_loading(act_dtype)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/attention/backends/mla/common.py\", line 1174, in process_weights_after_loading\n[rank0]:     kv_b_proj_weight = get_and_maybe_dequant_weights(self.kv_b_proj).T\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/attention/backends/mla/common.py\", line 1162, in get_and_maybe_dequant_weights\n[rank0]:     dequant_weights = layer.quant_method.apply(layer,\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/model_executor/layers/quantization/fp8.py\", line 377, in apply\n[rank0]:     return torch.ops.vllm.apply_w8a8_block_fp8_linear(\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/torch/_ops.py\", line 1116, in __call__\n[rank0]:     return self._op(*args, **(kwargs or {}))\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/model_executor/layers/quantization/utils/fp8_utils.py\", line 69, in apply_w8a8_block_fp8_linear\n[rank0]:     output = ops.cutlass_scaled_mm(q_input,\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/vllm/_custom_ops.py\", line 519, in cutlass_scaled_mm\n[rank0]:     torch.ops._C.cutlass_scaled_mm(out, a, b, scale_a, scale_b, bias)\n[rank0]:   File \"/root/anaconda3/envs/trtllm/lib/python3.10/site-packages/torch/_ops.py\", line 1116, in __call__\n[rank0]:     return self._op(*args, **(kwargs or {}))\n[rank0]: RuntimeError: Error Internal\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-11T07:34:43+00:00",
    "closed_at": "2025-03-11T12:16:39+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14601/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/14601"
  },
  {
    "number": 4967,
    "title": "[Feature]: support `stream_options` option",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nAccording to openAI doc: https://platform.openai.com/docs/api-reference/chat/create#chat-create-stream_options. The API provide the stream_options which can get token usage info for stream request. \n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_",
    "labels": [
      "good first issue",
      "feature request"
    ],
    "state": "closed",
    "created_at": "2024-05-22T02:44:58+00:00",
    "closed_at": "2024-06-07T03:29:26+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4967/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/4967"
  },
  {
    "number": 15435,
    "title": "[Installation]: Fail to build vLLM from source on CUDA 12.6",
    "body": "### Your current environment\n\n```text\nINFO 03-24 20:48:52 [__init__.py:239] Automatically detected platform cuda.\nCollecting environment information...\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: CentOS Stream 9 (x86_64)\nGCC version: (GCC) 11.5.0 20240719 (Red Hat 11.5.0-5)\nClang version: Could not collect\nCMake version: version 3.31.4\nLibc version: glibc-2.34\n\nPython version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-6.4.3-0_fbk14_hardened_2601_gcd42476b84e9-x86_64-with-glibc2.34\nIs CUDA available: True\nCUDA runtime version: 12.6.85\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: NVIDIA H100\nGPU 1: NVIDIA H100\nGPU 2: NVIDIA H100\nGPU 3: NVIDIA H100\nGPU 4: NVIDIA H100\nGPU 5: NVIDIA H100\nGPU 6: NVIDIA H100\nGPU 7: NVIDIA H100\n\nNvidia driver version: 550.90.07\ncuDNN version: Probably one of the following:\n/usr/lib64/libcudnn.so.9.6.0\n/usr/lib64/libcudnn_adv.so.9.6.0\n/usr/lib64/libcudnn_cnn.so.9.6.0\n/usr/lib64/libcudnn_engines_precompiled.so.9.6.0\n/usr/lib64/libcudnn_engines_runtime_compiled.so.9.6.0\n/usr/lib64/libcudnn_graph.so.9.6.0\n/usr/lib64/libcudnn_heuristic.so.9.6.0\n/usr/lib64/libcudnn_ops.so.9.6.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      52 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             368\nOn-line CPU(s) list:                0-367\nVendor ID:                          AuthenticAMD\nModel name:                         AMD EPYC 9654 96-Core Processor\nCPU family:                         25\nModel:                              17\nThread(s) per core:                 1\nCore(s) per socket:                 368\nSocket(s):                          1\nStepping:                           1\nBogoMIPS:                           4792.78\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm rep_good nopl cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr wbnoinvd arat npt lbrv nrip_save tsc_scale vmcb_clean pausefilter pfthreshold v_vmsave_vmload vgif vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm flush_l1d arch_capabilities\nVirtualization:                     AMD-V\nHypervisor vendor:                  KVM\nVirtualization type:                full\nL1d cache:                          23 MiB (368 instances)\nL1i cache:                          23 MiB (368 instances)\nL2 cache:                           184 MiB (368 instances)\nL3 cache:                           16 MiB (1 instance)\nNUMA node(s):                       1\nNUMA node0 CPU(s):                  0-367\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec store bypass:    Vulnerable\nVulnerability Spectre v1:           Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\nVulnerability Spectre v2:           Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pynvml==12.0.0\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.48.3\n[pip3] triton==3.2.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n[conda] nvidia-ml-py              12.570.86                pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pynvml                    12.0.0                   pypi_0    pypi\n[conda] pyzmq                     26.2.1                   pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi\n[conda] torchaudio                2.6.0                    pypi_0    pypi\n[conda] torchvision               0.21.0                   pypi_0    pypi\n[conda] transformers              4.48.3                   pypi_0    pypi\n[conda] triton                    3.2.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3.dev28+g97cfa65d\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    0-367   0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    0-367   0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    0-367   0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    0-367   0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    0-367   0               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    0-367   0               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    0-367   0               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      0-367   0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nCUDA_CACHE_PATH=/data/users/huydo/.nv/ComputeCache\nLD_LIBRARY_PATH=/usr/local/cuda-12.6/lib64/:\nCUDA_HOME=/usr/local/cuda\nCUDA_HOME=/usr/local/cuda\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n### How you are installing vllm\n\n```sh\npip install --editable .\n```\n\nAfter https://github.com/vllm-project/vllm/pull/14570, building vLLM from source starts to fail for me on H100.  Here is the build error\n\n```\n-- CUDA target architectures: 9.0\n-- CUDA supported target architectures: 9.0\n-- FetchContent base directory: /home/huydo/github/pytorch-integration-testing/vllm-benchmarks/vllm/.deps\n-- Enabling cumem allocator extension.\n-- CMake Version: 3.31.4\n-- CUTLASS 3.8.0\n-- Found CUDAToolkit: /usr/local/cuda-12.6/targets/x86_64-linux/include (found version \"12.6.85\")\n-- CUDART: /usr/local/cuda-12.6/lib64/libcudart.so\n-- CUDA Driver: /usr/local/cuda-12.6/lib64/stubs/libcuda.so\n-- NVRTC: /usr/local/cuda-12.6/lib64/libnvrtc.so\n-- Default Install Location: install\n-- CUDA Compilation Architectures: 70;72;75;80;86;87;89;90;90a\n-- Enable caching of reference results in conv unit tests\n-- Enable rigorous conv problem sizes in conv unit tests\n-- Using the following NVCC flags:\n  --expt-relaxed-constexpr\n  -DCUTLASS_TEST_LEVEL=0\n  -DCUTLASS_TEST_ENABLE_CACHED_RESULTS=1\n  -DCUTLASS_CONV_UNIT_TEST_RIGOROUS_SIZE_ENABLED=1\n  -DCUTLASS_DEBUG_TRACE_LEVEL=0\n  -Xcompiler=-Wconversion\n  -Xcompiler=-fno-strict-aliasing\n  -lineinfo\n-- Configuring cublas ...\n-- cuBLAS Disabled.\n-- Configuring cuBLAS ... done.\n-- Building Marlin kernels for archs: 9.0\n-- Not building AllSpark kernels as no compatible archs found in CUDA target architectures\n-- Building scaled_mm_c3x_sm90 for archs: 9.0a\n-- Not building scaled_mm_c3x_100 as no compatible archs found in CUDA target architectures\n-- Building scaled_mm_c2x for archs: 9.0\n-- Building sparse_scaled_mm_c3x for archs: 9.0a\n-- Not building NVFP4 as no compatible archs were found.\n-- Machete generation script hash: dec2c6596ac38e4b4ac06b8d7ca5054f\n-- Last run machete generate script hash: dec2c6596ac38e4b4ac06b8d7ca5054f\n-- Machete generation script has not changed, skipping generation.\n-- Building Machete kernels for archs: 9.0a\n-- Enabling C extension.\n-- Building Marlin MOE kernels for archs: 9.0\n-- Enabling moe extension.\nCMake Warning (dev) at /home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/cmake/data/share/cmake-3.31/Modules/FetchContent.cmake:1564 (cmake_parse_arguments):\n  The BUILD_COMMAND keyword was followed by an empty string or no value at\n  all.  Policy CMP0174 is not set, so cmake_parse_arguments() will unset the\n  ARG_BUILD_COMMAND variable rather than setting it to an empty string.\nCall Stack (most recent call first):\n  /home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/cmake/data/share/cmake-3.31/Modules/FetchContent.cmake:2145:EVAL:2 (__FetchContent_doPopulation)\n  /home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/cmake/data/share/cmake-3.31/Modules/FetchContent.cmake:2145 (cmake_language)\n  /home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/cmake/data/share/cmake-3.31/Modules/FetchContent.cmake:2384 (__FetchContent_Populate)\n  cmake/external_projects/flashmla.cmake:30 (FetchContent_MakeAvailable)\n  CMakeLists.txt:637 (include)\nThis warning is for project developers.  Use -Wno-dev to suppress it.\n\nCMake Warning (dev) at /home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/cmake/data/share/cmake-3.31/Modules/FetchContent.cmake:1564 (cmake_parse_arguments):\n  The CONFIGURE_COMMAND keyword was followed by an empty string or no value\n  at all.  Policy CMP0174 is not set, so cmake_parse_arguments() will unset\n  the ARG_CONFIGURE_COMMAND variable rather than setting it to an empty\n  string.\nCall Stack (most recent call first):\n  /home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/cmake/data/share/cmake-3.31/Modules/FetchContent.cmake:2145:EVAL:2 (__FetchContent_doPopulation)\n  /home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/cmake/data/share/cmake-3.31/Modules/FetchContent.cmake:2145 (cmake_language)\n  /home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/cmake/data/share/cmake-3.31/Modules/FetchContent.cmake:2384 (__FetchContent_Populate)\n  cmake/external_projects/flashmla.cmake:30 (FetchContent_MakeAvailable)\n  CMakeLists.txt:637 (include)\nThis warning is for project developers.  Use -Wno-dev to suppress it.\n\n-- FlashMLA is available at /home/huydo/github/pytorch-integration-testing/vllm-benchmarks/vllm/.deps/flashmla-src\n-- Build type: RelWithDebInfo\n-- Target device: cuda\n-- Found Python: /home/huydo/miniconda3/envs/py3.12/bin/python (found version \"3.12.9\") found components: Interpreter Development.Module Development.SABIModule\nCMake Warning at .deps/vllm-flash-attn-src/CMakeLists.txt:75 (message):\n  Pytorch version 2.4.0 expected for CUDA build, saw 2.6.0 instead.\n\n\n-- CUDA target architectures: 9.0\n-- CUDA supported target architectures: 9.0\n-- FA2_ARCHS: 9.0\n-- FA3_ARCHS: 9.0a\n-- vllm-flash-attn is available at /home/huydo/github/pytorch-integration-testing/vllm-benchmarks/vllm/.deps/vllm-flash-attn-src\n-- Configuring done (10.4s)\n-- Generating done (0.1s)\n-- Build files have been written to: /home/huydo/github/pytorch-integration-testing/vllm-benchmarks/vllm/build/temp.linux-x86_64-cpython-312\n[144/237] Building CUDA object vllm-flash-attn/CMakeFiles/_vllm_fa3_C.dir/hopper/instantiations/flash_fwd_hdimall_e4m3_paged_split_softcap_sm90.cu.o\nFAILED: vllm-flash-attn/CMakeFiles/_vllm_fa3_C.dir/hopper/instantiations/flash_fwd_hdimall_e4m3_paged_split_softcap_sm90.cu.o\nsccache /usr/local/cuda-12.6/bin/nvcc -forward-unknown-to-host-compiler -DFLASHATTENTION_DISABLE_BACKWARD -DFLASHATTENTION_DISABLE_DROPOUT -DFLASHATTENTION_DISABLE_PYBIND -DFLASHATTENTION_DISABLE_UNEVEN_K -DFLASHATTENTION_VARLEN_ONLY -DPy_LIMITED_API=3 -DTORCH_EXTENSION_NAME=_vllm_fa3_C -DUSE_C10D_GLOO -DUSE_C10D_NCCL -DUSE_DISTRIBUTED -DUSE_RPC -DUSE_TENSORPIPE -D_vllm_fa3_C_EXPORTS -I/home/huydo/github/pytorch-integration-testing/vllm-benchmarks/vllm/.deps/vllm-flash-attn-src/csrc -I/home/huydo/github/pytorch-integration-testing/vllm-benchmarks/vllm/.deps/vllm-flash-attn-src/hopper -I/home/huydo/github/pytorch-integration-testing/vllm-benchmarks/vllm/.deps/vllm-flash-attn-src/csrc/common -I/home/huydo/github/pytorch-integration-testing/vllm-benchmarks/vllm/.deps/vllm-flash-attn-src/csrc/cutlass/include -isystem /home/huydo/miniconda3/envs/py3.12/include/python3.12 -isystem /home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/torch/include -isystem /home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda-12.6/include -DONNX_NAMESPACE=onnx_c2 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda -O3 -g -DNDEBUG -std=c++17 -Xcompiler=-fPIC --expt-relaxed-constexpr -DENABLE_FP8 --threads=1 --expt-extended-lambda --use_fast_math -DCUTLASS_ENABLE_DIRECT_CUDA_DRIVER_CALL=1 -D_GLIBCXX_USE_CXX11_ABI=0 -gencode arch=compute_90a,code=sm_90a -MD -MT vllm-flash-attn/CMakeFiles/_vllm_fa3_C.dir/hopper/instantiations/flash_fwd_hdimall_e4m3_paged_split_softcap_sm90.cu.o -MF vllm-flash-attn/CMakeFiles/_vllm_fa3_C.dir/hopper/instantiations/flash_fwd_hdimall_e4m3_paged_split_softcap_sm90.cu.o.d -x cu -c /home/huydo/github/pytorch-integration-testing/vllm-benchmarks/vllm/.deps/vllm-flash-attn-src/hopper/instantiations/flash_fwd_hdimall_e4m3_paged_split_softcap_sm90.cu -o vllm-flash-attn/CMakeFiles/_vllm_fa3_C.dir/hopper/instantiations/flash_fwd_hdimall_e4m3_paged_split_softcap_sm90.cu.o\nptxas terminated (signal: 11)sccache: Compiler killed by signal 11\n[235/237] Building CUDA object vllm-flash-attn/CMakeFiles/_vllm_fa3_C.dir/hopper/instantiations/flash_fwd_hdimall_e4m3_split_softcap_sm90.cu.o\nninja: build stopped: subcommand failed.\nTraceback (most recent call last):\n  File \"/home/huydo/github/pytorch-integration-testing/vllm-benchmarks/vllm/setup.py\", line 676, in <module>\n    setup(\n  File \"/home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/setuptools/__init__.py\", line 117, in setup\n    return distutils.core.setup(**attrs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 186, in setup\n    return run_commands(dist)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 202, in run_commands\n    dist.run_commands()\n  File \"/home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 983, in run_commands\n    self.run_command(cmd)\n  File \"/home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/setuptools/dist.py\", line 999, in run_command\n    super().run_command(command)\n  File \"/home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1002, in run_command\n    cmd_obj.run()\n  File \"/home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/setuptools/command/bdist_wheel.py\", line 379, in run\n    self.run_command(\"build\")\n  File \"/home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 339, in run_command\n    self.distribution.run_command(command)\n  File \"/home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/setuptools/dist.py\", line 999, in run_command\n    super().run_command(command)\n  File \"/home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1002, in run_command\n    cmd_obj.run()\n  File \"/home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/setuptools/_distutils/command/build.py\", line 136, in run\n    self.run_command(cmd_name)\n  File \"/home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 339, in run_command\n    self.distribution.run_command(command)\n  File \"/home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/setuptools/dist.py\", line 999, in run_command\n    super().run_command(command)\n  File \"/home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1002, in run_command\n    cmd_obj.run()\n  File \"/home/huydo/github/pytorch-integration-testing/vllm-benchmarks/vllm/setup.py\", line 267, in run\n    super().run()\n  File \"/home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/setuptools/command/build_ext.py\", line 99, in run\n    _build_ext.run(self)\n  File \"/home/huydo/miniconda3/envs/py3.12/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 365, in run\n    self.build_extensions()\n  File \"/home/huydo/github/pytorch-integration-testing/vllm-benchmarks/vllm/setup.py\", line 238, in build_extensions\n    subprocess.check_call([\"cmake\", *build_args], cwd=self.build_temp)\n  File \"/home/huydo/miniconda3/envs/py3.12/lib/python3.12/subprocess.py\", line 415, in check_call\n    raise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command '['cmake', '--build', '.', '-j=368', '--target=_moe_C', '--target=_vllm_fa2_C', '--target=_vllm_fa3_C', '--target=cumem_allocator', '--target=_C']' returned non-zero exit status 1.\n```\n\nReverting https://github.com/vllm-project/vllm/pull/14570 and the build works.  Please advise.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation"
    ],
    "state": "open",
    "created_at": "2025-03-25T03:51:53+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15435/reactions",
      "total_count": 3,
      "+1": 3,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/15435"
  },
  {
    "number": 2906,
    "title": "--tensor-parallel-size 2 fails to load on GCP",
    "body": "Hi,\r\nI am trying to set up vLLM Mixtral 8x7b on GCP. I have a VM with two A100 80GBs, and am using the following setup:\r\n\r\ndocker image: vllm/vllm-openai:v0.3.0\r\nModel: mistralai/Mixtral-8x7B-Instruct-v0.1\r\n\r\nCommand I use inside the vm:\r\n\r\npython3 -m vllm.entrypoints.openai.api_server --model mistralai/Mixtral-8x7B-Instruct-v0.1 --tensor-parallel-size 2 --port 8888\r\n\r\nOutput (after a while):\r\n\r\n```\r\nFile \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1858, in softmax\r\n    ret = input.softmax(dim, dtype=dtype)\r\nRuntimeError: CUDA error: invalid device function\r\n```\r\n\r\nnvidia-smi output:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA A100-SXM...  Off  | 00000000:00:06.0 Off |                    0 |\r\n| N/A   32C    P0    62W / 400W |      0MiB / 81920MiB |      0%      Default |\r\n|                               |                      |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  NVIDIA A100-SXM...  Off  | 00000000:00:07.0 Off |                    0 |\r\n| N/A   31C    P0    61W / 400W |      0MiB / 81920MiB |      0%      Default |\r\n|                               |                      |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\nWhat's wrong? Is this a bug in vLLM?\r\n\r\nAdditional diagnostics:\r\n- Mistral Instruct 7B fails with the same error\r\n- Without tensor parallelism, it succeeds. (Not an option for 8x7B as it doesn't fit on one GPU)\r\n\r\n\r\n\r\n",
    "labels": [
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-02-18T06:49:47+00:00",
    "closed_at": "2024-11-30T02:02:35+00:00",
    "comments": 10,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2906/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/2906"
  },
  {
    "number": 5177,
    "title": "[Bug]: Incorrect Example for the Inference with Prefix ",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.3\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-97-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100 80GB PCIe\r\nGPU 1: NVIDIA A100 80GB PCIe\r\nGPU 2: NVIDIA A100 80GB PCIe\r\nGPU 3: NVIDIA A100 80GB PCIe\r\n\r\nNvidia driver version: 535.161.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             48\r\nOn-line CPU(s) list:                0-47\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 7443 24-Core Processor\r\nCPU family:                         25\r\nModel:                              1\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 24\r\nSocket(s):                          2\r\nStepping:                           1\r\nFrequency boost:                    enabled\r\nCPU max MHz:                        4035,6440\r\nCPU min MHz:                        1500,0000\r\nBogoMIPS:                           5700.55\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca\r\nVirtualization:                     AMD-V\r\nL1d cache:                          1,5 MiB (48 instances)\r\nL1i cache:                          1,5 MiB (48 instances)\r\nL2 cache:                           24 MiB (48 instances)\r\nL3 cache:                           256 MiB (8 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-23\r\nNUMA node1 CPU(s):                  24-47\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; safe RET\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flake8==7.0.0\r\n[pip3] flake8-polyfill==1.0.2\r\n[pip3] mypy-protobuf==3.4.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] torchinfo==1.8.0\r\n[pip3] torchvision==0.18.0\r\n[pip3] triton==2.3.0\r\n[pip3] vllm_nccl_cu11==2.18.1.0.4.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.4.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.3\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NODE    SYS     SYS     PHB     PHB     SYS     SYS     0-23    0               N/A\r\nGPU1    NODE     X      SYS     SYS     NODE    NODE    SYS     SYS     0-23    0               N/A\r\nGPU2    SYS     SYS      X      NODE    SYS     SYS     NODE    NODE    24-47   1               N/A\r\nGPU3    SYS     SYS     NODE     X      SYS     SYS     NODE    NODE    24-47   1               N/A\r\nNIC0    PHB     NODE    SYS     SYS      X      PIX     SYS     SYS\r\nNIC1    PHB     NODE    SYS     SYS     PIX      X      SYS     SYS\r\nNIC2    SYS     SYS     NODE    NODE    SYS     SYS      X      PIX\r\nNIC3    SYS     SYS     NODE    NODE    SYS     SYS     PIX      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nCurrently existing contents of the `vllm/examples/offline_inference_with_prefix.py` have notable issues, introduced during the implementation of `--enable_prefix_caching` in #2762 and its fixes in subsequent PRs. Point of this issue is to display the detected issues and provide a solid ground for upcoming PR to fix them.\r\n\r\n---\r\n### 1. Missleading inference speed comparison\r\n**Intended use**: file structure suggests, that example should contain comparion of inference `w/o prefix_caching` vs `w/ prefix_caching`, comparing the model inference speed for those two options.  \r\n**Issue**: for the currently existing implementation, `llm` object is introduced once with `enable_prefix_caching=True`. After that, it is used for generation two times, and those generations are compared with each other.  \r\nAs a result, both generations apply `prefix hashing`, and **_resulting inference speed comparison is irrelevant_**. \r\n\r\n```python\r\n# [current state of a file]\r\nfrom vllm import LLM, SamplingParams\r\n\r\nprefix = ...\r\nprompts = [...]\r\n\r\nsampling_params = SamplingParams(temperature=0.0)\r\n\r\nllm = LLM(model=\"facebook/opt-125m\", enable_prefix_caching=True)\r\ngenerating_prompts = [prefix + prompt for prompt in prompts]\r\n\r\n# [intended generation w/o prefix_caching]\r\noutputs = llm.generate(generating_prompts, sampling_params)\r\n\r\n...\r\n# [intended generation w/ prefix_caching]\r\n\r\n# The llm.generate call will batch all prompts and send the batch at once\r\n# if resources allow. The prefix will only be cached after the first batch\r\n# is processed, so we need to call generate once to calculate the prefix\r\n# and cache it.\r\noutputs = llm.generate(generating_prompts[0], sampling_params)\r\n\r\n# Subsequent batches can leverage the cached prefix\r\noutputs = llm.generate(generating_prompts, sampling_params)\r\n\r\n```\r\n\r\n**Proposed solution**: create two llm objects, named `regular_llm` and `prefix_cached_llm`, that are `w/o prefix_caching` and `w/ prefix_caching` respectively. Then, run the generation using those two models, obtaining varying results.\r\n\r\n---\r\n### 2. Missleading information\r\n**Intended use**: file should cover usage details and explain how to use `enable_prefix_caching` parameter correctly.\r\n**Issue**: currently, file contents states that _\"prefix will only be cached after the first batch is processed\"_, however my tests disprove that statement, you could test that using the following minimal reproducing code snippet:\r\n```python\r\n# [my code to show that cache works on the fly]\r\nfrom vllm import LLM, SamplingParams\r\nfrom time import time\r\n\r\nprefix = (\r\n    \"You are an expert school principal, skilled in effectively managing \"\r\n    \"faculty and staff. Draft 10-15 questions for a potential first grade \"\r\n    \"Head Teacher for my K-12, all-girls', independent school that emphasizes \"\r\n    \"community, joyful discovery, and life-long learning. The candidate is \"\r\n    \"coming in for a first-round panel interview for a 8th grade Math \"\r\n    \"teaching role. They have 5 years of previous teaching experience \"\r\n    \"as an assistant teacher at a co-ed, public school with experience \"\r\n    \"in middle school math teaching. Based on these information, fulfill \"\r\n    \"the following paragraph: \")\r\n\r\nprompts = [\r\n    \"Hello, my name is\",\r\n    \"The president of the United States is\",\r\n    \"The capital of France is\",\r\n    \"The future of AI is\",\r\n]\r\n\r\ngenerating_prompts = [prefix + prompt for prompt in prompts]\r\n\r\nsampling_params = SamplingParams(temperature=0.0)\r\nregular_llm = LLM(model=\"facebook/opt-125m\", \r\n                  gpu_memory_utilization=0.4)\r\n\r\nprefix_cached_llm = LLM(model=\"facebook/opt-125m\", \r\n                        gpu_memory_utilization=0.4,\r\n                        enable_prefix_caching=True)\r\n\r\n# execution time \"without prefix caching\"\r\ns = time()\r\noutputs = regular_llm.generate(generating_prompts, sampling_params)\r\nslow_gen_time = time() - s\r\n\r\n# execution time when \"prefix is not cached yet\" \r\n# according to what is written in example file now\r\ns = time()\r\noutputs = prefix_cached_llm.generate(generating_prompts, sampling_params)\r\nfirst_gen_time = time() - s\r\n\r\n# execution time when \"prefix is already cached\" \r\ns = time()\r\noutputs = prefix_cached_llm.generate(generating_prompts, sampling_params)\r\nsecond_gen_time = time() - s\r\n\r\nprint(round(first_gen_time / second_gen_time, 2))\r\nprint(round(slow_gen_time / second_gen_time, 2))\r\n```\r\n\r\nFor my setup: \r\n* speedup for \"without prefix caching\" vs \"prefix is already cached\" is `1.3`, \r\n* speedup for \"prefix is already cached\" vs \"prefix is not cached yet\" is `1.01`.   \r\n\r\nAs there is no significant speedup for the second comparison above, I assume that prefix is cached on the fly during the first answer generation by the model. Therefore, the comment on the \"warmup\" is missleading and it should be deleted. \r\n**Proposed solution**: delete missleading comment on the \"warmup\" generation for prefix caching.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-06-01T13:06:28+00:00",
    "closed_at": "2024-06-01T22:53:53+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5177/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "CONTRIBUTOR",
    "html_url": "https://github.com/vllm-project/vllm/issues/5177"
  }
]
[
  {
    "number": 16054,
    "title": "[Bug]: CI flake - v1/engine/test_async_llm.py::test_abort - assert has_unfinished_requests()",
    "body": "### Your current environment\n\n...\n\n### \ud83d\udc1b Describe the bug\n\nmain commit 51d7c6a2b23e100cd9e7d85b8e7c0eea656b331e\n\nSeen in https://github.com/vllm-project/vllm/pull/15894\n\nhttps://buildkite.com/organizations/vllm/pipelines/ci/builds/16742/jobs/0195f24d-e81a-46a3-ad08-6a51983d65d6/log\n\n\n```\n=================================== FAILURES ===================================\n[2025-04-01T17:38:12Z] _ test_abort[engine_args0-Hello my name is Robert and-RequestOutputKind.DELTA] _\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z] monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fd1fa052e70>\n[2025-04-01T17:38:12Z] output_kind = <RequestOutputKind.DELTA: 1>\n[2025-04-01T17:38:12Z] engine_args = AsyncEngineArgs(model='meta-llama/Llama-3.2-1B-Instruct', served_model_name=None, tokenizer='meta-llama/Llama-3.2-1B-I...additional_config=None, enable_reasoning=None, reasoning_parser=None, use_tqdm_on_load=True, disable_log_requests=True)\n[2025-04-01T17:38:12Z] prompt = 'Hello my name is Robert and'\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]     @pytest.mark.parametrize(\n[2025-04-01T17:38:12Z]         \"output_kind\", [RequestOutputKind.DELTA, RequestOutputKind.FINAL_ONLY])\n[2025-04-01T17:38:12Z]     @pytest.mark.parametrize(\"engine_args,prompt\",\n[2025-04-01T17:38:12Z]                              [(TEXT_ENGINE_ARGS, TEXT_PROMPT),\n[2025-04-01T17:38:12Z]                               (VISION_ENGINE_ARGS, VISION_PROMPT)])\n[2025-04-01T17:38:12Z]     @pytest.mark.asyncio\n[2025-04-01T17:38:12Z]     async def test_abort(monkeypatch: pytest.MonkeyPatch,\n[2025-04-01T17:38:12Z]                          output_kind: RequestOutputKind,\n[2025-04-01T17:38:12Z]                          engine_args: AsyncEngineArgs, prompt: PromptType):\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]         with monkeypatch.context() as m, ExitStack() as after:\n[2025-04-01T17:38:12Z]             m.setenv(\"VLLM_USE_V1\", \"1\")\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             engine = AsyncLLM.from_engine_args(engine_args)\n[2025-04-01T17:38:12Z]             after.callback(engine.shutdown)\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             NUM_REQUESTS = 100\n[2025-04-01T17:38:12Z]             NUM_EXPECTED_TOKENS = 100\n[2025-04-01T17:38:12Z]             NUM_EXPECTED_TOKENS_LONG = 50000\n[2025-04-01T17:38:12Z]             REQUEST_IDS_TO_ABORT = range(1, 100, 10)\n[2025-04-01T17:38:12Z]             PARALLEL_SAMPLE_REQ_IDS = range(1, 100, 15)\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             request_ids = [f\"request-{i}\" for i in range(NUM_REQUESTS)]\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             # Create concurrent requests.\n[2025-04-01T17:38:12Z]             tasks: list[asyncio.Task] = []\n[2025-04-01T17:38:12Z]             for idx, request_id in enumerate(request_ids):\n[2025-04-01T17:38:12Z]                 max_tokens = NUM_EXPECTED_TOKENS_LONG if (\n[2025-04-01T17:38:12Z]                     idx in REQUEST_IDS_TO_ABORT) else NUM_EXPECTED_TOKENS\n[2025-04-01T17:38:12Z]                 n = 3 if idx in PARALLEL_SAMPLE_REQ_IDS else 1\n[2025-04-01T17:38:12Z]                 tasks.append(\n[2025-04-01T17:38:12Z]                     asyncio.create_task(\n[2025-04-01T17:38:12Z]                         generate(engine, request_id, prompt, output_kind,\n[2025-04-01T17:38:12Z]                                  max_tokens, n)))\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             # API server cancels requests when they disconnect.\n[2025-04-01T17:38:12Z]             for idx in REQUEST_IDS_TO_ABORT:\n[2025-04-01T17:38:12Z]                 tasks[idx].cancel()\n[2025-04-01T17:38:12Z]                 await asyncio.sleep(0.1)\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             # Confirm the other requests are okay.\n[2025-04-01T17:38:12Z]             for idx, task in enumerate(tasks):\n[2025-04-01T17:38:12Z]                 # Confirm that it was actually canceled.\n[2025-04-01T17:38:12Z]                 if idx in REQUEST_IDS_TO_ABORT:\n[2025-04-01T17:38:12Z]                     with pytest.raises(asyncio.CancelledError):\n[2025-04-01T17:38:12Z]                         await task\n[2025-04-01T17:38:12Z]                 else:\n[2025-04-01T17:38:12Z]                     # Otherwise, make sure the request was not impacted.\n[2025-04-01T17:38:12Z]                     num_generated_tokens, request_id = await task\n[2025-04-01T17:38:12Z]                     n = 3 if idx in PARALLEL_SAMPLE_REQ_IDS else 1\n[2025-04-01T17:38:12Z]                     expected_tokens = NUM_EXPECTED_TOKENS * n\n[2025-04-01T17:38:12Z]                     assert num_generated_tokens == expected_tokens, (\n[2025-04-01T17:38:12Z]                         f\"{request_id} generated {num_generated_tokens} but \"\n[2025-04-01T17:38:12Z]                         f\"expected {expected_tokens}\")\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z]             # Make sure all aborted requests were really aborted.\n[2025-04-01T17:38:12Z] >           assert not engine.output_processor.has_unfinished_requests()\n[2025-04-01T17:38:12Z] E           assert not True\n[2025-04-01T17:38:12Z] E            +  where True = has_unfinished_requests()\n[2025-04-01T17:38:12Z] E            +    where has_unfinished_requests = <vllm.v1.engine.output_processor.OutputProcessor object at 0x7fd1ef1614c0>.has_unfinished_requests\n[2025-04-01T17:38:12Z] E            +      where <vllm.v1.engine.output_processor.OutputProcessor object at 0x7fd1ef1614c0> = <vllm.v1.engine.async_llm.AsyncLLM object at 0x7fd1ef132750>.output_processor\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z] v1/engine/test_async_llm.py:178: AssertionError\n[2025-04-01T17:38:12Z] =============================== warnings summary ===============================\n[2025-04-01T17:38:12Z] tests/v1/engine/test_async_llm.py: 12 warnings\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core_client.py: 1 warning\n[2025-04-01T17:38:12Z] tests/v1/engine/test_llm_engine.py: 2 warnings\n[2025-04-01T17:38:12Z]   /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1700) is multi-threaded, use of fork() may lead to deadlocks in the child.\n[2025-04-01T17:38:12Z]     self.pid = os.fork()\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core.py::test_engine_core\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core.py::test_engine_core_advanced_sampling\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core.py::test_engine_core_concurrent_batches\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core_client.py::test_engine_core_client[True]\n[2025-04-01T17:38:12Z] tests/v1/engine/test_engine_core_client.py::test_engine_core_client[False]\n[2025-04-01T17:38:12Z]   /vllm-workspace/tests/utils.py:720: DeprecationWarning: This process (pid=1700) is multi-threaded, use of fork() may lead to deadlocks in the child.\n[2025-04-01T17:38:12Z]     pid = os.fork()\n[2025-04-01T17:38:12Z]\n[2025-04-01T17:38:12Z] -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n[2025-04-01T17:38:12Z] =========================== short test summary info ============================\n[2025-04-01T17:38:12Z] FAILED v1/engine/test_async_llm.py::test_abort[engine_args0-Hello my name is Robert and-RequestOutputKind.DELTA] - assert not True\n[2025-04-01T17:38:12Z]  +  where True = has_unfinished_requests()\n[2025-04-01T17:38:12Z]  +    where has_unfinished_requests = <vllm.v1.engine.output_processor.OutputProcessor object at 0x7fd1ef1614c0>.has_unfinished_requests\n[2025-04-01T17:38:12Z]  +      where <vllm.v1.engine.output_processor.OutputProcessor object at 0x7fd1ef1614c0> = <vllm.v1.engine.async_llm.AsyncLLM object at 0x7fd1ef132750>.output_processor\n[2025-04-01T17:38:12Z] ============ 1 failed, 44 passed, 20 warnings in 1059.59s (0:17:39) ============\n[2025-04-01T17:38:14Z] \ud83d\udea8 Error: The command exited with status 1\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci/build",
      "stale",
      "v1"
    ],
    "state": "open",
    "created_at": "2025-04-04T09:48:13+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16054/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/16054"
  },
  {
    "number": 10089,
    "title": "[help wanted]: website ui improvement",
    "body": "### Anything you want to discuss about vllm.\n\n<img width=\"236\" alt=\"image\" src=\"https://github.com/user-attachments/assets/c8a23dda-5fe5-4829-b369-a65917e29bec\">\r\n\r\ncurrently, \"ask AI\" and doc version selection are overlapped.\r\n\r\nwe should move doc version selection to the left.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-11-06T19:33:16+00:00",
    "closed_at": "2024-11-08T17:51:05+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/10089/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/10089"
  },
  {
    "number": 19964,
    "title": "[CI Failure]: Quantization Test - quantization/test_bitsandbytes.py::test_4bit_bnb_embedding_model",
    "body": "### Name of failing test\n\n`quantization/test_bitsandbytes.py::test_4bit_bnb_embedding_model[half-intfloat/e5-mistral-7b-instruct-quantize embedding model inflight]`\n\n### Basic information\n\n- [ ] Flaky test\n- [x] Can reproduce locally\n- [ ] Caused by external libraries (e.g. bug in `transformers`)\n\n### \ud83e\uddea Describe the failing test\n\n```\npytest -s -v \"quantization/test_bitsandbytes.py::test_4bit_bnb_embedding_model[half-intfloat/e5-mistral-7b-instruct-quantize embedding model inflight]\"\nINFO 06-23 04:48:10 [__init__.py:244] Automatically detected platform cuda.\n/home/mgoin/venvs/vllm/lib/python3.12/site-packages/pytest_asyncio/plugin.py:208: PytestDeprecationWarning: The configuration option \"asyncio_default_fixture_loop_scope\" is unset.\nThe event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))\n============================================================================================ test session starts =============================================================================================\nplatform linux -- Python 3.12.4, pytest-8.3.3, pluggy-1.5.0 -- /home/mgoin/venvs/vllm/bin/python3\ncachedir: .pytest_cache\nhypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/home/mgoin/code/vllm/tests/.hypothesis/examples'))\nrootdir: /home/mgoin/code/vllm\nconfigfile: pyproject.toml\nplugins: forked-1.6.0, subtests-0.14.1, asyncio-0.24.0, shard-0.1.2, buildkite-test-collector-0.1.9, timeout-2.3.1, schemathesis-3.39.15, anyio-4.6.2.post1, mock-3.14.0, hypothesis-6.131.0, rerunfailures-14.0\nasyncio: mode=Mode.STRICT, default_loop_scope=None\ncollected 1 item                                                                                                                                                                                             \nRunning 1 items in this shard: tests/quantization/test_bitsandbytes.py::test_4bit_bnb_embedding_model[half-intfloat/e5-mistral-7b-instruct-quantize embedding model inflight]\n\nquantization/test_bitsandbytes.py::test_4bit_bnb_embedding_model[half-intfloat/e5-mistral-7b-instruct-quantize embedding model inflight] Fork a new process to run a test 1747225\nFork a new process to run a test 0\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:09<00:00,  4.80s/it]\nINFO 06-23 04:48:29 [config.py:588] Found sentence-transformers tokenize configuration.\nINFO 06-23 04:48:35 [config.py:484] Found sentence-transformers modules configuration.\nINFO 06-23 04:48:35 [config.py:504] Found pooling configuration.\nINFO 06-23 04:48:35 [config.py:1444] Using max model len 1024\nWARNING 06-23 04:48:35 [config.py:939] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nINFO 06-23 04:48:35 [arg_utils.py:1568] (Enabling) prefix caching by default\nWARNING 06-23 04:48:36 [utils.py:2613] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\nINFO 06-23 04:48:39 [__init__.py:244] Automatically detected platform cuda.\nINFO 06-23 04:48:42 [core.py:459] Waiting for init message from front-end.\nINFO 06-23 04:48:42 [core.py:69] Initializing a V1 LLM engine (v0.9.1.dev287+g89b1388d8) with config: model='intfloat/e5-mistral-7b-instruct', speculative_config=None, tokenizer='intfloat/e5-mistral-7b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=intfloat/e5-mistral-7b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type='LAST', normalize=True, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\nWARNING 06-23 04:48:42 [utils.py:2753] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ab4c475f5f0>\nINFO 06-23 04:48:42 [parallel_state.py:1072] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nWARNING 06-23 04:48:42 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\nINFO 06-23 04:48:42 [gpu_model_runner.py:1696] Starting to load model intfloat/e5-mistral-7b-instruct...\nINFO 06-23 04:48:43 [gpu_model_runner.py:1701] Loading model from scratch...\nINFO 06-23 04:48:43 [cuda.py:270] Using Flash Attention backend on V1 engine.\nINFO 06-23 04:48:43 [bitsandbytes_loader.py:454] Loading weights with BitsAndBytes quantization. May take a while ...\nINFO 06-23 04:48:43 [weight_utils.py:292] Using model weights format ['*.safetensors']\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.37s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.00s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.06s/it]\n\nINFO 06-23 04:48:46 [gpu_model_runner.py:1725] Model loading took 3.9099 GiB and 2.797580 seconds\nINFO 06-23 04:48:51 [backends.py:508] Using cache directory: /home/mgoin/.cache/vllm/torch_compile_cache/dee6dd784e/rank_0_0/backbone for vLLM's torch.compile\nINFO 06-23 04:48:51 [backends.py:519] Dynamo bytecode transform time: 4.86 s\nINFO 06-23 04:48:54 [backends.py:155] Directly load the compiled graph(s) for shape None from the cache, took 3.547 s\nINFO 06-23 04:48:55 [monitor.py:34] torch.compile takes 4.86 s in total\nINFO 06-23 04:48:56 [gpu_worker.py:232] Available KV cache memory: 66.87 GiB\nINFO 06-23 04:48:56 [kv_cache_utils.py:716] GPU KV cache size: 547,776 tokens\nINFO 06-23 04:48:56 [kv_cache_utils.py:720] Maximum concurrency for 1,024 tokens per request: 526.71x\nWARNING 06-23 04:48:56 [utils.py:101] Unable to detect current VLLM config. Defaulting to NHD kv cache layout.\nCapturing CUDA graphs:   0%|                                                                                                                                                           | 0/67 [00:00<?, ?it/s]\nERROR 06-23 04:48:56 [core.py:519] EngineCore failed to start.\nERROR 06-23 04:48:56 [core.py:519] Traceback (most recent call last):\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 510, in run_engine_core\nERROR 06-23 04:48:56 [core.py:519]     engine_core = EngineCoreProc(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 394, in __init__\nERROR 06-23 04:48:56 [core.py:519]     super().__init__(vllm_config, executor_class, log_stats,\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 82, in __init__\nERROR 06-23 04:48:56 [core.py:519]     self._initialize_kv_caches(vllm_config)\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 169, in _initialize_kv_caches\nERROR 06-23 04:48:56 [core.py:519]     self.model_executor.initialize_from_config(kv_cache_configs)\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/v1/executor/abstract.py\", line 66, in initialize_from_config\nERROR 06-23 04:48:56 [core.py:519]     self.collective_rpc(\"compile_or_warm_up_model\")\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\nERROR 06-23 04:48:56 [core.py:519]     answer = run_method(self.driver_worker, method, args, kwargs)\nERROR 06-23 04:48:56 [core.py:519]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/utils.py\", line 2687, in run_method\nERROR 06-23 04:48:56 [core.py:519]     return func(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_worker.py\", line 266, in compile_or_warm_up_model\nERROR 06-23 04:48:56 [core.py:519]     self.model_runner.capture_model()\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_model_runner.py\", line 2213, in capture_model\nERROR 06-23 04:48:56 [core.py:519]     self._dummy_run(num_tokens, capture_attn_cudagraph=full_cg)\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nERROR 06-23 04:48:56 [core.py:519]     return func(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_model_runner.py\", line 1969, in _dummy_run\nERROR 06-23 04:48:56 [core.py:519]     outputs = model(\nERROR 06-23 04:48:56 [core.py:519]               ^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\nERROR 06-23 04:48:56 [core.py:519]     return self._call_impl(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\nERROR 06-23 04:48:56 [core.py:519]     return forward_call(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/model_executor/models/llama.py\", line 581, in forward\nERROR 06-23 04:48:56 [core.py:519]     model_output = self.model(input_ids, positions, intermediate_tensors,\nERROR 06-23 04:48:56 [core.py:519]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/compilation/decorators.py\", line 246, in __call__\nERROR 06-23 04:48:56 [core.py:519]     model_output = self.forward(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/model_executor/models/llama.py\", line 368, in forward\nERROR 06-23 04:48:56 [core.py:519]     def forward(\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\nERROR 06-23 04:48:56 [core.py:519]     return self._call_impl(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\nERROR 06-23 04:48:56 [core.py:519]     return forward_call(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 838, in _fn\nERROR 06-23 04:48:56 [core.py:519]     return fn(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 830, in call_wrapped\nERROR 06-23 04:48:56 [core.py:519]     return self._wrapped_call(self, *args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 406, in __call__\nERROR 06-23 04:48:56 [core.py:519]     raise e\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 393, in __call__\nERROR 06-23 04:48:56 [core.py:519]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\nERROR 06-23 04:48:56 [core.py:519]     return self._call_impl(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\nERROR 06-23 04:48:56 [core.py:519]     return forward_call(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"<eval_with_key>.66\", line 337, in forward\nERROR 06-23 04:48:56 [core.py:519]     submod_2 = self.submod_2(getitem_3, s0, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_bnb_shard_offsets, getitem_4, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_bnb_shard_offsets, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_bnb_shard_offsets, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_bnb_shard_offsets, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_bnb_shard_offsets = getitem_4 = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_bnb_shard_offsets = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_bnb_shard_offsets = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_bnb_shard_offsets = None\nERROR 06-23 04:48:56 [core.py:519]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/compilation/cuda_piecewise_backend.py\", line 156, in __call__\nERROR 06-23 04:48:56 [core.py:519]     return entry.runnable(*args)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/compilation/compiler_interface.py\", line 510, in compiled_graph\nERROR 06-23 04:48:56 [core.py:519]     graph_output = inductor_compiled_graph(list_args)\nERROR 06-23 04:48:56 [core.py:519]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/_inductor/output_code.py\", line 460, in __call__\nERROR 06-23 04:48:56 [core.py:519]     return self.current_callable(inputs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/.cache/vllm/torch_compile_cache/dee6dd784e/rank_0_0/inductor_cache/3k/c3kedtjuicpyiyo55z5hejbsjwtnfepkyplcvf6hyoj4zxhhu3pa.py\", line 589, in call\nERROR 06-23 04:48:56 [core.py:519]     torch.ops.vllm.apply_bnb_4bit.default(buf6, arg6_1, arg7_1, buf5)\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/_ops.py\", line 756, in __call__\nERROR 06-23 04:48:56 [core.py:519]     return self._op(*args, **kwargs)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/code/vllm/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 372, in _apply_bnb_4bit\nERROR 06-23 04:48:56 [core.py:519]     out[:, current_index:current_index + output_size] = matmul_4bit(\nERROR 06-23 04:48:56 [core.py:519]                                                         ^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py\", line 533, in matmul_4bit\nERROR 06-23 04:48:56 [core.py:519]     return MatMul4Bit.apply(A, B, out, bias, quant_state)\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/autograd/function.py\", line 575, in apply\nERROR 06-23 04:48:56 [core.py:519]     return super().apply(*args, **kwargs)  # type: ignore[misc]\nERROR 06-23 04:48:56 [core.py:519]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519]   File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py\", line 462, in forward\nERROR 06-23 04:48:56 [core.py:519]     output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\nERROR 06-23 04:48:56 [core.py:519]                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 06-23 04:48:56 [core.py:519] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 77.44 MiB is free. Process 1747225 has 7.29 GiB memory in use. Including non-PyTorch memory, this process has 71.81 GiB memory in use. Of the allocated memory 71.01 GiB is allocated by PyTorch, and 73.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nProcess EngineCore_0:\nTraceback (most recent call last):\n  File \"/home/mgoin/.local/share/uv/python/cpython-3.12.4-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/home/mgoin/.local/share/uv/python/cpython-3.12.4-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 523, in run_engine_core\n    raise e\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 510, in run_engine_core\n    engine_core = EngineCoreProc(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 394, in __init__\n    super().__init__(vllm_config, executor_class, log_stats,\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 82, in __init__\n    self._initialize_kv_caches(vllm_config)\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core.py\", line 169, in _initialize_kv_caches\n    self.model_executor.initialize_from_config(kv_cache_configs)\n  File \"/home/mgoin/code/vllm/vllm/v1/executor/abstract.py\", line 66, in initialize_from_config\n    self.collective_rpc(\"compile_or_warm_up_model\")\n  File \"/home/mgoin/code/vllm/vllm/executor/uniproc_executor.py\", line 57, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/utils.py\", line 2687, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_worker.py\", line 266, in compile_or_warm_up_model\n    self.model_runner.capture_model()\n  File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_model_runner.py\", line 2213, in capture_model\n    self._dummy_run(num_tokens, capture_attn_cudagraph=full_cg)\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/v1/worker/gpu_model_runner.py\", line 1969, in _dummy_run\n    outputs = model(\n              ^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/model_executor/models/llama.py\", line 581, in forward\n    model_output = self.model(input_ids, positions, intermediate_tensors,\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/compilation/decorators.py\", line 246, in __call__\n    model_output = self.forward(*args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/model_executor/models/llama.py\", line 368, in forward\n    def forward(\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\", line 838, in _fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 830, in call_wrapped\n    return self._wrapped_call(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 406, in __call__\n    raise e\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/fx/graph_module.py\", line 393, in __call__\n    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<eval_with_key>.66\", line 337, in forward\n    submod_2 = self.submod_2(getitem_3, s0, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_bnb_shard_offsets, getitem_4, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_bnb_shard_offsets, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_bnb_shard_offsets, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_bnb_shard_offsets, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_bnb_shard_offsets = getitem_4 = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_bnb_shard_offsets = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_bnb_shard_offsets = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_bnb_shard_offsets = None\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/compilation/cuda_piecewise_backend.py\", line 156, in __call__\n    return entry.runnable(*args)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/compilation/compiler_interface.py\", line 510, in compiled_graph\n    graph_output = inductor_compiled_graph(list_args)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/_inductor/output_code.py\", line 460, in __call__\n    return self.current_callable(inputs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/.cache/vllm/torch_compile_cache/dee6dd784e/rank_0_0/inductor_cache/3k/c3kedtjuicpyiyo55z5hejbsjwtnfepkyplcvf6hyoj4zxhhu3pa.py\", line 589, in call\n    torch.ops.vllm.apply_bnb_4bit.default(buf6, arg6_1, arg7_1, buf5)\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/_ops.py\", line 756, in __call__\n    return self._op(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/model_executor/layers/quantization/bitsandbytes.py\", line 372, in _apply_bnb_4bit\n    out[:, current_index:current_index + output_size] = matmul_4bit(\n                                                        ^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py\", line 533, in matmul_4bit\n    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/torch/autograd/function.py\", line 575, in apply\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/venvs/vllm/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py\", line 462, in forward\n    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 77.44 MiB is free. Process 1747225 has 7.29 GiB memory in use. Including non-PyTorch memory, this process has 71.81 GiB memory in use. Of the allocated memory 71.01 GiB is allocated by PyTorch, and 73.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n[rank0]:[W623 04:48:57.557535085 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nTraceback (most recent call last):\n  File \"/home/mgoin/code/vllm/tests/utils.py\", line 741, in wrapper\n    f(*args, **kwargs)\n  File \"/home/mgoin/code/vllm/tests/quantization/test_bitsandbytes.py\", line 159, in test_4bit_bnb_embedding_model\n    with vllm_runner(model_name,\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/tests/conftest.py\", line 787, in __init__\n    self.model = LLM(\n                 ^^^^\n  File \"/home/mgoin/code/vllm/vllm/entrypoints/llm.py\", line 263, in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/engine/llm_engine.py\", line 501, in from_engine_args\n    return engine_cls.from_vllm_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/llm_engine.py\", line 124, in from_vllm_config\n    return cls(vllm_config=vllm_config,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/llm_engine.py\", line 101, in __init__\n    self.engine_core = EngineCoreClient.make_client(\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core_client.py\", line 75, in make_client\n    return SyncMPClient(vllm_config, executor_class, log_stats)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core_client.py\", line 558, in __init__\n    super().__init__(\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core_client.py\", line 422, in __init__\n    self._init_engines_direct(vllm_config, local_only,\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core_client.py\", line 491, in _init_engines_direct\n    self._wait_for_engine_startup(handshake_socket, input_address,\n  File \"/home/mgoin/code/vllm/vllm/v1/engine/core_client.py\", line 511, in _wait_for_engine_startup\n    wait_for_engine_startup(\n  File \"/home/mgoin/code/vllm/vllm/v1/utils.py\", line 494, in wait_for_engine_startup\n    raise RuntimeError(\"Engine core initialization failed. \"\nRuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\nFAILED\n\n================================================================================================== FAILURES ==================================================================================================\n___________________________________________________ test_4bit_bnb_embedding_model[half-intfloat/e5-mistral-7b-instruct-quantize embedding model inflight] ____________________________________________________\n\nargs = ()\nkwargs = {'description': 'quantize embedding model inflight', 'dtype': 'half', 'example_prompts': ['vLLM is a high-throughput a...n global economic structures and future business models.\\n', ...], 'hf_runner': <class 'tests.conftest.HfRunner'>, ...}\nSkipped = <class 'Skipped'>, pid = 1747225, pgid = 1747030, _pid = 1747225, _exitcode = 256, old_signal_handler = <Handlers.SIG_DFL: 0>\n\n    @functools.wraps(f)\n    def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> None:\n        # Make the process the leader of its own process group\n        # to avoid sending SIGTERM to the parent process\n        os.setpgrp()\n        from _pytest.outcomes import Skipped\n        pid = os.fork()\n        print(f\"Fork a new process to run a test {pid}\")\n        if pid == 0:\n            try:\n                f(*args, **kwargs)\n            except Skipped as e:\n                # convert Skipped to exit code 0\n                print(str(e))\n                os._exit(0)\n            except Exception:\n                import traceback\n                traceback.print_exc()\n                os._exit(1)\n            else:\n                os._exit(0)\n        else:\n            pgid = os.getpgid(pid)\n            _pid, _exitcode = os.waitpid(pid, 0)\n            # ignore SIGTERM signal itself\n            old_signal_handler = signal.signal(signal.SIGTERM, signal.SIG_IGN)\n            # kill all child processes\n            os.killpg(pgid, signal.SIGTERM)\n            # restore the signal handler\n            signal.signal(signal.SIGTERM, old_signal_handler)\n>           assert _exitcode == 0, (f\"function {f} failed when called with\"\n                                    f\" args {args} and kwargs {kwargs}\")\nE           AssertionError: function <function test_4bit_bnb_embedding_model at 0x75cfc981f9c0> failed when called with args () and kwargs {'model_name': 'intfloat/e5-mistral-7b-instruct', 'description': 'quantize embedding model inflight', 'hf_runner': <class 'tests.conftest.HfRunner'>, 'vllm_runner': <class 'tests.conftest.VllmRunner'>, 'example_prompts': ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the major milestones in the development of artificial intelligence from 1950 to 2020.\\n', 'Compare and contrast artificial intelligence with human intelligence in terms of processing information.\\n', 'Describe the basic components of a neural network and how it can be trained.\\n', 'Write a short story about a robot that dreams for the first time.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', 'Explain the cultural significance of the Mona Lisa painting, and how its perception might vary in Western versus Eastern societies.\\n', \"Translate the following English sentence into Japanese, French, and Swahili: 'The early bird catches the worm.'\\n\"], 'dtype': 'half'}\n\nutils.py:761: AssertionError\n============================================================================================== warnings summary ==============================================================================================\n../../../venvs/vllm/lib/python3.12/site-packages/schemathesis/generation/coverage.py:305\n  /home/mgoin/venvs/vllm/lib/python3.12/site-packages/schemathesis/generation/coverage.py:305: DeprecationWarning: jsonschema.exceptions.RefResolutionError is deprecated as of version 4.18.0. If you wish to catch potential reference resolution errors, directly catch referencing.exceptions.Unresolvable.\n    ref_error: type[Exception] = jsonschema.RefResolutionError,\n\ntests/quantization/test_bitsandbytes.py::test_4bit_bnb_embedding_model[half-intfloat/e5-mistral-7b-instruct-quantize embedding model inflight]\n  /home/mgoin/code/vllm/tests/utils.py:737: DeprecationWarning: This process (pid=1747030) is multi-threaded, use of fork() may lead to deadlocks in the child.\n    pid = os.fork()\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n========================================================================================== short test summary info ===========================================================================================\nFAILED quantization/test_bitsandbytes.py::test_4bit_bnb_embedding_model[half-intfloat/e5-mistral-7b-instruct-quantize embedding model inflight] - AssertionError: function <function test_4bit_bnb_embedding_model at 0x75cfc981f9c0> failed when called with args () and kwargs {'model_name': 'intfloat/e5-mistral-7b-instruct', 'description': 'quantize...\n======================================================================================= 1 failed, 2 warnings in 47.09s =======================================================================================\n```\n\n### \ud83d\udcdd History of failing test\n\nhttps://buildkite.com/vllm/ci/builds/22498/summary/annotations?jid=01979a3a-fc0d-4b8e-96a1-fe70e2d781b8\n\n### CC List.\n\n@jeejeelee ",
    "labels": [
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-06-23T04:53:19+00:00",
    "closed_at": "2025-06-23T13:30:57+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19964/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/19964"
  },
  {
    "number": 106,
    "title": "Documentation on running basic python server and FastAPI server",
    "body": null,
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2023-05-17T20:36:13+00:00",
    "closed_at": "2023-06-17T17:26:14+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/106/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/106"
  },
  {
    "number": 18708,
    "title": "[Bug][CI Failure] - VI Test - test_engine_core_client.py::test_kv_cache_events[True-tcp]",
    "body": "### Your current environment\n\nFlakey test for at least the past month: https://buildkite.com/organizations/vllm/analytics/suites/ci-1/tests/4abfbf0d-3a86-8a68-9ff3-0e0ab0fbb38b?period=28days&tags=scm.branch%3Amain%2Cresult%3Afailed\n\n### \ud83d\udc1b Describe the bug\n\nFailing tests:\n\n```\nFAILED v1/engine/test_engine_core_client.py::test_kv_cache_events[True-tcp] - AssertionError: No message received\nassert None is not None\n```\n\n<details>\n<summary>Logs:</summary>\n\n```\n=================================== FAILURES ===================================\n________________________ test_kv_cache_events[True-tcp] ________________________\n\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fc027da70e0>\nmultiprocessing_mode = True\npublisher_config = KVEventsConfig(enable_kv_cache_events=True, publisher='zmq', endpoint='tcp://*:51905', replay_endpoint='tcp://*:51906', buffer_steps=100, hwm=1000, max_queue_size=100000, topic='test')\n\n    @pytest.mark.parametrize(\n        \"multiprocessing_mode,publisher_config\",\n        [(True, \"tcp\"), (False, \"inproc\")],\n        indirect=[\"publisher_config\"],\n    )\n    def test_kv_cache_events(\n        monkeypatch: pytest.MonkeyPatch,\n        multiprocessing_mode: bool,\n        publisher_config,\n    ):\n    \n        with monkeypatch.context() as m:\n            m.setenv(\"VLLM_USE_V1\", \"1\")\n            block_size = 16\n            num_blocks = 2\n    \n            engine_args = EngineArgs(model=MODEL_NAME,\n                                     enforce_eager=True,\n                                     enable_prefix_caching=True,\n                                     block_size=block_size)\n            engine_args.kv_events_config = publisher_config\n    \n            vllm_config = engine_args.create_engine_config(\n                UsageContext.UNKNOWN_CONTEXT)\n    \n            executor_class = Executor.get_class(vllm_config)\n            client = EngineCoreClient.make_client(\n                multiprocess_mode=multiprocessing_mode,\n                asyncio_mode=False,\n                vllm_config=vllm_config,\n                executor_class=executor_class,\n                log_stats=False,\n            )\n            endpoint = publisher_config.endpoint.replace(\"*\", \"127.0.0.1\")\n            subscriber = MockSubscriber(endpoint,\n                                        topic=publisher_config.topic,\n                                        decode_type=KVEventBatch)\n    \n            try:\n                custom_tokens = list(range(num_blocks * block_size))\n                request = EngineCoreRequest(\n                    request_id=str(uuid.uuid4()),\n                    prompt_token_ids=custom_tokens,\n                    mm_inputs=None,\n                    mm_hashes=None,\n                    mm_placeholders=None,\n                    sampling_params=SamplingParams(\n                        max_tokens=1),  # Short completion for speed\n                    eos_token_id=None,\n                    arrival_time=time.time(),\n                    lora_request=None,\n                    cache_salt=None,\n                )\n                client.add_request(request)\n    \n                outputs: dict[str, list] = {request.request_id: []}\n                loop_until_done(client, outputs)\n    \n                result = subscriber.receive_one(timeout=1000)\n>               assert result is not None, \"No message received\"\nE               AssertionError: No message received\nE               assert None is not None\n\nv1/engine/test_engine_core_client.py:318: AssertionError\n=============================== warnings summary ===============================\n../../usr/local/lib/python3.12/dist-packages/schemathesis/generation/coverage.py:305\n  /usr/local/lib/python3.12/dist-packages/schemathesis/generation/coverage.py:305: DeprecationWarning: jsonschema.exceptions.RefResolutionError is deprecated as of version 4.18.0. If you wish to catch potential reference resolution errors, directly catch referencing.exceptions.Unresolvable.\n    ref_error: type[Exception] = jsonschema.RefResolutionError,\n\ntests/v1/engine/test_async_llm.py: 13 warnings\ntests/v1/engine/test_engine_core_client.py: 2 warnings\n  /usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=277) is multi-threaded, use of fork() may lead to deadlocks in the child.\n    self.pid = os.fork()\n\ntests/v1/engine/test_engine_core.py::test_engine_core\ntests/v1/engine/test_engine_core.py::test_engine_core_advanced_sampling\ntests/v1/engine/test_engine_core.py::test_engine_core_concurrent_batches\ntests/v1/engine/test_engine_core_client.py::test_engine_core_client[True]\ntests/v1/engine/test_engine_core_client.py::test_engine_core_client[False]\n  /vllm-workspace/tests/utils.py:723: DeprecationWarning: This process (pid=277) is multi-threaded, use of fork() may lead to deadlocks in the child.\n    pid = os.fork()\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ============================\nFAILED v1/engine/test_engine_core_client.py::test_kv_cache_events[True-tcp] - AssertionError: No message received\nassert None is not None\n============ 1 failed, 49 passed, 21 warnings in 706.77s (0:11:46) =============\n^^^ +++\n\ud83d\udea8 Error: The command exited with status 1\n^^^ +++\nuser command error: The plugin docker command hook exited with status 1\n~~~ Running global pre-exit hook\n$ /etc/buildkite-agent/hooks/pre-exit\n~~~ Running plugin docker pre-exit hook\n$ /var/lib/buildkite-agent/plugins/bk-gpu-1-queue-ci-i-0aabd234c4d03089e-1/github-com-buildkite-plugins-docker-buildkite-plugin-v5-2-0/hooks/pre-exit\n```\n\n</details>\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-05-26T11:38:43+00:00",
    "closed_at": "2025-06-04T12:57:32+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18708/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18708"
  },
  {
    "number": 47,
    "title": "Frontend Improvements",
    "body": "1. Current implementation of the FastAPI+asyncio+ray combination seems slow\r\n2. Merge Hao\u2019s throughput profiling code.\r\n3. Make the frontend looks like OpenAI\u2019s API.\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-22T03:57:50+00:00",
    "closed_at": "2023-05-24T04:39:52+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/47/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/47"
  },
  {
    "number": 18425,
    "title": "[Bug][Failing Test] - Quantization test - quantization/test_cpu_offload.py",
    "body": "### Your current environment\n\nFailing on main as of commit 9609327fa4\n\n### \ud83d\udc1b Describe the bug\n\nFailing tests:\n\n```\nFAILED quantization/test_cpu_offload.py::test_cpu_offload_gptq - RuntimeError: Server exited unexpectedly.\nFAILED quantization/test_cpu_offload.py::test_cpu_offload_awq - RuntimeError: Server exited unexpectedly.\nFAILED quantization/test_cpu_offload.py::test_cpu_offload_compressed_tensors - AssertionError: Results for model='nm-testing/llama7b-one-shot-2_4-w4a16-marlin24-t' are not the same.\nref_args=[] ref_envs=None\ncompare_args=['--cpu-offload-gb', '1'] compare_envs=None\nref_result={'test': 'single_completion', 'text': ' ... ... . Today I', 'finish_reason': 'length', 'usage': CompletionUsage(completion_tokens=5, prompt_tokens=6, total_tokens=11, completion_tokens_details=None, prompt_tokens_details=None)}\ncompare_result={'test': 'single_completion', 'text': ' ... ... .\\n I', 'finish_reason': 'length', 'usage': CompletionUsage(completion_tokens=5, prompt_tokens=6, total_tokens=11, completion_tokens_details=None, prompt_tokens_details=None)}\n```\n\n<details>\n<summary>Logs</summary>\n\n\n```\nquantization/test_torchao.py::test_opt_125m_int4wo_model_loading_with_params[cuda:0] SKIPPED\nquantization/test_torchao.py::test_opt_125m_int4wo_model_per_module_quant SKIPPED\n\n=================================== FAILURES ===================================\n_ test_load_8bit_bnb_model[meta-llama/Llama-Guard-3-8B-INT8-read pre-quantized llama 8-bit model] _\n\nargs = ()\nkwargs = {'description': 'read pre-quantized llama 8-bit model', 'example_prompts': ['vLLM is a high-throughput and memory-effi...odels.\\n', ...], 'hf_runner': <class 'tests.conftest.HfRunner'>, 'model_name': 'meta-llama/Llama-Guard-3-8B-INT8', ...}\nSkipped = <class 'Skipped'>, pid = 1736, pgid = 19, _pid = 1736, _exitcode = 256\nold_signal_handler = <Handlers.SIG_DFL: 0>\n\n    @functools.wraps(f)\n    def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> None:\n        # Make the process the leader of its own process group\n        # to avoid sending SIGTERM to the parent process\n        os.setpgrp()\n        from _pytest.outcomes import Skipped\n        pid = os.fork()\n        print(f\"Fork a new process to run a test {pid}\")\n        if pid == 0:\n            try:\n                f(*args, **kwargs)\n            except Skipped as e:\n                # convert Skipped to exit code 0\n                print(str(e))\n                os._exit(0)\n            except Exception:\n                import traceback\n                traceback.print_exc()\n                os._exit(1)\n            else:\n                os._exit(0)\n        else:\n            pgid = os.getpgid(pid)\n            _pid, _exitcode = os.waitpid(pid, 0)\n            # ignore SIGTERM signal itself\n            old_signal_handler = signal.signal(signal.SIGTERM, signal.SIG_IGN)\n            # kill all child processes\n            os.killpg(pgid, signal.SIGTERM)\n            # restore the signal handler\n            signal.signal(signal.SIGTERM, old_signal_handler)\n>           assert _exitcode == 0, (f\"function {f} failed when called with\"\n                                    f\" args {args} and kwargs {kwargs}\")\nE           AssertionError: function <function test_load_8bit_bnb_model at 0x7fc6998f1800> failed when called with args () and kwargs {'hf_runner': <class 'tests.conftest.HfRunner'>, 'vllm_runner': <class 'tests.conftest.VllmRunner'>, 'example_prompts': ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the major milestones in the development of artificial intelligence from 1950 to 2020.\\n', 'Compare and contrast artificial intelligence with human intelligence in terms of processing information.\\n', 'Describe the basic components of a neural network and how it can be trained.\\n', 'Write a short story about a robot that dreams for the first time.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', 'Explain the cultural significance of the Mona Lisa painting, and how its perception might vary in Western versus Eastern societies.\\n', \"Translate the following English sentence into Japanese, French, and Swahili: 'The early bird catches the worm.'\\n\"], 'model_name': 'meta-llama/Llama-Guard-3-8B-INT8', 'description': 'read pre-quantized llama 8-bit model'}\n\nutils.py:747: AssertionError\n____________________________ test_cpu_offload_gptq _____________________________\n\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fc5b4cb2600>\n\n    @pytest.mark.skipif(not is_quant_method_supported(\"gptq_marlin\"),\n                        reason=\"gptq_marlin is not supported on this GPU type.\")\n    def test_cpu_offload_gptq(monkeypatch):\n        # This quant method is sensitive to dummy weights, so we force real weights\n        monkeypatch.setenv('VLLM_TEST_FORCE_LOAD_FORMAT', 'auto')\n        # Test GPTQ Marlin\n>       compare_two_settings(\"Qwen/Qwen2-1.5B-Instruct-GPTQ-Int4\", [],\n                             [\"--cpu-offload-gb\", \"1\"],\n                             max_wait_seconds=480)\n\nquantization/test_cpu_offload.py:33: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nutils.py:465: in compare_two_settings\n    compare_all_settings(\nutils.py:529: in compare_all_settings\n    with RemoteOpenAIServer(model,\nutils.py:133: in __init__\n    self._wait_for_server(url=self.url_for(\"health\"),\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <tests.utils.RemoteOpenAIServer object at 0x7fc59e873800>\n\n    def _wait_for_server(self, *, url: str, timeout: float):\n        # run health check\n        start = time.time()\n        while True:\n            try:\n                if requests.get(url).status_code == 200:\n                    break\n            except Exception:\n                # this exception can only be raised by requests.get,\n                # which means the server is not ready yet.\n                # the stack trace is not useful, so we suppress it\n                # by using `raise from None`.\n                result = self.proc.poll()\n                if result is not None and result != 0:\n>                   raise RuntimeError(\"Server exited unexpectedly.\") from None\nE                   RuntimeError: Server exited unexpectedly.\n\nutils.py:161: RuntimeError\n_____________________________ test_cpu_offload_awq _____________________________\n\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fc59e8de900>\n\n    @pytest.mark.skipif(not is_quant_method_supported(\"awq_marlin\"),\n                        reason=\"awq_marlin is not supported on this GPU type.\")\n    def test_cpu_offload_awq(monkeypatch):\n        # This quant method is sensitive to dummy weights, so we force real weights\n        monkeypatch.setenv('VLLM_TEST_FORCE_LOAD_FORMAT', 'auto')\n        # Test AWQ Marlin\n>       compare_two_settings(\"Qwen/Qwen2-1.5B-Instruct-AWQ\", [],\n                             [\"--cpu-offload-gb\", \"1\"],\n                             max_wait_seconds=480)\n\nquantization/test_cpu_offload.py:49: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nutils.py:465: in compare_two_settings\n    compare_all_settings(\nutils.py:529: in compare_all_settings\n    with RemoteOpenAIServer(model,\nutils.py:133: in __init__\n    self._wait_for_server(url=self.url_for(\"health\"),\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <tests.utils.RemoteOpenAIServer object at 0x7fc5b55128d0>\n\n    def _wait_for_server(self, *, url: str, timeout: float):\n        # run health check\n        start = time.time()\n        while True:\n            try:\n                if requests.get(url).status_code == 200:\n                    break\n            except Exception:\n                # this exception can only be raised by requests.get,\n                # which means the server is not ready yet.\n                # the stack trace is not useful, so we suppress it\n                # by using `raise from None`.\n                result = self.proc.poll()\n                if result is not None and result != 0:\n>                   raise RuntimeError(\"Server exited unexpectedly.\") from None\nE                   RuntimeError: Server exited unexpectedly.\n\nutils.py:161: RuntimeError\n_____________________ test_cpu_offload_compressed_tensors ______________________\n\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fc5b4b958e0>\n\n    @pytest.mark.skipif(not is_quant_method_supported(\"gptq_marlin\"),\n                        reason=\"gptq_marlin is not supported on this GPU type.\")\n    def test_cpu_offload_compressed_tensors(monkeypatch):\n        # This quant method is sensitive to dummy weights, so we force real weights\n        monkeypatch.setenv('VLLM_TEST_FORCE_LOAD_FORMAT', 'auto')\n        # Test wNa16\n        compare_two_settings(\"nm-testing/tinyllama-oneshot-w4a16-channel-v2\", [],\n                             [\"--cpu-offload-gb\", \"1\"],\n                             max_wait_seconds=480)\n        # Test w4a16_marlin24\n>       compare_two_settings(\"nm-testing/llama7b-one-shot-2_4-w4a16-marlin24-t\",\n                             [], [\"--cpu-offload-gb\", \"1\"],\n                             max_wait_seconds=480)\n\nquantization/test_cpu_offload.py:69: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nutils.py:465: in compare_two_settings\n    compare_all_settings(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmodel = 'nm-testing/llama7b-one-shot-2_4-w4a16-marlin24-t'\nall_args = [[], ['--cpu-offload-gb', '1']], all_envs = [None, None]\n\n    def compare_all_settings(model: str,\n                             all_args: list[list[str]],\n                             all_envs: list[Optional[dict[str, str]]],\n                             *,\n                             method: str = \"generate\",\n                             max_wait_seconds: Optional[float] = None) -> None:\n        \"\"\"\n        Launch API server with several different sets of arguments/environments\n        and compare the results of the API calls with the first set of arguments.\n        Args:\n            model: The model to test.\n            all_args: A list of argument lists to pass to the API server.\n            all_envs: A list of environment dictionaries to pass to the API server.\n        \"\"\"\n    \n        trust_remote_code = False\n        for args in all_args:\n            if \"--trust-remote-code\" in args:\n                trust_remote_code = True\n                break\n    \n        tokenizer_mode = \"auto\"\n        for args in all_args:\n            if \"--tokenizer-mode\" in args:\n                tokenizer_mode = args[args.index(\"--tokenizer-mode\") + 1]\n                break\n    \n        tokenizer = get_tokenizer(\n            model,\n            trust_remote_code=trust_remote_code,\n            tokenizer_mode=tokenizer_mode,\n        )\n    \n        can_force_load_format = True\n    \n        for args in all_args:\n            if \"--load-format\" in args:\n                can_force_load_format = False\n                break\n    \n        prompt = \"Hello, my name is\"\n        token_ids = tokenizer(prompt).input_ids\n        ref_results: list = []\n        for i, (args, env) in enumerate(zip(all_args, all_envs)):\n            if can_force_load_format:\n                # we are comparing the results and\n                # usually we don't need real weights.\n                # we force to use dummy weights by default,\n                # and it should work for most of the cases.\n                # if not, we can use VLLM_TEST_FORCE_LOAD_FORMAT\n                # environment variable to force the load format,\n                # e.g. in quantization tests.\n                args = args + [\"--load-format\", envs.VLLM_TEST_FORCE_LOAD_FORMAT]\n            compare_results: list = []\n            results = ref_results if i == 0 else compare_results\n            with RemoteOpenAIServer(model,\n                                    args,\n                                    env_dict=env,\n                                    max_wait_seconds=max_wait_seconds) as server:\n                client = server.get_client()\n    \n                # test models list\n                models = client.models.list()\n                models = models.data\n                served_model = models[0]\n                results.append({\n                    \"test\": \"models_list\",\n                    \"id\": served_model.id,\n                    \"root\": served_model.root,\n                })\n    \n                if method == \"generate\":\n                    results += _test_completion(client, model, prompt, token_ids)\n                elif method == \"generate_close\":\n                    results += _test_completion_close(client, model, prompt)\n                elif method == \"generate_chat\":\n                    results += _test_chat(client, model, prompt)\n                elif method == \"generate_with_image\":\n                    results += _test_image_text(\n                        client, model,\n                        \"https://upload.wikimedia.org/wikipedia/commons/0/0b/RGBA_comp.png\"\n                    )\n                elif method == \"encode\":\n                    results += _test_embeddings(client, model, prompt)\n                else:\n                    raise ValueError(f\"Unknown method: {method}\")\n    \n                if i > 0:\n                    # if any setting fails, raise an error early\n                    ref_args = all_args[0]\n                    ref_envs = all_envs[0]\n                    compare_args = all_args[i]\n                    compare_envs = all_envs[i]\n                    for ref_result, compare_result in zip(ref_results,\n                                                          compare_results):\n                        ref_result = copy.deepcopy(ref_result)\n                        compare_result = copy.deepcopy(compare_result)\n                        if \"embedding\" in ref_result and method == \"encode\":\n                            sim = F.cosine_similarity(\n                                torch.tensor(ref_result[\"embedding\"]),\n                                torch.tensor(compare_result[\"embedding\"]),\n                                dim=0,\n                            )\n                            assert sim >= 0.999, (\n                                f\"Embedding for {model=} are not the same.\\n\"\n                                f\"cosine_similarity={sim}\\n\")\n                            del ref_result[\"embedding\"]\n                            del compare_result[\"embedding\"]\n>                       assert ref_result == compare_result, (\n                            f\"Results for {model=} are not the same.\\n\"\n                            f\"{ref_args=} {ref_envs=}\\n\"\n                            f\"{compare_args=} {compare_envs=}\\n\"\n                            f\"{ref_result=}\\n\"\n                            f\"{compare_result=}\\n\")\nE                       AssertionError: Results for model='nm-testing/llama7b-one-shot-2_4-w4a16-marlin24-t' are not the same.\nE                       ref_args=[] ref_envs=None\nE                       compare_args=['--cpu-offload-gb', '1'] compare_envs=None\nE                       ref_result={'test': 'single_completion', 'text': ' ... ... . Today I', 'finish_reason': 'length', 'usage': CompletionUsage(completion_tokens=5, prompt_tokens=6, total_tokens=11, completion_tokens_details=None, prompt_tokens_details=None)}\nE                       compare_result={'test': 'single_completion', 'text': ' ... ... .\\n I', 'finish_reason': 'length', 'usage': CompletionUsage(completion_tokens=5, prompt_tokens=6, total_tokens=11, completion_tokens_details=None, prompt_tokens_details=None)}\n\nutils.py:582: AssertionError\n...\n=========================== short test summary info ============================\nFAILED quantization/test_bitsandbytes.py::test_load_8bit_bnb_model[meta-llama/Llama-Guard-3-8B-INT8-read pre-quantized llama 8-bit model] - AssertionError: function <function test_load_8bit_bnb_model at 0x7fc6998f1800> failed when called with args () and kwargs {'hf_runner': <class 'tests.conftest.HfRunner'>, 'vllm_runner': <class 'tests.conftest.VllmRunner'>, 'example_prompts': ['vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\\n', 'Briefly describe the major milestones in the development of artificial intelligence from 1950 to 2020.\\n', 'Compare and contrast artificial intelligence with human intelligence in terms of processing information.\\n', 'Describe the basic components of a neural network and how it can be trained.\\n', 'Write a short story about a robot that dreams for the first time.\\n', 'Analyze the impact of the COVID-19 pandemic on global economic structures and future business models.\\n', 'Explain the cultural significance of the Mona Lisa painting, and how its perception might vary in Western versus Eastern societies.\\n', \"Translate the following English sentence into Japanese, French, and Swahili: 'The early bird catches the worm.'\\n\"], 'model_name': 'meta-llama/Llama-Guard-3-8B-INT8', 'description': 'read pre-quantized llama 8-bit model'}\nFAILED quantization/test_cpu_offload.py::test_cpu_offload_gptq - RuntimeError: Server exited unexpectedly.\nFAILED quantization/test_cpu_offload.py::test_cpu_offload_awq - RuntimeError: Server exited unexpectedly.\nFAILED quantization/test_cpu_offload.py::test_cpu_offload_compressed_tensors - AssertionError: Results for model='nm-testing/llama7b-one-shot-2_4-w4a16-marlin24-t' are not the same.\nref_args=[] ref_envs=None\ncompare_args=['--cpu-offload-gb', '1'] compare_envs=None\nref_result={'test': 'single_completion', 'text': ' ... ... . Today I', 'finish_reason': 'length', 'usage': CompletionUsage(completion_tokens=5, prompt_tokens=6, total_tokens=11, completion_tokens_details=None, prompt_tokens_details=None)}\ncompare_result={'test': 'single_completion', 'text': ' ... ... .\\n I', 'finish_reason': 'length', 'usage': CompletionUsage(completion_tokens=5, prompt_tokens=6, total_tokens=11, completion_tokens_details=None, prompt_tokens_details=None)}\n====== 4 failed, 78 passed, 35 skipped, 49 warnings in 4561.59s (1:16:01) ======\n^^^ +++\n\ud83d\udea8 Error: The command exited with status 1\n^^^ +++\nuser command error: The plugin docker command hook exited with status 1\n~~~ Running global pre-exit hook\n$ /etc/buildkite-agent/hooks/pre-exit\n~~~ Running plugin docker pre-exit hook\n$ /var/lib/buildkite-agent/plugins/bk-gpu-1-queue-ci-i-036cff6c74f0af4ae-1/github-com-buildkite-plugins-docker-buildkite-plugin-v5-2-0/hooks/pre-exit\n\n\n</details>",
    "labels": [
      "bug",
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-05-20T16:15:31+00:00",
    "closed_at": "2025-05-21T17:25:49+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18425/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18425"
  },
  {
    "number": 74,
    "title": "Add docstring",
    "body": null,
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2023-05-06T04:55:55+00:00",
    "closed_at": "2023-06-07T10:25:22+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/74/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/74"
  },
  {
    "number": 3921,
    "title": "[Misc]: Question re: ROCm + Triton Backend",
    "body": "### Anything you want to discuss about vllm.\n\nWe have one question re: the meet-up slides from last week:\r\n<img width=\"957\" alt=\"image\" src=\"https://github.com/vllm-project/vllm/assets/7945038/2bdceea7-65eb-4e67-91ac-432006251256\">\r\nWhat is the \"ROCm + Triton Backend\"? We don't see it in the code currently. Does that already exist somewhere in a PR/branch/fork somewhere? ",
    "labels": [
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-04-08T18:17:43+00:00",
    "closed_at": "2024-04-08T19:19:24+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3921/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/3921"
  },
  {
    "number": 5937,
    "title": "Virtual Office Hours: July 9 and July 25",
    "body": "## vLLM Virtual Open Office Hours\r\n\r\nWe enjoyed seeing everyone at the previous office hours and got great feedback. These office hours are a ~bi-weekly live event where you come to learn more about the vLLM project, how to contribute, and get help with your issues - with special topics and guests along the way.\r\n\r\nSign up here: https://neuralmagic.com/community-office-hours/\r\nHere is a recording from June 20 so you can see the format: https://www.youtube.com/watch?v=ss02R8ndKnk\r\n\r\nDates:\r\n- July 9, 2024, at 2:00 PM EST (11:00 AM PST), **Guest Topic:** FP8 Quantization Deep Dive\r\n- July 25, 2024, at 2:00 PM EST (11:00 AM PST), **Guest Topic:** Model Compression for Fast and Efficient Inference\r\n\r\nIf there are any themes or topics you would like to see addressed, please comment below.\r\n\r\nPrevious issues:\r\n- https://github.com/vllm-project/vllm/issues/4538\r\n- https://github.com/vllm-project/vllm/issues/4919",
    "labels": [
      "misc",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-06-27T22:04:46+00:00",
    "closed_at": "2024-12-19T02:04:54+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5937/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/5937"
  },
  {
    "number": 17123,
    "title": "[Feature]: Automatically detect numerical issues",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nModels such as Gemma-3 and GLM-4 may encounter numerical instability when `float16` dtype is used. Despite a warning message `Casting bfloat16 to float16` being printed, users can still get confused when the model returns empty or nonsense outputs.\n\nExamples:\n- https://github.com/vllm-project/vllm/issues/16489\n- https://github.com/vllm-project/vllm/pull/16618#issuecomment-2814399522\n\nIt would be great if we could automatically detect numerical issues while running the model. We should at least check for this during startup. Inference-time checking should be optional since it harms the performance.\n\n### Alternatives\n\nHardcode specific models to not allow them to be run in float16, like `plamo2`:\n\nhttps://github.com/vllm-project/vllm/blob/4115f19958d8b3628606d78355c277b328f011e1/vllm/config.py#L2834\n\nHowever, this has to be done manually.\n\n### Additional context\n\ncc @youkaichao \n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-04-24T16:58:29+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17123/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/17123"
  },
  {
    "number": 18561,
    "title": "[Bug]: MLA correctness issues when using FA2",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nCollecting environment information...\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 22.04.5 LTS (x86_64)\nGCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version                : 14.0.0-1ubuntu1.1\nCMake version                : version 3.31.2\nLibc version                 : glibc-2.35\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.0+cu126\nIs debug build               : False\nCUDA used to build PyTorch   : 12.6\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.12.4 (main, Jul 25 2024, 22:42:01) [Clang 18.1.8 ] (64-bit runtime)\nPython platform              : Linux-6.5.0-35-generic-x86_64-with-glibc2.35\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.8.93\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : \nGPU 0: NVIDIA H100 80GB HBM3\nGPU 1: NVIDIA H100 80GB HBM3\nGPU 2: NVIDIA H100 80GB HBM3\nGPU 3: NVIDIA H100 80GB HBM3\nGPU 4: NVIDIA H100 80GB HBM3\nGPU 5: NVIDIA H100 80GB HBM3\nGPU 6: NVIDIA H100 80GB HBM3\nGPU 7: NVIDIA H100 80GB HBM3\n\nNvidia driver version        : 570.133.20\ncuDNN version                : Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      46 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             128\nOn-line CPU(s) list:                0-127\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) Platinum 8462Y+\nCPU family:                         6\nModel:                              143\nThread(s) per core:                 2\nCore(s) per socket:                 32\nSocket(s):                          2\nStepping:                           8\nCPU max MHz:                        4100.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           5600.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hfi vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                     VT-x\nL1d cache:                          3 MiB (64 instances)\nL1i cache:                          2 MiB (64 instances)\nL2 cache:                           128 MiB (64 instances)\nL3 cache:                           120 MiB (2 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0-31,64-95\nNUMA node1 CPU(s):                  32-63,96-127\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] flake8==7.2.0\n[pip3] mypy==1.11.1\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-cufile-cu12==1.11.1.6\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-cutlass-dsl==4.0.0.dev1\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pynvml==12.0.0\n[pip3] pyzmq==26.4.0\n[pip3] sentence-transformers==3.2.1\n[pip3] torch==2.7.0\n[pip3] torchaudio==2.7.0\n[pip3] torchvision==0.22.0\n[pip3] transformers==4.52.3\n[pip3] transformers-stream-generator==0.0.5\n[pip3] triton==3.3.0\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.1.dev131+gcb506ecb5 (git sha: cb506ecb5)\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     0-31,64-95      0               N/A\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    32-63,96-127    1               N/A\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    32-63,96-127    1               N/A\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    32-63,96-127    1               N/A\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     32-63,96-127    1               N/A\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS\nNIC1    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     SYS     SYS     SYS\nNIC2    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS     SYS     SYS     SYS\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE\nNIC5    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n  NIC1: mlx5_1\n  NIC2: mlx5_2\n  NIC3: mlx5_3\n  NIC4: mlx5_4\n  NIC5: mlx5_5\n  NIC6: mlx5_6\n  NIC7: mlx5_7\n\n==============================\n     Environment Variables\n==============================\nLD_LIBRARY_PATH=/usr/local/gdrcopy/lib:/usr/local/ucx/lib:/home/mgoin/nixl/lib/x86_64-linux-gnu:/usr/local/cuda-12.8/lib64:/usr/lib:/usr/local/gdrcopy/lib:/usr/local/ucx/lib:/home/mgoin/nixl/lib/x86_64-linux-gnu:/usr/local/cuda-12.8/lib64:/usr/lib:\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nWhen running MLA with FA3 enabled (default on H100), models like `deepseek-ai/DeepSeek-V2-Lite-Chat` work perfectly fine\n\nFA3 TritonMLA on H100:\n```\nVLLM_ATTENTION_BACKEND=TRITON_MLA lm_eval --model vllm --model_args pretrained=deepseek-ai/DeepSeek-V2-Lite-Chat --trust_remote_code --tasks gsm8k --num_fewshot 5 --batch_size auto\nvllm (pretrained=deepseek-ai/DeepSeek-V2-Lite-Chat,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: 5, batch_size: auto\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|\u2191  |0.6672|\u00b1  | 0.013|\n|     |       |strict-match    |     5|exact_match|\u2191  |0.6603|\u00b1  | 0.013|\n```\n\nFA3 FlashMLA on H100:\n```\nlm_eval --model vllm --model_args pretrained=deepseek-ai/DeepSeek-V2-Lite-Chat --trust_remote_code --tasks gsm8k --num_fewshot 5 --batch_size auto\nvllm (pretrained=deepseek-ai/DeepSeek-V2-Lite-Chat,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: 5, batch_size: auto\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|\u2191  |0.6672|\u00b1  | 0.013|\n|     |       |strict-match    |     5|exact_match|\u2191  |0.6603|\u00b1  | 0.013|\n```\n\n\n\nHowever if I force FA2 to be used (using `VLLM_FLASH_ATTN_VERSION=2`) or I run on A100 which uses FA2 by default, the model quality quickly drops off.\n\nFA2 TritonMLA on H100:\n```\nVLLM_FLASH_ATTN_VERSION=2 VLLM_ATTENTION_BACKEND=TRITON_MLA lm_eval --model vllm --model_args pretrained=deepseek-ai/DeepSeek-V2-Lite-Chat --trust_remote_code --tasks gsm8k --num_fewshot 5 --batch_size auto\nvllm (pretrained=deepseek-ai/DeepSeek-V2-Lite-Chat,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: 5, batch_size: auto\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|\u2191  |0.0091|\u00b1  |0.0026|\n|     |       |strict-match    |     5|exact_match|\u2191  |0.0053|\u00b1  |0.0020|\n```\n\nFA2 FlashMLA on H100:\n```\nVLLM_FLASH_ATTN_VERSION=2 lm_eval --model vllm --model_args pretrained=deepseek-ai/DeepSeek-V2-Lite-Chat --trust_remote_code --tasks gsm8k --num_fewshot 5 --batch_size auto\nvllm (pretrained=deepseek-ai/DeepSeek-V2-Lite-Chat,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: 5, batch_size: auto\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|\u2191  |0.0129|\u00b1  |0.0031|\n|     |       |strict-match    |     5|exact_match|\u2191  |0.0076|\u00b1  |0.0024|\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-05-22T19:50:32+00:00",
    "closed_at": "2025-05-28T08:59:41+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18561/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18561"
  },
  {
    "number": 8204,
    "title": "[Bug]: [Errno 98] error while attempting to bind on address ('0.0.0.0', 8000): address already in use",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### \ud83d\udc1b Describe the bug\n\nThis is a bug we encounter a lot in our ci, e.g. https://buildkite.com/vllm/ci-aws/builds/8098#0191bf43-446d-411d-80c7-3ba10bc392e8/192-1557\r\n\r\nI have been tracking this for months, and try to add more logging information to help debugging.\r\n\r\nfrom the logging information:\r\n\r\n\r\n> [2024-09-05T00:38:34Z] INFO:     Started server process [60858]\r\n> --\r\n> \u00a0 | [2024-09-05T00:38:34Z] INFO:     Waiting for application startup.\r\n> \u00a0 | [2024-09-05T00:38:34Z] INFO:     Application startup complete.\r\n> \u00a0 | [2024-09-05T00:38:34Z] ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 44319): [errno 98] address already in use\r\n> \u00a0 | [2024-09-05T00:38:34Z] INFO:     Waiting for application shutdown.\r\n> \u00a0 | [2024-09-05T00:38:34Z] INFO:     Application shutdown complete.\r\n> \u00a0 | [2024-09-05T00:38:34Z] DEBUG 09-04 17:38:34 launcher.py:64] port 44319 is used by process psutil.Process(pid=60914, name='pt_main_thread', status='sleeping', started='17:37:05') launched with command:\r\n> \u00a0 | [2024-09-05T00:38:34Z] DEBUG 09-04 17:38:34 launcher.py:64] /usr/bin/python3 -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=16, pipe_handle=18) --multiprocessing-fork\r\n> \r\n> \r\n\r\nwe can see that the server process is pid 60858 , and the port 44319 is used by process 60914. scrolling up a little bit, we can find:\r\n\r\n\r\n> [2024-09-05T00:37:05Z] INFO 09-04 17:37:05 api_server.py:160] Multiprocessing frontend to use ipc:///tmp/b6851f4d-4d78-46b8-baba-ae179b0088c2 for RPC Path.\r\n> --\r\n> \u00a0 | [2024-09-05T00:37:05Z] INFO 09-04 17:37:05 api_server.py:176] Started engine process with PID 60914\r\n> \r\n\r\nit becomes clear that this is the engine process.\r\n\r\nI think the problem here, is that we only bind the port after the engine is ready. During engine setup, it might use some ports for ray, or for distributed communication.\r\n\r\nthere are two possible solutions:\r\n1. the api server immediately binds to the port after start, and returns unready status when client queries the `/healthy` endpoint\r\n2. the api server binds the port immediately (via `socket.socket(socket.AF_INET, socket.SOCK_STREAM).bind((\"\", uvicorn_kwargs[\"port\"]))`), and after engine is up, it releases the port, and bind again to serve requests\r\n\r\nI think 1 might be better. 2 would suffer from the fact that client will get 404 not found before the engine is up, because this is just a raw socket without any response.\r\n\r\ncc @robertgshaw2-neuralmagic @njhill @joerunde \r\n\r\nalso cc @richardliaw @rkooo567 how to turn on verbose ray logging, so that we can verify if the port is indeed used by ray.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-09-05T17:35:19+00:00",
    "closed_at": "2024-09-16T20:56:29+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8204/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/8204"
  },
  {
    "number": 2048,
    "title": "Mixtral generation speed performance.",
    "body": "I tried to deploy the Mixtral 7bx8 model on eight T4 GPUs, but the generation speed is only 6 tokens/s, while a 34B model achieves 14 tokens/s. \nI've heard someone mention that Mixtral 7bx8's generation performance is comparable to a 12B model, but I'm unsure what the issue might be.",
    "labels": [],
    "state": "closed",
    "created_at": "2023-12-12T05:05:28+00:00",
    "closed_at": "2023-12-14T00:32:26+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2048/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/2048"
  },
  {
    "number": 14386,
    "title": "[Usage]: Clean up Engine Args & Documentation",
    "body": "### Your current environment\n\nCurrently vLLM has a lot of engine arguments listed here https://docs.vllm.ai/en/latest/serving/engine_args.html. Over time as we add more and more features to vLLM, this list will be less maintainable and user friendly.\n\n\n\n### How would you like to use vllm\n\nAs a first step to clean up these args, they should be made **hierarchical** (for example, `--compilation-config`).\n\nThe documentation should also be updated so that engine arg documentations are **arranged in sections instead of in a flatten list**.\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "usage"
    ],
    "state": "open",
    "created_at": "2025-03-06T23:59:07+00:00",
    "closed_at": null,
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14386/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/14386"
  },
  {
    "number": 5337,
    "title": "[Bug]: non-deterministic Python gc order leads to flaky tests",
    "body": "### Your current environment\r\n\r\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI often see many flaky tests, and I think the Python gc system is one of the factor to blame.\r\n\r\nPython gc system is notoriously random. When we call `del x`, and `x`'s refcount is 0, it is not guaranteed that all resources held by `x` will be released immediately. This is true especially when `x` is a complicated object, and might contain self-reference inside it. That's one of the motivation for Python to propose the concept of context manager, to enforce some critical resource to be released immediately.\r\n\r\nTake the following code as an example:\r\n\r\n```python\r\nimport torch\r\nimport weakref\r\nimport gc\r\n\r\ndef tensor_destructed(tensor_ref):\r\n    # This function is called when the tensor is destructed.\r\n    print(f\"Tensor with id {id(tensor_ref)} is being destructed.\")\r\n\r\nclass A:\r\n    def __init__(self):\r\n        self.tensor = torch.tensor([1.0, 2.0, 3.0])\r\n\r\n    def __del__(self):\r\n        print(\"A is being destructed.\")\r\n        del self.tensor\r\n\r\ndef main():\r\n    # Create a complex object with a tensor.\r\n    data = A()\r\n    # the object is so complicated that it has a reference to itself.\r\n    data.self = data\r\n    # Attach a weak reference to the tensor with a callback for destruction.\r\n    tensor_ref = weakref.ref(data.tensor, tensor_destructed)\r\n\r\n    # Deleting the tensor manually.\r\n    print(\"before del data\")\r\n    del data\r\n    print(\"after del data, before gc collected\")\r\n    \r\n    # Manually trigger garbage collection to see the destruction callback in action.\r\n    gc.collect()\r\n    print(\"after gc collected\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n\r\nThe output is:\r\n\r\n```text\r\nbefore del data\r\nafter del data, before gc collected\r\nTensor with id 4379931520 is being destructed.\r\nA is being destructed.\r\nafter gc collected\r\n```\r\n\r\nAs it shows, the tensor is not destructed after we call `del data`. It is only destructed when we call `gc.collect()`, where Python will detect and break reference-cycles to destruct unreachable objects.\r\n\r\nThe following code in vLLM can suffer from Python gc:\r\n\r\nhttps://github.com/vllm-project/vllm/blob/388596c91437a51d428a447594e9faec340c29b2/tests/models/test_models.py#L37-L44\r\n\r\nAfter `del hf_model`, chances are GPU memory held by `hf_model` is not released yet. This will cause OOM for the later vLLM model, leading to a flaky test. I have seen it broken for many times, and sometimes retrying the test helps.\r\n\r\nAdding `import gc; gc.collect()` solves the problem. However, to avoid code duplication, it is better to wrap `hf_model` as a context manager, and to call `import gc; gc.collect()` automatically after we exit the context manager:\r\n\r\n```python\r\n   with hf_runner(model, dtype=dtype) as hf_model:\r\n        hf_outputs = hf_model.generate_greedy(example_prompts, max_tokens)\r\n```",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-06-07T05:39:26+00:00",
    "closed_at": "2024-06-08T05:31:33+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5337/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/5337"
  },
  {
    "number": 4698,
    "title": "[Performance]: benchmarking vllm copy kernel and pytorch index copy",
    "body": "### Proposal to improve performance\n\nI opened this issue to track a random idea:\r\n\r\nCurrently we have a copy kernel:\r\n\r\nhttps://github.com/vllm-project/vllm/blob/e288df0632d5bdde76c20bed8310b46d35b8e5ac/csrc/cache_kernels.cu#L214-L220\r\n\r\nEssentially this does the following vector copy:\r\n\r\n```python\r\n    key_cache_view = key_cache.reshape(-1, num_heads * head_size)\r\n    value_cache_view = value_cache.reshape(-1, num_heads * head_size)\r\n    key_view = key.reshape(-1, num_heads * head_size)\r\n    value_view = value.reshape(-1, num_heads * head_size)\r\n    key_cache_view[slot_mapping] = key_view\r\n    value_cache_view[slot_mapping] = value_view\r\n```\r\n\r\nThe caveat is, we have a special value in `slot_mapping`: `-1` means skip copying.\r\n\r\nIf possible, we can reserve a slot in block manager for padded kv, then we can just use pytorch's index copying, without maintaining a separate copy kernel ourselves.\r\n\r\nTwo TODOs:\r\n\r\n- [ ] What is the overhead of reserving a slot for padded kv in the block manager?\r\n- [ ] Does PyTorch copy kernel outperform the current hand-written one?\r\n\r\ncc @cadedaniel who knows a lot about block manager, and @WoosukKwon who knows a lot about cuda kernels.\n\n### Report of performance regression\n\n_No response_\n\n### Misc discussion on performance\n\n_No response_\n\n### Your current environment (if you think it is necessary)\n\n_No response_",
    "labels": [
      "help wanted",
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-09T02:38:35+00:00",
    "closed_at": "2024-11-28T02:05:08+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4698/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/4698"
  },
  {
    "number": 18008,
    "title": "[Feature]: Support FP8 Marlin MoE for CompressedTensorsW8A8Fp8MoEMethod",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nLike what was added in https://github.com/vllm-project/vllm/pull/16850 for enabling marlin in fp8.py MoE layers, we should enable FP8 Marlin MoE for compressed tensors models to support users wanting to run them on older hardware.\n\nBasically you want to take the changes in fp8.py's moe method (https://github.com/vllm-project/vllm/pull/16850/files#diff-5511bfcc9c53f7d96517ad43e4087f6777bef21302da983f42cafae40a866644) and apply them to `CompressedTensorsW8A8Fp8MoEMethod`\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "good first issue",
      "feature request",
      "quantization"
    ],
    "state": "closed",
    "created_at": "2025-05-12T18:02:12+00:00",
    "closed_at": "2025-05-20T11:58:40+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18008/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18008"
  },
  {
    "number": 12429,
    "title": "Flash Attention 3 (FA3) Support",
    "body": "As of https://github.com/vllm-project/vllm/pull/12093 Flash Attention 3 is now supported in vLLM for Hopper GPUs (SM 9.0).\n\nIt can also be enabled for SM 8.0 and 8.7 using `VLLM_FLASH_ATTN_VERSION=3`.\n\nFor 8.6 and 8.9 its fully disabled since they don't have enough shared memory for the current implementation, some work needs to be done here.\n\nThis issue tracks the remaining features that have yet to be implemented\n\n\n### Hardware Support\n- [ ] SM 8.9 Ada Lovelace (L4, L40s) Support \n- [ ] SM 8.6 Ampere (A6000) Support\n\n\n### Optimizations\n- [x] FP8 Attention",
    "labels": [],
    "state": "open",
    "created_at": "2025-01-25T19:48:27+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12429/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/12429"
  },
  {
    "number": 696,
    "title": "Unexpected latency of StarCoder when enable tensor parallel",
    "body": "### Discussed in https://github.com/vllm-project/vllm/discussions/666\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **zhaoyang-star** August  3, 2023</sup>\r\nThe latency of StarCoder running on 2 A100 40GB is higher than that running on 1 A100. \r\nWhile, the latency of LLaMA-13B running on 2 A100 40GB is lower than that running on 1 A100 as expected. \r\n\r\nSo does MultiQueryAttenion impl in vLLM cause this? \r\n\r\n![image](https://github.com/vllm-project/vllm/assets/24290792/ea66ad84-d8f3-4793-9970-ca70b07c13b0)\r\n![image](https://github.com/vllm-project/vllm/assets/24290792/37a8c774-1b5a-4891-86db-ec464e79f58f)\r\nNote: Prompt token length and output token length both are set to 1k.\r\n</div>",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-08-08T04:25:01+00:00",
    "closed_at": "2024-03-08T16:56:48+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/696/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/696"
  },
  {
    "number": 6887,
    "title": "[Bug]: dag teardown error AttributeError: 'Worker' object has no attribute 'core_worker'",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\ncommand:\r\n\r\n`python benchmarks/benchmark_throughput.py --input-len 100 --output-len 100 --num-prompts 100 --model facebook/opt-125m -tp 2 --distributed-executor-backend ray`\r\n\r\nerror:\r\n\r\n```text\r\n2024-07-28 22:30:36,078 INFO compiled_dag_node.py:1202 -- Tearing down compiled DAG\r\nException ignored in: <function RayGPUExecutor.__del__ at 0x7ff2ee7048b0>\r\nTraceback (most recent call last):\r\n  File \"/data/youkaichao/vllm/vllm/executor/ray_gpu_executor.py\", line 396, in __del__\r\n    self.forward_dag.teardown()\r\n  File \"/data/youkaichao/miniconda/envs/vllm/lib/python3.9/site-packages/ray/dag/compiled_dag_node.py\", line 1402, in teardown\r\n    monitor.teardown(wait=True)\r\n  File \"/data/youkaichao/miniconda/envs/vllm/lib/python3.9/site-packages/ray/dag/compiled_dag_node.py\", line 1204, in teardown\r\n    outer._dag_submitter.close()\r\n  File \"/data/youkaichao/miniconda/envs/vllm/lib/python3.9/site-packages/ray/experimental/channel/common.py\", line 383, in close\r\n    self._output_channel.close()\r\n  File \"/data/youkaichao/miniconda/envs/vllm/lib/python3.9/site-packages/ray/experimental/channel/shared_memory_channel.py\", line 629, in close\r\n    channel.close()\r\n  File \"/data/youkaichao/miniconda/envs/vllm/lib/python3.9/site-packages/ray/experimental/channel/shared_memory_channel.py\", line 512, in close\r\n    self._worker.core_worker.experimental_channel_set_error(self._writer_ref)\r\nAttributeError: 'Worker' object has no attribute 'core_worker'\r\n[1]    3100846 segmentation fault (core dumped)  python benchmarks/benchmark_throughput.py --input-len 100 --output-len 100  \r\n```\r\n\r\ncc @ruisearch42  @rkooo567 @stephanie-wang ",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-29T05:32:43+00:00",
    "closed_at": "2024-12-01T02:14:37+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6887/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/6887"
  },
  {
    "number": 3587,
    "title": "[RFC]: Interface and Abstraction for Distributed Inference Environment",
    "body": "This RFC describes a proposal for interfaces and abstractions for distributed inference environments. I plan to solicit discussions for a week (until March 31st) before I begin to actually refactor the code.\r\n\r\n# Motivation\r\n\r\nThe current distributed inference environment in `vllm` is quite tangled, and we often see deadlocks and hangs (see https://github.com/vllm-project/vllm/issues/3455 , https://github.com/vllm-project/vllm/issues/2770 , https://github.com/vllm-project/vllm/issues/3559 , to name a few). The problem becomes prominent when we try to upgrade to pytorch 2.2.0 (see https://github.com/vllm-project/vllm/pull/3442 , https://github.com/vllm-project/vllm/pull/3442 ), because `pytorch 2.2.0` upgrades from `nccl==2.18.1` to `2.19.3` (see https://pypi.org/pypi/torch/2.1.2/json and https://pypi.org/pypi/torch/2.2.0/json to compare the dependency), and `nccl==2.19.3` breaks `vllm` due to increased memory cost during cudagraph capture (from 10MB per graph to 100MB per graph, adds up to several GBs because we have dozens of cudagraph).\r\n\r\nTL,DR; distributed inference in current codebase is a headache. If it works, hooray; if not, don't be surprised.\r\n\r\n# Proposal\r\n\r\n## Abstraction\r\n\r\nI think we should have three levels of abstraction:\r\n\r\n1. Launcher, responsible for launching processes (potentially across multi-node). Currently it is `ray`, but we can also have another choices like Python's native `multiprocessing` in single-node cases. See https://github.com/vllm-project/vllm/pull/3466 for example.\r\n2. Coordinator, responsible for coordinating shared resources (e.g. filesystem usage) and broadcasting some messages. Currently we don't have this, and there are lots of hacks for ad-hoc implementation, e.g. use `filelock` to lock on filesystems ( https://github.com/vllm-project/vllm/pull/3578 ), use `TCP` to initialize communication in `cupy` ( https://github.com/vllm-project/vllm/pull/2811 ), use `MPI` to initialize communication in AMD's `cupy` version ( https://github.com/vllm-project/vllm/pull/3123 ).\r\n3. Communicator, responsible for cross-device communication of large tensor data (e.g. perform allreduce). Currently we support `nccl`, and AMD also has its own communication library. Note that this is vendor-specific, and vendors usually have their own way of cross-device communication.\r\n\r\nThe most messy one, and **the missing one, is the Coordinator abstraction level**. More on this later.\r\n\r\n## Interface\r\n\r\nBetween each consecutive abstractions, lies the interface. \r\n\r\n### Interface between Launcher and Coordinator\r\n\r\nAfter Launcher launches processes, it needs to at least tell the processes the following information: \r\n- `launch_id`, used to distinguish current launch with possibly concurrent launch (e.g. when 4 users want to set up 4 inference engines in the same node, each with 2 GPUs). Note: the `launch_id` can be used as a \"random seed\" to draw values for `master_port`, instead of keeping only one default `master_port` value and having to kill all processes after the last run crashes. A reference implementation would be hashing the `launch_id` to a port number, and increasing the port number to find the first free port. This is a strategy taken by [Jupyter Notebook/Lab Server](https://github.com/jupyter/) .\r\n- `world_size`, number of processes participating in the current launch (may span over multiple nodes)\r\n- `local_world_size`, number of processes participating in the current launch **in the current node** (not necessarily the same across nodes)\r\n- `rank`, range from 0 (inclusive) to `world_size` (exclusive) , unique in the launch for each process\r\n- `local_rank`, range from 0 (inclusive) to `local_world_size` (exclusive), unique in each node, **can use this to assign devices in a node!**\r\n- `master_addr`, the IP address of the master node, should be reachable from all nodes\r\n- `master_port`, a free port in the master node, reserved for possible coordination\r\n- other custom information can be added, but the above are required.\r\n\r\nHow does Launcher pass these information to each process? Basically we have two choices:\r\n1. through environment variables, the simplest way, but will disable the usage of thread-level distributed inference because environment variables are shared within threads in one process. (However, thread-level distributed inference seems rare. Do we need to consider this?)\r\n2. through serialization and deserialization (e.g. passing bytes in a shared object store), the most general way, at the cost of complexity and runtime efficiency to design and execute the serialization/deserialization\r\n\r\n### Interface between Coordinator and Communicator\r\n\r\nDevice communicators (e.g. `nccl`) often need to initialize the communication by sharing some unique token (see [`nccl` documentation](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html)). In addition, processes sometimes need to coordinate the resource in a node or across the cluster.\r\n\r\nIn sight of the above consideration, `Coordinator` should at least have the following interfaces:\r\n\r\n1. `is_master()`: tell if the current process is a master process, i.e. convenient wrapper for boilerplate code `rank == 0`\r\n2. `is_local_master()`: tell if the current process is a **local** master process, i.e. convenient wrapper for boilerplate code `local_rank == 0`\r\n3. `broadcast(bytes, src)`: broadcast some message (in the form of `bytes`) from rank `src` to all the processes. The semantic is standard, no need for more explanation.\r\n4. `barrier()`: block until all processes reaches here. Also standard communication primitive.\r\n\r\nNote: very often than not, we want to execute something in just one process per node (e.g. creating directories, downloading files to the node). Inspired by [this thread](https://discuss.pytorch.org/t/torch-distributed-barrier-used-in-multi-node-distributed-data-parallel-training/89711/12), we can write code like this:\r\n\r\n```python\r\nif is_local_master():\r\n    do_something() # download file, create directory, etc.\r\nbarrier()\r\n```\r\n\r\nFurthermore, there are more complicated requirements like  \"only one process in each node does something, but this something is different across nodes\", essentially the requirement of `local_barrier()`, a function that block until all processes **in the current node** reaches here. It is debatable if we want this (currently I don't see any requirements like this in `vllm`.)\r\n\r\n### Communicator interface\r\n\r\nThe following functionality of communicator is suggested (mostly taken from the `nccl` design):\r\n\r\n1. the master process get unique token to identify the communication group\r\n2. the master process broadcast unique token to all ranks\r\n3. each process initializes communication by the unique token and their rank, world_size\r\n4. an in-place allreduce function: `allreduce(char* input, size_t count, size_t dtype, size_t op)`. More functionality would be better (e.g. out-of-place allreduce, broadcast/reduce/scatter etc.), but inplace allreduce is all we need currently.\r\n\r\nThe intended usage would be something like this:\r\n\r\n```python\r\n# inside each process\r\ncoor = Coordinator(); # initialize Coordinator, done by vllm\r\ncomm = Communicator(coor) # hardware vendor can use `coor` to initialize their communicator\r\ndata = torch.tensor((1024, 1024)).to(device=f\"xpu:{coor.local_rank}\")\r\ncomm.allreduce(data) # hardware vendor can access the raw data via pytorch's [`Tensor.data_ptr`](https://pytorch.org/docs/stable/generated/torch.Tensor.data_ptr.html) mechanism.\r\n# please implement Communicator.__del__ to destroy communicator, so that programs can exit gracefully \r\n```\r\n\r\n## A reference implementation of Coordinator\r\n\r\nA reference implementation of Coordinator can be `torch.distributed`, with the `gloo` backend designed to communicate CPU tensors. \r\n\r\nOther considerations include MPI and custom-implemented TCP store. However, since we live in `torch` framework, `torch.distributed` is a natural choice without any new dependency.\r\n\r\nNote: `torch.distributed` can also be used as a fully functional communicator for GPU devices. However, `torch.distributed.all_reduce` is way more complicated than just an allreduce operation. It might initialize autograd engine, might keep track of gradients, might dispatch to different device kernels. Even if we are in [`torch.inference_mode`](https://pytorch.org/docs/stable/generated/torch.inference_mode.html), its `c10` engine might perform some additional operations that fails functionalities like cudagraph. Therefore, I prefer to call vendor-provided communication libraries directly to bypass the problem. After all, we just want an allreduce operation on dense tensors, without any hustle and bustle.\r\n\r\n# Benefits\r\n\r\nAfter we have the above abstraction and interface, we can have the following benefits:\r\n- We are always in a distributed environment, just with different sizes of wold_size. Distributed concerns will always be considered, so that we can easily scale to multi-node environments (if any LLM needs this).\r\n- Hardware vendors can plug in their communication libraries very easily. All they need to provide are: integration into pytorch `torch.Tensor` (only forward computation ops are enough), a c library (an .so file would be enough) for calling communication ops with raw data (i.e. `char*` in c). And if they want to move quickly, just one `allreduce` op would be enough for inference. No need to wait for the whole functionality completed within pytorch.\r\n\r\n# Things not to be considered\r\n\r\nWe don't aim for a fully-fledged distributed execution environment. And since inference tasks are almost stateless, we don't need to consider elasticness and fault-tolerance. As opposed to training, we don't need to save checkpoints, we don't need to resume from previous failure ... ",
    "labels": [
      "RFC",
      "misc"
    ],
    "state": "closed",
    "created_at": "2024-03-23T23:41:40+00:00",
    "closed_at": "2024-06-14T01:00:32+00:00",
    "comments": 18,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3587/reactions",
      "total_count": 10,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 10,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/3587"
  },
  {
    "number": 22,
    "title": "Tensor Parallel profiling result",
    "body": "Will update the profiling results in this PR.\r\n\r\n## BS=8, input_len=32, output_len=128\r\n\r\n```\r\nOPT-13B\r\nTP 1: 3.5404738585154214 seconds\r\nTP 2: 4.742188215255737 seconds\r\nTP 4: 4.907034238179524 seconds\r\n\r\nOPT-30B\r\nTP 1: OOM\r\nTP 2: 5.9848620891571045 seconds\r\nTP 4: 5.943212985992432 seconds\r\n```",
    "labels": [],
    "state": "closed",
    "created_at": "2023-04-02T06:50:04+00:00",
    "closed_at": "2023-06-16T02:38:15+00:00",
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/22/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/22"
  },
  {
    "number": 1149,
    "title": "The First vLLM SF Bay Area Meetup",
    "body": "We are excited to announce that we will hold the first vLLM SF bay area meetup at 6pm on 10/5 (Thu). The vLLM team will give a deep dive on vLLM and share the future plans and roadmaps of the project. We will also have our users and contributors come and share their experiences. You can find the event details and RSVP [here](https://lu.ma/first-vllm-meetup). Please join us if you are around the bay area. Let's come together, share our stories, and envision the future of vLLM.\r\n",
    "labels": [],
    "state": "closed",
    "created_at": "2023-09-22T18:38:22+00:00",
    "closed_at": "2023-10-08T04:10:22+00:00",
    "comments": 8,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/1149/reactions",
      "total_count": 6,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 6,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/1149"
  },
  {
    "number": 6038,
    "title": "[Bug]: Speculative decoding does not respect per-request seed",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.5\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) [GCC 12.3.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-107-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 1: NVIDIA H100 80GB HBM3\r\nGPU 2: NVIDIA H100 80GB HBM3\r\nGPU 3: NVIDIA H100 80GB HBM3\r\nGPU 4: NVIDIA H100 80GB HBM3\r\nGPU 5: NVIDIA H100 80GB HBM3\r\nGPU 6: NVIDIA H100 80GB HBM3\r\nGPU 7: NVIDIA H100 80GB HBM3\r\n\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             192\r\nOn-line CPU(s) list:                0-191\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8474C\r\nCPU family:                         6\r\nModel:                              143\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 48\r\nSocket(s):                          2\r\nStepping:                           8\r\nCPU max MHz:                        3800.0000\r\nCPU min MHz:                        800.0000\r\nBogoMIPS:                           4200.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          4.5 MiB (96 instances)\r\nL1i cache:                          3 MiB (96 instances)\r\nL2 cache:                           192 MiB (96 instances)\r\nL3 cache:                           195 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-47,96-143\r\nNUMA node1 CPU(s):                  48-95,144-191\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.9.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] transformers==4.41.2\r\n[pip3] triton==2.3.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] transformers              4.41.2                   pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tPIX\tPIX\t0-47,96-143\t0\t\tN/A\r\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\r\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\r\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\r\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tSYS\tSYS\t48-95,144-191\t1\t\tN/A\r\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tSYS\tSYS\t48-95,144-191\t1\t\tN/A\r\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tSYS\tSYS\t48-95,144-191\t1\t\tN/A\r\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tSYS\tSYS\t48-95,144-191\t1\t\tN/A\r\nNIC0\tPIX\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPIX\t\t\t\t\r\nNIC1\tPIX\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPIX\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n```\r\n\n\n### \ud83d\udc1b Describe the bug\n\nSpeculative decoding currently does not request the per-request seed. This bug was discovered by @jvlunteren while performing performance evaluation for speculative decoding under different kinds of workloads. \r\n\r\nIt can be reproduced by starting a server with:\r\n```bash\r\npython3 -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-2-13b-chat-hf --speculative_model https://huggingface.co/ibm-fms/llama-13b-accelerator --use-v2-block-manager\r\n```\r\nthen sending a couple of requests with:\r\n```python\r\nfrom openai import OpenAI\r\n\r\n# Modify OpenAI's API key and API base to use vLLM's API server.\r\nopenai_api_key = \"EMPTY\"\r\nopenai_api_base = \"http://localhost:8000/v1\"\r\n\r\nclient = OpenAI(\r\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\r\n    api_key=openai_api_key,\r\n    base_url=openai_api_base,\r\n)\r\n\r\nmodels = client.models.list()\r\nmodel = models.data[0].id\r\n\r\n# Completion API\r\nstream = True\r\ncompletion = client.completions.create(\r\n    model=model,\r\n    prompt=\"A robot may not injure a human being\",\r\n    echo=False,\r\n    temperature=2.0,\r\n    max_tokens=10,\r\n    stream=stream,\r\n    seed=42,\r\n)\r\n\r\nprint(\"Completion results:\")\r\nif stream:\r\n    for c in completion:\r\n        print(c)\r\nelse:\r\n    print(completion)\r\n ```\r\n The output will be different each time. \r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-07-01T17:11:21+00:00",
    "closed_at": "2024-07-19T02:22:09+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6038/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/6038"
  },
  {
    "number": 18592,
    "title": "[Bug[Failing Test]: Entrypoints Test - entrypoints/openai/test_transcription_validation.py",
    "body": "### Your current environment\n\nN/A\n\n### \ud83d\udc1b Describe the bug\n\n```[2025-05-23T04:39:51Z] Traceback (most recent call last):\n[2025-05-23T04:39:51Z]   File \"/usr/local/bin/vllm\", line 10, in <module>\n[2025-05-23T04:39:51Z]     sys.exit(main())\n[2025-05-23T04:39:51Z]              ^^^^^^\n[2025-05-23T04:39:51Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/main.py\", line 53, in main\n[2025-05-23T04:39:51Z]     args.dispatch_function(args)\n[2025-05-23T04:39:51Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/serve.py\", line 40, in cmd\n[2025-05-23T04:39:51Z]     uvloop.run(run_server(args))\n[2025-05-23T04:39:51Z]   File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\n[2025-05-23T04:39:51Z]     return __asyncio.run(\n[2025-05-23T04:39:51Z]            ^^^^^^^^^^^^^^\n[2025-05-23T04:39:51Z]   File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n[2025-05-23T04:39:51Z]     return runner.run(main)\n[2025-05-23T04:39:51Z]            ^^^^^^^^^^^^^^^^\n[2025-05-23T04:39:51Z]   File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n[2025-05-23T04:39:51Z]     return self._loop.run_until_complete(task)\n[2025-05-23T04:39:51Z]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-23T04:39:51Z]   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n[2025-05-23T04:39:51Z]   File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n[2025-05-23T04:39:51Z]     return await main\n[2025-05-23T04:39:51Z]            ^^^^^^^^^^\n[2025-05-23T04:39:51Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1324, in run_server\n[2025-05-23T04:39:51Z]     async with build_async_engine_client(args) as engine_client:\n[2025-05-23T04:39:51Z]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-23T04:39:51Z]   File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n[2025-05-23T04:39:51Z]     return await anext(self.gen)\n[2025-05-23T04:39:51Z]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-23T04:39:51Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 153, in build_async_engine_client\n[2025-05-23T04:39:51Z]     async with build_async_engine_client_from_engine_args(\n[2025-05-23T04:39:51Z]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-23T04:39:51Z]   File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n[2025-05-23T04:39:51Z]     return await anext(self.gen)\n[2025-05-23T04:39:51Z]            ^^^^^^^^^^^^^^^^^^^^^\n[2025-05-23T04:39:51Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 173, in build_async_engine_client_from_engine_args\n[2025-05-23T04:39:51Z]     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n[2025-05-23T04:39:51Z]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-23T04:39:51Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\", line 992, in create_engine_config\n[2025-05-23T04:39:51Z]     if try_v1 and self._is_v1_supported_oracle(model_config):\n[2025-05-23T04:39:51Z]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-05-23T04:39:51Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\", line 1304, in _is_v1_supported_oracle\n[2025-05-23T04:39:51Z]     _raise_or_fallback(feature_name=f\"--task {model_config.task}\",\n[2025-05-23T04:39:51Z]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\", line 1584, in _raise_or_fallback\n[2025-05-23T04:39:51Z]     raise NotImplementedError(\n[2025-05-23T04:39:51Z] NotImplementedError: VLLM_USE_V1=1 is not supported with --task transcription.\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "ci-failure"
    ],
    "state": "closed",
    "created_at": "2025-05-23T05:33:05+00:00",
    "closed_at": "2025-05-23T12:51:54+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/18592/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/18592"
  },
  {
    "number": 4694,
    "title": "[Feature]: bind python and c++ through tools other than pybind11",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\nAs vLLM goes into a fast release schedule (currently one release every two weeks), we will quickly hit the project-wide limit of pypi (around 5GB per project). One solution, as pointed out in https://github.com/pypi/support/issues/3792#issuecomment-2099941677 , is to build one wheel for all python versions (Python 3.8+).\r\n\r\nI have figured out the procedure https://github.com/pypi/support/issues/3792#issuecomment-2101360740 , but pybind11 does not support this Python Limited API protocol.\r\n\r\nOne possible solution is to replace pybind11 with some other tools, so that the binding procedure can be used with Python Limited API.\r\n\r\nPossible solutions:\r\n\r\n- Nanobind (seems to support it starting from Python 3.12 only: https://github.com/wjakob/nanobind/pull/561 )\r\n- register ops through pytorch directly https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_",
    "labels": [
      "help wanted",
      "feature request",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-05-08T23:18:56+00:00",
    "closed_at": "2024-10-27T22:53:29+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4694/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/4694"
  },
  {
    "number": 7309,
    "title": "[Bug][0.5.4] Front-end server errors when overloaded with pending requests",
    "body": "### Your current environment\r\n\r\nThe output of `python collect_env.py`:\r\n\r\n<details>\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-107-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.5.82\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nGPU 1: NVIDIA A100-SXM4-80GB\r\nGPU 2: NVIDIA A100-SXM4-80GB\r\nGPU 3: NVIDIA A100-SXM4-80GB\r\nGPU 4: NVIDIA A100-SXM4-80GB\r\nGPU 5: NVIDIA A100-SXM4-80GB\r\nGPU 6: NVIDIA A100-SXM4-80GB\r\nGPU 7: NVIDIA A100-SXM4-80GB\r\n\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             128\r\nOn-line CPU(s) list:                0-127\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8462Y+\r\nCPU family:                         6\r\nModel:                              143\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 32\r\nSocket(s):                          2\r\nStepping:                           8\r\nBogoMIPS:                           5600.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          3 MiB (64 instances)\r\nL1i cache:                          2 MiB (64 instances)\r\nL2 cache:                           128 MiB (64 instances)\r\nL3 cache:                           120 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126\r\nNUMA node1 CPU(s):                  1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.0\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.4\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV12    NV12    NV12    NV12    NV12    NV12    NV12    PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0,2,4,6,8,10    0               N/A\r\nGPU1    NV12     X      NV12    NV12    NV12    NV12    NV12    NV12    SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     0,2,4,6,8,10    0               N/A\r\nGPU2    NV12    NV12     X      NV12    NV12    NV12    NV12    NV12    SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     0,2,4,6,8,10    0               N/A\r\nGPU3    NV12    NV12    NV12     X      NV12    NV12    NV12    NV12    SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     0,2,4,6,8,10    0               N/A\r\nGPU4    NV12    NV12    NV12    NV12     X      NV12    NV12    NV12    SYS     SYS     SYS     SYS     PXB     SYS     SYS     SYS     1,3,5,7,9,11    1               N/A\r\nGPU5    NV12    NV12    NV12    NV12    NV12     X      NV12    NV12    SYS     SYS     SYS     SYS     SYS     PXB     SYS     SYS     1,3,5,7,9,11    1               N/A\r\nGPU6    NV12    NV12    NV12    NV12    NV12    NV12     X      NV12    SYS     SYS     SYS     SYS     SYS     SYS     PXB     SYS     1,3,5,7,9,11    1               N/A\r\nGPU7    NV12    NV12    NV12    NV12    NV12    NV12    NV12     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB     1,3,5,7,9,11    1               N/A\r\nNIC0    PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC1    SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC2    SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS\r\nNIC3    SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS\r\nNIC4    SYS     SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS\r\nNIC5    SYS     SYS     SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS\r\nNIC6    SYS     SYS     SYS     SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS\r\nNIC7    SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n```\r\n\r\n</details>\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nIt seems the front-end server can easily get overloaded when there are many pending requests (>1000 seems to roughly be the threshold).\r\n\r\nIndividual requests over the threshold being failing with:\r\n```\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/responses.py\", line 265, in __call__\r\n    await wrap(partial(self.listen_for_disconnect, receive))\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/responses.py\", line 261, in wrap\r\n    await func()\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/responses.py\", line 238, in listen_for_disconnect\r\n    message = await receive()\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 553, in receive\r\n    await self.message_event.wait()\r\n  File \"/usr/lib/python3.10/asyncio/locks.py\", line 214, in wait\r\n    await fut\r\nasyncio.exceptions.CancelledError: Cancelled by cancel scope 7f6028dc8ee0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 399, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/applications.py\", line 123, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n    raise exc\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/routing.py\", line 756, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/routing.py\", line 776, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/routing.py\", line 297, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/routing.py\", line 77, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/routing.py\", line 75, in app\r\n    await response(scope, receive, send)\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/responses.py\", line 258, in __call__\r\n    async with anyio.create_task_group() as task_group:\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 680, in __aexit__\r\n    raise BaseExceptionGroup(\r\nexceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\r\n``` \r\n\r\nThis quickly fills the server's output as it throw an exception for each request.\r\n\r\n## Steps to replicate\r\n\r\nServer command:\r\n```\r\nvllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --disable-log-requests\r\n```\r\n\r\nBenchmark command (needs more than 1000 pending prompts to trigger):\r\n```\r\npython benchmarks/benchmark_serving.py --model meta-llama/Meta-Llama-3.1-8B-Instruct --dataset-name random --port 8000 --num-prompts 2000\r\n```\r\n\r\nNOTE: The backend engine seems to continue running fine, it is just new requests throw exceptions in the front-end\r\n```\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/starlette/responses.py\", line 258, in __call__\r\n    async with anyio.create_task_group() as task_group:\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 680, in __aexit__\r\n    raise BaseExceptionGroup(\r\nexceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\r\nINFO 08-08 16:36:33 metrics.py:406] Avg prompt throughput: 10724.5 tokens/s, Avg generation throughput: 651.8 tokens/s, Running: 77 reqs, Swapped: 0 reqs, Pending: 941 reqs, GPU KV cache usage: 19.3%, CPU KV cache usage: 0.0%.\r\nINFO 08-08 16:36:38 metrics.py:406] Avg prompt throughput: 4585.6 tokens/s, Avg generation throughput: 480.5 tokens/s, Running: 96 reqs, Swapped: 0 reqs, Pending: 920 reqs, GPU KV cache usage: 24.3%, CPU KV cache usage: 0.0%.\r\nINFO 08-08 16:36:43 metrics.py:406] Avg prompt throughput: 9370.5 tokens/s, Avg generation throughput: 1124.9 tokens/s, Running: 95 reqs, Swapped: 0 reqs, Pending: 875 reqs, GPU KV cache usage: 24.0%, CPU KV cache usage: 0.0%.\r\nINFO 08-08 16:36:48 metrics.py:406] Avg prompt throughput: 8883.0 tokens/s, Avg generation throughput: 1006.5 tokens/s, Running: 90 reqs, Swapped: 0 reqs, Pending: 833 reqs, GPU KV cache usage: 22.7%, CPU KV cache usage: 0.0%.\r\nINFO 08-08 16:36:53 metrics.py:406] Avg prompt throughput: 9190.6 tokens/s, Avg generation throughput: 1026.0 tokens/s, Running: 92 reqs, Swapped: 0 reqs, Pending: 790 reqs, GPU KV cache usage: 23.3%, CPU KV cache usage: 0.0%.\r\n```\r\n\r\nRunning with `--disable-frontend-multiprocessing` or downgrading to `v0.5.3` will resolve the issue. ",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-08T17:12:34+00:00",
    "closed_at": "2024-12-08T02:10:45+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7309/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/7309"
  },
  {
    "number": 7116,
    "title": "[Performance]: From SequenceGroup-native code to Sequence-native code",
    "body": "### Proposal to improve performance\n\nWe have two concepts in vLLM:\r\n\r\n- SequenceGroup, a group of sequence, that originates from the same request. In most usecases, a sequence group contains only one sequence. In parallel sampling, a request can fork into many sequences, depending on the sampling parameter `n`. In beam search, sequences in the sequence group can change, grow, die.\r\n- Sequence, consists of a sequence seen by the inference engine. It has prompt, generated tokens, kv cache...\r\n\r\nIn order to support diverse sampling algorithms, vLLM currently takes a SequenceGroup-native approach: many functions operate in the SequenceGroup-level, e.g. `prepare_input` takes in a list of `SequenceGroup`.\r\n\r\nThe problem is, many functions in an inference engine, naturally fit into Sequence-level operations. For example, when we talk about the batchsize for decoding, it is the number of Sequence we are running for decoding, not the number of SequenceGroup.\r\n\r\nTo fill in the gap, there are many code in vLLM, that receives SequenceGroup, and unpack the SequenceGroup into Sequence for further operations. Notably, prepare input:\r\n\r\nhttps://github.com/vllm-project/vllm/blob/825b044863a8e3af82a82a80cd2617486cc829ca/vllm/worker/model_runner.py#L507-L510\r\n\r\nThis turns out to be very inefficient, makes the code difficult to read/maintain.\r\n\r\nTo have a rough impression about how inefficient these conversion can be, take a look at https://github.com/vllm-project/vllm/pull/7051 , where simply removing some `get_seqs` call in `SequenceGroup`, can lead to 1% end-to-end throughput gain.\r\n\r\nPer the discussion in https://github.com/vllm-project/vllm/issues/6226 , we will not directly drop beam search support. However, we should figure out a way to support it, without hurting the performance of majority usecase.\r\n\r\nThe proposal I want to discuss, is to move the vLLM code into a Sequence-native approach. It is inspired by the [lightllm approach](https://github.com/ModelTC/lightllm/blob/main/lightllm/server/router/model_infer/infer_batch.py):\r\n\r\n- each request will have a request id, a sequence group id\r\n- a sequence in the sequence group, will have a sequence group id, and a sequence id\r\n- there will be a global mapping `Dict[int, List[int]]`, maps the sequence group id to the ids of sequences inside the group, **only for a sequence group with parallel sampling or beam search**\r\n\r\nAll functions that operate on the Sequence level (mainly the model runner part), will natively receive a list of Sequence. They don't need to unpack `SequenceGroup` any more.\r\n\r\nFor some functions that operate on the SequenceGroup level (mainly the scheduler logic for gang-scheduling a sequence group, and the output processor logic that creates/removes sequence in the group), they have to reconstruct the sequence group from given list of sequence, leveraging the global mapping. Note that, **an important optimization, is we can skip all the sequence group logic, when we find the global mapping is empty, meaning that we don't have any parallel sampling or beam search**.\r\n\r\nWhen we do have parallel sampling or beam search, this will incur some performance drop. However, with the greatly simplified code in the model runner, we can expect the other part of vLLM can be greatly accelerated. Therefore, beam search or parallel sampling can also be faster in the end of the day.\r\n\r\nAn example benefit, is that this function can be greatly simplified ( we can return early):\r\n\r\nhttps://github.com/vllm-project/vllm/blob/825b044863a8e3af82a82a80cd2617486cc829ca/vllm/engine/output_processor/single_step.py#L82\n\n### Report of performance regression\n\n_No response_\n\n### Misc discussion on performance\n\n_No response_\n\n### Your current environment (if you think it is necessary)\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n",
    "labels": [
      "performance",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-08-04T00:54:31+00:00",
    "closed_at": "2024-12-04T02:07:32+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7116/reactions",
      "total_count": 4,
      "+1": 4,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/7116"
  },
  {
    "number": 9168,
    "title": "[Bug]: vLLM OpenAI-api server `/docs` endpoint fails to load",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-35-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.5.82\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 1: NVIDIA H100 80GB HBM3\r\nGPU 2: NVIDIA H100 80GB HBM3\r\nGPU 3: NVIDIA H100 80GB HBM3\r\nGPU 4: NVIDIA H100 80GB HBM3\r\nGPU 5: NVIDIA H100 80GB HBM3\r\nGPU 6: NVIDIA H100 80GB HBM3\r\nGPU 7: NVIDIA H100 80GB HBM3\r\n\r\nNvidia driver version: 555.42.02\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             128\r\nOn-line CPU(s) list:                0-127\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8462Y+\r\nCPU family:                         6\r\nModel:                              143\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 32\r\nSocket(s):                          2\r\nStepping:                           8\r\nCPU max MHz:                        4100.0000\r\nCPU min MHz:                        800.0000\r\nBogoMIPS:                           5600.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hfi vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          3 MiB (64 instances)\r\nL1i cache:                          2 MiB (64 instances)\r\nL2 cache:                           128 MiB (64 instances)\r\nL3 cache:                           120 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-31,64-95\r\nNUMA node1 CPU(s):                  32-63,96-127\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.77\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.2\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS     0-31,64-95      0               N/A\r\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-31,64-95      0               N/A\r\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     0-31,64-95      0               N/A\r\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     0-31,64-95      0               N/A\r\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    32-63,96-127    1               N/A\r\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    32-63,96-127    1               N/A\r\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    32-63,96-127    1               N/A\r\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     32-63,96-127    1               N/A\r\nNIC0    PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    SYS     SYS     SYS     SYS\r\nNIC1    NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    SYS     SYS     SYS     SYS\r\nNIC2    NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    SYS     SYS     SYS     SYS\r\nNIC3    NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS\r\nNIC4    SYS     SYS     SYS     SYS     PIX     NODE    NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE\r\nNIC5    SYS     SYS     SYS     SYS     NODE    PIX     NODE    NODE    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE\r\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    PIX     NODE    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE\r\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  ```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nSimply run `vllm serve facebook/opt-125m --enforce-eager` and navigate to `http://localhost:8000/docs`\r\n\r\nI can replicate this on the last several releases and using latest main:\r\n<img width=\"978\" alt=\"Screenshot 2024-10-08 at 4 58 52\u202fPM\" src=\"https://github.com/user-attachments/assets/7ffca9a0-cee1-41c0-b5f6-d9067c6b00ad\">\r\n\r\n\r\nThe last release where the endpoint works properly is `vllm==0.5.4`. The following image is the expected result on page load.\r\n<img width=\"978\" alt=\"Screenshot 2024-10-08 at 5 00 28\u202fPM\" src=\"https://github.com/user-attachments/assets/e0384d39-432b-4dc9-bc29-bc5d543031fa\">\r\n\r\nIn both cases, the server reports that the GET requests were successful:\r\n```\r\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\r\nINFO:     127.0.0.1:49758 - \"GET /docs HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:49758 - \"GET /openapi.json HTTP/1.1\" 200 OK\r\n```\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-08T21:03:03+00:00",
    "closed_at": "2025-01-17T02:04:04+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9168/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "MEMBER",
    "html_url": "https://github.com/vllm-project/vllm/issues/9168"
  }
]
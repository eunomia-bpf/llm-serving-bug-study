[
  {
    "number": 6890,
    "title": "[Bug]: Vllm api server does not receive supported parameter `truncate_prompt_tokens`",
    "body": "### Your current environment\r\n\r\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI used the openai compatible server deployed with vllm:\r\n```bash\r\npython -m vllm.entrypoints.openai.api_server --model meta-llama/Meta-Llama-3-8B-Instruct--host 127.0.0.1 --port 8077 --enforce-eager --gpu-memory-utilization 0.8 --swap-space 32\r\n```\r\nWhen I send a request with the following snippet (openai client):\r\n```python\r\nopenai_api_key=\"EMPTY\"\r\nopenai_api_base=\"http://localhost:8077/v1\"\r\nfrom openai import OpenAI\r\nclient = OpenAI(\r\n        api_key=openai_api_key,\r\n        base_url=openai_api_base,\r\n)\r\nmodels = client.models.list()\r\nmodel = models.data[0].id\r\nclient.chat.completions.create(\r\n    messages=[\r\n            {\r\n            \"role\": \"user\",\r\n            \"content\": \"How are you today?\"\r\n            },\r\n        ],\r\n        model=model,\r\n        max_tokens=128,\r\n        temperature=0.0,\r\n        seed=42,\r\n        extra_body=dict(\r\n            truncate_prompt_tokens=1792,\r\n        )\r\n    )\r\n```\r\nI got the error:\r\n```\r\nin _request\r\n    raise self._make_status_error_from_response(err.response) from None\r\nopenai.BadRequestError: Error code: 400 - {'object': 'error', 'message': \"[{'type': 'extra_forbidden', 'loc': ('body', 'truncate_prompt_tokens'), 'msg': 'Extra inputs are not permitted', 'input': 1792}]\", 'type': 'BadRequestError', 'param': None, 'code': 400}\r\n```\r\n\r\nThe following code, however, works:\r\n```python\r\nopenai_api_key=\"EMPTY\"\r\nopenai_api_base=\"http://localhost:8077/v1\"\r\nfrom openai import OpenAI\r\nclient = OpenAI(\r\n        api_key=openai_api_key,\r\n        base_url=openai_api_base,\r\n)\r\nmodels = client.models.list()\r\nmodel = models.data[0].id\r\nclient.chat.completions.create(\r\n    messages=[\r\n            {\r\n            \"role\": \"user\",\r\n            \"content\": \"How are you today?\"\r\n            },\r\n        ],\r\n        model=model,\r\n        max_tokens=128,\r\n        temperature=0.0,\r\n        seed=42,\r\n        extra_body=dict(\r\n            min_tokens=20, # replace with another random extra parameter\r\n        )\r\n    )\r\n```\r\n\r\nI wonder why in https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#extra-parameters, `truncate_prompt_tokens` is supported but I am getting the error here ?\r\n\r\n",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-07-29T05:56:58+00:00",
    "closed_at": "2024-12-01T02:14:32+00:00",
    "comments": 16,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/6890/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/6890"
  },
  {
    "number": 9184,
    "title": "[Bug]: qwen2.5 function calling,ChatLanguageModel is ok, but in StreamingChatLanguageModel,the logger report error",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n![image](https://github.com/user-attachments/assets/039317df-453c-47e2-a449-297df451503b)\r\n\n\n### \ud83d\udc1b Describe the bug\n\nqwen2.5 function calling,ChatLanguageModel is ok, but in StreamingChatLanguageModel,the logger report error\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-09T07:34:57+00:00",
    "closed_at": "2025-02-07T01:59:36+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9184/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9184"
  },
  {
    "number": 14513,
    "title": "[Usage]: How to improve concurrent processing capacity",
    "body": "### Your current environment\n\nvllm version: 0.6.1.post2\n\nWhen I was testing the performance at 200 concurrent users, I found that vLLM can handle up to 100 requests at most.\n![Image](https://github.com/user-attachments/assets/941c2fcf-30fd-4e67-a74f-46a43cdfa571)\nAfter each request is processed, a new request will be added, but the maximum number of requests is 100.\n\nBelow is my startup script.\n![Image](https://github.com/user-attachments/assets/8f9857b9-81e5-401b-891d-8cae195733ba)\n\nSo\uff0cmy question is how to increase the number of concurrent connections from 100 to 200 or more\uff1f\n\nThank you\u3002\n\n### How would you like to use vllm\n\nI want to increase the number of concurrent connections from 100 to 200 or more.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-03-09T08:47:52+00:00",
    "closed_at": "2025-03-10T03:55:45+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/14513/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/14513"
  },
  {
    "number": 8654,
    "title": "[Bug]: RuntimeError in gptq_marlin_24_gemm",
    "body": "### Your current environment\r\n\r\npython 3.8\r\nL20*4\r\nvllm 0.5.4\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\n$python -m vllm.entrypoints.api_server --model='/mntfn/yanyi/Qwen2-7B-Instruct_24_w4a16/stage_quantization' --max-model-len=16000 --tensor-parallel-size=4 --use-v2-block-manager  --enable-prefix-caching \r\n\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/engine/async_llm_engine.py\", line 735, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/engine/async_llm_engine.py\", line 631, in __init__\r\n[rank0]:     self.engine = self._init_engine(*args, **kwargs)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/engine/async_llm_engine.py\", line 830, in _init_engine\r\n[rank0]:     return engine_class(*args, **kwargs)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/engine/async_llm_engine.py\", line 267, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/engine/llm_engine.py\", line 283, in __init__\r\n[rank0]:     self._initialize_kv_caches()\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/engine/llm_engine.py\", line 389, in _initialize_kv_caches\r\n[rank0]:     self.model_executor.determine_num_available_blocks())\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/executor/distributed_gpu_executor.py\", line 38, in determine_num_available_blocks\r\n[rank0]:     num_blocks = self._run_workers(\"determine_num_available_blocks\", )\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 192, in _run_workers\r\n[rank0]:     driver_worker_output = driver_worker_method(*args, **kwargs)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/worker/worker.py\", line 195, in determine_num_available_blocks\r\n[rank0]:     self.model_runner.profile_run()\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/worker/model_runner.py\", line 1110, in profile_run\r\n[rank0]:     self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/worker/model_runner.py\", line 1539, in execute_model\r\n[rank0]:     hidden_or_intermediate_states = model_executable(\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/model_executor/models/qwen2.py\", line 361, in forward\r\n[rank0]:     hidden_states = self.model(input_ids, positions, kv_caches,\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/model_executor/models/qwen2.py\", line 277, in forward\r\n[rank0]:     hidden_states, residual = layer(\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/model_executor/models/qwen2.py\", line 210, in forward\r\n[rank0]:     hidden_states = self.self_attn(\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/model_executor/models/qwen2.py\", line 154, in forward\r\n[rank0]:     qkv, _ = self.qkv_proj(hidden_states)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/model_executor/layers/linear.py\", line 359, in forward\r\n[rank0]:     output_parallel = self.quant_method.apply(self, input_, bias)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py\", line 358, in apply\r\n[rank0]:     return scheme.apply_weights(layer, x, bias=bias)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py\", line 144, in apply_weights\r\n[rank0]:     output_2d = ops.gptq_marlin_24_gemm(x_2d, qweight, meta, scales,\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/_custom_ops.py\", line 28, in wrapper\r\n[rank0]:     return fn(*args, **kwargs)\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/vllm/_custom_ops.py\", line 222, in gptq_marlin_24_gemm\r\n[rank0]:     return torch.ops._C.gptq_marlin_24_gemm(a, b_q_weight, b_meta, b_scales,\r\n[rank0]:   File \"/opt/conda/lib/python3.8/site-packages/torch/_ops.py\", line 1061, in __call__\r\n[rank0]:     return self_._op(*args, **(kwargs or {}))\r\n[rank0]: RuntimeError: prob_m = 1152 is not divisible by thread_m = 512\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-20T07:32:12+00:00",
    "closed_at": "2025-05-17T02:09:39+00:00",
    "comments": 9,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8654/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8654"
  },
  {
    "number": 12649,
    "title": "[Installation]: how to create envs.py file for build on CPU machine?",
    "body": "### Your current environment\n\n```text\nThe output of `python collect_env.py`\n```\nHere's the output of collect_env.py file:\n\nWARNING 02-01 09:54:47 _custom_ops.py:20] Failed to import from vllm._C with ImportError('libcudart.so.12: cannot open shared object file: No such file or directory')\nCollecting environment information...\nPyTorch version: 2.5.1+cpu\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 24.04.1 LTS (x86_64)\nGCC version: (Ubuntu 12.3.0-17ubuntu1) 12.3.0\nClang version: Could not collect\nCMake version: version 3.31.4\nLibc version: glibc-2.39\n\nPython version: 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               20\nOn-line CPU(s) list:                  0-19\nVendor ID:                            GenuineIntel\nModel name:                           13th Gen Intel(R) Core(TM) i9-13900H\nCPU family:                           6\nModel:                                186\nThread(s) per core:                   2\nCore(s) per socket:                   10\nSocket(s):                            1\nStepping:                             2\nBogoMIPS:                             5990.42\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves avx_vnni umip waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize flush_l1d arch_capabilities\nVirtualization:                       VT-x\nHypervisor vendor:                    Microsoft\nVirtualization type:                  full\nL1d cache:                            480 KiB (10 instances)\nL1i cache:                            320 KiB (10 instances)\nL2 cache:                             12.5 MiB (10 instances)\nL3 cache:                             24 MiB (1 instance)\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Mitigation; Clear Register File\nVulnerability Retbleed:               Mitigation; Enhanced IBRS\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] flashinfer==0.1.6+cu124torch2.4\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.1.3.1\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.0.2.54\n[pip3] nvidia-curand-cu12==10.3.2.106\n[pip3] nvidia-cusolver-cu12==11.4.5.107\n[pip3] nvidia-cusparse-cu12==12.1.0.106\n[pip3] nvidia-ml-py==12.570.86\n[pip3] nvidia-nccl-cu12==2.20.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.1.105\n[pip3] pynvml==12.0.0\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1+cpu\n[pip3] torchao==0.8.0\n[pip3] torchaudio==2.5.1+cpu\n[pip3] torchvision==0.20.1+cpu\n[pip3] transformers==4.48.2\n[pip3] triton==3.0.0\n[conda] Could not collect\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.6.4.post1\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n\nLD_LIBRARY_PATH=/home/haoxchen/deepseek/deepseek_testenv/lib/python3.12/site-packages/cv2/../../lib64:\n\n\n\n\n### How you are installing vllm\n\ntry to install vllm on a CPU machine by following guide https://docs.vllm.ai/en/latest/getting_started/installation/cpu/index.html. When I run command VLLM_TARGET_DEVICE=cpu python setup.py install, I got the following error:\nFileNotFoundError: [Errno 2] No such file or directory: '/.../vllm/envs.py'\nhow do I build the envs.py file?\n\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-02-01T15:57:48+00:00",
    "closed_at": "2025-05-07T16:34:34+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12649/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12649"
  },
  {
    "number": 3736,
    "title": "[Usage]: Accessing stat_logger from AsyncLLMEngine",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: 2.1.2+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: 14.0.0-1ubuntu1.1\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1051-aws-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.3.107\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A10G\r\nGPU 1: NVIDIA A10G\r\nGPU 2: NVIDIA A10G\r\nGPU 3: NVIDIA A10G\r\nGPU 4: NVIDIA A10G\r\nGPU 5: NVIDIA A10G\r\nGPU 6: NVIDIA A10G\r\nGPU 7: NVIDIA A10G\r\n\r\nNvidia driver version: 535.104.12\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             192\r\nOn-line CPU(s) list:                0-191\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 7R32\r\nCPU family:                         23\r\nModel:                              49\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 48\r\nSocket(s):                          2\r\nStepping:                           0\r\nBogoMIPS:                           5599.92\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save rdpid\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          3 MiB (96 instances)\r\nL1i cache:                          3 MiB (96 instances)\r\nL2 cache:                           48 MiB (96 instances)\r\nL3 cache:                           384 MiB (24 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-47,96-143\r\nNUMA node1 CPU(s):                  48-95,144-191\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec rstack overflow: Mitigation; safe RET\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.1.2\r\n[pip3] triton==2.1.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\tX \tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t0-191\t0-1\t\tN/A\r\nGPU1\tPHB\tX \tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\t0-191\t0-1\t\tN/A\r\nGPU2\tPHB\tPHB\tX \tPHB\tPHB\tPHB\tPHB\tPHB\t0-191\t0-1\t\tN/A\r\nGPU3\tPHB\tPHB\tPHB\tX \tPHB\tPHB\tPHB\tPHB\t0-191\t0-1\t\tN/A\r\nGPU4\tPHB\tPHB\tPHB\tPHB\tX \tPHB\tPHB\tPHB\t0-191\t0-1\t\tN/A\r\nGPU5\tPHB\tPHB\tPHB\tPHB\tPHB\tX \tPHB\tPHB\t0-191\t0-1\t\tN/A\r\nGPU6\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tX \tPHB\t0-191\t0-1\t\tN/A\r\nGPU7\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tPHB\tX \t0-191\t0-1\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### How would you like to use vllm\n\nI have an instance of `AsyncLLMEngine` and I am trying to extract `counter_prompt_tokens` and `counter_generation_tokens` from the `stat_logger`. It's available in `LLMEngine` but I can't seem to get to it from an `AsyncLLMEngine`.\r\n\r\nI am able to access `_AsyncLLMEngine` and that shows `log_stats = True`. When I `dir()` the corresponding `_AsyncLLMEngine` I find:\r\n\r\n```python\r\nprint(dir(async_llm_engine.engine))\r\n```\r\n\r\n```python\r\n['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__subclasshook__', '__weakref__', '_check_beam_search_early_stopping', '_check_stop', '_decode_sequence', '_init_cache', '_init_tokenizer', '_init_workers', '_init_workers_ray', '_log_system_stats', '_process_model_outputs', '_process_sequence_group_outputs', '_run_workers', '_run_workers_async', '_verify_args', 'abort_request', 'add_lora', 'add_request', 'add_request_async', 'cache_config', 'do_log_stats', 'driver_dummy_worker', 'driver_worker', 'encode_request', 'encode_request_async', 'from_engine_args', 'get_model_config', 'get_num_unfinished_requests', 'get_tokenizer_for_seq', 'has_unfinished_requests', 'last_logging_time', 'list_loras', 'log_stats', 'lora_config', 'model_config', 'num_generation_tokens', 'num_prompt_tokens', 'parallel_config', 'remove_lora', 'scheduler', 'scheduler_config', 'seq_counter', 'step', 'step_async', 'tokenizer', 'workers']\r\n```\r\n\r\nIs there a technical reason why this is? Are the metrics not reliable when running async?\r\n\r\nThanks!\r\n",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-03-29T22:26:35+00:00",
    "closed_at": "2024-11-28T02:07:16+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/3736/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/3736"
  },
  {
    "number": 4252,
    "title": "[Bug]: the ttft and latency for each request calculated by benchmark_serving.py seems abnormal",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\nCollecting environment information...\r\nPyTorch version: 2.2.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.27.6\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-52-shopee-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A30\r\nGPU 1: NVIDIA A30\r\n\r\nNvidia driver version: 535.104.12\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.5\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          144\r\nOn-line CPU(s) list:             0-143\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Platinum 8352V CPU @ 2.10GHz\r\nCPU family:                      6\r\nModel:                           106\r\nThread(s) per core:              2\r\nCore(s) per socket:              36\r\nSocket(s):                       2\r\nStepping:                        6\r\nCPU max MHz:                     3500.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        4200.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\r\nVirtualization:                  VT-x\r\nL1d cache:                       3.4 MiB (72 instances)\r\nL1i cache:                       2.3 MiB (72 instances)\r\nL2 cache:                        90 MiB (72 instances)\r\nL3 cache:                        108 MiB (2 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142\r\nNUMA node1 CPU(s):               1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.22.2\r\n[pip3] onnx==1.14.0\r\n[pip3] pytorch-quantization==2.1.2\r\n[pip3] torch==2.2.1\r\n[pip3] torch-tensorrt==0.0.0\r\n[pip3] torchdata==0.7.0a0\r\n[pip3] torchtext==0.16.0a0\r\n[pip3] torchvision==0.16.0a0\r\n[pip3] triton==2.2.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: 8.0; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\n\ufffd[4mGPU0\tGPU1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\ufffd[0m\r\nGPU0\t X \tSYS\t0,2,4,6,8,10\t0\t\tN/A\r\nGPU1\tSYS\t X \t1,3,5,7,9,11\t1\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n### \ud83d\udc1b Describe the bug\n\nI have set breakpoint in the offical `benchmark_serving.py` and ran following command:\r\n```bash\r\npython benchmark_serving.py \\ \r\n--model /data/models/vicuna-13b-v1.5 \\\r\n--dataset-name sharegpt \\\r\n--dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \\\r\n--port 8014 \\\r\n--num-prompts 5\r\n```\r\nThe server is launched with `--max-num-seqs 1` to test the performance for `bach_szie = 1`:\r\n\r\n```bash\r\npython3 -m vllm.entrypoints.openai.api_server \\\r\n--host 0.0.0.0 --port 8014 \\\r\n--model /data/models/vicuna-13b-v1.5 \\\r\n--dtype auto -tp 2 --max-model-len 4096 \\\r\n--max-num-seqs 1 \\\r\n--gpu-memory-utilization 0.85 \\\r\n--enable-prefix-caching \\\r\n--swap-space 16\r\n```\r\n\r\nThe **ttft** seems incorrect for prompt_len=30, which elapsed 3.9547242913395166s. The other cases also show the same phenomenon.\r\n![image](https://github.com/vllm-project/vllm/assets/24476563/f93bf461-5a05-49e0-9e8f-b69d1bd4360b)",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-04-22T03:53:26+00:00",
    "closed_at": "2024-04-23T04:23:33+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4252/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4252"
  },
  {
    "number": 15106,
    "title": "First tpot/itl is too long?",
    "body": "I discovered a somewhat strange (or is it reasonable?) phenomenon: For example, I started a vllm OpenAI server with chunked prefill=True and max_num_batched_tokens=2k, then sent an input of 5k tokens using the /chat endpoint with stream=True. The server first returned a token that includes the role but with an empty content, taking 0.9 seconds (on my hardware and model, the prefill time for 2k tokens is 0.9 seconds), which is counted as ttft. Then, it returned a second token with non-empty content, taking 1.3 seconds (on my hardware and model, the prefill time for 3k tokens is 1.3 seconds), which is counted as the first tpot/itl. Next, it returned a third token with non-empty content, taking 0.02 seconds (on my hardware and model, the decode time is 0.02 seconds), and all subsequent tokens took 0.02 seconds each. This is inconsistent with the tpot/itl I usually observe in benchmarks (which are all 0.02 seconds), so I traced it down to the implementation of the chat interface:\n\nhttps://github.com/vllm-project/vllm/blob/ed6e9075d31e32c8548b480a47d1ffb77da1f54c/vllm/entrypoints/openai/serving_chat.py#L372-L438\n\nhttps://github.com/vllm-project/vllm/blob/ed6e9075d31e32c8548b480a47d1ffb77da1f54c/vllm/entrypoints/openai/serving_chat.py#L461-L464\n\nIs this behavior reasonable? If not, should the code for `if first_iteration` be placed after the code for `continue`?",
    "labels": [
      "performance"
    ],
    "state": "open",
    "created_at": "2025-03-19T07:52:03+00:00",
    "closed_at": null,
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15106/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15106"
  },
  {
    "number": 9294,
    "title": "[New Model]: meta-llama/Llama-Guard-3-1B",
    "body": "### The model to consider.\n\nmeta-llama/Llama-Guard-3-1B\n\n### The closest model vllm already supports.\n\nmeta-llama/Llama-Guard-3-8B\n\n### What's your difficulty of supporting the model you want?\n\nCurrently the model runs, but its outputs are completely random, so the same prompt can be safe or unsafe at any point. Setting the temperature to 0.0 makes EVERY prompt return safe.\r\n\r\nMy hunch is the issue comes from the model pruning:\r\n\r\nOutput Layer Pruning\r\nThe Llama Guard model is trained to generate 128k output tokens out of which only 20 tokens (e.g. safe, unsafe, S, 1,...) are used. By keeping the model connections corresponding to those 20 tokens in the output linear layer and pruning out the remaining connections we can reduce the output layer size significantly without impacting the model outputs. Using output layer pruning, we reduced the output layer size from 262.6M parameters (2048x128k) to 40.96k parameters (2048x20), giving us a total savings of 131.3MB with 4-bit quantized weights. Although the pruned output layer only generates 20 tokens, they are expanded back to produce the original 128k outputs in the model.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "help wanted",
      "new-model"
    ],
    "state": "closed",
    "created_at": "2024-10-11T18:26:49+00:00",
    "closed_at": "2024-10-24T05:05:50+00:00",
    "comments": 5,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9294/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9294"
  },
  {
    "number": 20426,
    "title": "[Bug]: kv_cache incorrect with cuda graph batch inputs vllm0.6.3",
    "body": "### Your current environment\n\nvllm0.6.3  about 2024.OCT\n\n### \ud83d\udc1b Describe the bug\n\nDue to some project constraints, I'm currently still using vLLM 0.6.3 and don't have time to upgrade for now. I've noticed that when CUDA Graph is enabled, the output becomes incorrect for some batches in multi-batch inputs. I constructed a batch with identical inputs, so theoretically, the outputs of each batch should be the same.\n\nIn my testing:\n    Single-batch with CUDA Graph: output is correct\n    Multi-batch without CUDA Graph (eager mode): output is also correct\n    Multi-batch with CUDA Graph: after attention computation, the hidden_states across batches start to differ \n        unexpectedly\n    Interestingly, when I forcibly set the kv_cache values to 1, the hidden_states become identical again after \n        attention.\n\nHas anyone encountered this issue before? Are there any related issues or discussions about this? Thanks!\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-07-03T08:32:47+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20426/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20426"
  },
  {
    "number": 17877,
    "title": "[Bug]: vllm run bge-m3 error",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nYour output of `python collect_env.py` here\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nDeploy  local BGE-M3 using VLLM and start the command as follows\uff1a\npython3 -m vllm.entrypoints.openai.api_server --served-model-name embed --model /app/bge-m3 --gpu-memory-utilization 0.8 --trust-remote-code --port 8080 --task embed \u3002\n\nService error: huggingface_hub.errors.HFValidationError: Repo id must be in the from 'repo_name' or 'namespace/repo_name': '/app/bge_m3'. usee 'repo_type' argument if needed\u3002 \n\uff08This model can run and start normally without using VLLM for testing\uff09\n\nenvironment\uff1a \nVLLM 0.7.3\ntransformers 4.51.3\ntorch 2.5.1\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-05-09T03:48:20+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17877/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17877"
  },
  {
    "number": 2546,
    "title": "[Question] Usage with Audio Models?",
    "body": "I did some initial search but couldn't find any frameworks that provide the same functionality as vLLM for Audio models. Currently I'm running a Flask server for running a Distil-Whisper model , any leads for a more performant server will be great. Thanks.",
    "labels": [],
    "state": "closed",
    "created_at": "2024-01-22T09:31:46+00:00",
    "closed_at": "2024-04-04T08:04:30+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/2546/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/2546"
  },
  {
    "number": 720,
    "title": "[Question] Is it possible to crop the KV cache with the currently supported operations?",
    "body": "Title. ",
    "labels": [],
    "state": "closed",
    "created_at": "2023-08-09T17:11:09+00:00",
    "closed_at": "2024-03-08T10:37:07+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/720/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/720"
  },
  {
    "number": 9136,
    "title": "[Bug]: out-of-bound in attention.cu",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nI didn\u2019t run the code, just manually reviewed it on my MacBook.\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nhttps://github.com/vllm-project/vllm/blob/c0d9a98d0c7182b73c2e7f88508e690a186bf0e3/csrc/rocm/attention.cu#L199-L225\r\nhttps://github.com/vllm-project/vllm/blob/c0d9a98d0c7182b73c2e7f88508e690a186bf0e3/csrc/rocm/attention.cu#L264\r\nhttps://github.com/vllm-project/vllm/blob/c0d9a98d0c7182b73c2e7f88508e690a186bf0e3/csrc/rocm/attention.cu#L352-L359\r\nhttps://github.com/vllm-project/vllm/blob/c0d9a98d0c7182b73c2e7f88508e690a186bf0e3/csrc/rocm/attention.cu#L914-L917\r\nhttps://github.com/vllm-project/vllm/blob/c0d9a98d0c7182b73c2e7f88508e690a186bf0e3/csrc/rocm/attention.cu#L960-L973\r\n\r\nSo, for `alibi_slopes[wg_start_head_idx + qhead_idx]` in line 358:\r\n- `wg_start_head_idx = blockIdx.z * GQA_RATIO`\r\n- `blockIdx.z = num_kv_heads`\r\n- `GQA_RATIO = num_heads / num_kv_heads`\r\n\r\nThus, `wg_start_head_idx = num_heads`. It appears that `alibi_slopes[wg_start_head_idx + qhead_idx]` may result in an overflow, as the size of `alibi_slopes` is only `num_heads`, and the computed index exceeds this limit.\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-07T22:22:48+00:00",
    "closed_at": "2025-02-06T01:59:33+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9136/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9136"
  },
  {
    "number": 16494,
    "title": "[Usage]: How to use vllm serve for batch inference?",
    "body": "### Your current environment\n\n```Collecting environment information...\nPyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.26.0\nLibc version: glibc-2.35\n\nPython version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-1045-azure-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA A100 80GB PCIe\nGPU 1: NVIDIA A100 80GB PCIe\n\nNvidia driver version: 535.86.10\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      48 bits physical, 48 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             96\nOn-line CPU(s) list:                0-95\nVendor ID:                          AuthenticAMD\nModel name:                         AMD EPYC 7V13 64-Core Processor\nCPU family:                         25\nModel:                              1\nThread(s) per core:                 1\nCore(s) per socket:                 48\nSocket(s):                          2\nStepping:                           1\nBogoMIPS:                           4890.87\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr rdpru arat umip vaes vpclmulqdq rdpid fsrm\nHypervisor vendor:                  Microsoft\nVirtualization type:                full\nL1d cache:                          3 MiB (96 instances)\nL1i cache:                          3 MiB (96 instances)\nL2 cache:                           48 MiB (96 instances)\nL3 cache:                           384 MiB (12 instances)\nNUMA node(s):                       4\nNUMA node0 CPU(s):                  0-23\nNUMA node1 CPU(s):                  24-47\nNUMA node2 CPU(s):                  48-71\nNUMA node3 CPU(s):                  72-95\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec store bypass:    Vulnerable\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] gpytorch==1.14\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] pytorch-lightning==2.5.0.post0\n[pip3] pyzmq==26.2.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchmetrics==1.6.2\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.49.0\n[pip3] triton==3.1.0\n[conda] gpytorch                  1.14                     pypi_0    pypi\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.5.147               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.21.5                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\n[conda] pytorch-lightning         2.5.0.post0              pypi_0    pypi\n[conda] pyzmq                     26.2.1                   pypi_0    pypi\n[conda] torch                     2.5.1                    pypi_0    pypi\n[conda] torchaudio                2.5.1                    pypi_0    pypi\n[conda] torchmetrics              1.6.2                    pypi_0    pypi\n[conda] torchvision               0.20.1                   pypi_0    pypi\n[conda] transformers              4.49.0                   pypi_0    pypi\n[conda] triton                    3.1.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.7.3\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n        GPU0    GPU1    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      SYS     NODE    0-23    0               N/A\nGPU1    SYS      X      SYS     48-71   2               N/A\nNIC0    NODE    SYS      X \n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nNIC Legend:\n\n  NIC0: mlx5_0\n\nNVIDIA_VISIBLE_DEVICES=2,0\nNVIDIA_REQUIRE_CUDA=cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\nTORCH_DISTRIBUTED_INIT_STORE_STORAGE_ENDPOINT=MustSpecifyViaApplicationParameters\nNCCL_VERSION=2.17.1-1\nNCCL_SOCKET_IFNAME=eth0\nNCCL_DEBUG_SUBSYS=INIT,GRAPH\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNCCL_DEBUG=INFO\nNCCL_IB_HCA=\nNVIDIA_PRODUCT_NAME=CUDA\nPYTORCH_TYPE=stable\nTORCH_DISTRIBUTED_INIT_STORE_STORAGE_NAME=MustSpecifyViaApplicationParameters\nCUDA_VERSION=12.1.1\nNCCL_ASYNC_ERROR_HANDLING=1\nTORCH_DISTRIBUTED_INIT_STORE_MOUNT_PERMISSIONS=MustSpecifyViaApplicationParameters\nTORCH_DISTRIBUTED_INIT_STORE_MOUNT_PATH=MustSpecifyViaApplicationParameters\nLD_LIBRARY_PATH=/home/aiscuser/.conda/envs/cot/lib/python3.12/site-packages/cv2/../../lib64:/opt/hpcx/nccl_rdma_sharp_plugin/lib:/opt/nccl-rdma-sharp-plugins/lib:/opt/hpcx/ompi/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nTORCH_DISTRIBUTED_INIT_STORE_STORAGE_KIND=None\nPYTORCH_BUILD_VERSION=2.3.1\nTORCHINDUCTOR_WORKER_START=fork\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n\n### How would you like to use vllm\n\nGreetings,\n\nI wonder how exactly should I achieve batch inference with an `vllm serve` model. For example, I give it a list of outputs and it return me a list of completions\n\n\nI just want to do something as simple as below, making the model to generate multiple outputs at once, just like the offline batch inference. But I am really struggling to find the documentations.\n\n\n```\nfrom openai import OpenAI\nimport openai\n# Set OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\nmodel = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n\nmessage_1 = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n    ]\nmessage_2 = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Tell me about the weather today.\"},\n    ]\n\nchat_response = client.chat.completions.create(\n    model= model,\n    messages=[message_1, message_2]\n)\nprint(\"Chat response:\", chat_response)\n```\n\nI've looked at #2441 #2529, but they are not exactly what I am looking for.\n\nThank you very much for your help! An example will be greatly appreciated.\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "closed",
    "created_at": "2025-04-11T16:03:51+00:00",
    "closed_at": "2025-04-11T22:17:25+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16494/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16494"
  },
  {
    "number": 795,
    "title": "\"attn_bias is not correctly aligned\" on A100 for MPT-30B",
    "body": "Hello,\r\n\r\nI saw a similar issue to this for MPT30B0-chat on H100, but I see the same error on A100 80Gb. Using vllm 0.1.3. Is there any workaround to fix this currently? It does happen for random prompt, so not straightforward to understand where it's coming from:\r\n\r\n    96 \u2502   \u2502   \u2502   \u2502   prompt_template = PromptTemplate(input_variables=[\"text\"] \u2502\r\n\u2502    97 \u2502   \u2502   \u2502   \u2502   answer_chain = LLMChain(llm=self.llm , prompt=prompt_temp \u2502\r\n\u2502    98 \u2502   \u2502   \u2502   \u2502                                                             \u2502\r\n\u2502 \u2771  99 \u2502   \u2502   \u2502   \u2502   response = answer_chain.run(query)                        \u2502\r\n\u2502   100 \u2502   \u2502   \u2502                                                                 \u2502\r\n\u2502   101 \u2502   \u2502   \u2502   else:                                                         \u2502\r\n\u2502   102                                                                           \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/langchain/chains/base. \u2502\r\n\u2502 py:440 in run                                                                   \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   437 \u2502   \u2502   if args and not kwargs:                                           \u2502\r\n\u2502   438 \u2502   \u2502   \u2502   if len(args) != 1:                                            \u2502\r\n\u2502   439 \u2502   \u2502   \u2502   \u2502   raise ValueError(\"`run` supports only one positional argu \u2502\r\n\u2502 \u2771 440 \u2502   \u2502   \u2502   return self(args[0], callbacks=callbacks, tags=tags, metadata \u2502\r\n\u2502   441 \u2502   \u2502   \u2502   \u2502   _output_key                                               \u2502\r\n\u2502   442 \u2502   \u2502   \u2502   ]                                                             \u2502\r\n\u2502   443                                                                           \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/langchain/chains/base. \u2502\r\n\u2502 py:243 in __call__                                                              \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   240 \u2502   \u2502   \u2502   )                                                             \u2502\r\n\u2502   241 \u2502   \u2502   except (KeyboardInterrupt, Exception) as e:                       \u2502\r\n\u2502   242 \u2502   \u2502   \u2502   run_manager.on_chain_error(e)                                 \u2502\r\n\u2502 \u2771 243 \u2502   \u2502   \u2502   raise e                                                       \u2502\r\n\u2502   244 \u2502   \u2502   run_manager.on_chain_end(outputs)                                 \u2502\r\n\u2502   245 \u2502   \u2502   final_outputs: Dict[str, Any] = self.prep_outputs(                \u2502\r\n\u2502   246 \u2502   \u2502   \u2502   inputs, outputs, return_only_outputs                          \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/langchain/chains/base. \u2502\r\n\u2502 py:237 in __call__                                                              \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   234 \u2502   \u2502   )                                                                 \u2502\r\n\u2502   235 \u2502   \u2502   try:                                                              \u2502\r\n\u2502   236 \u2502   \u2502   \u2502   outputs = (                                                   \u2502\r\n\u2502 \u2771 237 \u2502   \u2502   \u2502   \u2502   self._call(inputs, run_manager=run_manager)               \u2502\r\n\u2502   238 \u2502   \u2502   \u2502   \u2502   if new_arg_supported                                      \u2502\r\n\u2502   239 \u2502   \u2502   \u2502   \u2502   else self._call(inputs)                                   \u2502\r\n\u2502   240 \u2502   \u2502   \u2502   )                                                             \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/langchain/chains/llm.p \u2502\r\n\u2502 y:92 in _call                                                                   \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502    89 \u2502   \u2502   inputs: Dict[str, Any],                                           \u2502\r\n\u2502    90 \u2502   \u2502   run_manager: Optional[CallbackManagerForChainRun] = None,         \u2502\r\n\u2502    91 \u2502   ) -> Dict[str, str]:                                                  \u2502\r\n\u2502 \u2771  92 \u2502   \u2502   response = self.generate([inputs], run_manager=run_manager)       \u2502\r\n\u2502    93 \u2502   \u2502   return self.create_outputs(response)[0]                           \u2502\r\n\u2502    94 \u2502                                                                         \u2502\r\n\u2502    95 \u2502   def generate(                                                         \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/langchain/chains/llm.p \u2502\r\n\u2502 y:102 in generate                                                               \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502    99 \u2502   ) -> LLMResult:                                                       \u2502\r\n\u2502   100 \u2502   \u2502   \"\"\"Generate LLM result from inputs.\"\"\"                            \u2502\r\n\u2502   101 \u2502   \u2502   prompts, stop = self.prep_prompts(input_list, run_manager=run_man \u2502\r\n\u2502 \u2771 102 \u2502   \u2502   return self.llm.generate_prompt(                                  \u2502\r\n\u2502   103 \u2502   \u2502   \u2502   prompts,                                                      \u2502\r\n\u2502   104 \u2502   \u2502   \u2502   stop,                                                         \u2502\r\n\u2502   105 \u2502   \u2502   \u2502   callbacks=run_manager.get_child() if run_manager else None,   \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/langchain/llms/base.py \u2502\r\n\u2502 :186 in generate_prompt                                                         \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   183 \u2502   \u2502   **kwargs: Any,                                                    \u2502\r\n\u2502   184 \u2502   ) -> LLMResult:                                                       \u2502\r\n\u2502   185 \u2502   \u2502   prompt_strings = [p.to_string() for p in prompts]                 \u2502\r\n\u2502 \u2771 186 \u2502   \u2502   return self.generate(prompt_strings, stop=stop, callbacks=callbac \u2502\r\n\u2502   187 \u2502                                                                         \u2502\r\n\u2502   188 \u2502   async def agenerate_prompt(                                           \u2502\r\n\u2502   189 \u2502   \u2502   self,                                                             \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/langchain/llms/base.py \u2502\r\n\u2502 :279 in generate                                                                \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   276 \u2502   \u2502   \u2502   run_managers = callback_manager.on_llm_start(                 \u2502\r\n\u2502   277 \u2502   \u2502   \u2502   \u2502   dumpd(self), prompts, invocation_params=params, options=o \u2502\r\n\u2502   278 \u2502   \u2502   \u2502   )                                                             \u2502\r\n\u2502 \u2771 279 \u2502   \u2502   \u2502   output = self._generate_helper(                               \u2502\r\n\u2502   280 \u2502   \u2502   \u2502   \u2502   prompts, stop, run_managers, bool(new_arg_supported), **k \u2502\r\n\u2502   281 \u2502   \u2502   \u2502   )                                                             \u2502\r\n\u2502   282 \u2502   \u2502   \u2502   return output                                                 \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/langchain/llms/base.py \u2502\r\n\u2502 :223 in _generate_helper                                                        \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   220 \u2502   \u2502   except (KeyboardInterrupt, Exception) as e:                       \u2502\r\n\u2502   221 \u2502   \u2502   \u2502   for run_manager in run_managers:                              \u2502\r\n\u2502   222 \u2502   \u2502   \u2502   \u2502   run_manager.on_llm_error(e)                               \u2502\r\n\u2502 \u2771 223 \u2502   \u2502   \u2502   raise e                                                       \u2502\r\n\u2502   224 \u2502   \u2502   flattened_outputs = output.flatten()                              \u2502\r\n\u2502   225 \u2502   \u2502   for manager, flattened_output in zip(run_managers, flattened_outp \u2502\r\n\u2502   226 \u2502   \u2502   \u2502   manager.on_llm_end(flattened_output)                          \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/langchain/llms/base.py \u2502\r\n\u2502 :210 in _generate_helper                                                        \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   207 \u2502   ) -> LLMResult:                                                       \u2502\r\n\u2502   208 \u2502   \u2502   try:                                                              \u2502\r\n\u2502   209 \u2502   \u2502   \u2502   output = (                                                    \u2502\r\n\u2502 \u2771 210 \u2502   \u2502   \u2502   \u2502   self._generate(                                           \u2502\r\n\u2502   211 \u2502   \u2502   \u2502   \u2502   \u2502   prompts,                                              \u2502\r\n\u2502   212 \u2502   \u2502   \u2502   \u2502   \u2502   stop=stop,                                            \u2502\r\n\u2502   213 \u2502   \u2502   \u2502   \u2502   \u2502   # TODO: support multiple run managers                 \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/langchain/llms/base.py \u2502\r\n\u2502 :604 in _generate                                                               \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   601 \u2502   \u2502   \u2502   text = (                                                      \u2502\r\n\u2502   602 \u2502   \u2502   \u2502   \u2502   self._call(prompt, stop=stop, run_manager=run_manager, ** \u2502\r\n\u2502   603 \u2502   \u2502   \u2502   \u2502   if new_arg_supported                                      \u2502\r\n\u2502 \u2771 604 \u2502   \u2502   \u2502   \u2502   else self._call(prompt, stop=stop, **kwargs)              \u2502\r\n\u2502   605 \u2502   \u2502   \u2502   )                                                             \u2502\r\n\u2502   606 \u2502   \u2502   \u2502   generations.append([Generation(text=text)])                   \u2502\r\n\u2502   607 \u2502   \u2502   return LLMResult(generations=generations)                         \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/xxx.py:64 in _call                            \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   61 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    max_tokens=300,                               \u2502\r\n\u2502   62 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   )                                              \u2502\r\n\u2502   63 \u2502   \u2502                                                                      \u2502\r\n\u2502 \u2771 64 \u2502   \u2502   output = model.generate(prompt, sampling_params)                   \u2502\r\n\u2502   65 \u2502   \u2502                                                                      \u2502\r\n\u2502   66 \u2502   \u2502   return output[0].outputs[0].text                                   \u2502\r\n\u2502   67                                                                            \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/vllm/vllm/entrypoints/llm.py:130 in generate                              \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   127 \u2502   \u2502   \u2502   else:                                                         \u2502\r\n\u2502   128 \u2502   \u2502   \u2502   \u2502   token_ids = prompt_token_ids[i]                           \u2502\r\n\u2502   129 \u2502   \u2502   \u2502   self._add_request(prompt, sampling_params, token_ids)         \u2502\r\n\u2502 \u2771 130 \u2502   \u2502   return self._run_engine(use_tqdm)                                 \u2502\r\n\u2502   131 \u2502                                                                         \u2502\r\n\u2502   132 \u2502   def _add_request(                                                     \u2502\r\n\u2502   133 \u2502   \u2502   self,                                                             \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/vllm/vllm/entrypoints/llm.py:150 in _run_engine                           \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   147 \u2502   \u2502   # Run the engine.                                                 \u2502\r\n\u2502   148 \u2502   \u2502   outputs: List[RequestOutput] = []                                 \u2502\r\n\u2502   149 \u2502   \u2502   while self.llm_engine.has_unfinished_requests():                  \u2502\r\n\u2502 \u2771 150 \u2502   \u2502   \u2502   step_outputs = self.llm_engine.step()                         \u2502\r\n\u2502   151 \u2502   \u2502   \u2502   for output in step_outputs:                                   \u2502\r\n\u2502   152 \u2502   \u2502   \u2502   \u2502   if output.finished:                                       \u2502\r\n\u2502   153 \u2502   \u2502   \u2502   \u2502   \u2502   outputs.append(output)                                \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/vllm/vllm/engine/llm_engine.py:313 in step                                \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   310 \u2502   \u2502   \u2502   ]                                                             \u2502\r\n\u2502   311 \u2502   \u2502                                                                     \u2502\r\n\u2502   312 \u2502   \u2502   # Execute the model.                                              \u2502\r\n\u2502 \u2771 313 \u2502   \u2502   output = self._run_workers(                                       \u2502\r\n\u2502   314 \u2502   \u2502   \u2502   \"execute_model\",                                              \u2502\r\n\u2502   315 \u2502   \u2502   \u2502   seq_group_metadata_list=seq_group_metadata_list,              \u2502\r\n\u2502   316 \u2502   \u2502   \u2502   blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,        \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/vllm/vllm/engine/llm_engine.py:470 in _run_workers                        \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   467 \u2502   \u2502   \u2502   else:                                                         \u2502\r\n\u2502   468 \u2502   \u2502   \u2502   \u2502   executor = getattr(worker, method)                        \u2502\r\n\u2502   469 \u2502   \u2502   \u2502                                                                 \u2502\r\n\u2502 \u2771 470 \u2502   \u2502   \u2502   output = executor(*args, **kwargs)                            \u2502\r\n\u2502   471 \u2502   \u2502   \u2502   all_outputs.append(output)                                    \u2502\r\n\u2502   472 \u2502   \u2502                                                                     \u2502\r\n\u2502   473 \u2502   \u2502   if self.parallel_config.worker_use_ray:                           \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/_contextli \u2502\r\n\u2502 b.py:115 in decorate_context                                                    \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   112 \u2502   @functools.wraps(func)                                                \u2502\r\n\u2502   113 \u2502   def decorate_context(*args, **kwargs):                                \u2502\r\n\u2502   114 \u2502   \u2502   with ctx_factory():                                               \u2502\r\n\u2502 \u2771 115 \u2502   \u2502   \u2502   return func(*args, **kwargs)                                  \u2502\r\n\u2502   116 \u2502                                                                         \u2502\r\n\u2502   117 \u2502   return decorate_context                                               \u2502\r\n\u2502   118                                                                           \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/vllm/vllm/worker/worker.py:293 in execute_model                           \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   290 \u2502   \u2502   \u2502   seq_group_metadata_list)                                      \u2502\r\n\u2502   291 \u2502   \u2502                                                                     \u2502\r\n\u2502   292 \u2502   \u2502   # Execute the model.                                              \u2502\r\n\u2502 \u2771 293 \u2502   \u2502   output = self.model(                                              \u2502\r\n\u2502   294 \u2502   \u2502   \u2502   input_ids=input_tokens,                                       \u2502\r\n\u2502   295 \u2502   \u2502   \u2502   positions=input_positions,                                    \u2502\r\n\u2502   296 \u2502   \u2502   \u2502   kv_caches=self.gpu_cache,                                     \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/modul \u2502\r\n\u2502 e.py:1501 in _call_impl                                                         \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks  \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):  \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                         \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                         \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []            \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                          \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/vllm/vllm/model_executor/models/mpt.py:234 in forward                     \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   231 \u2502   \u2502   input_metadata: InputMetadata,                                    \u2502\r\n\u2502   232 \u2502   \u2502   cache_events: Optional[List[torch.cuda.Event]],                   \u2502\r\n\u2502   233 \u2502   ) -> Dict[int, SequenceOutputs]:                                      \u2502\r\n\u2502 \u2771 234 \u2502   \u2502   hidden_states = self.transformer(input_ids, positions, kv_caches, \u2502\r\n\u2502   235 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    input_metadata, cache_events)    \u2502\r\n\u2502   236 \u2502   \u2502   next_tokens = self.sampler(self.lm_head_weight, hidden_states,    \u2502\r\n\u2502   237 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      input_metadata)                        \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/modul \u2502\r\n\u2502 e.py:1501 in _call_impl                                                         \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks  \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):  \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                         \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                         \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []            \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                          \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/vllm/vllm/model_executor/models/mpt.py:202 in forward                     \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   199 \u2502   \u2502   \u2502   else:                                                         \u2502\r\n\u2502   200 \u2502   \u2502   \u2502   \u2502   cache_event = cache_events[i]                             \u2502\r\n\u2502   201 \u2502   \u2502   \u2502   block = self.blocks[i]                                        \u2502\r\n\u2502 \u2771 202 \u2502   \u2502   \u2502   hidden_states = block(                                        \u2502\r\n\u2502   203 \u2502   \u2502   \u2502   \u2502   position_ids,                                             \u2502\r\n\u2502   204 \u2502   \u2502   \u2502   \u2502   hidden_states,                                            \u2502\r\n\u2502   205 \u2502   \u2502   \u2502   \u2502   kv_caches[i],                                             \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/modul \u2502\r\n\u2502 e.py:1501 in _call_impl                                                         \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks  \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):  \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                         \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                         \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []            \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                          \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/vllm/vllm/model_executor/models/mpt.py:153 in forward                     \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   150 \u2502   \u2502   cache_event: Optional[torch.cuda.Event],                          \u2502\r\n\u2502   151 \u2502   ) -> torch.Tensor:                                                    \u2502\r\n\u2502   152 \u2502   \u2502   x = self.norm_1(hidden_states)                                    \u2502\r\n\u2502 \u2771 153 \u2502   \u2502   x = self.attn(                                                    \u2502\r\n\u2502   154 \u2502   \u2502   \u2502   position_ids=position_ids,                                    \u2502\r\n\u2502   155 \u2502   \u2502   \u2502   hidden_states=x,                                              \u2502\r\n\u2502   156 \u2502   \u2502   \u2502   kv_cache=kv_cache,                                            \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/modul \u2502\r\n\u2502 e.py:1501 in _call_impl                                                         \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks  \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):  \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                         \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                         \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []            \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                          \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/vllm/vllm/model_executor/models/mpt.py:102 in forward                     \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502    99 \u2502   \u2502   \u2502   q = self.q_ln(q)                                              \u2502\r\n\u2502   100 \u2502   \u2502   \u2502   k = self.k_ln(k)                                              \u2502\r\n\u2502   101 \u2502   \u2502   k_cache, v_cache = kv_cache                                       \u2502\r\n\u2502 \u2771 102 \u2502   \u2502   attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata \u2502\r\n\u2502   103 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   cache_event)                              \u2502\r\n\u2502   104 \u2502   \u2502   output, _ = self.out_proj(attn_output)                            \u2502\r\n\u2502   105 \u2502   \u2502   return output                                                     \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/modul \u2502\r\n\u2502 e.py:1501 in _call_impl                                                         \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks  \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):  \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                         \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                         \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []            \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                          \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/vllm/vllm/model_executor/layers/attention.py:202 in forward               \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   199 \u2502   \u2502   \u2502   # Prompt run.                                                 \u2502\r\n\u2502   200 \u2502   \u2502   \u2502   assert input_metadata.num_generation_tokens == 0              \u2502\r\n\u2502   201 \u2502   \u2502   \u2502   self.set_attn_bias(input_metadata)                            \u2502\r\n\u2502 \u2771 202 \u2502   \u2502   \u2502   self.multi_query_kv_attention(                                \u2502\r\n\u2502   203 \u2502   \u2502   \u2502   \u2502   output[:num_prompt_tokens],                               \u2502\r\n\u2502   204 \u2502   \u2502   \u2502   \u2502   query[:num_prompt_tokens],                                \u2502\r\n\u2502   205 \u2502   \u2502   \u2502   \u2502   key[:num_prompt_tokens],                                  \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/vllm/vllm/model_executor/layers/attention.py:399 in                       \u2502\r\n\u2502 multi_query_kv_attention                                                        \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   396 \u2502   \u2502   start = 0                                                         \u2502\r\n\u2502   397 \u2502   \u2502   for i, prompt_len in enumerate(input_metadata.prompt_lens):       \u2502\r\n\u2502   398 \u2502   \u2502   \u2502   end = start + prompt_len                                      \u2502\r\n\u2502 \u2771 399 \u2502   \u2502   \u2502   out = xops.memory_efficient_attention_forward(                \u2502\r\n\u2502   400 \u2502   \u2502   \u2502   \u2502   query[None, start:end],                                   \u2502\r\n\u2502   401 \u2502   \u2502   \u2502   \u2502   key[None, start:end],                                     \u2502\r\n\u2502   402 \u2502   \u2502   \u2502   \u2502   value[None, start:end],                                   \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/xformers/ops/fmha/__in \u2502\r\n\u2502 it__.py:213 in memory_efficient_attention_forward                               \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   210 \u2502   \"\"\"                                                                   \u2502\r\n\u2502   211 \u2502   Calculates the forward pass of :attr:`xformers.ops.memory_efficient_a \u2502\r\n\u2502   212 \u2502   \"\"\"                                                                   \u2502\r\n\u2502 \u2771 213 \u2502   return _memory_efficient_attention_forward(                           \u2502\r\n\u2502   214 \u2502   \u2502   Inputs(                                                           \u2502\r\n\u2502   215 \u2502   \u2502   \u2502   query=query, key=key, value=value, p=p, attn_bias=attn_bias,  \u2502\r\n\u2502   216 \u2502   \u2502   ),                                                                \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/xformers/ops/fmha/__in \u2502\r\n\u2502 it__.py:310 in _memory_efficient_attention_forward                              \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   307 \u2502   else:                                                                 \u2502\r\n\u2502   308 \u2502   \u2502   _ensure_op_supports_or_raise(ValueError, \"memory_efficient_attent \u2502\r\n\u2502   309 \u2502                                                                         \u2502\r\n\u2502 \u2771 310 \u2502   out, *_ = op.apply(inp, needs_gradient=False)                         \u2502\r\n\u2502   311 \u2502   return out.reshape(output_shape)                                      \u2502\r\n\u2502   312                                                                           \u2502\r\n\u2502   313                                                                           \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/xformers/ops/fmha/cutl \u2502\r\n\u2502 ass.py:175 in apply                                                             \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   172 \u2502   \u2502   if type(inp.attn_bias) not in FwOp.SUPPORTED_ATTN_BIAS_TYPES:     \u2502\r\n\u2502   173 \u2502   \u2502   \u2502   raise NotImplementedError(\"Unsupported attn_bias type\")       \u2502\r\n\u2502   174 \u2502   \u2502   seqstart_k, seqstart_q, max_seqlen_q, _ = _get_seqlen_info(inp)   \u2502\r\n\u2502 \u2771 175 \u2502   \u2502   out, lse, rng_seed, rng_offset = cls.OPERATOR(                    \u2502\r\n\u2502   176 \u2502   \u2502   \u2502   query=inp.query,                                              \u2502\r\n\u2502   177 \u2502   \u2502   \u2502   key=inp.key,                                                  \u2502\r\n\u2502   178 \u2502   \u2502   \u2502   value=inp.value,                                              \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502 /root/miniconda3/envs/py311/lib/python3.11/site-packages/torch/_ops.py:502 in   \u2502\r\n\u2502 __call__                                                                        \u2502\r\n\u2502                                                                                 \u2502\r\n\u2502   499 \u2502   \u2502   # is still callable from JIT                                      \u2502\r\n\u2502   500 \u2502   \u2502   # We save the function ptr as the `op` attribute on               \u2502\r\n\u2502   501 \u2502   \u2502   # OpOverloadPacket to access it here.                             \u2502\r\n\u2502 \u2771 502 \u2502   \u2502   return self._op(*args, **kwargs or {})                            \u2502\r\n\u2502   503 \u2502                                                                         \u2502\r\n\u2502   504 \u2502   # TODO: use this to make a __dir__                                    \u2502\r\n\u2502   505 \u2502   def overloads(self):                                                  \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nRuntimeError: attn_bias is not correctly aligned\r\n\r\ncurrently generating with: \r\nmodel = vllm.LLM(model= \"MPT-30B\" , trust_remote_code=True)\r\nsampling_params = vllm.SamplingParams(n=1,\r\n                             temperature=0.2,\r\n                             top_p=0.9,\r\n                             top_k = -1,\r\n                             best_of=1,\r\n                             use_beam_search=False,\r\n                             max_tokens=300,\r\n                            )\r\n\r\n        output = model.generate(prompt, sampling_params)\r\n\r\nOther libraries version:\r\nxformers==0.0.20\r\nlangchain==0.0.232\r\ntorch==2.0.1\r\npytorch-triton==2.1.0+440fd1bf20",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2023-08-18T21:59:05+00:00",
    "closed_at": "2023-08-23T08:44:23+00:00",
    "comments": 3,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/795/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/795"
  },
  {
    "number": 20303,
    "title": "[Bug]: Unable to use tensor parallel",
    "body": "### Your current environment\n\n<details>\n<summary>The output of <code>python collect_env.py</code></summary>\n\n```text\nCollecting environment information...\nPyTorch version: 2.7.0+cu126\nIs debug build: False\nCUDA used to build PyTorch: 12.6\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.6 LTS (x86_64)\nGCC version: (conda-forge gcc 13.3.0-2) 13.3.0\nClang version: Could not collect\nCMake version: version 3.16.3\nLibc version: glibc-2.31\n\nPython version: 3.12.9 | packaged by conda-forge | (main, Mar  4 2025, 22:48:41) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-5.4.0-144-generic-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: 12.6.68\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration:\nGPU 0: Tesla V100-SXM2-32GB\nGPU 1: Tesla V100-SXM2-32GB\nGPU 2: Tesla V100-SXM2-32GB\nGPU 3: Tesla V100-SXM2-32GB\nGPU 4: Tesla V100-SXM2-32GB\nGPU 5: Tesla V100-SXM2-32GB\nGPU 6: Tesla V100-SXM2-32GB\nGPU 7: Tesla V100-SXM2-32GB\n\nNvidia driver version: 560.35.03\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nByte Order:                      Little Endian\nAddress sizes:                   46 bits physical, 48 bits virtual\nCPU(s):                          40\nOn-line CPU(s) list:             0-39\nThread(s) per core:              1\nCore(s) per socket:              20\nSocket(s):                       2\nNUMA node(s):                    2\nVendor ID:                       GenuineIntel\nCPU family:                      6\nModel:                           85\nModel name:                      Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz\nStepping:                        7\nCPU MHz:                         880.197\nCPU max MHz:                     3900.0000\nCPU min MHz:                     800.0000\nBogoMIPS:                        4200.00\nVirtualization:                  VT-x\nL1d cache:                       1.3 MiB\nL1i cache:                       1.3 MiB\nL2 cache:                        40 MiB\nL3 cache:                        55 MiB\nNUMA node0 CPU(s):               0-19\nNUMA node1 CPU(s):               20-39\nVulnerability Itlb multihit:     KVM: Mitigation: Split huge pages\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT disabled\nVulnerability Retbleed:          Mitigation; Enhanced IBRS\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Mitigation; TSX disabled\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-cufile-cu12==1.11.1.6\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] pyzmq==27.0.0\n[pip3] torch==2.7.0\n[pip3] torchaudio==2.7.0\n[pip3] torchvision==0.22.0\n[pip3] transformers==4.53.0\n[pip3] triton==3.3.0\n[conda] cuda-toolkit                                12.6.0           0                   nvidia/label/cuda-12.6.0\n[conda] cuda-visual-tools                           12.6.0           0                   nvidia/label/cuda-12.6.0\n[conda] numpy                                       2.2.6            pypi_0              pypi\n[conda] nvidia-cublas-cu12                          12.6.4.1         pypi_0              pypi\n[conda] nvidia-cuda-cupti-cu12                      12.6.80          pypi_0              pypi\n[conda] nvidia-cuda-nvrtc-cu12                      12.6.77          pypi_0              pypi\n[conda] nvidia-cuda-runtime-cu12                    12.6.77          pypi_0              pypi\n[conda] nvidia-cudnn-cu12                           9.5.1.17         pypi_0              pypi\n[conda] nvidia-cufft-cu12                           11.3.0.4         pypi_0              pypi\n[conda] nvidia-cufile-cu12                          1.11.1.6         pypi_0              pypi\n[conda] nvidia-curand-cu12                          10.3.7.77        pypi_0              pypi\n[conda] nvidia-cusolver-cu12                        11.7.1.2         pypi_0              pypi\n[conda] nvidia-cusparse-cu12                        12.5.4.2         pypi_0              pypi\n[conda] nvidia-cusparselt-cu12                      0.6.3            pypi_0              pypi\n[conda] nvidia-nccl-cu12                            2.26.2           pypi_0              pypi\n[conda] nvidia-nvjitlink-cu12                       12.6.85          pypi_0              pypi\n[conda] nvidia-nvtx-cu12                            12.6.77          pypi_0              pypi\n[conda] pyzmq                                       27.0.0           pypi_0              pypi\n[conda] torch                                       2.7.0            pypi_0              pypi\n[conda] torchaudio                                  2.7.0            pypi_0              pypi\n[conda] torchvision                                 0.22.0           pypi_0              pypi\n[conda] transformers                                4.53.0           pypi_0              pypi\n[conda] triton                                      3.3.0            pypi_0              pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.0\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      NV2     NV2     NV1     NV1     NODE    NODE    NODE    20-39   1               N/A\nGPU1    NV2      X      NV1     NV1     NODE    NV2     NODE    NODE    20-39   1               N/A\nGPU2    NV2     NV1      X      NV2     NODE    NODE    NV1     NODE    20-39   1               N/A\nGPU3    NV1     NV1     NV2      X      NODE    NODE    NODE    NV2     20-39   1               N/A\nGPU4    NV1     NODE    NODE    NODE     X      NV2     NV2     NV1     20-39   1               N/A\nGPU5    NODE    NV2     NODE    NODE    NV2      X      NV1     NV1     20-39   1               N/A\nGPU6    NODE    NODE    NV1     NODE    NV2     NV1      X      NV2     20-39   1               N/A\nGPU7    NODE    NODE    NODE    NV2     NV1     NV1     NV2      X      20-39   1               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nVLLM_USE_MODELSCOPE=True\nLD_LIBRARY_PATH=/usr/local/cuda/lib64:\nCUDA_HOME=/usr/local/cuda\nCUDA_HOME=/usr/local/cuda\nNCCL_CUMEM_ENABLE=0\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n```\n\n</details>\n\n\n### \ud83d\udc1b Describe the bug\n\nFailed to execute tensor parallel inference\n\ncode:\n``` python\nfrom vllm import LLM, SamplingParams\n\nprompts = [\n    \"Hello, my name is\",\n    \"The president of the United States is\",\n    \"The capital of France is\",\n    \"The future of AI is\",\n]\n\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\nllm = LLM(model=\"LLM-Research/Meta-Llama-3.1-8B-Instruct\", tensor_parallel_size=8)\n\noutputs = llm.generate(prompts, sampling_params)\n\n# Print the outputs.\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n```\n\ncrucial stderr output as below:\n```\nFailures have been detected while processing an MLIR pass pipeline\n/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/attention/ops/prefix_prefill.py:36:0: note: Pipeline failed while executing [`ConvertTritonGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`\n/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/attention/ops/prefix_prefill.py:36:0: error: Failures have been detected while processing an MLIR pass pipeline\n/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/attention/ops/prefix_prefill.py:36:0: note: Pipeline failed while executing [`ConvertTritonGPUToLLVM` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/yexuming/bwl/hello/hello_vllm.py\", line 14, in <module>\n[rank0]:     outputs = llm.generate(prompts, sampling_params)\n[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/utils.py\", line 1267, in inner\n[rank0]:     return fn(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/entrypoints/llm.py\", line 474, in generate\n[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)\n[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/entrypoints/llm.py\", line 1517, in _run_engine\n[rank0]:     step_outputs = self.llm_engine.step()\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/engine/llm_engine.py\", line 1352, in step\n[rank0]:     outputs = self.model_executor.execute_model(\n[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 300, in execute_model\n[rank0]:     driver_outputs = self._driver_execute_model(execute_model_req)\n[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/executor/mp_distributed_executor.py\", line 145, in _driver_execute_model\n[rank0]:     return self.driver_worker.execute_model(execute_model_req)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/worker/worker_base.py\", line 421, in execute_model\n[rank0]:     output = self.model_runner.execute_model(\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[rank0]:     return func(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 1844, in execute_model\n[rank0]:     hidden_or_intermediate_states = model_executable(\n[rank0]:                                     ^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 581, in forward\n[rank0]:     model_output = self.model(input_ids, positions, intermediate_tensors,\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 173, in __call__\n[rank0]:     return self.forward(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 392, in forward\n[rank0]:     hidden_states, residual = layer(positions, hidden_states, residual)\n[rank0]:                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 305, in forward\n[rank0]:     hidden_states = self.self_attn(positions=positions,\n[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 203, in forward\n[rank0]:     attn_output = self.attn(q, k, v)\n[rank0]:                   ^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/attention/layer.py\", line 253, in forward\n[rank0]:     return torch.ops.vllm.unified_attention(\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/torch/_ops.py\", line 1158, in __call__\n[rank0]:     return self._op(*args, **(kwargs or {}))\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/attention/layer.py\", line 402, in unified_attention\n[rank0]:     output = self.impl.forward(self, query, key, value, kv_cache,\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/attention/backends/xformers.py\", line 582, in forward\n[rank0]:     out = PagedAttention.forward_prefix(\n[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/attention/ops/paged_attn.py\", line 214, in forward_prefix\n[rank0]:     context_attention_fwd(\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[rank0]:     return func(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/vllm/attention/ops/prefix_prefill.py\", line 850, in context_attention_fwd\n[rank0]:     _fwd_kernel[grid](\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/triton/runtime/jit.py\", line 347, in <lambda>\n[rank0]:     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n[rank0]:                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/triton/runtime/jit.py\", line 569, in run\n[rank0]:     kernel = self.compile(src, target=target, options=options.__dict__)\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/triton/compiler/compiler.py\", line 284, in compile\n[rank0]:     next_module = compile_ir(module, metadata)\n[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/triton/backends/nvidia/compiler.py\", line 450, in <lambda>\n[rank0]:     stages[\"llir\"] = lambda src, metadata: self.make_llir(src, metadata, options, capability)\n[rank0]:                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/site-packages/triton/backends/nvidia/compiler.py\", line 341, in make_llir\n[rank0]:     pm.run(mod)\n[rank0]: RuntimeError: PassManager::run failed\nProcessed prompts:   0%|              | 0/4 [00:05<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n[rank0]:[W701 13:59:11.991304831 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n/usr/local/bwlsoft/miniconda3/envs/kvcake/lib/python3.12/multiprocessing/resource_tracker.py:255: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n```\n\ndebug log can be find here: https://gist.github.com/WilliamBy/0c5719d2bb00fa65c08c187a9d7d1d56\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "open",
    "created_at": "2025-07-01T06:18:54+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/20303/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/20303"
  },
  {
    "number": 9216,
    "title": "[Bug]: error when the concurrency reaches 10 with gptq4bits",
    "body": "### Your current environment\n\n\r\n\r\nvllm 6.0\r\n\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\n\r\n\r\n```Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 411, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 113, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 715, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 735, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 288, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 76, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 73, in app\r\n    response = await f(request)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 297, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 210, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n  File \"/fschat/vllm_worker.py\", line 267, in api_generate\r\n    await engine.abort(request_id)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 789, in abort\r\n    raise AsyncEngineDeadError(\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Background loop is not running. If it was running, inspect the output to find the stacktrace of the error that caused the background loop to stop (AsyncEngineDeadError).\r\n```\r\n\r\n\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-10-10T03:52:15+00:00",
    "closed_at": "2025-02-09T02:01:31+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/9216/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/9216"
  },
  {
    "number": 15359,
    "title": "[Bug]: Can't create non-root user using vllm/vllm-openai:v0.8.1 as a base image",
    "body": "### Your current environment\n\nU used to create a non-root user Docker image for vLLM and the following version was working fine up to v0.7.3:\n\n``` \nFROM vllm/vllm-openai:v0.7.3\n\nENV PYTHONUNBUFFERED=1\nENV HF_HUB_CACHE=/api/models\nENV HF_HOME=/api/models\n\nRUN mkdir -p /api/models/\n\n# RUN chmod +x /api/entrypoint.sh\nRUN chmod 777 -R /api \\\n  && umask 000\n\nEXPOSE 8000\n\n# Set user and group\nARG user=appuser\nARG group=appuser\nARG uid=1000\nARG gid=1000\nRUN groupadd -g ${gid} ${group}\nRUN useradd -u ${uid} -g ${group} -s /bin/sh -m ${user}\n    \nRUN chown ${user}:${group} /api\n\n# Switch to user\nUSER ${uid}:${gid}\n```\n\nToday was trying to make the same with v0.8.1 and got permissions errors like this:\n`bash: /opt/venv/bin/vllm: /opt/venv/bin/python3: bad interpreter: Permission denied`\n\nTo my understanding, in addition, `/opt/venv/bin/python3` is actually a symbolic link pointing to `/root/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/bin/python3.12`\n\nAnd even Dockefile modification that eventually has the following content does not work:\n```\nFROM vllm/vllm-openai:v0.8.1\n\nENV PYTHONUNBUFFERED=1\nENV HF_HUB_CACHE=/api/models\nENV HF_HOME=/api/models\n\nRUN mkdir -p /api/models/\n\n# Set user and group\nARG user=appuser\nARG group=appuser\nARG uid=1000\nARG gid=1000\n\n# Create non-root user and group\nRUN groupadd -g ${gid} ${group} \\\n    && useradd -u ${uid} -g ${group} -s /bin/sh -m ${user}\n\n# Adjust permissions for Python environment and model directories\nRUN chmod -R 777 /root/.local/share/uv/python/\nRUN chown -R ${user}:${group} /api/models\n\n# Ensure vllm workspace and virtual environment have correct permissions\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN chown -R ${user}:${group} /vllm-workspace \\\n    && chown -R ${user}:${group} ${VIRTUAL_ENV}\n\n# Explicitly fix interpreter permissions\nUSER root\nRUN chmod -R 755 ${VIRTUAL_ENV} \\\n    && chmod +x ${VIRTUAL_ENV}/bin/python3 \\\n    && chmod +x ${VIRTUAL_ENV}/bin/vllm \\\n    && chown -R ${user}:${group} ${VIRTUAL_ENV} \\\n    && chmod -R a+rX /root/.local/share/uv/python\n\n# Restore non-root user context\nUSER ${uid}:${gid}\n\n# Ensure PATH is correctly set\nENV PATH=\"${VIRTUAL_ENV}/bin:/home/${user}/.local/bin:${PATH}\"\n\nRUN chmod -R 777 /api && umask 000\n\nEXPOSE 8000\nWORKDIR /vllm-workspace\n\n```\n\nI would very appreciate if anyone shares his solution for this task or any advice in troubleshooting,\n\n### \ud83d\udc1b Describe the bug\n\n`bash: /opt/venv/bin/vllm: /opt/venv/bin/python3: bad interpreter: Permission denied`\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-03-23T15:02:23+00:00",
    "closed_at": "2025-03-24T12:53:12+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/15359/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/15359"
  },
  {
    "number": 16538,
    "title": "[Usage]: how to use cpu offload gb with v1 engine my 4080 only has 16gb vram i want to use 64 of my system rams of gigabytes",
    "body": "### Your current environment\n\n<details>```text\nPyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 24.10 (x86_64)\nGCC version: (Ubuntu 14.2.0-4ubuntu2) 14.2.0\nClang version: 19.1.1 (1ubuntu1)\nCMake version: version 3.31.6\nLibc version: glibc-2.40\n\nPython version: 3.12.7 (main, Feb  4 2025, 14:46:03) [GCC 14.2.0] (64-bit runtime)\nPython platform: Linux-6.11.0-19-generic-x86_64-with-glibc2.40\nIs CUDA available: True\nCUDA runtime version: 12.6.77\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4080\nNvidia driver version: 565.57.01\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               32\nOn-line CPU(s) list:                  0-31\nVendor ID:                            AuthenticAMD\nModel name:                           AMD Ryzen 9 7950X 16-Core Processor\nCPU family:                           25\nModel:                                97\nThread(s) per core:                   2\nCore(s) per socket:                   16\nSocket(s):                            1\nStepping:                             2\nFrequency boost:                      enabled\nCPU(s) scaling MHz:                   57%\nCPU max MHz:                          5881.0000\nCPU min MHz:                          545.0000\nBogoMIPS:                             8983.96\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl xtopology nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid overflow_recov succor smca fsrm flush_l1d amd_lbr_pmc_freeze\nVirtualization:                       AMD-V\nL1d cache:                            512 KiB (16 instances)\nL1i cache:                            512 KiB (16 instances)\nL2 cache:                             16 MiB (16 instances)\nL3 cache:                             64 MiB (2 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-31\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-ml-py==12.560.30\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] onnxruntime==1.20.1\n[pip3] pynvml==12.0.0\n[pip3] pyzmq==26.2.0\n[pip3] rapidocr-onnxruntime==1.3.24\n[pip3] sentence-transformers==3.3.1\n[pip3] torch==2.6.0\n[pip3] torchac_cuda==0.2.5\n[pip3] torchao==0.7.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] transformers==4.51.2\n[pip3] triton==3.2.0\n[conda] No relevant packages\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.8.3\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\nGPU0     X      0-31    0               N/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\nLD_LIBRARY_PATH=/usr/local/cuda-12.6/lib64:/usr/local/cuda-12.6/lib64:\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY```</details>\n\n\n### How would you like to use vllm\n\nI want to run inference of a [qwen 2.5 7b 1m bnb 4bit](https://huggingface.co/unsloth/Qwen2.5-7B-Instruct-1M-unsloth-bnb-4bit). I don't know how to get vllm v1 engine to accept my system ram as well, or just run on the cpu alltogether. it says it dont support cpu.\nalso when quant kv cache for ada arch (i think thats rtx 4080?)\n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage",
      "stale"
    ],
    "state": "open",
    "created_at": "2025-04-12T16:54:10+00:00",
    "closed_at": null,
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/16538/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/16538"
  },
  {
    "number": 8783,
    "title": "[Bug]: Decode n tokens gives different output for first seq position compared to decode 1 token",
    "body": "### Your current environment\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.27.6\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-176-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nGPU 1: NVIDIA A100-SXM4-80GB\r\nGPU 2: NVIDIA A100-SXM4-80GB\r\nGPU 3: NVIDIA A100-SXM4-80GB\r\nGPU 4: NVIDIA A100-SXM4-80GB\r\nGPU 5: NVIDIA A100-SXM4-80GB\r\nGPU 6: NVIDIA A100-SXM4-80GB\r\nGPU 7: NVIDIA A100-SXM4-80GB\r\n\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.5\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             224\r\nOn-line CPU(s) list:                0-223\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8480+\r\nCPU family:                         6\r\nModel:                              143\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 56\r\nSocket(s):                          2\r\nStepping:                           8\r\nBogoMIPS:                           4000.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid cldemote movdiri movdir64b md_clear pconfig flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          5.3 MiB (112 instances)\r\nL1i cache:                          3.5 MiB (112 instances)\r\nL2 cache:                           224 MiB (112 instances)\r\nL3 cache:                           210 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190,192,194,196,198,200,202,204,206,208,210,212,214,216,218,220,222\r\nNUMA node1 CPU(s):                  1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159,161,163,165,167,169,171,173,175,177,179,181,183,185,187,189,191,193,195,197,199,201,203,205,207,209,211,213,215,217,219,221,223\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.68\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torch-xla==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.2\r\n[pip3] triton==3.0.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\r\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\n[conda] nvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\r\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\r\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\r\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\r\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\r\n[conda] nvidia-ml-py              12.560.30                pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] nvidia-nvjitlink-cu12     12.6.68                  pypi_0    pypi\r\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\r\n[conda] pyzmq                     26.2.0                   pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torch-xla                 2.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.44.2                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.1.post2@9ba0817ff1eb514f51cc6de9cb8e16c98d6ee44f\r\nvLLM Build Flags:\r\nCUDA Archs: 5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV12    NV12    NV12    NV12    NV12    NV12    NV12    PXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0,2,4,6,8,10    0               N/A\r\nGPU1    NV12     X      NV12    NV12    NV12    NV12    NV12    NV12    SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0,2,4,6,8,10    0               N/A\r\nGPU2    NV12    NV12     X      NV12    NV12    NV12    NV12    NV12    SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     0,2,4,6,8,10    0               N/A\r\nGPU3    NV12    NV12    NV12     X      NV12    NV12    NV12    NV12    SYS     SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     0,2,4,6,8,10    0               N/A\r\nGPU4    NV12    NV12    NV12    NV12     X      NV12    NV12    NV12    SYS     SYS     SYS     SYS     SYS     PXB     PXB     SYS     SYS     SYS     1,3,5,7,9,11    1               N/A\r\nGPU5    NV12    NV12    NV12    NV12    NV12     X      NV12    NV12    SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB     SYS     SYS     1,3,5,7,9,11    1               N/A\r\nGPU6    NV12    NV12    NV12    NV12    NV12    NV12     X      NV12    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB     SYS     1,3,5,7,9,11    1               N/A\r\nGPU7    NV12    NV12    NV12    NV12    NV12    NV12    NV12     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB     1,3,5,7,9,11    1               N/A\r\nNIC0    PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC1    PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC2    SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC3    SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC4    SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS\r\nNIC5    SYS     SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     SYS     SYS     SYS\r\nNIC6    SYS     SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      SYS     SYS     SYS\r\nNIC7    SYS     SYS     SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS\r\nNIC8    SYS     SYS     SYS     SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS\r\nNIC9    SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  NIC8: mlx5_8\r\n  NIC9: mlx5_9\r\n```\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI'm running speculative decoding with the same model as the draft and the target. We should expect to see 100% acceptance rate, and logits that are the same. Here is the minimal script I run. \r\n\r\n```python\r\nfrom vllm import LLM, SamplingParams\r\nimport time\r\nimport torch\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\n\r\nllm = LLM(\r\n    model=\"meta-llama/Meta-Llama-3.1-8b\",\r\n    dtype='bfloat16',\r\n    tensor_parallel_size=1,\r\n    enforce_eager=True,\r\n    speculative_model=\"meta-llama/Meta-Llama-3.1-8b\",\r\n    num_speculative_tokens=1,\r\n    use_v2_block_manager=True,\r\n    seed=0,\r\n    disable_log_stats=False,\r\n)\r\n\r\nmax_gen_tokens = 2\r\nsampling_params = SamplingParams(temperature=1.0, min_tokens=max_gen_tokens, max_tokens=max_gen_tokens)\r\n\r\nbsz = 1\r\nwarmup_prompts = [\r\n    \"The future of AI is \",\r\n]   \r\nwarmup_prompts = warmup_prompts * bsz\r\n\r\nstart = time.perf_counter()\r\noutputs = llm.generate(warmup_prompts, sampling_params)\r\ntorch.cuda.synchronize()\r\nend = time.perf_counter()\r\nprint(f\"elapsed time: {end - start:.2f} s\")\r\n```\r\n\r\nAnd I get the following output. The tensors are hidden_states. Note that I modified the llama.py file so that these would be printed. I replace `compute_logits` with. \r\n\r\n```python\r\ndef compute_logits(\r\n    self,\r\n    hidden_states: torch.Tensor,\r\n    sampling_metadata: SamplingMetadata,\r\n) -> Optional[torch.Tensor]:\r\n    torch.set_printoptions(profile=\"full\")\r\n    print(hidden_states[0,:10])\r\n    logits = self.logits_processor(self.lm_head, hidden_states,\r\n                                   sampling_metadata)\r\n    return logits\r\n```\r\n```\r\nProcessed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\r\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.78it/s, est. speed input: 47.63 toks/s, output: 13.61 toks/s]\r\ntensor([-1.1953, -1.8750,  0.5742,  0.3691,  0.1396,  0.0369,  1.5859,  0.3027,\r\n        -1.2734, -0.6406], device='cuda:0', dtype=torch.bfloat16)\r\ntensor([-1.1953, -1.8750,  0.5742,  0.3691,  0.1396,  0.0369,  1.5859,  0.3027,\r\n        -1.2734, -0.6406], device='cuda:0', dtype=torch.bfloat16)\r\ntensor([ 2.2500,  2.4844, -0.9688, -2.5469,  0.6836,  3.3125,  1.6875,  0.1318,\r\n        -0.9297, -4.6875], device='cuda:0', dtype=torch.bfloat16)\r\ntensor([ 2.2344,  2.4844, -0.9766, -2.5781,  0.6719,  3.2969,  1.7031,  0.1533,\r\n        -0.9375, -4.7188], device='cuda:0', dtype=torch.bfloat16)\r\nelapsed time: 0.16 s\r\n```\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-09-24T22:00:05+00:00",
    "closed_at": "2025-01-24T01:58:43+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8783/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8783"
  },
  {
    "number": 17780,
    "title": "[Feature][Improvement]: Benchmarking with random conversation lengths",
    "body": "### \ud83d\ude80 The feature, motivation and pitch\n\n### Background\nI'm trying to figure out how many users my vLLM server can handle and how increases in number of users and average request rate per user over time effect the important metrics (ttft, tpot, itl). \nIn this research, I'm assuming the users will create conversation of varying lengths.\n\nI've started this research using the ShareGPT dataset, and I found that the number of users I was able to host for was a good amount above my expectations.\n\nI started looking into how sampling was implemented for ShareGPT, and found that each SampleRequest is generated using only the two first turns of the conversation. (In ShareGPT many of the conversations are multi turn conversation).\n\n### Feature\nI would suggest a feature that makes the benchmark user able to control a maximum random length of conversations. Such that when sampling occurs, it isn't only 2 first turns of the conversation, but perhaps the first 6 turns.\n\n### Why\nSampling only the first two turns of each conversation underrepresents real-world usage, where conversations are often multi-turn and longer. This can lead to overly optimistic benchmark results. Supporting variable-length sampling would better reflect realistic workloads and improve the accuracy of performance evaluations.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "feature request"
    ],
    "state": "open",
    "created_at": "2025-05-07T10:01:05+00:00",
    "closed_at": null,
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/17780/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/17780"
  },
  {
    "number": 12225,
    "title": "[Usage]: Guided choice not working as expected",
    "body": "### Your current environment\n\n```\nPyTorch version: 2.5.1\n\n\nVersions of relevant libraries:\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] nvidia-ml-py==12.560.30\n[pip3] onnxruntime==1.20.1\n[pip3] pyzmq==26.2.0\n[pip3] sentence-transformers==3.3.1\n[pip3] torch==2.5.1\n[pip3] torchvision==0.20.1\n[pip3] transformers==4.48.0\n[conda] Could not collect\nvLLM Version: 0.6.6.post1\nvLLM Build Flags:\n```\n\n### How would you like to use vllm\n\nI am using a hosted vLLM model for inference. I would like to be able to use the `guided_choice` param to constrain my outputs. I was using the example provided on the vLLM documentation [here](https://docs.vllm.ai/en/latest/features/structured_outputs.html). \n\n\nThis is my current setup: \n\n\n```python\nfrom openai import OpenAI\n\napi_key = \"<api_key>\"\nhosted_url = \"<hosted_url>\"\n\nclient = OpenAI(\n    base_url=hosted_url,\n    api_key=api_key\n)\n\ncompletion = client.chat.completions.create(\n    model=\"<model>\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Classify this sentiment: vLLM is wonderful!\"}\n    ],\n    extra_body={\"guided_choice\": [\"positive\", \"negative\"]},\n)\nprint(completion.choices[0].message.content)\n```\n\nThis is the output that I get: \n\n```\nThe sentiment of the statement \"vLLM is wonderful!\" is positive.\n```\n\nAs is seen, the model does not give me back just positive as I would like it to be. I would like to know how I can enforce the model to strictly adhere to the format provided. \n\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "usage"
    ],
    "state": "open",
    "created_at": "2025-01-20T16:02:03+00:00",
    "closed_at": null,
    "comments": 11,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12225/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12225"
  },
  {
    "number": 7623,
    "title": "[Doc]: Has the offline chat inference function been updated?",
    "body": "### \ud83d\udcda The doc issue\n\nI did see the official documentation contains the offline inference chat function. But I still get the Attribute Error where LLM object does not have chat() attribute. Has this been updated in the latest package?\n\n### Suggest a potential alternative/fix\n\n_No response_",
    "labels": [
      "documentation"
    ],
    "state": "closed",
    "created_at": "2024-08-17T09:00:17+00:00",
    "closed_at": "2024-09-14T03:10:21+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/7623/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/7623"
  },
  {
    "number": 19213,
    "title": "[Usage]: Can multimodal models, such as qwen2.5vl, use the PD separation feature?",
    "body": "### Your current environment\n\nCan multimodal models, such as qwen2.5vl, use the PD separation feature?\n\n\n",
    "labels": [
      "usage"
    ],
    "state": "open",
    "created_at": "2025-06-05T12:56:17+00:00",
    "closed_at": null,
    "comments": 0,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19213/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19213"
  },
  {
    "number": 5420,
    "title": "[Bug]: Automatic Prefix caching not working while hitting same request multiple times",
    "body": "### Your current environment\r\n\r\nPyTorch version: 2.1.2+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.3\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.16 (main, Oct 26 2023, 03:04:46)  [GCC 9.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1048-aws-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A10G\r\nNvidia driver version: 535.104.12\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nCPU(s):                             4\r\nOn-line CPU(s) list:                0-3\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 2\r\nSocket(s):                          1\r\nNUMA node(s):                       1\r\nVendor ID:                          AuthenticAMD\r\nCPU family:                         23\r\nModel:                              49\r\nModel name:                         AMD EPYC 7R32\r\nStepping:                           0\r\nCPU MHz:                            2800.000\r\nBogoMIPS:                           5600.00\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          64 KiB\r\nL1i cache:                          64 KiB\r\nL2 cache:                           1 MiB\r\nL3 cache:                           8 MiB\r\nNUMA node0 CPU(s):                  0-3\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec rstack overflow: Mitigation; safe RET\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr ssesse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save rdpid\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.18.1\r\n[pip3] onnxruntime==1.17.3\r\n[pip3] sentence-transformers==2.6.1\r\n[pip3] torch==2.1.2\r\n[pip3] torchaudio==2.1.2\r\n[pip3] torchvision==0.16.2\r\n[pip3] transformers==4.40.2\r\n[pip3] triton==2.1.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.4.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-3             N/A             N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nI am trying to hit the same request twice to the open ai compatible server but the response is taking almost the same time inspite of the 2000 input tokens being the same. I am using mistral instruct v2 7b model. I assume when i hit the same request again with temperature 0 the second response should be faster as it would have cached the input tokens and also as the output tokens also are unchanged.\r\n\r\nCan anyone please help me with this @simon-mo @robertgshaw2-neuralmagic @SageMoore @ElizaWszola @mgoin \r\n![frf](https://github.com/vllm-project/vllm/assets/44570240/0e83ff8e-c428-4372-b951-5029cd228380)\r\n![test2](https://github.com/vllm-project/vllm/assets/44570240/e7e03d73-f13f-426d-8444-a242af9f7ee4)\r\nI observe the KV cache is automatically offloaded after the request is done\r\n\r\n",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-06-11T13:14:14+00:00",
    "closed_at": "2024-06-11T13:19:34+00:00",
    "comments": 2,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/5420/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/5420"
  },
  {
    "number": 12267,
    "title": "[Bug]: AttributeError: '_OpNamespace' '_C' object has no attribute 'gptq_marlin_repack'",
    "body": "### Your current environment\n\n<details>\n<summary>The output of `python collect_env.py`</summary>\n\n```text\nPyTorch version: 2.5.1+cpu\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Red Hat Enterprise Linux 9.5 (Plow) (x86_64)\nGCC version: (conda-forge gcc 11.4.0-13) 11.4.0\nClang version: Could not collect\nCMake version: version 3.28.4\nLibc version: glibc-2.34\n\nPython version: 3.12.8 | packaged by conda-forge | (main, Dec  5 2024, 14:24:40) [GCC 13.3.0] (64-bit runtime)\nPython platform: Linux-5.14.0-503.21.1.el9_5.x86_64-x86_64-with-glibc2.34\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        46 bits physical, 57 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               112\nOn-line CPU(s) list:                  0-111\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Xeon(R) Platinum 8480C\nCPU family:                           6\nModel:                                143\nThread(s) per core:                   2\nCore(s) per socket:                   56\nSocket(s):                            1\nStepping:                             8\nCPU(s) scaling MHz:                   29%\nCPU max MHz:                          3800.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             4000.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            2.6 MiB (56 instances)\nL1i cache:                            1.8 MiB (56 instances)\nL2 cache:                             112 MiB (56 instances)\nL3 cache:                             105 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-111\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] intel_extension_for_pytorch==2.5.0+git6973d57\n[pip3] numpy==1.26.4\n[pip3] pynvml==11.5.3\n[pip3] pyzmq==26.2.0\n[pip3] torch==2.5.1+cpu\n[pip3] torchaudio==2.5.0+cpu\n[pip3] torchvision==0.20.1+cpu\n[pip3] transformers==4.47.1\n[pip3] triton==3.1.0\n[conda] intel-extension-for-pytorch 2.5.0+git6973d57          pypi_0    pypi\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] pynvml                    11.5.3                   pypi_0    pypi\n[conda] pyzmq                     26.2.0          py312hbf22597_3    conda-forge\n[conda] torch                     2.5.1+cpu                pypi_0    pypi\n[conda] torchaudio                2.5.0+cpu                pypi_0    pypi\n[conda] torchvision               0.20.1+cpu               pypi_0    pypi\n[conda] transformers              4.47.1                   pypi_0    pypi\n[conda] triton                    3.1.0                    pypi_0    pypi\nROCM Version: Could not collect\nNeuron SDK Version: N/A\nvLLM Version: 0.6.5\nvLLM Build Flags:\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\nCould not collect\n```\n\n</details>\n\n\n### Model Input Dumps\n\n_No response_\n\n### \ud83d\udc1b Describe the bug\n\nGetting error ('_OpNamespace' '_C' object has no attribute 'gptq_marlin_repack') on loading a FP8-Dynamic (llmcompressor) quantized model on vllm 0.6.5 with ipex 2.5.0 on CPU\n\n```python\nfrom vllm import LLM, SamplingParams\nimport pandas as pd\nimport csv\n\ndef run_inference(model_path: str =None):\n    # Initialize the model explicitly for CPU with adjusted settings\n    llm = LLM(\n        model=model_path,  # Replace with the path to your model\n        tokenizer=model_path,  # Replace with the path to your tokenizer\n        device=\"cpu\",  # Explicitly use CPU\n        max_model_len=8124\n    )\n\n    tokenizer = llm.get_tokenizer()\n    sampling_params = SamplingParams(\n        temperature=0.0,\n        max_tokens=750,\n        top_k=1,\n        # top_p=0.95,\n        stop_token_ids=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],\n        # stop=EOS\n        )\n\nif __name__ == \"__main__\":\n    #take the model path as input argument from terminal\n    import sys\n    model_path=sys.argv[1]\n    # print(model_path)\n    run_inference(model_path)\n```\n\n```\n/home/shubham/miniconda3/lib/python3.12/site-packages/oneccl_bindings_for_pytorch/__init__.py:25: UserWarning: Warning: Cannot load xpu CCL. CCL doesn't work for XPU device due to /home/shubham/miniconda3/lib/python3.12/site-packages/oneccl_bindings_for_pytorch/lib/liboneccl_bindings_for_pytorch_xpu.so: cannot open shared object file: No such file or directory\n  warnings.warn(f\"Warning: Cannot load xpu CCL. CCL doesn't work for XPU device due to {e}\")\nINFO 01-21 08:44:38 config.py:478] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed', 'score'}. Defaulting to 'generate'.\nWARNING 01-21 08:44:38 _logger.py:72] Async output processing is not supported on the current platform type cpu.\nWARNING 01-21 08:44:38 _logger.py:72] CUDA graph is not supported on CPU, fallback to the eager mode.\nWARNING 01-21 08:44:38 _logger.py:72] Environment variable VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by default.\nINFO 01-21 08:44:38 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='/home/shubham/quantization/llama-8B/Llama-3.1-8B-Instruct-FP8-Dynamic', speculative_config=None, tokenizer='/home/shubham/quantization/llama-8B/Llama-3.1-8B-Instruct-FP8-Dynamic', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8124, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/shubham/quantization/llama-8B/Llama-3.1-8B-Instruct-FP8-Dynamic, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \nINFO 01-21 08:44:39 cpu.py:33] Cannot use _Backend.FLASH_ATTN backend on CPU.\nINFO 01-21 08:44:39 selector.py:141] Using Torch SDPA backend.\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.93s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.66s/it]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.70s/it]\n\nWARNING 01-21 08:44:44 utils.py:673] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.\nERROR 01-21 08:44:44 _custom_ops.py:61] Error in calling custom op gptq_marlin_repack: '_OpNamespace' '_C' object has no attribute 'gptq_marlin_repack'\nERROR 01-21 08:44:44 _custom_ops.py:61] Possibly you have built or installed an obsolete version of vllm.\nERROR 01-21 08:44:44 _custom_ops.py:61] Please try a clean build and install of vllm,or remove old built files such as vllm/*cpython*.so and build/ .\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/shubham/quantization/quantization.py\", line 63, in <module>\n[rank0]:     run_inference(model_path)\n[rank0]:   File \"/home/shubham/quantization/quantization.py\", line 9, in run_inference\n[rank0]:     llm = LLM(\n[rank0]:           ^^^^\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/utils.py\", line 990, in inner\n[rank0]:     return fn(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/entrypoints/llm.py\", line 230, in __init__\n[rank0]:     self.llm_engine = self.engine_class.from_engine_args(\n[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/engine/llm_engine.py\", line 532, in from_engine_args\n[rank0]:     engine = cls(\n[rank0]:              ^^^^\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/engine/llm_engine.py\", line 288, in __init__\n[rank0]:     self.model_executor = executor_class(vllm_config=vllm_config, )\n[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/executor/executor_base.py\", line 36, in __init__\n[rank0]:     self._init_executor()\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/executor/cpu_executor.py\", line 107, in _init_executor\n[rank0]:     self._run_workers(\"load_model\")\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/executor/cpu_executor.py\", line 162, in _run_workers\n[rank0]:     driver_worker_output = self.driver_method_invoker(\n[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/executor/cpu_executor.py\", line 298, in _driver_method_invoker\n[rank0]:     return getattr(driver, method)(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/worker/cpu_worker.py\", line 221, in load_model\n[rank0]:     self.model_runner.load_model()\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/worker/cpu_model_runner.py\", line 438, in load_model\n[rank0]:     self.model = get_model(vllm_config=self.vllm_config)\n[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/model_executor/model_loader/__init__.py\", line 12, in get_model\n[rank0]:     return loader.load_model(vllm_config=vllm_config)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/model_executor/model_loader/loader.py\", line 384, in load_model\n[rank0]:     quant_method.process_weights_after_loading(module)\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py\", line 342, in process_weights_after_loading\n[rank0]:     layer.scheme.process_weights_after_loading(layer)\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py\", line 56, in process_weights_after_loading\n[rank0]:     prepare_fp8_layer_for_marlin(layer, strategy=\"channel\")\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py\", line 66, in prepare_fp8_layer_for_marlin\n[rank0]:     marlin_qweight = ops.gptq_marlin_repack(b_q_weight=pack_fp8_to_int32(\n[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/_custom_ops.py\", line 62, in wrapper\n[rank0]:     raise e\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/_custom_ops.py\", line 44, in wrapper\n[rank0]:     return fn(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/_custom_ops.py\", line 574, in gptq_marlin_repack\n[rank0]:     return torch.ops._C.gptq_marlin_repack(b_q_weight, perm, size_k, size_n,\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/home/shubham/miniconda3/lib/python3.12/site-packages/torch/_ops.py\", line 1225, in __getattr__\n[rank0]:     raise AttributeError(\n[rank0]: AttributeError: '_OpNamespace' '_C' object has no attribute 'gptq_marlin_repack'\nException ignored in: <function LLM.__del__ at 0x7ff3cdfdfba0>\nTraceback (most recent call last):\n  File \"/home/shubham/miniconda3/lib/python3.12/site-packages/vllm-0.6.5+cpu-py3.12-linux-x86_64.egg/vllm/entrypoints/llm.py\", line 236, in __del__\n    if self.llm_engine and hasattr(self.llm_engine, \"shutdown\"):\n       ^^^^^^^^^^^^^^^\nAttributeError: 'LLM' object has no attribute 'llm_engine'\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug",
      "stale"
    ],
    "state": "closed",
    "created_at": "2025-01-21T13:55:04+00:00",
    "closed_at": "2025-05-22T02:11:02+00:00",
    "comments": 4,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/12267/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/12267"
  },
  {
    "number": 4313,
    "title": "[Installation]: Compile and Install from source",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: 2.2.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-155-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.91\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A800 80GB PCIe\r\nGPU 1: NVIDIA A800 80GB PCIe\r\nGPU 2: NVIDIA A800 80GB PCIe\r\nGPU 3: NVIDIA A800 80GB PCIe\r\nGPU 4: NVIDIA A800 80GB PCIe\r\nGPU 5: NVIDIA A800 80GB PCIe\r\nGPU 6: NVIDIA A800 80GB PCIe\r\nGPU 7: NVIDIA A800 80GB PCIe\r\n\r\nNvidia driver version: 535.54.03\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nThread(s) per core:              2\r\nCore(s) per socket:              32\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           106\r\nModel name:                      Intel(R) Xeon(R) Gold 6338 CPU @ 2.00GHz\r\nStepping:                        6\r\nCPU MHz:                         800.000\r\nCPU max MHz:                     3200.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        4000.00\r\nVirtualization:                  VT-x\r\nL1d cache:                       3 MiB\r\nL1i cache:                       2 MiB\r\nL2 cache:                        80 MiB\r\nL3 cache:                        96 MiB\r\nNUMA node0 CPU(s):               0-31,64-95\r\nNUMA node1 CPU(s):               32-63,96-127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.19.3\r\n[pip3] torch==2.2.1\r\n[pip3] triton==2.2.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.3.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.19.3                   pypi_0    pypi\r\n[conda] torch                     2.2.1                    pypi_0    pypi\r\n[conda] triton                    2.2.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.3.0             pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV8\tPXB\tPXB\tPXB\tPXB\tPXB\tPXB\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU1\tNV8\t X \tPXB\tPXB\tPXB\tPXB\tPXB\tPXB\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU2\tPXB\tPXB\t X \tPXB\tPXB\tPXB\tPXB\tNV8\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU3\tPXB\tPXB\tPXB\t X \tNV8\tPXB\tPXB\tPXB\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU4\tPXB\tPXB\tPXB\tNV8\t X \tPXB\tPXB\tPXB\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU5\tPXB\tPXB\tPXB\tPXB\tPXB\t X \tNV8\tPXB\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU6\tPXB\tPXB\tPXB\tPXB\tPXB\tNV8\t X \tPXB\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU7\tPXB\tPXB\tNV8\tPXB\tPXB\tPXB\tPXB\t X \tSYS\t0-31,64-95\t0\t\tN/A\r\nNIC0\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_bond_0\r\n```\r\n\n\n### How you are installing vllm\n\n```sh\r\n# I git clone the repo in 2024-04-10, then compile to get a whl package.\r\nexport MAX_JOBS=6\r\nexport CUDA_HOME=/usr/local/cuda-12.2\r\nexport PATH=\"${CUDA_HOME}/bin:$PATH\"\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.2/bin:/usr/local/cuda-12.2/lib64:$LD_LIBRARY_PATH\r\npython3 setup.py bdist_wheel --dist-dir=dist\r\n```\r\n\r\nlatest commit:\r\n\r\ncommit c2e00af523b0638dcca68c9a42a9187449841ced (HEAD -> main, origin/main, origin/HEAD)\r\nAuthor: zhaotyer <89376832+zhaotyer@users.noreply.github.com>\r\nDate:   Wed Apr 10 12:49:11 2024 +0800\r\n\r\n    [Bugfix]  fix utils.py/merge_dict func TypeError: 'type' object is not subscriptable (#3955)\r\n    \r\n    Co-authored-by: tianyi_zhao <tianyi.zhao@transwarp.io>\r\n",
    "labels": [
      "installation",
      "stale"
    ],
    "state": "closed",
    "created_at": "2024-04-24T02:05:22+00:00",
    "closed_at": "2024-11-29T02:06:27+00:00",
    "comments": 13,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/4313/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/4313"
  },
  {
    "number": 19890,
    "title": "[Bug]: enqueue.cc:1556 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'",
    "body": "### Your current environment\n\nINFO 06-20 10:37:01 [__init__.py:243] Automatically detected platform cuda.\nCollecting environment information...\nuv is set\n==============================\n        System Info\n==============================\nOS                           : Ubuntu 24.04.2 LTS (x86_64)\nGCC version                  : (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\nClang version                : Could not collect\nCMake version                : version 4.0.3\nLibc version                 : glibc-2.39\n\n==============================\n       PyTorch Info\n==============================\nPyTorch version              : 2.7.1+cu128\nIs debug build               : False\nCUDA used to build PyTorch   : 12.8\nROCM used to build PyTorch   : N/A\n\n==============================\n      Python Environment\n==============================\nPython version               : 3.11.13 (main, Jun  4 2025, 17:37:17) [Clang 20.1.4 ] (64-bit runtime)\nPython platform              : Linux-6.11.0-26-generic-x86_64-with-glibc2.39\n\n==============================\n       CUDA / GPU Info\n==============================\nIs CUDA available            : True\nCUDA runtime version         : 12.8.61\nCUDA_MODULE_LOADING set to   : LAZY\nGPU models and configuration : \nGPU 0: NVIDIA GeForce RTX 5090\nGPU 1: NVIDIA GeForce RTX 5090\n\nNvidia driver version        : 570.86.10\ncuDNN version                : Could not collect\nHIP runtime version          : N/A\nMIOpen runtime version       : N/A\nIs XNNPACK available         : True\n\n==============================\n          CPU Info\n==============================\n\u67b6\u6784\uff1a                                x86_64\nCPU \u8fd0\u884c\u6a21\u5f0f\uff1a                        32-bit, 64-bit\nAddress sizes:                        46 bits physical, 48 bits virtual\n\u5b57\u8282\u5e8f\uff1a                              Little Endian\nCPU:                                  32\n\u5728\u7ebf CPU \u5217\u8868\uff1a                       0-31\n\u5382\u5546 ID\uff1a                             GenuineIntel\n\u578b\u53f7\u540d\u79f0\uff1a                            Intel(R) Core(TM) i9-14900KS\nCPU \u7cfb\u5217\uff1a                            6\n\u578b\u53f7\uff1a                                183\n\u6bcf\u4e2a\u6838\u7684\u7ebf\u7a0b\u6570\uff1a                      2\n\u6bcf\u4e2a\u5ea7\u7684\u6838\u6570\uff1a                        24\n\u5ea7\uff1a                                  1\n\u6b65\u8fdb\uff1a                                1\nCPU(s) scaling MHz:                   32%\nCPU \u6700\u5927 MHz\uff1a                        6300.0000\nCPU \u6700\u5c0f MHz\uff1a                        800.0000\nBogoMIPS\uff1a                            6374.40\n\u6807\u8bb0\uff1a                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect user_shstk avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities\n\u865a\u62df\u5316\uff1a                              VT-x\nL1d \u7f13\u5b58\uff1a                            896 KiB (24 instances)\nL1i \u7f13\u5b58\uff1a                            1.3 MiB (24 instances)\nL2 \u7f13\u5b58\uff1a                             32 MiB (12 instances)\nL3 \u7f13\u5b58\uff1a                             36 MiB (1 instance)\nNUMA \u8282\u70b9\uff1a                           1\nNUMA \u8282\u70b90 CPU\uff1a                      0-31\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Mitigation; Clear Register File\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\n==============================\nVersions of relevant libraries\n==============================\n[pip3] numpy==2.1.2\n[pip3] nvidia-cublas-cu12==12.8.3.14\n[pip3] nvidia-cuda-cupti-cu12==12.8.57\n[pip3] nvidia-cuda-nvrtc-cu12==12.8.61\n[pip3] nvidia-cuda-runtime-cu12==12.8.57\n[pip3] nvidia-cudnn-cu12==9.7.1.26\n[pip3] nvidia-cufft-cu12==11.3.3.41\n[pip3] nvidia-cufile-cu12==1.13.0.11\n[pip3] nvidia-curand-cu12==10.3.9.55\n[pip3] nvidia-cusolver-cu12==11.7.2.55\n[pip3] nvidia-cusparse-cu12==12.5.7.53\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-ml-py==12.575.51\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.8.61\n[pip3] nvidia-nvtx-cu12==12.8.55\n[pip3] pynvml==12.0.0\n[pip3] pyzmq==27.0.0\n[pip3] torch==2.7.1+cu128\n[pip3] torchaudio==2.7.1+cu128\n[pip3] torchcodec==0.4.0\n[pip3] torchdata==0.11.0\n[pip3] torchvision==0.22.1+cu128\n[pip3] transformers==4.51.3\n[pip3] triton==3.3.1\n[conda] Could not collect\n\n==============================\n         vLLM Info\n==============================\nROCM Version                 : Could not collect\nNeuron SDK Version           : N/A\nvLLM Version                 : 0.9.1.dev0+g587387724.d20250617 (git sha: 587387724, date: 20250617)\nvLLM Build Flags:\n  CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\nGPU Topology:\n  \t\u001b[4mGPU0\tGPU1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\nGPU0\t X \tPHB\t0-31\t0\t\tN/A\nGPU1\tPHB\t X \t0-31\t0\t\tN/A\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n==============================\n     Environment Variables\n==============================\nNCCL_CUMEM_ENABLE=0\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nTORCHINDUCTOR_COMPILE_THREADS=1\nCUDA_MODULE_LOADING=LAZY\n\n\n\n### \ud83d\udc1b Describe the bug\n\nI tried using the command to install vllm, and it runs without issues on a single GPU. However, when running on multiple GPUs (setting tensor_parallel_size=2), it throws an error.\n\n```\ngit clone https://github.com/vllm-project/vllm\ncd vllm/\ngit checkout tags/v0.9.0 -b mybranch\nconda install -c conda-forge libstdcxx-ng\npip install torch==2.7.0 --index-url https://download.pytorch.org/whl/cu128\npython use_existing_torch.py\npip list |grep torch\nuv pip install -r requirements/build.txt && uv pip install -r requirements/common.txt && MAX_JOBS=16 uv pip install -e . --no-build-isolation\n```\n\n```\n  File \"/home/lc/EasyR1/verl/single_controller/ray/base.py\", line 432, in func\n    return getattr(self.worker_dict[key], name)(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lc/EasyR1/verl/single_controller/base/decorator.py\", line 207, in inner\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lc/EasyR1/verl/workers/fsdp_workers.py\", line 366, in init_model\n    self._build_model_optimizer(\n  File \"/home/lc/EasyR1/verl/workers/fsdp_workers.py\", line 276, in _build_model_optimizer\n    fsdp_module = FSDP(\n                  ^^^^^\n  File \"/home/lc/.venv/easyr1/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 473, in __init__\n    _auto_wrap(\n  File \"/home/lc/.venv/easyr1/lib/python3.11/site-packages/torch/distributed/fsdp/_wrap_utils.py\", line 101, in _auto_wrap\n    _recursive_wrap(**recursive_wrap_kwargs, **root_kwargs)  # type: ignore[arg-type]\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lc/.venv/easyr1/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py\", line 533, in _recursive_wrap\n    wrapped_child, num_wrapped_params = _recursive_wrap(\n                                        ^^^^^^^^^^^^^^^^\n  File \"/home/lc/.venv/easyr1/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py\", line 533, in _recursive_wrap\n    wrapped_child, num_wrapped_params = _recursive_wrap(\n                                        ^^^^^^^^^^^^^^^^\n  File \"/home/lc/.venv/easyr1/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py\", line 533, in _recursive_wrap\n    wrapped_child, num_wrapped_params = _recursive_wrap(\n                                        ^^^^^^^^^^^^^^^^\n  File \"/home/lc/.venv/easyr1/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py\", line 551, in _recursive_wrap\n    return _wrap(module, wrapper_cls, **kwargs), nonwrapped_numel\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lc/.venv/easyr1/lib/python3.11/site-packages/torch/distributed/fsdp/wrap.py\", line 480, in _wrap\n    return wrapper_cls(module, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/lc/.venv/easyr1/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 499, in __init__\n    _init_param_handle_from_module(\n  File \"/home/lc/.venv/easyr1/lib/python3.11/site-packages/torch/distributed/fsdp/_init_utils.py\", line 615, in _init_param_handle_from_module\n    _sync_module_params_and_buffers(\n  File \"/home/lc/.venv/easyr1/lib/python3.11/site-packages/torch/distributed/fsdp/_init_utils.py\", line 1112, in _sync_module_params_and_buffers\n    _sync_params_and_buffers(\n  File \"/home/lc/.venv/easyr1/lib/python3.11/site-packages/torch/distributed/utils.py\", line 322, in _sync_params_and_buffers\n    dist._broadcast_coalesced(\ntorch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:3356, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.26.2\nncclUnhandledCudaError: Call to CUDA function failed.\nLast error:\nCuda failure 700 'an illegal memory access was encountered'\n(WorkerDict pid=3290198)\n(WorkerDict pid=3290198) [2025-06-20 10:15:41] lc-System-Product-Name:3290198:3290198 [0] enqueue.cc:1556 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'\n(WorkerDict pid=3291061)\n(WorkerDict pid=3291061) [2025-06-20 10:15:41] lc-System-Product-Name:3291061:3291061 [0] enqueue.cc:1556 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'\n```\n\n### Before submitting a new issue...\n\n- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2025-06-20T02:39:23+00:00",
    "closed_at": "2025-06-20T02:47:00+00:00",
    "comments": 1,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/19890/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/19890"
  },
  {
    "number": 8410,
    "title": "[Bug]: Obvious hang caused by Custom All Reduce OP\uff08Valuable Debug Info Obtained\uff09",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121                                                                                                                                                                           \r\nIs debug build: False                                                                                                                                                                                  \r\nCUDA used to build PyTorch: 12.1                                                                                                                                                                       \r\nROCM used to build PyTorch: N/A                                                                                                                                                                        \r\n                                                                                                                                                                                                       \r\nOS: Ubuntu 20.04.6 LTS (x86_64)                                                                                                                                                                        \r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0                                                                                                                                                     \r\nClang version: Could not collect                                                                                                                                                                       \r\nCMake version: version 3.30.1                                                                                                                                                                          \r\nLibc version: glibc-2.31                                                                                                                                                                               \r\n                                                                                                                                                                                                       \r\nPython version: 3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0] (64-bit runtime)                                                                                                                     \r\nPython platform: Linux-5.4.0-125-generic-x86_64-with-glibc2.31                                                                                                                                         \r\nIs CUDA available: True                                                                                                                                                                                \r\nCUDA runtime version: Could not collect                                                                                                                                                                \r\nCUDA_MODULE_LOADING set to: LAZY                                                                                                                                                                       \r\nGPU models and configuration:                                                                                                                                                                          \r\nGPU 0: NVIDIA A800-SXM4-80GB                                                                                                                                                                           \r\nGPU 1: NVIDIA A800-SXM4-80GB                                                                                                                                                                           \r\nGPU 2: NVIDIA A800-SXM4-80GB                                                                                                                                                                           \r\nGPU 3: NVIDIA A800-SXM4-80GB                                                                                                                                                                           \r\nGPU 4: NVIDIA A800-SXM4-80GB                                                                                                                                                                           \r\nGPU 5: NVIDIA A800-SXM4-80GB                                                                                                                                                                           \r\nGPU 6: NVIDIA A800-SXM4-80GB                                                                                                                                                                           \r\nGPU 7: NVIDIA A800-SXM4-80GB                                                                                                                                                                           \r\n                                                                                                                                                                                                       \r\nNvidia driver version: 525.105.17                                                                                                                                                                      \r\ncuDNN version: Could not collect                                                                                                                                                                       \r\nHIP runtime version: N/A                                                                                                                                                                               \r\nMIOpen runtime version: N/A                                                                                                                                                                            \r\nIs XNNPACK available: True                                                                                                                                                                             \r\n                                                                                                                                                                                                       \r\nCPU:                                                                                                                                                                                                   \r\nArchitecture:                    x86_64                                                                                                                                                                \r\nCPU op-mode(s):                  32-bit, 64-bit                                                                                                                                                        \r\nByte Order:                      Little Endian                                                                                                                                                         \r\nAddress sizes:                   46 bits physical, 57 bits virtual                                                                                                                                     \r\nCPU(s):                          128                                                                                                                                                                   \r\nOn-line CPU(s) list:             0-127                                                                                                                                                                 \r\nThread(s) per core:              2                                                                                                                                                                     \r\nCore(s) per socket:              32                                                                                                                                                                    \r\nSocket(s):                       2                                                                                                                                                                     \r\nNUMA node(s):                    2                                                                                                                                                                     \r\nVendor ID:                       GenuineIntel                                                                                                                                                          \r\nCPU family:                      6                                                                                                                                                                     \r\nModel:                           106                                                                                                                                                                   \r\nModel name:                      Intel(R) Xeon(R) Platinum 8358P CPU @ 2.60GHz                                                                                                                         \r\nStepping:                        6                                                                                                                                                                     \r\nFrequency boost:                 enabled                                                                                                                                                               \r\nCPU MHz:                         900.000                                                                                                                                                               \r\nCPU max MHz:                     3400.0000                                                                                                                                                             \r\nCPU min MHz:                     800.0000                                                                                                                                                              \r\nBogoMIPS:                        5200.00                                                                                                                                                               \r\nVirtualization:                  VT-x                                                                                                                                                                  \r\nL1d cache:                       3 MiB                                                                                                                                                                 \r\nL1i cache:                       2 MiB                                                                                                                                                                 \r\nL2 cache:                        80 MiB                                                                                                                                                                \r\nL3 cache:                        96 MiB                                                                                                                                                                \r\nNUMA node0 CPU(s):               0-31,64-95                                                                                                                                                            \r\nNUMA node1 CPU(s):               32-63,96-127                                                                                                                                                          \r\nVulnerability Itlb multihit:     Not affected                                                                                                                                                          \r\nVulnerability L1tf:              Not affected                                                                                                                                                          \r\nVulnerability Mds:               Not affected                                                                                                                                                          \r\nVulnerability Meltdown:          Not affected                                                                                                                                                          \r\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable                                                                                                                         \r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp                                                                                                   \r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization                                                                                                  \r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling                                                                                                              \r\nVulnerability Srbds:             Not affected                                                                                                                                                          \r\nVulnerability Tsx async abort:   Not affected                                                                                                                                                          \r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art \r\narch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_dea\r\ndline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_\r\nadjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occ\r\nup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig f\r\nlush_l1d arch_capabilities                                                                                                                                                                             \r\n                                                                                                                                                                                                       \r\nVersions of relevant libraries:                                                                                                                                                                        \r\n[pip3] flashinfer==0.0.9+cu121torch2.3                                                                                                                                                                 \r\n[pip3] mypy==1.11.2                                                                                                                                                                                    \r\n[pip3] mypy-extensions==1.0.0                                                                                                                                                                          \r\n[pip3] numpy==1.26.4                                                                                                                                                                                   \r\n[pip3] nvidia-cublas-cu12==12.1.3.1                                                                                                                                                                    \r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105                                                                                                                                                                \r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105                                                                                                                                                                \r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105                                                                                                                                                              \r\n[pip3] nvidia-cudnn-cu12==9.1.0.70                                                                                                                                                                     \r\n[pip3] nvidia-cufft-cu12==11.0.2.54                                                                                                                                                                    \r\n[pip3] nvidia-curand-cu12==10.3.2.106                                                                                                                                                                  \r\n[pip3] nvidia-cusolver-cu12==11.4.5.107                                                                                                                                                                \r\n[pip3] nvidia-cusparse-cu12==12.1.0.106                                                                                                                                                                \r\n[pip3] nvidia-ml-py==12.555.43                                                                                                                                                                         \r\n[pip3] nvidia-nccl-cu12==2.20.5                                                                                                                                                                        \r\n[pip3] nvidia-nvjitlink-cu12==12.5.82                                                                                                                                                                  \r\n[pip3] nvidia-nvtx-cu12==12.1.105                                                                                                                                                                      \r\n[pip3] pyzmq==26.0.3                                                                                                                                                                                   \r\n[pip3] torch==2.4.0                                                                                                                                                                                    \r\n[pip3] torchvision==0.19.0                                                                                                                                                                             \r\n[pip3] transformers==4.43.2                                                                                                                                                                            \r\n[pip3] triton==3.0.0                                                                                                                                                                                   \r\n[conda] Could not collect                                                                                                                                                                              \r\nROCM Version: Could not collect                                                                                                                                                                        \r\nNeuron SDK Version: N/A                                                                                                                                                                                \r\nvLLM Version: 0.5.4@4db5176d9758b720b05460c50ace3c01026eb158                                                                                                                                           \r\nvLLM Build Flags:                                                                                                                                                                                      \r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled                                                                                                                                                  \r\nGPU Topology:                                                                                                                                                                                          \r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    CPU Affinity    NUMA Affinity                                          \r\nGPU0     X      NV8     NV8     NV8     NV8     NV8     NV8     NV8     PXB     PXB     PXB     NODE    SYS     SYS     SYS     SYS     0-31,64-95      0                                              \r\nGPU1    NV8      X      NV8     NV8     NV8     NV8     NV8     NV8     PXB     PXB     PXB     NODE    SYS     SYS     SYS     SYS     0-31,64-95      0                                              \r\nGPU2    NV8     NV8      X      NV8     NV8     NV8     NV8     NV8     NODE    NODE    NODE    PXB     SYS     SYS     SYS     SYS     0-31,64-95      0                                              \r\nGPU3    NV8     NV8     NV8      X      NV8     NV8     NV8     NV8     NODE    NODE    NODE    PXB     SYS     SYS     SYS     SYS     0-31,64-95      0                                              \r\nGPU4    NV8     NV8     NV8     NV8      X      NV8     NV8     NV8     SYS     SYS     SYS     SYS     PXB     PXB     PXB     NODE    32-63,96-127    1                                              \r\nGPU5    NV8     NV8     NV8     NV8     NV8      X      NV8     NV8     SYS     SYS     SYS     SYS     PXB     PXB     PXB     NODE    32-63,96-127    1                                              \r\nGPU6    NV8     NV8     NV8     NV8     NV8     NV8      X      NV8     SYS     SYS     SYS     SYS     NODE    NODE    NODE    PXB     32-63,96-127    1                                              \r\nGPU7    NV8     NV8     NV8     NV8     NV8     NV8     NV8      X      SYS     SYS     SYS     SYS     NODE    NODE    NODE    PXB     32-63,96-127    1                                              \r\nNIC0    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS      X      PIX     PIX     NODE    SYS     SYS     SYS     SYS                                                                    \r\nNIC1    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     PIX      X      PIX     NODE    SYS     SYS     SYS     SYS                                                                    \r\nNIC2    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     PIX     PIX      X      NODE    SYS     SYS     SYS     SYS                                                                    \r\nNIC3    NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      SYS     SYS     SYS     SYS                                                                    \r\nNIC4    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS      X      PIX     PIX     NODE                                                                   \r\nNIC5    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     PIX      X      PIX     NODE                                                                   \r\nNIC6    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     PIX     PIX      X      NODE                                                                   \r\nNIC7    SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE    NODE    NODE     X                                                                     \r\n                                                                                                                                                                                                       \r\nLegend:                                                                                                                                                                                                \r\n                                                                                                                                                                                                       \r\n  X    = Self                                                                                                                                                                                          \r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)                                                                                                 \r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node                                                                                           \r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)                                                                                                                  \r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)                                                                                                         \r\n  PIX  = Connection traversing at most a single PCIe bridge                                                                                                                                            \r\n  NV#  = Connection traversing a bonded set of # NVLinks                                                                                                                                               \r\n                                                                                                                                                                                                       \r\nNIC Legend:                                                                                                                                                                                            \r\n                                                                                                                                                                                                       \r\n  NIC0: mlx5_0                                                                                                                                                                                         \r\n  NIC1: mlx5_1                                                                                                                                                                                         \r\n  NIC2: mlx5_2                                                                                                                                                                                         \r\n  NIC3: mlx5_3                                                                                                                                                                                         \r\n  NIC4: mlx5_4                                                                                                                                                                                         \r\n  NIC5: mlx5_5                                                                                                                                                                                         \r\n  NIC6: mlx5_6                                                                                                                                                                                         \r\n  NIC7: mlx5_7\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\nNot available at the moment.\r\n\r\n### \ud83d\udc1b Describe the bug\r\n\r\nThe problem I encountered was \"NCCL hangs and causes timeout\", which is  similar to https://github.com/vllm-project/vllm/issues/5484\r\n\r\nBefore debugging, I have read https://docs.vllm.ai/en/latest/getting_started/debugging.html \uff0cand add additional log in https://github.com/vllm-project/vllm/blob/7de49aa86c7f169eb0962b6db29ad53fff519ffb/vllm/_custom_ops.py#L825-L827\r\n\r\n\r\n\r\nHere is the service launch code for debugging\r\n`\r\nCUDA_LAUNCH_BLOCKING=1 python3 -m vllm.entrypoints.openai.api_server --served-model-name Qwen2-72B-Instrust --model /data/Qwen2-72B-Instruct --tensor-parallel-size 8 --port 8000 --max_model_len 4096 --trust-remote-code --max_num_seqs 32 --dytpe bfloat16 --enforce-eager\r\n`\r\n\r\nWhen the hang occurs, I see an obvious phenomenon, as shown in the picture below\uff1a\r\n![1726142733383](https://github.com/user-attachments/assets/8f60b020-b87a-4c09-bee4-d646e9185218)\r\n\r\n\r\n\uff081\uff09Obviously, the root cause is that RANK2 is hanged. In my opinion they should end communication at the same time and print a log, when \u2018CUDA_LAUNCH_BLOCKING=1\u2019 was added.\r\n\uff082\uff09\u2018--disable_custom_all_reduce\u2019 seem to work.\r\n\uff083\uff09This bug appears in both version 0.5.4 and version 0.6.0.\r\nI'd be happy to take more debugging guidance from you and provide more debugging information.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "labels": [
      "bug"
    ],
    "state": "closed",
    "created_at": "2024-09-12T12:19:12+00:00",
    "closed_at": "2024-09-24T08:08:15+00:00",
    "comments": 6,
    "reactions": {
      "url": "https://api.github.com/repos/vllm-project/vllm/issues/8410/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "author_association": "NONE",
    "html_url": "https://github.com/vllm-project/vllm/issues/8410"
  }
]